<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs</link>


<item>
<title>Graph Neural Networks and Reinforcement Learning for Proactive Application Image Placement</title>
<link>https://arxiv.org/abs/2407.00007</link>
<guid>https://arxiv.org/abs/2407.00007</guid>
<content:encoded><![CDATA[
<div> 关键词：Edge computing, Cloud-Edge continuum, Service placement, Application placement, Reinforcement Learning.

总结:<br />
云计算向云边计算的转变为数据密集型和交互式应用带来了机遇与挑战。边缘计算作为下一代应用严格需求的关键支持，通过将计算任务移至用户附近，实现了低延迟和高带宽。然而，边缘计算的分布式、动态和异构特性使得服务部署成为一个难题。本文提出了一种结合图神经网络和强化学习（Actor-Critic）的前瞻性图像部署方法，旨在减少图像传输时间并优化应用部署。尽管在某些情况下可能导致执行时间稍长，但实验结果显示，该方法在整体应用部署上表现更优。 <div>
arXiv:2407.00007v1 Announce Type: new 
Abstract: The shift from Cloud Computing to a Cloud-Edge continuum presents new opportunities and challenges for data-intensive and interactive applications. Edge computing has garnered a lot of attention from both industry and academia in recent years, emerging as a key enabler for meeting the increasingly strict demands of Next Generation applications. In Edge computing the computations are placed closer to the end-users, to facilitate low-latency and high-bandwidth applications and services. However, the distributed, dynamic, and heterogeneous nature of Edge computing, presents a significant challenge for service placement. A critical aspect of Edge computing involves managing the placement of applications within the network system to minimize each application's runtime, considering the resources available on system devices and the capabilities of the system's network. The placement of application images must be proactively planned to minimize image tranfer time, and meet the strict demands of the applications. In this regard, this paper proposes an approach for proactive image placement that combines Graph Neural Networks and actor-critic Reinforcement Learning, which is evaluated empirically and compared against various solutions. The findings indicate that although the proposed approach may result in longer execution times in certain scenarios, it consistently achieves superior outcomes in terms of application placement.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>A New Approach for Evaluating the Performance of Distributed Latency-Sensitive Services</title>
<link>https://arxiv.org/abs/2407.00015</link>
<guid>https://arxiv.org/abs/2407.00015</guid>
<content:encoded><![CDATA[
<div> 关键词：latency metrics, Service Level Agreement (SLA), distributed computing, immersive services, latency performance.

总结:<br />本文提出了一篇关于新型延迟度量标准（latency metrics）的研究论文，针对传统指标在评估现代服务和分布式计算环境中性能的局限性。研究者强调了现有指标无法充分捕捉两个关键性能方面：超过服务级别协议（SLA）阈值的频率以及恢复到可接受水平的时间。为解决这一问题，作者开发了五个创新的延迟度量，它们能够提供更深入的服务性能洞察。这些新指标尤其适用于对低延迟有严格要求的沉浸式服务。论文通过大规模实验验证了新指标的有效性和实用性，以促进服务性能优化。 <div>
arXiv:2407.00015v1 Announce Type: new 
Abstract: Conventional latency metrics are formulated based on a broad definition of traditional monolithic services, and hence lack the capacity to address the complexities inherent in modern services and distributed computing paradigms. Consequently, their effectiveness in identifying areas for improvement is restricted, falling short of providing a comprehensive evaluation of service performance within the context of contemporary services and computing paradigms. More specifically, these metrics do not offer insights into two critical aspects of service performance: the frequency of latency surpassing specified Service Level Agreement (SLA) thresholds and the time required for latency to return to an acceptable level once the threshold is exceeded. This limitation is quite significant in the frame of contemporary latency-sensitive services, and especially immersive services that require deterministic low latency that behaves in a consistent manner. Towards addressing this limitation, the authors of this work propose 5 novel latency metrics that when leveraged alongside the conventional latency metrics manage to provide advanced insights that can be potentially used to improve service performance. The validity and usefulness of the proposed metrics in the frame of providing advanced insights into service performance is evaluated using a large-scale experiment.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Preble: Efficient Distributed Prompt Scheduling for LLM Serving</title>
<link>https://arxiv.org/abs/2407.00023</link>
<guid>https://arxiv.org/abs/2407.00023</guid>
<content:encoded><![CDATA[
<div> 关键词：Preble、大型语言模型（LLMs）、分布式 serving 平台、prompt 分享、计算重用。

总结:
Preble 是一项创新，它提出了一种针对大型语言模型（LLMs）服务的分布式平台，专注于优化提示共享。文章研究了五个流行的工作负载，发现当前系统忽视了重复提示的注意力计算可以复用的潜力。为此，Preble 设计了一个分布式调度系统，兼顾计算重用和负载均衡。实验证明，与现有技术相比，Preble 在两到八GPU上分别提高了平均延迟1.5倍至14.5倍和p99值的2倍至10倍，显著提升了性能。 <div>
arXiv:2407.00023v1 Announce Type: new 
Abstract: Prompts to large language models (LLMs) have evolved beyond simple user questions. For LLMs to solve complex problems, today's practices include domain-specific instructions, illustration of tool usages, and long context, such as textbook chapters in prompts. As such, many parts of prompts are repetitive across requests, and their attention computation results can be reused. However, today's LLM serving systems treat every request in isolation, missing the opportunity of computation reuse.
  This paper proposes Preble, the first distributed LLM serving platform that targets and optimizes for prompt sharing. We perform a study on five popular LLM workloads. Based on our study results, we designed a distributed scheduling system that co-optimizes computation reuse and load balancing. Our evaluation of Preble on two to 8 GPUs with real workloads and request arrival patterns on two open-source LLM models shows that Preble outperforms the state-of-the-art average latency by 1.5X to 14.5X and p99 by 2X to 10X.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Distributed Inference Performance Optimization for LLMs on CPUs</title>
<link>https://arxiv.org/abs/2407.00029</link>
<guid>https://arxiv.org/abs/2407.00029</guid>
<content:encoded><![CDATA[
<div> 关键词：large language models (LLMs), distributed computing, resource-limited hardware, memory capacity, inference optimization.

总结:
本文介绍了一种针对大型语言模型（LLMs）的分布式推理优化解决方案，旨在缓解资源受限硬件设备上的内存限制和提高计算性能。该方法特别适用于部署在5代Intel Xeon Scalable处理器上。实验结果显示，使用72亿参数的LLM，通过优化后的方案，每输出一个令牌的时间降低到140毫秒，远低于人类阅读的平均速度（约200毫秒/令牌），显著提高了效率。这一成果表明分布式计算在有效利用CPU资源方面具有巨大潜力。 <div>
arXiv:2407.00029v1 Announce Type: new 
Abstract: Large language models (LLMs) hold tremendous potential for addressing numerous real-world challenges, yet they typically demand significant computational resources and memory. Deploying LLMs onto a resource-limited hardware device with restricted memory capacity presents considerable challenges. Distributed computing emerges as a prevalent strategy to mitigate single-node memory constraints and expedite LLM inference performance. To reduce the hardware limitation burden, we proposed an efficient distributed inference optimization solution for LLMs on CPUs. We conduct experiments with the proposed solution on 5th Gen Intel Xeon Scalable Processors, and the result shows the time per output token for the LLM with 72B parameter is 140 ms/token, much faster than the average human reading speed about 200ms per token.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>On Orchestrating Parallel Broadcasts for Distributed Ledgers</title>
<link>https://arxiv.org/abs/2407.00030</link>
<guid>https://arxiv.org/abs/2407.00030</guid>
<content:encoded><![CDATA[
<div> 关键词：ticketing, atomic broadcasts, distributed system, managed ticketing, unmanaged ticketing

总结:<br />本文主要探讨了“票证”(ticketing)的概念在分布式系统中对原子广播的组织。文章研究了不同类型的票证制度，允许并行执行但防止慢节点影响整体进度。一种混合方案被提出，结合了管理和未管理的票证模式，旨在平衡适应性和鲁棒性。性能评估显示，无论是静态还是动态场景，资源异构的系统中，管理票证制度对吞吐量有优势，因为它能更好地适应。最后，实验表明，使用混合票证制度可以同时享受管理票证的适应性与未管理票证的活度保证。 <div>
arXiv:2407.00030v1 Announce Type: new 
Abstract: This paper introduces and develops the concept of ``ticketing'', through which atomic broadcasts are orchestrated by nodes in a distributed system. The paper studies different ticketing regimes that allow parallelism, yet prevent slow nodes from hampering overall progress. It introduces a hybrid scheme which combines managed and unmanaged ticketing regimes, striking a balance between adaptivity and resilience. The performance evaluation demonstrates how managed and unmanaged ticketing regimes benefit throughput in systems with heterogeneous resources both in static and dynamic scenarios, with the managed ticketing regime performing better among the two as it adapts better. Finally, it demonstrates how using the hybrid ticketing regime performance can enjoy both the adaptivity of the managed regime and the liveness guarantees of the unmanaged regime.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Distributed Systems in Fintech</title>
<link>https://arxiv.org/abs/2407.00034</link>
<guid>https://arxiv.org/abs/2407.00034</guid>
<content:encoded><![CDATA[
<div> 关键词：分布式系统、金融技术（Fintech）、区块链、去中心化金融（DeFi）、分布式 ledger technology (DLT)

总结:<br />
分布式系统在Fintech中的作用日益凸显，推动了行业革新。本文分析了分布式系统的架构，探讨了它们如何通过区块链、DeFi和DLT提升金融操作的安全性、扩展性和效率。这些技术影响了金融行业的多个方面，如支付、资产管理等。未来，分布式系统有望引领Fintech的进一步发展，实现更透明、包容的金融服务生态。 <div>
arXiv:2407.00034v1 Announce Type: new 
Abstract: The emergence of distributed systems has revolutionized the financial technology (Fintech) landscape, offering unprecedented opportunities for enhancing security, scalability, and efficiency in financial operations. This paper explores the role of distributed systems in Fintech, analyzing their architecture, benefits, challenges, and applications. It examines key distributed technologies such as blockchain, decentralized finance (DeFi), and distributed ledger technology (DLT), and their impact on various aspects of the financial industry, and future directions for distributed systems in Fintech.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Streamline Intelligent Crowd Monitoring with IoT Cloud Computing Middleware</title>
<link>https://arxiv.org/abs/2407.00045</link>
<guid>https://arxiv.org/abs/2407.00045</guid>
<content:encoded><![CDATA[
<div> 关键词：middleware, Raspberry Pi, wireless sensor networks (WSNs), MapReduce, fault tolerance

总结:<br />
本文介绍了一种新颖的中间件系统，它利用低成本、低功耗的设备如Raspberry Pi分析无线传感器网络（WSN）的数据。该系统针对室内环境，如历史建筑和博物馆，用于监控访客、识别兴趣点并作为疏散辅助工具。通过MapReduce算法收集和分布数据，结合故障容忍的领导选举算法，其性能与资源密集型方法相当，但使用简单硬件。在希腊一处历史建筑（哈茨迪亚基斯故居）进行了成功测试。与现有实现相比，这种设计的优势在于其经济、分布式且具有故障恢复能力。尤其在COVID-19大流行期间，这种中间件对于室内位置监控具有重要意义，能有效追踪访客数量和整体建筑占用情况。 <div>
arXiv:2407.00045v1 Announce Type: new 
Abstract: This article introduces a novel middleware that utilizes cost-effective, low-power computing devices like Raspberry Pi to analyze data from wireless sensor networks (WSNs). It is designed for indoor settings like historical buildings and museums, tracking visitors and identifying points of interest. It serves as an evacuation aid by monitoring occupancy and gauging the popularity of specific areas, subjects, or art exhibitions. The middleware employs a basic form of the MapReduce algorithm to gather WSN data and distribute it across available computer nodes. Data collected by RFID sensors on visitor badges is stored on mini-computers placed in exhibition rooms and then transmitted to a remote database after a preset time frame. Utilizing MapReduce for data analysis and a leader election algorithm for fault tolerance, this middleware showcases its viability through metrics, demonstrating applications like swift prototyping and accurate validation of findings. Despite using simpler hardware, its performance matches resource-intensive methods involving audiovisual and AI techniques. This design's innovation lies in its fault-tolerant, distributed setup using budget-friendly, low-power devices rather than resource-heavy hardware or methods. Successfully tested at a historical building in Greece (M. Hatzidakis' residence), it is tailored for indoor spaces. This paper compares its algorithmic application layer with other implementations, highlighting its technical strengths and advantages. Particularly relevant in the wake of the COVID-19 pandemic and general monitoring middleware for indoor locations, this middleware holds promise in tracking visitor counts and overall building occupancy.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Tracing Distributed Algorithms Using Replay Clocks</title>
<link>https://arxiv.org/abs/2407.00069</link>
<guid>https://arxiv.org/abs/2407.00069</guid>
<content:encoded><![CDATA[
<div> 关键词：replay clocks (RepCl), distributed computations, concurrent events, constraint violations, visualization.

总结:<br />
本文介绍了一种新的时钟基础设施——replay clocks (RepCl)，它旨在支持离线分析分布式计算。RepCl结合了向量时钟(VC)和混合逻辑时钟(HLC)的优势，提供高效重放功能，允许用户检查并发事件下的约束条件和潜在执行路径。文章强调了RepCl的低开销实现（最多4个整数表示64个进程）以及与同步时间的关系。通过模拟和NS-3网络模拟器，作者评估了RepCl的预期开销，并确定了其可行性的区域。此外，文中提出了一种基于RepCl的分布式计算追踪器，可实时分析系统的特性，同时考虑并发路径。这个可视化工具提供了逐进程和全局视图，便于深入理解计算过程。 <div>
arXiv:2407.00069v1 Announce Type: new 
Abstract: In this thesis, we introduce replay clocks (RepCl), a novel clock infrastructure that allows us to do offline analyses of distributed computations. The replay clock structure provides a methodology to replay a computation as it happened, with the ability to represent concurrent events effectively. It builds on the structures introduced by vector clocks (VC) and the Hybrid Logical Clock (HLC), combining their infrastructures to provide efficient replay. With such a clock, a user can replay a computation whilst considering multiple paths of executions, and check for constraint violations and properties that potential pathways could take in the presence of concurrent events. Specifically, if event e must occur before f then the replay clock must ensure that e is replayed before f. On the other hand, if e and f could occur in any order, replay should not force an order between them. We demonstrate that RepCl can be implemented with less than four integers for 64 processes for various system parameters if clocks are synchronized within 1ms. Furthermore, the overhead of RepCl (for computing timestamps and message size) is proportional to the size of the clock. Using simulations in a custom distributed system and NS-3, a state-of-the-art network simulator, we identify the expected overhead of RepCl. We also identify how a user can then identify feasibility region for RepCl, where unabridged replay is possible. Using the RepCl, we provide a tracer for distributed computations, that allows any computation using the RepCl to be replayed efficiently. The visualization allows users to analyze specific properties and constraints in an online fashion, with the ability to consider concurrent paths independently. The visualization provides per-process views and an overarching view of the whole computation based on the time recorded by the RepCl for each event.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Decentralized Task Offloading and Load-Balancing for Mobile Edge Computing in Dense Networks</title>
<link>https://arxiv.org/abs/2407.00080</link>
<guid>https://arxiv.org/abs/2407.00080</guid>
<content:encoded><![CDATA[
<div> 关键词：decentralized task offloading, load-balancing, dense network, edge servers, mean field multi-agent MAB game.

总结:
该研究关注于密集网络中众多设备与边缘服务器间的任务卸载与负载均衡问题。由于网络信息未知和任务大小随机，优化该问题颇具挑战。论文提出结合mean field多代理多臂赌博游戏(MAB)模型与动态调整服务器奖励的负载平衡策略，旨在实现目标用户分布，即使在分布式决策制定中也能达到平衡。数值结果验证了方法的有效性，并展示了其能导向目标负载分布的收敛性能。 <div>
arXiv:2407.00080v1 Announce Type: new 
Abstract: We study the problem of decentralized task offloading and load-balancing in a dense network with numerous devices and a set of edge servers. Solving this problem optimally is complicated due to the unknown network information and random task sizes. The shared network resources also influence the users' decisions and resource distribution. Our solution combines the mean field multi-agent multi-armed bandit (MAB) game with a load-balancing technique that adjusts the servers' rewards to achieve a target population profile despite the distributed user decision-making. Numerical results demonstrate the efficacy of our approach and the convergence to the target load distribution.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Predicting Elevated Risk of Hospitalization Following Emergency Department Discharges</title>
<link>https://arxiv.org/abs/2407.00147</link>
<guid>https://arxiv.org/abs/2407.00147</guid>
<content:encoded><![CDATA[
<div> 关键词：数据挖掘、医院化、急诊部门、预测准确性、诊断错误。

总结: 这篇文章探讨了如何利用数据挖掘技术分析大型医院化数据，以预测患者在急诊部门出院后可能的早期住院。通过组合运用逻辑回归、朴素贝叶斯和关联规则分类器，研究者实现了对3天、7天和14天内住院的高精度预测。这种方法不仅准确，而且生成的可解释模型便于医生理解，规则可以直接转化为实践中的决策工具，帮助他们在患者出院前识别潜在的早期住院风险，从而改善诊断质量和患者安全。 <div>
arXiv:2407.00147v1 Announce Type: new 
Abstract: Hospitalizations that follow closely on the heels of one or more emergency department visits are often symptoms of missed opportunities to form a proper diagnosis. These diagnostic errors imply a failure to recognize the need for hospitalization and deliver appropriate care, and thus also bear important connotations for patient safety. In this paper, we show how data mining techniques can be applied to a large existing hospitalization data set to learn useful models that predict these upcoming hospitalizations with high accuracy. Specifically, we use an ensemble of logistics regression, na\"ive Bayes and association rule classifiers to successfully predict hospitalization within 3, 7 and 14 days of an emergency department discharge. Aside from high accuracy, one of the advantages of the techniques proposed here is that the resulting classifier is easily inspected and interpreted by humans so that the learned rules can be readily operationalized. These rules can then be easily distributed and applied directly by physicians in emergency department settings to predict the risk of early admission prior to discharging their emergency department patients.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Dual-view Aware Smart Contract Vulnerability Detection for Ethereum</title>
<link>https://arxiv.org/abs/2407.00336</link>
<guid>https://arxiv.org/abs/2407.00336</guid>
<content:encoded><![CDATA[
arXiv:2407.00336v1 Announce Type: new 
Abstract: The wide application of Ethereum technology has brought technological innovation to traditional industries. As one of Ethereum's core applications, smart contracts utilize diverse contract codes to meet various functional needs and have gained widespread use. However, the non-tamperability of smart contracts, coupled with vulnerabilities caused by natural flaws or human errors, has brought unprecedented challenges to blockchain security. Therefore, in order to ensure the healthy development of blockchain technology and the stability of the blockchain community, it is particularly important to study the vulnerability detection techniques for smart contracts. In this paper, we propose a Dual-view Aware Smart Contract Vulnerability Detection Framework named DVDet. The framework initially converts the source code and bytecode of smart contracts into weighted graphs and control flow sequences, capturing potential risk features from these two perspectives and integrating them for analysis, ultimately achieving effective contract vulnerability detection. Comprehensive experiments on the Ethereum dataset show that our method outperforms others in detecting vulnerabilities.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>C-MASS: Combinatorial Mobility-Aware Sensor Scheduling for Collaborative Perception with Second-Order Topology Approximation</title>
<link>https://arxiv.org/abs/2407.00412</link>
<guid>https://arxiv.org/abs/2407.00412</guid>
<content:encoded><![CDATA[
arXiv:2407.00412v1 Announce Type: new 
Abstract: Collaborative Perception (CP) has been a promising solution to address occlusions in the traffic environment by sharing sensor data among collaborative vehicles (CoV) via vehicle-to-everything (V2X) network. With limited wireless bandwidth, CP necessitates task-oriented and receiver-aware sensor scheduling to prioritize important and complementary sensor data. However, due to vehicular mobility, it is challenging and costly to obtain the up-to-date perception topology, i.e., whether a combination of CoVs can jointly detect an object. In this paper, we propose a combinatorial mobility-aware sensor scheduling (C-MASS) framework for CP with minimal communication overhead. Specifically, detections are replayed with sensor data from individual CoVs and pairs of CoVs to maintain an empirical perception topology up to the second order, which approximately represents the complete perception topology. A hybrid greedy algorithm is then proposed to solve a variant of the budgeted maximum coverage problem with a worst-case performance guarantee. The C-MASS scheduling algorithm adapts the greedy algorithm by incorporating the topological uncertainty and the unexplored time of CoVs to balance exploration and exploitation, addressing the mobility challenge. Extensive numerical experiments demonstrate the near-optimality of the proposed C-MASS framework in both edge-assisted and distributed CP configurations. The weighted recall improvements over object-level CP are 5.8% and 4.2%, respectively. Compared to distance-based and area-based greedy heuristics, the gaps to the offline optimal solutions are reduced by up to 75% and 71%, respectively.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Data-Driven Control of Linear Parabolic Systems using Koopman Eigenstructure Assignment</title>
<link>https://arxiv.org/abs/2407.00432</link>
<guid>https://arxiv.org/abs/2407.00432</guid>
<content:encoded><![CDATA[
arXiv:2407.00432v1 Announce Type: new 
Abstract: This paper considers the data-driven stabilization of linear boundary controlled parabolic PDEs by making use of the Koopman operator. For this, a Koopman eigenstructure assignment problem is solved, which amounts to determine a feedback of the Koopman open-loop eigenfunctionals assigning a desired finite set of closed-loop Koopman eigenvalues and eigenfunctionals to the closed-loop system. It is shown that the designed controller only needs a finite number of open-loop Koopman eigenvalues and modes of the state. They are determined by extending the classical Krylov-DMD to parabolic systems. For this, only a finite number of pointlike outputs and their temporal samples as well as temporal samples of the inputs are required resulting in a data-driven solution of the eigenstructure assignment problem. Exponential stability of the closed-loop system in the presence of small Krylov-DMD errors is verified. An unstable diffusion-reaction system demonstrates the new data-driven controller design technique for distributed-parameter systems.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Graph Neural Networks Gone Hogwild</title>
<link>https://arxiv.org/abs/2407.00494</link>
<guid>https://arxiv.org/abs/2407.00494</guid>
<content:encoded><![CDATA[
arXiv:2407.00494v1 Announce Type: new 
Abstract: Message passing graph neural networks (GNNs) would appear to be powerful tools to learn distributed algorithms via gradient descent, but generate catastrophically incorrect predictions when nodes update asynchronously during inference. This failure under asynchrony effectively excludes these architectures from many potential applications, such as learning local communication policies between resource-constrained agents in, e.g., robotic swarms or sensor networks. In this work we explore why this failure occurs in common GNN architectures, and identify "implicitly-defined" GNNs as a class of architectures which is provably robust to partially asynchronous "hogwild" inference, adapting convergence guarantees from work in asynchronous and distributed optimization, e.g., Bertsekas (1982); Niu et al. (2011). We then propose a novel implicitly-defined GNN architecture, which we call an energy GNN. We show that this architecture outperforms other GNNs from this class on a variety of synthetic tasks inspired by multi-agent systems, and achieves competitive performance on real-world datasets.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Blockchain based Decentralized Petition System</title>
<link>https://arxiv.org/abs/2407.00534</link>
<guid>https://arxiv.org/abs/2407.00534</guid>
<content:encoded><![CDATA[
arXiv:2407.00534v1 Announce Type: new 
Abstract: A decentralized online petition system enables individuals or groups to create, sign, and share petitions without a central authority. Using blockchain technology, these systems ensure the integrity and transparency of the petition process by recording every signature or action on the blockchain, making alterations or deletions impossible. This provides a permanent, tamper-proof record of the petition's progress. Such systems allow users to bypass traditional intermediaries like government or social media platforms, fostering more democratic and transparent decision-making.
  This paper reviews research on petition systems, highlighting the shortcomings of existing systems such as lack of accountability, vulnerability to hacking, and security issues. The proposed blockchain-based implementation aims to overcome these challenges. Decentralized voting systems have garnered interest recently due to their potential to provide secure and transparent voting platforms without intermediaries, addressing issues like voter fraud, manipulation, and trust in the electoral process.
  We propose a decentralized voting system web application using blockchain technology to ensure the integrity and security of the voting process. This system aims to provide a transparent, decentralized decision-making process that counts every vote while eliminating the need for centralized authorities. The paper presents an overview of the system architecture, design considerations, and implementation details, along with the potential benefits and limitations.
  Finally, we discuss future research directions, examining the technical aspects of the application, including underlying algorithms and protocols. Our research aims to enhance the integrity and accessibility of democratic processes, improve security, and ensure fairness, transparency, and tamper-proofness.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Challenging the Need for Packet Spraying in Large-Scale Distributed Training</title>
<link>https://arxiv.org/abs/2407.00550</link>
<guid>https://arxiv.org/abs/2407.00550</guid>
<content:encoded><![CDATA[
arXiv:2407.00550v1 Announce Type: new 
Abstract: Large-scale distributed training in production datacenters constitutes a challenging workload bottlenecked by network communication. In response, both major industry players (e.g., Ultra Ethernet Consortium) and parts of academia have surprisingly, and almost unanimously, agreed that packet spraying is necessary to improve the performance of large-scale distributed training workloads.
  In this paper, we challenge this prevailing belief and pose the question: How close can a singlepath transport approach an optimal multipath transport? We demonstrate that singlepath transport (from a NIC's perspective) is sufficient and can perform nearly as well as an ideal multipath transport with packet spraying, particularly in the context of distributed training in leaf-spine topologies. Our assertion is based on four key observations about workloads driven by collective communication patterns: (i) flows within a collective start almost simultaneously, (ii) flow sizes are nearly equal, (iii) the completion time of a collective is more crucial than individual flow completion times, and (iv) flows can be split upon arrival. We analytically prove that singlepath transport, using minimal flow splitting (at the application layer), is equivalent to an ideal multipath transport with packet spraying in terms of maximum congestion. Our preliminary evaluations support our claims. This paper suggests an alternative agenda for developing next-generation transport protocols tailored for large-scale distributed training.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Joint Task Allocation and Scheduling for Multi-Hop Distributed Computing</title>
<link>https://arxiv.org/abs/2407.00565</link>
<guid>https://arxiv.org/abs/2407.00565</guid>
<content:encoded><![CDATA[
arXiv:2407.00565v1 Announce Type: new 
Abstract: The rise of the Internet of Things and edge computing has shifted computing resources closer to end-users, benefiting numerous delay-sensitive, computation-intensive applications. To speed up computation, distributed computing is a promising technique that allows parallel execution of tasks across multiple compute nodes. However, current research predominantly revolves around the master-worker paradigm, limiting resource sharing within one-hop neighborhoods. This limitation can render distributed computing ineffective in scenarios with limited nearby resources or constrained/dynamic connectivity. In this paper, we address this limitation by introducing a new distributed computing framework that extends resource sharing beyond one-hop neighborhoods through exploring layered network structures and multi-hop routing. Our framework involves transforming the network graph into a sink tree and formulating a joint optimization problem based on the layered tree structure for task allocation and scheduling. To solve this problem, we propose two exact methods that find optimal solutions and three heuristic strategies to improve efficiency and scalability. The performances of these methods are analyzed and evaluated through theoretical analyses and comprehensive simulation studies. The results demonstrate their promising performances over the traditional distributed computing and computation offloading strategies.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>DDRM: Distributed Drone Reputation Management for Trust and Reliability in Crowdsourced Drone Services</title>
<link>https://arxiv.org/abs/2407.00591</link>
<guid>https://arxiv.org/abs/2407.00591</guid>
<content:encoded><![CDATA[
arXiv:2407.00591v1 Announce Type: new 
Abstract: This study introduces the Distributed Drone Reputation Management (DDRM) framework, designed to fortify trust and authenticity within the Internet of Drone Things (IoDT) ecosystem. As drones increasingly play a pivotal role across diverse sectors, integrating crowdsourced drone services within the IoDT has emerged as a vital avenue for democratizing access to these services. A critical challenge, however, lies in ensuring the authenticity and reliability of drone service reviews. Leveraging the Ethereum blockchain, DDRM addresses this challenge by instituting a verifiable and transparent review mechanism. The framework innovates with a dual-token system, comprising the Service Review Authorization Token (SRAT) for facilitating review authorization and the Drone Reputation Enhancement Token (DRET) for rewarding and recognizing drones demonstrating consistent reliability. Comprehensive analysis within this paper showcases DDRM's resilience against various reputation frauds and underscores its operational effectiveness, particularly in enhancing the efficiency and reliability of drone services.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>BAZAM: A Blockchain-Assisted Zero-Trust Authentication in Multi-UAV Wireless Networks</title>
<link>https://arxiv.org/abs/2407.00630</link>
<guid>https://arxiv.org/abs/2407.00630</guid>
<content:encoded><![CDATA[
arXiv:2407.00630v1 Announce Type: new 
Abstract: Unmanned aerial vehicles (UAVs) are vulnerable to interception and attacks when operated remotely without a unified and efficient identity authentication. Meanwhile, the openness of wireless communication environments potentially leads to data leakage and system paralysis. However, conventional authentication schemes in the UAV network are system-centric, failing to adapt to the diversity of UAVs identities and access, resulting in changes in network environments and connection statuses. Additionally, UAVs are not subjected to periodic identity compliance checks once authenticated, leading to difficulties in controlling access anomalies. Therefore, in this work, we consider a zero-trust framework for UAV network authentication, aiming to achieve UAVs identity authentication through the principle of ``never trust and always verify''. We introduce a blockchain-assisted zero-trust authentication scheme, namely BAZAM, designed for multi-UAV wireless networks. In this scheme, UAVs follow a key generation approach using physical unclonable functions (PUFs), and cryptographic technique helps verify registration and access requests of UAVs. The blockchain is applied to store UAVs authentication information in immutable storage. Through thorough security analysis and extensive evaluation, we demonstrate the effectiveness and efficiency of the proposed BAZAM.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>CAMON: Cooperative Agents for Multi-Object Navigation with LLM-based Conversations</title>
<link>https://arxiv.org/abs/2407.00632</link>
<guid>https://arxiv.org/abs/2407.00632</guid>
<content:encoded><![CDATA[
arXiv:2407.00632v1 Announce Type: new 
Abstract: Visual navigation tasks are critical for household service robots. As these tasks become increasingly complex, effective communication and collaboration among multiple robots become imperative to ensure successful completion. In recent years, large language models (LLMs) have exhibited remarkable comprehension and planning abilities in the context of embodied agents. However, their application in household scenarios, specifically in the use of multiple agents collaborating to complete complex navigation tasks through communication, remains unexplored. Therefore, this paper proposes a framework for decentralized multi-agent navigation, leveraging LLM-enabled communication and collaboration. By designing the communication-triggered dynamic leadership organization structure, we achieve faster team consensus with fewer communication instances, leading to better navigation effectiveness and collaborative exploration efficiency. With the proposed novel communication scheme, our framework promises to be conflict-free and robust in multi-object navigation tasks, even when there is a surge in team size.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Field Knowledge as a Dual to Distributed Knowledge: A Characterization by Weighted Modal Logic</title>
<link>https://arxiv.org/abs/2407.00687</link>
<guid>https://arxiv.org/abs/2407.00687</guid>
<content:encoded><![CDATA[
arXiv:2407.00687v1 Announce Type: new 
Abstract: The study of group knowledge concepts such as mutual, common, and distributed knowledge is well established within the discipline of epistemic logic. In this work, we incorporate epistemic abilities of agents to refine the formal definition of distributed knowledge and introduce a formal characterization of field knowledge. We propose that field knowledge serves as a dual to distributed knowledge. Our approach utilizes epistemic logics with various group knowledge constructs, interpreted through weighted models. We delve into the eight logics that stem from these considerations, explore their relative expressivity and develop sound and complete axiomatic systems.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>A posteriori error estimator for elliptic interface problems in the fictitious formulation</title>
<link>https://arxiv.org/abs/2407.00786</link>
<guid>https://arxiv.org/abs/2407.00786</guid>
<content:encoded><![CDATA[
arXiv:2407.00786v1 Announce Type: new 
Abstract: A posteriori error estimator is derived for an elliptic interface problem in the fictitious domain formulation with distributed Lagrange multiplier considering a discontinuous Lagrange multiplier finite element space. A posteriori error estimation plays a pivotal role in assessing the accuracy and reliability of computational solutions across various domains of science and engineering. This study delves into the theoretical underpinnings and computational considerations of a residual-based estimator.
  Theoretically, the estimator is studied for cases with constant coefficients which jump across an interface as well as generalized scenarios with smooth coefficients that jump across an interface. Theoretical findings demonstrate the reliability and efficiency of the proposed estimators under all considered cases.
  Numerical experiments are conducted to validate the theoretical results, incorporating various immersed geometries and instances of high coefficients jumps at the interface. Leveraging an adaptive algorithm, the estimator identifies regions with singularities and applies refinement accordingly. Results substantiate the theoretical findings, highlighting the reliability and efficiency of the estimators. Furthermore, numerical solutions exhibit optimal convergence properties, demonstrating resilience against geometric singularities or coefficients jumps.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Privacy-Aware Spectrum Pricing and Power Control Optimization for LEO Satellite Internet-of-Things</title>
<link>https://arxiv.org/abs/2407.00814</link>
<guid>https://arxiv.org/abs/2407.00814</guid>
<content:encoded><![CDATA[
arXiv:2407.00814v1 Announce Type: new 
Abstract: Low earth orbit (LEO) satellite systems play an important role in next generation communication networks due to their ability to provide extensive global coverage with guaranteed communications in remote areas and isolated areas where base stations cannot be cost-efficiently deployed. With the pervasive adoption of LEO satellite systems, especially in the LEO Internet-of-Things (IoT) scenarios, their spectrum resource management requirements have become more complex as a result of massive service requests and high bandwidth demand from terrestrial terminals. For instance, when leasing the spectrum to terrestrial users and controlling the uplink transmit power, satellites collect user data for machine learning purposes, which usually are sensitive information such as location, budget and quality of service (QoS) requirement. To facilitate model training in LEO IoT while preserving the privacy of data, blockchain-driven federated learning (FL) is widely used by leveraging on a fully decentralized architecture. In this paper, we propose a hybrid spectrum pricing and power control framework for LEO IoT by combining blockchain technology and FL. We first design a local deep reinforcement learning algorithm for LEO satellite systems to learn a revenue-maximizing pricing and power control scheme. Then the agents collaborate to form a FL system. We also propose a reputation-based blockchain which is used in the global model aggregation phase of FL. Based on the reputation mechanism, a node is selected for each global training round to perform model aggregation and block generation, which can further enhance the decentralization of the network and guarantee the trust. Simulation tests are conducted to evaluate the performances of the proposed scheme. Our results show the efficiency of finding the maximum revenue scheme for LEO satellite systems while preserving the privacy of each agent.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Privacy-First Crowdsourcing: Blockchain and Local Differential Privacy in Crowdsourced Drone Services</title>
<link>https://arxiv.org/abs/2407.00873</link>
<guid>https://arxiv.org/abs/2407.00873</guid>
<content:encoded><![CDATA[
arXiv:2407.00873v1 Announce Type: new 
Abstract: We introduce a privacy-preserving framework for integrating consumer-grade drones into bushfire management. This system creates a marketplace where bushfire management authorities obtain essential data from drone operators. Key features include local differential privacy to protect data providers and a blockchain-based solution ensuring fair data exchanges and accountability. The framework is validated through a proof-of-concept implementation, demonstrating its scalability and potential for various large-scale data collection scenarios. This approach addresses privacy concerns and compliance with regulations like Australia's Privacy Act 1988, offering a practical solution for enhancing bushfire detection and management through crowdsourced drone services.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Decentralized PKI Framework for Data Integrity in Spatial Crowdsourcing Drone Services</title>
<link>https://arxiv.org/abs/2407.00876</link>
<guid>https://arxiv.org/abs/2407.00876</guid>
<content:encoded><![CDATA[
arXiv:2407.00876v1 Announce Type: new 
Abstract: In the domain of spatial crowdsourcing drone services, which includes tasks like delivery, surveillance, and data collection, secure communication is paramount. The Public Key Infrastructure (PKI) ensures this by providing a system for digital certificates that authenticate the identities of entities involved, securing data and command transmissions between drones and their operators. However, the centralized trust model of traditional PKI, dependent on Certificate Authorities (CAs), presents a vulnerability due to its single point of failure, risking security breaches. To counteract this, the paper presents D2XChain, a blockchain-based PKI framework designed for the Internet of Drone Things (IoDT). By decentralizing the CA infrastructure, D2XChain eliminates this single point of failure, thereby enhancing the security and reliability of drone communications. Fully compatible with the X.509 standard, it integrates seamlessly with existing PKI systems, supporting all key operations such as certificate registration, validation, verification, and revocation in a distributed manner. This innovative approach not only strengthens the defense of drone services against various security threats but also showcases its practical application through deployment on a private Ethereum testbed, representing a significant advancement in addressing the unique security challenges of drone-based services and ensuring their trustworthy operation in critical tasks.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Ares II: Tracing the Flaws of a (Storage) God</title>
<link>https://arxiv.org/abs/2407.00881</link>
<guid>https://arxiv.org/abs/2407.00881</guid>
<content:encoded><![CDATA[
arXiv:2407.00881v1 Announce Type: new 
Abstract: Ares is a modular framework, designed to implement dynamic, reconfigurable, fault-tolerant, read/write and strongly consistent distributed shared memory objects. Recent enhancements of the framework have realized the efficient implementation of large objects, by introducing versioning and data striping techniques. In this work, we identify performance bottlenecks of the Ares's variants by utilizing distributed tracing, a popular technique for monitoring and profiling distributed systems. We then propose optimizations across all versions of Ares, aiming in overcoming the identified flaws, while preserving correctness. We refer to the optimized version of Ares as Ares II, which now features a piggyback mechanism, a garbage collection mechanism, and a batching reconfiguration technique for improving the performance and storage efficiency of the original Ares. We rigorously prove the correctness of Ares II, and we demonstrate the performance improvements by an experimental comparison (via distributed tracing) of the Ares II variants with their original counterparts.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>SplitLoRA: A Split Parameter-Efficient Fine-Tuning Framework for Large Language Models</title>
<link>https://arxiv.org/abs/2407.00952</link>
<guid>https://arxiv.org/abs/2407.00952</guid>
<content:encoded><![CDATA[
arXiv:2407.00952v1 Announce Type: new 
Abstract: The scalability of large language models (LLMs) in handling high-complexity models and large-scale datasets has led to tremendous successes in pivotal domains. While there is an urgent need to acquire more training data for LLMs, a concerning reality is the depletion of high-quality public datasets within a few years. In view of this, the federated learning (FL) LLM fine-tuning paradigm recently has been proposed to facilitate collaborative LLM fine-tuning on distributed private data, where multiple data owners collaboratively fine-tune a shared LLM without sharing raw data. However, the staggering model size of LLMs imposes heavy computing and communication burdens on clients, posing significant barriers to the democratization of the FL LLM fine-tuning paradigm. To address this issue, split learning (SL) has emerged as a promising solution by offloading the primary training workload to a server via model partitioning while exchanging activation/activation's gradients with smaller data sizes rather than the entire LLM. Unfortunately, research on the SL LLM fine-tuning paradigm is still in its nascent stage. To fill this gap, in this paper, we propose the first SL LLM fine-tuning framework, named SplitLoRA. SplitLoRA is built on the split federated learning (SFL) framework, amalgamating the advantages of parallel training from FL and model splitting from SL and thus greatly enhancing the training efficiency. It is worth noting that SplitLoRA is the inaugural open-source benchmark for SL LLM fine-tuning, providing a foundation for research efforts dedicated to advancing SL LLM fine-tuning. Extensive simulations validate that SplitLoRA achieves target accuracy in significantly less time than state-of-the-art LLM fine-tuning frameworks, demonstrating the superior training performance of SplitLoRA. The project page is available at https://fduinc.github.io/splitlora/.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Opportunities for Shape-based Optimization of Link Traversal Queries</title>
<link>https://arxiv.org/abs/2407.00998</link>
<guid>https://arxiv.org/abs/2407.00998</guid>
<content:encoded><![CDATA[
arXiv:2407.00998v1 Announce Type: new 
Abstract: Data on the web is naturally unindexed and decentralized. Centralizing web data, especially personal data, raises ethical and legal concerns. Yet, compared to centralized query approaches, decentralization-friendly alternatives such as Link Traversal Query Processing (LTQP) are significantly less performant and understood. The two main difficulties of LTQP are the lack of apriori information about data sources and the high number of HTTP requests. Exploring decentralized-friendly ways to document unindexed networks of data sources could lead to solutions to alleviate those difficulties. RDF data shapes are widely used to validate linked data documents, therefore, it is worthwhile to investigate their potential for LTQP optimization. In our work, we built an early version of a source selection algorithm for LTQP using RDF data shape mappings with linked data documents and measured its performance in a realistic setup. In this article, we present our algorithm and early results, thus, opening opportunities for further research for shape-based optimization of link traversal queries. Our initial experiments show that with little maintenance and work from the server, our method can reduce up to 80% the execution time and 97% the number of links traversed during realistic queries. Given our early results and the descriptive power of RDF data shapes it would be worthwhile to investigate non-heuristic-based query planning using RDF shapes.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Parallel Computing Architectures for Robotic Applications: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2407.01011</link>
<guid>https://arxiv.org/abs/2407.01011</guid>
<content:encoded><![CDATA[
arXiv:2407.01011v1 Announce Type: new 
Abstract: With the growing complexity and capability of contemporary robotic systems, the necessity of sophisticated computing solutions to efficiently handle tasks such as real-time processing, sensor integration, decision-making, and control algorithms is also increasing. Conventional serial computing frequently fails to meet these requirements, underscoring the necessity for high-performance computing alternatives. Parallel computing, the utilization of several processing elements simultaneously to solve computational problems, offers a possible answer. Various parallel computing designs, such as multi-core CPUs, GPUs, FPGAs, and distributed systems, provide substantial enhancements in processing capacity and efficiency. By utilizing these architectures, robotic systems can attain improved performance in functionalities such as real-time image processing, sensor fusion, and path planning. The transformative potential of parallel computing architectures in advancing robotic technology has been underscored, real-life case studies of these architectures in the robotics field have been discussed, and comparisons are presented. Challenges pertaining to these architectures have been explored, and possible solutions have been mentioned for further research and enhancement of the robotic applications.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>DistML.js: Installation-free Distributed Deep Learning Framework for Web Browsers</title>
<link>https://arxiv.org/abs/2407.01023</link>
<guid>https://arxiv.org/abs/2407.01023</guid>
<content:encoded><![CDATA[
arXiv:2407.01023v1 Announce Type: new 
Abstract: We present "DistML.js", a library designed for training and inference of machine learning models within web browsers. Not only does DistML.js facilitate model training on local devices, but it also supports distributed learning through communication with servers. Its design and define-by-run API for deep learning model construction resemble PyTorch, thereby reducing the learning curve for prototyping. Matrix computations involved in model training and inference are executed on the backend utilizing WebGL, enabling high-speed calculations. We provide a comprehensive explanation of DistML.js's design, API, and implementation, alongside practical applications including data parallelism in learning. The source code is publicly available at https://github.com/mil-tokyo/distmljs.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Randomized linear solvers for computational architectures with straggling workers</title>
<link>https://arxiv.org/abs/2407.01098</link>
<guid>https://arxiv.org/abs/2407.01098</guid>
<content:encoded><![CDATA[
arXiv:2407.01098v1 Announce Type: new 
Abstract: In this paper, we consider the iterative solution of sparse systems of linear algebraic equations under the condition that sparse matrix-vector products with the coefficient matrix are computed only partially. At the same time, non-computed entries are set to zeros. We assume that both the number of computed entries and their associated row index set are random variables, with the row index set sampled uniformly given the number of computed entries. This model of computations is prevalent to that realized in hybrid cloud computing architectures following the controller-worker distributed model under the influence of straggling workers. We propose a randomized Richardson iterative scheme and a randomized Chebyshev semi-iterative method within this model and prove the sufficient conditions for their convergence in expectation. Numerical experiments verify the presented theoretical results as well as the effectiveness of the proposed schemes on a few sparse matrix problems.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>FedRC: A Rapid-Converged Hierarchical Federated Learning Framework in Street Scene Semantic Understanding</title>
<link>https://arxiv.org/abs/2407.01103</link>
<guid>https://arxiv.org/abs/2407.01103</guid>
<content:encoded><![CDATA[
arXiv:2407.01103v1 Announce Type: new 
Abstract: Street Scene Semantic Understanding (denoted as TriSU) is a crucial but complex task for world-wide distributed autonomous driving (AD) vehicles (e.g., Tesla). Its inference model faces poor generalization issue due to inter-city domain-shift. Hierarchical Federated Learning (HFL) offers a potential solution for improving TriSU model generalization, but suffers from slow convergence rate because of vehicles' surrounding heterogeneity across cities. Going beyond existing HFL works that have deficient capabilities in complex tasks, we propose a rapid-converged heterogeneous HFL framework (FedRC) to address the inter-city data heterogeneity and accelerate HFL model convergence rate. In our proposed FedRC framework, both single RGB image and RGB dataset are modelled as Gaussian distributions in HFL aggregation weight design. This approach not only differentiates each RGB sample instead of typically equalizing them, but also considers both data volume and statistical properties rather than simply taking data quantity into consideration. Extensive experiments on the TriSU task using across-city datasets demonstrate that FedRC converges faster than the state-of-the-art benchmark by 38.7%, 37.5%, 35.5%, and 40.6% in terms of mIoU, mPrecision, mRecall, and mF1, respectively. Furthermore, qualitative evaluations in the CARLA simulation environment confirm that the proposed FedRC framework delivers top-tier performance.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>SCIF: A Language for Compositional Smart Contract Security</title>
<link>https://arxiv.org/abs/2407.01204</link>
<guid>https://arxiv.org/abs/2407.01204</guid>
<content:encoded><![CDATA[
arXiv:2407.01204v1 Announce Type: new 
Abstract: Securing smart contracts remains a fundamental challenge. At its core, it is about building software that is secure in composition with untrusted code, a challenge that extends far beyond blockchains. We introduce SCIF, a language for building smart contracts that are compositionally secure. SCIF is based on the fundamentally compositional principle of secure information flow, but extends this core mechanism to include protection against reentrancy attacks, confused deputy attacks, and improper error handling, even in the presence of malicious contracts that do not follow SCIF's rules. SCIF supports a rich ecosystem of interacting principals with partial trust through its mechanisms for dynamic trust management. SCIF has been implemented as a compiler to Solidity. We describe the SCIF language, including its static checking rules and runtime. Finally, we implement several applications with intricate security reasoning, showing how SCIF supports building complex smart contracts securely and gives programmer accurate diagnostics about potential security bugs.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>On the Parameters of Codes for Data Access</title>
<link>https://arxiv.org/abs/2407.01229</link>
<guid>https://arxiv.org/abs/2407.01229</guid>
<content:encoded><![CDATA[
arXiv:2407.01229v1 Announce Type: new 
Abstract: This paper studies two crucial problems in the context of coded distributed storage systems directly related to their performance: 1) for a fixed alphabet size, determine the minimum number of servers the system must have for its service rate region to contain a prescribed set of points; 2) for a given number of servers, determine the minimum alphabet size for which the service rate region of the system contains a prescribed set of points. The paper establishes rigorous upper and lower bounds, as well as code constructions based on techniques from coding theory, optimization, and projective geometry.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Energy-Aware Decentralized Learning with Intermittent Model Training</title>
<link>https://arxiv.org/abs/2407.01283</link>
<guid>https://arxiv.org/abs/2407.01283</guid>
<content:encoded><![CDATA[
arXiv:2407.01283v1 Announce Type: new 
Abstract: Decentralized learning (DL) offers a powerful framework where nodes collaboratively train models without sharing raw data and without the coordination of a central server. In the iterative rounds of DL, models are trained locally, shared with neighbors in the topology, and aggregated with other models received from neighbors. Sharing and merging models contribute to convergence towards a consensus model that generalizes better across the collective data captured at training time. In addition, the energy consumption while sharing and merging model parameters is negligible compared to the energy spent during the training phase. Leveraging this fact, we present SkipTrain, a novel DL algorithm, which minimizes energy consumption in decentralized learning by strategically skipping some training rounds and substituting them with synchronization rounds. These training-silent periods, besides saving energy, also allow models to better mix and finally produce models with superior accuracy than typical DL algorithms that train at every round. Our empirical evaluations with 256 nodes demonstrate that SkipTrain reduces energy consumption by 50% and increases model accuracy by up to 12% compared to D-PSGD, the conventional DL algorithm.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>An Operational Semantics for Yul</title>
<link>https://arxiv.org/abs/2407.01365</link>
<guid>https://arxiv.org/abs/2407.01365</guid>
<content:encoded><![CDATA[
arXiv:2407.01365v1 Announce Type: new 
Abstract: We present a big-step and small-step operational semantics for Yul -- the intermediate language used by the Solidity compiler to produce EVM bytecode -- in a mathematical notation that is congruous with the literature of programming languages, lends itself to language proofs, and can serve as a precise, widely accessible specification for the language. Our two semantics stay faithful to the original, informal specification of the language but also clarify under-specified cases such as void function calls. Our presentation allows us to prove the equivalence between the two semantics. We also implement the small-step semantics in an interpreter for Yul which avails of optimisations that are provably correct. We have tested the interpreter using tests from the Solidity compiler and our own. We envisage that this work will enable the development of verification and symbolic execution technology directly in Yul, contributing to the Ethereum security ecosystem, as well as aid the development of a provably sound future type system.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Beyond Throughput and Compression Ratios: Towards High End-to-end Utility of Gradient Compression</title>
<link>https://arxiv.org/abs/2407.01378</link>
<guid>https://arxiv.org/abs/2407.01378</guid>
<content:encoded><![CDATA[
arXiv:2407.01378v1 Announce Type: new 
Abstract: Gradient aggregation has long been identified as a major bottleneck in today's large-scale distributed machine learning training systems. One promising solution to mitigate such bottlenecks is gradient compression, directly reducing communicated gradient data volume. However, in practice, many gradient compression schemes do not achieve acceleration of the training process while also preserving accuracy.
  In this work, we identify several common issues in previous gradient compression systems and evaluation methods. These issues include excessive computational overheads; incompatibility with all-reduce; and inappropriate evaluation metrics, such as not using an end-to-end metric or using a 32-bit baseline instead of a 16-bit baseline. We propose several general design and evaluation techniques to address these issues and provide guidelines for future work. Our preliminary evaluation shows that our techniques enhance the system's performance and provide a clearer understanding of the end-to-end utility of gradient compression methods.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>C-MP: A decentralized adaptive-coordinated traffic signal control using the Max Pressure framework</title>
<link>https://arxiv.org/abs/2407.01421</link>
<guid>https://arxiv.org/abs/2407.01421</guid>
<content:encoded><![CDATA[
arXiv:2407.01421v1 Announce Type: new 
Abstract: Coordinated traffic signals seek to provide uninterrupted flow through a series of closely spaced intersections, typically using pre-defined fixed signal timings and offsets. Adaptive traffic signals dynamically change signal timings based on observed traffic conditions in a way that might disrupt coordinated movements, particularly when these decisions are made independently at each intersection. To alleviate this issue, this paper introduces a novel Max Pressure-based traffic signal framework that can provide coordination even under decentralized decision-making. The proposed Coordinated Max Pressure (C-MP) algorithm uses the space mean speeds of vehicles to explicitly detect freely flowing platoons of vehicles and prioritizes their movement along a corridor. Specifically, upstream platoons are detected and their weight in the MP framework increased to provide priority, while downstream platoons are detected and their weight reduced to ensure smooth traffic flow across corridors. The study analytically proves that C-MP maintains the desirable maximum stability property, while micro-simulation analyses conducted on an arterial network demonstrate its ability to achieve a larger stable region compared to benchmark MP control policies. Simulation results also reveal that the proposed control algorithm can effectively coordinate traffic signals in both directions along an arterial without explicitly assigned offsets or constraints. The results also reveal C-MP's superiority to benchmark coordination strategies in reducing travel time, and fuel consumption both at the corridor level and the network level by balancing the negative impact imparted to vehicles in the minor direction.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Maximizing Blockchain Performance: Mitigating Conflicting Transactions through Parallelism and Dependency Management</title>
<link>https://arxiv.org/abs/2407.01426</link>
<guid>https://arxiv.org/abs/2407.01426</guid>
<content:encoded><![CDATA[
arXiv:2407.01426v1 Announce Type: new 
Abstract: While blockchains initially gained popularity in the realm of cryptocurrencies, their widespread adoption is expanding beyond conventional applications, driven by the imperative need for enhanced data security. Despite providing a secure network, blockchains come with certain tradeoffs, including high latency, lower throughput, and an increased number of transaction failures. A pivotal issue contributing to these challenges is the improper management of "conflicting transactions", commonly referred to as "contention". When a number of pending transactions within a blockchain collide with each other, this results in a state of contention. This situation worsens network latency, leads to the wastage of system resources, and ultimately contributes to reduced throughput and higher transaction failures. In response to this issue, in this work, we present a novel blockchain scheme that integrates transaction parallelism and an intelligent dependency manager aiming to reduce the occurrence of conflicting transactions within blockchain networks. In terms of effectiveness and efficiency, experimental results show that our scheme not only mitigates the challenges posed by conflicting transactions, but also outperforms both existing parallel and non-parallel Hyperledger Fabric blockchain networks achieving higher transaction success rate, throughput, and latency. The integration of our scheme with Hyperledger Fabric appears to be a promising solution for improving the overall performance and stability of blockchain networks in real-world applications.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Reinforcement Learning-driven Data-intensive Workflow Scheduling for Volunteer Edge-Cloud</title>
<link>https://arxiv.org/abs/2407.01428</link>
<guid>https://arxiv.org/abs/2407.01428</guid>
<content:encoded><![CDATA[
arXiv:2407.01428v1 Announce Type: new 
Abstract: In recent times, Volunteer Edge-Cloud (VEC) has gained traction as a cost-effective, community computing paradigm to support data-intensive scientific workflows. However, due to the highly distributed and heterogeneous nature of VEC resources, centralized workflow task scheduling remains a challenge. In this paper, we propose a Reinforcement Learning (RL)-driven data-intensive scientific workflow scheduling approach that takes into consideration: i) workflow requirements, ii) VEC resources' preference on workflows, and iii) diverse VEC resource policies, to ensure robust resource allocation. We formulate the long-term average performance optimization problem as a Markov Decision Process, which is solved using an event-based Asynchronous Advantage Actor-Critic RL approach. Our extensive simulations and testbed implementations demonstrate our approach's benefits over popular baseline strategies in terms of workflow requirement satisfaction, VEC preference satisfaction, and available VEC resource utilization.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>FastCLIP: A Suite of Optimization Techniques to Accelerate CLIP Training with Limited Resources</title>
<link>https://arxiv.org/abs/2407.01445</link>
<guid>https://arxiv.org/abs/2407.01445</guid>
<content:encoded><![CDATA[
arXiv:2407.01445v1 Announce Type: new 
Abstract: Existing studies of training state-of-the-art Contrastive Language-Image Pretraining (CLIP) models on large-scale data involve hundreds of or even thousands of GPUs due to the requirement of a large batch size. However, such a large amount of resources is not accessible to most people. While advanced compositional optimization techniques for optimizing global contrastive losses have been demonstrated effective for removing the requirement of large batch size, their performance on large-scale data remains underexplored and not optimized. To bridge the gap, this paper explores several aspects of CLIP training with limited resources (e.g., up to tens of GPUs). First, we introduce FastCLIP, a general CLIP training framework built on advanced compositional optimization techniques while designed and optimized for the distributed setting. Our framework is equipped with an efficient gradient reduction strategy to reduce communication overhead. Second, to further boost training efficiency, we investigate three components of the framework from an optimization perspective: the schedule of the inner learning rate, the update rules of the temperature parameter and the model parameters, respectively. Experiments on different strategies for each component shed light on how to conduct CLIP training more efficiently. Finally, we benchmark the performance of FastCLIP and the state-of-the-art training baseline (OpenCLIP) on different compute scales up to 32 GPUs on 8 nodes, and three data scales ranging from 2.7 million, 9.1 million to 315 million image-text pairs to demonstrate the significant improvement of FastCLIP in the resource-limited setting. We release the code of FastCLIP at https://github.com/Optimization-AI/fast_clip .
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>How Clustering Affects the Convergence of Decentralized Optimization over Networks: A Monte-Carlo-based Approach</title>
<link>https://arxiv.org/abs/2407.01460</link>
<guid>https://arxiv.org/abs/2407.01460</guid>
<content:encoded><![CDATA[
arXiv:2407.01460v1 Announce Type: new 
Abstract: Decentralized algorithms have gained substantial interest owing to advancements in cloud computing, Internet of Things (IoT), intelligent transportation networks, and parallel processing over sensor networks. The convergence of such algorithms is directly related to specific properties of the underlying network topology. Specifically, the clustering coefficient is known to affect, for example, the controllability/observability and the epidemic growth over networks. In this work, we study the effects of the clustering coefficient on the convergence rate of networked optimization approaches. In this regard, we model the structure of large-scale distributed systems by random scale-free (SF) and clustered scale-free (CSF) networks and compare the convergence rate by tuning the network clustering coefficient. This is done by keeping other relevant network properties (such as power-law degree distribution, number of links, and average degree) unchanged. Monte-Carlo-based simulations are used to compare the convergence rate over many trials of SF graph topologies. Furthermore, to study the convergence rate over real case studies, we compare the clustering coefficient of some real-world networks with the eigenspectrum of the underlying network (as a measure of convergence rate). The results interestingly show higher convergence rate over low-clustered networks. This is significant as one can improve the learning rate of many existing decentralized machine-learning scenarios by tuning the network clustering.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Immutable in Principle, Upgradeable by Design: Exploratory Study of Smart Contract Upgradeability</title>
<link>https://arxiv.org/abs/2407.01493</link>
<guid>https://arxiv.org/abs/2407.01493</guid>
<content:encoded><![CDATA[
arXiv:2407.01493v1 Announce Type: new 
Abstract: Smart contracts, known for their immutable nature to ensure trust via automated enforcement, have evolved to require upgradeability due to unforeseen vulnerabilities and the need for feature enhancements post-deployment. This contradiction between immutability and the need for modifications has led to the development of upgradeable smart contracts. These contracts are immutable in principle yet upgradable by design, allowing updates without altering the underlying data or state, thus preserving the contract's intent while allowing improvements. This study aims to understand the application and implications of upgradeable smart contracts on the Ethereum blockchain. By introducing a dataset that catalogs the versions and evolutionary trajectories of smart contracts, the research explores key dimensions: the prevalence and adoption patterns of upgrade mechanisms, the likelihood and occurrences of contract upgrades, the nature of modifications post-upgrade, and their impact on user engagement and contract activity. Through empirical analysis, this study identifies upgradeable contracts and examines their upgrade history to uncover trends, preferences, and challenges associated with modifications. The evidence from analyzing over 44 million contracts shows that only 3% have upgradeable characteristics, with only 0.34% undergoing upgrades. This finding underscores a cautious approach by developers towards modifications, possibly due to the complexity of upgrade processes or a preference for maintaining stability. Furthermore, the study shows that upgrades are mainly aimed at feature enhancement and vulnerability mitigation, particularly when the contracts' source codes are accessible. However, the relationship between upgrades and user activity is complex, suggesting that additional factors significantly affect the use of smart contracts beyond their evolution.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>The Inverted 3-Sum Box: General Formulation and Quantum Information Theoretic Optimality</title>
<link>https://arxiv.org/abs/2407.01498</link>
<guid>https://arxiv.org/abs/2407.01498</guid>
<content:encoded><![CDATA[
arXiv:2407.01498v1 Announce Type: new 
Abstract: The $N$-sum box protocol specifies a class of $\mathbb{F}_d$ linear functions $f(W_1,\cdots,W_K)=V_1W_1+V_2W_2+\cdots+V_KW_K\in\mathbb{F}_d^{m\times 1}$ that can be computed at information theoretically optimal communication cost (minimum number of qudits $\Delta_1,\cdots,\Delta_K$ sent by the transmitters Alice$_1$, Alice$_2$,$\cdots$, Alice$_K$, respectively, to the receiver, Bob, per computation instance) over a noise-free quantum multiple access channel (QMAC), when the input data streams $W_k\in\mathbb{F}_d^{m_k\times 1}, k\in[K]$, originate at the distributed transmitters, who share quantum entanglement in advance but are not otherwise allowed to communicate with each other. In prior work this set of optimally computable functions is identified in terms of a strong self-orthogonality (SSO) condition on the transfer function of the $N$-sum box. In this work we consider an `inverted' scenario, where instead of a feasible $N$-sum box transfer function, we are given an arbitrary $\mathbb{F}_d$ linear function, i.e., arbitrary matrices $V_k\in\mathbb{F}_d^{m\times m_k}$ are specified, and the goal is to characterize the set of all feasible communication cost tuples $(\Delta_1,\cdots,\Delta_K)$, not just based on $N$-sum box protocols, but across all possible quantum coding schemes. As our main result, we fully solve this problem for $K=3$ transmitters ($K\geq 4$ settings remain open). Coding schemes based on the $N$-sum box protocol (along with elementary ideas such as treating qudits as classical dits, time-sharing and batch-processing) are shown to be information theoretically optimal in all cases. As an example, in the symmetric case where rk$(V_1)$=rk$(V_2)$=rk$(V_3) \triangleq r_1$, rk$([V_1, V_2])$=rk$([V_2, V_3])$=rk$([V_3, V_1])\triangleq r_2$, and rk$([V_1, V_2, V_3])\triangleq r_3$ (rk = rank), the minimum total-download cost is $\max \{1.5r_1 + 0.75(r_3 - r_2), r_3\}$.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Linear and Nonlinear MMSE Estimation in One-Bit Quantized Systems under a Gaussian Mixture Prior</title>
<link>https://arxiv.org/abs/2407.01305</link>
<guid>https://arxiv.org/abs/2407.01305</guid>
<content:encoded><![CDATA[
arXiv:2407.01305v1 Announce Type: cross 
Abstract: We present new fundamental results for the mean square error (MSE)-optimal conditional mean estimator (CME) in one-bit quantized systems for a Gaussian mixture model (GMM) distributed signal of interest, possibly corrupted by additive white Gaussian noise (AWGN). We first derive novel closed-form analytic expressions for the Bussgang estimator, the well-known linear minimum mean square error (MMSE) estimator in quantized systems. Afterward, closed-form analytic expressions for the CME in special cases are presented, revealing that the optimal estimator is linear in the one-bit quantized observation, opposite to higher resolution cases. Through a comparison to the recently studied Gaussian case, we establish a novel MSE inequality and show that that the signal of interest is correlated with the auxiliary quantization noise. We extend our analysis to multiple observation scenarios, examining the MSE-optimal transmit sequence and conducting an asymptotic analysis, yielding analytic expressions for the MSE and its limit. These contributions have broad impact for the analysis and design of various signal processing applications.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Neural Distributed Source Coding</title>
<link>https://arxiv.org/abs/2106.02797</link>
<guid>https://arxiv.org/abs/2106.02797</guid>
<content:encoded><![CDATA[
arXiv:2106.02797v4 Announce Type: replace 
Abstract: Distributed source coding (DSC) is the task of encoding an input in the absence of correlated side information that is only available to the decoder. Remarkably, Slepian and Wolf showed in 1973 that an encoder without access to the side information can asymptotically achieve the same compression rate as when the side information is available to it. While there is vast prior work on this topic, practical DSC has been limited to synthetic datasets and specific correlation structures. Here we present a framework for lossy DSC that is agnostic to the correlation structure and can scale to high dimensions. Rather than relying on hand-crafted source modeling, our method utilizes a conditional Vector-Quantized Variational Autoencoder (VQ-VAE) to learn the distributed encoder and decoder. We evaluate our method on multiple datasets and show that our method can handle complex correlations and achieves state-of-the-art PSNR. Our code is made available at https://github.com/acnagle/neural-dsc.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Directional Antenna Based Scheduling Protocol for IoT Networks</title>
<link>https://arxiv.org/abs/2305.02511</link>
<guid>https://arxiv.org/abs/2305.02511</guid>
<content:encoded><![CDATA[
arXiv:2305.02511v2 Announce Type: replace 
Abstract: Scheduling and Channel Access at the MAC layer of the IoT network plays a pivotal role in enhancing the performance of IoT networks. State-of-the-art Omni-directional antenna based application data transmission has relatively less achievable throughput in comparison with directional antenna based scheduling protocols. To enhance the performance of the IoT networks, this paper propose a distributed one-hop scheduling algorithm called Directional Scheduling protocol for constrained deterministic 6TiSCH-IoT network. With this, in-creased number of IoT nodes can have concurrent application data transmission with efficient spatial reuse. This in-turn results in higher number of cell allocation to the one-hop IoT nodes during data transmission. The proposed algorithm makes use of through directional transmissions avoids head of line blocking.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration</title>
<link>https://arxiv.org/abs/2305.19476</link>
<guid>https://arxiv.org/abs/2305.19476</guid>
<content:encoded><![CDATA[
arXiv:2305.19476v2 Announce Type: replace 
Abstract: A promising technique for exploration is to maximize the entropy of visited state distribution, i.e., state entropy, by encouraging uniform coverage of visited state space. While it has been effective for an unsupervised setup, it tends to struggle in a supervised setup with a task reward, where an agent prefers to visit high-value states to exploit the task reward. Such a preference can cause an imbalance between the distributions of high-value states and low-value states, which biases exploration towards low-value state regions as a result of the state entropy increasing when the distribution becomes more uniform. This issue is exacerbated when high-value states are narrowly distributed within the state space, making it difficult for the agent to complete the tasks. In this paper, we present a novel exploration technique that maximizes the value-conditional state entropy, which separately estimates the state entropies that are conditioned on the value estimates of each state, then maximizes their average. By only considering the visited states with similar value estimates for computing the intrinsic bonus, our method prevents the distribution of low-value states from affecting exploration around high-value states, and vice versa. We demonstrate that the proposed alternative to the state entropy baseline significantly accelerates various reinforcement learning algorithms across a variety of tasks within MiniGrid, DeepMind Control Suite, and Meta-World benchmarks. Source code is available at https://sites.google.com/view/rl-vcse.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Distributed Pilot Assignment for Distributed Massive-MIMO Networks</title>
<link>https://arxiv.org/abs/2309.15709</link>
<guid>https://arxiv.org/abs/2309.15709</guid>
<content:encoded><![CDATA[
arXiv:2309.15709v3 Announce Type: replace 
Abstract: Pilot contamination is a critical issue in distributed massive MIMO networks, where the reuse of pilot sequences due to limited availability of orthogonal pilots for channel estimation leads to performance degradation. In this work, we propose a novel distributed pilot assignment scheme to effectively mitigate the impact of pilot contamination. Our proposed scheme not only reduces signaling overhead, but it also enhances fault-tolerance. Extensive numerical simulations are conducted to evaluate the performance of the proposed scheme. Our results establish that the proposed scheme outperforms existing centralized and distributed schemes in terms of mitigating pilot contamination and significantly enhancing network throughput.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>CO-ASnet :A Smart Contract Architecture Design based on Blockchain Technology with Active Sensor Networks</title>
<link>https://arxiv.org/abs/2310.05070</link>
<guid>https://arxiv.org/abs/2310.05070</guid>
<content:encoded><![CDATA[
arXiv:2310.05070v2 Announce Type: replace 
Abstract: The influence of opinion leaders impacts different aspects of social finance. How to analyse the utility of opinion leaders' influence in realizing assets on the blockchain and adopt a compliant regulatory scheme is worth exploring and pondering. Taking Musk's call on social media to buy Dogecoin as an example, this paper uses an event study to empirically investigate the phenomenon in which opinion leaders use ICOs (initial coin offerings) to exert influence. The results show that opinion leaders can use ICOs to influence the price of token assets with money and data traffic in their social network. They can obtain excess returns and reduce the cost of realization so that the closed loop of influence realization will be accelerated. Based on this phenomenon and the results of its impact, we use the ChainLink Oracle with Active Sensor Networks(CO-ASnet) to design a safe and applicable decentralized regulatory scheme that can constructively provide risk assessment strategies and early warning measures for token issuance. The influence realization of opinion leaders in blockchain issuance is bound to receive widespread attention, and this paper will provide an exemplary reference for regulators and enterprises to explore the boundaries of blockchain financial product development and governance.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>A Two-Layer Blockchain Sharding Protocol Leveraging Safety and Liveness for Enhanced Performance</title>
<link>https://arxiv.org/abs/2310.11373</link>
<guid>https://arxiv.org/abs/2310.11373</guid>
<content:encoded><![CDATA[
arXiv:2310.11373v4 Announce Type: replace 
Abstract: Sharding is essential for improving blockchain scalability. Existing protocols overlook diverse adversarial attacks, limiting transaction throughput. This paper presents Reticulum, a groundbreaking sharding protocol addressing this issue, boosting blockchain scalability.
  Reticulum employs a two-phase approach, adapting transaction throughput based on runtime adversarial attacks. It comprises "control" and "process" shards in two layers. Process shards contain at least one trustworthy node, while control shards have a majority of trusted nodes. In the first phase, transactions are written to blocks and voted on by nodes in process shards. Unanimously accepted blocks are confirmed. In the second phase, blocks without unanimous acceptance are voted on by control shards. Blocks are accepted if the majority votes in favor, eliminating first-phase opponents and silent voters. Reticulum uses unanimous voting in the first phase, involving fewer nodes, enabling more parallel process shards. Control shards finalize decisions and resolve disputes.
  Experiments confirm Reticulum's innovative design, providing high transaction throughput and robustness against various network attacks, outperforming existing sharding protocols for blockchain networks.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>History Trees and Their Applications</title>
<link>https://arxiv.org/abs/2404.02673</link>
<guid>https://arxiv.org/abs/2404.02673</guid>
<content:encoded><![CDATA[
arXiv:2404.02673v3 Announce Type: replace 
Abstract: In the theoretical study of distributed communication networks, "history trees" are a discrete structure that naturally models the concept that anonymous agents become distinguishable upon receiving different sets of messages from neighboring agents. By conveniently organizing temporal information in a systematic manner, history trees have been instrumental in the development of optimal deterministic algorithms for networks that are both anonymous and dynamically evolving.
  This note provides an accessible introduction to history trees, drawing comparisons with more traditional structures found in existing literature and reviewing the latest advancements in the applications of history trees, especially within dynamic networks. Furthermore, it expands the theoretical framework of history trees in new directions, also highlighting several open problems for further investigation.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches</title>
<link>https://arxiv.org/abs/2404.02817</link>
<guid>https://arxiv.org/abs/2404.02817</guid>
<content:encoded><![CDATA[
arXiv:2404.02817v4 Announce Type: replace 
Abstract: Task and Motion Planning (TAMP) integrates high-level task planning and low-level motion planning to equip robots with the autonomy to effectively reason over long-horizon, dynamic tasks. Optimization-based TAMP focuses on hybrid optimization approaches that define goal conditions via objective functions and are capable of handling open-ended goals, robotic dynamics, and physical interaction between the robot and the environment. Therefore, optimization-based TAMP is particularly suited to solve highly complex, contact-rich locomotion and manipulation problems. This survey provides a comprehensive review on optimization-based TAMP, covering (i) planning domain representations, including action description languages and temporal logic, (ii) individual solution strategies for components of TAMP, including AI planning and trajectory optimization (TO), and (iii) the dynamic interplay between logic-based task planning and model-based TO. A particular focus of this survey is to highlight the algorithm structures to efficiently solve TAMP, especially hierarchical and distributed approaches. Additionally, the survey emphasizes the synergy between the classical methods and contemporary learning-based innovations such as large language models. Furthermore, the future research directions for TAMP is discussed in this survey, highlighting both algorithmic and application-specific challenges.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Static Application Security Testing (SAST) Tools for Smart Contracts: How Far Are We?</title>
<link>https://arxiv.org/abs/2404.18186</link>
<guid>https://arxiv.org/abs/2404.18186</guid>
<content:encoded><![CDATA[
arXiv:2404.18186v3 Announce Type: replace 
Abstract: In recent years, the importance of smart contract security has been heightened by the increasing number of attacks against them. To address this issue, a multitude of static application security testing (SAST) tools have been proposed for detecting vulnerabilities in smart contracts. However, objectively comparing these tools to determine their effectiveness remains challenging. Existing studies often fall short due to the taxonomies and benchmarks only covering a coarse and potentially outdated set of vulnerability types, which leads to evaluations that are not entirely comprehensive and may display bias.
  In this paper, we fill this gap by proposing an up-to-date and fine-grained taxonomy that includes 45 unique vulnerability types for smart contracts. Taking it as a baseline, we develop an extensive benchmark that covers 40 distinct types and includes a diverse range of code characteristics, vulnerability patterns, and application scenarios. Based on them, we evaluated 8 SAST tools using this benchmark, which comprises 788 smart contract files and 10,394 vulnerabilities. Our results reveal that the existing SAST tools fail to detect around 50% of vulnerabilities in our benchmark and suffer from high false positives, with precision not surpassing 10%. We also discover that by combining the results of multiple tools, the false negative rate can be reduced effectively, at the expense of flagging 36.77 percentage points more functions. Nevertheless, many vulnerabilities, especially those beyond Access Control and Reentrancy vulnerabilities, remain undetected. We finally highlight the valuable insights from our study, hoping to provide guidance on tool development, enhancement, evaluation, and selection for developers, researchers, and practitioners.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>AB-Training: A Communication-Efficient Approach for Distributed Low-Rank Learning</title>
<link>https://arxiv.org/abs/2405.01067</link>
<guid>https://arxiv.org/abs/2405.01067</guid>
<content:encoded><![CDATA[
arXiv:2405.01067v2 Announce Type: replace 
Abstract: Communication bottlenecks severely hinder the scalability of distributed neural network training, particularly in high-performance computing (HPC) environments. We introduce AB-training, a novel data-parallel method that leverages low-rank representations and independent training groups to significantly reduce communication overhead. Our experiments demonstrate an average reduction in network traffic of approximately 70.31\% across various scaling scenarios, increasing the training potential of communication-constrained systems and accelerating convergence at scale. AB-training also exhibits a pronounced regularization effect at smaller scales, leading to improved generalization while maintaining or even reducing training time. We achieve a remarkable 44.14 : 1 compression ratio on VGG16 trained on CIFAR-10 with minimal accuracy loss, and outperform traditional data parallel training by 1.55\% on ResNet-50 trained on ImageNet-2012. While AB-training is promising, our findings also reveal that large batch effects persist even in low-rank regimes, underscoring the need for further research into optimized update mechanisms for massively distributed training.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Facilitating Feature and Topology Lightweighting: An Ethereum Transaction Graph Compression Method for Malicious Account Detection</title>
<link>https://arxiv.org/abs/2405.08278</link>
<guid>https://arxiv.org/abs/2405.08278</guid>
<content:encoded><![CDATA[
arXiv:2405.08278v2 Announce Type: replace 
Abstract: Ethereum has become one of the primary global platforms for cryptocurrency, playing an important role in promoting the diversification of the financial ecosystem. However, the relative lag in regulation has led to a proliferation of malicious activities in Ethereum, posing a serious threat to fund security. Existing regulatory methods usually detect malicious accounts through feature engineering or large-scale transaction graph mining. However, due to the immense scale of transaction data and malicious attacks, these methods suffer from inefficiency and low robustness during data processing and anomaly detection. In this regard, we propose an Ethereum Transaction Graph Compression method named TGC4Eth, which assists malicious account detection by lightweighting both features and topology of the transaction graph. At the feature level, we select transaction features based on their low importance to improve the robustness of the subsequent detection models against feature evasion attacks; at the topology level, we employ focusing and coarsening processes to compress the structure of the transaction graph, thereby improving both data processing and inference efficiency of detection models. Extensive experiments demonstrate that TGC4Eth significantly improves the computational efficiency of existing detection models while preserving the connectivity of the transaction graph. Furthermore, TGC4Eth enables existing detection models to maintain stable performance and exhibit high robustness against feature evasion attacks.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Distributed Adaptive Control of Disturbed Interconnected Systems with High-Order Tuners</title>
<link>https://arxiv.org/abs/2405.15178</link>
<guid>https://arxiv.org/abs/2405.15178</guid>
<content:encoded><![CDATA[
arXiv:2405.15178v2 Announce Type: replace 
Abstract: This paper addresses the challenge of network synchronization under limited communication, involving heterogeneous agents with different dynamics and various network topologies, to achieve consensus. We investigate the distributed adaptive control for interconnected unknown linear subsystems with a leader and followers, in the presence of input-output disturbance. We enhance the communication within multi-agent systems to achieve consensus under the leadership's guidance. While the measured variable is similar among the followers, the incoming measurements are weighted and constructed based on their proximity to the leader. We also explore the convergence rates across various balanced topologies (Star-like, Cyclic-like, Path, Random), featuring different numbers of agents, using three distributed algorithms, ranging from first- to high-order tuners to effectively address time-varying regressors. The mathematical foundation is rigorously presented from the network designs of the unknown agents following a leader, to the distributed methods. Moreover, we conduct several numerical simulations across various networks, agents and tuners to evaluate the effects of sparsity in the interaction between subsystems using the $L_2-$norm and $L_\infty-$norm. Some networks exhibit a trend where an increasing number of agents results in smaller errors, although this is not universally the case. Additionally, patterns observed at initial times may not reliably predict overall performance across different networks. Finally, we demonstrate that the proposed modified high-order tuner outperforms its counterparts, and we provide related insights along with our conclusions.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Ents: An Efficient Three-party Training Framework for Decision Trees by Communication Optimization</title>
<link>https://arxiv.org/abs/2406.07948</link>
<guid>https://arxiv.org/abs/2406.07948</guid>
<content:encoded><![CDATA[
arXiv:2406.07948v3 Announce Type: replace 
Abstract: Multi-party training frameworks for decision trees based on secure multi-party computation enable multiple parties to train high-performance models on distributed private data with privacy preservation. The training process essentially involves frequent dataset splitting according to the splitting criterion (e.g. Gini impurity). However, existing multi-party training frameworks for decision trees demonstrate communication inefficiency due to the following issues: (1) They suffer from huge communication overhead in securely splitting a dataset with continuous attributes. (2) They suffer from huge communication overhead due to performing almost all the computations on a large ring to accommodate the secure computations for the splitting criterion.
  In this paper, we are motivated to present an efficient three-party training framework, namely Ents, for decision trees by communication optimization. For the first issue, we present a series of training protocols based on the secure radix sort protocols to efficiently and securely split a dataset with continuous attributes. For the second issue, we propose an efficient share conversion protocol to convert shares between a small ring and a large ring to reduce the communication overhead incurred by performing almost all the computations on a large ring. Experimental results from eight widely used datasets show that Ents outperforms state-of-the-art frameworks by $5.5\times \sim 9.3\times$ in communication sizes and $3.9\times \sim 5.3\times$ in communication rounds. In terms of training time, Ents yields an improvement of $3.5\times \sim 6.7\times$. To demonstrate its practicality, Ents requires less than three hours to securely train a decision tree on a widely used real-world dataset (Skin Segmentation) with more than 245,000 samples in the WAN setting.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>TemPrompt: Multi-Task Prompt Learning for Temporal Relation Extraction in RAG-based Crowdsourcing Systems</title>
<link>https://arxiv.org/abs/2406.14825</link>
<guid>https://arxiv.org/abs/2406.14825</guid>
<content:encoded><![CDATA[
arXiv:2406.14825v3 Announce Type: replace 
Abstract: Temporal relation extraction (TRE) aims to grasp the evolution of events or actions, and thus shape the workflow of associated tasks, so it holds promise in helping understand task requests initiated by requesters in crowdsourcing systems. However, existing methods still struggle with limited and unevenly distributed annotated data. Therefore, inspired by the abundant global knowledge stored within pre-trained language models (PLMs), we propose a multi-task prompt learning framework for TRE (TemPrompt), incorporating prompt tuning and contrastive learning to tackle these issues. To elicit more effective prompts for PLMs, we introduce a task-oriented prompt construction approach that thoroughly takes the myriad factors of TRE into consideration for automatic prompt generation. In addition, we present temporal event reasoning as a supplement to bolster the model's focus on events and temporal cues. The experimental results demonstrate that TemPrompt outperforms all compared baselines across the majority of metrics under both standard and few-shot settings. A case study is provided to validate its effectiveness in crowdsourcing scenarios.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>ammBoost: State Growth Control for AMMs</title>
<link>https://arxiv.org/abs/2406.17094</link>
<guid>https://arxiv.org/abs/2406.17094</guid>
<content:encoded><![CDATA[
arXiv:2406.17094v2 Announce Type: replace 
Abstract: Automated market makers (AMMs) are a form of decentralized cryptocurrency exchanges and considered a prime example of Decentralized Finance (DeFi) applications. Their popularity and high trading activity have resulted in millions of on-chain transactions leading to serious scalability issues. In this paper, we address the on-chain storage overhead problem of AMMs by utilizing a new sidechain architecture as a layer 2 solution, building a system called ammBoost. Our system reduces the amount of on-chain transactions, boosts throughput, and supports blockchain pruning. We devise several techniques to enable layer 2 processing for AMMs while preserving correctness and security of the underlying AMM. We also build a proof-of-concept of ammBoost for a Uniswap-inspired use case to empirically evaluate its performance. Our experiments show that ammBoost decreases the gas cost by 94.53% and the chain growth by at least 80%, and that it can support up to 500x of the daily traffic volume observed for Uniswap in practice.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Protecting the 'Stop Using My Data' Right through Blockchain-assisted Evidence Generation</title>
<link>https://arxiv.org/abs/2406.17694</link>
<guid>https://arxiv.org/abs/2406.17694</guid>
<content:encoded><![CDATA[
arXiv:2406.17694v2 Announce Type: replace 
Abstract: In order to provide personalized services to users, Internet-based platforms collect and utilize user-generated behavioral data. Although the 'stop using my data' right should be a fundamental data right, which allows individuals to request their personal data to be no longer utilized by online platforms, the existing preventive data protection measures (e.g., cryptographic data elimination, differential privacy) are unfortunately not applicable. This work aims to develop the first Evidence Generation Framework for deterring post-acquisition data right violations. We formulated the 'stop using my data' problem, which captures a vantage facet of the multi-faceted notion of 'right to be forgotten'. We designed and implemented the first blockchain-assisted system to generate evidence for deterring the violations of the 'stop using my data' right. Our system employs a novel two-stage evidence generation protocol whose efficacy is ensured by a newly proposed Lemma. To validate our framework, we conducted a case study on recommendation systems with systematic evaluation experiments using two real-world datasets: the measured success rate exceeds 99%.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Federated Graph Semantic and Structural Learning</title>
<link>https://arxiv.org/abs/2406.18937</link>
<guid>https://arxiv.org/abs/2406.18937</guid>
<content:encoded><![CDATA[
arXiv:2406.18937v2 Announce Type: replace 
Abstract: Federated graph learning collaboratively learns a global graph neural network with distributed graphs, where the non-independent and identically distributed property is one of the major challenges. Most relative arts focus on traditional distributed tasks like images and voices, incapable of graph structures. This paper firstly reveals that local client distortion is brought by both node-level semantics and graph-level structure. First, for node-level semantics, we find that contrasting nodes from distinct classes is beneficial to provide a well-performing discrimination. We pull the local node towards the global node of the same class and push it away from the global node of different classes. Second, we postulate that a well-structural graph neural network possesses similarity for neighbors due to the inherent adjacency relationships. However, aligning each node with adjacent nodes hinders discrimination due to the potential class inconsistency. We transform the adjacency relationships into the similarity distribution and leverage the global model to distill the relation knowledge into the local model, which preserves the structural information and discriminability of the local model. Empirical results on three graph datasets manifest the superiority of the proposed method over its counterparts.
]]></content:encoded>
<pubDate></pubDate>
</item>
</channel>
</rss>