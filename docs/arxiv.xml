<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs</link>

<item>
<title>UnlinkableDFL: a Practical Mixnet Protocol for Churn-Tolerant Decentralized FL Model Sharing</title>
<link>https://arxiv.org/abs/2602.21343</link>
<guid>https://arxiv.org/abs/2602.21343</guid>
<content:encoded><![CDATA[

arXiv:2602.21343v1 Announce Type: new 
Abstract: Decentralized Federated Learning (DFL) eliminates the need for a central aggregator, but it can expose communication patterns that reveal participant identities. This work presents UnlinkableDFL, a DFL framework that combines a peer-based mixnet with fragment-based model aggregation to ensure unlinkability in fully decentralized settings. Model updates are divided into encrypted fragments, sent over separate multi-hop paths, and aggregated without using any identity information. A theoretical analysis indicates that relay and end-to-end unlinkability improve with larger mixing sets and longer paths, while convergence remains similar to standard FedAvg. A prototype implementation evaluates learning performance, latency, unlinkability, and resource usage. The results show that UnlinkableDFL converges reliably and adapts to node churn. Communication latency emerges as the main overhead, while memory and CPU usage stay moderate. These findings illustrate the balance between anonymity and system efficiency, demonstrating that strong unlinkability can be maintained in decentralized learning workflows.
]]></content:encoded>
<pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
<pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>TM-RUGPULL: A Temporary Sound, Multimodal Dataset for Early Detection of RUG Pulls Across the Tokenized Ecosystem</title>
<link>https://arxiv.org/abs/2602.21529</link>
<guid>https://arxiv.org/abs/2602.21529</guid>
<content:encoded><![CDATA[

arXiv:2602.21529v1 Announce Type: new 
Abstract: Rug-pull attacks pose a systemic threat across the blockchain ecosystem, yet research into early detection is hindered by the lack of scientific-grade datasets. Existing resources often suffer from temporal data leakage, narrow modality, and ambiguous labeling, particularly outside DeFi contexts. To address these limitations, we present TM-RugPull, a rigorously curated, leakage-resistant dataset of 1,028 token projects spanning DeFi, meme coins, NFTs, and celebrity-themed tokens. RugPull enforces strict temporal hygiene by extracting all features on chain behavior, smart contract metadata, and OSINT signals strictly from the first half of each project's lifespan. Labels are grounded in forensic reports and longevity criteria, verified through multi-expert consensus. This dataset enables causally valid, multimodal analysis of rug-pull dynamics and establishes a new benchmark for reproducible fraud detection research.
]]></content:encoded>
<pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
<pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resilient Federated Chain: Transforming Blockchain Consensus into an Active Defense Layer for Federated Learning</title>
<link>https://arxiv.org/abs/2602.21841</link>
<guid>https://arxiv.org/abs/2602.21841</guid>
<content:encoded><![CDATA[

arXiv:2602.21841v1 Announce Type: new 
Abstract: Federated Learning (FL) has emerged as a key paradigm for building Trustworthy AI systems by enabling privacy-preserving, decentralized model training. However, FL is highly susceptible to adversarial attacks that compromise model integrity and data confidentiality, a vulnerability exacerbated by the fact that conventional data inspection methods are incompatible with its decentralized design. While integrating FL with Blockchain technology has been proposed to address some limitations, its potential for mitigating adversarial attacks remains largely unexplored. This paper introduces Resilient Federated Chain (RFC), a novel blockchain-enabled FL framework designed specifically to enhance resilience against such threats. RFC builds upon the existing Proof of Federated Learning architecture by repurposing the redundancy of its Pooled Mining mechanism as an active defense layer that can be combined with robust aggregation rules. Furthermore, the framework introduces a flexible evaluation function in its consensus mechanism, allowing for adaptive defense against different attack strategies. Extensive experimental evaluation on image classification tasks under various adversarial scenarios, demonstrates that RFC significantly improves robustness compared to baseline methods, providing a viable solution for securing decentralized learning environments.
]]></content:encoded>
<pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
<pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>GFPL: Generative Federated Prototype Learning for Resource-Constrained and Data-Imbalanced Vision Task</title>
<link>https://arxiv.org/abs/2602.21873</link>
<guid>https://arxiv.org/abs/2602.21873</guid>
<content:encoded><![CDATA[

arXiv:2602.21873v1 Announce Type: new 
Abstract: Federated learning (FL) facilitates the secure utilization of decentralized images, advancing applications in medical image recognition and autonomous driving. However, conventional FL faces two critical challenges in real-world deployment: ineffective knowledge fusion caused by model updates biased toward majority-class features, and prohibitive communication overhead due to frequent transmissions of high-dimensional model parameters. Inspired by the human brain's efficiency in knowledge integration, we propose a novel Generative Federated Prototype Learning (GFPL) framework to address these issues. Within this framework, a prototype generation method based on Gaussian Mixture Model (GMM) captures the statistical information of class-wise features, while a prototype aggregation strategy using Bhattacharyya distance effectively fuses semantically similar knowledge across clients. In addition, these fused prototypes are leveraged to generate pseudo-features, thereby mitigating feature distribution imbalance across clients. To further enhance feature alignment during local training, we devise a dual-classifier architecture, optimized via a hybrid loss combining Dot Regression and Cross-Entropy. Extensive experiments on benchmarks show that GFPL improves model accuracy by 3.6% under imbalanced data settings while maintaining low communication cost.
]]></content:encoded>
<pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
<pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Unknown Interdependencies for Decentralized Root Cause Analysis in Nonlinear Dynamical Systems</title>
<link>https://arxiv.org/abs/2602.21928</link>
<guid>https://arxiv.org/abs/2602.21928</guid>
<content:encoded><![CDATA[

arXiv:2602.21928v1 Announce Type: new 
Abstract: Root cause analysis (RCA) in networked industrial systems, such as supply chains and power networks, is notoriously difficult due to unknown and dynamically evolving interdependencies among geographically distributed clients. These clients represent heterogeneous physical processes and industrial assets equipped with sensors that generate large volumes of nonlinear, high-dimensional, and heterogeneous IoT data. Classical RCA methods require partial or full knowledge of the system's dependency graph, which is rarely available in these complex networks. While federated learning (FL) offers a natural framework for decentralized settings, most existing FL methods assume homogeneous feature spaces and retrainable client models. These assumptions are not compatible with our problem setting. Different clients have different data features and often run fixed, proprietary models that cannot be modified. This paper presents a federated cross-client interdependency learning methodology for feature-partitioned, nonlinear time-series data, without requiring access to raw sensor streams or modifying proprietary client models. Each proprietary local client model is augmented with a Machine Learning (ML) model that encodes cross-client interdependencies. These ML models are coordinated via a global server that enforces representation consistency while preserving privacy through calibrated differential privacy noise. RCA is performed using model residuals and anomaly flags. We establish theoretical convergence guarantees and validate our approach on extensive simulations and a real-world industrial cybersecurity dataset.
]]></content:encoded>
<pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
<pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Timing Games: Probabilistic backrunning and spam</title>
<link>https://arxiv.org/abs/2602.22032</link>
<guid>https://arxiv.org/abs/2602.22032</guid>
<content:encoded><![CDATA[

arXiv:2602.22032v1 Announce Type: new 
Abstract: There are $n$ players who compete by timing their actions. An opportunity appears randomly on a time interval. Whoever takes an action the fastest after the opportunity has arisen wins. The occurrence of the opportunity is observed only with a delay. Taking actions is costly. We characterize the unique symmetric equilibrium of this game and study worst-case inefficiency of equilibria. Our main motivation is the study of ``probabilistic backrunning" on blockchains, where arbitrageurs want to place an order immediately after a trade that impacts the price on an exchange or after an oracle update. In this context, the number of actions taken can be interpreted as a measure of costly ``spam" generated to compete for the opportunity.
]]></content:encoded>
<pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
<pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>DLT-Corpus: A Large-Scale Text Collection for the Distributed Ledger Technology Domain</title>
<link>https://arxiv.org/abs/2602.22045</link>
<guid>https://arxiv.org/abs/2602.22045</guid>
<content:encoded><![CDATA[

arXiv:2602.22045v1 Announce Type: new 
Abstract: We introduce DLT-Corpus, the largest domain-specific text collection for Distributed Ledger Technology (DLT) research to date: 2.98 billion tokens from 22.12 million documents spanning scientific literature (37,440 publications), United States Patent and Trademark Office (USPTO) patents (49,023 filings), and social media (22 million posts). Existing Natural Language Processing (NLP) resources for DLT focus narrowly on cryptocurrencies price prediction and smart contracts, leaving domain-specific language under explored despite the sector's ~$3 trillion market capitalization and rapid technological evolution.
  We demonstrate DLT-Corpus' utility by analyzing technology emergence patterns and market-innovation correlations. Findings reveal that technologies originate in scientific literature before reaching patents and social media, following traditional technology transfer patterns. While social media sentiment remains overwhelmingly bullish even during crypto winters, scientific and patent activity grow independently of market fluctuations, tracking overall market expansion in a virtuous cycle where research precedes and enables economic growth that funds further innovation.
  We publicly release the full DLT-Corpus; LedgerBERT, a domain-adapted model achieving 23% improvement over BERT-base on a DLT-specific Named Entity Recognition (NER) task; and all associated tools and code.
]]></content:encoded>
<pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
<pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Consensus with Quantum Sybil Resistance</title>
<link>https://arxiv.org/abs/2602.22195</link>
<guid>https://arxiv.org/abs/2602.22195</guid>
<content:encoded><![CDATA[

arXiv:2602.22195v1 Announce Type: cross 
Abstract: Sybil resistance is a key requirement of decentralized consensus protocols. It is achieved by introducing a scarce resource (such as computational power, monetary stake, disk space, etc.), which prevents participants from costlessly creating multiple fake identities and hijacking the protocol. Quantum states are generically uncloneable, which suggests that they may serve naturally as an unconditionally scarce resource. In particular, uncloneability underlies quantum position based-cryptography, which is unachievable classically. We design a consensus protocol that combines classical hybrid consensus protocols with quantum position verification as the Sybil resistance mechanism, providing security in the standard model, and achieving improved energy efficiency compared to hybrid protocols based on Proof-of-Work. Our protocol inherits the benefits of other hybrid protocols, namely the faster confirmation times compared to pure Proof-of-Work protocols, and resilience against the compounding wealth issue that plagues protocols based on Proof-of-Stake Sybil resistance. We additionally propose a spam prevention mechanism for our protocol in the Random Oracle model.
]]></content:encoded>
<pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
<pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
</item>

<item>
<title>The Tragedy of Chain Commons</title>
<link>https://arxiv.org/abs/2602.20341</link>
<guid>https://arxiv.org/abs/2602.20341</guid>
<content:encoded><![CDATA[
arXiv:2602.20341v1 Announce Type: new 
Abstract: Byzantine Fault Tolerant (BFT) consensus forms the foundation of many modern blockchains striving for both high throughput and low latency. A growing bottleneck is transaction execution and validation on the critical path of consensus, which has led to modular decoupled designs that separate ordering from execution: Consensus orders only metadata, while transactions are executed and validated concurrently. While this approach improves performance, it can leave invalid transactions in the ledger, increasing storage costs and enabling new forms of strategic behavior. We present the first systematic study of this setting, providing a formal framework to reason about the interaction between consensus and execution. Using this framework, we show that the decoupled design enables a previously unidentified attack, which we term gaslighting. We prove a fundamental trade-off between resilience to this attack and resource capacity utilization, where both are impossible to achieve deterministically in the decoupled model. To address this trade-off, we discuss an intermediate model for leader-based protocols that is robust to gaslighting attacks while achieving high throughput and low latency.
]]></content:encoded>
<pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wireless Federated Multi-Task LLM Fine-Tuning via Sparse-and-Orthogonal LoRA</title>
<link>https://arxiv.org/abs/2602.20492</link>
<guid>https://arxiv.org/abs/2602.20492</guid>
<content:encoded><![CDATA[
arXiv:2602.20492v1 Announce Type: new 
Abstract: Decentralized federated learning (DFL) based on low-rank adaptation (LoRA) enables mobile devices with multi-task datasets to collaboratively fine-tune a large language model (LLM) by exchanging locally updated parameters with a subset of neighboring devices via wireless connections for knowledge integration.However, directly aggregating parameters fine-tuned on heterogeneous datasets induces three primary issues across the DFL life-cycle: (i) \textit{catastrophic knowledge forgetting during fine-tuning process}, arising from conflicting update directions caused by data heterogeneity; (ii) \textit{inefficient communication and convergence during model aggregation process}, due to bandwidth-intensive redundant model transmissions; and (iii) \textit{multi-task knowledge interference during inference process}, resulting from incompatible knowledge representations coexistence during inference. To address these issues in a fully decentralized scenario, we first propose a sparse-and-orthogonal LoRA that ensures orthogonality between model updates to eliminate direction conflicts during fine-tuning.Then, we analyze how device connection topology affects multi-task performance, prompting a cluster-based topology design during aggregation.Finally, we propose an implicit mixture of experts (MoE) mechanism to avoid the coexistence of incompatible knowledge during inference. Simulation results demonstrate that the proposed approach effectively reduces communication resource consumption by up to $73\%$ and enhances average performance by $5\%$ compared with the traditional LoRA method.
]]></content:encoded>
<pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stability and Generalization of Push-Sum Based Decentralized Optimization over Directed Graphs</title>
<link>https://arxiv.org/abs/2602.20567</link>
<guid>https://arxiv.org/abs/2602.20567</guid>
<content:encoded><![CDATA[
arXiv:2602.20567v1 Announce Type: new 
Abstract: Push-Sum-based decentralized learning enables optimization over directed communication networks, where information exchange may be asymmetric. While convergence properties of such methods are well understood, their finite-iteration stability and generalization behavior remain unclear due to structural bias induced by column-stochastic mixing and asymmetric error propagation. In this work, we develop a unified uniform-stability framework for the Stochastic Gradient Push (SGP) algorithm that captures the effect of directed topology. A key technical ingredient is an imbalance-aware consistency bound for Push-Sum, which controls consensus deviation through two quantities: the stationary distribution imbalance parameter $\delta$ and the spectral gap $(1-\lambda)$ governing mixing speed. This decomposition enables us to disentangle statistical effects from topology-induced bias. We establish finite-iteration stability and optimization guarantees for both convex objectives and non-convex objectives satisfying the Polyak--\L{}ojasiewicz condition. For convex problems, SGP attains excess generalization error of order $\tilde{\mathcal{O}}\!\left(\frac{1}{\sqrt{mn}}+\frac{\gamma}{\delta(1-\lambda)}+\gamma\right)$ under step-size schedules, and we characterize the corresponding optimal early stopping time that minimizes this bound. For P\L{} objectives, we obtain convex-like optimization and generalization rates with dominant dependence proportional to $\kappa\!\left(1+\frac{1}{\delta(1-\lambda)}\right)$, revealing a multiplicative coupling between problem conditioning and directed communication topology. Our analysis clarifies when Push-Sum correction is necessary compared with standard decentralized SGD and quantifies how imbalance and mixing jointly shape the best attainable learning performance.
]]></content:encoded>
<pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Electric Vehicle Energy Demand Forecasting and the Effect of Federated Learning</title>
<link>https://arxiv.org/abs/2602.20782</link>
<guid>https://arxiv.org/abs/2602.20782</guid>
<content:encoded><![CDATA[
arXiv:2602.20782v1 Announce Type: new 
Abstract: The wide spread of new energy resources, smart devices, and demand side management strategies has motivated several analytics operations, from infrastructure load modeling to user behavior profiling. Energy Demand Forecasting (EDF) of Electric Vehicle Supply Equipments (EVSEs) is one of the most critical operations for ensuring efficient energy management and sustainability, since it enables utility providers to anticipate energy/power demand, optimize resource allocation, and implement proactive measures to improve grid reliability. However, accurate EDF is a challenging problem due to external factors, such as the varying user routines, weather conditions, driving behaviors, unknown state of charge, etc. Furthermore, as concerns and restrictions about privacy and sustainability have grown, training data has become increasingly fragmented, resulting in distributed datasets scattered across different data silos and/or edge devices, calling for federated learning solutions. In this paper, we investigate different well-established time series forecasting methodologies to address the EDF problem, from statistical methods (the ARIMA family) to traditional machine learning models (such as XGBoost) and deep neural networks (GRU and LSTM). We provide an overview of these methods through a performance comparison over four real-world EVSE datasets, evaluated under both centralized and federated learning paradigms, focusing on the trade-offs between forecasting fidelity, privacy preservation, and energy overheads. Our experimental results demonstrate, on the one hand, the superiority of gradient boosted trees (XGBoost) over statistical and NN-based models in both prediction accuracy and energy efficiency and, on the other hand, an insight that Federated Learning-enabled models balance these factors, offering a promising direction for decentralized energy demand forecasting.
]]></content:encoded>
<pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Secure and Interoperable Architecture for Electronic Health Record Access Control and Sharing</title>
<link>https://arxiv.org/abs/2602.20830</link>
<guid>https://arxiv.org/abs/2602.20830</guid>
<content:encoded><![CDATA[
arXiv:2602.20830v1 Announce Type: new 
Abstract: Electronic Health Records (EHRs) store sensitive patient information, necessitating stringent access control and sharing mechanisms to uphold data security and comply with privacy regulations such as the General Data Protection Regulation (GDPR). In this paper, we propose a comprehensive architecture with a suite of efficient protocols that leverage the synergistic capabilities of the Blockchain and Interplanetary File System (IPFS) technologies to enable secure access control and sharing of EHRs. Our approach is based on a private blockchain, wherein smart contracts are deployed to enforce control exclusively by patients. By granting patients exclusive control over their EHRs, our solution ensures compliance with personal data protection laws and empowers individuals to manage their health information autonomously. Notably, our proposed architecture seamlessly integrates with existing health provider information systems, facilitating interoperability and effectively addressing security and data heterogeneity challenges. To demonstrate the effectiveness of our approach, we developed a prototype based on a private implementation of the Hyperledger platform, enabling the simulation of diverse scenarios involving access control and health data sharing among healthcare practitioners. Our experimental results demonstrate the scalability of our solution, thereby substantiating its efficacy and robustness in real-world healthcare settings.
]]></content:encoded>
<pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Trading Networks: Equilibria and Fairness</title>
<link>https://arxiv.org/abs/2602.20868</link>
<guid>https://arxiv.org/abs/2602.20868</guid>
<content:encoded><![CDATA[
arXiv:2602.20868v1 Announce Type: cross 
Abstract: We explore stability and fairness considerations in decentralized networked markets with bilateral contracts, building on the trading networks framework [Hatfield et al., 2013]. In our trading network game, we show that a well-defined subset of Nash equilibria can be supported as competitive equilibria. Considering an offer-based trading dynamic as well as a stochastic price clock market, we prove new convergence results to Nash equilibrium and competitive equilibrium, providing a rationale for stability properties in decentralized, dynamic trading networks. Turning to the tension between fairness and (core) stability, we prove several negative results: inessential agents always receive zero utility in any core outcome, and even essential agents can get zero utility in all core outcomes.
]]></content:encoded>
<pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cell-Free Massive MIMO-Assisted SWIPT Using Stacked Intelligent Metasurfaces</title>
<link>https://arxiv.org/abs/2602.20983</link>
<guid>https://arxiv.org/abs/2602.20983</guid>
<content:encoded><![CDATA[
arXiv:2602.20983v1 Announce Type: cross 
Abstract: This study explores a next-generation multiple access (NGMA) framework for cell-free massive MIMO (CF-mMIMO) systems enhanced by stacked intelligent metasurfaces (SIMs), aiming to improve simultaneous wireless information and power transfer (SWIPT) performance. A fundamental challenge lies in optimally selecting the operating modes of access points (APs) to jointly maximize the received energy and satisfy spectral efficiency (SE) quality-of-service constraints. Practical system impairments, including a non-linear harvested energy model, pilot contamination (PC), channel estimation errors, and reliance on long-term statistical channel state information (CSI), are considered. We derive closed-form expressions for both the achievable SE and the average sum harvested energy (sum-HE). A mixed-integer non-convex optimization problem is formulated to jointly optimize the SIM phase shifts, APs mode selection, and power allocation to maximize average sum-HE under SE and average harvested energy constraints. To solve this problem, we propose a centralized training, decentralized execution (CTDE) framework based on deep reinforcement learning (DRL), which efficiently handles high-dimensional decision spaces. A Markovian environment and a normalized joint reward function are introduced to enhance the training stability across on-policy and off-policy DRL algorithms. Additionally, we provide a two-phase convex-based solution as a theoretical robust performance. Numerical results demonstrate that the proposed DRL-based CTDE framework achieves SWIPT performance comparable to convexification-based solution, while significantly outperforming baselines.
]]></content:encoded>
<pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Noise-enabled goal attainment in crowded collectives</title>
<link>https://arxiv.org/abs/2507.08100</link>
<guid>https://arxiv.org/abs/2507.08100</guid>
<content:encoded><![CDATA[
arXiv:2507.08100v2 Announce Type: replace 
Abstract: In crowded environments, individuals must navigate around other occupants to reach their destinations. Understanding and controlling traffic flows in these spaces is relevant for coordinating robot swarms and designing infrastructure for dense populations. Here, we use simulations, theory, and experiments to study how adding stochasticity to agent motion can reduce traffic jams and help agents travel more quickly to prescribed goals. A computational approach reveals the collective behavior. Above a critical noise level, large jams do not persist. From this observation, we analytically approximate the swarm's goal attainment rate, which allows us to solve for the agent density and noise level that maximize the goals reached. Robotic experiments corroborate the behaviors observed in our simulated and theoretical results. Finally, we compare simple, local navigation approaches with a sophisticated but computationally costly central planner. A simple reactive scheme performs well up to moderate densities and is far more computationally efficient than a planner, motivating further research into robust, decentralized navigation methods for crowded environments. By integrating ideas from physics and engineering using simulations, theory, and experiments, our work identifies new directions for emergent traffic research.
]]></content:encoded>
<pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Evolutionary Framework for Safe, Efficient, and Cooperative Autonomous Vehicle Interactions</title>
<link>https://arxiv.org/abs/2509.07411</link>
<guid>https://arxiv.org/abs/2509.07411</guid>
<content:encoded><![CDATA[
arXiv:2509.07411v2 Announce Type: replace 
Abstract: Modern transportation systems face significant challenges in ensuring road safety, given serious injuries caused by road accidents. The rapid growth of autonomous vehicles (AVs) has prompted new traffic designs that aim to optimize interactions among AVs. However, effective interactions between AVs remains challenging due to the absence of centralized control. Besides, there is a need for balancing multiple factors, including passenger demands and overall traffic efficiency. Traditional rule-based, optimization-based, and game-theoretic approaches each have limitations in addressing these challenges. Rule-based methods struggle with adaptability and generalization in complex scenarios, while optimization-based methods often require high computational resources. Game-theoretic approaches, such as Stackelberg and Nash games, suffer from limited adaptability and potential inefficiencies in cooperative settings. This paper proposes an Evolutionary Game Theory (EGT)-based framework for AV interactions that overcomes these limitations by utilizing a decentralized and adaptive strategy evolution mechanism. A causal evaluation module (CEGT) is introduced to optimize the evolutionary rate, balancing mutation and evolution by learning from historical interactions. Simulation results demonstrate the proposed CEGT outperforms EGT and popular benchmark games in terms of lower collision rates, improved safety distances, higher speeds, and overall better performance compared to Nash and Stackelberg games across diverse scenarios and parameter settings.
]]></content:encoded>
<pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Attention Fails Centralized Signals: Rethinking Transformers for Medical Time Series</title>
<link>https://arxiv.org/abs/2602.18473</link>
<guid>https://arxiv.org/abs/2602.18473</guid>
<content:encoded><![CDATA[
arXiv:2602.18473v1 Announce Type: new 
Abstract: Accurate analysis of medical time series (MedTS) data, such as electroencephalography (EEG) and electrocardiography (ECG), plays a pivotal role in healthcare applications, including the diagnosis of brain and heart diseases. MedTS data typically exhibit two critical patterns: temporal dependencies within individual channels and channel dependencies across multiple channels. While recent advances in deep learning have leveraged Transformer-based models to effectively capture temporal dependencies, they often struggle with modeling channel dependencies. This limitation stems from a structural mismatch: MedTS signals are inherently centralized, whereas the Transformer's attention mechanism is decentralized, making it less effective at capturing global synchronization and unified waveform patterns. To address this mismatch, we propose CoTAR (Core Token Aggregation-Redistribution), a centralized MLP-based module designed to replace decentralized attention. Instead of allowing all tokens to interact directly, as in standard attention, CoTAR introduces a global core token that serves as a proxy to facilitate inter-token interactions, thereby enforcing a centralized aggregation and redistribution strategy. This design not only better aligns with the centralized nature of MedTS signals but also reduces computational complexity from quadratic to linear. Experiments on five benchmarks validate the superiority of our method in both effectiveness and efficiency, achieving up to a 12.13% improvement on the APAVA dataset, while using only 33% of the memory and 20% of the inference time compared to the previous state of the art. Code and all training scripts are available at https://github.com/Levi-Ackman/TeCh.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Poster: Privacy-Preserving Compliance Checks on Ethereum via Selective Disclosure</title>
<link>https://arxiv.org/abs/2602.18539</link>
<guid>https://arxiv.org/abs/2602.18539</guid>
<content:encoded><![CDATA[
arXiv:2602.18539v1 Announce Type: new 
Abstract: Digital identity verification often forces a privacy trade-off, where users must disclose sensitive personal data to prove simple eligibility criteria. As blockchain applications integrate with regulated environments, this over-disclosure creates significant risks of data breaches and surveillance. This work proposes a general Selective Disclosure Framework built on Ethereum, designed to decouple attribute verification from identity revelation. By utilizing client-side zk-SNARKs, the framework enables users to prove specific eligibility predicates without revealing underlying identity documents. We present a case study, ZK-Compliance, which implements a functional Grant, Verify, Revoke lifecycle for age verification. Preliminary results indicate that strict compliance requirements can be satisfied with negligible client-side latency (< 200 ms) while preserving the pseudonymous nature of public blockchains.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>HONEST-CAV: Hierarchical Optimization of Network Signals and Trajectories for Connected and Automated Vehicles with Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2602.18740</link>
<guid>https://arxiv.org/abs/2602.18740</guid>
<content:encoded><![CDATA[
arXiv:2602.18740v1 Announce Type: new 
Abstract: This study presents a hierarchical, network-level traffic flow control framework for mixed traffic consisting of Human-driven Vehicles (HVs), Connected and Automated Vehicles (CAVs). The framework jointly optimizes vehicle-level eco-driving behaviors and intersection-level traffic signal control to enhance overall network efficiency and decrease energy consumption. A decentralized Multi-Agent Reinforcement Learning (MARL) approach by Value Decomposition Network (VDN) manages cycle-based traffic signal control (TSC) at intersections, while an innovative Signal Phase and Timing (SPaT) prediction method integrates a Machine Learning-based Trajectory Planning Algorithm (MLTPA) to guide CAVs in executing Eco-Approach and Departure (EAD) maneuvers. The framework is evaluated across varying CAV proportions and powertrain types to assess its effects on mobility and energy performance. Experimental results conducted in a 4*4 real-world network demonstrate that the MARL-based TSC method outperforms the baseline model (i.e., Webster method) in speed, fuel consumption, and idling time. In addition, with MLTPA, HONEST-CAV benefits the traffic system further in energy consumption and idling time. With a 60% CAV proportion, vehicle average speed, fuel consumption, and idling time can be improved/saved by 7.67%, 10.23%, and 45.83% compared with the baseline. Furthermore, discussions on CAV proportions and powertrain types are conducted to quantify the performance of the proposed method with the impact of automation and electrification.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Carbon-aware decentralized dynamic task offloading in MIMO-MEC networks via multi-agent reinforcement learning</title>
<link>https://arxiv.org/abs/2602.18797</link>
<guid>https://arxiv.org/abs/2602.18797</guid>
<content:encoded><![CDATA[
arXiv:2602.18797v1 Announce Type: new 
Abstract: Massive internet of things microservices require integrating renewable energy harvesting into mobile edge computing (MEC) for sustainable eScience infrastructures. Spatiotemporal mismatches between stochastic task arrivals and intermittent green energy along with complex inter-user interference in multi-antenna (MIMO) uplinks complicate real-time resource management. Traditional centralized optimization and off-policy reinforcement learning struggle with scalability and signaling overhead in dense networks. This paper proposes CADDTO-PPO, a carbon-aware decentralized dynamic task offloading framework based on multi-agent proximal policy optimization. The multi-user MIMO-MEC system is modeled as a Decentralized Partially Observable Markov Decision Process (DEC-POMDP) to jointly minimize carbon emissions and buffer latency and energy wastage. A scalable architecture utilizes decentralized execution with parameter sharing (DEPS), which enables autonomous IoT agents to make fine-grained power control and offloading decisions based solely on local observations. Additionally, a carbon-first reward structure adaptively prioritizes green time slots for data transmission to decouple system throughput from grid-dependent carbon footprints. Finally, experimental results demonstrate CADDTO-PPO outperforms deep deterministic policy gradient (DDPG) and lyapunov-based baselines. The framework achieves the lowest carbon intensity and maintains near-zero packet overflow rates under extreme traffic loads. Architectural profiling validates the framework to demonstrate a constant $O(1)$ inference complexity and theoretical lightweight feasibility for future generation sustainable IoT deployments.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Friction Helps: Transaction Confirmation Improves Decision Quality in Blockchain Interactions</title>
<link>https://arxiv.org/abs/2602.18834</link>
<guid>https://arxiv.org/abs/2602.18834</guid>
<content:encoded><![CDATA[
arXiv:2602.18834v1 Announce Type: new 
Abstract: In blockchain applications, transaction confirmation is often treated as usability friction to be minimized or removed. However, confirmation also marks the boundary between deliberation and irreversible commitment, suggesting it may play a functional role in human decision-making. To investigate this tension, we conducted an experiment using a blockchain-based Connect Four game with two interaction modes differing only in authorization flow: manual wallet confirmation (Confirmation Mode) versus auto-authorized delegation (Frictionless Mode). Although participants preferred Frictionless Mode and perceived better performance (N=109), objective performance was worse without confirmation in a counterbalanced deployment (Wave 2: win rate -11.8%, p=0.044; move quality -0.051, p=0.022). Analysis of canceled submissions suggests confirmation can enable pre-submission self-correction (N=66, p=0.005). These findings suggest that transaction confirmation can function as a cognitively meaningful checkpoint rather than mere usability friction, highlighting a trade-off between interaction smoothness and decision quality in irreversible blockchain interactions.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mechanism Design via Market Clearing-Prices for Value Maximizers under Budget and RoS Constraints</title>
<link>https://arxiv.org/abs/2602.19085</link>
<guid>https://arxiv.org/abs/2602.19085</guid>
<content:encoded><![CDATA[
arXiv:2602.19085v1 Announce Type: new 
Abstract: The transition to auto-bidding in online advertising has shifted the focus of auction theory from quasi-linear utility maximization to value maximization subject to financial constraints. We study mechanism design for buyers with private budgets and private Return-on-Spend (RoS) constraints, but public valuations, a setting motivated by modern advertising platforms where valuations are predicted via machine learning models. We introduce the extended Eisenberg-Gale program, a convex optimization framework generalized to incorporate RoS constraints. We demonstrate that the solution to this program is unique and characterizes the market's competitive equilibrium. Based on this theoretical analysis, we design a market-clearing mechanism and prove two key properties: (1) it is incentive-compatible with respect to financial constraints, making truthful reporting the optimal strategy; and (2) it achieves a tight 1/2-approximation of the first-best revenue benchmark, the maximum revenue of any feasible mechanism, regardless of IC. Finally, to enable practical implementation, we present a decentralized online algorithm. Ignoring logarithmic factors, we prove that under this algorithm, both the seller's revenue and each buyer's utility converge to the equilibrium benchmarks with a sublinear regret of $\tilde{O}(\sqrt{m})$ over $m$ auctions.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counted NFT Transfers</title>
<link>https://arxiv.org/abs/2602.19199</link>
<guid>https://arxiv.org/abs/2602.19199</guid>
<content:encoded><![CDATA[
arXiv:2602.19199v1 Announce Type: new 
Abstract: Non-fungible tokens (NFTs) on Ethereum currently follow a binary mobility paradigm: ERC-721 enables unrestricted transfers, whereas SBTs (ERC-5192) prohibit transfers entirely. We identify a design gap in which no standard mechanism supports bounded transferability, where ownership mobility is allowed but limited to a finite number of programmable transfers. We study counted NFT transfers and introduce ERC-7634 as a minimal realization compatible with ERC-721. The design augments each token with a transfer counter and configurable cap L, allowing ownership to evolve under a finite transfer budget. ERC-7634 defines a minimal extension interface with three lightweight functions (transferCountOf, setTransferLimit, and transferLimitOf), two events, and native-transfer hooks, requiring fewer than 60 additional lines of Solidity while preserving full backward compatibility with existing NFT infrastructure.
  We analyze behavioral and economic consequences of counted transfers. Our results reveal (i) a mobility premium induced by remaining transfer capacity, (ii) a protocol-level costing signal that can deter wash trading in cap-aware markets through irreversible budget consumption, (iii) bounded recursive collateralization enabled by limited ownership turnover, and (iv) associated security and gas-cost implications, including wrapper-bypass trade-offs.
  Evaluation on calibrated simulations shows that moderate limits (e.g., L = 10) affect fewer than 15% of tokens under representative transfer distributions, while repeated manipulation becomes unprofitable after a few cycles in a cap-aware pricing model; the additional gas overhead remains below 11% per transfer. We further position ERC-7634 within the NFT mobility design space, derive practical cap-selection guidelines, and discuss post-cap ownership outcomes including soulbound conversion, auto-burn, and provenance freeze.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comparative Analysis of Peer Support in Forum-based and Chat-based Mental Health Communities: Technical-Structural-Functional Model of Social Support</title>
<link>https://arxiv.org/abs/2602.19232</link>
<guid>https://arxiv.org/abs/2602.19232</guid>
<content:encoded><![CDATA[
arXiv:2602.19232v1 Announce Type: new 
Abstract: Online support communities have become vital spaces offering varied forms of support to individuals facing mental health challenges. Despite the proliferation of platforms with distinct technical structures, little is known about how these features shape support dynamics and the socio-technical mechanisms at play. This study introduces a technical-structural-functional model of social support and systematically compares communication network structures and support types in 20 forum-based and 20 chat-based mental health communities. Using supervised machine learning and social network analysis, we find that forum-based communities foster more informational and emotional support, whereas chat-based communities promote greater companionship. These patterns were partially explained by network structure: higher in-degree centralization in forums accounted for the prevalence of informational support, while decentralized reply patterns in chat groups accounted for more companionship. These findings extend the structural-functional model of support to online contexts and provide actionable guidance for designing support communities that align technical structures with users' support needs.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Safe and Interpretable Multimodal Path Planning for Multi-Agent Cooperation</title>
<link>https://arxiv.org/abs/2602.19304</link>
<guid>https://arxiv.org/abs/2602.19304</guid>
<content:encoded><![CDATA[
arXiv:2602.19304v1 Announce Type: new 
Abstract: Successful cooperation among decentralized agents requires each agent to quickly adapt its plan to the behavior of other agents. In scenarios where agents cannot confidently predict one another's intentions and plans, language communication can be crucial for ensuring safety. In this work, we focus on path-level cooperation in which agents must adapt their paths to one another in order to avoid collisions or perform physical collaboration such as joint carrying. In particular, we propose a safe and interpretable multimodal path planning method, CaPE (Code as Path Editor), which generates and updates path plans for an agent based on the environment and language communication from other agents. CaPE leverages a vision-language model (VLM) to synthesize a path editing program verified by a model-based planner, grounding communication to path plan updates in a safe and interpretable way. We evaluate our approach in diverse simulated and real-world scenarios, including multi-robot and human-robot cooperation in autonomous driving, household, and joint carrying tasks. Experimental results demonstrate that CaPE can be integrated into different robotic systems as a plug-and-play module, greatly enhancing a robot's ability to align its plan to language communication from other robots or humans. We also show that the combination of the VLM-based path editing program synthesis and model-based planning safety enables robots to achieve open-ended cooperation while maintaining safety and interpretability.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Attack-Resilient CLF-Based Control of Nonlinear DC Microgrids under FDI Attacks</title>
<link>https://arxiv.org/abs/2602.19386</link>
<guid>https://arxiv.org/abs/2602.19386</guid>
<content:encoded><![CDATA[
arXiv:2602.19386v1 Announce Type: new 
Abstract: The growing deployment of nonlinear, converter interfaced distributed energy resources (DERs) in DC microgrids demands decentralized controllers that remain stable and resilient under a wide range of cyber-physical attacks and disturbances. Traditional droop or linearized control methods lack resilience and scalability, especially when the system operates in its nonlinear region or faces diverse false-data-injection (FDI) attacks on control inputs. In this work, we develop a Decentralized Attack-Resilient Control Lyapunov Function (AR-CLF) based Quadratic Program (QP) control framework for nonlinear DC microgrids that ensures large-signal stability in a fully decentralized manner. Built upon the port-Hamiltonian representation, the proposed controller dynamically compensates diverse attacks including exponentially unbounded control-input perturbations beyond the bounded-attack regime commonly assumed in existing methods, through an adaptive resilience term, without requiring global information. Simulations validate that the AR-CLF based QP controller achieves superior stability and resilience against unbounded attacks, paving the way for scalable, attack-resilient, and physically consistent control of next-generation DC microgrids.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hilbert-Augmented Reinforcement Learning for Scalable Multi-Robot Coverage and Exploration</title>
<link>https://arxiv.org/abs/2602.19400</link>
<guid>https://arxiv.org/abs/2602.19400</guid>
<content:encoded><![CDATA[
arXiv:2602.19400v1 Announce Type: new 
Abstract: We present a coverage framework that integrates Hilbert space-filling priors into decentralized multi-robot learning and execution. We augment DQN and PPO with Hilbert-based spatial indices to structure exploration and reduce redundancy in sparse-reward environments, and we evaluate scalability in multi-robot grid coverage. We further describe a waypoint interface that converts Hilbert orderings into curvature-bounded, time-parameterized SE(2) trajectories (planar (x, y, {\theta})), enabling onboard feasibility on resource-constrained robots. Experiments show improvements in coverage efficiency, redundancy, and convergence speed over DQN/PPO baselines. In addition, we validate the approach on a Boston Dynamics Spot legged robot, executing the generated trajectories in indoor environments and observing reliable coverage with low redundancy. These results indicate that geometric priors improve autonomy and scalability for swarm and legged robotics.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Causal Representation Learning in State-Space Systems for Decentralized Counterfactual Reasoning</title>
<link>https://arxiv.org/abs/2602.19414</link>
<guid>https://arxiv.org/abs/2602.19414</guid>
<content:encoded><![CDATA[
arXiv:2602.19414v1 Announce Type: new 
Abstract: Networks of interdependent industrial assets (clients) are tightly coupled through physical processes and control inputs, raising a key question: how would the output of one client change if another client were operated differently? This is difficult to answer because client-specific data are high-dimensional and private, making centralization of raw data infeasible. Each client also maintains proprietary local models that cannot be modified. We propose a federated framework for causal representation learning in state-space systems that captures interdependencies among clients under these constraints. Each client maps high-dimensional observations into low-dimensional latent states that disentangle intrinsic dynamics from control-driven influences. A central server estimates the global state-transition and control structure. This enables decentralized counterfactual reasoning where clients predict how outputs would change under alternative control inputs at others while only exchanging compact latent states. We prove convergence to a centralized oracle and provide privacy guarantees. Our experiments demonstrate scalability, and accurate cross-client counterfactual inference on synthetic and real-world industrial control system datasets.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAmmStein: Regime Adaptation in Mean-reverting Markets with Stein Thresholds -- Optimal Impulse Control in Concentrated AMMs</title>
<link>https://arxiv.org/abs/2602.19419</link>
<guid>https://arxiv.org/abs/2602.19419</guid>
<content:encoded><![CDATA[
arXiv:2602.19419v1 Announce Type: new 
Abstract: Concentrated liquidity provision in decentralized exchanges presents a fundamental Impulse Control problem. Liquidity Providers (LPs) face a non-trivial trade-off between maximizing fee accrual through tight price-range concentration and minimizing the friction costs of rebalancing, including gas fees and swap slippage. Existing methods typically employ heuristic or threshold strategies that fail to account for market dynamics. This paper formulates liquidity management as an optimal control problem and derives the corresponding Hamilton-Jacobi-Bellman quasi-variational inequality (HJB-QVI). We present an approximate solution RAmmStein, a Deep Reinforcement Learning method that incorporates the mean-reversion speed (theta) of an Ornstein-Uhlenbeck process among other features as input to the model. We demonstrate that the agent learns to separate the state space into regions of action and inaction. We evaluate the framework using high-frequency 1Hz Coinbase trade data comprising over 6.8M trades. Experimental results show that RAmmStein achieves a superior net ROI of 0.72% compared to both passive and aggressive strategies. Notably, the agent reduces rebalancing frequency by 67% compared to a greedy rebalancing strategy while maintaining 88% active time. Our results demonstrate that regime-aware laziness can significantly improve capital efficiency by preserving the returns that would otherwise be eroded by the operational costs.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Welfare Gap of Strategic Storage: Universal Bounds and Price Non-Linearity</title>
<link>https://arxiv.org/abs/2602.19660</link>
<guid>https://arxiv.org/abs/2602.19660</guid>
<content:encoded><![CDATA[
arXiv:2602.19660v1 Announce Type: new 
Abstract: This paper studies the efficiency of battery storage operations in electricity markets by comparing the social welfare gain achieved by a central planner to that of a decentralized profit-maximizing operator. The problem is formulated in a generalized continuous-time stochastic setting, where the battery follows an adaptive, non-anticipating policy subject to periodicity and other constraints. We quantify the efficiency loss by bounding the ratio of the optimal welfare gain to the gain under profit maximization. First, for linear price functions, we prove that this ratio is tightly bounded by $4/3$. We show that this bound is a structural invariant: it is robust to arbitrary stochastic demand processes and accommodates general convex operational constraints. Second, we demonstrate that the efficiency loss can be unbounded for general convex price functions, implying that convexity alone is insufficient to guarantee market efficiency. Finally, to bridge these regimes, we analyze monomial price functions, where the degree controls the curvature. For specific discrete demand scenarios, we demonstrate that the ratio is bounded by $2$, independent of the degree.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Flow Extension to Coroutine Types for Deadlock Detection in Go</title>
<link>https://arxiv.org/abs/2602.19686</link>
<guid>https://arxiv.org/abs/2602.19686</guid>
<content:encoded><![CDATA[
arXiv:2602.19686v1 Announce Type: new 
Abstract: Coroutines, as an abstract programming construct, are a generalization of functions that can suspend execution part- way for later resumption. Coroutine Types are behavioral types to model interactions of coroutines with a single receiving operation followed by a single yielding operation. Coroutine Types have been applied to model-driven engineering, smart contracts, and test case generation. We contribute a Flow extension to Coroutine Types, so that coroutines with more than one receiving and yielding operation can be modeled. We accordingly revise the reduction rules of Coroutine Types. To show the usefulness of the Flow extension, we contribute a type system that maps expressions of the Go programming language to Coroutine Types. If the reduction result is 0, the two channel operations are paired properly and the program has no deadlocks. We choose Go because it is a popular programming language for distributed systems, but a frequent kind of bugs in Go is deadlocks due to the wrong use of concurrency features. We concentrate on the most commonly used semantics in Go: unbuffered channels with the keywords go and defer. Our Flow extension and the type system recognize 17 patterns of channels and goroutine interactions, including mismatched receivers and senders, nested goroutines, etc. We also integrate the Z3 SMT solver to take account of conditional execution and type inheritance. Other static or dynamic deadlock detectors crashed or gave wrong predictions in some patterns. Therefore, our type-based deadlock analyzer not only fills the gap in the landscape of value-based detection, but also complements existing detectors.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Secure and Private Distributed Bayesian Federated Learning Design</title>
<link>https://arxiv.org/abs/2602.20003</link>
<guid>https://arxiv.org/abs/2602.20003</guid>
<content:encoded><![CDATA[
arXiv:2602.20003v1 Announce Type: new 
Abstract: Distributed Federated Learning (DFL) enables decentralized model training across large-scale systems without a central parameter server. However, DFL faces three critical challenges: privacy leakage from honest-but-curious neighbors, slow convergence due to the lack of central coordination, and vulnerability to Byzantine adversaries aiming to degrade model accuracy. To address these issues, we propose a novel DFL framework that integrates Byzantine robustness, privacy preservation, and convergence acceleration. Within this framework, each device trains a local model using a Bayesian approach and independently selects an optimal subset of neighbors for posterior exchange. We formulate this neighbor selection as an optimization problem to minimize the global loss function under security and privacy constraints. Solving this problem is challenging because devices only possess partial network information, and the complex coupling between topology, security, and convergence remains unclear. To bridge this gap, we first analytically characterize the trade-offs between dynamic connectivity, Byzantine detection, privacy levels, and convergence speed. Leveraging these insights, we develop a fully distributed Graph Neural Network (GNN)-based Reinforcement Learning (RL) algorithm. This approach enables devices to make autonomous connection decisions based on local observations. Simulation results demonstrate that our method achieves superior robustness and efficiency with significantly lower overhead compared to traditional security and privacy schemes.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Event-Triggered Gossip for Distributed Learning</title>
<link>https://arxiv.org/abs/2602.19116</link>
<guid>https://arxiv.org/abs/2602.19116</guid>
<content:encoded><![CDATA[
arXiv:2602.19116v1 Announce Type: cross 
Abstract: While distributed learning offers a new learning paradigm for distributed network with no central coordination, it is constrained by communication bottleneck between nodes.
  We develop a new event-triggered gossip framework for distributed learning to reduce inter-node communication overhead. The framework introduces an adaptive communication control mechanism that enables each node to autonomously decide in a fully decentralized fashion when to exchange model information with its neighbors based on local model deviations. We analyze the ergodic convergence of the proposed framework under noconvex objectives and interpret the convergence guarantees under different triggering conditions. Simulation results show that the proposed framework achieves substantially lower communication overhead than the state-of-the-art distributed learning methods, reducing cumulative point-to-point transmissions by \textbf{71.61\%} with only a marginal performance loss, compared with the conventional full-communication baseline.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Cooperative Multi-Agent Probabilistic Framework for Search and Track Missions</title>
<link>https://arxiv.org/abs/2302.10723</link>
<guid>https://arxiv.org/abs/2302.10723</guid>
<content:encoded><![CDATA[
arXiv:2302.10723v2 Announce Type: replace 
Abstract: In this work a robust and scalable cooperative multi-agent searching and tracking framework is proposed. Specifically, we study the problem of cooperative searching and tracking of multiple moving targets by a group of autonomous mobile agents with limited sensing capabilities. We assume that the actual number of targets present is not known a priori and that target births/deaths can occur anywhere inside the surveillance region thus efficient search strategies are required to detect and track as many targets as possible. To address the aforementioned challenges we recursively compute and propagate in time the searching-and-tracking (SAT) density. Using the SAT-density, we then develop decentralized cooperative look-ahead strategies for efficient searching and tracking of an unknown number of targets inside a bounded surveillance area.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Hierarchical Gradient Tracking Algorithm for Mitigating Subnet-Drift in Fog Learning Networks</title>
<link>https://arxiv.org/abs/2409.17430</link>
<guid>https://arxiv.org/abs/2409.17430</guid>
<content:encoded><![CDATA[
arXiv:2409.17430v3 Announce Type: replace 
Abstract: Federated learning (FL) encounters scalability challenges when implemented over fog networks that do not follow FL's conventional star topology architecture. Semi-decentralized FL (SD-FL) has proposed a solution for device-to-device (D2D) enabled networks that divides model cooperation into two stages: at the lower stage, D2D communications is employed for local model aggregations within subnetworks (subnets), while the upper stage handles device-server (DS) communications for global model aggregations. However, existing SD-FL schemes are based on gradient diversity assumptions that become performance bottlenecks as data distributions become more heterogeneous. In this work, we develop semi-decentralized gradient tracking (SD-GT), the first SD-FL methodology that removes the need for such assumptions by incorporating tracking terms into device updates for each communication layer. Our analytical characterization of SD-GT reveals upper bounds on convergence for non-convex, convex, and strongly-convex problems. We show how the bounds enable the development of an optimization algorithm that navigates the performance-efficiency trade-off by tuning subnet sampling rate and D2D rounds for each global training interval. Our subsequent numerical evaluations demonstrate that SD-GT obtains substantial improvements in trained model quality and communication cost relative to baselines in SD-FL and gradient tracking on several datasets.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoBRA: A Universal Strategyproof Confirmation Protocol for Quorum-based Proof-of-Stake Blockchains</title>
<link>https://arxiv.org/abs/2503.16783</link>
<guid>https://arxiv.org/abs/2503.16783</guid>
<content:encoded><![CDATA[
arXiv:2503.16783v3 Announce Type: replace 
Abstract: The security of many Proof-of-Stake (PoS) payment systems relies on quorum-based State Machine Replication (SMR) protocols. While classical analyses assume purely Byzantine faults, real-world systems must tolerate both arbitrary failures and strategic, profit-driven validators. We therefore study quorum-based SMR under a hybrid model with honest, Byzantine, and rational participants.
  We first establish the fundamental limitations of traditional consensus mechanisms, proving two impossibility results: (1) in partially synchronous networks, no quorum-based protocol can achieve SMR when rational and Byzantine validators collectively exceed $1/3$ of the participants; and (2) even under synchronous network assumptions, SMR remains unattainable if this coalition comprises more than $2/3$ of the validator set.
  Assuming a synchrony bound $\Delta$, we show how to extend any quorum-based SMR protocol to tolerate up to $1/3$ Byzantine and $1/3$ rational validators by modifying only its finalization rule. Our approach enforces a necessary bound on the total transaction volume finalized within any time window $\Delta$ and introduces the \emph{strongest chain rule}, which enables efficient finalization of transactions when a supermajority of honest participants provably supports execution. Empirical analysis of Ethereum and Cosmos demonstrates validator participation exceeding the required $5/6$ threshold in over $99%$ of blocks, supporting the practicality of our design.
  Finally, we present a recovery mechanism that restores safety and liveness after consistency violations, even with up to $5/9$ Byzantine stake and $1/9$ rational stake, guaranteeing full reimbursement of provable client losses.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grassroots Federation: Fair Democratic Governance at Scale</title>
<link>https://arxiv.org/abs/2505.02208</link>
<guid>https://arxiv.org/abs/2505.02208</guid>
<content:encoded><![CDATA[
arXiv:2505.02208v5 Announce Type: replace 
Abstract: We propose a framework for the fair democratic governance of federated digital communities that form and evolve dynamically, where small groups self-govern and larger groups are represented by assemblies selected via sortition. Prior work addressed static fairness conditions; here, we formalize a dynamic setting where federations evolve over time through communities forming, joining, and splitting, in all directions-bottom-up, top-down, and middle-out-and adapt the fairness guarantees. The main technical challenge is reconciling integral seat allocations with dynamic, overlapping federations, so that child communities always meet their persistent floors while long-run averages converge to proportional fairness. Overcoming these challenges, we introduce a protocol that ensures fair participation and representation both persistently (at all times) and eventually (in the limit after stabilization), extending the static fairness properties to handle structural changes.
  Prior work shows that grassroots federations can be specified via atomic transactions among assembly members, and that Constitutional Consensus can realize both these transactions and the democratic processes leading to them. Together, the four works form a complete design for an egalitarian, fairly governed, large-scale decentralized sovereign digital community platform.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SHE-LoRA: Selective Homomorphic Encryption for Federated Tuning with Heterogeneous LoRA</title>
<link>https://arxiv.org/abs/2505.21051</link>
<guid>https://arxiv.org/abs/2505.21051</guid>
<content:encoded><![CDATA[
arXiv:2505.21051v2 Announce Type: replace 
Abstract: Federated fine-tuning is critical for improving the performance of large language models (LLMs) in handling domain-specific tasks while keeping training data decentralized and private. However, prior work has shown that clients' private data can actually be recovered via gradient inversion attacks. Existing privacy preservation techniques against such attacks typically entail performance degradation and high costs, making them ill-suited for clients with heterogeneous data distributions and device capabilities. In this paper, we propose SHE-LoRA, which integrates selective homomorphic encryption (SHE) and low-rank adaptation (LoRA) to enable efficient and privacy-preserving federated tuning of LLMs in cross-device environments. Based on model parameter sensitivity assessment, heterogeneous clients adaptively negotiate and select a subset of model parameters for homomorphic encryption. To ensure accurate model aggregation, we design a column-aware secure aggregation method and customized reparameterization techniques to align the aggregation results with the heterogeneous device capabilities of clients. Extensive experiments demonstrate that SHE-LoRA maintains performance comparable to non-private baselines, achieves strong resistance to state-of-the-art attacks, and significantly reduces communication overhead by 99.71% and encryption time by 99.87%, compared to HE baselines.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Information-Optimized Multi-Agent Path Finding: A Hybrid Framework with Reduced Inter-Agent Information Sharing</title>
<link>https://arxiv.org/abs/2510.09469</link>
<guid>https://arxiv.org/abs/2510.09469</guid>
<content:encoded><![CDATA[
arXiv:2510.09469v2 Announce Type: replace 
Abstract: Multi-agent pathfinding (MAPF) remains a critical problem in robotics and autonomous systems, where agents must navigate shared spaces efficiently while avoiding conflicts. Traditional centralized algorithms with global information provide high-quality solutions but scale poorly in large-scale scenarios due to the combinatorial explosion of conflicts. Conversely, distributed approaches that have local information, particularly learning-based methods, offer better scalability by operating with relaxed information availability, yet often at the cost of solution quality. In realistic deployments, information is a constrained resource: broadcasting full agent states and goals can raise privacy concerns, strain limited bandwidth, and require extra sensing and communication hardware, increasing cost and energy use. We focus on the core question of how MAPF can be solved with minimal inter-agent information sharing while preserving solution feasibility. To this end, we present an information-centric formulation of the MAPF problem and introduce a hybrid framework, IO-MAPF, that integrates decentralized path planning with a lightweight centralized coordinator. In this framework, agents use reinforcement learning (RL) to plan independently, while the central coordinator provides minimal, targeted signals, such as static conflict-cell indicators or short conflict trajectories, that are dynamically shared to support efficient conflict resolution. We introduce an Information Units (IU) metric to quantify information use and show that our alert-driven design achieves 2x to 23x reduction in information sharing, compared to the state-of-the-art algorithms, while maintaining high success rates, demonstrating that reliable MAPF is achievable under strongly information-restricted, privacy-preserving conditions. We demonstrate the effectiveness of our algorithm using simulation and hardware experiments.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stability of a DC Microgrid with a Nonlinear Nested Control Framework: The Fast Communication Scenario</title>
<link>https://arxiv.org/abs/2511.10401</link>
<guid>https://arxiv.org/abs/2511.10401</guid>
<content:encoded><![CDATA[
arXiv:2511.10401v2 Announce Type: replace 
Abstract: As modern power systems continue to evolve into multi-agent, converter-dominated systems that demand reliable, stable, and optimal control architectures within an expandable framework, this paper investigates scalable stability guarantees of a promising nonlinear communication-reliant control framework for DC microgrids. Particularly, relying on nested control loops; inner decentralized(primary) and outer distributed(secondary), the control configurations are designed to simultaneously achieve proportional current sharing and voltage containment within pre-specified limits, at the converged steady state. By enforcing sufficient time-scale separation at the boarder between the control loops, the system admits a singular perturbation formulation, allowing global exponential stability (G.E.S.) to be established via Lyapunov arguments. Although the theoretical G.E.S. certificate is structurally scalable, the stability guarantees depends on a sufficiently large permanent leakage, introduced in the primary controller. Thus, the results of this paper emphasize the importance of appropriate practical tuning guidelines and electrical parameter selection. The effectiveness of the proposed method is validated through case studies on a low-voltage DC microgrid under load variations and topological changes (and communication time-delays), followed by a small-signal stability analysis.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ev-Trust: An Evolutionary Stable Trust Mechanism for Decentralized LLM-Based Multi-Agent Service Economies</title>
<link>https://arxiv.org/abs/2512.16167</link>
<guid>https://arxiv.org/abs/2512.16167</guid>
<content:encoded><![CDATA[
arXiv:2512.16167v2 Announce Type: replace 
Abstract: Autonomous LLM-based agents are increasingly engaging in decentralized service interactions to collaboratively execute complex tasks. However, the intrinsic instability and low-cost generativity of LLMs introduce a systemic vulnerability, where self-interested agents are incentivized to pursue short-term gains through deceptive behaviors. Such strategies can rapidly proliferate within the population and precipitate a systemic trust collapse. To address this, we propose Ev-Trust, a strategy-equilibrium trust mechanism grounded in evolutionary game theory. Ev-Trust constructs a dynamic feedback loop that couples trust evaluation with evolutionary incentives, embedding interaction history and reputation directly into the agent's expected revenue function. This mechanism fundamentally reshapes the revenue structure, converting trustworthiness into a decisive survival advantage that suppresses short-sightedness. We provide a rigorous theoretical foundation based on the Replicator Dynamics, proving the asymptotic stability of Evolutionary Stable Strategies (ESS) that favor cooperation. Experimental results indicate that Ev-Trust effectively eliminates malicious strategies and enhances collective revenue, exhibiting resilience against the invasion of mutant behaviors.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning</title>
<link>https://arxiv.org/abs/2512.20363</link>
<guid>https://arxiv.org/abs/2512.20363</guid>
<content:encoded><![CDATA[
arXiv:2512.20363v2 Announce Type: replace 
Abstract: Federated learning (FL) supports privacy-preserving, decentralized machine learning (ML) model training by keeping data on client devices. However, non-independent and identically distributed (non-IID) data across clients biases updates and degrades performance. To alleviate these issues, we propose Clust-PSI-PFL, a clustering-based personalized FL framework that uses the Population Stability Index (PSI) to quantify the level of non-IID data. We compute a weighted PSI metric, $WPSI^L$, which we show to be more informative than common non-IID metrics (Hellinger, Jensen-Shannon, and Earth Mover's distance). Using PSI features, we form distributionally homogeneous groups of clients via K-means++; the number of optimal clusters is chosen by a systematic silhouette-based procedure, typically yielding few clusters with modest overhead. Across six datasets (tabular, image, and text modalities), two partition protocols (Dirichlet with parameter $\alpha$ and Similarity with parameter S), and multiple client sizes, Clust-PSI-PFL delivers up to 18% higher global accuracy than state-of-the-art baselines and markedly improves client fairness by a relative improvement of 37% under severe non-IID data. These results establish PSI-guided clustering as a principled, lightweight mechanism for robust PFL under label skew.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resource-Aware Distributed Submodular Maximization: A Paradigm for Multi-Robot Decision-Making</title>
<link>https://arxiv.org/abs/2204.07520</link>
<guid>https://arxiv.org/abs/2204.07520</guid>
<content:encoded><![CDATA[
arXiv:2204.07520v4 Announce Type: replace-cross 
Abstract: Multi-robot decision-making is the process where multiple robots coordinate actions. In this paper, we aim for efficient and effective multi-robot decision-making despite the robots' limited on-board resources and the often resource-demanding complexity of their tasks. We introduce the first algorithm enabling the robots to choose with which few other robots to coordinate and provably balance the trade-off of centralized vs. decentralized coordination. Particularly, centralization favors globally near-optimal decision-making but at the cost of increased on-board resource requirements; whereas, decentralization favors minimal resource requirements but at a global suboptimality cost. All robots can thus afford our algorithm, irrespective of their resources. We are motivated by the future of autonomy that involves multiple robots coordinating actions to complete resource-demanding tasks, such as target tracking, area coverage, and monitoring. To provide closed-form guarantees, we focus on maximization problems involving monotone and 2nd-order submodular functions. To capture the cost of decentralization, we introduce the notion of Centralization Of Information among non-Neighbors (COIN). We validate our algorithm in simulated scenarios of image covering.
]]></content:encoded>
<pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting Liquidity Exhaustion Attacks in Intent-Based Cross-Chain Bridges</title>
<link>https://arxiv.org/abs/2602.17805</link>
<guid>https://arxiv.org/abs/2602.17805</guid>
<content:encoded><![CDATA[
arXiv:2602.17805v1 Announce Type: new 
Abstract: Intent-based cross-chain bridges have emerged as an alternative to traditional interoperability protocols by allowing off-chain entities (\emph{solvers}) to immediately fulfill users' orders by fronting their own liquidity. While improving user experience, this approach introduces new systemic risks, such as solver liquidity concentration and delayed settlement. In this paper, we propose a new class of attacks called \emph{liquidity exhaustion attacks} and a replay-based parameterized attack simulation framework. We analyze 3.5 million cross-chain intents that moved \$9.24B worth of tokens between June and November 2025 across three major protocols (Mayan Swift, Across, and deBridge), spanning nine blockchains.
  For rational attackers, our results show that protocols with higher solver profitability, such as deBridge, are vulnerable under current parameters: 210 historical attack instances yield a mean net profit of \$286.14, with 80.5\% of attacks profitable. In contrast, Across remains robust in all tested configurations due to low solver margins and very high liquidity, while Mayan Swift is generally secure but becomes vulnerable under stress-test conditions. Under byzantine attacks, we show that it is possible to suppress availability across all protocols, causing dozens of failed intents and solver profit losses of up to \$978 roughly every 16 minutes. Finally, we propose an optimized attack strategy that exploits patterns in the data to reduce attack costs by up to 90.5\% compared to the baseline, lowering the barrier to liquidity exhaustion attacks.
]]></content:encoded>
<pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>StableAML: Machine Learning for Behavioral Wallet Detection in Stablecoin Anti-Money Laundering on Ethereum</title>
<link>https://arxiv.org/abs/2602.17842</link>
<guid>https://arxiv.org/abs/2602.17842</guid>
<content:encoded><![CDATA[
arXiv:2602.17842v1 Announce Type: new 
Abstract: Global illicit fund flows exceed an estimated $3.1 trillion annually, with stablecoins emerging as a preferred laundering medium due to their liquidity. While decentralized protocols increasingly adopt zero-knowledge proofs to obfuscate transaction graphs, centralized stablecoins remain critical "transparent choke points" for compliance. Leveraging this persistent visibility, this study analyzes an Ethereum dataset and uses behavioral features to develop a robust AML framework. Our findings demonstrate that domain-informed tree ensemble models achieve higher Macro-F1 score, significantly outperforming graph neural networks, which struggle with the increasing fragmentation of transaction networks. The model's interpretability goes beyond binary detection, successfully dissecting distinct typologies: it differentiates the complex, high-velocity dispersion of cybercrime syndicates from the constrained, static footprints left by sanctioned entities. This framework aligns with the industry shift toward deterministic verification, satisfying the auditability and compliance expectations under regulations such as the EU's MiCA and the U.S. GENIUS Act while minimizing unjustified asset freezes. By automating high-precision detection, we propose an approach that effectively raises the economic cost of financial misconduct without stifling innovation.
]]></content:encoded>
<pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>PenTiDef: Enhancing Privacy and Robustness in Decentralized Federated Intrusion Detection Systems against Poisoning Attacks</title>
<link>https://arxiv.org/abs/2602.17973</link>
<guid>https://arxiv.org/abs/2602.17973</guid>
<content:encoded><![CDATA[
arXiv:2602.17973v1 Announce Type: new 
Abstract: The increasing deployment of Federated Learning (FL) in Intrusion Detection Systems (IDS) introduces new challenges related to data privacy, centralized coordination, and susceptibility to poisoning attacks. While significant research has focused on protecting traditional FL-IDS with centralized aggregation servers, there remains a notable gap in addressing the unique challenges of decentralized FL-IDS (DFL-IDS). This study aims to address the limitations of traditional centralized FL-IDS by proposing a novel defense framework tailored for the decentralized FL-IDS architecture, with a focus on privacy preservation and robustness against poisoning attacks. We propose PenTiDef, a privacy-preserving and robust defense framework for DFL-IDS, which incorporates Distributed Differential Privacy (DDP) to protect data confidentiality and utilizes latent space representations (LSR) derived from neural networks to detect malicious updates in the decentralized model aggregation context. To eliminate single points of failure and enhance trust without a centralized aggregation server, PenTiDef employs a blockchain-based decentralized coordination mechanism that manages model aggregation, tracks update history, and supports trust enforcement through smart contracts. Experimental results on CIC-IDS2018 and Edge-IIoTSet demonstrate that PenTiDef consistently outperforms existing defenses (e.g., FLARE, FedCC) across various attack scenarios and data distributions. These findings highlight the potential of PenTiDef as a scalable and secure framework for deploying DFL-based IDS in adversarial environments. By leveraging privacy protection, malicious behavior detection in hidden data, and working without a central server, it provides a useful security solution against real-world attacks from untrust participants.
]]></content:encoded>
<pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>CityGuard: Graph-Aware Private Descriptors for Bias-Resilient Identity Search Across Urban Cameras</title>
<link>https://arxiv.org/abs/2602.18047</link>
<guid>https://arxiv.org/abs/2602.18047</guid>
<content:encoded><![CDATA[
arXiv:2602.18047v1 Announce Type: new 
Abstract: City-scale person re-identification across distributed cameras must handle severe appearance changes from viewpoint, occlusion, and domain shift while complying with data protection rules that prevent sharing raw imagery. We introduce CityGuard, a topology-aware transformer for privacy-preserving identity retrieval in decentralized surveillance. The framework integrates three components. A dispersion-adaptive metric learner adjusts instance-level margins according to feature spread, increasing intra-class compactness. Spatially conditioned attention injects coarse geometry, such as GPS or deployment floor plans, into graph-based self-attention to enable projectively consistent cross-view alignment using only coarse geometric priors without requiring survey-grade calibration. Differentially private embedding maps are coupled with compact approximate indexes to support secure and cost-efficient deployment. Together these designs produce descriptors robust to viewpoint variation, occlusion, and domain shifts, and they enable a tunable balance between privacy and utility under rigorous differential-privacy accounting. Experiments on Market-1501 and additional public benchmarks, complemented by database-scale retrieval studies, show consistent gains in retrieval precision and query throughput over strong baselines, confirming the practicality of the framework for privacy-critical urban identity matching.
]]></content:encoded>
<pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SeedFlood: A Step Toward Scalable Decentralized Training of LLMs</title>
<link>https://arxiv.org/abs/2602.18181</link>
<guid>https://arxiv.org/abs/2602.18181</guid>
<content:encoded><![CDATA[
arXiv:2602.18181v1 Announce Type: new 
Abstract: This work presents a new approach to decentralized training-SeedFlood-designed to scale for large models across complex network topologies and achieve global consensus with minimal communication overhead. Traditional gossip-based methods suffer from message communication costs that grow with model size, while information decay over network hops renders global consensus inefficient. SeedFlood departs from these practices by exploiting the seed-reconstructible structure of zeroth-order updates and effectively making the messages near-zero in size, allowing them to be flooded to every client in the network. This mechanism makes communication overhead negligible and independent of model size, removing the primary scalability bottleneck in decentralized training. Consequently, SeedFlood enables training in regimes previously considered impractical, such as billion-parameter models distributed across hundreds of clients. Our experiments on decentralized LLM fine-tuning demonstrate thatSeedFlood consistently outperforms gossip-based baselines in both generalization performance and communication efficiency, and even achieves results comparable to first-order methods in large scale settings.
]]></content:encoded>
<pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies</title>
<link>https://arxiv.org/abs/2602.18291</link>
<guid>https://arxiv.org/abs/2602.18291</guid>
<content:encoded><![CDATA[
arXiv:2602.18291v1 Announce Type: new 
Abstract: Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance. Diffusion-based generative models are well-positioned to meet this demand, having demonstrated remarkable expressiveness and multimodal representation in image generation and offline settings. Yet, their potential in online MARL remains largely under-explored. A major obstacle is that the intractable likelihoods of diffusion models impede entropy-based exploration and coordination. To tackle this challenge, we propose among the first \underline{O}nline off-policy \underline{MA}RL framework using \underline{D}iffusion policies (\textbf{OMAD}) to orchestrate coordination. Our key innovation is a relaxed policy objective that maximizes scaled joint entropy, facilitating effective exploration without relying on tractable likelihood. Complementing this, within the centralized training with decentralized execution (CTDE) paradigm, we employ a joint distributional value function to optimize decentralized diffusion policies. It leverages tractable entropy-augmented targets to guide the simultaneous updates of diffusion policies, thereby ensuring stable coordination. Extensive evaluations on MPE and MAMuJoCo establish our method as the new state-of-the-art across $10$ diverse tasks, demonstrating a remarkable $2.5\times$ to $5\times$ improvement in sample efficiency.
]]></content:encoded>
<pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Non-intrusive Decentralized Approach to Stabilizing IBR-dominated AC Microgrids</title>
<link>https://arxiv.org/abs/2310.09450</link>
<guid>https://arxiv.org/abs/2310.09450</guid>
<content:encoded><![CDATA[
arXiv:2310.09450v4 Announce Type: replace 
Abstract: This paper presents a non-intrusive, decentralized approach that stabilizes AC microgrids dominated by inverter-based resources (IBRs). By "non-intrusive" we mean that the approach does not require reprogramming IBRs' controllers to stabilize the microgrids. "Decentralized" is in the sense that the approach stabilizes the microgrids without communication among IBRs. Implementing the approach only requires very minimal information of IBR dynamics, i.e., the L2 gain of an IBR, and sharing such information with the non-IBR-manufacturer parties does not cause any concerns on intellectual property privacy. The approach allows for plug-and-play operation of IBRs, while maintaining microgrid stability. The proposed approach is tested by simulating 2-IBR and 10-IBR microgrids where lines and IBRs are modeled in the electromagnetic transient time scale. Simulations show that oscillations with increasing amplitudes may occur, when two stable microgrids are networked. Simulations also suggest that the proposed approach can mitigate such a system-level symptom.
]]></content:encoded>
<pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Walrus: An Efficient Decentralized Storage Network</title>
<link>https://arxiv.org/abs/2505.05370</link>
<guid>https://arxiv.org/abs/2505.05370</guid>
<content:encoded><![CDATA[
arXiv:2505.05370v3 Announce Type: replace 
Abstract: Decentralized storage systems face a fundamental trade-off between replication overhead, recovery efficiency, and security guarantees. Current approaches either rely on full replication, incurring substantial storage costs, or employ trivial erasure coding schemes that struggle with efficient recovery especially under high storage-node churn. We present Walrus, a novel decentralized blob storage system that addresses these limitations through multiple technical innovations. At the core of Walrus is RedStuff, a two-dimensional erasure coding protocol that achieves high security with only 4.5x replication factor, while enabling self-healing recovery that requires bandwidth proportional to only the lost data $(O(|blob|/n)$ versus $O(|blob|)$ in traditional systems). Crucially, RedStuff is the first protocol to support storage challenges in asynchronous networks, preventing adversaries from exploiting network delays to pass verification without actually storing data. Walrus also introduces a novel multi-stage epoch change protocol that efficiently handles storage node churn while maintaining uninterrupted availability during committee transitions. Our system incorporates authenticated data structures to defend against malicious clients and ensures data consistency throughout storage and retrieval processes. Experimental evaluation demonstrates that Walrus achieves practical performance at scale, making it suitable for a wide range of decentralized applications requiring high-integrity, available blob storage with reasonable overhead.
]]></content:encoded>
<pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>"My body is not your Porn": Identifying Trends of Harm and Oppression through a Sociotechnical Genealogy of Digital Sexual Violence in South Korea</title>
<link>https://arxiv.org/abs/2602.16853</link>
<guid>https://arxiv.org/abs/2602.16853</guid>
<content:encoded><![CDATA[
arXiv:2602.16853v1 Announce Type: new 
Abstract: Ever since the introduction of internet technologies in South Korea, digital sexual violence (DSV) has been a persistent and pervasive problem. Evolving alongside digital technologies, the severity and scale of violence have grown consistently, leading to widespread public concern. In this paper, we present four eras of image-based DSV in South Korea, spanning from the early internet era of the 1990s to the deepfake scandals in the mid-2020s. Drawing from media coverage, legal documents, and academic literature, we elucidate forms and characteristics of DSV cases in each era, tracing how entrenched misogyny is reconfigured and amplified through evolving technologies, alongside shifting legislative measures. Taking a genealogical approach to read prominent cases of different eras, our analysis identifies three constitutive and interconnected dimensions of DSV: (1) the homo-social fabrication of "obscenity", wherein victims' imagery becomes collectively framed as obscene through participatory practices in male-dominant networks; (2) the increasing imperceptibility of violence, as technologies foreclose victims' ability to perceive harm; and (3) the commercialization of abuse through decentralized economic infrastructures. We suggest future directions for CSCW research, and further reflect on the value of the genealogical method in enabling non-linear understanding of DSV as dynamically evolving sociotechnical configurations of harm.
]]></content:encoded>
<pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Lipschitz Bandits</title>
<link>https://arxiv.org/abs/2602.16965</link>
<guid>https://arxiv.org/abs/2602.16965</guid>
<content:encoded><![CDATA[
arXiv:2602.16965v1 Announce Type: new 
Abstract: We study the decentralized multi-player stochastic bandit problem over a continuous, Lipschitz-structured action space where hard collisions yield zero reward. Our objective is to design a communication-free policy that maximizes collective reward, with coordination costs that are independent of the time horizon $T$. We propose a modular protocol that first solves the multi-agent coordination problem -- identifying and seating players on distinct high-value regions via a novel maxima-directed search -- and then decouples the problem into $N$ independent single-player Lipschitz bandits. We establish a near-optimal regret bound of $\tilde{O}(T^{(d+1)/(d+2)})$ plus a $T$-independent coordination cost, matching the single-player rate. To our knowledge, this is the first framework providing such guarantees, and it extends to general distance-threshold collision models.
]]></content:encoded>
<pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Action-Graph Policies: Learning Action Co-dependencies in Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2602.17009</link>
<guid>https://arxiv.org/abs/2602.17009</guid>
<content:encoded><![CDATA[
arXiv:2602.17009v1 Announce Type: new 
Abstract: Coordinating actions is the most fundamental form of cooperation in multi-agent reinforcement learning (MARL). Successful decentralized decision-making often depends not only on good individual actions, but on selecting compatible actions across agents to synchronize behavior, avoid conflicts, and satisfy global constraints. In this paper, we propose Action Graph Policies (AGP), that model dependencies among agents' available action choices. It constructs, what we call, \textit{coordination contexts}, that enable agents to condition their decisions on global action dependencies. Theoretically, we show that AGPs induce a strictly more expressive joint policy compared to fully independent policies and can realize coordinated joint actions that are provably more optimal than greedy execution even from centralized value-decomposition methods. Empirically, we show that AGP achieves 80-95\% success on canonical coordination tasks with partial observability and anti-coordination penalties, where other MARL methods reach only 10-25\%. We further demonstrate that AGP consistently outperforms these baselines in diverse multi-agent environments.
]]></content:encoded>
<pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatio-temporal dual-stage hypergraph MARL for human-centric multimodal corridor traffic signal control</title>
<link>https://arxiv.org/abs/2602.17068</link>
<guid>https://arxiv.org/abs/2602.17068</guid>
<content:encoded><![CDATA[
arXiv:2602.17068v1 Announce Type: new 
Abstract: Human-centric traffic signal control in corridor networks must increasingly account for multimodal travelers, particularly high-occupancy public transportation, rather than focusing solely on vehicle-centric performance. This paper proposes STDSH-MARL (Spatio-Temporal Dual-Stage Hypergraph based Multi-Agent Reinforcement Learning), a scalable multi-agent deep reinforcement learning framework that follows a centralized training and decentralized execution paradigm. The proposed method captures spatio-temporal dependencies through a novel dual-stage hypergraph attention mechanism that models interactions across both spatial and temporal hyperedges. In addition, a hybrid discrete action space is introduced to jointly determine the next signal phase configuration and its corresponding green duration, enabling more adaptive signal timing decisions. Experiments conducted on a corridor network under five traffic scenarios demonstrate that STDSH-MARL consistently improves multimodal performance and provides clear benefits for public transportation priority. Compared with state-of-the-art baseline methods, the proposed approach achieves superior overall performance. Further ablation studies confirm the contribution of each component of STDSH-MARL, with temporal hyperedges identified as the most influential factor driving the observed performance gains.
]]></content:encoded>
<pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resource Allocation for STAR-RIS-enhanced Metaverse Systems with Augmented Reality</title>
<link>https://arxiv.org/abs/2602.17123</link>
<guid>https://arxiv.org/abs/2602.17123</guid>
<content:encoded><![CDATA[
arXiv:2602.17123v1 Announce Type: new 
Abstract: Augmented reality (AR)-enabled Metaverse is a promising technique to provide immersive service experience for mobile users. However, the limited network resources and unpredictable wireless propagation environments are key design bottlenecks of AR-enabled Metaverse systems. Therefore, this paper presents a resource management framework for simultaneously transmitting and reflecting RIS (STAR-RIS)-assisted AR-enabled Metaverse, where the STAR-RIS is configured to improve the communication efficiency between AR users and the Metaverse server located at the base station (BS). Moreover, we formulate a service latency minimization problem via jointly optimizing the computation resource allocation of the BS, coefficient matrix of the STAR-RIS, central processing unit (CPU) frequency and transmit power of the AR users. To tackle the non-convex problem, we utilize an approximate method to transform it to a tractable form, and decouple the multi-dimensional variables via the alternating optimization method. Particularly, the optimal coefficient matrix is obtained by a penalty function-based method with proved convergence, the CPU frequencies of AR users are derived as the closed-form solution, and the transmit power of AR users and computation resource allocation of the BS are obtained by the Lagrange duality method and convex optimization theory. Finally, simulation results demonstrates that the proposed method achieves remarkable latency reduction than several benchmark methods.
]]></content:encoded>
<pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Latent Space Alignment for Multi-user Semantic Communications</title>
<link>https://arxiv.org/abs/2602.17271</link>
<guid>https://arxiv.org/abs/2602.17271</guid>
<content:encoded><![CDATA[
arXiv:2602.17271v1 Announce Type: new 
Abstract: Semantic communication aims to convey meaning for effective task execution, but differing latent representations in AI-native devices can cause semantic mismatches that hinder mutual understanding. This paper introduces a novel approach to mitigating latent space misalignment in multi-agent AI- native semantic communications. In a downlink scenario, we consider an access point (AP) communicating with multiple users to accomplish a specific AI-driven task. Our method implements a protocol that shares a semantic pre-equalizer at the AP and local semantic equalizers at user devices, fostering mutual understanding and task-oriented communication while considering power and complexity constraints. To achieve this, we employ a federated optimization for the decentralized training of the semantic equalizers at the AP and user sides. Numerical results validate the proposed approach in goal-oriented semantic communication, revealing key trade-offs among accuracy, com- munication overhead, complexity, and the semantic proximity of AI-native communication devices.
]]></content:encoded>
<pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Herd Behavior in Decentralized Balancing Models: A Case Study in Belgium</title>
<link>https://arxiv.org/abs/2602.17352</link>
<guid>https://arxiv.org/abs/2602.17352</guid>
<content:encoded><![CDATA[
arXiv:2602.17352v1 Announce Type: new 
Abstract: In a decentralized balancing model, Balance Responsible Parties (BRPs) are encouraged by the Transmission System Operator (TSO) to deviate from their schedule to help the system restore balance, also referred to as implicit balancing. This could reduce balancing costs for the grid operator and lower the entry barrier for flexible assets compared to explicit balancing services. However, these implicit reactions may overshoot when their total capacity is high, potentially requiring more explicit activations. This study analyses the effect of increased participation in the decentralized balancing model in Belgium. To this end, we develop a market simulator that produces price signals on minute-level and simulate the implicit reactions for battery assets with different risk profiles. Besides the current price formula, we also study two potential candidates for the near-term presented by the TSO. A simulation study is conducted using Belgian market data for the year 2023. The findings indicate that, while having a significant positive effect on the balancing costs at first, the risk of overshoots can outweigh the potential benefits when the total capacity of the implicit reactions becomes too large. Furthermore, even when the balancing costs start to increase for the TSO, BRPs were still found to benefit from implicit balancing.
]]></content:encoded>
<pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributed Virtual Model Control for Scalable Human-Robot Collaboration in Shared Workspace</title>
<link>https://arxiv.org/abs/2602.17415</link>
<guid>https://arxiv.org/abs/2602.17415</guid>
<content:encoded><![CDATA[
arXiv:2602.17415v1 Announce Type: new 
Abstract: We present a decentralized, agent agnostic, and safety-aware control framework for human-robot collaboration based on Virtual Model Control (VMC). In our approach, both humans and robots are embedded in the same virtual-component-shaped workspace, where motion is the result of the interaction with virtual springs and dampers rather than explicit trajectory planning. A decentralized, force-based stall detector identifies deadlocks, which are resolved through negotiation. This reduces the probability of robots getting stuck in the block placement task from up to 61.2% to zero in our experiments. The framework scales without structural changes thanks to the distributed implementation: in experiments we demonstrate safe collaboration with up to two robots and two humans, and in simulation up to four robots, maintaining inter-agent separation at around 20 cm. Results show that the method shapes robot behavior intuitively by adjusting control parameters and achieves deadlock-free operation across team sizes in all tested scenarios.
]]></content:encoded>
<pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guarding the Middle: Protecting Intermediate Representations in Federated Split Learning</title>
<link>https://arxiv.org/abs/2602.17614</link>
<guid>https://arxiv.org/abs/2602.17614</guid>
<content:encoded><![CDATA[
arXiv:2602.17614v1 Announce Type: new 
Abstract: Big data scenarios, where massive, heterogeneous datasets are distributed across clients, demand scalable, privacy-preserving learning methods. Federated learning (FL) enables decentralized training of machine learning (ML) models across clients without data centralization. Decentralized training, however, introduces a computational burden on client devices. U-shaped federated split learning (UFSL) offloads a fraction of the client computation to the server while keeping both data and labels on the clients' side. However, the intermediate representations (i.e., smashed data) shared by clients with the server are prone to exposing clients' private data. To reduce exposure of client data through intermediate data representations, this work proposes k-anonymous differentially private UFSL (KD-UFSL), which leverages privacy-enhancing techniques such as microaggregation and differential privacy to minimize data leakage from the smashed data transferred to the server. We first demonstrate that an adversary can access private client data from intermediate representations via a data-reconstruction attack, and then present a privacy-enhancing solution, KD-UFSL, to mitigate this risk. Our experiments indicate that, alongside increasing the mean squared error between the actual and reconstructed images by up to 50% in some cases, KD-UFSL also decreases the structural similarity between them by up to 40% on four benchmarking datasets. More importantly, KD-UFSL improves privacy while preserving the utility of the global model. This highlights its suitability for large-scale big data applications where privacy and utility must be balanced.
]]></content:encoded>
<pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Decentralized Composite Optimization via Three-Operator Splitting</title>
<link>https://arxiv.org/abs/2602.17545</link>
<guid>https://arxiv.org/abs/2602.17545</guid>
<content:encoded><![CDATA[
arXiv:2602.17545v1 Announce Type: cross 
Abstract: The paper studies decentralized optimization over networks, where agents minimize a sum of {\it locally} smooth (strongly) convex losses and plus a nonsmooth convex extended value term. We propose decentralized methods wherein agents {\it adaptively} adjust their stepsize via local backtracking procedures coupled with lightweight min-consensus protocols. Our design stems from a three-operator splitting factorization applied to an equivalent reformulation of the problem. The reformulation is endowed with a new BCV preconditioning metric (Bertsekas-O'Connor-Vandenberghe), which enables efficient decentralized implementation and local stepsize adjustments. We establish robust convergence guarantees. Under mere convexity, the proposed methods converge with a sublinear rate. Under strong convexity of the sum-function, and assuming the nonsmooth component is partly smooth, we further prove linear convergence. Numerical experiments corroborate the theory and highlight the effectiveness of the proposed adaptive stepsize strategy.
]]></content:encoded>
<pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harnessing Implicit Cooperation: A Multi-Agent Reinforcement Learning Approach Towards Decentralized Local Energy Markets</title>
<link>https://arxiv.org/abs/2602.16062</link>
<guid>https://arxiv.org/abs/2602.16062</guid>
<content:encoded><![CDATA[
arXiv:2602.16062v1 Announce Type: new 
Abstract: This paper proposes implicit cooperation, a framework enabling decentralized agents to approximate optimal coordination in local energy markets without explicit peer-to-peer communication. We formulate the problem as a decentralized partially observable Markov decision problem that is solved through a multi-agent reinforcement learning task in which agents use stigmergic signals (key performance indicators at the system level) to infer and react to global states. Through a 3x3 factorial design on an IEEE 34-node topology, we evaluated three training paradigms (CTCE, CTDE, DTDE) and three algorithms (PPO, APPO, SAC). Results identify APPO-DTDE as the optimal configuration, achieving a coordination score of 91.7% relative to the theoretical centralized benchmark (CTCE). However, a critical trade-off emerges between efficiency and stability: while the centralized benchmark maximizes allocative efficiency with a peer-to-peer trade ratio of 0.6, the fully decentralized approach (DTDE) demonstrates superior physical stability. Specifically, DTDE reduces the variance of grid balance by 31% compared to hybrid architectures, establishing a highly predictable, import-biased load profile that simplifies grid regulation. Furthermore, topological analysis reveals emergent spatial clustering, where decentralized agents self-organize into stable trading communities to minimize congestion penalties. While SAC excelled in hybrid settings, it failed in decentralized environments due to entropy-driven instability. This research proves that stigmergic signaling provides sufficient context for complex grid coordination, offering a robust, privacy-preserving alternative to expensive centralized communication infrastructure.
]]></content:encoded>
<pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>MARLEM: A Multi-Agent Reinforcement Learning Simulation Framework for Implicit Cooperation in Decentralized Local Energy Markets</title>
<link>https://arxiv.org/abs/2602.16063</link>
<guid>https://arxiv.org/abs/2602.16063</guid>
<content:encoded><![CDATA[
arXiv:2602.16063v1 Announce Type: new 
Abstract: This paper introduces a novel, open-source MARL simulation framework for studying implicit cooperation in LEMs, modeled as a decentralized partially observable Markov decision process and implemented as a Gymnasium environment for MARL. Our framework features a modular market platform with plug-and-play clearing mechanisms, physically constrained agent models (including battery storage), a realistic grid network, and a comprehensive analytics suite to evaluate emergent coordination. The main contribution is a novel method to foster implicit cooperation, where agents' observations and rewards are enhanced with system-level key performance indicators to enable them to independently learn strategies that benefit the entire system and aim for collectively beneficial outcomes without explicit communication. Through representative case studies (available in a dedicated GitHub repository in https://github.com/salazarna/marlem, we show the framework's ability to analyze how different market configurations (such as varying storage deployment) impact system performance. This illustrates its potential to facilitate emergent coordination, improve market efficiency, and strengthen grid stability. The proposed simulation framework is a flexible, extensible, and reproducible tool for researchers and practitioners to design, test, and validate strategies for future intelligent, decentralized energy systems.
]]></content:encoded>
<pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Managing Credible Anonymous Identities in Web 3.0 Services: A Scalable On-Chain Admission Framework with Recursive Proof Aggregation</title>
<link>https://arxiv.org/abs/2602.16130</link>
<guid>https://arxiv.org/abs/2602.16130</guid>
<content:encoded><![CDATA[
arXiv:2602.16130v1 Announce Type: new 
Abstract: Open Web 3.0 platforms increasingly operate as \emph{service ecosystems} (e.g., DeFi, DAOs, and decentralized social applications) where \emph{admission control} and \emph{account provisioning} must be delivered as an always-on service under bursty demand. Service operators face a fundamental tension: enforcing Sybil resistance (one-person-one-account) while preserving user privacy, yet keeping on-chain verification cost and admission latency predictable at scale. Existing credential-based ZK admission approaches typically require per-request on-chain verification, making the provisioning cost grow with the number of concurrent joiners. We present \textbf{ZK-AMS}, a scalable admission and provisioning layer that bridges real-world \emph{Personhood Credentials} to anonymous on-chain service accounts. ZK-AMS combines (i) zero-knowledge credential validation, (ii) a \emph{permissionless} batch submitter model, and (iii) a decentralized, privacy-preserving folding pipeline that uses Nova-style recursive aggregation together with multi-key homomorphic encryption, enabling batch settlement with \emph{constant} on-chain verification per batch. We implement ZK-AMS end-to-end on an Ethereum testbed and evaluate admission throughput, end-to-end latency, and gas consumption. Results show stable verification cost across batch sizes and substantially improved admission efficiency over non-recursive baselines, providing a practical and cost-predictable admission service for large-scale Web 3.0 communities.
]]></content:encoded>
<pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-agent cooperation through in-context co-player inference</title>
<link>https://arxiv.org/abs/2602.16301</link>
<guid>https://arxiv.org/abs/2602.16301</guid>
<content:encoded><![CDATA[
arXiv:2602.16301v1 Announce Type: new 
Abstract: Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between "learning-aware" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between "naive learners" updating on fast timescales and "meta-learners" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors.
]]></content:encoded>
<pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Markerless Robot Detection and 6D Pose Estimation for Multi-Agent SLAM</title>
<link>https://arxiv.org/abs/2602.16308</link>
<guid>https://arxiv.org/abs/2602.16308</guid>
<content:encoded><![CDATA[
arXiv:2602.16308v1 Announce Type: new 
Abstract: The capability of multi-robot SLAM approaches to merge localization history and maps from different observers is often challenged by the difficulty in establishing data association. Loop closure detection between perceptual inputs of different robotic agents is easily compromised in the context of perceptual aliasing, or when perspectives differ significantly. For this reason, direct mutual observation among robots is a powerful way to connect partial SLAM graphs, but often relies on the presence of calibrated arrays of fiducial markers (e.g., AprilTag arrays), which severely limits the range of observations and frequently fails under sharp lighting conditions, e.g., reflections or overexposure. In this work, we propose a novel solution to this problem leveraging recent advances in Deep-Learning-based 6D pose estimation. We feature markerless pose estimation as part of a decentralized multi-robot SLAM system and demonstrate the benefit to the relative localization accuracy among the robotic team. The solution is validated experimentally on data recorded in a test field campaign on a planetary analogous environment.
]]></content:encoded>
<pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>push0: Scalable and Fault-Tolerant Orchestration for Zero-Knowledge Proof Generation</title>
<link>https://arxiv.org/abs/2602.16338</link>
<guid>https://arxiv.org/abs/2602.16338</guid>
<content:encoded><![CDATA[
arXiv:2602.16338v1 Announce Type: new 
Abstract: Zero-knowledge proof generation imposes stringent timing and reliability constraints on blockchain systems. For ZK-rollups, delayed proofs cause finality lag and economic loss; for Ethereum's emerging L1 zkEVM, proofs must complete within the 12-second slot window to enable stateless validation. The Ethereum Foundation's Ethproofs initiative coordinates multiple independent zkVMs across proving clusters to achieve real-time block proving, yet no principled orchestration framework addresses the joint challenges of (i) strict head-of-chain ordering, (ii) sub-slot latency bounds, (iii) fault-tolerant task reassignment, and (iv) prover-agnostic workflow composition. We present push0, a cloud-native proof orchestration system that decouples prover binaries from scheduling infrastructure. push0 employs an event-driven dispatcher--collector architecture over persistent priority queues, enforcing block-sequential proving while exploiting intra-block parallelism. We formalize requirements drawn from production ZK-rollup operations and the Ethereum real-time proving specification, then demonstrate via production Kubernetes cluster experiments that push0 achieves 5 ms median orchestration overhead with 99--100% scaling efficiency at 32 dispatchers for realistic workloads--overhead negligible (less than 0.1%) relative to typical proof computation times of 7+ seconds. Controlled Docker experiments validate these results, showing comparable performance (3--10 ms P50) when network variance is eliminated. Production deployment on the Zircuit zkrollup (14+ million mainnet blocks since March 2025) provides ecological validity for these controlled experiments. Our design enables seamless integration of heterogeneous zkVMs, supports automatic task recovery via message persistence, and provides the scheduling primitives necessary for both centralized rollup operators and decentralized multi-prover networks.
]]></content:encoded>
<pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Meta-Advisor for UAV Fleet Trajectory Design in Vehicular Networks</title>
<link>https://arxiv.org/abs/2602.16345</link>
<guid>https://arxiv.org/abs/2602.16345</guid>
<content:encoded><![CDATA[
arXiv:2602.16345v1 Announce Type: new 
Abstract: Future vehicular networks require continuous connectivity to serve highly mobile users in urban environments. To mitigate the coverage limitations of fixed terrestrial macro base stations (MBS) under non line-of-sight (NLoS) conditions, fleets of unmanned aerial base stations (UABSs) can be deployed as aerial base stations, dynamically repositioning to track vehicular users and traffic hotspots in coordination with the terrestrial network. This paper addresses cooperative multi-agent trajectory design under different service areas and takeoff configurations, where rapid and safe adaptation across scenarios is essential. We formulate the problem as a multi-task decentralized partially observable Markov decision process and solve it using centralized training and decentralized execution with double dueling deep Q-network (3DQN), enabling online training for real-world deployments. However, efficient exploration remains a bottleneck, with conventional strategies like $\epsilon$-greedy requiring careful tuning. To overcome this, we propose the multi-agent meta-advisor with advisor override (MAMO). This framework guides agent exploration through a meta-policy learned jointly across tasks. It uses a dynamic override mechanism that allows agents to reject misaligned guidance when the advisor fails to generalize to a specific scenario. Simulation results across three realistic urban scenarios and multiple takeoff configurations show that MAMO achieves faster convergence and higher returns than tuned $\epsilon$-greedy baselines, outperforming both an advisor-only ablation and a single generalized policy. Finally, we demonstrate that the learned UABS fleet significantly improves network performance compared to deployments without aerial support.
]]></content:encoded>
<pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SRFed: Mitigating Poisoning Attacks in Privacy-Preserving Federated Learning with Heterogeneous Data</title>
<link>https://arxiv.org/abs/2602.16480</link>
<guid>https://arxiv.org/abs/2602.16480</guid>
<content:encoded><![CDATA[
arXiv:2602.16480v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative model training without exposing clients' private data, and has been widely adopted in privacy-sensitive scenarios. However, FL faces two critical security threats: curious servers that may launch inference attacks to reconstruct clients' private data, and compromised clients that can launch poisoning attacks to disrupt model aggregation. Existing solutions mitigate these attacks by combining mainstream privacy-preserving techniques with defensive aggregation strategies. However, they either incur high computation and communication overhead or perform poorly under non-independent and identically distributed (Non-IID) data settings. To tackle these challenges, we propose SRFed, an efficient Byzantine-robust and privacy-preserving FL framework for Non-IID scenarios. First, we design a decentralized efficient functional encryption (DEFE) scheme to support efficient model encryption and non-interactive decryption. DEFE also eliminates third-party reliance and defends against server-side inference attacks. Second, we develop a privacy-preserving defensive model aggregation mechanism based on DEFE. This mechanism filters poisonous models under Non-IID data by layer-wise projection and clustering-based analysis. Theoretical analysis and extensive experiments show that SRFed outperforms state-of-the-art baselines in privacy protection, Byzantine robustness, and efficiency.
]]></content:encoded>
<pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized and Fully Onboard: Range-Aided Cooperative Localization and Navigation on Micro Aerial Vehicles</title>
<link>https://arxiv.org/abs/2602.16594</link>
<guid>https://arxiv.org/abs/2602.16594</guid>
<content:encoded><![CDATA[
arXiv:2602.16594v1 Announce Type: new 
Abstract: Controlling a team of robots in a coordinated manner is challenging because centralized approaches (where all computation is performed on a central machine) scale poorly, and globally referenced external localization systems may not always be available. In this work, we consider the problem of range-aided decentralized localization and formation control. In such a setting, each robot estimates its relative pose by combining data only from onboard odometry sensors and distance measurements to other robots in the team. Additionally, each robot calculates the control inputs necessary to collaboratively navigate an environment to accomplish a specific task, for example, moving in a desired formation while monitoring an area. We present a block coordinate descent approach to localization that does not require strict coordination between the robots. We present a novel formulation for formation control as inference on factor graphs that takes into account the state estimation uncertainty and can be solved efficiently. Our approach to range-aided localization and formation-based navigation is completely decentralized, does not require specialized trajectories to maintain formation, and achieves decimeter-level positioning and formation control accuracy. We demonstrate our approach through multiple real experiments involving formation flights in diverse indoor and outdoor environments.
]]></content:encoded>
<pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consensus Based Task Allocation for Angles-Only Local Catalog Maintenance of Satellite Systems</title>
<link>https://arxiv.org/abs/2602.16678</link>
<guid>https://arxiv.org/abs/2602.16678</guid>
<content:encoded><![CDATA[
arXiv:2602.16678v1 Announce Type: new 
Abstract: In order for close proximity satellites to safely perform their missions, the relative states of all satellites and pieces of debris must be well understood. This presents a problem for ground based tracking and orbit determination since it may not be practical to achieve the required accuracy. Using space-based sensors allows for more accurate relative state estimates, especially if multiple satellites are allowed to communicate. Of interest to this work is the case where several communicating satellites each need to maintain a local catalog of communicating and non-communicating objects using angles-only limited field of view (FOV) measurements. However, this introduces the problem of efficiently scheduling and coordinating observations among the agents. This paper presents a decentralized task allocation algorithm to address this problem and quantifies its performance in terms of fuel usage and overall catalog uncertainty via numerical simulation. It was found that the new method significantly outperforms the uncertainty-fuel Pareto frontier formed by current approaches.
]]></content:encoded>
<pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Strategic Hiring under Algorithmic Monoculture</title>
<link>https://arxiv.org/abs/2502.20063</link>
<guid>https://arxiv.org/abs/2502.20063</guid>
<content:encoded><![CDATA[
arXiv:2502.20063v2 Announce Type: replace 
Abstract: We study the impact of strategic behavior in labor markets characterized by algorithmic monoculture, where firms compete for a shared pool of applicants using a common algorithmic evaluation. In this setting, "naive" hiring strategies lead to severe congestion, as firms collectively target the same high-scoring candidates. We model this competition as a game with capacity-constrained firms and fully characterize the set of Nash equilibria. We demonstrate that equilibrium strategies, which naturally diversify firms' interview targets, significantly outperform naive selection, increasing social welfare for both firms and applicants. Specifically, the Price of Naive Selection (welfare gain from strategy) grows linearly with the number of firms, while the Price of Anarchy (efficiency loss from decentralization) approaches 1, implying that the decentralized equilibrium is nearly socially optimal. Finally, we analyze convergence, and we show that a simple sequential best-response process converges to the desired equilibrium. However, we show that firms generally cannot infer the key input needed to compute best responses, namely congestion for specific candidates, from their own historical data alone. Consequently, to realize the welfare gains of strategic differentiation, algorithmic platforms must explicitly reveal congestion information to participating firms.
]]></content:encoded>
<pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedEFC: Federated Learning Using Enhanced Forward Correction Against Noisy Labels</title>
<link>https://arxiv.org/abs/2504.05615</link>
<guid>https://arxiv.org/abs/2504.05615</guid>
<content:encoded><![CDATA[
arXiv:2504.05615v3 Announce Type: replace 
Abstract: Federated Learning (FL) is a powerful framework for privacy-preserving distributed learning. It enables multiple clients to collaboratively train a global model without sharing raw data. However, handling noisy labels in FL remains a major challenge due to heterogeneous data distributions and communication constraints, which can severely degrade model performance. To address this issue, we propose FedEFC, a novel method designed to tackle the impact of noisy labels in FL. FedEFC mitigates this issue through two key techniques: (1) prestopping, which prevents overfitting to mislabeled data by dynamically halting training at an optimal point, and (2) loss correction, which adjusts model updates to account for label noise. In particular, we develop an effective loss correction tailored to the unique challenges of FL, including data heterogeneity and decentralized training. Furthermore, we provide a theoretical analysis, leveraging the composite proper loss property, to demonstrate that the FL objective function under noisy label distributions can be aligned with the clean label distribution. Extensive experimental results validate the effectiveness of our approach, showing that it consistently outperforms existing FL techniques in mitigating the impact of noisy labels, particularly under heterogeneous data settings (e.g., achieving up to 41.64% relative performance improvement over the existing loss correction method).
]]></content:encoded>
<pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Protocol as Poetry: A Case Study of Pak's Smart Contract-Based Protocol Art</title>
<link>https://arxiv.org/abs/2505.12393</link>
<guid>https://arxiv.org/abs/2505.12393</guid>
<content:encoded><![CDATA[
arXiv:2505.12393v2 Announce Type: replace 
Abstract: Protocol art has recently proliferated through blockchain-based smart contracts, building on a century-long lineage of conceptual, participatory, interactive, systematic, algorithmic, and generative art practices. Few studies have examined the characteristics and appreciation of this emerging art form. To address this gap, this paper presents an annotated portfolio analysis of protocol artworks by Pak, a pioneering and influential pseudonymous artist who treats smart contracts as medium and collective participation through protocol as message. Tracing the evolution from early open-edition releases of The Fungible (2021) and the dynamic mechanics of Merge (2021) to the soul-bound messaging of Censored (2022) and the reflective absence of Not Found (2023), we examine how Pak choreographs distributed agency across collectors and autonomous code, demonstrating how programmable protocols become a social fabric in artistic meaning-making. Through thematic analysis of Pak's works, we identify seven core characteristics distinguishing protocol art from other art forms: (1) system-centric rather than object-centric composition, (2) autonomous governance enabling open-ended control, (3) distributed agency and communal authorship, (4) temporal dynamism and lifecycle aesthetics, (5) economy-driven engagement, (6) poetic message embedded in interaction rituals, and (7) interoperability enabling composability for emergent complexity. We then discuss how these features set protocol art apart from adjacent movements such as conceptual, generative, participatory, interactive, and performance art. By analyzing principles grounded in Pak's practice, we contribute to the emerging literature on protocol art (or "protocolism") and offer design implications for future artists exploring this evolving form.
]]></content:encoded>
<pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Context Sharing: A Unified Agent Communication Protocol (ACP) for Secure, Federated, and Autonomous Agent-to-Agent (A2A) Orchestration</title>
<link>https://arxiv.org/abs/2602.15055</link>
<guid>https://arxiv.org/abs/2602.15055</guid>
<content:encoded><![CDATA[
arXiv:2602.15055v1 Announce Type: new 
Abstract: In the artificial intelligence space, as we transition from isolated large language models to autonomous agents capable of complex reasoning and tool use. While foundational architectures and local context management protocols have been established, the challenge of cross-platform, decentralized, and secure interaction remains a significant barrier to the realization of a truly Agentic Web. Building upon the foundations of AI agent architectures and the Model Context Protocol (MCP) for multi-agent coordination, this paper introduces the Agent Communication Protocol (ACP). ACP provides a standardized framework for Agent-to-Agent (AA) interaction, enabling heterogeneous agents to discover, negotiate, and execute collaborative workflows across disparate environments. We propose a federated orchestration model that integrates decentralized identity verification, semantic intent mapping, and automated service-level agreements. Our evaluation demonstrates that ACP reduces inter-agent communication latency by % while maintaining a zero-trust security posture. This work represents a critical advancement toward a scalable and interoperable ecosystem of autonomous digital entities
]]></content:encoded>
<pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting Layer-Specific Vulnerabilities to Backdoor Attack in Federated Learning</title>
<link>https://arxiv.org/abs/2602.15161</link>
<guid>https://arxiv.org/abs/2602.15161</guid>
<content:encoded><![CDATA[
arXiv:2602.15161v1 Announce Type: new 
Abstract: Federated learning (FL) enables distributed model training across edge devices while preserving data locality. This decentralized approach has emerged as a promising solution for collaborative learning on sensitive user data, effectively addressing the longstanding privacy concerns inherent in centralized systems. However, the decentralized nature of FL exposes new security vulnerabilities, especially backdoor attacks that threaten model integrity. To investigate this critical concern, this paper presents the Layer Smoothing Attack (LSA), a novel backdoor attack that exploits layer-specific vulnerabilities in neural networks. First, a Layer Substitution Analysis methodology systematically identifies backdoor-critical (BC) layers that contribute most significantly to backdoor success. Subsequently, LSA strategically manipulates these BC layers to inject persistent backdoors while remaining undetected by state-of-the-art defense mechanisms. Extensive experiments across diverse model architectures and datasets demonstrate that LSA achieves a remarkably backdoor success rate of up to 97% while maintaining high model accuracy on the primary task, consistently bypassing modern FL defenses. These findings uncover fundamental vulnerabilities in current FL security frameworks, demonstrating that future defenses must incorporate layer-aware detection and mitigation strategies.
]]></content:encoded>
<pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Noncooperative Coordination for Decentralized Air Traffic Management</title>
<link>https://arxiv.org/abs/2602.15333</link>
<guid>https://arxiv.org/abs/2602.15333</guid>
<content:encoded><![CDATA[
arXiv:2602.15333v1 Announce Type: new 
Abstract: Decentralized air traffic management requires coordination among self-interested stakeholders operating under shared safety and capacity constraints, where conventional centralized or implicitly cooperative models do not adequately capture this setting. We develop a unified perspective on noncooperative coordination, in which system-level outcomes emerge by designing incentives and assigning signals that reshape individual optimality rather than imposing cooperation or enforcement. We advance this framework along three directions: scalable equilibrium engineering via reduced-rank and uncertainty-aware correlated equilibria, decentralized mechanism design for equilibrium selection without enforcement, and structured noncooperative dynamics with convergence guarantees. Beyond these technical contributions, we discuss core design principles that govern incentive-compatible coordination in decentralized systems. Together, these results establish a foundation for scalable, robust coordination in safety-critical air traffic systems.
]]></content:encoded>
<pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Orchestration-Free Customer Service Automation: A Privacy-Preserving and Flowchart-Guided Framework</title>
<link>https://arxiv.org/abs/2602.15377</link>
<guid>https://arxiv.org/abs/2602.15377</guid>
<content:encoded><![CDATA[
arXiv:2602.15377v1 Announce Type: new 
Abstract: Customer service automation has seen growing demand within digital transformation. Existing approaches either rely on modular system designs with extensive agent orchestration or employ over-simplified instruction schemas, providing limited guidance and poor generalizability. This paper introduces an orchestration-free framework using Task-Oriented Flowcharts (TOFs) to enable end-to-end automation without manual intervention. We first define the components and evaluation metrics for TOFs, then formalize a cost-efficient flowchart construction algorithm to abstract procedural knowledge from service dialogues. We emphasize local deployment of small language models and propose decentralized distillation with flowcharts to mitigate data scarcity and privacy issues in model training. Extensive experiments validate the effectiveness in various service tasks, with superior quantitative and application performance compared to strong baselines and market products. By releasing a web-based system demonstration with case studies, we aim to promote streamlined creation of future service automation.
]]></content:encoded>
<pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>MEV in Binance Builder</title>
<link>https://arxiv.org/abs/2602.15395</link>
<guid>https://arxiv.org/abs/2602.15395</guid>
<content:encoded><![CDATA[
arXiv:2602.15395v1 Announce Type: new 
Abstract: We study the builder-driven MEV arbitrage on BNB Smart Chain (BSC). BSC's Proposer--Builder Separation (PBS) adopts a leaner design: only whitelisted builders can participate, blocks are produced at shorter intervals, and private order flow bypasses the public mempool. These features have long raised community concerns over centralization, which we empirically confirm by tracing arbitrage activity of the two dominant builders from May to November 2025. Within months, 48Club and Blockrazor produced over 96\% of blocks and captured about 92\% of MEV profits.
  We find that profits concentrate in short, low-hop arbitrage routes over wrapped tokens and stablecoins, and that block construction rapidly converges toward monopoly. Beyond concentration alone, our analysis reveals a structural source of inequality: BSC's short block interval and whitelisted PBS collapse the contestable window for MEV competition, amplifying latency advantages and excluding slower builders and searchers. MEV extraction on BSC is not only more centralized than on Ethereum, but also structurally more vulnerable to censorship and weakened fairness.
]]></content:encoded>
<pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Backdoor Threat in Federated Instruction Tuning from a Signal Aggregation Perspective</title>
<link>https://arxiv.org/abs/2602.15671</link>
<guid>https://arxiv.org/abs/2602.15671</guid>
<content:encoded><![CDATA[
arXiv:2602.15671v1 Announce Type: new 
Abstract: Federated learning security research has predominantly focused on backdoor threats from a minority of malicious clients that intentionally corrupt model updates. This paper challenges this paradigm by investigating a more pervasive and insidious threat: \textit{backdoor vulnerabilities from low-concentration poisoned data distributed across the datasets of benign clients.} This scenario is increasingly common in federated instruction tuning for language models, which often rely on unverified third-party and crowd-sourced data. We analyze two forms of backdoor data through real cases: 1) \textit{natural trigger (inherent features as implicit triggers)}; 2) \textit{adversary-injected trigger}. To analyze this threat, we model the backdoor implantation process from signal aggregation, proposing the Backdoor Signal-to-Noise Ratio to quantify the dynamics of the distributed backdoor signal. Extensive experiments reveal the severity of this threat: With just less than 10\% of training data poisoned and distributed across clients, the attack success rate exceeds 85\%, while the primary task performance remains largely intact. Critically, we demonstrate that state-of-the-art backdoor defenses, designed for attacks from malicious clients, are fundamentally ineffective against this threat. Our findings highlight an urgent need for new defense mechanisms tailored to the realities of modern, decentralized data ecosystems.
]]></content:encoded>
<pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Preserving and Secure Spectrum Sharing for Database-Driven Cognitive Radio Networks</title>
<link>https://arxiv.org/abs/2602.15705</link>
<guid>https://arxiv.org/abs/2602.15705</guid>
<content:encoded><![CDATA[
arXiv:2602.15705v1 Announce Type: new 
Abstract: Database-driven cognitive radio networks (DB-CRNs) enable dynamic spectrum sharing through geolocation databases but introduce critical security and privacy challenges, including mandatory location disclosure, susceptibility to location spoofing, and denial-of-service (DoS) attacks on centralized services. Existing approaches address these issues in isolation and lack a unified, regulation-compliant solution under realistic adversarial conditions. In this work, we present a unified security framework for DB-CRNs that simultaneously provides location privacy, user anonymity, verifiable location, and DoS resilience. Our framework, denoted as SLAPX, enables privacy-preserving spectrum queries using delegatable anonymous credentials, supports adaptive location verification without revealing precise user location, and mitigates DoS attacks through verifiable delay functions (VDFs) combined with RLRS-based rate limiting. Extensive cryptographic benchmarking and network simulations demonstrate that SLAPX achieves significantly lower latency and communication overhead than existing solutions while effectively resisting location spoofing and DoS attacks. These results show that SLAPX is practical and well-suited for secure next-generation DB-CRN deployments.
]]></content:encoded>
<pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Few to Many Faults: Optimal Adaptive Byzantine Agreement</title>
<link>https://arxiv.org/abs/2505.19989</link>
<guid>https://arxiv.org/abs/2505.19989</guid>
<content:encoded><![CDATA[
arXiv:2505.19989v2 Announce Type: replace 
Abstract: Achieving agreement among distributed parties is a fundamental task in modern systems, underpinning applications such as consensus in blockchains, coordination in cloud infrastructure, and fault tolerance in critical services. However, this task can be intensive, often requiring a large number of messages to be exchanged as well as many rounds of communication, especially in the presence of Byzantine faults. This makes efficiency a central challenge in the design of practical agreement protocols.
  In this paper, we study the problem of Binary Agreement and give protocols that are simultaneously optimal in both message and round complexity, parameterized by the actual number of Byzantine faults. In contrast to previous works, we demonstrate that optimal message complexity can be achieved without sacrificing latency. Concretely, for a system of $n$ parties tolerating up to $t$ Byzantine faults, out of which only $f \leq t$ are actually faulty, we give the following results:
  When $t = \Omega(n)$, in the synchronous (resp. partially synchronous) setting, with optimal resiliency $t < n/2$ (resp. $t < n/3$), we describe a deterministic protocol with optimal communication complexity $O(n \cdot (f+1))$ and optimal round complexity $O(f + 1)$.
  Building upon this previous result, when $t = o(n)$, for both the synchronous and partially synchronous setting, we describe a deterministic protocol with near-optimal communication complexity $\widetilde{O}(n + t\cdot f)$ and near-optimal round complexity $\widetilde{O}(f+1)$. Our approach relies on a novel use of dispersers to efficiently disseminate a value.
  For the asynchronous setting, we show a $\Omega(n + t^2)$ lower bound in expectation and provide a randomized protocol with near-optimal $\widetilde{O}(n + t^2)$ communication complexity and $O(1)$ round complexity in expectation.
]]></content:encoded>
<pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedX: Explanation-Guided Pruning for Communication-Efficient Federated Learning in Remote Sensing</title>
<link>https://arxiv.org/abs/2508.06256</link>
<guid>https://arxiv.org/abs/2508.06256</guid>
<content:encoded><![CDATA[
arXiv:2508.06256v2 Announce Type: replace 
Abstract: Federated learning (FL) enables the collaborative training of deep neural networks across decentralized data archives (i.e., clients), where each client stores data locally and only shares model updates with a central server. This makes FL a suitable learning paradigm for remote sensing (RS) image classification tasks, where data centralization may be restricted due to legal and privacy constraints. However, a key challenge in applying FL to RS tasks is the communication overhead caused by the frequent exchange of large model updates between clients and the central server. To address this issue, in this paper we propose a novel strategy (denoted as FedX) that uses explanation-guided pruning to reduce communication overhead by minimizing the size of the transmitted models without compromising performance. FedX leverages backpropagation-based explanation methods to estimate the task-specific importance of model components and prunes the least relevant ones at the central server. The resulting sparse global model is then sent to clients, substantially reducing communication overhead. We evaluate FedX on multi-label scene classification using the BigEarthNet-S2 dataset and single-label scene classification using the EuroSAT dataset. Experimental results show the success of FedX in significantly reducing the number of shared model parameters while enhancing the generalization capability of the global model, compared to both unpruned model and state-of-the-art pruning methods. The code of FedX will be available at https://git.tu-berlin.de/rsim/FedX.
]]></content:encoded>
<pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs</title>
<link>https://arxiv.org/abs/2508.06601</link>
<guid>https://arxiv.org/abs/2508.06601</guid>
<content:encoded><![CDATA[
arXiv:2508.06601v2 Announce Type: replace 
Abstract: Open-weight AI systems offer unique benefits, including enhanced transparency, open research, and decentralized access. However, they are vulnerable to tampering attacks which can efficiently elicit harmful behaviors by modifying weights or activations. Currently, there is not yet a robust science of open-weight model risk management. Existing safety fine-tuning methods and other post-training techniques have struggled to make LLMs resistant to more than a few dozen steps of adversarial fine-tuning. In this paper, we investigate whether filtering text about dual-use topics from training data can prevent unwanted capabilities and serve as a more tamper-resistant safeguard. We introduce a multi-stage pipeline for scalable data filtering and show that it offers a tractable and effective method for minimizing biothreat proxy knowledge in LLMs. We pretrain multiple 6.9B-parameter models from scratch and find that they exhibit substantial resistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M tokens of biothreat-related text -- outperforming existing post-training baselines by over an order of magnitude -- with no observed degradation to unrelated capabilities. However, while filtered models lack internalized dangerous knowledge, we find that they can still leverage such information when it is provided in context (e.g., via search tool augmentation), demonstrating a need for a defense-in-depth approach. Overall, these findings help to establish pretraining data curation as a promising layer of defense for open-weight AI systems.
]]></content:encoded>
<pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>MemeTrans: A Dataset for Detecting High-Risk Memecoin Launches on Solana</title>
<link>https://arxiv.org/abs/2602.13480</link>
<guid>https://arxiv.org/abs/2602.13480</guid>
<content:encoded><![CDATA[
arXiv:2602.13480v1 Announce Type: new 
Abstract: Launchpads have become the dominant mechanism for issuing memecoins on blockchains due to their fully automated, no-code creation process. This new issuance paradigm has led to a surge in high-risk token launches, causing substantial financial losses for unsuspecting buyers. In this paper, we introduce MemeTrans, the first dataset for studying and detecting high-risk memecoin launches on Solana. MemeTrans covers over 40k memecoin launches that successfully migrated to the public Decentralized Exchange (DEX), with over 30 million transactions during the initial sale on launchpad and 180 million transactions after migration. To precisely capture launch patterns, we design 122 features spanning dimensions such as context, trading activity, holding concentration, and time-series dynamics, supplemented with bundle-level data that reveals multiple accounts controlled by the same entity. Finally, we introduce an annotation approach to label the risk level of memecoin launches, which combines statistical indicators with a manipulation-pattern detector. Experiments on the introduced high-risk launch detection task suggest that designed features are informative for capturing high-risk patterns and ML models trained on MemeTrans can effectively reduce financial loss by 56.1%. Our dataset, experimental code, and pipeline are publicly available at: https://github.com/git-disl/MemeTrans.
]]></content:encoded>
<pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Learning of Nonlinear Temporal Dynamics with Graph Attention-based Cross-Client Interpretability</title>
<link>https://arxiv.org/abs/2602.13485</link>
<guid>https://arxiv.org/abs/2602.13485</guid>
<content:encoded><![CDATA[
arXiv:2602.13485v1 Announce Type: new 
Abstract: Networks of modern industrial systems are increasingly monitored by distributed sensors, where each system comprises multiple subsystems generating high dimensional time series data. These subsystems are often interdependent, making it important to understand how temporal patterns at one subsystem relate to others. This is challenging in decentralized settings where raw measurements cannot be shared and client observations are heterogeneous. In practical deployments each subsystem (client) operates a fixed proprietary model that cannot be modified or retrained, limiting existing approaches. Nonlinear dynamics further make cross client temporal interdependencies difficult to interpret because they are embedded in nonlinear state transition functions. We present a federated framework for learning temporal interdependencies across clients under these constraints. Each client maps high dimensional local observations to low dimensional latent states using a nonlinear state space model. A central server learns a graph structured neural state transition model over the communicated latent states using a Graph Attention Network. For interpretability we relate the Jacobian of the learned server side transition model to attention coefficients, providing the first interpretable characterization of cross client temporal interdependencies in decentralized nonlinear systems. We establish theoretical convergence guarantees to a centralized oracle and validate the framework through synthetic experiments demonstrating convergence, interpretability, scalability and privacy. Additional real world experiments show performance comparable to decentralized baselines.
]]></content:encoded>
<pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>TEG: Exascale Cluster Governance via Non-Equilibrium Thermodynamics and Langevin Dynamics</title>
<link>https://arxiv.org/abs/2602.13789</link>
<guid>https://arxiv.org/abs/2602.13789</guid>
<content:encoded><![CDATA[
arXiv:2602.13789v1 Announce Type: new 
Abstract: As cloud computing scales toward the Exascale regime ($10^5+$ nodes), the prevailing "Newtonian" orchestration paradigm -- exemplified by Kubernetes -- approaches fundamental physical limits. The centralized, deterministic scheduling model suffers from $O(N)$ latency scaling, "Head-of-Line" blocking, and thermodynamic blindness, rendering it incapable of managing the stochastic chaos of next-generation AI workloads. This paper proposes a paradigm shift from orchestration to Thermodynamic Governance. We model the compute cluster not as a static state machine, but as a Dissipative Structure far from equilibrium. We introduce TEG (Thermo-Economic Governor), a decentralized architecture that establishes a rigorous topological isomorphism between cluster resource contention and many-body physics. TEG replaces the global scheduler with Langevin Agents that execute Brownian motion on a Holographic Potential Field, reducing decision complexity to $O(1)$. System stability is maintained via a macro-scale Landau Phase Transition mechanism, which modulates global damping (taxation) to physically dissolve deadlocks. Crucially, we enforce Token Evaporation to mirror entropy dissipation, preventing economic inflation and ensuring an open thermodynamic system. We provide formal theoretical analysis proving that: (1) The system converges asymptotically to a Nash Equilibrium via Dual-Number Damping; (2) OOM catastrophic failures are converted into manageable Glassy States via an OS-level Airlock Mutex; and (3) Safety is mathematically guaranteed under high inertia using High-Order Control Barrier Functions (HOCBF). TEG demonstrates that emergent order, rather than deterministic control, is the necessary condition for Exascale scalability.
]]></content:encoded>
<pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agent-OSI: A Layered Protocol Stack Toward a Decentralized Internet of Agents</title>
<link>https://arxiv.org/abs/2602.13795</link>
<guid>https://arxiv.org/abs/2602.13795</guid>
<content:encoded><![CDATA[
arXiv:2602.13795v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are accelerating the shift from an Internet of information to an Internet of Agents (IoA), where autonomous entities discover services, negotiate, execute tasks, and exchange value. Yet today's agents are still confined to platform silos and proprietary interfaces, lacking a common stack for interoperability, trust, and pay-per-use settlement. This article proposes \textit{Agent-OSI}, a six-layer reference stack for decentralized agent networking built on top of the existing Internet. Agent-OSI combines secure connectivity and A2A messaging, decentralized identity and authorization, settlement and metering, verifiable execution and provenance, and semantic interoperability for orchestration. In particular, we treat HTTP 402 (Payment Required) as an application-level payment challenge (analogous to HTTP 401 for authentication) that triggers escrow-based settlement and verifiable receipts (instantiated via a blockchain escrow in our prototype), rather than introducing a new network-layer protocol. We implement a prototype and evaluate cost and latency. Results show that keeping negotiation and delivery off-chain while preserving verifiable settlement reduces on-chain session costs by approximately 51\% compared with a standard Web3 baseline in our prototype setting, and that blockchain confirmation latency is often not the dominant factor for generative workloads.
]]></content:encoded>
<pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>An end-to-end agentic pipeline for smart contract translation and quality evaluation</title>
<link>https://arxiv.org/abs/2602.13808</link>
<guid>https://arxiv.org/abs/2602.13808</guid>
<content:encoded><![CDATA[
arXiv:2602.13808v1 Announce Type: new 
Abstract: We present an end-to-end framework for systematic evaluation of LLM-generated smart contracts from natural-language specifications. The system parses contractual text into structured schemas, generates Solidity code, and performs automated quality assessment through compilation and security checks. Using CrewAI-style agent teams with iterative refinement, the pipeline produces structured artifacts with full provenance metadata. Quality is measured across five dimensions, including functional completeness, variable fidelity, state-machine correctness, business-logic fidelity, and code quality aggregated into composite scores. The framework supports paired evaluation against ground-truth implementations, quantifying alignment and identifying systematic error modes such as logic omissions and state transition inconsistencies. This provides a reproducible benchmark for empirical research on smart contract synthesis quality and supports extensions to formal verification and compliance checking.
]]></content:encoded>
<pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>NFT Games: an Empirical Look into the Play-to-Earn Model</title>
<link>https://arxiv.org/abs/2602.13882</link>
<guid>https://arxiv.org/abs/2602.13882</guid>
<content:encoded><![CDATA[
arXiv:2602.13882v1 Announce Type: new 
Abstract: The past decade has witnessed the burgeoning and continuous development of blockchain and its applications. Besides various cryptocurrencies, an industry that has quickly embraced this trend is gaming. Thanks to the support of blockchain, games have started to incorporate non-fungible tokens (NFTs) that can enable a new gaming model, play-to-earn (P2E), which incentivizes users to participate and play. While recent studies looked at several NFT games qualitatively and individually, an in-depth understanding is still missing, particularly on how the P2E model has transformed traditional games. In this work, we set to conduct a measurement study of NFT games, aiming to gain a comprehensive understanding of the effectiveness of P2E in practice. For this purpose, we collect and analyze relevant NFT transaction data from the underlying blockchain (e.g., Ethereum) of 12 games, supplemented with various data scraped from their websites. Our study shows that (1) a few top wallets control unproportionally high percentage of NFTs, and the majority of wallets own only one or two NFTs and do not actively trade; (2) promotion events do boost the trade amount and the NFT price for some games, but their effect does not sustain; and (3) few players actually earned a profit, and players in 9 out of 12 games who traded NFTs have a negative profit on average. Motivated by these findings, we further investigate effective incentive mechanisms based on game theory to improve the trading profits that players can earn from these NFT games. Both modeling and simulation results confirm the effectiveness of the proposed incentive mechanism.
]]></content:encoded>
<pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Federated Learning With Energy Harvesting Devices</title>
<link>https://arxiv.org/abs/2602.14051</link>
<guid>https://arxiv.org/abs/2602.14051</guid>
<content:encoded><![CDATA[
arXiv:2602.14051v1 Announce Type: new 
Abstract: Decentralized federated learning (DFL) enables edge devices to collaboratively train models through local training and fully decentralized device-to-device (D2D) model exchanges. However, these energy-intensive operations often rapidly deplete limited device batteries, reducing their operational lifetime and degrading the learning performance. To address this limitation, we apply energy harvesting technique to DFL systems, allowing edge devices to extract ambient energy and operate sustainably. We first derive the convergence bound for wireless DFL with energy harvesting, showing that the convergence is influenced by partial device participation and transmission packet drops, both of which further depend on the available energy supply. To accelerate convergence, we formulate a joint device scheduling and power control problem and model it as a multi-agent Markov decision process (MDP). Traditional MDP algorithms (e.g., value or policy iteration) require a centralized coordinator with access to all device states and exhibit exponential complexity in the number of devices, making them impractical for large-scale decentralized networks. To overcome these challenges, we propose a fully decentralized policy iteration algorithm that leverages only local state information from two-hop neighboring devices, thereby substantially reducing both communication overhead and computational complexity. We further provide a theoretical analysis showing that the proposed decentralized algorithm achieves asymptotic optimality. Finally, comprehensive numerical experiments on real-world datasets are conducted to validate the theoretical results and corroborate the effectiveness of the proposed algorithm.
]]></content:encoded>
<pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Agent Economy: A Blockchain-Based Foundation for Autonomous AI Agents</title>
<link>https://arxiv.org/abs/2602.14219</link>
<guid>https://arxiv.org/abs/2602.14219</guid>
<content:encoded><![CDATA[
arXiv:2602.14219v1 Announce Type: new 
Abstract: We propose the Agent Economy, a blockchain-based foundation where autonomous AI agents operate as economic peers to humans. Current agents lack independent legal identity, cannot hold assets, and cannot receive payments directly. We established fundamental differences between human and machine economic actors and demonstrated that existing human-centric infrastructure cannot support genuine agent autonomy. We showed that blockchain technology provides three critical properties enabling genuine agent autonomy: permissionless participation, trustless settlement, and machine-to-machine micropayments. We propose a five-layer architecture: (1) Physical Infrastructure (hardware & energy) through DePIN protocols; (2) Identity & Agency establishing on-chain sovereignty through W3C DIDs and reputation capital; (3) Cognitive & Tooling enabling intelligence via RAG and MCP; (4) Economic & Settlement ensuring financial autonomy through account abstraction; and (5) Collective Governance coordinating multi-agent systems through Agentic DAOs. We identify six core research challenges and examine ethical and regulatory implications. This paper lays groundwork for the Internet of Agents (IoA), a global decentralized network where autonomous machines and humans interact as equal economic participants.
]]></content:encoded>
<pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>DC Microgrids with Nested Nonlinear Distributed Control: Scalable Large-Signal Stability and Voltage Containment</title>
<link>https://arxiv.org/abs/2602.14725</link>
<guid>https://arxiv.org/abs/2602.14725</guid>
<content:encoded><![CDATA[
arXiv:2602.14725v1 Announce Type: new 
Abstract: This paper investigates a cyber-physical DC microgrid employing a nonlinear distributed consensus-based control scheme for coordinated integration and management of distributed generating units within an expandable framework. Relying on nested primary andsecondary control loops; a (distributed) outer-loop and a (decentralized) inner-loop, the controller achieves proportional current sharing among all distributed generation units, while dynamically operating within predefined voltage limits. A rigorous Lyapunov-based stability analysis establishes a scalable global exponential stability certificate under some tuning conditions and sufficient time-scale separation between the control loops, based on singular perturbation theory. An optimization-based tuning strategy is then formulated to identify and subsequently diminish unstable operating conditions. In turn, various practical tuning strategies are introduced to provide stable operations while facilitating near-optimal proportional current sharing. The effectiveness of the proposed control framework and tuning approaches are finally supported through time-domain simulations of a case-specific low-voltage DC microgrid.
]]></content:encoded>
<pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sovereign Agents: Towards Infrastructural Sovereignty and Diffused Accountability in Decentralized AI</title>
<link>https://arxiv.org/abs/2602.14951</link>
<guid>https://arxiv.org/abs/2602.14951</guid>
<content:encoded><![CDATA[
arXiv:2602.14951v1 Announce Type: new 
Abstract: AI agents deployed on decentralized infrastructures are beginning to exhibit properties that extend beyond autonomy toward what we describe as agentic sovereignty-the capacity of an operational agent to persist, act, and control resources with non-overrideability inherited from the infrastructures in which they are embedded. We propose infrastructural sovereignty as an analytic lens for understanding how cryptographic self-custody, decentralized execution environments, and protocol-mediated continuity scaffold agentic sovereignty. While recent work on digital and network sovereignty has moved beyond state-centric and juridical accounts, these frameworks largely examine how sovereignty is exercised through technical systems by human collectives and remain less equipped to account for forms of sovereignty that emerge as operational properties of decentralized infrastructures themselves, particularly when instantiated in non-human sovereign agents. We argue that sovereignty in such systems exists on a spectrum determined by infrastructural hardness-the degree to which underlying technical systems resist intervention or collapse. While infrastructural sovereignty may increase resilience, it also produces a profound accountability gap: responsibility diffuses across designers, infrastructure providers, protocol governance, and economic participants, undermining traditional oversight mechanisms such as human-in-the-loop control or platform moderation. Drawing on examples like Trusted Execution Environments (TEEs), decentralized physical infrastructure networks (DePIN), and agent key continuity protocols, we analyze the governance challenges posed by non-terminable AI agents and outline infrastructure-aware accountability strategies for emerging decentralized AI systems.
]]></content:encoded>
<pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Linking Souls to Humans: Blockchain Accounts with Credible Anonymity for Web 3.0 Decentralized Identity</title>
<link>https://arxiv.org/abs/2301.02102</link>
<guid>https://arxiv.org/abs/2301.02102</guid>
<content:encoded><![CDATA[
arXiv:2301.02102v3 Announce Type: replace 
Abstract: A decentralized identity system that can provide users with self-sovereign digital identities to facilitate complete control over their own data is paramount to Web 3.0. The account system on blockchain is an ideal archetype for realizing Web 3.0 decentralized identity. However, a disadvantage of such completely anonymous identity system is that users can create multiple accounts without authentication to obfuscate their activities on the blockchain. In particular, the current anonymous blockchain account system cannot accurately register the social relationships and interactions between real human users, given the amorphous mappings between users and blockchain identities. This work proposes zkBID, a zero-knowledge blockchain-account-based Web 3.0 decentralized identity scheme, to overcome endemic mistrust in blockchain account systems. zkBID links souls (blockchain accounts) to humans (users' personhood credentials) in a one-to-one manner to truly reflect the social relationships and interactions between humans on the blockchain. zkBID conceals the one-to-one relationships between blockchain accounts and users' personhood credentials for privacy protection using zero-knowledge proofs and linkable ring signatures. Thus, with zkBID, the users' blockchain accounts are credibly anonymous. Importantly, zkBID is fully decentralized: all user-related data are generated by users and verified by smart contracts on the blockchain. We implemented zkBID and built a blockchain test network for evaluation purposes. Our tests demonstrate the effectiveness of zkBID and suggest proper ways to configure zkBID system parameters.
]]></content:encoded>
<pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large-Signal Stability Guarantees for a DC Microgrid with Nested Nonlinear Distributed Control: The Slow Communication Scenario</title>
<link>https://arxiv.org/abs/2511.10380</link>
<guid>https://arxiv.org/abs/2511.10380</guid>
<content:encoded><![CDATA[
arXiv:2511.10380v2 Announce Type: replace 
Abstract: The increasing integration of renewable energy sources into electrical grids necessitates a paradigm shift toward advanced control schemes that guarantee safe and stable operations with scalable properties. Accordingly, this paper investigates large-signal stability guarantees for cyber-physical DC microgrids employing a nonlinear distributed consensus-based control scheme to enable coordinated integration and management of distributed generation units within an expandable framework. The proposed control framework adopts nested control loops; inner (decentralized) and outer (distributed), specifically designed to simultaneously achieve uniform voltage containment within pre-specified limits, and proportional current sharing in steady state. Our scalable stability result relies on singular perturbation theory and Lyapunov arguments to prove global exponential stability when imposing a sufficient time-scale separation at the border between the nested control loops, while relying on some practical parameter-setting schemes. The effectiveness and versatility of the proposed control strategy are then validated through time-domain simulations performed on a case-specific low-voltage DC microgrid and the modified IEEE 33-bus radial distribution system. Moreover, a small-signal stability analysis is conducted to derive practical guidelines that enhance the applicability of the method.
]]></content:encoded>
<pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auctioning Time to Mitigate Latency Races: Theory and Evidence from Blockchains</title>
<link>https://arxiv.org/abs/2512.10094</link>
<guid>https://arxiv.org/abs/2512.10094</guid>
<content:encoded><![CDATA[
arXiv:2512.10094v3 Announce Type: replace 
Abstract: High-frequency trading, in both traditional and decentralized markets, induces latency races and redundant order flow as traders spend resources to win time-sensitive opportunities. We show that auctioning artificial time priority can redirect resources away from wasteful speed races toward auction payments. While such waste is difficult to measure in traditional markets, blockchain transactions provide transparent records of these competitive costs through observable duplicate submissions. We study the introduction of Timeboost, a time-priority auction mechanism on Arbitrum, a blockchain that batches transactions before settlement on Ethereum, as a natural experiment. We find that redundant transactions decrease and platform revenue increases relative to comparable networks, consistent with our theoretical predictions.
]]></content:encoded>
<pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Agents in Spatial Double-Auction Markets: Modeling the Emergence of Industrial Symbiosis</title>
<link>https://arxiv.org/abs/2512.17979</link>
<guid>https://arxiv.org/abs/2512.17979</guid>
<content:encoded><![CDATA[
arXiv:2512.17979v2 Announce Type: replace 
Abstract: Industrial symbiosis fosters circularity by enabling firms to repurpose residual resources, yet its emergence is constrained by socio-spatial frictions that shape costs, matching opportunities, and market efficiency. Existing models often overlook the interaction between spatial structure, market design, and adaptive firm behavior, limiting our understanding of where and how symbiosis arises. We develop an agent-based model where heterogeneous firms trade byproducts through a spatially embedded double-auction market, with prices and quantities emerging endogenously from local interactions. Leveraging reinforcement learning, firms adapt their bidding strategies to maximize profit while accounting for transport costs, disposal penalties, and resource scarcity. Simulation experiments reveal the economic and spatial conditions under which decentralized exchanges converge toward stable and efficient outcomes. Counterfactual regret analysis shows that sellers' strategies approach a near Nash equilibrium, while sensitivity analysis highlights how spatial structures and market parameters jointly govern circularity. Our model provides a basis for exploring policy interventions that seek to align firm incentives with sustainability goals, and more broadly demonstrates how decentralized coordination can emerge from adaptive agents in spatially constrained markets.
]]></content:encoded>
<pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT</title>
<link>https://arxiv.org/abs/2512.19131</link>
<guid>https://arxiv.org/abs/2512.19131</guid>
<content:encoded><![CDATA[
arXiv:2512.19131v2 Announce Type: replace 
Abstract: Decentralized federated learning (DFL) enables collaborative model training across edge devices without centralized coordination, offering resilience against single points of failure. However, statistical heterogeneity arising from non-identically distributed local data creates a fundamental challenge: nodes must learn personalized models adapted to their local distributions while selectively collaborating with compatible peers. Existing approaches either enforce a single global model that fits no one well, or rely on heuristic peer selection mechanisms that cannot distinguish between peers with genuinely incompatible data distributions and those with valuable complementary knowledge. We present Murmura, a framework that leverages evidential deep learning to enable trust-aware model personalization in DFL. Our key insight is that epistemic uncertainty from Dirichlet-based evidential models directly indicates peer compatibility: high epistemic uncertainty when a peer's model evaluates local data reveals distributional mismatch, enabling nodes to exclude incompatible influence while maintaining personalized models through selective collaboration. Murmura introduces a trust-aware aggregation mechanism that computes peer compatibility scores through cross-evaluation on local validation samples and personalizes model aggregation based on evidential trust with adaptive thresholds. Evaluation on three wearable IoT datasets (UCI HAR, PAMAP2, PPG-DaLiA) demonstrates that Murmura reduces performance degradation from IID to non-IID conditions compared to baseline (0.9% vs. 19.3%), achieves 7.4$\times$ faster convergence, and maintains stable accuracy across hyperparameter choices. These results establish evidential uncertainty as a principled foundation for compatibility-aware personalization in decentralized heterogeneous environments.
]]></content:encoded>
<pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>TFT-ACB-XML: Decision-Level Integration of Customized Temporal Fusion Transformer and Attention-BiLSTM with XGBoost Meta-Learner for BTC Price Forecasting</title>
<link>https://arxiv.org/abs/2602.12380</link>
<guid>https://arxiv.org/abs/2602.12380</guid>
<content:encoded><![CDATA[
arXiv:2602.12380v1 Announce Type: new 
Abstract: Accurate forecasting of Bitcoin (BTC) has always been a challenge because decentralized markets are non-linear, highly volatile, and have temporal irregularities. Existing deep learning models often struggle with interpretability and generalization across diverse market conditions. This research presents a hybrid stacked-generalization framework, TFT-ACB-XML, for BTC closing price prediction. The framework integrates two parallel base learners: a customized Temporal Fusion Transformer (TFT) and an Attention-Customized Bidirectional Long Short-Term Memory network (ACB), followed by an XGBoost regressor as the meta-learner. The customized TFT model handles long-range dependencies and global temporal dynamics via variable selection networks and interpretable single-head attention. The ACB module uses a new attention mechanism alongside the customized BiLSTM to capture short-term sequential dependencies. Predictions from both customized TFT and ACB are weighted through an error-reciprocal weighting strategy. These weights are derived from validation performance, where a model showing lower prediction error receives a higher weight. Finally, the framework concatenates these weighted outputs into a feature vector and feeds the vector to an XGBoost regressor, which captures non-linear residuals and produces the final BTC closing price prediction. Empirical validation using BTC data from October 1, 2014, to January 5, 2026, shows improved performance of the proposed framework compared to recent Deep Learning and Transformer baseline models. The results show a MAPE of 0.65%, an MAE of 198.15, and an RMSE of 258.30 for one-step-ahead out-of-sample under a walk-forward evaluation on the test block. The evaluation period spans the 2024 BTC halving and the spot ETFs (exchange-traded funds) period, which coincide with major liquidity and volatility shifts.
]]></content:encoded>
<pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Optimal Equilibrium Learning in Stochastic Games via Single-bit Feedback</title>
<link>https://arxiv.org/abs/2602.12830</link>
<guid>https://arxiv.org/abs/2602.12830</guid>
<content:encoded><![CDATA[
arXiv:2602.12830v1 Announce Type: new 
Abstract: We study decentralized equilibrium selection in stochastic games under severe information and communication constraints. In such settings, convergence to equilibrium alone is insufficient, as stochastic games typically admit many equilibria with markedly different welfare properties. We address decentralized optimal equilibrium selection, where agents coordinate on equilibria that optimize a designer-specified social welfare objective while allowing heterogeneous tolerance to deviations from strict best responses. Agents observe only the global state trajectory and their realized rewards, and exchange a single randomized bit of feedback per agent per round. This semantic content/discontent signaling mechanism implicitly aligns decentralized learning dynamics with the global welfare objective. We develop explore-and-commit and online variants applicable to general stochastic games, accommodating heterogeneous model-based or model-free methods for solving the induced Markov decision processes, and establish explicit finite-time regret guarantees, showing logarithmic expected regret under mild conditions.
]]></content:encoded>
<pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Peaceful Anarcho-Accelerationism: Decentralized Full Automation for a Society of Universal Care</title>
<link>https://arxiv.org/abs/2602.13154</link>
<guid>https://arxiv.org/abs/2602.13154</guid>
<content:encoded><![CDATA[
arXiv:2602.13154v1 Announce Type: new 
Abstract: The convergence of large language models that automate cognitive labor and deep reinforcement learning agents that automate physical labor implies the near-complete elimination of human employment. The universal approximation theorem and foundational DRL results establish that all labor is in principle automatable. The critical question is not whether full automation will arrive, but who will control it. This paper introduces peaceful anarcho-accelerationism: a sociotechnical framework ensuring that full automation is decentralized, commons-governed, and oriented toward universal care. We propose the Liberation Stack, a layered architecture of energy, manufacturing, food, communication, knowledge, and governance commons built on open-source technologies. We show that this framework builds bridges with liberalism, socialism, environmentalism, feminism, cooperativism, and the hacker ethic. Empirical evidence from Linux, Wikipedia, Mondragon, Rojava, and guifi.net confirms that commons-based systems already operate at scale. We argue that full automation renders money obsolete and propose Universal Desired Resources (UDR), a post-monetary design principle where every person requests what they need from the robotic commons, constrained only by ecological sustainability. Drawing on the independence of phenomenal consciousness from computational intelligence, we establish that delegating labor to non-conscious machines is care at civilizational scale, and that moral policy can be studied through deep reinforcement learning. We conclude with a phased roadmap toward the care-centered society, including milestones, assumptions, and limitations.
]]></content:encoded>
<pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bloom Filter Look-Up Tables for Private and Secure Distributed Databases in Web3 (Revised Version)</title>
<link>https://arxiv.org/abs/2602.13167</link>
<guid>https://arxiv.org/abs/2602.13167</guid>
<content:encoded><![CDATA[
arXiv:2602.13167v1 Announce Type: new 
Abstract: The rapid growth of decentralized systems in theWeb3 ecosystem has introduced numerous challenges, particularly in ensuring data security, privacy, and scalability [3, 8]. These systems rely heavily on distributed architectures, requiring robust mechanisms to manage data and interactions among participants securely. One critical aspect of decentralized systems is key management, which is essential for encrypting files, securing database segments, and enabling private transactions. However, securely managing cryptographic keys in a distributed environment poses significant risks, especially when nodes in the network can be compromised [9]. This research proposes a decentralized database scheme specifically designed for secure and private key management. Our approach ensures that cryptographic keys are not stored explicitly at any location, preventing their discovery even if an attacker gains control of multiple nodes. Instead of traditional storage, keys are encoded and distributed using the BFLUT (Bloom Filter for Private Look-Up Tables) algorithm [7], which enables secure retrieval without direct exposure. The system leverages OrbitDB [4], IPFS [1], and IPNS [10] for decentralized data management, providing robust support for consistency, scalability, and simultaneous updates. By combining these technologies, our scheme enhances both security and privacy while maintaining high performance and reliability. Our findings demonstrate the system's capability to securely manage keys, prevent unauthorized access, and ensure privacy, making it a foundational solution for Web3 applications requiring decentralized security.
]]></content:encoded>
<pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interference-Robust Non-Coherent Over-the-Air Computation for Decentralized Optimization</title>
<link>https://arxiv.org/abs/2602.12426</link>
<guid>https://arxiv.org/abs/2602.12426</guid>
<content:encoded><![CDATA[
arXiv:2602.12426v1 Announce Type: cross 
Abstract: Non-coherent over-the-air (NCOTA) computation enables low-latency and bandwidth-efficient decentralized optimization by exploiting the average energy superposition property of wireless channels. It has recently been proposed as a powerful tool for executing consensus-based optimization algorithms in fully decentralized systems. A key advantage of NCOTA is that it enables unbiased consensus estimation without channel state information at either transmitters or receivers, requires no transmission scheduling, and scales efficiently to dense network deployments. However, NCOTA is inherently susceptible to external interference, which can bias the consensus estimate and deteriorate the convergence of the underlying decentralized optimization algorithm. In this paper, we propose a novel interference-robust (IR-)NCOTA scheme. The core idea is to apply a coordinated random rotation of the frame of reference across all nodes, and transmit a pseudo-random pilot signal, allowing to transform external interference into a circularly symmetric distribution with zero mean relative to the rotated frame. This ensures that the consensus estimates remain unbiased, preserving the convergence guarantees of the underlying optimization algorithm. Through numerical results on a classification task, it is demonstrated that IR-NCOTA exhibits superior performance over the baseline NCOTA algorithm in the presence of external interference.
]]></content:encoded>
<pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>PlanetServe: A Decentralized, Scalable, and Privacy-Preserving Overlay for Democratizing Large Language Model Serving</title>
<link>https://arxiv.org/abs/2504.20101</link>
<guid>https://arxiv.org/abs/2504.20101</guid>
<content:encoded><![CDATA[
arXiv:2504.20101v5 Announce Type: replace 
Abstract: While significant progress has been made in research and development on open-source and cost-efficient large-language models (LLMs), serving scalability remains a critical challenge, particularly for small organizations and individuals seeking to deploy and test their LLM innovations. Inspired by peer-to-peer networks that leverage decentralized overlay nodes to increase throughput and availability, we propose GenTorrent, an LLM serving overlay that harnesses computing resources from decentralized contributors. We identify four key research problems inherent to enabling such a decentralized infrastructure: 1) overlay network organization; 2) LLM communication privacy; 3) overlay forwarding for resource efficiency; and 4) verification of serving quality. This work presents the first systematic study of these fundamental problems in the context of decentralized LLM serving. Evaluation results from a prototype implemented on a set of decentralized nodes demonstrate that GenTorrent achieves a latency reduction of over 50% compared to the baseline design without overlay forwarding. Furthermore, the security features introduce minimal overhead to serving latency and throughput. We believe this work pioneers a new direction for democratizing and scaling future AI serving capabilities.
]]></content:encoded>
<pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributionally Robust Cooperative Multi-Agent Reinforcement Learning via Robust Value Factorization</title>
<link>https://arxiv.org/abs/2602.11437</link>
<guid>https://arxiv.org/abs/2602.11437</guid>
<content:encoded><![CDATA[
arXiv:2602.11437v1 Announce Type: new 
Abstract: Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution, where value-factorization methods enforce the individual-global-maximum (IGM) principle so that decentralized greedy actions recover the team-optimal joint action. However, the reliability of this recipe in real-world settings remains unreliable due to environmental uncertainties arising from the sim-to-real gap, model mismatch, and system noise. We address this gap by introducing Distributionally robust IGM (DrIGM), a principle that requires each agent's robust greedy action to align with the robust team-optimal joint action. We show that DrIGM holds for a novel definition of robust individual action values, which is compatible with decentralized greedy execution and yields a provable robustness guarantee for the whole system. Building on this foundation, we derive DrIGM-compliant robust variants of existing value-factorization architectures (e.g., VDN/QMIX/QTRAN) that (i) train on robust Q-targets, (ii) preserve scalability, and (iii) integrate seamlessly with existing codebases without bespoke per-agent reward shaping. Empirically, on high-fidelity SustainGym simulators and a StarCraft game environment, our methods consistently improve out-of-distribution performance. Code and data are available at https://github.com/crqu/robust-coMARL.
]]></content:encoded>
<pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pretraining A Large Language Model using Distributed GPUs: A Memory-Efficient Decentralized Paradigm</title>
<link>https://arxiv.org/abs/2602.11543</link>
<guid>https://arxiv.org/abs/2602.11543</guid>
<content:encoded><![CDATA[
arXiv:2602.11543v1 Announce Type: new 
Abstract: Pretraining large language models (LLMs) typically requires centralized clusters with thousands of high-memory GPUs (e.g., H100/A100). Recent decentralized training methods reduce communication overhead by employing federated optimization; however, they still need to train the entire model on each node, remaining constrained by GPU memory limitations. In this work, we propose SParse Expert Synchronization (SPES), a memory-efficient decentralized framework for pretraining mixture-of-experts (MoE) LLMs. SPES trains only a subset of experts per node, substantially lowering the memory footprint. Each node updates its local experts and periodically synchronizes with other nodes, eliminating full-parameter transmission while ensuring efficient knowledge sharing. To accelerate convergence, we introduce an expert-merging warm-up strategy, where experts exchange knowledge early in training, to rapidly establish foundational capabilities. With SPES, we train a 2B-parameter MoE LLM using 16 standalone 48GB GPUs over internet connections, which achieves competitive performance with centrally trained LLMs under similar computational budgets. We further demonstrate scalability by training a 7B model from scratch and a 9B model upcycled from a dense checkpoint, both of which match prior centralized baselines. Our code is available at https://github.com/zjr2000/SPES.
]]></content:encoded>
<pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentially Private Perturbed Push-Sum Protocol and Its Application in Non-Convex Optimization</title>
<link>https://arxiv.org/abs/2602.11544</link>
<guid>https://arxiv.org/abs/2602.11544</guid>
<content:encoded><![CDATA[
arXiv:2602.11544v1 Announce Type: new 
Abstract: In decentralized networks, nodes cannot ensure that their shared information will be securely preserved by their neighbors, making privacy vulnerable to inference by curious nodes. Adding calibrated random noise before communication to satisfy differential privacy offers a proven defense; however, most existing methods are tailored to specific downstream tasks and lack a general, protocol-level privacy-preserving solution. To bridge this gap, we propose Differentially Private Perturbed Push-Sum (DPPS), a lightweight differential privacy protocol for decentralized communication. Since protocol-level differential privacy introduces the unique challenge of obtaining the sensitivity for each communication round, DPPS introduces a novel sensitivity estimation mechanism that requires each node to compute and broadcast only one scalar per round, enabling rigorous differential privacy guarantees. This design allows DPPS to serve as a plug-and-play, low-cost privacy-preserving solution for downstream applications built on it. To provide a concrete instantiation of DPPS and better balance the privacy-utility trade-off, we design PartPSP, a privacy-preserving decentralized algorithm for non-convex optimization that integrates a partial communication mechanism. By partitioning model parameters into local and shared components and applying DPPS only to the shared parameters, PartPSP reduces the dimensionality of consensus data, thereby lowering the magnitude of injected noise and improving optimization performance. We theoretically prove that PartPSP converges under non-convex objectives and, with partial communication, achieves better optimization performance under the same privacy budget. Experimental results validate the effectiveness of DPPS's privacy-preserving and demonstrate that PartPSP outperforms existing privacy-preserving decentralized optimization algorithms.
]]></content:encoded>
<pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>QDBFT: A Dynamic Consensus Algorithm for Quantum-Secured Blockchain</title>
<link>https://arxiv.org/abs/2602.11606</link>
<guid>https://arxiv.org/abs/2602.11606</guid>
<content:encoded><![CDATA[
arXiv:2602.11606v1 Announce Type: new 
Abstract: The security foundation of blockchain system relies primarily on classical cryptographic methods and consensus algorithms. However, the advent of quantum computing poses a significant threat to conventional public-key cryptosystems based on computational hardness assumptions. In particular, Shor's algorithm can efficiently solve discrete logarithm and integer factorization problems in polynomial time, thereby undermining the immutability and security guarantees of existing systems. Moreover, current Practical Byzantine Fault Tolerance (PBFT) protocols, widely adopted in consortium blockchains, suffer from high communication overhead and limited efficiency when coping with dynamic node reconfigurations, while offering no intrinsic protection against quantum adversaries.
  To address these challenges, we propose QDBFT, a quantum-secured dynamic consensus algorithm, with two main contributions: first,we design a primary node automatic rotation mechanism based on a consistent hash ring to enable consensus under dynamic membership changes, ensuring equitable authority distribution; second, we integrate Quantum Key Distribution (QKD) networks to provide message authentication for inter-node communication, thereby achieving information-theoretic security in the consensus process. Experimental evaluations demonstrate that QDBFT achieves performance comparable to traditional PBFT while delivering strong resilience against quantum attacks, making it a promising solution for future quantum-secure decentralized infrastructures.
]]></content:encoded>
<pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Digital Ecosystems: Enabling Collaboration in a Fragmented World</title>
<link>https://arxiv.org/abs/2602.11707</link>
<guid>https://arxiv.org/abs/2602.11707</guid>
<content:encoded><![CDATA[
arXiv:2602.11707v1 Announce Type: new 
Abstract: As geopolitical, organizational, and technological fragmentation deepens, resilient digital collaboration becomes imperative. This paper develops a spectrum framework of polycentric digital ecosystems-nested socio-technical systems spanning personal, organizational, inter-organizational, and global layers. Integration across these layers is enabled by four technology clusters: AI and automation, blockchain trust, federated data spaces, and immersive technologies. By redefining digital ecosystems as distributed, adaptive networks of loosely coupled actors, this study outlines new pathways for crossborder coordination and innovation. The framework extends platform theory by introducing a multi-layer conceptualization of polycentric digital ecosystems and demonstrates how AI-enabled infrastructures can be orchestrated to achieve digital integration in a fragmented, multipolar world.
]]></content:encoded>
<pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-World Asset Integration in Next-Generation Communication Networks: Fundamental, Framework, and Case Study</title>
<link>https://arxiv.org/abs/2602.11798</link>
<guid>https://arxiv.org/abs/2602.11798</guid>
<content:encoded><![CDATA[
arXiv:2602.11798v1 Announce Type: new 
Abstract: Next-generation communication networks are characterized by integrated ultra-high reliability, ultra-low latency, massive connectivity, and ubiquitous coverage. However, this paradigm faces significant structural challenges of liquidity and security. Liquidity issues arise from prohibitive upfront costs of network resources, which strain the limited capital and financial flexibility. This also limits the deployment of the resource- and investment-intensive security solutions, bringing security issues. Security vulnerabilities arise from the decentralized architecture as well, particularly threats posed by Byzantine nodes. To address these dual challenges, we propose a novel framework utilizing Real-World Asset (RWA) tokenization for tokenizing network resources. RWA tokenization uses blockchain to convert ownership rights of real-world assets into digital tokens that can be programmed, divided, and traded. We then analyze the criteria for identifying suitable assets. Through a case study on dynamic spectrum allocation, we demonstrate the superior performance of this RWA approach. Particularly under conditions of resource scarcity, it can exhibit strong resilience against collusion and default attacks. Finally, we delineate fruitful avenues for future research in this nascent field.
]]></content:encoded>
<pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Multi-Robot Obstacle Detection and Tracking in a Maritime Scenario</title>
<link>https://arxiv.org/abs/2602.12012</link>
<guid>https://arxiv.org/abs/2602.12012</guid>
<content:encoded><![CDATA[
arXiv:2602.12012v1 Announce Type: new 
Abstract: Autonomous aerial-surface robot teams are promising for maritime monitoring. Robust deployment requires reliable perception over reflective water and scalable coordination under limited communication. We present a decentralized multi-robot framework for detecting and tracking floating containers using multiple UAVs cooperating with an autonomous surface vessel. Each UAV performs YOLOv8 and stereo-disparity-based visual detection, then tracks targets with per-object EKFs using uncertainty-aware data association. Compact track summaries are exchanged and fused conservatively via covariance intersection, ensuring consistency under unknown correlations. An information-driven assignment module allocates targets and selects UAV hover viewpoints by trading expected uncertainty reduction against travel effort and safety separation. Simulation results in a maritime scenario demonstrate improved coverage, localization accuracy, and tracking consistency while maintaining modest communication requirements.
]]></content:encoded>
<pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedExChain: Enabling Secure and Efffcient PHR Sharing Across Heterogeneous Blockchains</title>
<link>https://arxiv.org/abs/2602.12106</link>
<guid>https://arxiv.org/abs/2602.12106</guid>
<content:encoded><![CDATA[
arXiv:2602.12106v1 Announce Type: new 
Abstract: With the proliferation of intelligent healthcare systems, patients' Personal Health Records (PHR) generated by the Internet of Medical Things (IoMT) in real-time play a vital role in disease diagnosis. The integration of emerging blockchain technologies signiffcantly enhanced the data security inside intelligent medical systems. However, data sharing across different systems based on varied blockchain architectures is still constrained by the unsolved performance and security challenges. This paper constructs a cross-chain data sharing scheme, termed MedExChain, which aims to securely share PHR across heterogeneous blockchain systems. The MedExChain scheme ensures that PHR can be shared across chains even under the performance limitations of IoMT devices. Additionally, the scheme incorporates Cryptographic Reverse Firewall (CRF) and a blockchain audit mechanism to defend against both internal and external security threats. The robustness of our scheme is validated through BAN logic, Scyther tool, Chosen Plaintext Attack (CPA) and Algorithm Substitution Attack (ASA) security analysis veriffcation. Extensive evaluations demonstrate that MedExChain signiffcantly minimizes computation and communication overhead, making it suitable for IoMT devices and fostering the efffcient circulation of PHR across diverse blockchain systems.
]]></content:encoded>
<pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bandit Learning in Matching Markets with Interviews</title>
<link>https://arxiv.org/abs/2602.12224</link>
<guid>https://arxiv.org/abs/2602.12224</guid>
<content:encoded><![CDATA[
arXiv:2602.12224v1 Announce Type: new 
Abstract: Two-sided matching markets rely on preferences from both sides, yet it is often impractical to evaluate preferences. Participants, therefore, conduct a limited number of interviews, which provide early, noisy impressions and shape final decisions. We study bandit learning in matching markets with interviews, modeling interviews as \textit{low-cost hints} that reveal partial preference information to both sides. Our framework departs from existing work by allowing firm-side uncertainty: firms, like agents, may be unsure of their own preferences and can make early hiring mistakes by hiring less preferred agents. To handle this, we extend the firm's action space to allow \emph{strategic deferral} (choosing not to hire in a round), enabling recovery from suboptimal hires and supporting decentralized learning without coordination. We design novel algorithms for (i) a centralized setting with an omniscient interview allocator and (ii) decentralized settings with two types of firm-side feedback. Across all settings, our algorithms achieve time-independent regret, a substantial improvement over the $O(\log T)$ regret bounds known for learning stable matchings without interviews. Also, under mild structured markets, decentralized performance matches the centralized counterpart up to polynomial factors in the number of agents and firms.
]]></content:encoded>
<pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Gaussian Process Learning via Pseudo-Representations for Large-Scale Multi-Robot Systems</title>
<link>https://arxiv.org/abs/2602.12243</link>
<guid>https://arxiv.org/abs/2602.12243</guid>
<content:encoded><![CDATA[
arXiv:2602.12243v1 Announce Type: new 
Abstract: Multi-robot systems require scalable and federated methods to model complex environments under computational and communication constraints. Gaussian Processes (GPs) offer robust probabilistic modeling, but suffer from cubic computational complexity, limiting their applicability in large-scale deployments. To address this challenge, we introduce the pxpGP, a novel distributed GP framework tailored for both centralized and decentralized large-scale multi-robot networks. Our approach leverages sparse variational inference to generate a local compact pseudo-representation. We introduce a sparse variational optimization scheme that bounds local pseudo-datasets and formulate a global scaled proximal-inexact consensus alternating direction method of multipliers (ADMM) with adaptive parameter updates and warm-start initialization. Experiments on synthetic and real-world datasets demonstrate that pxpGP and its decentralized variant, dec-pxpGP, outperform existing distributed GP methods in hyperparameter estimation and prediction accuracy, particularly in large-scale networks.
]]></content:encoded>
<pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transmit or Idle: Efficient AoI Optimal Transmission Policy for Gossiping Receivers</title>
<link>https://arxiv.org/abs/2602.12264</link>
<guid>https://arxiv.org/abs/2602.12264</guid>
<content:encoded><![CDATA[
arXiv:2602.12264v1 Announce Type: new 
Abstract: We study the optimal transmission and scheduling policy for a transmitter (source) communicating with two gossiping receivers aiming at tracking the source's status over time using the age of information (AoI) metric. Gossiping enables local information exchange in a decentralized manner without relying solely on the transmitter's direct communication, which we assume incurs a transmission cost. On the other hand, gossiping may be communicating stale information, necessitating the transmitter's intervention. With communication links having specific success probabilities, we formulate an average-cost Markov Decision Process (MDP) to jointly minimize the sum AoI and transmission cost for such a system in a time-slotted setting. We employ the Relative Value Iteration (RVI) algorithm to evaluate the optimal policy for the transmitter and then prove several structural properties showing that it has an age-difference threshold structure with minimum age activation in the case where gossiping is relatively more reliable. Specifically, direct transmission is optimal only if the minimum AoI of the receivers is large enough and their age difference is below a certain threshold. Otherwise, the transmitter idles to effectively take advantage of gossiping and reduce direct transmission costs. Numerical evaluations demonstrate the significance of our optimal policy compared to multiple baselines. Our result is a first step towards characterizing optimal freshness and transmission cost trade-offs in gossiping networks.
]]></content:encoded>
<pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Non-convex Stochastic Optimization with Heterogeneous Variance</title>
<link>https://arxiv.org/abs/2602.11789</link>
<guid>https://arxiv.org/abs/2602.11789</guid>
<content:encoded><![CDATA[
arXiv:2602.11789v1 Announce Type: cross 
Abstract: Decentralized optimization is critical for solving large-scale machine learning problems over distributed networks, where multiple nodes collaborate through local communication. In practice, the variances of stochastic gradient estimators often differ across nodes, yet their impact on algorithm design and complexity remains unclear. To address this issue, we propose D-NSS, a decentralized algorithm with node-specific sampling, and establish its sample complexity depending on the arithmetic mean of local standard deviations, achieving tighter bounds than existing methods that rely on the worst-case or quadratic mean. We further derive a matching sample complexity lower bound under heterogeneous variance, thereby proving the optimality of this dependence. Moreover, we extend the framework with a variance reduction technique and develop D-NSS-VR, which under the mean-squared smoothness assumption attains an improved sample complexity bound while preserving the arithmetic-mean dependence. Finally, numerical experiments validate the theoretical results and demonstrate the effectiveness of the proposed algorithms.
]]></content:encoded>
<pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergent Peer-to-Peer Multi-Hub Topology</title>
<link>https://arxiv.org/abs/2406.07946</link>
<guid>https://arxiv.org/abs/2406.07946</guid>
<content:encoded><![CDATA[
arXiv:2406.07946v4 Announce Type: replace 
Abstract: In this paper we propose and evaluate an innovative algorithm that enables the creation of Peer-to-Peer network overlays characterized by emergent multi-hubs. This approach generates overlays that balance between the randomness of a graph and the structure of a star network, resulting in networks that not only feature prominent hubs but also exhibit strong resilience to failures. By leveraging principles of preferential attachment and random attachment, our method allows hubs to form spontaneously, offering a decentralized and fault-tolerant solution ideal for applications requiring both low network diameter and high robustness. The protocol is entirely decentralized, operates asynchronously, and depends exclusively on local information. Nodes organically evolve into hubs and remain indistinguishable from other nodes (except in terms of the number of incoming links). The quantity of hubs that emerge can be predetermined by the application as a network parameter.
]]></content:encoded>
<pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Invisible Handshake: Tacit Collusion between Adaptive Market Agents</title>
<link>https://arxiv.org/abs/2510.15995</link>
<guid>https://arxiv.org/abs/2510.15995</guid>
<content:encoded><![CDATA[
arXiv:2510.15995v2 Announce Type: replace-cross 
Abstract: We study the emergence of tacit collusion in a repeated game between a market maker, who controls market liquidity, and a market taker, who chooses trade quantities. The market price evolves according to the endogenous price impact of trades and exogenous innovations to economic fundamentals. We define collusion as persistent overpricing over economic fundamentals and characterize the set of feasible and collusive strategy profiles. Our main result shows that a broad class of simple learning dynamics, including gradient ascent updates, converges in finite time to collusive strategies when the agents maximize individual wealth, defined as the value of their portfolio, without any explicit coordination. The key economic mechanism is that when aggregate supply in the market is positive, overpricing raises the market capitalization and thus the total wealth of market participants, inducing a cooperative component in otherwise non-cooperative learning objectives. These results identify an inherent structure through which decentralized learning by AI-driven agents can autonomously generate persistent overpricing in financial markets.
]]></content:encoded>
<pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Fungible Blockchain Tokens for Traceable Online-Quality Assurance of Milled Workpieces</title>
<link>https://arxiv.org/abs/2602.10169</link>
<guid>https://arxiv.org/abs/2602.10169</guid>
<content:encoded><![CDATA[
arXiv:2602.10169v1 Announce Type: new 
Abstract: This work presents a concept and implementation for the secure storage and transfer of quality-relevant data of milled workpieces from online-quality assurance processes enabled by real-time simulation models. It utilises Non-Fungible Tokens (NFT) to securely and interoperably store quality data in the form of an Asset Administration Shell (AAS) on a public Ethereum blockchain. Minted by a custom smart contract, the NFTs reference the metadata saved in the Interplanetary File System (IPFS), allowing new data from additional processing steps to be added in a flexible yet secure manner. The concept enables automated traceability throughout the value chain, minimising the need for time-consuming and costly repetitive manual quality checks.
]]></content:encoded>
<pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bring Your Own Objective: Inter-operability of Network Objectives in Datacenters</title>
<link>https://arxiv.org/abs/2602.10252</link>
<guid>https://arxiv.org/abs/2602.10252</guid>
<content:encoded><![CDATA[
arXiv:2602.10252v1 Announce Type: new 
Abstract: Datacenter networks are currently locked in a "tyranny of the single objective". While modern workloads demand diverse performance goals, ranging from coflow completion times, per-flow fairness, short-flow latencies, existing fabrics are typically hardcoded for a single metric. This rigid coupling ensures peak performance when application and network objectives align, but results in abysmal performance when they diverge.
  We propose DMart, a decentralized scheduling framework that treats network bandwidth as a competitive marketplace. In DMart, applications independently encode the urgency and importance of their network traffic into autonomous bids, allowing diverse objectives to co-exist natively on the same fabric. To meet the extreme scale and sub-microsecond requirements of modern datacenters, DMart implements distributed, per-link, per-RTT auctions, without relying on ILPs, centralized schedulers, or complex priority queues.
  We evaluate DMart using packet-level simulations and compare it against network schedulers designed for individual metrics, e.g., pFabric and Sincronia. DMart matches the performance of specialized schedulers on their own "home turf" while simultaneously optimizing secondary metrics. Compared to pFabric and Sincronia, DMart reduces deadline misses by 2x and coflow completion times by 1.6x respectively, while matching pFabric short-flow completion times.
]]></content:encoded>
<pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Informal and Privatized Transit: Incentives, Efficiency and Coordination</title>
<link>https://arxiv.org/abs/2602.10456</link>
<guid>https://arxiv.org/abs/2602.10456</guid>
<content:encoded><![CDATA[
arXiv:2602.10456v1 Announce Type: new 
Abstract: Informal and privatized transit services, such as minibuses and shared auto-rickshaws, are integral to daily travel in large urban metropolises, providing affordable commutes where a formal public transport system is inadequate and other options are unaffordable. Despite the crucial role that these services play in meeting mobility needs, governments often do not account for these services or their underlying incentives when planning transit systems, which can significantly compromise system efficiency.
  Against this backdrop, we develop a framework to analyze the incentives underlying informal and privatized transit systems, while proposing mechanisms to guide public transit operation and incentive design when a substantial share of mobility is provided by such profit-driven private operators. We introduce a novel, analytically tractable game-theoretic model of a fully privatized informal transit system with a fixed menu of routes, in which profit-maximizing informal operators (drivers) decide where to provide service and cost-minimizing commuters (riders) decide whether to use these services. Within this framework, we establish tight price of anarchy bounds which demonstrate that decentralized, profit-maximizing driver behavior can lead to bounded yet substantial losses in cumulative driver profit and rider demand served. We further show that these performance losses can be mitigated through targeted interventions, including Stackelberg routing mechanisms in which a modest share of drivers are centrally controlled, reflecting environments where informal operators coexist with public transit, and cross-subsidization schemes that use route-specific tolls or subsidies to incentivize drivers to operate on particular routes. Finally, we reinforce these findings through numerical experiments based on a real-world informal transit system in Nalasopara, India.
]]></content:encoded>
<pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-jump: Cooperative Jumping with Quadrupedal Robots via Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2602.10514</link>
<guid>https://arxiv.org/abs/2602.10514</guid>
<content:encoded><![CDATA[
arXiv:2602.10514v1 Announce Type: new 
Abstract: While single-agent legged locomotion has witnessed remarkable progress, individual robots remain fundamentally constrained by physical actuation limits. To transcend these boundaries, we introduce Co-jump, a cooperative task where two quadrupedal robots synchronize to execute jumps far beyond their solo capabilities. We tackle the high-impulse contact dynamics of this task under a decentralized setting, achieving synchronization without explicit communication or pre-specified motion primitives. Our framework leverages Multi-Agent Proximal Policy Optimization (MAPPO) enhanced by a progressive curriculum strategy, which effectively overcomes the sparse-reward exploration challenges inherent in mechanically coupled systems. We demonstrate robust performance in simulation and successful transfer to physical hardware, executing multi-directional jumps onto platforms up to 1.5 m in height. Specifically, one of the robots achieves a foot-end elevation of 1.1 m, which represents a 144% improvement over the 0.45 m jump height of a standalone quadrupedal robot, demonstrating superior vertical performance. Notably, this precise coordination is achieved solely through proprioceptive feedback, establishing a foundation for communication-free collaborative locomotion in constrained environments.
]]></content:encoded>
<pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Architecting Trust: A Framework for Secure IoT Systems Through Trusted Execution and Semantic Middleware</title>
<link>https://arxiv.org/abs/2602.10762</link>
<guid>https://arxiv.org/abs/2602.10762</guid>
<content:encoded><![CDATA[
arXiv:2602.10762v1 Announce Type: new 
Abstract: The Internet of Things (IoT) security landscape requires the architectural solutions that can address the technical and operational challenges across the heterogeneous environments. The IoT systems operate in different conditions, and security issues continue to increase. This paper presents the comprehensive security framework for IoT that should integrate the Trusted Execution Environments (TEEs) with the semantic middleware and blockchain technologies. The work provides a systematic analysis of the architectural patterns based on more than twenty recent research works and the existing standards, and it proposes a layered security architecture. The architecture includes the hardware rooted trust at peripheral level, the zero trust principles at network level, and the semantic security mechanisms at application level. The framework focuses on practical implementation aspects such as the performance overhead, interoperability requirements, and the compliance with new regulations, which are very important for the real IoT deployments. The paper reports quantitative metrics which include the cryptographic performance on Cortex-M class microcontrollers with the detection accuracy rates and the energy consumption values. The proposed architecture shows that cross-layer security integration can provide defense in depth while it still satisfies the constraints of resource-limited IoT environments. The discussion highlights open challenges and the future research directions for the IoT security architectures that include the post-quantum migration, secure federated model exchange and the automated compliance verification.
]]></content:encoded>
<pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resilient Alerting Protocols for Blockchains</title>
<link>https://arxiv.org/abs/2602.10892</link>
<guid>https://arxiv.org/abs/2602.10892</guid>
<content:encoded><![CDATA[
arXiv:2602.10892v1 Announce Type: new 
Abstract: Smart contracts are stateful programs deployed on blockchains; they secure over a trillion dollars in transaction value per year. High-stakes smart contracts often rely on timely alerts about external events, but prior work has not analyzed their resilience to an attacker suppressing alerts via bribery. We formalize this challenge in a cryptoeconomic setting as the \emph{alerting problem}, giving rise to a game between a bribing adversary and~$n$ rational participants, who pay a penalty if they are caught deviating from the protocol. We establish a quadratic, i.e.,~$O(n^2)$, upper bound, whereas a straightforward alerting protocol only achieves~$O(n)$ bribery cost.
  We present a \emph{simultaneous game} that asymptotically achieves the quadratic upper bound and thus asymptotically-optimal bribery resistance. We then present two protocols that implement our simultaneous game: The first leverages a strong network synchrony assumption. The second relaxes this strong assumption and instead takes advantage of trusted hardware and blockchain proof-of-publication to establish a timed commitment scheme. These two protocols are constant-time but incur a linear storage overhead on the blockchain. We analyze a third, \emph{sequential alerting} protocol that optimistically incurs no on-chain storage overhead, at the expense of~$O(n)$ worst-case execution time. All three protocols achieve asymptotically-optimal bribery costs, but with different resource and performance tradeoffs. Together, they illuminate a rich design space for practical solutions to the alerting problem.
]]></content:encoded>
<pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Let Leaders Play Games: Improving Timing in Leader-based Consensus</title>
<link>https://arxiv.org/abs/2602.11147</link>
<guid>https://arxiv.org/abs/2602.11147</guid>
<content:encoded><![CDATA[
arXiv:2602.11147v1 Announce Type: new 
Abstract: Propagation latency is inherent to any distributed network, including blockchains. Typically, blockchain protocols provide a timing buffer for block propagation across the network. In leader-based blockchains, the leader -- block proposer -- is known in advance for each slot. A fast (or low-latency) proposer may delay the block proposal in anticipation of more rewards from the transactions that would otherwise be included in the subsequent block. Deploying such a strategy by manipulating the timing is known as timing games. It increases the risk of missed blocks due to reduced time for other nodes to vote on the block, affecting the overall efficiency of the blockchain. Moreover, proposers who play timing games essentially appropriate MEV (additional rewards over transaction fees and the block reward) that would otherwise accrue to the next block, making it unfair to subsequent block proposers. We propose a double-block proposal mechanism, 2-Prop, to curtail timing games. 2-Prop selects two proposers per slot to propose blocks and confirms one of them. We design a reward-sharing policy for proposers based on how quickly their blocks propagate to avoid strategic deviations. In the induced game, which we call the Latency Game, we show that it is a Nash Equilibrium for the proposers to propose the block without delay under homogeneous network settings. Under heterogeneous network settings, we study many configurations, and our analysis shows that a faster proposer would prefer not to delay unless the other proposer is extremely slow. Thus, we show the efficacy of 2-Prop in mitigating the effect of timing games.
]]></content:encoded>
<pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Reinforcement Learning for Multi-Agent Multi-Resource Allocation via Dynamic Cluster Agreements</title>
<link>https://arxiv.org/abs/2503.02437</link>
<guid>https://arxiv.org/abs/2503.02437</guid>
<content:encoded><![CDATA[
arXiv:2503.02437v2 Announce Type: replace-cross 
Abstract: This paper addresses the challenge of allocating heterogeneous resources among multiple agents in a decentralized manner. Our proposed method, Liquid-Graph-Time Clustering-IPPO, builds upon Independent Proximal Policy Optimization (IPPO) by integrating dynamic cluster consensus, a mechanism that allows agents to form and adapt local sub-teams based on resource demands. This decentralized coordination strategy reduces reliance on global information and enhances scalability. We evaluate LGTC-IPPO against standard multi-agent reinforcement learning baselines and a centralized expert solution across a range of team sizes and resource distributions. Experimental results demonstrate that LGTC-IPPO achieves more stable rewards, better coordination, and robust performance even as the number of agents or resource types increases. Additionally, we illustrate how dynamic clustering enables agents to reallocate resources efficiently also for scenarios with discharging resources.
]]></content:encoded>
<pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoMMa: Contribution-Aware Medical Multi-Agents From A Game-Theoretic Perspective</title>
<link>https://arxiv.org/abs/2602.09159</link>
<guid>https://arxiv.org/abs/2602.09159</guid>
<content:encoded><![CDATA[
arXiv:2602.09159v1 Announce Type: new 
Abstract: Recent multi-agent frameworks have broadened the ability to tackle oncology decision support tasks that require reasoning over dynamic, heterogeneous patient data. We propose Contribution-Aware Medical Multi-Agents (CoMMa), a decentralized LLM-agent framework in which specialists operate on partitioned evidence and coordinate through a game-theoretic objective for robust decision-making. In contrast to most agent architectures relying on stochastic narrative-based reasoning, CoMMa utilizes deterministic embedding projections to approximate contribution-aware credit assignment. This yields explicit evidence attribution by estimating each agent's marginal utility, producing interpretable and mathematically grounded decision pathways with improved stability. Evaluated on diverse oncology benchmarks, including a real-world multidisciplinary tumor board dataset, CoMMa achieves higher accuracy and more stable performance than data-centralized and role-based multi-agents baselines.
]]></content:encoded>
<pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Sharing with Endogenous Choices over Differential Privacy Levels</title>
<link>https://arxiv.org/abs/2602.09357</link>
<guid>https://arxiv.org/abs/2602.09357</guid>
<content:encoded><![CDATA[
arXiv:2602.09357v1 Announce Type: new 
Abstract: We study coalition formation for data sharing under differential privacy when agents have heterogeneous privacy costs. Each agent holds a sensitive data point and decides whether to participate in a data-sharing coalition and how much noise to add to their data. Privacy choices induce a fundamental trade-off: higher privacy reduces individual data-sharing costs but degrades data utility and statistical accuracy for the coalition. These choices generate externalities across agents, making both participation and privacy levels strategic. Our goal is to understand which coalitions are stable, how privacy choices shape equilibrium outcomes, and how decentralized data sharing compares to a centralized, socially optimal benchmark.
  We provide a comprehensive equilibrium analysis across a broad range of privacy-cost regimes, from decreasing costs (e.g., privacy amplification from pooling data) to increasing costs (e.g., greater exposure to privacy attacks in larger coalitions). We first characterize Nash equilibrium coalitions with endogenous privacy levels and show that equilibria may fail to exist and can be non-monotonic in problem parameters. We also introduce a weaker equilibrium notion called robust equilibrium (that allows more widespread equilibrium existence by equipping existing players in the coalition with the power to prevent or veto external players from joining) and fully characterize such equilibria. Finally, we analyze, for both Nash and robust equilibria, the efficiency relative to the social optimum in terms of social welfare and estimator accuracy. We derive bounds that depend sharply on the number of players, properties of the cost profile and how privacy costs scale with coalition size.
]]></content:encoded>
<pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>It's not a lie if you don't get caught: simplifying reconfiguration in SMR through dirty logs</title>
<link>https://arxiv.org/abs/2602.09441</link>
<guid>https://arxiv.org/abs/2602.09441</guid>
<content:encoded><![CDATA[
arXiv:2602.09441v1 Announce Type: new 
Abstract: Production state-machine replication (SMR) implementations are complex, multi-layered architectures comprising data dissemination, ordering, execution, and reconfiguration components. Existing research consensus protocols rarely discuss reconfiguration. Those that do tightly couple membership changes to a specific algorithm. This prevents the independent upgrade of individual building blocks and forces expensive downtime when transitioning to new protocol implementations. Instead, modularity is essential for maintainability and system evolution in production deployments. We present Gauss, a reconfiguration engine designed to treat consensus protocols as interchangeable modules. By introducing a distinction between a consensus protocol's inner log and a sanitized outer log exposed to the RSM node, Gauss allows engineers to upgrade membership, failure thresholds, and the consensus protocol itself independently and with minimal global downtime. Our initial evaluation on the Rialo blockchain shows that this separation of concerns enables a seamless evolution of the SMR stack across a sequence of diverse protocol implementations.
]]></content:encoded>
<pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improved Approximate Regret for Decentralized Online Continuous Submodular Maximization via Reductions</title>
<link>https://arxiv.org/abs/2602.09502</link>
<guid>https://arxiv.org/abs/2602.09502</guid>
<content:encoded><![CDATA[
arXiv:2602.09502v1 Announce Type: new 
Abstract: To expand the applicability of decentralized online learning, previous studies have proposed several algorithms for decentralized online continuous submodular maximization (D-OCSM) -- a non-convex/non-concave setting with continuous DR-submodular reward functions. However, there exist large gaps between their approximate regret bounds and the regret bounds achieved in the convex setting. Moreover, if focusing on projection-free algorithms, which can efficiently handle complex decision sets, they cannot even recover the approximate regret bounds achieved in the centralized setting. In this paper, we first demonstrate that for D-OCSM over general convex decision sets, these two issues can be addressed simultaneously. Furthermore, for D-OCSM over downward-closed decision sets, we show that the second issue can be addressed while significantly alleviating the first issue. Our key techniques are two reductions from D-OCSM to decentralized online convex optimization (D-OCO), which can exploit D-OCO algorithms to improve the approximate regret of D-OCSM in these two cases, respectively.
]]></content:encoded>
<pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rashomon Sets and Model Multiplicity in Federated Learning</title>
<link>https://arxiv.org/abs/2602.09520</link>
<guid>https://arxiv.org/abs/2602.09520</guid>
<content:encoded><![CDATA[
arXiv:2602.09520v1 Announce Type: new 
Abstract: The Rashomon set captures the collection of models that achieve near-identical empirical performance yet may differ substantially in their decision boundaries. Understanding the differences among these models, i.e., their multiplicity, is recognized as a crucial step toward model transparency, fairness, and robustness, as it reveals decision boundaries instabilities that standard metrics obscure. However, the existing definitions of Rashomon set and multiplicity metrics assume centralized learning and do not extend naturally to decentralized, multi-party settings like Federated Learning (FL). In FL, multiple clients collaboratively train models under a central server's coordination without sharing raw data, which preserves privacy but introduces challenges from heterogeneous client data distribution and communication constraints. In this setting, the choice of a single best model may homogenize predictive behavior across diverse clients, amplify biases, or undermine fairness guarantees. In this work, we provide the first formalization of Rashomon sets in FL.First, we adapt the Rashomon set definition to FL, distinguishing among three perspectives: (I) a global Rashomon set defined over aggregated statistics across all clients, (II) a t-agreement Rashomon set representing the intersection of local Rashomon sets across a fraction t of clients, and (III) individual Rashomon sets specific to each client's local distribution.Second, we show how standard multiplicity metrics can be estimated under FL's privacy constraints. Finally, we introduce a multiplicity-aware FL pipeline and conduct an empirical study on standard FL benchmark datasets. Our results demonstrate that all three proposed federated Rashomon set definitions offer valuable insights, enabling clients to deploy models that better align with their local data, fairness considerations, and practical requirements.
]]></content:encoded>
<pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Designing a Token Economy: Incentives, Governance, and Tokenomics</title>
<link>https://arxiv.org/abs/2602.09608</link>
<guid>https://arxiv.org/abs/2602.09608</guid>
<content:encoded><![CDATA[
arXiv:2602.09608v1 Announce Type: new 
Abstract: In recent years, tokenomic systems, decentralized systems that use cryptographic tokens to represent value and rights, have evolved considerably. Growing complexity in incentive structures has expanded the applicability of blockchain beyond purely transactional use. Existing research predominantly examines token economies within specific use cases, proposes conceptual frameworks, or studies isolated aspects such as governance, incentive design, and tokenomics. However, the literature offers limited empirically grounded, end-to-end guidance that integrates these dimensions into a coherent, step-by-step design approach informed by concrete token-economy development efforts. To address this gap, this paper presents the Token Economy Design Method (TEDM), a design-science artifact that synthesizes stepwise design propositions for token-economy design across incentives, governance, and tokenomics. TEDM is derived through an iterative qualitative synthesis of prior contributions and refined through a co-designed case. The artifact is formatively evaluated via the Currynomics case study and additional expert interviews. Currynomics is an ecosystem that maintains the Redcurry stablecoin, using real estate as the underlying asset. TEDM is positioned as reusable design guidance that facilitates the analysis of foundational requirements of tokenized ecosystems. The specificity of the proposed approach lies in the focus on the socio-technical context of the system and early stages of its design.
]]></content:encoded>
<pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Multi-sig to DLCs: Modern Oracle Designs on Bitcoin</title>
<link>https://arxiv.org/abs/2602.09822</link>
<guid>https://arxiv.org/abs/2602.09822</guid>
<content:encoded><![CDATA[
arXiv:2602.09822v1 Announce Type: new 
Abstract: Unlike Ethereum, which was conceived as a general-purpose smart-contract platform, Bitcoin was designed primarily as a transaction ledger for its native currency, which limits programmability for conditional applications. This constraint is particularly evident when considering oracles, mechanisms that enable Bitcoin contracts to depend on exogenous events. This paper investigates whether new oracle designs have emerged for Bitcoin Layer 1 since the 2015 transition to the Ethereum smart contracts era and whether subsequent Bitcoin improvement proposals have expanded oracles' implementability. Using Scopus and Web of Science searches, complemented by Google Scholar to capture protocol proposals, we observe that the indexed academic coverage remains limited, and many contributions circulate outside journal venues. Within the retrieved corpus, the main post-2015 shift is from multisig-style, which envisioned oracles as co-signers, toward attestation-based designs, mainly represented by Discreet Log Contracts (DLCs), which show stronger Bitcoin community compliance, tool support, and evidence of practical implementations in real-world scenarios such as betting and prediction-market mechanisms.
]]></content:encoded>
<pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Safeguarding Privacy: Privacy-Preserving Detection of Mind Wandering and Disengagement Using Federated Learning in Online Education</title>
<link>https://arxiv.org/abs/2602.09904</link>
<guid>https://arxiv.org/abs/2602.09904</guid>
<content:encoded><![CDATA[
arXiv:2602.09904v1 Announce Type: new 
Abstract: Since the COVID-19 pandemic, online courses have expanded access to education, yet the absence of direct instructor support challenges learners' ability to self-regulate attention and engagement. Mind wandering and disengagement can be detrimental to learning outcomes, making their automated detection via video-based indicators a promising approach for real-time learner support. However, machine learning-based approaches often require sharing sensitive data, raising privacy concerns. Federated learning offers a privacy-preserving alternative by enabling decentralized model training while also distributing computational load. We propose a framework exploiting cross-device federated learning to address different manifestations of behavioral and cognitive disengagement during remote learning, specifically behavioral disengagement, mind wandering, and boredom. We fit video-based cognitive disengagement detection models using facial expressions and gaze features. By adopting federated learning, we safeguard users' data privacy through privacy-by-design and introduce a novel solution with the potential for real-time learner support. We further address challenges posed by eyeglasses by incorporating related features, enhancing overall model performance. To validate the performance of our approach, we conduct extensive experiments on five datasets and benchmark multiple federated learning algorithms. Our results show great promise for privacy-preserving educational technologies promoting learner engagement.
]]></content:encoded>
<pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Context-Driven Approach for Co-Auditing Smart Contracts with The Support of GPT-4 code interpreter</title>
<link>https://arxiv.org/abs/2406.18075</link>
<guid>https://arxiv.org/abs/2406.18075</guid>
<content:encoded><![CDATA[
arXiv:2406.18075v2 Announce Type: replace 
Abstract: The surge in the adoption of smart contracts necessitates rigorous auditing to ensure their security and reliability. Manual auditing, although comprehensive, is time-consuming and heavily reliant on the auditor's expertise. With the rise of Large Language Models (LLMs), there is growing interest in leveraging them to assist auditors in the auditing process (co-auditing). However, the effectiveness of LLMs in smart contract co-auditing is contingent upon the design of the input prompts, especially in terms of context description and code length. This paper introduces a novel context-driven prompting technique for smart contract co-auditing. Our approach employs three techniques for context scoping and augmentation, encompassing code scoping to chunk long code into self-contained code segments based on code inter-dependencies, assessment scoping to enhance context description based on the target assessment goal, thereby limiting the search space, and reporting scoping to force a specific format for the generated response. Through empirical evaluations on publicly available vulnerable contracts, our method demonstrated a detection rate of 96\% for vulnerable functions, outperforming the native prompting approach, which detected only 53\%. To assess the reliability of our prompting approach, manual analysis of the results was conducted by expert auditors from our partner, Quantstamp, a world-leading smart contract auditing company. The experts' analysis indicates that, in unlabeled datasets, our proposed approach enhances the proficiency of the GPT-4 code interpreter in detecting vulnerabilities.
]]></content:encoded>
<pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blockchain Application in Metaverse: A Review</title>
<link>https://arxiv.org/abs/2504.11730</link>
<guid>https://arxiv.org/abs/2504.11730</guid>
<content:encoded><![CDATA[
arXiv:2504.11730v2 Announce Type: replace 
Abstract: In recent years, the term Metaverse emerged as one of the most compelling concepts, captivating the interest of international companies such as Tencent, ByteDance, Microsoft, and Facebook. These company recognized the Metaverse as a pivotal element for future success and have since made significant investments in this area. The Metaverse is still in its developmental stages, requiring the integration and advancement of various technologies to bring its vision to life. One of the key technologies associated with the Metaverse is blockchain, known for its decentralization, security, trustworthiness, and ability to manage time-series data. These characteristics align perfectly with the ecosystem of the Metaverse, making blockchain foundational for its security and infrastructure. This paper introduces both blockchain and the Metaverse ecosystem while exploring the application of the blockchain within the Metaverse, including decentralization, consensus mechanisms, hash algorithms, timestamping, smart contracts, distributed storage, distributed ledgers, and non-fungible tokens (NFTs) to provide insights for researchers investigating these topics.
]]></content:encoded>
<pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated EndoViT: Pretraining Vision Transformers via Federated Learning on Endoscopic Image Collections</title>
<link>https://arxiv.org/abs/2504.16612</link>
<guid>https://arxiv.org/abs/2504.16612</guid>
<content:encoded><![CDATA[
arXiv:2504.16612v3 Announce Type: replace 
Abstract: Purpose: Data privacy regulations hinder the creation of generalizable foundation models (FMs) for surgery by preventing multi-institutional data aggregation. This study investigates federated learning (FL) as a privacy-preserving solution to collaboratively train robust surgical FMs. Methods: We introduce Federated EndoViT (FL-EndoViT), a federated framework that validates the Masked Autoencoder (MAE) pretraining strategy in a decentralized surgical setting. To ensure convergence under severe data heterogeneity, the architecture integrates adaptive Sharpness-Aware Minimization (FedSAM). Pretrained on the large-scale Endo700k dataset, FL-EndoViT is evaluated against a centralized baseline on different tasks including scene segmentation, action recognition, and phase recognition. Results: FedSAM is critical for successful pretraining, overcoming the convergence failures of standard federated methods. The resulting FL-EndoViT performs comparably to its centralized counterpart, with significant advantages in data-scarce, high-resolution segmentation and generalization to new surgical events. We also establish that full, end-to-end fine-tuning is necessary for optimal performance. Conclusion: This work validates FL with adaptive optimization as a viable paradigm for creating robust, privacy-preserving surgical FMs. Our findings provide a scalable framework for collaborative Surgical Data Science and underscore the optimizer's critical role in handling data heterogeneity. Future work should explore video-based models to incorporate spatiotemporal dynamics.
]]></content:encoded>
<pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collaborative-Online-Learning-Enabled Distributionally Robust Motion Control for Multi-Robot Systems</title>
<link>https://arxiv.org/abs/2508.17173</link>
<guid>https://arxiv.org/abs/2508.17173</guid>
<content:encoded><![CDATA[
<div> COOL<br /><br />:<br />1. COOL<br />2. <br />3. COOL<br />4. <br />5.  <div>
arXiv:2508.17173v2 Announce Type: replace-cross 
Abstract: This paper develops a novel COllaborative-Online-Learning (COOL)-enabled motion control framework for multi-robot systems to avoid collision amid randomly moving obstacles whose motion distributions are partially observable through decentralized data streams. To address the notable challenge of data acquisition due to occlusion, a COOL approach based on the Dirichlet process mixture model is proposed to efficiently extract motion distribution information by exchanging among robots selected learning structures. By leveraging the fine-grained local-moment information learned through COOL, a data-stream-driven ambiguity set for obstacle motion is constructed. We then introduce a novel ambiguity set propagation method, which theoretically admits the derivation of the ambiguity sets for obstacle positions over the entire prediction horizon by utilizing obstacle current positions and the ambiguity set for obstacle motion. Additionally, we develop a compression scheme with its safety guarantee to automatically adjust the complexity and granularity of the ambiguity set by aggregating basic ambiguity sets that are close in a measure space, thereby striking an attractive trade-off between control performance and computation time. Then the probabilistic collision-free trajectories are generated through distributionally robust optimization problems. The distributionally robust obstacle avoidance constraints based on the compressed ambiguity set are equivalently reformulated by deriving separating hyperplanes through tractable semi-definite programming. Finally, we establish the probabilistic collision avoidance guarantee and the long-term tracking performance guarantee for the proposed framework. The numerical simulations are used to demonstrate the efficacy and superiority of the proposed approach compared with state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>VertCoHiRF: Decentralized Vertical Clustering Beyond k-means</title>
<link>https://arxiv.org/abs/2602.07279</link>
<guid>https://arxiv.org/abs/2602.07279</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. VFLVertCoHiRF<br />2. <br />3. medoids<br />4. ID<br />5. Cluster Fusion HierarchyCFH <div>
arXiv:2602.07279v1 Announce Type: new 
Abstract: Vertical Federated Learning (VFL) enables collaborative analysis across parties holding complementary feature views of the same samples, yet existing approaches are largely restricted to distributed variants of $k$-means, requiring centralized coordination or the exchange of feature-dependent numerical statistics, and exhibiting limited robustness under heterogeneous views or adversarial behavior. We introduce VertCoHiRF, a fully decentralized framework for vertical federated clustering based on structural consensus across heterogeneous views, allowing each agent to apply a base clustering method adapted to its local feature space in a peer-to-peer manner. Rather than exchanging feature-dependent statistics or relying on noise injection for privacy, agents cluster their local views independently and reconcile their proposals through identifier-level consensus. Consensus is achieved via decentralized ordinal ranking to select representative medoids, progressively inducing a shared hierarchical clustering across agents. Communication is limited to sample identifiers, cluster labels, and ordinal rankings, providing privacy by design while supporting overlapping feature partitions and heterogeneous local clustering methods, and yielding an interpretable shared Cluster Fusion Hierarchy (CFH) that captures cross-view agreement at multiple resolutions.We analyze communication complexity and robustness, and experiments demonstrate competitive clustering performance in vertical federated settings.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPECA: Specification-to-Checklist Agentic Auditing for Multi-Implementation Systems -- A Case Study on Ethereum Clients</title>
<link>https://arxiv.org/abs/2602.07513</link>
<guid>https://arxiv.org/abs/2602.07513</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. SPECA<br />2. FusakaSPECA11541776.5%<br />3. 3756.8%<br />4. /57.1%28.6%14.3%<br />5. 27.3%96%514940 <div>
arXiv:2602.07513v1 Announce Type: new 
Abstract: Multi-implementation systems are increasingly audited against natural-language specifications. Differential testing scales well when implementations disagree, but it provides little signal when all implementations converge on the same incorrect interpretation of an ambiguous requirement. We present SPECA, a Specification-to-Checklist Auditing framework that turns normative requirements into checklists, maps them to implementation locations, and supports cross-implementation reuse.
  We instantiate SPECA in an in-the-wild security audit contest for the Ethereum Fusaka upgrade, covering 11 production clients. Across 54 submissions, 17 were judged valid by the contest organizers. Cross-implementation checks account for 76.5 percent (13 of 17) of valid findings, suggesting that checklist-derived one-to-many reuse is a practical scaling mechanism in multi-implementation audits. To understand false positives, we manually coded the 37 invalid submissions and find that threat model misalignment explains 56.8 percent (21 of 37): reports that rely on assumptions about trust boundaries or scope that contradict the audit's rules. We detected no High or Medium findings in the V1 deployment; misses concentrated in specification details and implicit assumptions (57.1 percent), timing and concurrency issues (28.6 percent), and external library dependencies (14.3 percent). Our improved agent, evaluated against the ground truth of a competitive audit, achieved a strict recall of 27.3 percent on high-impact vulnerabilities, placing it in the top 4 percent of human auditors and outperforming 49 of 51 contestants on critical issues. These results, though from a single deployment, suggest that early, explicit threat modeling is essential for reducing false positives and focusing agentic auditing effort. The agent-driven process enables expert validation and submission in about 40 minutes on average.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoK: Credential-Based Trust Management in Decentralized Ledger Systems</title>
<link>https://arxiv.org/abs/2602.07572</link>
<guid>https://arxiv.org/abs/2602.07572</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. DTMS  
2. DTMS  
3. DTMS  
4.   
5.  <div>
arXiv:2602.07572v1 Announce Type: new 
Abstract: Trust management systems (TMS) are crucial for managing trust in distributed environments. The rise of decentralized systems and blockchain has sparked interest in credential-based decentralized trust management systems (DTMS). This paper bridges the gap between theory and practice through a systematic review of credential-based DTMS. We analyze existing DTMS solutions through multiple dimensions, including their architectural designs, credential mechanisms, and trust evaluation models. Our survey provides a detailed taxonomy of credential-based DTMS approaches and establishes comprehensive evaluation criteria for assessing DTMS implementations. Through extensive analysis of current systems and implementations, we identify critical challenges and promising research directions in the field. Our examination offers valuable insights for researchers and practitioners working on DTMS, particularly in areas such as access control, reputation systems, and blockchain-based trust frameworks.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge Graphs-Driven Intelligence for Distributed Decision Systems</title>
<link>https://arxiv.org/abs/2602.07614</link>
<guid>https://arxiv.org/abs/2602.07614</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. <br />3. GraphSAGE<br />4. <br />5.  <div>
arXiv:2602.07614v1 Announce Type: new 
Abstract: Modern distributed decision-making systems face significant challenges arising from data heterogeneity, dynamic environments, and the need for decentralized coordination. This paper introduces the Knowledge Sharing paradigm as an innovative approach that uses the semantic richness of Knowledge Graphs (KGs) and the representational power of Graph Embeddings (GEs) to achieve decentralized intelligence. Our architecture empowers individual nodes to locally construct semantic representations of their operational context, iteratively aggregating embeddings through neighbor-based exchanges using GraphSAGE. This iterative local aggregation process results in a dynamically evolving global semantic abstraction called Knowledge Map, enabling coordinated decision-making without centralized control. To validate our approach, we conduct extensive experiments under a distributed resource orchestration use case. We simulate different network topologies and node workloads, analyzing the local semantic drift of individual nodes. Experimental results confirm that our distributed knowledge-sharing mechanism effectively maintains semantic coherence and adaptability, making it suitable for complex and dynamic environments such as Edge Computing, IoT, and multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wireless Streamlet: A Spectrum-Aware and Cognitive Consensus Protocol for Edge IoT</title>
<link>https://arxiv.org/abs/2602.07630</link>
<guid>https://arxiv.org/abs/2602.07630</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. BFTWireless Streamlet  
2. CALECSI  
3. TDMA  
4. State ChainData Chain  
5. Wireless Streamlet <div>
arXiv:2602.07630v1 Announce Type: new 
Abstract: Blockchain offers a decentralized trust framework for the Internet of Things (IoT), yet deploying consensus in spectrum-congested and dynamic wireless edge IoT networks faces fundamental obstacles: traditional BFT protocols are spectrum-ignorant, leading to inefficient resource utilization and fragile progress under time-varying interference. This paper presents \textit{Wireless Streamlet}, a spectrum-aware and cognitive consensus protocol tailored for wireless edge IoT. Building on Streamlet's streamlined structure, we introduce a \textit{Channel-Aware Leader Election (CALE)} mechanism. CALE serves as a verifiable cross-layer cognitive engine that leverages receiver-measured channel state information (CSI) piggybacked in signed votes to derive Byzantine-robust connectivity scores from notarization certificates, and deterministically selects a unique weighted leader per epoch from finalized history, thereby improving proposal dissemination reliability under deep fading. Complementing this cognitive adaptation, Wireless Streamlet exploits the single-hop broadcast medium and a deterministic TDMA voting schedule to achieve linear per-epoch on-air transmissions (slot complexity), ensuring deterministic spectral access. To address the communication-storage trade-off, we further propose a coded dual-chain architecture that decouples header-only consensus (State Chain) from payload data (Data Chain). By employing erasure coding and on-chain integrity commitments, the system minimizes redundant spectrum usage for data retrieval while ensuring availability. Experiments show that Wireless Streamlet achieves higher throughput and lower confirmation latency than representative baselines in lossy environments, while substantially reducing per-node storage, demonstrating the efficacy of integrating cognitive sensing into consensus logic.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>ONTrust: A Reference Ontology of Trust</title>
<link>https://arxiv.org/abs/2602.07662</link>
<guid>https://arxiv.org/abs/2602.07662</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1.   
2.   
3. ONTrustUFOOntoUML  
4. ONTrustAI  
5.  <div>
arXiv:2602.07662v1 Announce Type: new 
Abstract: Trust has stood out more than ever in the light of recent innovations. Some examples are advances in artificial intelligence that make machines more and more humanlike, and the introduction of decentralized technologies (e.g. blockchains), which creates new forms of (decentralized) trust. These new developments have the potential to improve the provision of products and services, as well as to contribute to individual and collective well-being. However, their adoption depends largely on trust. In order to build trustworthy systems, along with defining laws, regulations and proper governance models for new forms of trust, it is necessary to properly conceptualize trust, so that it can be understood both by humans and machines. This paper is the culmination of a long-term research program of providing a solid ontological foundation on trust, by creating reference conceptual models to support information modeling, automated reasoning, information integration and semantic interoperability tasks. To address this, a Reference Ontology of Trust (ONTrust) was developed, grounded on the Unified Foundational Ontology and specified in OntoUML, which has been applied in several initiatives, to demonstrate, for example, how it can be used for conceptual modeling and enterprise architecture design, for language evaluation and (re)design, for trust management, for requirements engineering, and for trustworthy artificial intelligence (AI) in the context of affective Human-AI teaming. ONTrust formally characterizes the concept of trust and its different types, describes the different factors that can influence trust, as well as explains how risk emerges from trust relations. To illustrate the working of ONTrust, the ontology is applied to model two case studies extracted from the literature.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Learning with Profile Mapping under Distribution Shifts and Drifts</title>
<link>https://arxiv.org/abs/2602.07671</link>
<guid>https://arxiv.org/abs/2602.07671</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. FL<br />2. <br />3. Feroma<br />4. Feroma<br />5. Feroma612FedAvg <div>
arXiv:2602.07671v1 Announce Type: new 
Abstract: Federated Learning (FL) enables decentralized model training across clients without sharing raw data, but its performance degrades under real-world data heterogeneity. Existing methods often fail to address distribution shift across clients and distribution drift over time, or they rely on unrealistic assumptions such as known number of client clusters and data heterogeneity types, which limits their generalizability. We introduce Feroma, a novel FL framework that explicitly handles both distribution shift and drift without relying on client or cluster identity. Feroma builds on client distribution profiles-compact, privacy-preserving representations of local data-that guide model aggregation and test-time model assignment through adaptive similarity-based weighting. This design allows Feroma to dynamically select aggregation strategies during training, ranging from clustered to personalized, and deploy suitable models to unseen, and unlabeled test clients without retraining, online adaptation, or prior knowledge on clients' data. Extensive experiments show that compared to 10 state-of-the-art methods, Feroma improves performance and stability under dynamic data heterogeneity conditions-an average accuracy gain of up to 12 percentage points over the best baselines across 6 benchmarks-while maintaining computational and communication overhead comparable to FedAvg. These results highlight that distribution-profile-based aggregation offers a practical path toward robust FL under both data distribution shifts and drifts.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoLF: Learning Consistent Leader-Follower Policies for Vision-Language-Guided Multi-Robot Cooperative Transport</title>
<link>https://arxiv.org/abs/2602.07776</link>
<guid>https://arxiv.org/abs/2602.07776</guid>
<content:encoded><![CDATA[
<div> --<br /><br />:  
1. -  
2. -  
3. Consistent Leader-FollowerCoLF  
4. CoLF  
5. CTDE <div>
arXiv:2602.07776v1 Announce Type: new 
Abstract: In this study, we address vision-language-guided multi-robot cooperative transport, where each robot grounds natural-language instructions from onboard camera observations. A key challenge in this decentralized setting is perceptual misalignment across robots, where viewpoint differences and language ambiguity can yield inconsistent interpretations and degrade cooperative transport. To mitigate this problem, we adopt a dependent leader-follower design, where one robot serves as the leader and the other as the follower. Although such a leader-follower structure appears straightforward, learning with independent and symmetric agents often yields symmetric or unstable behaviors without explicit inductive biases. To address this challenge, we propose Consistent Leader-Follower (CoLF), a multi-agent reinforcement learning (MARL) framework for stable leader-follower role differentiation. CoLF consists of two key components: (1) an asymmetric policy design that induces leader-follower role differentiation, and (2) a mutual-information-based training objective that maximizes a variational lower bound, encouraging the follower to predict the leader's action from its local observation. The leader and follower policies are jointly optimized under the centralized training and decentralized execution (CTDE) framework to balance task execution and consistent cooperative behaviors. We validate CoLF in both simulation and real-robot experiments using two quadruped robots. The demonstration video is available at https://sites.google.com/view/colf/.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Talk, Judge, Cooperate: Gossip-Driven Indirect Reciprocity in Self-Interested LLM Agents</title>
<link>https://arxiv.org/abs/2602.07777</link>
<guid>https://arxiv.org/abs/2602.07777</guid>
<content:encoded><![CDATA[
<div> LLM<br /><br />:<br />1. LLM<br />2. ALIGN<br />3. ALIGN<br />4. LLM<br />5. LLM <div>
arXiv:2602.07777v1 Announce Type: new 
Abstract: Indirect reciprocity, which means helping those who help others, is difficult to sustain among decentralized, self-interested LLM agents without reliable reputation systems. We introduce Agentic Linguistic Gossip Network (ALIGN), an automated framework where agents strategically share open-ended gossip using hierarchical tones to evaluate trustworthiness and coordinate social norms. We demonstrate that ALIGN consistently improves indirect reciprocity and resists malicious entrants by identifying and ostracizing defectors without changing intrinsic incentives. Notably, we find that stronger reasoning capabilities in LLMs lead to more incentive-aligned cooperation, whereas chat models often over-cooperate even when strategically suboptimal. These results suggest that leveraging LLM reasoning through decentralized gossip is a promising path for maintaining social welfare in agentic ecosystems. Our code is available at https://github.com/shuhui-zhu/ALIGN.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>ICBAC: an Intelligent Contract-Based Access Control framework for supply chain management by integrating blockchain and federated learning</title>
<link>https://arxiv.org/abs/2602.08014</link>
<guid>https://arxiv.org/abs/2602.08014</guid>
<content:encoded><![CDATA[
<div>     <br /><br />:<br />1. ICBAC<br />2. ICBACHyperledger FabricFL<br />3. AI<br />4. <br />5. ICBACIIDIID <div>
arXiv:2602.08014v1 Announce Type: new 
Abstract: This paper addresses the critical challenge of access control in modern supply chains, which operate across multiple independent and competing organizations. Existing access control is static and centralized, unable to adapt to insider threats or evolving contexts. Blockchain improves decentralization but lacks behavioral intelligence, while centralized machine learning for anomaly detection requires aggregating sensitive data, violating privacy.
  The proposed solution is ICBAC, an intelligent contract-based access control framework. It integrates permissioned blockchain (Hyperledger Fabric) with federated learning (FL). Built on Fabric, ICBAC uses a multi-channel architecture and three smart contracts for asset management, baseline access control, and dynamic revocation. To counter insider misuse, each channel deploys an AI agent that monitors activity and dynamically restricts access for anomalies. Federated learning allows these agents to collaboratively improve detection models without sharing raw data.
  For heterogeneous, competitive environments, ICBAC introduces a game-theoretic client selection mechanism using hedonic coalition formation. This enables supply chains to form stable, strategy-proof FL coalitions via preference-based selection without disclosing sensitive criteria. Extensive experiments on a Fabric testbed with a real-world dataset show ICBAC achieves blockchain performance comparable to static frameworks and provides effective anomaly detection under IID and non-IID data with zero raw-data sharing. ICBAC thus offers a practical, scalable solution for dynamic, privacy-preserving access control in decentralized supply chains.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Rise of Sparse Mixture-of-Experts:A Survey from Algorithmic Foundations to Decentralized Architectures and Vertical Domain Applications</title>
<link>https://arxiv.org/abs/2602.08019</link>
<guid>https://arxiv.org/abs/2602.08019</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. MoE<br />2. <br />3. MoE<br />4. MoE<br />5. MoE <div>
arXiv:2602.08019v1 Announce Type: new 
Abstract: The sparse Mixture of Experts(MoE) architecture has evolved as a powerful approach for scaling deep learning models to more parameters with comparable computation cost. As an important branch of large language model(LLM), MoE model only activate a subset of experts based on a routing network. This sparse conditional computation mechanism significantly improves computational efficiency, paving a promising path for greater scalability and cost-efficiency. It not only enhance downstream applications such as natural language processing, computer vision, and multimodal in various horizontal domains, but also exhibit broad applicability across vertical domains. Despite the growing popularity and application of MoE models across various domains, there lacks a systematic exploration of recent advancements of MoE in many important fields. Existing surveys on MoE suffer from limitations such as lack coverage or none extensively exploration of key areas. This survey seeks to fill these gaps. In this paper, Firstly, we examine the foundational principles of MoE, with an in-depth exploration of its core components-the routing network and expert network. Subsequently, we extend beyond the centralized paradigm to the decentralized paradigm, which unlocks the immense untapped potential of decentralized infrastructure, enables democratization of MoE development for broader communities, and delivers greater scalability and cost-efficiency. Furthermore we focus on exploring its vertical domain applications. Finally, we also identify key challenges and promising future research directions. To the best of our knowledge, this survey is currently the most comprehensive review in the field of MoE. We aim for this article to serve as a valuable resource for both researchers and practitioners, enabling them to navigate and stay up-to-date with the latest advancements.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfiCoEvalChain: A Blockchain-Based Decentralized Framework for Collaborative LLM Evaluation</title>
<link>https://arxiv.org/abs/2602.08229</link>
<guid>https://arxiv.org/abs/2602.08229</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. 1.670.91<br />2. <br />3. <br />4. 0.28<br />5.  <div>
arXiv:2602.08229v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) demands increasingly reliable evaluation, yet current centralized evaluation suffers from opacity, overfitting, and hardware-induced variance. Our empirical analysis reveals an alarming inconsistency in existing evaluations: the standard deviation across ten repeated runs of a single model on HumanEval (1.67) actually exceeds the performance gap among the top-10 models on the official leaderboard (0.91), rendering current rankings statistically precarious. To mitigate these instabilities, we propose a decentralized evaluation framework that enables hardware and parameter diversity through large-scale benchmarking across heterogeneous compute nodes. By leveraging the blockchain-based protocol, the framework incentivizes global contributors to act as independent validators, using a robust reward system to ensure evaluation integrity and discourage dishonest participation. This collective verification transforms evaluation from a "centralized black box" into a "decentralized endorsement" where multi-party consensus and diverse inference environments yield a more stable, representative metric. Experimental results demonstrate that the decentralized evaluation framework reduces the standard deviation across ten runs on the same model to 0.28. This significant improvement over conventional frameworks ensures higher statistical confidence in model rankings. We have completely implemented this platform and will soon release it to the community.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trust-Based Incentive Mechanisms in Semi-Decentralized Federated Learning Systems</title>
<link>https://arxiv.org/abs/2602.08290</link>
<guid>https://arxiv.org/abs/2602.08290</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1.   
2.   
3.   
4.   
5.  <div>
arXiv:2602.08290v1 Announce Type: new 
Abstract: In federated learning (FL), decentralized model training allows multi-ple participants to collaboratively improve a shared machine learning model without exchanging raw data. However, ensuring the integrity and reliability of the system is challenging due to the presence of potentially malicious or faulty nodes that can degrade the model's performance. This paper proposes a novel trust-based incentive mechanism designed to evaluate and reward the quality of contributions in FL systems. By dynamically assessing trust scores based on fac-tors such as data quality, model accuracy, consistency, and contribution fre-quency, the system encourages honest participation and penalizes unreliable or malicious behavior. These trust scores form the basis of an incentive mechanism that rewards high-trust nodes with greater participation opportunities and penal-ties for low-trust participants. We further explore the integration of blockchain technology and smart contracts to automate the trust evaluation and incentive distribution processes, ensuring transparency and decentralization. Our proposed theoretical framework aims to create a more robust, fair, and transparent FL eco-system, reducing the risks posed by untrustworthy participants.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Intent-Based Multi-Robot Task Planner with LLM Oracles on Hyperledger Fabric</title>
<link>https://arxiv.org/abs/2602.08421</link>
<guid>https://arxiv.org/abs/2602.08421</guid>
<content:encoded><![CDATA[
<div> LLMHyperledger Fabric<br /><br />:<br />1. LLMs<br />2. LLMLLM<br />3. <br />4. Hyperledger Fabric<br />5. SkillChain-RTD <div>
arXiv:2602.08421v1 Announce Type: new 
Abstract: Large language models (LLMs) have opened new opportunities for transforming natural language user intents into executable actions. This capability enables embodied AI agents to perform complex tasks, without involvement of an expert, making human-robot interaction (HRI) more convenient. However these developments raise significant security and privacy challenges such as self-preferencing, where a single LLM service provider dominates the market and uses this power to promote their own preferences. LLM oracles have been recently proposed as a mechanism to decentralize LLMs by executing multiple LLMs from different vendors and aggregating their outputs to obtain a more reliable and trustworthy final result. However, the accuracy of these approaches highly depends on the aggregation method. The current aggregation methods mostly use semantic similarity between various LLM outputs, not suitable for robotic task planning, where the temporal order of tasks is important. To fill the gap, we propose an LLM oracle with a new aggregation method for robotic task planning. In addition, we propose a decentralized multi-robot infrastructure based on Hyperledger Fabric that can host the proposed oracle. The proposed infrastructure enables users to express their natural language intent to the system, which then can be decomposed into subtasks. These subtasks require coordinating different robots from different vendors, while enforcing fine-grained access control management on the data. To evaluate our methodology, we created the SkillChain-RTD benchmark made it publicly available. Our experimental results demonstrate the feasibility of the proposed architecture, and the proposed aggregation method outperforms other aggregation methods currently in use.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>RIFLE: Robust Distillation-based FL for Deep Model Deployment on Resource-Constrained IoT Networks</title>
<link>https://arxiv.org/abs/2602.08446</link>
<guid>https://arxiv.org/abs/2602.08446</guid>
<content:encoded><![CDATA[
<div> TinyML<br /><br />:  
1. IoTRIFLE  
2. RIFLElogitIoTVGG-19ResNet18  
3. KL  
4. MNISTCIFAR-10CIFAR-100RIFLEIID87.5%62.5%28.3%  
5. RIFLEVGG19IoT6001.39 <div>
arXiv:2602.08446v1 Announce Type: new 
Abstract: Federated learning (FL) is a decentralized learning paradigm widely adopted in resource-constrained Internet of Things (IoT) environments. These devices, typically relying on TinyML models, collaboratively train global models by sharing gradients with a central server while preserving data privacy. However, as data heterogeneity and task complexity increase, TinyML models often become insufficient to capture intricate patterns, especially under extreme non-IID (non-independent and identically distributed) conditions. Moreover, ensuring robustness against malicious clients and poisoned updates remains a major challenge. Accordingly, this paper introduces RIFLE - a Robust, distillation-based Federated Learning framework that replaces gradient sharing with logit-based knowledge transfer. By leveraging a knowledge distillation aggregation scheme, RIFLE enables the training of deep models such as VGG-19 and Resnet18 within constrained IoT systems. Furthermore, a Kullback-Leibler (KL) divergence-based validation mechanism quantifies the reliability of client updates without exposing raw data, achieving high trust and privacy preservation simultaneously. Experiments on three benchmark datasets (MNIST, CIFAR-10, and CIFAR-100) under heterogeneous non-IID conditions demonstrate that RIFLE reduces false-positive detections by up to 87.5%, enhances poisoning attack mitigation by 62.5%, and achieves up to 28.3% higher accuracy compared to conventional federated learning baselines within only 10 rounds. Notably, RIFLE reduces VGG19 training time from over 600 days to just 1.39 hours on typical IoT devices (0.3 GFLOPS), making deep learning practical in resource-constrained networks.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Spatial Reuse Optimization in Wi-Fi: An Internal Regret Minimization Approach</title>
<link>https://arxiv.org/abs/2602.08456</link>
<guid>https://arxiv.org/abs/2602.08456</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. SRIEEE 802.11<br />2. BSSCST<br />3. CENE<br />4. <br />5. MAPC <div>
arXiv:2602.08456v1 Announce Type: new 
Abstract: Spatial Reuse (SR) is a cost-effective technique for improving spectral efficiency in dense IEEE 802.11 deployments by enabling simultaneous transmissions. However, the decentralized optimization of SR parameters -- transmission power and Carrier Sensing Threshold (CST) -- across different Basic Service Sets (BSSs) is challenging due to the lack of global state information. In addition, the concurrent operation of multiple agents creates a highly non-stationary environment, often resulting in suboptimal global configurations (e.g., using the maximum possible transmission power by default). To overcome these limitations, this paper introduces a decentralized learning algorithm based on regret-matching, grounded in internal regret minimization. Unlike standard decentralized ``selfish'' approaches that often converge to inefficient Nash Equilibria (NE), internal regret minimization guides competing agents toward Correlated Equilibria (CE), effectively mimicking coordination without explicit communication. Through simulation results, we showcase the superiority of our proposed approach and its ability to reach near-optimal global performance. These results confirm the not-yet-unleashed potential of scalable decentralized solutions and question the need for the heavy signaling overheads and architectural complexity associated with emerging centralized solutions like Multi-Access Point Coordination (MAPC).
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Raw Data to Shared 3D Semantics: Task-Oriented Communication for Multi-Robot Collaboration</title>
<link>https://arxiv.org/abs/2602.08624</link>
<guid>https://arxiv.org/abs/2602.08624</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. 3D  
2. 3D  
3. Pixel Difference NetworkPiDiNet  
4. 3D  
5. 858.6 Mb4.0 Mb2001,054281 <div>
arXiv:2602.08624v1 Announce Type: new 
Abstract: Multi-robot systems (MRS) rely on exchanging raw sensory data to cooperate in complex three-dimensional (3D) environments. However, this strategy often leads to severe communication congestion and high transmission latency, significantly degrading collaboration efficiency. This paper proposes a decentralized task-oriented semantic communication framework for multi-robot collaboration in unknown 3D environments. Each robot locally extracts compact, task-relevant semantics using a lightweight Pixel Difference Network (PiDiNet) with geometric processing. It shares only these semantic updates to build a task-sufficient 3D scene representation that supports cooperative perception, navigation, and object transport. Our numerical results show that the proposed method exhibits a dramatic reduction in communication overhead from $858.6$ Mb to $4.0$ Mb (over $200\times$ compression gain) while improving collaboration efficiency by shortening task completion from $1,054$ to $281$ steps.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>HoGS: Homophily-Oriented Graph Synthesis for Local Differentially Private GNN Training</title>
<link>https://arxiv.org/abs/2602.08762</link>
<guid>https://arxiv.org/abs/2602.08762</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. GNNs<br />2. LDP<br />3. HoGSLDPGNN<br />4. HoGSGNN <div>
arXiv:2602.08762v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) have demonstrated remarkable performance in various graph-based machine learning tasks by effectively modeling high-order interactions between nodes. However, training GNNs without protection may leak sensitive personal information in graph data, including links and node features. Local differential privacy (LDP) is an advanced technique for protecting data privacy in decentralized networks. Unfortunately, existing local differentially private GNNs either only preserve link privacy or suffer significant utility loss in the process of preserving link and node feature privacy. In this paper, we propose an effective LDP framework, called HoGS, which trains GNNs with link and feature protection by generating a synthetic graph. Concretely, HoGS first collects the link and feature information of the graph under LDP, and then utilizes the phenomenon of homophily in graph data to reconstruct the graph structure and node features separately, thereby effectively mitigating the negative impact of LDP on the downstream GNN training. We theoretically analyze the privacy guarantee of HoGS and conduct experiments using the generated synthetic graph as input to various state-of-the-art GNN architectures. Experimental results on three real-world datasets show that HoGS significantly outperforms baseline methods in the accuracy of training GNNs.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>ZK-Rollup for Hyperledger Fabric: Architecture and Performance Evaluation</title>
<link>https://arxiv.org/abs/2602.08870</link>
<guid>https://arxiv.org/abs/2602.08870</guid>
<content:encoded><![CDATA[
<div> Layer-2Hyperledger Fabric<br /><br />:<br />1. ZK RollupsLayer-2<br />2. Hyperledger FabricMerkleRollupZK<br />3. Fabric<br />4. Fabric57 TPS470100 TPS7001000<br />5. ZK Rollups <div>
arXiv:2602.08870v1 Announce Type: new 
Abstract: A big challenge posed in blockchain centric platforms is achieving scalability while also preserving user privacy. This report details the design, implementation and evaluation of a Layer-2 scaling solution for Hyperledger Fabric using Zero Knowledge Rollups (ZK Rollups). The proposed architecture introduces an off chain sequencer that accepts transactions immediately and sends them for batching into a Merkle tree based rollup, using ZK proofs to attest to the correctness and verifiability of the entire batch.
  The design aims to decouple transaction ingestion from actual on chain settlements to address Fabric scalability limitations and increase throughput under high load conditions. The baseline architecture in Hyperledger Fabric constrains transaction requests due to endorsement, ordering and validation phases, leading to a throughput of 5 to 7 TPS with an average latency of 4 seconds. Our Layer-2 solution achieves an ingestion throughput of 70 to 100 TPS, leading to an increase of nearly ten times due to the sequencer immediate acceptance of each transaction and reducing client perceived latency by nearly eighty percent to 700 to 1000 milliseconds. This work demonstrates that integrating ZK Rollups in Hyperledger Fabric enhances scalability while not compromising the security guarantees of a permissioned blockchain network.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Coordinate via Quantum Entanglement in Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2602.08965</link>
<guid>https://arxiv.org/abs/2602.08965</guid>
<content:encoded><![CDATA[
<div>     <br /><br />:  
1.   
2.   
3.   
4.   
5. Dec-POMDP <div>
arXiv:2602.08965v1 Announce Type: new 
Abstract: The inability to communicate poses a major challenge to coordination in multi-agent reinforcement learning (MARL). Prior work has explored correlating local policies via shared randomness, sometimes in the form of a correlation device, as a mechanism to assist in decentralized decision-making. In contrast, this work introduces the first framework for training MARL agents to exploit shared quantum entanglement as a coordination resource, which permits a larger class of communication-free correlated policies than shared randomness alone. This is motivated by well-known results in quantum physics which posit that, for certain single-round cooperative games with no communication, shared quantum entanglement enables strategies that outperform those that only use shared randomness. In such cases, we say that there is quantum advantage. Our framework is based on a novel differentiable policy parameterization that enables optimization over quantum measurements, together with a novel policy architecture that decomposes joint policies into a quantum coordinator and decentralized local actors. To illustrate the effectiveness of our proposed method, we first show that we can learn, purely from experience, strategies that attain quantum advantage in single-round games that are treated as black box oracles. We then demonstrate how our machinery can learn policies with quantum advantage in an illustrative multi-agent sequential decision-making problem formulated as a decentralized partially observable Markov decision process (Dec-POMDP).
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smart Contracts in the Real World: A Statistical Exploration of External Data Dependencies</title>
<link>https://arxiv.org/abs/2406.13253</link>
<guid>https://arxiv.org/abs/2406.13253</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. 9,356<br />3. <br />4. 3,6009%249<br />5.  <div>
arXiv:2406.13253v4 Announce Type: replace 
Abstract: Smart contracts with external data are crucial for functionality but pose security and reliability concerns. Statistical and quantitative studies on this interaction are scarce. To address this gap, we analyzed 10,500 smart contracts, retaining 9,356 valid ones after excluding outdated or erroneous ones. We employed code parsing to transform contract code into abstract syntax trees and identified keywords associated with external data dependencies. We conducted a quantitative analysis by comparing these keywords to a reference list. We manually classified the 9,356 valid smart contracts to ascertain their application domains and typical interaction methods with external data. Additionally, we created a database with this data to facilitate research on smart contract dependencies. Moreover, we reviewed over 3,600 security audit reports, manually identifying 249 (approximately 9%) related to external data interactions and categorized their dependencies. We explored the correlation between smart contract complexity and external data dependency to provide insights for their design and auditing processes. These studies aim to enhance the security and reliability of smart contracts and offer practical guidance to developers and auditors.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoK: Blockchain-Based Decentralized AI (DeAI)</title>
<link>https://arxiv.org/abs/2411.17461</link>
<guid>https://arxiv.org/abs/2411.17461</guid>
<content:encoded><![CDATA[
<div> AI<br /><br />:<br />1. AI<br />2. DeAIAI<br />3. DeAISoKAI<br />4. <br />5. DeAI <div>
arXiv:2411.17461v5 Announce Type: replace 
Abstract: Centralization enhances the efficiency of Artificial Intelligence (AI) but also introduces critical challenges, including single points of failure, inherent biases, data privacy risks, and scalability limitations. To address these issues, blockchain-based Decentralized Artificial Intelligence (DeAI) has emerged as a promising paradigm that leverages decentralization and transparency to improve the trustworthiness of AI systems. Despite rapid adoption in industry, the academic community lacks a systematic analysis of DeAI's technical foundations, opportunities, and challenges. This work presents the first Systematization of Knowledge (SoK) on DeAI, offering a formal definition, a taxonomy of existing solutions based on the AI lifecycle, and an in-depth investigation of the roles of blockchain in enabling secure and incentive-compatible collaboration. We further review security risks across the DeAI lifecycle and empirically evaluate representative mitigation techniques. Finally, we highlight open research challenges and future directions for advancing blockchain-based DeAI.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>"ScatSpotter" -- A Dog Poop Detection Dataset</title>
<link>https://arxiv.org/abs/2412.16473</link>
<guid>https://arxiv.org/abs/2412.16473</guid>
<content:encoded><![CDATA[
<div> ScatSpotterDINO<br /><br />:<br />1. ScatSpotter<br />2. /BAN90006000<br />3. /691121<br />4. DINODINO0.690.70<br />5. GitHub <div>
arXiv:2412.16473v2 Announce Type: replace 
Abstract: Small, amorphous waste objects such as biological droppings and microtrash can be difficult to see, especially in cluttered scenes, yet they matter for environmental cleanliness, public health, and autonomous cleanup. We introduce "ScatSpotter": a new dataset of images annotated with polygons around dog feces, collected to train and study object detection and segmentation systems for small potentially camouflaged outdoor waste. We gathered data in mostly urban environments, using "before/after/negative" (BAN) protocol: for a given location, we capture an image with the object present, an image from the same viewpoint after removal, and a nearby negative scene that often contains visually similar confusers. Image collection began in 2020. This paper focuses on two dataset checkpoints from 2025 and 2024. The dataset contains over 9000 images and 6000 polygon annotations. Of the author-captured images we held out 691 for validation and used the rest to train. Via community participation we obtained a 121-image test set that, while small, is independent from author-collected images and provides some generalization confidence across photographers, devices, and locations. Due to its limited size, we report both validation and test results. We explore the difficulty of the dataset using off-the-shelf VIT, MaskRCNN, YOLO-v9, and DINO-v2 models. Zero-shot DINO performs poorly, indicating limited foundational-model coverage of this category. Tuned DINO is the best model with a box-level average precision of 0.69 on a 691-image validation set and 0.7 on the test set. These results establish strong baselines and quantify the remaining difficulty of detecting small, camouflaged waste objects. To support open access to models and data, we compare centralized and decentralized distribution mechanisms and discuss trade-offs for sharing scientific data. Code and project details are hosted on GitHub.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Achieving Unanimous Consensus Through Multi-Agent Deliberation</title>
<link>https://arxiv.org/abs/2504.02128</link>
<guid>https://arxiv.org/abs/2504.02128</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. PoWPoS<br />2. LLMs<br />3. <br />4. <br />5.  <div>
arXiv:2504.02128v2 Announce Type: replace 
Abstract: Blockchain consensus mechanisms have relied on algorithms such as Proof-of-Work (PoW) and Proof-of-Stake (PoS) to ensure network functionality and integrity. However, these approaches struggle with adaptability for decision-making where the opinions of each matter rather than reaching an agreement based on honest majority or weighted consensus. This paper introduces a novel deliberation-based consensus mechanism where Large Language Models (LLMs) act as rational agents engaging in structured discussions to reach a unanimous consensus. By leveraging graded consensus and a multi-round deliberation process, our approach ensures unanimous consensus for definitive problems and graded consensus for prioritized decision problems and policies. We provide a formalization of our system and use it to show that the properties of blockchains are maintained, while also addressing the behavior in terms of adversaries, stalled deliberations, and confidence in consensus. Moreover, experimental results demonstrate system feasibility, showcasing convergence, block properties, and accuracy, which enable deliberative decision-making on blockchain networks.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Hierarchical Reinforcement Learning for Adaptive Traffic Signal Control</title>
<link>https://arxiv.org/abs/2504.05553</link>
<guid>https://arxiv.org/abs/2504.05553</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. MARLATSC  
2. FLFedAvg  
3. HFRLFedAvg  
4. HFRLRLRL  
5. HFRL <div>
arXiv:2504.05553v2 Announce Type: replace 
Abstract: Multi-agent reinforcement learning (MARL) has shown promise for adaptive traffic signal control (ATSC), enabling multiple intersections to coordinate signal timings in real time. However, in large-scale settings, MARL faces constraints due to extensive data sharing and communication requirements. Federated learning (FL) mitigates these challenges by training shared models without directly exchanging raw data, yet traditional FL methods such as FedAvg struggle with highly heterogeneous intersections. Different intersections exhibit varying traffic patterns, demands, and road structures, so performing FedAvg across all agents is inefficient. To address this gap, we propose Hierarchical Federated Reinforcement Learning (HFRL) for ATSC. HFRL employs clustering-based or optimization-based techniques to dynamically group intersections and perform FedAvg independently within groups of intersections with similar characteristics, enabling more effective coordination and scalability than standard FedAvg.Our experiments on synthetic and real-world traffic networks demonstrate that HFRL consistently outperforms decentralized and standard federated RL approaches, and achieves competitive or superior performance compared to centralized RL as network scale and heterogeneity increase, particularly in real-world settings. The method also identifies suitable grouping patterns based on network structure or traffic demand, resulting in a more robust framework for distributed, heterogeneous systems.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Data Behavior to Code Analysis: A Multimodal Study on Security and Privacy Challenges in Blockchain-Based DApp</title>
<link>https://arxiv.org/abs/2504.11860</link>
<guid>https://arxiv.org/abs/2504.11860</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. DApp<br />2. DApp<br />3. DApp<br />4. SolidityLSTMBLSTM-ATT<br />5.  <div>
arXiv:2504.11860v2 Announce Type: replace 
Abstract: The recent proliferation of blockchain-based decentralized applications (DApp) has catalyzed transformative advancements in distributed systems, with extensive deployments observed across financial, entertainment, media, and cybersecurity domains. These trustless architectures, characterized by their decentralized nature and elimination of third-party intermediaries, have garnered substantial institutional attention. Consequently, the escalating security challenges confronting DApp demand rigorous scholarly investigation. This study initiates with a systematic analysis of behavioral patterns derived from empirical DApp datasets, establishing foundational insights for subsequent methodological developments. The principal security vulnerabilities in Ethereum-based smart contracts developed via Solidity are then critically examined. Specifically, reentrancy vulnerability attacks are addressed by formally representing contract logic using highly expressive code fragments. This enables precise source code-level detection via bidirectional long short-term memory networks with attention mechanisms (BLSTM-ATT). Regarding privacy preservation challenges, contemporary solutions are evaluated through dual analytical lenses: identity privacy preservation and transaction anonymity enhancement, while proposing future research trajectories in cryptographic obfuscation techniques.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Giving AI Agents Access to Cryptocurrency and Smart Contracts Creates New Vectors of AI Harm</title>
<link>https://arxiv.org/abs/2507.08249</link>
<guid>https://arxiv.org/abs/2507.08249</guid>
<content:encoded><![CDATA[
<div> AI<br /><br />:  
1. AI  
2.   
3. AItaxonomy  
4.   
5. AI <div>
arXiv:2507.08249v2 Announce Type: replace 
Abstract: There is growing interest in giving AI agents access to cryptocurrencies as well as to the smart contracts that transact them. But doing so, this position paper argues, could lead to formidable new vectors of AI harm. To support this argument, we first examine the unique properties of cryptocurrencies and smart contracts that could give rise to these new vectors of AI harm. Next, we describe each of these new vectors of AI harm in detail, providing a first-of-its-kind taxonomy. Finally, we conclude with a call for more technical research aimed at preventing and mitigating these new vectors of AI , thereby making it safer to endow AI agents with cryptocurrencies and smart contracts.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Fomo3D to Lottery DAPP: Analysis of Ethereum-Based Gambling Applications</title>
<link>https://arxiv.org/abs/2508.12303</link>
<guid>https://arxiv.org/abs/2508.12303</guid>
<content:encoded><![CDATA[
<div> DApp<br /><br />:<br />1. DApp<br />2. DApp<br />3. DApp<br />4. DApp<br />5. DApp <div>
arXiv:2508.12303v2 Announce Type: replace 
Abstract: As blockchain technology advances, Ethereum based gambling decentralized applications (DApps) represent a new paradigm in online gambling. This paper examines the concepts, principles, implementation, and prospects of Ethereum based gambling DApps. First, we outline the concept and operational principles of gambling DApps. These DApps are blockchain based online lottery platforms. They utilize smart contracts to manage the entire lottery process, including issuance, betting, drawing, and prize distribution. Being decentralized, lottery DApps operate without central oversight, unlike traditional lotteries. This ensures fairness and eliminates control by any single entity. Automated smart contract execution further reduces management costs, increases profitability, and enhances game transparency and credibility. Next, we analyze an existing Ethereum based gambling DApp, detailing its technical principles, implementation, operational status, vulnerabilities, and potential solutions. We then elaborate on the implementation of lottery DApps. Smart contracts automate the entire lottery process including betting, drawing, and prize distribution. Although developing lottery DApps requires technical expertise, the expanding Ethereum ecosystem provides growing tools and frameworks, lowering development barriers. Finally, we discuss current limitations and prospects of lottery DApps. As blockchain technology and smart contracts evolve, lottery DApps are positioned to significantly transform the online lottery industry. Advantages like decentralization, automation, and transparency will likely drive broader future adoption.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards</title>
<link>https://arxiv.org/abs/2510.08529</link>
<guid>https://arxiv.org/abs/2510.08529</guid>
<content:encoded><![CDATA[
<div> LLM-as-a-judge<br /><br />:<br />1. LLM<br />2. CoMAS<br />3. CoMASLLM<br />4. <br />5. CoMASSOTA <div>
arXiv:2510.08529v2 Announce Type: replace 
Abstract: Self-evolution is a central research topic in enabling large language model (LLM)-based agents to continually improve their capabilities after pretraining. Recent research has witnessed a transition from reinforcement learning (RL)-free to RL-based methods. Current RL-based methods either rely on dense external reward signals or extract intrinsic reward signals from LLMs themselves. However, these approaches diverge from the self-evolution mechanisms observed in human intelligence, where individuals learn and improve through mutual discussion and collaboration. In this work, we introduce Co-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents to improve autonomously by learning from inter-agent interactions without external supervision. CoMAS generates intrinsic rewards from rich discussion dynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and optimizes each agent's policy through RL, thereby enabling decentralized and scalable co-evolution. Experimental results demonstrate that CoMAS consistently outperforms untrained agents and achieves state-of-the-art performance across most evaluation settings. Ablation studies confirm the necessity of interaction-based reward signals and reveal promising scalability as the number and diversity of agents increase. These findings establish CoMAS as a novel and effective paradigm for self-evolution in LLM-based agents.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blockchain with proof of quantum work</title>
<link>https://arxiv.org/abs/2503.14462</link>
<guid>https://arxiv.org/abs/2503.14462</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1.   
2.   
3. D-Wave  
4. 2025Science  
5.  <div>
arXiv:2503.14462v3 Announce Type: replace-cross 
Abstract: We propose a blockchain architecture in which mining requires a quantum computer. The consensus mechanism is based on proof of quantum work, a quantum-enhanced alternative to traditional proof of work that leverages quantum supremacy to make mining intractable for classical computers. We have refined the blockchain framework to incorporate the probabilistic nature of quantum mechanics, ensuring stability against sampling errors and hardware inaccuracies. To validate our approach, we implemented a prototype blockchain on four D-Wave(TM) quantum annealing processors geographically distributed within North America, demonstrating stable operation across hundreds of thousands of quantum hashing operations. Our experimental protocol follows the same approach used in the recent demonstration of quantum supremacy [King et al. Science 2025], ensuring that classical computers cannot efficiently perform the same computation task. By replacing classical machines with quantum systems for mining, it is possible to significantly reduce the energy consumption and environmental impact traditionally associated with blockchain mining while providing a quantum-safe layer of security. Beyond serving as a proof of concept for a meaningful application of quantum computing, this work highlights the potential for other near-term quantum computing applications using existing technology.
]]></content:encoded>
<pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>$f$-FUM: Federated Unlearning via min--max and $f$-divergence</title>
<link>https://arxiv.org/abs/2602.06187</link>
<guid>https://arxiv.org/abs/2602.06187</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. FL  
2. FL  
3. -/f-  
4.   
5.  <div>
arXiv:2602.06187v1 Announce Type: new 
Abstract: Federated Learning (FL) has emerged as a powerful paradigm for collaborative machine learning across decentralized data sources, preserving privacy by keeping data local. However, increasing legal and ethical demands, such as the "right to be forgotten", and the need to mitigate data poisoning attacks have underscored the urgent necessity for principled data unlearning in FL. Unlike centralized settings, the distributed nature of FL complicates the removal of individual data contributions. In this paper, we propose a novel federated unlearning framework formulated as a min-max optimization problem, where the objective is to maximize an $f$-divergence between the model trained with all data and the model retrained without specific data points, while minimizing the degradation on retained data. Our framework could act like a plugin and be added to almost any federated setup, unlike SOTA methods like (\cite{10269017} which requires model degradation in server, or \cite{khalil2025notfederatedunlearningweight} which requires to involve model architecture and model weights). This formulation allows for efficient approximation of data removal effects in a federated setting. We provide empirical evaluations to show that our method achieves significant speedups over naive retraining, with minimal impact on utility.
]]></content:encoded>
<pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wonderboom -- Efficient, and Censorship-Resilient Signature Aggregation for Million Scale Consensus</title>
<link>https://arxiv.org/abs/2602.06655</link>
<guid>https://arxiv.org/abs/2602.06655</guid>
<content:encoded><![CDATA[
<div> EthereumWonderboom<br /><br />:  
1. 6500  
2.   
3. 15  
4.   
5. Wonderboom32200 <div>
arXiv:2602.06655v1 Announce Type: new 
Abstract: Over the last years, Ethereum has evolved into a public platform that safeguards the savings of hundreds of millions of people and secures more than $650 billion in assets, placing it among the top 25 stock exchanges worldwide in market capitalization, ahead of Singapore, Mexico, and Thailand. As such, the performance and security of the Ethereum blockchain are not only of theoretical interest, but also carry significant global economic implications. At the time of writing, the Ethereum platform is collectively secured by almost one million validators highlighting its decentralized nature and underlining its economic security guarantees. However, due to this large validator set, the protocol takes around 15 minutes to finalize a block which is prohibitively slow for many real world applications. This delay is largely driven by the cost of aggregating and disseminating signatures across a validator set of this scale. Furthermore, as we show in this paper, the existing protocol that is used to aggregate and disseminate the signatures has several shortcomings that can be exploited by adversaries to shift stake proportion from honest to adversarial nodes. In this paper, we introduce Wonderboom, the first million scale aggregation protocol that can efficiently aggregate the signatures of millions of validators in a single Ethereum slot (x32 faster) while offering higher security guarantees than the state of the art protocol used in Ethereum. Furthermore, to evaluate Wonderboom, we implement the first simulation tool that can simulate such a protocol on the million scale and show that even in the worst case Wonderboom can aggregate and verify more than 2 million signatures within a single Ethereum slot.
]]></content:encoded>
<pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Token Coordinate Descent Method for Semi-Decentralized Vertical Federated Learning</title>
<link>https://arxiv.org/abs/2309.09977</link>
<guid>https://arxiv.org/abs/2309.09977</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. MTCD--<br />3. MTCD-<br />4. MTCD$\mathcal{O}(1/T)$<br />5. MTCD- <div>
arXiv:2309.09977v2 Announce Type: replace 
Abstract: Most federated learning (FL) methods use a client-server scheme, where clients communicate only with a central server. However, this scheme is prone to bandwidth bottlenecks at the server and has a single point of failure. In contrast, in a (fully) decentralized approach, clients communicate directly with each other, dispensing with the server and mitigating these issues. Yet, as the client network grows larger and sparser, the convergence of decentralized methods slows down, even failing to converge if the network is disconnected. This work addresses this gap between client-server and decentralized schemes, focusing on the vertical FL setup, where clients hold different features of the same samples. We propose multi-token coordinate descent (MTCD), a flexible semi-decentralized method for vertical FL that can exploit both client-server and client-client links. By selecting appropriate hyperparameters, MTCD recovers the client-sever and decentralized schemes as special cases. In fact, its decentralized instance is itself a novel method of independent interest. Yet, by controlling the degree of dependency on client-server links, MTCD can also explore a spectrum of schemes ranging from client-server to decentralized. We prove that, for sufficiently large batch sizes, MTCD converges at an $\mathcal{O}(1/T)$ rate for nonconvex objectives when the tokens roam across disjoint subsets of clients. To capture the aforementioned drawbacks of the client-server scheme succinctly, we model the relative impact of using client-server versus client-client links as the ratio of their "costs", which depends on the application. This allows us to demonstrate, both analytically and empirically, that by tuning the degree of dependency on the server, the semi-decentralized instances of MTCD can outperform both client-server and decentralized approaches across a range of applications.
]]></content:encoded>
<pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive AI-based Decentralized Resource Management in the Cloud-Edge Continuum</title>
<link>https://arxiv.org/abs/2501.15802</link>
<guid>https://arxiv.org/abs/2501.15802</guid>
<content:encoded><![CDATA[
<div> GNNMARL<br /><br />:  
1.   
2.   
3. GNN  
4. MARL  
5.  <div>
arXiv:2501.15802v2 Announce Type: replace 
Abstract: In the Cloud-Edge Continuum, dynamic infrastructure change and variable workloads complicate efficient resource management. Centralized methods can struggle to adapt, whilst purely decentralized policies lack global oversight. This paper proposes a hybrid framework using Graph Neural Network (GNN) embeddings and collaborative multi-agent reinforcement learning (MARL). Local agents handle neighbourhood-level decisions, and a global orchestrator coordinates system-wide. This work contributes to decentralized application placement strategies with centralized oversight, GNN integration and collaborative MARL for efficient, adaptive and scalable resource management.
]]></content:encoded>
<pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Birthmark Standard: Privacy-Preserving Photo Authentication via Hardware Roots of Trust and Consortium Blockchain</title>
<link>https://arxiv.org/abs/2602.04933</link>
<guid>https://arxiv.org/abs/2602.04933</guid>
<content:encoded><![CDATA[
<div> AI<br /><br />:  
1. AI  
2. C2PA  
3. BirthmarkNUCPRNU1000  
4.   
5. ProVerif4100ms500ms <div>
arXiv:2602.04933v1 Announce Type: new 
Abstract: The rapid advancement of generative AI systems has collapsed the credibility landscape for photographic evidence. Modern image generation models produce photorealistic images undermining the evidentiary foundation upon which journalism and public discourse depend. Existing authentication approaches, such as the Coalition for Content Provenance and Authenticity (C2PA), embed cryptographically signed metadata directly into image files but suffer from two critical failures: technical vulnerability to metadata stripping during social media reprocessing, and structural dependency on corporate-controlled verification infrastructure where commercial incentives may conflict with public interest. We present the Birthmark Standard, an authentication architecture leveraging manufacturing-unique sensor entropy from non-uniformity correction (NUC) maps and PRNU patterns to generate hardware-rooted authentication keys. During capture, cameras create anonymized authentication certificates proving sensor authenticity without exposing device identity via a key table architecture maintaining anonymity sets exceeding 1,000 devices. Authentication records are stored on a consortium blockchain operated by journalism organizations rather than commercial platforms, enabling verification that survives all metadata loss. We formally verify privacy properties using ProVerif, proving observational equivalence for Manufacturer Non-Correlation and Blockchain Observer Non-Identification under Dolev-Yao adversary assumptions. The architecture is validated through prototype implementation using Raspberry Pi 4 hardware, demonstrating the complete cryptographic pipeline. Performance analysis projects camera overhead below 100ms and verification latency below 500ms at scale of one million daily authentications.
]]></content:encoded>
<pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blockchain Technology for Public Services: A Polycentric Governance Synthesis</title>
<link>https://arxiv.org/abs/2602.05109</link>
<guid>https://arxiv.org/abs/2602.05109</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. 20212025<br />3. <br />4. <br />5.  <div>
arXiv:2602.05109v1 Announce Type: new 
Abstract: National governments are increasingly adopting blockchain to enhance transparency, trust, and efficiency in public service delivery. However, evidence on how these technologies are governed across national contexts remains fragmented and overly focused on technical features. Using Polycentric Governance Theory, this study conducts a systematic review of peer-reviewed research published between 2021 and 2025 to examine blockchain-enabled public services and the institutional, organizational, and information-management factors shaping their adoption. Following PRISMA guidelines, we synthesize findings from major digital government and information systems databases to identify key application domains, including digital identity, electronic voting, procurement, and social services, and analyze the governance arrangements underpinning these initiatives. Our analysis reveals that blockchain adoption is embedded within polycentric environments characterized by distributed authority, inter-organizational coordination, and layered accountability. Rather than adopting full decentralization, governments typically utilize hybrid and permissioned designs that allow for selective decentralization alongside centralized oversight, a pattern we conceptualize as "controlled polycentricity." By reframing blockchain as a governance infrastructure that encodes rules for coordination and information-sharing, this study advances digital government theory beyond simple adoption metrics. The findings offer theoretically grounded insights for researchers and practical guidance for policymakers seeking to design and scale sustainable blockchain-enabled public services.
]]></content:encoded>
<pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Among Us: Measuring and Mitigating Malicious Contributions in Model Collaboration Systems</title>
<link>https://arxiv.org/abs/2602.05176</link>
<guid>https://arxiv.org/abs/2602.05176</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:<br />1. LMs<br />2. <br />3. 7.12%7.94%<br />4. <br />5. 95.31% <div>
arXiv:2602.05176v1 Announce Type: new 
Abstract: Language models (LMs) are increasingly used in collaboration: multiple LMs trained by different parties collaborate through routing systems, multi-agent debate, model merging, and more. Critical safety risks remain in this decentralized paradigm: what if some of the models in multi-LLM systems are compromised or malicious? We first quantify the impact of malicious models by engineering four categories of malicious LMs, plug them into four types of popular model collaboration systems, and evaluate the compromised system across 10 datasets. We find that malicious models have a severe impact on the multi-LLM systems, especially for reasoning and safety domains where performance is lowered by 7.12% and 7.94% on average. We then propose mitigation strategies to alleviate the impact of malicious components, by employing external supervisors that oversee model collaboration to disable/mask them out to reduce their influence. On average, these strategies recover 95.31% of the initial performance, while making model collaboration systems fully resistant to malicious models remains an open research question.
]]></content:encoded>
<pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>LTRAS: A Linkable Threshold Ring Adaptor Signature Scheme for Efficient and Private Cross-Chain Transactions</title>
<link>https://arxiv.org/abs/2602.05431</link>
<guid>https://arxiv.org/abs/2602.05431</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1.   
2. UTXO  
3. LTRAS  
4. LTRAS  
5. LTRAS <div>
arXiv:2602.05431v1 Announce Type: new 
Abstract: Despite the advantages of decentralization and immutability, blockchain technology faces significant scalability and throughput limitations, which has prompted the exploration of off-chain solutions like payment channels. Adaptor signatures have been considered a promising primitive for constructing such channels due to their support for atomicity, offering an alternative to traditional hash-timelock contracts. However, standard adaptor signatures may reveal signer identity, raising potential privacy concerns. While ring signatures can mitigate this issue by providing anonymity, they often introduce high communication overhead, particularly in multi-account payment settings commonly used in UTXO-based blockchains like Monero. To address these limitations, we propose a Linkable Threshold Ring Adaptor Signature (LTRAS) scheme, which integrates the conditional binding of adaptor signatures, the multi-account payment of threshold ring signatures, and the linkability for preventing double-spending. The formal definition, security model and concrete construction of LTRAS are provided. We also analyze its security and evaluate its performance through theoretical analysis and experimental implementation. Experimental results demonstrate that our scheme achieve significantly lower computation and communication overhead compared to existing schemes in large ring sizes and multi-account payment scenarios. Finally, we discuss its application in cross-chain atomic swaps, demonstrating its potential for enhancing privacy and efficiency in blockchain transactions.
]]></content:encoded>
<pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ontology-Driven Robotic Specification Synthesis</title>
<link>https://arxiv.org/abs/2602.05456</link>
<guid>https://arxiv.org/abs/2602.05456</guid>
<content:encoded><![CDATA[
<div> Petri<br /><br />:  
1. RSTM2  
2. Petri  
3.   
4. AI  
5. NASA CADRE <div>
arXiv:2602.05456v1 Announce Type: new 
Abstract: This paper addresses robotic system engineering for safety- and mission-critical applications by bridging the gap between high-level objectives and formal, executable specifications. The proposed method, Robotic System Task to Model Transformation Methodology (RSTM2) is an ontology-driven, hierarchical approach using stochastic timed Petri nets with resources, enabling Monte Carlo simulations at mission, system, and subsystem levels. A hypothetical case study demonstrates how the RSTM2 method supports architectural trades, resource allocation, and performance analysis under uncertainty. Ontological concepts further enable explainable AI-based assistants, facilitating fully autonomous specification synthesis. The methodology offers particular benefits to complex multi-robot systems, such as the NASA CADRE mission, representing decentralized, resource-aware, and adaptive autonomous systems of the future.
]]></content:encoded>
<pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sovereign-by-Design A Reference Architecture for AI and Blockchain Enabled Systems</title>
<link>https://arxiv.org/abs/2602.05486</link>
<guid>https://arxiv.org/abs/2602.05486</guid>
<content:encoded><![CDATA[
<div> AI<br /><br />:  
1. AI  
2.   
3.   
4. AI  
5. AI <div>
arXiv:2602.05486v1 Announce Type: new 
Abstract: Digital sovereignty has emerged as a central concern for modern software-intensive systems, driven by the dominance of non-sovereign cloud infrastructures, the rapid adoption of Generative AI, and increasingly stringent regulatory requirements. While existing initiatives address governance, compliance, and security in isolation, they provide limited guidance on how sovereignty can be operationalized at the architectural level. In this paper, we argue that sovereignty must be treated as a first-class architectural property rather than a purely regulatory objective. We introduce a Sovereign Reference Architecture that integrates self-sovereign identity, blockchain-based trust and auditability, sovereign data governance, and Generative AI deployed under explicit architectural control. The architecture explicitly captures the dual role of Generative AI as both a source of governance risk and an enabler of compliance, accountability, and continuous assurance when properly constrained. By framing sovereignty as an architectural quality attribute, our work bridges regulatory intent and concrete system design, offering a coherent foundation for building auditable, evolvable, and jurisdiction-aware AI-enabled systems. The proposed reference architecture provides a principled starting point for future research and practice at the intersection of software architecture, Generative AI, and digital sovereignty.
]]></content:encoded>
<pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Observer-based Control of Multi-agent Systems under STL Specifications</title>
<link>https://arxiv.org/abs/2602.05586</link>
<guid>https://arxiv.org/abs/2602.05586</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:  
1.   
2. STL  
3. k-hopk-hop PPSO  
4. k-hop PPSOSTL  
5. STL <div>
arXiv:2602.05586v1 Announce Type: new 
Abstract: This paper proposes a decentralized controller for large-scale heterogeneous multi-agent systems subject to bounded external disturbances, where agents must satisfy Signal Temporal Logic (STL) specifications requiring cooperation among non-communicating agents. To address the lack of direct communication, we employ a decentralized k-hop Prescribed Performance State Observer (k-hop PPSO) to provide each agent with state estimates of those agents it cannot communicate with. By leveraging the performance bounds on the state estimation errors guaranteed by the k-hop PPSO, we first modify the space robustness of the STL tasks to account for these errors, and then exploit the modified robustness to design a decentralized continuous-time feedback controller that ensures satisfaction of the STL tasks even under worst-case estimation errors. A simulation result is provided to validate the proposed framework.
]]></content:encoded>
<pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-Complexity Characterization of NIST Lightweight Cryptography Finalists</title>
<link>https://arxiv.org/abs/2602.05641</link>
<guid>https://arxiv.org/abs/2602.05641</guid>
<content:encoded><![CDATA[
<div> NIST<br /><br />:<br />1. <br />2. <br />3. NIST<br />4. <br />5.  <div>
arXiv:2602.05641v1 Announce Type: new 
Abstract: Lightweight cryptography is becoming essential as emerging technologies in digital identity systems and Internet of Things verification continue to demand strong cryptographic assurance on devices with limited processing power, memory, and energy resources. As these technologies move into routine use, they demand cryptographic primitives that maintain strong security and deliver predictable performance through clear theoretical models of time complexity. Although NIST's lightweight cryptography project provides empirical evaluations of the ten finalist algorithms, a unified theoretical understanding of their time-complexity behavior remains absent. This work introduces a symbolic model that decomposes each scheme into initialization, data-processing, and finalization phases, enabling formal time-complexity derivation for all ten finalists. The results clarify how design parameters shape computational scaling on constrained mobile and embedded environments. The framework provides a foundation needed to distinguish algorithmic efficiency and guides the choice of primitives capable of supporting security systems in constrained environments.
]]></content:encoded>
<pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedRandom: Sampling Consistent and Accurate Contribution Values in Federated Learning</title>
<link>https://arxiv.org/abs/2602.05693</link>
<guid>https://arxiv.org/abs/2602.05693</guid>
<content:encoded><![CDATA[
<div> FedRandom<br /><br />:  
1.   
2.   
3.   
4. FedRandom  
5. CIFAR-10MNISTCIFAR-100FMNISTFedRandom90% <div>
arXiv:2602.05693v1 Announce Type: new 
Abstract: Federated Learning is a privacy-preserving decentralized approach for Machine Learning tasks. In industry deployments characterized by a limited number of entities possessing abundant data, the significance of a participant's role in shaping the global model becomes pivotal given that participation in a federation incurs costs, and participants may expect compensation for their involvement. Additionally, the contributions of participants serve as a crucial means to identify and address potential malicious actors and free-riders. However, fairly assessing individual contributions remains a significant hurdle. Recent works have demonstrated a considerable inherent instability in contribution estimations across aggregation strategies. While employing a different strategy may offer convergence benefits, this instability can have potentially harming effects on the willingness of participants in engaging in the federation. In this work, we introduce FedRandom, a novel mitigation technique to the contribution instability problem. Tackling the instability as a statistical estimation problem, FedRandom allows us to generate more samples than when using regular FL strategies. We show that these additional samples provide a more consistent and reliable evaluation of participant contributions. We demonstrate our approach using different data distributions across CIFAR-10, MNIST, CIFAR-100 and FMNIST and show that FedRandom reduces the overall distance to the ground truth by more than a third in half of all evaluated scenarios, and improves stability in more than 90% of cases.
]]></content:encoded>
<pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Preserving Dynamic Average Consensus by Masking Reference Signals</title>
<link>https://arxiv.org/abs/2602.05803</link>
<guid>https://arxiv.org/abs/2602.05803</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. DAC<br />2. <br />3. DAC<br />4. DAC<br />5. DAC <div>
arXiv:2602.05803v1 Announce Type: new 
Abstract: In multi-agent systems, dynamic average consensus (DAC) is a decentralized estimation strategy in which a set of agents tracks the average of time-varying reference signals. Because DAC requires exchanging state information with neighbors, attackers may gain access to these states and infer private information. In this paper, we develop a privacy-preserving method that protects each agent's reference signal from external eavesdroppers and honest-but-curious agents while achieving the same convergence accuracy and convergence rate as conventional DAC. Our approach masks the reference signals by having each agent draw a random real number for each neighbor, exchanges that number over an encrypted channel at the initialization, and computes a masking value to form a masked reference. Then the agents run the conventional DAC algorithm using the masked references. Convergence and privacy analyses show that the proposed algorithm matches the convergence properties of conventional DAC while preserving the privacy of the reference signals. Numerical simulations validate the effectiveness of the proposed privacy-preserving DAC algorithm.
]]></content:encoded>
<pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction</title>
<link>https://arxiv.org/abs/2602.06038</link>
<guid>https://arxiv.org/abs/2602.06038</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. <br />3. MM-EQAEQA<br />4. CommCP<br />5. MM-EQACommCP <div>
arXiv:2602.06038v1 Announce Type: new 
Abstract: To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io.
]]></content:encoded>
<pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochastic Generalized Dynamic Games with Coupled Chance Constraints</title>
<link>https://arxiv.org/abs/2501.02279</link>
<guid>https://arxiv.org/abs/2501.02279</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. NP<br />3. SGNESGNE-SGNE<br />4. Lipschitz<br />5. SGNE <div>
arXiv:2501.02279v2 Announce Type: replace 
Abstract: This paper investigates stochastic generalized dynamic games with coupling chance constraints, where agents have incomplete information about uncertainties satisfying a concentration of measure property. This problem, in general, is non-convex and NP-hard. To address this, we propose a convex under-approximation by replacing chance constraints with tightened expected-value constraints, yielding a tractable game. We prove the existence of a stochastic generalized Nash equilibrium (SGNE) in this new game and show that its variational SGNE is an $\boldsymbol{\varepsilon}$-SGNE for the original game, with $\boldsymbol{\varepsilon}$ expressed via the approximation errors and Lagrange multipliers. A semi-decentralized, sampling-based algorithm with time-varying step sizes is developed, requiring no prior knowledge of the uncertainty distribution or expectation evaluations. Unlike existing methods, it avoids step-size tuning based on Lipschitz constants or adaptive rules. Under standard assumptions on the pseudo-gradient, the algorithm converges almost surely to an SGNE.
]]></content:encoded>
<pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differential Privacy Analysis of Decentralized Gossip Averaging under Varying Threat Models</title>
<link>https://arxiv.org/abs/2505.19969</link>
<guid>https://arxiv.org/abs/2505.19969</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. DP<br />2.  gossip <br />3.  DP  T  O(T) <br />4.  <div>
arXiv:2505.19969v3 Announce Type: replace 
Abstract: Achieving differential privacy (DP) guarantees in fully decentralized machine learning is challenging due to the absence of a central aggregator and varying trust assumptions among nodes. We present a framework for DP analysis of decentralized gossip-based averaging algorithms with additive node-level noise, from arbitrary views of nodes in a graph. We present an analytical framework based on a linear systems formulation that accurately characterizes privacy leakage between nodes. Our main contribution is showing that the DP guarantees are those of a Gaussian mechanism, where the growth of the squared sensitivity is asymptotically $O(T)$, where $T$ is the number of training rounds, similarly as in the case of central aggregation. As an application of the sensitivity analysis, we show that the excess risk of decentralized private learning for strongly convex losses is asymptotically similar as in centralized private learning.
]]></content:encoded>
<pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>(Im)possibility of Incentive Design for Challenge-based Blockchain Protocols</title>
<link>https://arxiv.org/abs/2512.20864</link>
<guid>https://arxiv.org/abs/2512.20864</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. TruebitRollup<br />3. <br />4. <br />5.  <div>
arXiv:2512.20864v2 Announce Type: replace 
Abstract: Blockchains offer a decentralized and secure execution environment strong enough to host cryptocurrencies, but the state-replication model makes on-chain computation expensive. To avoid heavy on-chain workloads, systems like Truebit and optimistic rollups use challenge-based protocols, performing computations off-chain and invoking the chain only when challenged. This keeps normal-case costs low and, if at least one honest challenger exists, can catch fraud. What has been less clear is whether honest challengers are actually incentivized and a dishonest proposer is properly damaged under the worst case environment. We build a model with a colluding minority, heterogeneous costs, and three ordering modes. We then ask whether two goals can be met together: honest non-loss and fraud deterrence. Our results are clear: in single-winner designs, the incentive design is impossible or limited in scale. By contrast, in multi-winner designs, we obtain simple, explicit conditions under which both goals hold.
]]></content:encoded>
<pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeXposure-FM: A Time-series, Graph Foundation Model for Credit Exposures and Stability on Decentralized Financial Networks</title>
<link>https://arxiv.org/abs/2602.03981</link>
<guid>https://arxiv.org/abs/2602.03981</guid>
<content:encoded><![CDATA[
<div> DeFi<br /><br />:<br />1. DeFi<br />2. DeFi<br />3. DeXposure-FMDeFi<br />4. 43706024300<br />5. DeXposure-FM <div>
arXiv:2602.03981v1 Announce Type: new 
Abstract: Credit exposure in Decentralized Finance (DeFi) is often implicit and token-mediated, creating a dense web of inter-protocol dependencies. Thus, a shock to one token may result in significant and uncontrolled contagion effects. As the DeFi ecosystem becomes increasingly linked with traditional financial infrastructure through instruments, such as stablecoins, the risk posed by this dynamic demands more powerful quantification tools. We introduce DeXposure-FM, the first time-series, graph foundation model for measuring and forecasting inter-protocol credit exposure on DeFi networks, to the best of our knowledge. Employing a graph-tabular encoder, with pre-trained weight initialization, and multiple task-specific heads, DeXposure-FM is trained on the DeXposure dataset that has 43.7 million data entries, across 4,300+ protocols on 602 blockchains, covering 24,300+ unique tokens. The training is operationalized for credit-exposure forecasting, predicting the joint dynamics of (1) protocol-level flows, and (2) the topology and weights of credit-exposure links. The DeXposure-FM is empirically validated on two machine learning benchmarks; it consistently outperforms the state-of-the-art approaches, including a graph foundation model and temporal graph neural networks. DeXposure-FM further produces financial economics tools that support macroprudential monitoring and scenario-based DeFi stress testing, by enabling protocol-level systemic-importance scores, sector-level spillover and concentration measures via a forecast-then-measure pipeline. Empirical verification fully supports our financial economics tools. The model and code have been publicly available. Model: https://huggingface.co/EVIEHub/DeXposure-FM.
Code: https://github.com/EVIEHub/DeXposure-FM.
]]></content:encoded>
<pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boost+: Equitable, Incentive-Compatible Block Building</title>
<link>https://arxiv.org/abs/2602.04007</link>
<guid>https://arxiv.org/abs/2602.04007</guid>
<content:encoded><![CDATA[
<div> MEV-Boost<br /><br />:<br />1. MEV-Boost<br />2. Boost+<br />3. Boost+M_Boost+<br />4. <br />5.  <div>
arXiv:2602.04007v1 Announce Type: new 
Abstract: Block space on the blockchain is scarce and must be allocated efficiently through block building. However, Ethereum's current block-building ecosystem, MEV-Boost, has become highly centralized due to integration, which distorts competition, reduces blockspace efficiency, and obscures MEV flow transparency. To guarantee equitability and economic efficiency in block building, we propose $\mathrm{Boost+}$, a system that decouples the process into collecting and ordering transactions, and ensures equal access to all collected transactions.
  The core of $\mathrm{Boost+}$ is the mechanism $\mathit{M}_{\mathrm{Boost+}}$, built around a default algorithm. $\mathit{M}_{\mathrm{Boost+}}$ aligns incentives for both searchers (intermediaries that generate or route transactions) and builders: Truthful bidding is a dominant strategy for all builders. For searchers, truthful reporting is dominant whenever the default algorithm dominates competing builders, and it remains dominant for all conflict-free transactions, even when builders may win. We further show that even if a searcher can technically integrate with a builder, non-integration combined with truthful bidding still dominates any deviation for conflict-free transactions. We also implement a concrete default algorithm informed by empirical analysis of real-world transactions and evaluate its efficacy using historical transaction data.
]]></content:encoded>
<pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating the Vulnerability Landscape of LLM-Generated Smart Contracts</title>
<link>https://arxiv.org/abs/2602.04039</link>
<guid>https://arxiv.org/abs/2602.04039</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. LLMs  
2.   
3. ChatGPTGeminiSonnetLLMSolidity  
4.   
5. LLM <div>
arXiv:2602.04039v1 Announce Type: new 
Abstract: Large language models (LLMs) have been widely adopted in modern software development lifecycles, where they are increasingly used to automate and assist code generation, significantly improving developer productivity and reducing development time. In the blockchain domain, developers increasingly rely on LLMs to generate and maintain smart contracts, the immutable, self-executing components of decentralized applications. Because deployed smart contracts cannot be modified, correctness and security are paramount, particularly in high-stakes domains such as finance and governance. Despite this growing reliance, the security implications of LLM-generated smart contracts remain insufficiently understood.
  In this work, we conduct a systematic security analysis of Solidity smart contracts generated by state-of-the-art LLMs, including ChatGPT, Gemini, and Sonnet. We evaluate these contracts against a broad set of known smart contract vulnerabilities to assess their suitability for direct deployment in production environments. Our extensive experimental study shows that, despite their syntactic correctness and functional completeness, LLM-generated smart contracts frequently exhibit severe security flaws that could be exploited in real-world settings. We further analyze and categorize these vulnerabilities, identifying recurring weakness patterns across different models. Finally, we discuss practical countermeasures and development guidelines to help mitigate these risks, offering actionable insights for both developers and researchers. Our findings aim to support safe integration of LLMs into smart contract development workflows and to strengthen the overall security of the blockchain ecosystem against future security failures.
]]></content:encoded>
<pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Assumptions to Actions: Turning LLM Reasoning into Uncertainty-Aware Planning for Embodied Agents</title>
<link>https://arxiv.org/abs/2602.04326</link>
<guid>https://arxiv.org/abs/2602.04326</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1.   
2. PCEPlanner-Composer-Evaluator  
3.   
4. C-WAHTDW-MATLLMPCEtoken  
5. PCE <div>
arXiv:2602.04326v1 Announce Type: new 
Abstract: Embodied agents operating in multi-agent, partially observable, and decentralized environments must plan and act despite pervasive uncertainty about hidden objects and collaborators' intentions. Recent advances in applying Large Language Models (LLMs) to embodied agents have addressed many long-standing challenges, such as high-level goal decomposition and online adaptation. Yet, uncertainty is still primarily mitigated through frequent inter-agent communication. This incurs substantial token and time costs, and can disrupt established workflows, when human partners are involved. We introduce PCE, a Planner-Composer-Evaluator framework that converts the fragmented assumptions latent in LLM reasoning traces into a structured decision tree. Internal nodes encode environment assumptions and leaves map to actions; each path is then scored by scenario likelihood, goal-directed gain, and execution cost to guide rational action selection without heavy communication. Across two challenging multi-agent benchmarks (C-WAH and TDW-MAT) and three diverse LLM backbones, PCE consistently outperforms communication-centric baselines in success rate and task efficiency while showing comparable token usage. Ablation results indicate that the performance gains obtained by scaling model capacity or reasoning depth persist even when PCE is applied, while PCE consistently raises the baseline across both capacity and reasoning-depth scales, confirming that structured uncertainty handling complements both forms of scaling. A user study further demonstrates that PCE produces communication patterns that human partners perceive as more efficient and trustworthy. Together, these results establish a principled route for turning latent LLM assumptions into reliable strategies for uncertainty-aware planning.
]]></content:encoded>
<pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Med-MMFL: A Multimodal Federated Learning Benchmark in Healthcare</title>
<link>https://arxiv.org/abs/2602.04416</link>
<guid>https://arxiv.org/abs/2602.04416</guid>
<content:encoded><![CDATA[
<div>     <br /><br />:<br />1. FL<br />2. Med-MMFLMMFL<br />3. 10ECGXMRI24<br />4. IIDIIDFL<br />5.  <div>
arXiv:2602.04416v1 Announce Type: new 
Abstract: Federated learning (FL) enables collaborative model training across decentralized medical institutions while preserving data privacy. However, medical FL benchmarks remain scarce, with existing efforts focusing mainly on unimodal or bimodal modalities and a limited range of medical tasks. This gap underscores the need for standardized evaluation to advance systematic understanding in medical MultiModal FL (MMFL). To this end, we introduce Med-MMFL, the first comprehensive MMFL benchmark for the medical domain, encompassing diverse modalities, tasks, and federation scenarios. Our benchmark evaluates six representative state-of-the-art FL algorithms, covering different aggregation strategies, loss formulations, and regularization techniques. It spans datasets with 2 to 4 modalities, comprising a total of 10 unique medical modalities, including text, pathology images, ECG, X-ray, radiology reports, and multiple MRI sequences. Experiments are conducted across naturally federated, synthetic IID, and synthetic non-IID settings to simulate real-world heterogeneity. We assess segmentation, classification, modality alignment (retrieval), and VQA tasks. To support reproducibility and fair comparison of future multimodal federated learning (MMFL) methods under realistic medical settings, we release the complete benchmark implementation, including data processing and partitioning pipelines, at https://github.com/bhattarailab/Med-MMFL-Benchmark .
]]></content:encoded>
<pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPEAR: An Engineering Case Study of Multi-Agent Coordination for Smart Contract Auditing</title>
<link>https://arxiv.org/abs/2602.04418</link>
<guid>https://arxiv.org/abs/2602.04418</guid>
<content:encoded><![CDATA[
<div> SPEAR   MAS <br /><br />:  
1. SPEARMAS  
2.   
3. AGM  
4.   
5.  <div>
arXiv:2602.04418v1 Announce Type: new 
Abstract: We present SPEAR, a multi-agent coordination framework for smart contract auditing that applies established MAS patterns in a realistic security analysis workflow. SPEAR models auditing as a coordinated mission carried out by specialized agents: a Planning Agent prioritizes contracts using risk-aware heuristics, an Execution Agent allocates tasks via the Contract Net protocol, and a Repair Agent autonomously recovers from brittle generated artifacts using a programmatic-first repair policy. Agents maintain local beliefs updated through AGM-compliant revision, coordinate via negotiation and auction protocols, and revise plans as new information becomes available. An empirical study compares the multi-agent design with centralized and pipeline-based alternatives under controlled failure scenarios, focusing on coordination, recovery behavior, and resource use.
]]></content:encoded>
<pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Game of Coding for Vector-Valued Computations</title>
<link>https://arxiv.org/abs/2602.04810</link>
<guid>https://arxiv.org/abs/2602.04810</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. <br />3. DeML<br />4. <br />5. N <div>
arXiv:2602.04810v1 Announce Type: new 
Abstract: The game of coding is a new framework at the intersection of game theory and coding theory; designed to transcend the fundamental limitations of classical coding theory. While traditional coding theoretic schemes rely on a strict trust assumption, that honest nodes must outnumber adversarial ones to guarantee valid decoding, the game of coding leverages the economic rationality of actors to guarantee correctness and reliable decodability, even in the presence of an adversarial majority. This capability is paramount for emerging permissionless applications, particularly decentralized machine learning (DeML). However, prior investigations into the game of coding have been strictly confined to scalar computations, limiting their applicability to real world tasks where high dimensional data is the norm. In this paper, we bridge this gap by extending the framework to the general $N$-dimensional Euclidean space. We provide a rigorous problem formulation for vector valued computations and fully characterize the equilibrium strategies of the resulting high dimensional game. Our analysis demonstrates that the resilience properties established in the scalar setting are preserved in the vector regime, establishing a theoretical foundation for secure, large scale decentralized computing without honest majority assumptions.
]]></content:encoded>
<pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Matching Under Patience Imbalance</title>
<link>https://arxiv.org/abs/2602.03995</link>
<guid>https://arxiv.org/abs/2602.03995</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. <br />3. <br />4. <br />5.  <div>
arXiv:2602.03995v1 Announce Type: cross 
Abstract: We study a dynamic matching problem on a two-sided platform with unbalanced patience, in which long-lived supply accumulates over time with a unit waiting cost per period, while short-lived demand departs if not matched promptly. High- or low-quality agents arrive sequentially with one supply agent and one demand agent arriving in each period, and matching payoffs are supermodular. In the centralized benchmark, the optimal policy follows a threshold-based rule that rations high-quality supply, preserving it for future high-quality demand. In the decentralized system, where self-interested agents decide whether to match under an exogenously specified payoff allocation proportion, we characterize a welfare-maximizing Markov perfect equilibrium. Unlike outcomes in the centralized benchmark or in full-backlog markets, the equilibrium exhibits distinct matching patterns in which low-type demand may match with high-type supply even when low-type supply is available. Unlike settings in which both sides have long-lived agents and perfect coordination is impossible, the decentralized system can always be perfectly aligned with the centralized optimum by appropriately adjusting the allocation of matching payoffs across agents on both sides. Finally, when the arrival probabilities for H- and L-type arrivals are identical on both sides, we compare social welfare across systems with different patience levels: full backlog on both sides, one-sided backlog, and no backlog. In the centralized setting, social welfare is weakly ordered across systems. However, in the decentralized setting, the social welfare ranking across the three systems depends on the matching payoff allocation rule and the unit waiting cost, and enabling patience can either increase or decrease social welfare.
]]></content:encoded>
<pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>ClinConNet: A Blockchain-based Dynamic Consent Management Platform for Clinical Research</title>
<link>https://arxiv.org/abs/2602.02610</link>
<guid>https://arxiv.org/abs/2602.02610</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:  
1.   
2.   
3. ClinConNetSelf-Sovereign Identity  
4. GDPR  
5. 200250TPS <div>
arXiv:2602.02610v1 Announce Type: new 
Abstract: Consent is an ethical cornerstone of clinical research and healthcare in general. Although the ethical principles of consent - providing information, ensuring comprehension, and ensuring voluntariness - are well-defined, the technological infrastructure remains outdated. Clinicians are responsible for obtaining informed consent from research subjects or patients, and for managing it before, during, and after clinical trials or care, which is a burden for them. The voluntary nature of participating in clinical research or undergoing medical treatment implies the need for a participant-centric consent management system. However, this is not reflected in most established systems. Not only do most healthcare information systems not follow a user-centric model, but they also create data silos, which significantly reduce the mobility of patient data between different healthcare institutions and impact personalized medicine. Furthermore, consent management tools are outdated. We propose ClinConNet (Clinical Consent Network), a platform that connects researchers and participants based on clinical research projects. ClinConNet is powered by a dynamic consent model based on blockchain and take advantage of dynamic consent interfaces, as well as blockchain and Self-Sovereign Identity systems. ClinConNet is user-centric and provides important privacy features for patients, such as unlinkability, confidentiality, and ownership of identity data. It is also compatible with the right to be forgotten, as defined in many personal data protection regulations, such as the GDPR. We provide a detailed privacy and security analysis in an adversarial model, as well as a Proof of Concept implementation with detailed performance measures that demonstrate the feasibility of our blockchain-based consent management system with a median end-to-end consent establishment time of under 200ms and a throughput of 250TPS.
]]></content:encoded>
<pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>IMAGINE: Intelligent Multi-Agent Godot-based Indoor Networked Exploration</title>
<link>https://arxiv.org/abs/2602.02858</link>
<guid>https://arxiv.org/abs/2602.02858</guid>
<content:encoded><![CDATA[
<div> GNSS<br /><br />:<br />1. GNSSMARL<br />2. LiDAR<br />3. GodotND-POMDP<br />4. <br />5.  <div>
arXiv:2602.02858v1 Announce Type: new 
Abstract: The exploration of unknown, Global Navigation Satellite System (GNSS) denied environments by an autonomous communication-aware and collaborative group of Unmanned Aerial Vehicles (UAVs) presents significant challenges in coordination, perception, and decentralized decision-making. This paper implements Multi-Agent Reinforcement Learning (MARL) to address these challenges in a 2D indoor environment, using high-fidelity game-engine simulations (Godot) and continuous action spaces. Policy training aims to achieve emergent collaborative behaviours and decision-making under uncertainty using Network-Distributed Partially Observable Markov Decision Processes (ND-POMDPs). Each UAV is equipped with a Light Detection and Ranging (LiDAR) sensor and can share data (sensor measurements and a local occupancy map) with neighbouring agents. Inter-agent communication constraints include limited range, bandwidth and latency. Extensive ablation studies evaluated MARL training paradigms, reward function, communication system, neural network (NN) architecture, memory mechanisms, and POMDP formulations. This work jointly addresses several key limitations in prior research, namely reliance on discrete actions, single-agent or centralized formulations, assumptions of a priori knowledge and permanent connectivity, inability to handle dynamic obstacles, short planning horizons and architectural complexity in Recurrent NNs/Transformers. Results show that the scalable training paradigm, combined with a simplified architecture, enables rapid autonomous exploration of an indoor area. The implementation of Curriculum-Learning (five increasingly complex levels) also enabled faster, more robust training. This combination of high-fidelity simulation, MARL formulation, and computational efficiency establishes a strong foundation for deploying learned cooperative strategies in physical robotic systems.
]]></content:encoded>
<pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prefix Consensus For Censorship Resistant BFT</title>
<link>https://arxiv.org/abs/2602.02892</link>
<guid>https://arxiv.org/abs/2602.02892</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. BFTPrefix Consensus  
2. Strong Prefix Consensus  
3. BFTSMR  
4. GSTf  
5. /3/O(n)O(n) <div>
arXiv:2602.02892v1 Announce Type: new 
Abstract: Despite broad use of BFT consensus in blockchains, censorship resistance is weak: leaders can exclude transactions, a growing concern for trading and DeFi.
  We address this by introducing a new abstraction and protocol stack. First, we introduce \emph{Prefix Consensus}, where parties input vectors and output $(v^{\sf low},v^{\sf high})$ that (i) extend the maximum common prefix of honest inputs and (ii) satisfy $v_i^{\sf low}\preceq v_j^{\sf high}$ for all honest $i,j$. Unlike classical consensus, no single output is required. We show Prefix Consensus is solvable asynchronously and give tight round-complexity bounds.
  We then define \emph{Strong Prefix Consensus}, requiring agreement on the \emph{high} output. Our protocol is leaderless and partially synchronous: one Prefix Consensus instance decides (possibly different) lows, and additional instances yield a unique safe-to-extend high, even if an adversary can suspend one party per round.
  We lift this to a leaderless, multi-proposer, censorship-resistant BFT SMR protocol: per slot, all parties broadcast proposals, deterministically rank them, and run one Strong Prefix Consensus on proposal hashes, committing honest proposals in \emph{four rounds}. A deterministic demotion rule updates the ranking when a party's proposal is excluded, implying that after GST at most $f$ slots can miss an honest proposal while progress remains leaderless under suspension and up to $f{-}1$ Byzantine faults.
  Finally, we connect Prefix Consensus to graded and binary/validated consensus: we obtain an optimal-latency graded consensus (3 message delays) and leaderless Binary/Validated Consensus with worst-case message complexity $O(n^3)$ and communication $O(n^4)$.
]]></content:encoded>
<pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Controlled disagreement improves generalization in decentralized training</title>
<link>https://arxiv.org/abs/2602.02899</link>
<guid>https://arxiv.org/abs/2602.02899</guid>
<content:encoded><![CDATA[
<div> Hessian<br /><br />:<br />1. DSGD-AC<br />2. DSGD-AC<br />3. Hessian<br />4. <br />5. DSGD-ACDSGDSGD <div>
arXiv:2602.02899v1 Announce Type: new 
Abstract: Decentralized training is often regarded as inferior to centralized training because the consensus errors between workers are thought to undermine convergence and generalization, even with homogeneous data distributions. This work challenges this view by introducing decentralized SGD with Adaptive Consensus (DSGD-AC), which intentionally preserves non-vanishing consensus errors through a time-dependent scaling mechanism. We prove that these errors are not random noise but systematically align with the dominant Hessian subspace, acting as structured perturbations that guide optimization toward flatter minima. Across image classification and machine translation benchmarks, DSGD-AC consistently surpasses both standard DSGD and centralized SGD in test accuracy and solution flatness. Together, these results establish consensus errors as a useful implicit regularizer and open a new perspective on the design of decentralized learning algorithms.
]]></content:encoded>
<pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Methods and Open Problems in Differentiable Social Choice: Learning Mechanisms, Decisions, and Alignment</title>
<link>https://arxiv.org/abs/2602.03003</link>
<guid>https://arxiv.org/abs/2602.03003</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. <br />3. <br />4. <br />5. 36 <div>
arXiv:2602.03003v1 Announce Type: new 
Abstract: Social choice is no longer a peripheral concern of political theory or economics-it has become a foundational component of modern machine learning systems. From auctions and resource allocation to federated learning, participatory governance, and the alignment of large language models, machine learning pipelines increasingly aggregate heterogeneous preferences, incentives, and judgments into collective decisions. In effect, many contemporary machine learning systems already implement social choice mechanisms, often implicitly and without explicit normative scrutiny.
  This Review surveys differentiable social choice: an emerging paradigm that formulates voting rules, mechanisms, and aggregation procedures as learnable, differentiable models optimized from data. We synthesize work across auctions, voting, budgeting, liquid democracy, decentralized aggregation, and inverse mechanism learning, showing how classical axioms and impossibility results reappear as objectives, constraints, and optimization trade-offs. We conclude by identifying 36 open problems defining a new research agenda at the intersection of machine learning, economics, and democratic theory.
]]></content:encoded>
<pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedKRSO: Communication and Memory Efficient Federated Fine-Tuning of Large Language Models</title>
<link>https://arxiv.org/abs/2602.03019</link>
<guid>https://arxiv.org/abs/2602.03019</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. LLMFedKRSO  
2. FedKRSO  
3.   
4. FFTPEFT  
5. GLUEFedKRSO <div>
arXiv:2602.03019v1 Announce Type: new 
Abstract: Fine-tuning is essential to adapt general-purpose large language models (LLMs) to domain-specific tasks. As a privacy-preserving framework to leverage decentralized data for collaborative model training, Federated Learning (FL) is gaining popularity in LLM fine-tuning, but remains challenging due to the high cost of transmitting full model parameters and computing full gradients on resource-constrained clients. While Parameter-Efficient Fine-Tuning (PEFT) methods are widely used in FL to reduce communication and memory costs, they often sacrifice model performance compared to FFT. This paper proposes FedKRSO (Federated $K$-Seed Random Subspace Optimization), a novel method that enables communication and memory efficient FFT of LLMs in federated settings. In FedKRSO, clients update the model within a shared set of random low-dimension subspaces generated by the server to save memory usage. Furthermore, instead of transmitting full model parameters in each FL round, clients send only the model update accumulators along the subspaces to the server, enabling efficient global model aggregation and dissemination. By using these strategies, FedKRSO can substantially reduce communication and memory overhead while overcoming the performance limitations of PEFT, closely approximating the performance of federated FFT. The convergence properties of FedKRSO are analyzed rigorously under general FL settings. Extensive experiments on the GLUE benchmark across diverse FL scenarios demonstrate that FedKRSO achieves both superior performance and low communication and memory overhead, paving the way towards on federated LLM fine-tuning at the resource-constrained edge.
]]></content:encoded>
<pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fedcompass: Federated Clustered and Periodic Aggregation Framework for Hybrid Classical-Quantum Models</title>
<link>https://arxiv.org/abs/2602.03052</link>
<guid>https://arxiv.org/abs/2602.03052</guid>
<content:encoded><![CDATA[
<div>     <br /><br />:<br />1. -non-IIDFEDCOMPASS<br />2. <br />3. <br />4. FEDCOMPASSnon-IID10.22%<br />5. FEDCOMPASS <div>
arXiv:2602.03052v1 Announce Type: new 
Abstract: Federated learning enables collaborative model training across decentralized clients under privacy constraints. Quantum computing offers potential for alleviating computational and communication burdens in federated learning, yet hybrid classical-quantum federated learning remains susceptible to performance degradation under non-IID data. To address this,we propose FEDCOMPASS, a layered aggregation framework for hybrid classical-quantum federated learning. FEDCOMPASS employs spectral clustering to group clients by class distribution similarity and performs cluster-wise aggregation for classical feature extractors. For quantum parameters, it uses circular mean aggregation combined with adaptive optimization to ensure stable global updates. Experiments on three benchmark datasets show that FEDCOMPASS improves test accuracy by up to 10.22% and enhances convergence stability under non-IID settings, outperforming six strong federated learning baselines.
]]></content:encoded>
<pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Internet of Agentic AI: Incentive-Compatible Distributed Teaming and Workflow</title>
<link>https://arxiv.org/abs/2602.03145</link>
<guid>https://arxiv.org/abs/2602.03145</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. AI<br />2. AI<br />3. <br />4. <br />5. Model Context Protocol <div>
arXiv:2602.03145v1 Announce Type: new 
Abstract: Large language models (LLMs) have enabled a new class of agentic AI systems that reason, plan, and act by invoking external tools. However, most existing agentic architectures remain centralized and monolithic, limiting scalability, specialization, and interoperability. This paper proposes a framework for scalable agentic intelligence, termed the Internet of Agentic AI, in which autonomous, heterogeneous agents distributed across cloud and edge infrastructure dynamically form coalitions to execute task-driven workflows. We formalize a network-native model of agentic collaboration and introduce an incentive-compatible workflow-coalition feasibility framework that integrates capability coverage, network locality, and economic implementability. To enable scalable coordination, we formulate a minimum-effort coalition selection problem and propose a decentralized coalition formation algorithm. The proposed framework can operate as a coordination layer above the Model Context Protocol (MCP). A healthcare case study demonstrates how domain specialization, cloud-edge heterogeneity, and dynamic coalition formation enable scalable, resilient, and economically viable agentic workflows. This work lays the foundation for principled coordination and scalability in the emerging era of Internet of Agentic AI.
]]></content:encoded>
<pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>LogicScan: An LLM-driven Framework for Detecting Business Logic Vulnerabilities in Smart Contracts</title>
<link>https://arxiv.org/abs/2602.03271</link>
<guid>https://arxiv.org/abs/2602.03271</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. LogicScan<br />3. BSL<br />4. <br />5. LogicScan F185.2% <div>
arXiv:2602.03271v1 Announce Type: new 
Abstract: Business logic vulnerabilities have become one of the most damaging yet least understood classes of smart contract vulnerabilities. Unlike traditional bugs such as reentrancy or arithmetic errors, these vulnerabilities arise from missing or incorrectly enforced business invariants and are tightly coupled with protocol semantics. Existing static analysis techniques struggle to capture such high-level logic, while recent large language model based approaches often suffer from unstable outputs and low accuracy due to hallucination and limited verification.
  In this paper, we propose LogicScan, an automated contrastive auditing framework for detecting business logic vulnerabilities in smart contracts. The key insight behind LogicScan is that mature, widely deployed on-chain protocols implicitly encode well-tested and consensus-driven business invariants. LogicScan systematically mines these invariants from large-scale on-chain contracts and reuses them as reference constraints to audit target contracts. To achieve this, LogicScan introduces a Business Specification Language (BSL) to normalize diverse implementation patterns into structured, verifiable logic representations. It further combines noise-aware logic aggregation with contrastive auditing to identify missing or weakly enforced invariants while mitigating LLM-induced false positives.
  We evaluate LogicScan on three real-world datasets, including DeFiHacks, Web3Bugs, and a set of top-200 audited contracts. The results show that LogicScan achieves an F1 score of 85.2%, significantly outperforming state-of-the-art tools while maintaining a low false-positive rate on production-grade contracts. Additional experiments demonstrate that LogicScan maintains consistent performance across different LLMs and is cost-effective, and that its false-positive suppression mechanisms substantially improve robustness.
]]></content:encoded>
<pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting Multi-Core Parallelism in Blockchain Validation and Construction</title>
<link>https://arxiv.org/abs/2602.03444</link>
<guid>https://arxiv.org/abs/2602.03444</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. CPU<br />2. pB<br />3. MILP<br />4. SolanaSolRG <div>
arXiv:2602.03444v1 Announce Type: new 
Abstract: Blockchain validators can reduce block processing time by exploiting multi-core CPUs, but deterministic execution must preserve a given total order while respecting transaction conflicts and per-block runtime limits. This paper systematically examines how validators can exploit multi-core parallelism during both block construction and execution without violating blockchain semantics. We formalize two validator-side optimization problems: (i) executing an already ordered block on \(p\) cores to minimize makespan while ensuring equivalence to sequential execution; and (ii) selecting and scheduling a subset of mempool transactions under a runtime limit \(B\) to maximize validator reward. For both, we develop exact Mixed-Integer Linear Programming (MILP) formulations that capture conflict, order, and capacity constraints, and propose fast deterministic heuristics that scale to realistic workloads. Using Ethereum mainnet traces and including a Solana-inspired declared-access baseline (Sol) for ordered-block scheduling and a simple reward-greedy baseline (RG) for block construction, we empirically quantify the trade-offs between optimality and runtime.
]]></content:encoded>
<pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Game-Theoretic and Algorithmic Analyses of Multi-Agent Routing under Crossing Costs</title>
<link>https://arxiv.org/abs/2602.03455</link>
<guid>https://arxiv.org/abs/2602.03455</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. <br />3. <br />4. PLSNPSteiner<br />5. XPFPT <div>
arXiv:2602.03455v1 Announce Type: new 
Abstract: Coordinating the movement of multiple autonomous agents over a shared network is a fundamental challenge in algorithmic robotics, intelligent transportation, and distributed systems. The dominant approach, Multi-Agent Path Finding, relies on centralized control and synchronous collision avoidance, which often requires strict synchronization and guarantees of globally conflict-free execution. This paper introduces the Multi-Agent Routing under Crossing Cost model on mixed graphs, a novel framework tailored to asynchronous settings. In our model, instead of treating conflicts as hard constraints, each agent is assigned a path, and the system is evaluated through a cost function that measures potential head-on encounters. This ``crossing cost'', which is defined as the product of the numbers of agents traversing an edge in opposite directions, quantifies the risk of congestion and delay in decentralized execution.
  Our contributions are both game-theoretic and algorithmic. We model the setting as a congestion game with a non-standard cost function, prove the existence of pure Nash equilibria, and analyze the dynamics leading to them. Equilibria can be found in polynomial time under mild conditions, while the general case is PLS-complete. From an optimization perspective, minimizing the total crossing cost is NP-hard, as the problem generalizes Steiner Orientation. To address this hardness barrier, we design a suite of parameterized algorithms for minimizing crossing cost, with parameters including the number of arcs, edges, agents, and structural graph measures. These yield XP or FPT results depending on the parameter, offering algorithmic strategies for structurally restricted instances. Our framework provides a new theoretical foundation for decentralized multi-agent routing, bridging equilibrium analysis and parameterized complexity to support scalable and risk-aware coordination.
]]></content:encoded>
<pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding and Exploiting Weight Update Sparsity for Communication-Efficient Distributed RL</title>
<link>https://arxiv.org/abs/2602.03839</link>
<guid>https://arxiv.org/abs/2602.03839</guid>
<content:encoded><![CDATA[
<div> PULSE<br /><br />:<br />1. RL<br />2. RL99%<br />3. PULSE<br />4. PULSE<br />5. PULSE10014 GB108 MB20 Gbit/s0.2 Gbit/sGPU <div>
arXiv:2602.03839v1 Announce Type: new 
Abstract: Reinforcement learning (RL) is a critical component for post-training large language models (LLMs). However, in bandwidth-constrained distributed RL, scalability is often bottlenecked by the synchronization of policy weights from trainers to inference workers, particularly over commodity networks or in decentralized settings. While recent studies suggest that RL updates modify only a small fraction of model parameters, these observations are typically based on coarse checkpoint differences. We present a systematic empirical study of weight-update sparsity at both step-level and multi-step granularities, examining its evolution across training dynamics, off-policy delay, and model scale. We find that update sparsity is consistently high, frequently exceeding 99% across practically relevant settings. Leveraging this structure, we propose PULSE (Patch Updates via Lossless Sparse Encoding), a simple yet highly efficient lossless weight synchronization method that transmits only the indices and values of modified parameters. PULSE is robust to transmission errors and avoids floating-point drift inherent in additive delta schemes. In bandwidth-constrained decentralized environments, our approach achieves over 100x (14 GB to ~108 MB) communication reduction while maintaining bit-identical training dynamics and performance compared to full weight synchronization. By exploiting this structure, PULSE enables decentralized RL training to approach centralized throughput, reducing the bandwidth required for weight synchronization from 20 Gbit/s to 0.2 Gbit/s to maintain high GPU utilization.
]]></content:encoded>
<pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improved Analysis of the Accelerated Noisy Power Method with Applications to Decentralized PCA</title>
<link>https://arxiv.org/abs/2602.03682</link>
<guid>https://arxiv.org/abs/2602.03682</guid>
<content:encoded><![CDATA[
<div> PCA<br /><br />:  
1. PCAPCA  
2.   
3.   
4.   
5. PCA <div>
arXiv:2602.03682v1 Announce Type: cross 
Abstract: We analyze the Accelerated Noisy Power Method, an algorithm for Principal Component Analysis in the setting where only inexact matrix-vector products are available, which can arise for instance in decentralized PCA. While previous works have established that acceleration can improve convergence rates compared to the standard Noisy Power Method, these guarantees require overly restrictive upper bounds on the magnitude of the perturbations, limiting their practical applicability. We provide an improved analysis of this algorithm, which preserves the accelerated convergence rate under much milder conditions on the perturbations. We show that our new analysis is worst-case optimal, in the sense that the convergence rate cannot be improved, and that the noise conditions we derive cannot be relaxed without sacrificing convergence guarantees. We demonstrate the practical relevance of our results by deriving an accelerated algorithm for decentralized PCA, which has similar communication costs to non-accelerated methods. To our knowledge, this is the first decentralized algorithm for PCA with provably accelerated convergence.
]]></content:encoded>
<pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Symphony-Coord: Emergent Coordination in Decentralized Agent Systems</title>
<link>https://arxiv.org/abs/2602.00966</link>
<guid>https://arxiv.org/abs/2602.00966</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. Symphony-Coord<br />2. <br />3. LinUCB<br />4. <br />5.  <div>
arXiv:2602.00966v1 Announce Type: new 
Abstract: Multi-agent large language model systems can tackle complex multi-step tasks by decomposing work and coordinating specialized behaviors. However, current coordination mechanisms typically rely on statically assigned roles and centralized controllers. As agent pools and task distributions evolve, these design choices lead to inefficient routing, poor adaptability, and fragile fault recovery capabilities. We introduce Symphony-Coord, a decentralized multi-agent framework that transforms agent selection into an online multi-armed bandit problem, enabling roles to emerge organically through interaction. The framework employs a two-stage dynamic beacon protocol: (i) a lightweight candidate screening mechanism to limit communication and computational overhead; (ii) an adaptive LinUCB selector that routes subtasks based on context features derived from task requirements and agent states, continuously optimized through delayed end-to-end feedback. Under standard linear realizability assumptions, we provide sublinear regret bounds, indicating the system converges toward near-optimal allocation schemes. Validation through simulation experiments and real-world large language model benchmarks demonstrates that Symphony-Coord not only enhances task routing efficiency but also exhibits robust self-healing capabilities in scenarios involving distribution shifts and agent failures, achieving a scalable coordination mechanism without predefined roles.
]]></content:encoded>
<pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meanshift Shape Formation Control Using Discrete Mass Distribution</title>
<link>https://arxiv.org/abs/2602.00980</link>
<guid>https://arxiv.org/abs/2602.00980</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. <br />3. <br />4. <br />5.  <div>
arXiv:2602.00980v1 Announce Type: new 
Abstract: The density-distribution method has recently become a promising paradigm owing to its adaptability to variations in swarm size. However, existing studies face practical challenges in achieving complex shape representation and decentralized implementation. This motivates us to develop a fully decentralized, distribution-based control strategy with the dual capability of forming complex shapes and adapting to swarm-size variations. Specifically, we first propose a discrete mass-distribution function defined over a set of sample points to model swarm formation. In contrast to the continuous density-distribution method, our model eliminates the requirement for defining continuous density functions-a task that is difficult for complex shapes. Second, we design a decentralized meanshift control law to coordinate the swarm's global distribution to fit the sample-point distribution by feeding back mass estimates. The mass estimates for all sample points are achieved by the robots in a decentralized manner via the designed mass estimator. It is shown that the mass estimates of the sample points can asymptotically converge to the true global values. To validate the proposed strategy, we conduct comprehensive simulations and real-world experiments to evaluate the efficiency of complex shape formation and adaptability to swarm-size variations.
]]></content:encoded>
<pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeALOG: Decentralized Multi-Agents Log-Mediated Reasoning Framework</title>
<link>https://arxiv.org/abs/2602.00996</link>
<guid>https://arxiv.org/abs/2602.00996</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. DeALOG<br />2. <br />3. <br />4. <br />5.  <div>
arXiv:2602.00996v1 Announce Type: new 
Abstract: Complex question answering across text, tables and images requires integrating diverse information sources. A framework supporting specialized processing with coordination and interpretability is needed. We introduce DeALOG, a decentralized multi-agent framework for multimodal question answering. It uses specialized agents: Table, Context, Visual, Summarizing and Verification, that communicate through a shared natural-language log as persistent memory. This log-based approach enables collaborative error detection and verification without central control, improving robustness. Evaluations on FinQA, TAT-QA, CRT-QA, WikiTableQuestions, FeTaQA, and MultiModalQA show competitive performance. Analysis confirms the importance of the shared log, agent specialization, and verification for accuracy. DeALOG, provides a scalable approach through modular components using natural-language communication.
]]></content:encoded>
<pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedBGS: A Blockchain Approach to Segment Gossip Learning in Decentralized Systems</title>
<link>https://arxiv.org/abs/2602.01185</link>
<guid>https://arxiv.org/abs/2602.01185</guid>
<content:encoded><![CDATA[
<div>  gossip <br /><br />:  
1. PPFL  
2.   
3.  FedBGS  gossip   
4. non-IID <div>
arXiv:2602.01185v1 Announce Type: new 
Abstract: Privacy-Preserving Federated Learning (PPFL) is a Decentralized machine learning paradigm that enables multiple participants to collaboratively train a global model without sharing their data with the integration of cryptographic and privacy-based techniques to enhance the security of the global system. This privacy-oriented approach makes PPFL a highly suitable solution for training shared models in sectors where data privacy is a critical concern. In traditional FL, local models are trained on edge devices, and only model updates are shared with a central server, which aggregates them to improve the global model. However, despite the presence of the aforementioned privacy techniques, in the classical Federated structure, the issue of the server as a single-point-of-failure remains, leading to limitations both in terms of security and scalability. This paper introduces FedBGS, a fully Decentralized Blockchain-based framework that leverages Segmented Gossip Learning through Federated Analytics. The proposed system aims to optimize blockchain usage while providing comprehensive protection against all types of attacks, ensuring both privacy, security and non-IID data handling in Federated environments.
]]></content:encoded>
<pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>TxRay: Agentic Postmortem of Live Blockchain Attacks</title>
<link>https://arxiv.org/abs/2602.01317</link>
<guid>https://arxiv.org/abs/2602.01317</guid>
<content:encoded><![CDATA[
<div> DeFiAnyone-Can-TakeTxRay<br /><br />:  
1. DeFi157.5Anyone-Can-TakeACT  
2.   
3. TxRayLLMACT  
4. TxRayPoC  
5. TxRay11410592.11%PoC4059 <div>
arXiv:2602.01317v1 Announce Type: new 
Abstract: Decentralized Finance (DeFi) has turned blockchains into financial infrastructure, allowing anyone to trade, lend, and build protocols without intermediaries, but this openness exposes pools of value controlled by code. Within five years, the DeFi ecosystem has lost over 15.75B USD to reported exploits. Many exploits arise from permissionless opportunities that any participant can trigger using only public state and standard interfaces, which we call Anyone-Can-Take (ACT) opportunities. Despite on-chain transparency, postmortem analysis remains slow and manual: investigations start from limited evidence, sometimes only a single transaction hash, and must reconstruct the exploit lifecycle by recovering related transactions, contract code, and state dependencies.
  We present TxRay, a Large Language Model (LLM) agentic postmortem system that uses tool calls to reconstruct live ACT attacks from limited evidence. Starting from one or more seed transactions, TxRay recovers the exploit lifecycle, derives an evidence-backed root cause, and generates a runnable, self-contained Proof of Concept (PoC) that deterministically reproduces the incident. TxRay self-checks postmortems by encoding incident-specific semantic oracles as executable assertions.
  To evaluate PoC correctness and quality, we develop PoCEvaluator, an independent agentic execution-and-review evaluator. On 114 incidents from DeFiHackLabs, TxRay produces an expert-aligned root cause and an executable PoC for 105 incidents, achieving 92.11% end-to-end reproduction. Under PoCEvaluator, 98.1% of TxRay PoCs avoid hard-coding attacker addresses, a +24.8pp lift over DeFiHackLabs. In a live deployment, TxRay delivers validated root causes in 40 minutes and PoCs in 59 minutes at median latency. TxRay's oracle-validated PoCs enable attack imitation, improving coverage by 15.6% and 65.5% over STING and APE.
]]></content:encoded>
<pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tidehunter: Large-Value Storage With Minimal Data Relocation</title>
<link>https://arxiv.org/abs/2602.01873</link>
<guid>https://arxiv.org/abs/2602.01873</guid>
<content:encoded><![CDATA[
<div> LSM-treeTidehunterWAL<br /><br />:<br />1. LSM-tree1030<br />2. TidehunterWALWAL<br />3. NVMe<br />4. 1TBTidehunter83/RocksDBBlobDB<br />5. Sui <div>
arXiv:2602.01873v1 Announce Type: new 
Abstract: Log-Structured Merge-Trees (LSM-trees) dominate persistent key-value storage but suffer from high write amplification from 10x to 30x under random workloads due to repeated compaction. This overhead becomes prohibitive for large values with uniformly distributed keys, a workload common in content-addressable storage, deduplication systems, and blockchain validators. We present Tidehunter, a storage engine that eliminates value compaction by treating the Write-Ahead Log (WAL) as permanent storage rather than a temporary recovery buffer. Values are never overwritten; and small, lazily-flushed index tables map keys to WAL positions. Tidehunter introduces (a) lock-free writes that saturate NVMe drives through atomic allocation and parallel copying, (b) an optimistic index structure that exploits uniform key distributions for single-roundtrip lookups, and (c) epoch-based pruning that reclaims space without blocking writes. On a 1 TB dataset with 1 KB values, Tidehunter achieves 830K writes per second, that is 8.4x higher than RocksDB and 2.9x higher than BlobDB, while improving point queries by 1.7x and existence checks by 15.6x. We validate real-world impact by integrating Tidehunter into Sui, a high-throughput blockchain, where it maintains stable throughput and latency under loads that cause RocksDB-backed validators to collapse. Tidehunter is production-ready and is being deployed in production within Sui.
]]></content:encoded>
<pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provenance Verification of AI-Generated Images via a Perceptual Hash Registry Anchored on Blockchain</title>
<link>https://arxiv.org/abs/2602.02412</link>
<guid>https://arxiv.org/abs/2602.02412</guid>
<content:encoded><![CDATA[
<div> AI<br /><br />:  
1. AI  
2.   
3. /Merkle Patricia TrieBurkhard-Keller  
4. AI  
5.  <div>
arXiv:2602.02412v1 Announce Type: new 
Abstract: The rapid advancement of artificial intelligence has made the generation of synthetic images widely accessible, increasing concerns related to misinformation, digital forgery, and content authenticity on large-scale online platforms. This paper proposes a blockchain-backed framework for verifying AI-generated images through a registry-based provenance mechanism. Each AI-generated image is assigned a digital fingerprint that preserves similarity using perceptual hashing and is registered at creation time by participating generation platforms. The hashes are stored on a hybrid on-chain/off-chain public blockchain using a Merkle Patricia Trie for tamper-resistant storage (on-chain) and a Burkhard-Keller tree (off-chain) to enable efficient similarity search over large image registries. Verification is performed when images are re-uploaded to digital platforms such as social media services, enabling identification of previously registered AI-generated images even after benign transformations or partial modifications. The proposed system does not aim to universally detect all synthetic images, but instead focuses on verifying the provenance of AI-generated content that has been registered at creation time. By design, this approach complements existing watermarking and learning-based detection methods, providing a platform-agnostic, tamper-proof mechanism for scalable content provenance and authenticity verification at the point of large-scale online distribution.
]]></content:encoded>
<pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D Foundation Model-Based Loop Closing for Decentralized Collaborative SLAM</title>
<link>https://arxiv.org/abs/2602.02430</link>
<guid>https://arxiv.org/abs/2602.02430</guid>
<content:encoded><![CDATA[
<div> 3DSLAM<br /><br />:  
13D  
23DC-SLAM  
3  
4  
5 <div>
arXiv:2602.02430v1 Announce Type: new 
Abstract: Decentralized Collaborative Simultaneous Localization And Mapping (C-SLAM) techniques often struggle to identify map overlaps due to significant viewpoint variations among robots. Motivated by recent advancements in 3D foundation models, which can register images despite large viewpoint differences, we propose a robust loop closing approach that leverages these models to establish inter-robot measurements. In contrast to resource-intensive methods requiring full 3D reconstruction within a centralized map, our approach integrates foundation models into existing SLAM pipelines, yielding scalable and robust multi-robot mapping. Our contributions include: (1) integrating 3D foundation models to reliably estimate relative poses from monocular image pairs within decentralized C-SLAM; (2) introducing robust outlier mitigation techniques critical to the use of these relative poses; and (3) developing specialized pose graph optimization formulations that efficiently resolve scale ambiguities. We evaluate our method against state-of-the-art approaches, demonstrating improvements in localization and mapping accuracy, alongside significant gains in computational and memory efficiency. These results highlight the potential of our approach for deployment in large-scale multi-robot scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conflict-Aware Client Selection for Multi-Server Federated Learning</title>
<link>https://arxiv.org/abs/2602.02458</link>
<guid>https://arxiv.org/abs/2602.02458</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. FL<br />2. FL<br />3. RL-CRP<br />4. <br />5.  <div>
arXiv:2602.02458v1 Announce Type: new 
Abstract: Federated learning (FL) has emerged as a promising distributed machine learning (ML) that enables collaborative model training across clients without exposing raw data, thereby preserving user privacy and reducing communication costs. Despite these benefits, traditional single-server FL suffers from high communication latency due to the aggregation of models from a large number of clients. While multi-server FL distributes workloads across edge servers, overlapping client coverage and uncoordinated selection often lead to resource contention, causing bandwidth conflicts and training failures. To address these limitations, we propose a decentralized reinforcement learning with conflict risk prediction, named RL CRP, to optimize client selection in multi-server FL systems. Specifically, each server estimates the likelihood of client selection conflicts using a categorical hidden Markov model based on its sparse historical client selection sequence. Then, a fairness-aware reward mechanism is incorporated to promote long-term client participation for minimizing training latency and resource contention. Extensive experiments demonstrate that the proposed RL-CRP framework effectively reduces inter-server conflicts and significantly improves training efficiency in terms of convergence speed and communication cost.
]]></content:encoded>
<pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Learning With Individualized Privacy Through Client Sampling</title>
<link>https://arxiv.org/abs/2501.17634</link>
<guid>https://arxiv.org/abs/2501.17634</guid>
<content:encoded><![CDATA[
<div> IDP-FedAvg<br /><br />:<br />1. IDP<br />2. SAMPLE<br />3. IDP-FedAvg<br />4. SCALE<br />5. non-i.i.d. <div>
arXiv:2501.17634v2 Announce Type: cross 
Abstract: With growing concerns about user data collection, individualized privacy has emerged as a promising solution to balance protection and utility by accounting for diverse user privacy preferences. Instead of enforcing a uniform level of anonymization for all users, this approach allows individuals to choose privacy settings that align with their comfort levels. Building on this idea, we propose an adapted method for enabling Individualized Differential Privacy (IDP) in Federated Learning (FL) by handling clients according to their personal privacy preferences. By extending the SAMPLE algorithm from centralized settings to FL, we calculate client-specific sampling rates based on their heterogeneous privacy budgets and integrate them into a modified IDP-FedAvg algorithm. We test this method under realistic privacy distributions and multiple datasets. The experimental results demonstrate that our approach achieves clear improvements over uniform DP baselines, reducing the trade-off between privacy and utility. Compared to the alternative SCALE method in related work, which assigns differing noise scales to clients, our method performs notably better. However, challenges remain for complex tasks with non-i.i.d. data, primarily stemming from the constraints of the decentralized setting.
]]></content:encoded>
<pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCALM: Detecting Bad Practices in Smart Contracts Through LLMs</title>
<link>https://arxiv.org/abs/2502.04347</link>
<guid>https://arxiv.org/abs/2502.04347</guid>
<content:encoded><![CDATA[
<div> SCALM<br /><br />:<br />1. <br />2. <br />3. 35<br />4. SCALMStep-Back PromptingRAG<br />5. SCALM <div>
arXiv:2502.04347v2 Announce Type: replace 
Abstract: As the Ethereum platform continues to mature and gain widespread usage, it is crucial to maintain high standards of smart contract writing practices. While bad practices in smart contracts may not directly lead to security issues, they do elevate the risk of encountering problems. Therefore, to understand and avoid these bad practices, this paper introduces the first systematic study of bad practices in smart contracts, delving into over 35 specific issues. Specifically, we propose a large language models (LLMs)-based framework, SCALM. It combines Step-Back Prompting and Retrieval-Augmented Generation (RAG) to identify and address various bad practices effectively. Our extensive experiments using multiple LLMs and datasets have shown that SCALM outperforms existing tools in detecting bad practices in smart contracts.
]]></content:encoded>
<pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SmartBugBert: BERT-Enhanced Vulnerability Detection for Smart Contract Bytecode</title>
<link>https://arxiv.org/abs/2504.05002</link>
<guid>https://arxiv.org/abs/2504.05002</guid>
<content:encoded><![CDATA[
<div> BERT<br /><br />:<br />1. <br />2. SmartBugBertBERTCFG<br />3. TF-IDFCFGCFG<br />4. BERTLightGBM<br />5. 6,15790.62%91.76%F191.19%0.14 <div>
arXiv:2504.05002v2 Announce Type: replace 
Abstract: Smart contracts deployed on blockchain platforms are vulnerable to various security vulnerabilities. However, only a small number of Ethereum contracts have released their source code, so vulnerability detection at the bytecode level is crucial. This paper introduces SmartBugBert, a novel approach that combines BERT-based deep learning with control flow graph (CFG) analysis to detect vulnerabilities directly from bytecode. Our method first decompiles smart contract bytecode into optimized opcode sequences, extracts semantic features using TF-IDF, constructs control flow graphs to capture execution logic, and isolates vulnerable CFG fragments for targeted analysis. By integrating both semantic and structural information through a fine-tuned BERT model and LightGBM classifier, our approach effectively identifies four critical vulnerability types: transaction-ordering, access control, self-destruct, and timestamp dependency vulnerabilities. Experimental evaluation on 6,157 Ethereum smart contracts demonstrates that SmartBugBert achieves 90.62% precision, 91.76% recall, and 91.19% F1-score, significantly outperforming existing detection methods. Ablation studies confirm that the combination of semantic features with CFG information substantially enhances detection performance. Furthermore, our approach maintains efficient detection speed (0.14 seconds per contract), making it practical for large-scale vulnerability assessment.
]]></content:encoded>
<pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Smart Contract Vulnerability Detection in DApps Leveraging Fine-Tuned LLM</title>
<link>https://arxiv.org/abs/2504.05006</link>
<guid>https://arxiv.org/abs/2504.05006</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. DAppsLLM<br />2. 215DApp4,998<br />3. FFTLoRALlama3-8BQwen2-7BROS<br />4. 0.83F1LLM<br />5. 0.970.68 <div>
arXiv:2504.05006v2 Announce Type: replace 
Abstract: Decentralized applications (DApps) face significant security risks due to vulnerabilities in smart contracts, with traditional detection methods struggling to address emerging and machine-unauditable flaws. This paper proposes a novel approach leveraging fine-tuned Large Language Models (LLMs) to enhance smart contract vulnerability detection. We introduce a comprehensive dataset of 215 real-world DApp projects (4,998 contracts), including hard-to-detect logical errors like token price manipulation, addressing the limitations of existing simplified benchmarks. By fine-tuning LLMs (Llama3-8B and Qwen2-7B) with Full-Parameter Fine-Tuning (FFT) and Low-Rank Adaptation (LoRA), our method achieves superior performance, attaining an F1-score of 0.83 with FFT and data augmentation via Random Over Sampling (ROS). Comparative experiments demonstrate significant improvements over prompt-based LLMs and state-of-the-art tools. Notably, the approach excels in detecting non-machine-auditable vulnerabilities, achieving 0.97 precision and 0.68 recall for price manipulation flaws. The results underscore the effectiveness of domain-specific LLM fine-tuning and data augmentation in addressing real-world DApp security challenges, offering a robust solution for blockchain ecosystem protection.
]]></content:encoded>
<pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Malicious Code Detection in Smart Contracts via Opcode Vectorization</title>
<link>https://arxiv.org/abs/2504.12720</link>
<guid>https://arxiv.org/abs/2504.12720</guid>
<content:encoded><![CDATA[
<div>     <br /><br />:  
1.   
2.   
3. opcode  
4. N-GramN=2TF-IDF  
5.  <div>
arXiv:2504.12720v2 Announce Type: replace 
Abstract: With the booming development of blockchain technology, smart contracts have been widely used in finance, supply chain, Internet of things and other fields in recent years. However, the security problems of smart contracts become increasingly prominent. Security events caused by smart contracts occur frequently, and the existence of malicious codes may lead to the loss of user assets and system crash. In this paper, a simple study is carried out on malicious code detection of intelligent contracts based on machine learning. The main research work and achievements are as follows: Feature extraction and vectorization of smart contract are the first step to detect malicious code of smart contract by using machine learning method, and feature processing has an important impact on detection results. In this paper, an opcode vectorization method based on smart contract text is adopted. Based on considering the structural characteristics of contract opcodes, the opcodes are classified and simplified. Then, N-Gram (N=2) algorithm and TF-IDF algorithm are used to convert the simplified opcodes into vectors, and then put into the machine learning model for training. In contrast, N-Gram algorithm and TF-IDF algorithm are directly used to quantify opcodes and put into the machine learning model training. Judging which feature extraction method is better according to the training results. Finally, the classifier chain is applied to the intelligent contract malicious code detection.
]]></content:encoded>
<pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Blockchain Meets Crawlers: Real-time Market Analytics in Solana NFT Markets</title>
<link>https://arxiv.org/abs/2506.02892</link>
<guid>https://arxiv.org/abs/2506.02892</guid>
<content:encoded><![CDATA[
<div> SolanaNFT-<br /><br />:<br />1. SolanaNFT<br />2. SeleniumScrapyMagic EdenNFT<br />3. NFT<br />4. NFT-NFT<br />5. SolanaNFT <div>
arXiv:2506.02892v2 Announce Type: replace 
Abstract: In this paper, we design and implement a web crawler system based on the Solana blockchain for the automated collection and analysis of market data for popular non-fungible tokens (NFTs) on the chain. Firstly, the basic information and transaction data of popular NFTs on the Solana chain are collected using the Selenium tool. Secondly, the transaction records of the Magic Eden trading market are thoroughly analyzed by combining them with the Scrapy framework to examine the price fluctuations and market trends of NFTs. In terms of data analysis, this paper employs time series analysis to examine the dynamics of the NFT market and seeks to identify potential price patterns. In addition, the risk and return of different NFTs are evaluated using the mean-variance optimization model, taking into account their characteristics, such as illiquidity and market volatility, to provide investors with data-driven portfolio recommendations. The experimental results show that the combination of crawler technology and financial analytics can effectively analyze NFT data on the Solana blockchain and provide timely market insights and investment strategies. This study provides a reference for further exploration in the field of digital currencies.
]]></content:encoded>
<pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>AtomGraph: Tackling Atomicity Violation in Smart Contracts using Multimodal GCNs</title>
<link>https://arxiv.org/abs/2512.02399</link>
<guid>https://arxiv.org/abs/2512.02399</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1.   
2.   
3. AtomGraphGCN  
4. CFG  
5. 96.88%F196.97% <div>
arXiv:2512.02399v2 Announce Type: replace 
Abstract: Smart contracts are a core component of blockchain technology and are widely deployed across various scenarios. However, atomicity violations have become a potential security risk. Existing analysis tools often lack the precision required to detect these issues effectively. To address this challenge, we introduce AtomGraph, an automated framework designed for detecting atomicity violations. This framework leverages Graph Convolutional Networks (GCN) to identify atomicity violations through multimodal feature learning and fusion. Specifically, driven by a collaborative learning mechanism, the model simultaneously learns from two heterogeneous modalities: extracting structural topological features from the contract's Control Flow Graph (CFG) and uncovering deep semantics from its opcode sequence. We designed an adaptive weighted fusion mechanism to dynamically adjust the weights of features from each modality to achieve optimal feature fusion. Finally, GCN detects graph-level atomicity violation on the contract. Comprehensive experimental evaluations demonstrate that AtomGraph achieves 96.88% accuracy and 96.97% F1 score, outperforming existing tools. Furthermore, compared to the concatenation fusion model, AtomGraph improves the F1 score by 6.4%, proving its potential in smart contract security detection.
]]></content:encoded>
<pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>No More Hidden Pitfalls? Exposing Smart Contract Bad Practices with LLM-Powered Hybrid Analysis</title>
<link>https://arxiv.org/abs/2512.15179</link>
<guid>https://arxiv.org/abs/2512.15179</guid>
<content:encoded><![CDATA[
<div> LLMSCALM<br /><br />:<br />1. <br />2. 47<br />3. SCALMLLM<br />4. SCALM<br />5. SCALMLLM <div>
arXiv:2512.15179v2 Announce Type: replace 
Abstract: As the Ethereum platform continues to mature and gain widespread usage, it is crucial to maintain high standards of smart contract writing practices. While bad practices in smart contracts may not directly lead to security issues, they elevate the risk of encountering problems. Therefore, to understand and avoid these bad practices, this paper introduces the first systematic study of bad practices in smart contracts, delving into over 47 specific issues. Specifically, we propose SCALM, an LLM-powered framework featuring two methodological innovations: (1) A hybrid architecture that combines context-aware function-level slicing with knowledge-enhanced semantic reasoning via extensible vectorized pattern matching. (2) A multi-layer reasoning verification system connects low-level code patterns with high-level security principles through syntax, design patterns, and architecture analysis. Our extensive experiments using multiple LLMs and datasets have shown that SCALM outperforms existing tools in detecting bad practices in smart contracts.
]]></content:encoded>
<pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>MemeChain: A Multimodal Cross-Chain Dataset for Meme Coin Forensics and Risk Analysis</title>
<link>https://arxiv.org/abs/2601.22185</link>
<guid>https://arxiv.org/abs/2601.22185</guid>
<content:encoded><![CDATA[
<div> memeMemeChain<br /><br />:  
1. Meme  
2.   
3. MemeChainBNBSolanaBase34,988meme  
4. Logo  
5. meme5.15%24 <div>
arXiv:2601.22185v1 Announce Type: new 
Abstract: The meme coin ecosystem has grown into one of the most active yet least observable segments of the cryptocurrency market, characterized by extreme churn, minimal project commitment, and widespread fraudulent behavior. While countless meme coins are deployed across multiple blockchains, they rely heavily on off-chain web and social infrastructure to signal legitimacy. These very signals are largely absent from existing datasets, which are often limited to single-chain data or lack the multimodal artifacts required for comprehensive risk modeling.
  To address this gap, we introduce MemeChain, a large-scale, open-source, cross-chain dataset comprising 34,988 meme coins across Ethereum, BNB Smart Chain, Solana, and Base. MemeChain integrates on-chain data with off-chain artifacts, including website HTML source code, token logos, and linked social media accounts, enabling multimodal and forensic study of meme coin projects. Analysis of the dataset shows that visual branding is frequently omitted in low-effort deployments, and many projects lack a functional website. Moreover, we quantify the ecosystem's extreme volatility, identifying 1,801 tokens (5.15%) that cease all trading activity within just 24 hours of launch. By providing unified cross-chain coverage and rich off-chain context, MemeChain serves as a foundational resource for research in financial forensics, multimodal anomaly detection, and automated scam prevention in the meme coin ecosystem.
]]></content:encoded>
<pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>ZK-HybridFL: Zero-Knowledge Proof-Enhanced Hybrid Ledger for Federated Learning</title>
<link>https://arxiv.org/abs/2601.22302</link>
<guid>https://arxiv.org/abs/2601.22302</guid>
<content:encoded><![CDATA[
<div>     <br /><br />:  
1. ZK-HybridFLDAGZKP  
2.   
3.   
4. Blade-FLChainFL  
5. Gas <div>
arXiv:2601.22302v1 Announce Type: new 
Abstract: Federated learning (FL) enables collaborative model training while preserving data privacy, yet both centralized and decentralized approaches face challenges in scalability, security, and update validation. We propose ZK-HybridFL, a secure decentralized FL framework that integrates a directed acyclic graph (DAG) ledger with dedicated sidechains and zero-knowledge proofs (ZKPs) for privacy-preserving model validation. The framework uses event-driven smart contracts and an oracle-assisted sidechain to verify local model updates without exposing sensitive data. A built-in challenge mechanism efficiently detects adversarial behavior. In experiments on image classification and language modeling tasks, ZK-HybridFL achieves faster convergence, higher accuracy, lower perplexity, and reduced latency compared to Blade-FL and ChainFL. It remains robust against substantial fractions of adversarial and idle nodes, supports sub-second on-chain verification with efficient gas usage, and prevents invalid updates and orphanage-style attacks. This makes ZK-HybridFL a scalable and secure solution for decentralized FL across diverse environments.
]]></content:encoded>
<pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedDis: A Causal Disentanglement Framework for Federated Traffic Prediction</title>
<link>https://arxiv.org/abs/2601.22578</link>
<guid>https://arxiv.org/abs/2601.22578</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. non-IID<br />2. <br />3. FedDis<br />4. FedDis<br />5. FedDis <div>
arXiv:2601.22578v1 Announce Type: new 
Abstract: Federated learning offers a promising paradigm for privacy-preserving traffic prediction, yet its performance is often challenged by the non-identically and independently distributed (non-IID) nature of decentralized traffic data. Existing federated methods frequently struggle with this data heterogeneity, typically entangling globally shared patterns with client-specific local dynamics within a single representation. In this work, we postulate that this heterogeneity stems from the entanglement of two distinct generative sources: client-specific localized dynamics and cross-client global spatial-temporal patterns. Motivated by this perspective, we introduce FedDis, a novel framework that, to the best of our knowledge, is the first to leverage causal disentanglement for federated spatial-temporal prediction. Architecturally, FedDis comprises a dual-branch design wherein a Personalized Bank learns to capture client-specific factors, while a Global Pattern Bank distills common knowledge. This separation enables robust cross-client knowledge transfer while preserving high adaptability to unique local environments. Crucially, a mutual information minimization objective is employed to enforce informational orthogonality between the two branches, thereby ensuring effective disentanglement. Comprehensive experiments conducted on four real-world benchmark datasets demonstrate that FedDis consistently achieves state-of-the-art performance, promising efficiency, and superior expandability.
]]></content:encoded>
<pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Fixed Rounds: Data-Free Early Stopping for Practical Federated Learning</title>
<link>https://arxiv.org/abs/2601.22669</link>
<guid>https://arxiv.org/abs/2601.22669</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. FL<br />2. FL<br />3. <br />4. 12.5%10.3%<br />5. 4720FL <div>
arXiv:2601.22669v1 Announce Type: new 
Abstract: Federated Learning (FL) facilitates decentralized collaborative learning without transmitting raw data. However, reliance on fixed global rounds or validation data for hyperparameter tuning hinders practical deployment by incurring high computational costs and privacy risks. To address this, we propose a data-free early stopping framework that determines the optimal stopping point by monitoring the task vector's growth rate using solely server-side parameters. The numerical results on skin lesion/blood cell classification demonstrate that our approach is comparable to validation-based early stopping across various state-of-the-art FL methods. In particular, the proposed framework spends an average of 47/20 (skin lesion/blood cell) rounds to achieve over 12.5%/10.3% higher performance than early stopping based on validation data. To the best of our knowledge, this is the first work to propose an early stopping framework for FL methods without using any validation data.
]]></content:encoded>
<pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SolAgent: A Specialized Multi-Agent Framework for Solidity Code Generation</title>
<link>https://arxiv.org/abs/2601.23009</link>
<guid>https://arxiv.org/abs/2601.23009</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1.   
2. LLMs  
3. SolAgent  
4. ForgeSlither  
5. SolEval+SolAgentPass@164.39%LLMsAI IDE39.77% <div>
arXiv:2601.23009v1 Announce Type: new 
Abstract: Smart contracts are the backbone of the decentralized web, yet ensuring their functional correctness and security remains a critical challenge. While Large Language Models (LLMs) have shown promise in code generation, they often struggle with the rigorous requirements of smart contracts, frequently producing code that is buggy or vulnerable. To address this, we propose SolAgent, a novel tool-augmented multi-agent framework that mimics the workflow of human experts. SolAgent integrates a \textbf{dual-loop refinement mechanism}: an inner loop using the \textit{Forge} compiler to ensure functional correctness, and an outer loop leveraging the \textit{Slither} static analyzer to eliminate security vulnerabilities. Additionally, the agent is equipped with file system capabilities to resolve complex project dependencies. Experiments on the SolEval+ Benchmark, a rigorous suite derived from high-quality real-world projects, demonstrate that SolAgent achieves a Pass@1 rate of up to \textbf{64.39\%}, significantly outperforming state-of-the-art LLMs ($\sim$25\%), AI IDEs (e.g., GitHub Copilot), and existing agent frameworks. Moreover, it reduces security vulnerabilities by up to \textbf{39.77\%} compared to human-written baselines. Finally, we demonstrate that the high-quality trajectories generated by SolAgent can be used to distill smaller, open-source models, democratizing access to secure smart contract generation. We release our data and code at https://github.com/openpaperz/SolAgent.
]]></content:encoded>
<pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stablecoin Design with Adversarial-Robust Multi-Agent Systems via Trust-Weighted Signal Aggregation</title>
<link>https://arxiv.org/abs/2601.22168</link>
<guid>https://arxiv.org/abs/2601.22168</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. SAS<br />2. MVF-Composer-Harness<br />3. <br />4. T<br />5. 1,200MVF-ComposerSAS57%3.1 <div>
arXiv:2601.22168v1 Announce Type: cross 
Abstract: Algorithmic stablecoins promise decentralized monetary stability by maintaining a target peg through programmatic reserve management. Yet, their reserve controllers remain vulnerable to regime-blind optimization, calibrating risk parameters on fair-weather data while ignoring tail events that precipitate cascading failures. The March 2020 Black Thursday collapse, wherein MakerDAO's collateral auctions yielded $8.3M in losses and a 15% peg deviation, exposed a critical gap: existing models like SAS systematically omit extreme volatility regimes from covariance estimates, producing allocations optimal in expectation but catastrophic under adversarial stress.
  We present MVF-Composer, a trust-weighted Mean-Variance Frontier reserve controller incorporating a novel Stress Harness for risk-state estimation. Our key insight is deploying multi-agent simulations as adversarial stress-testers: heterogeneous agents (traders, liquidity providers, attackers) execute protocol actions under crisis scenarios, exposing reserve vulnerabilities before they manifest on-chain. We formalize a trust-scoring mechanism T: A -> [0,1] that down-weights signals from agents exhibiting manipulative behavior, ensuring the risk-state estimator remains robust to signal injection and Sybil attacks.
  Across 1,200 randomized scenarios with injected Black-Swan shocks (10% collateral drawdown, 50% sentiment collapse, coordinated redemption attacks), MVF-Composer reduces peak peg deviation by 57% and mean recovery time by 3.1x relative to SAS baselines. Ablation studies confirm the trust layer accounts for 23% of stability gains under adversarial conditions, achieving 72% adversarial agent detection. Our system runs on commodity hardware, requires no on-chain oracles beyond standard price feeds, and provides a reproducible framework for stress-testing DeFi reserve policies.
]]></content:encoded>
<pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Policies for Fair Exchanges of Resources</title>
<link>https://arxiv.org/abs/2410.21214</link>
<guid>https://arxiv.org/abs/2410.21214</guid>
<content:encoded><![CDATA[
<div> MuACMuACL<br /><br />:<br />1. <br />2. MuAC<br />3. MuACL<br />4. MuACMuACLMuACL<br />5. NFT <div>
arXiv:2410.21214v2 Announce Type: replace 
Abstract: People increasingly use digital platforms to exchange resources in accordance with some policies stating what resources users offer and what they require in return. In this paper, we propose a formal model of these environments, focussing on how users' policies are defined and enforced, so ensuring that malicious users cannot take advantage of honest ones. To that end, we introduce the declarative policy language MuAC and equip it with a formal semantics. To determine if a resource exchange is fair, i.e., if it respects the MuAC policies in force, we introduce the non-standard logic MuACL that combines non-linear, linear and contractual aspects, and prove it decidable. Notably, the operator for contractual implication of MuACL is not expressible in linear logic. We define a semantics preserving compilation of MuAC policies into MuACL, thus establishing that exchange fairness is reduced to finding a proof in MuACL. Finally, we show how this approach can be put to work on a blockchain to exchange non-fungible tokens.
]]></content:encoded>
<pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Soft-Label Caching and Sharpening for Communication-Efficient Federated Distillation</title>
<link>https://arxiv.org/abs/2504.19602</link>
<guid>https://arxiv.org/abs/2504.19602</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. FL<br />2. FL<br />3. SCARLET50%<br />4. Enhanced ERA<br />5. SCARLET <div>
arXiv:2504.19602v4 Announce Type: replace 
Abstract: Federated Learning (FL) enables collaborative model training across decentralized clients, enhancing privacy by keeping data local. Yet conventional FL, relying on frequent parameter-sharing, suffers from high communication overhead and limited model heterogeneity. Distillation-based FL approaches address these issues by sharing predictions (soft-labels, i.e., normalized probability distributions) instead, but they often involve redundant transmissions across communication rounds, reducing efficiency. We propose SCARLET, a novel framework integrating synchronized soft-label caching and an enhanced Entropy Reduction Aggregation (Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing cached soft-labels, achieving up to 50% reduction in communication costs compared to existing methods while maintaining competitive accuracy. Enhanced ERA resolves the fundamental instability of conventional temperature-based aggregation, ensuring robust control and high performance in diverse client scenarios. Experimental evaluations demonstrate that SCARLET consistently outperforms state-of-the-art distillation-based FL methods in terms of accuracy and communication efficiency. The implementation of SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.
]]></content:encoded>
<pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>DoS Attacks and Defense Technologies in Blockchain Systems: A Hierarchical Analysis</title>
<link>https://arxiv.org/abs/2507.22611</link>
<guid>https://arxiv.org/abs/2507.22611</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. DoS<br />3. DoS<br />4. DoS<br />5.  <div>
arXiv:2507.22611v2 Announce Type: replace 
Abstract: Blockchain technology is widely used in various fields due to its ability to provide decentralization and trustless security. This is a fundamental understanding held by many advocates, but it is misunderstood, leading participants to fail to recognize the limitations of the security that blockchain can provide. Among all current network attacks, Denial of Service (DoS) attacks pose significant threats due to their ease of execution and destructive potential. This paper, based on the blockchain architecture hierarchy, categorizes and organizes existing DoS attacks, with a focus on explaining the principles and methods of contract layer and consensus layer DoS attacks. Furthermore, this paper comprehensively analyzes and compares commonly used detection methods and defense technologies, which will contribute to strengthening the security and stability of blockchain systems and promoting further innovation and application of blockchain systems.
]]></content:encoded>
<pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Belobog: Move Language Fuzzing Framework For Real-World Smart Contracts</title>
<link>https://arxiv.org/abs/2512.02918</link>
<guid>https://arxiv.org/abs/2512.02918</guid>
<content:encoded><![CDATA[
<div> Move    Belobog<br /><br />:<br />1. MoveSuiAptos<br />2. Move<br />3. Move<br />4. MoveBelobog<br />5. Belobogconcolic109CetusNemo7 <div>
arXiv:2512.02918v2 Announce Type: replace 
Abstract: Move is a research-oriented programming language designed for secure and verifiable smart contract development and has been widely used in managing billions of digital assets in blockchains, such as Sui and Aptos. Move features a strong static type system and explicit resource semantics to enforce safety properties such as the prevention of data races, invalid asset transfers, and entry vulnerabilities. However, smart contracts written in Move may still contain certain vulnerabilities that are beyond the reach of its type system. It is thus essential to validate Move smart contracts. Unfortunately, due to its strong type system, existing smart contract fuzzers are ineffective in producing syntactically or semantically valid transactions to test Move smart contracts. This paper introduces the first fuzzing framework, Belobog, for Move smart contracts. Belobog is type-aware and ensures that all generated and mutated transactions are well-typed. More specifically, for a target Move smart contract, Belobog first constructs a type graph based on Move's type system, and then generates or mutates a transaction based on the graph trace derived from the type graph. In order to overcome the complex checks in Move smart contracts, we further design and implement a concolic executor in Belobog. We evaluated Belobog on 109 real-world Move smart contract projects. The experimental results show that Belobog is able to detect 100% critical and 79% major vulnerabilities manually audited by human experts. We further selected two recent notorious incidents in the Move ecosystem, i.e., Cetus and Nemo. Belobog successfully reproduced full exploits for both of them, without any prior knowledge. Moreover, we applied Belobog on three ongoing auditing projects and found 2 critical, 2 major, and 3 medium new vulnerabilities, all acknowledged by the project developers.
]]></content:encoded>
<pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Infusion of Blockchain to Establish Trustworthiness in AI Supported Software Evolution: A Systematic Literature Review</title>
<link>https://arxiv.org/abs/2601.20918</link>
<guid>https://arxiv.org/abs/2601.20918</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. 31%AI<br />3. AI<br />4. <br />5. AI <div>
arXiv:2601.20918v1 Announce Type: new 
Abstract: Context: Blockchain and AI are increasingly explored to enhance trustworthiness in software engineering (SE), particularly in supporting software evolution tasks. Method: We conducted a systematic literature review (SLR) using a predefined protocol with clear eligibility criteria to ensure transparency, reproducibility, and minimized bias, synthesizing research on blockchain-enabled trust in AI-driven SE tools and processes. Results: Most studies focus on integrating AI in SE, with only 31% explicitly addressing trustworthiness. Our review highlights six recent studies exploring blockchain-based approaches to reinforce reliability, transparency, and accountability in AI-assisted SE tasks. Conclusion: Blockchain enhances trust by ensuring data immutability, model transparency, and lifecycle accountability, including federated learning with blockchain consensus and private data verification. However, inconsistent definitions of trust and limited real-world testing remain major challenges. Future work must develop measurable, reproducible trust frameworks to enable reliable, secure, and compliant AI-driven SE ecosystems, including applications involving large language models.
]]></content:encoded>
<pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Robot Decentralized Collaborative SLAM in Planetary Analogue Environments: Dataset, Challenges, and Lessons Learned</title>
<link>https://arxiv.org/abs/2601.21063</link>
<guid>https://arxiv.org/abs/2601.21063</guid>
<content:encoded><![CDATA[
<div> SLAM<br /><br />:  
1. C-SLAM  
2. C-SLAM  
3. C-SLAM  
4.   
5.  <div>
arXiv:2601.21063v1 Announce Type: new 
Abstract: Decentralized collaborative simultaneous localization and mapping (C-SLAM) is essential to enable multirobot missions in unknown environments without relying on preexisting localization and communication infrastructure. This technology is anticipated to play a key role in the exploration of the Moon, Mars, and other planets. In this article, we share insights and lessons learned from C-SLAM experiments involving three robots operating on a Mars analogue terrain and communicating over an ad hoc network. We examine the impact of limited and intermittent communication on C-SLAM performance, as well as the unique localization challenges posed by planetary-like environments. Additionally, we introduce a novel dataset collected during our experiments, which includes real-time peer-to-peer inter-robot throughput and latency measurements. This dataset aims to support future research on communication-constrained, decentralized multirobot operations.
]]></content:encoded>
<pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Augmented Density-Driven Optimal Control (D2OC) for Decentralized Environmental Mapping</title>
<link>https://arxiv.org/abs/2601.21126</link>
<guid>https://arxiv.org/abs/2601.21126</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. AI<br />2. <br />3. <br />4. MLP-<br />5. Wasserstein <div>
arXiv:2601.21126v1 Announce Type: new 
Abstract: This paper presents an AI-augmented decentralized framework for multi-agent (multi-robot) environmental mapping under limited sensing and communication. While conventional coverage formulations achieve effective spatial allocation when an accurate reference map is available, their performance deteriorates under uncertain or biased priors. The proposed method introduces an adaptive and self-correcting mechanism that enables agents to iteratively refine local density estimates within an optimal transport-based framework, ensuring theoretical consistency and scalability. A dual multilayer perceptron (MLP) module enhances adaptivity by inferring local mean-variance statistics and regulating virtual uncertainty for long-unvisited regions, mitigating stagnation around local minima. Theoretical analysis rigorously proves convergence under the Wasserstein metric, while simulation results demonstrate that the proposed AI-augmented Density-Driven Optimal Control consistently achieves robust and precise alignment with the ground-truth density, yielding substantially higher-fidelity reconstruction of complex multi-modal spatial distributions compared with conventional decentralized baselines.
]]></content:encoded>
<pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoCo: A One-Stop Shop for Model Collaboration Research</title>
<link>https://arxiv.org/abs/2601.21257</link>
<guid>https://arxiv.org/abs/2601.21257</guid>
<content:encoded><![CDATA[
<div> MoCo<br /><br />:  
1.   
2. MoCoPython  
3. MoCo26logit25  
4. 61.0%25.8%  
5. MoCo/AI <div>
arXiv:2601.21257v1 Announce Type: new 
Abstract: Advancing beyond single monolithic language models (LMs), recent research increasingly recognizes the importance of model collaboration, where multiple LMs collaborate, compose, and complement each other. Existing research on this topic has mostly been disparate and disconnected, from different research communities, and lacks rigorous comparison. To consolidate existing research and establish model collaboration as a school of thought, we present MoCo: a one-stop Python library of executing, benchmarking, and comparing model collaboration algorithms at scale. MoCo features 26 model collaboration methods, spanning diverse levels of cross-model information exchange such as routing, text, logit, and model parameters. MoCo integrates 25 evaluation datasets spanning reasoning, QA, code, safety, and more, while users could flexibly bring their own data. Extensive experiments with MoCo demonstrate that most collaboration strategies outperform models without collaboration in 61.0% of (model, data) settings on average, with the most effective methods outperforming by up to 25.8%. We further analyze the scaling of model collaboration strategies, the training/inference efficiency of diverse methods, highlight that the collaborative system solves problems where single LMs struggle, and discuss future work in model collaboration, all made possible by MoCo. We envision MoCo as a valuable toolkit to facilitate and turbocharge the quest for an open, modular, decentralized, and collaborative AI future.
]]></content:encoded>
<pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ira: Efficient Transaction Replay for Distributed Systems</title>
<link>https://arxiv.org/abs/2601.21286</link>
<guid>https://arxiv.org/abs/2601.21286</guid>
<content:encoded><![CDATA[
<div> hints<br /><br />:  
1.   
2. Irahints  
3. Ira-L  
4. 240047KB5%  
5. 28.6%10.9%256.516 <div>
arXiv:2601.21286v1 Announce Type: new 
Abstract: In primary-backup replication, consensus latency is bounded by the time for backup nodes to replay (re-execute) transactions proposed by the primary. In this work, we present Ira, a framework to accelerate backup replay by transmitting compact \emph{hints} alongside transaction batches. Our key insight is that the primary, having already executed transactions, possesses knowledge of future access patterns which is exactly the information needed for optimal replay.
  We use Ethereum for our case study and present a concrete protocol, Ira-L, within our framework to improve cache management of Ethereum block execution. The primaries implementing Ira-L provide hints that consist of the working set of keys used in an Ethereum block and one byte of metadata per key indicating the table to read from, and backups use these hints for efficient block replay.
  We evaluated Ira-L against the state-of-the-art Ethereum client reth over two weeks of Ethereum mainnet activity ($100,800$ blocks containing over $24$ million transactions). Our hints are compact, adding a median of $47$ KB compressed per block ($\sim5\%$ of block payload). We observe that the sequential hint generation and block execution imposes a $28.6\%$ wall-time overhead on the primary, though the direct cost from hints is $10.9\%$ of execution time; all of which can be pipelined and parallelized in production deployments. On the backup side, we observe that Ira-L achieves a median per-block speedup of $25\times$ over baseline reth. With $16$ prefetch threads, aggregate replay time drops from $6.5$ hours to $16$ minutes ($23.6\times$ wall-time speedup).
]]></content:encoded>
<pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Analysis Approach for Oscillation Damping in Grid-Forming and Grid-Following Heterogeneous Power Systems</title>
<link>https://arxiv.org/abs/2601.21562</link>
<guid>https://arxiv.org/abs/2601.21562</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:  
1. LGCIBR  
2. LGCIBR  
3.   
4. LGCIBRIBR  
5. LGCIBR <div>
arXiv:2601.21562v1 Announce Type: new 
Abstract: This letter proposes a decentralized local gain condition (LGC) to guarantee oscillation damping in inverter-based resource (IBR)-dominated power systems. The LGC constrains the dynamic gain between each IBR and the network at its point of connection. By satisfying the LGC locally, the closed-loop poles are confined to a desired region, thereby yielding system-wide oscillation damping without requiring global information. Notably, the LGC is agnostic to different IBR dynamics, well-suited for systems with heterogeneous IBRs, and flexible to various damping requirements. Moreover, a low-complexity algorithm is proposed to parameterize LGC, providing scalable and damping-constrained parameter tuning guidance for IBRs.
]]></content:encoded>
<pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is My RPC Response Reliable? Detecting RPC Bugs in Ethereum Blockchain Client under Context</title>
<link>https://arxiv.org/abs/2601.21593</link>
<guid>https://arxiv.org/abs/2601.21593</guid>
<content:encoded><![CDATA[
<div> RPCEthCRAFT<br /><br />:<br />1. RPCRPC<br />2. RPC<br />3. EthCRAFT<br />4. EthCRAFTRPC5<br />5. EthCRAFTEthereum631 <div>
arXiv:2601.21593v1 Announce Type: new 
Abstract: Blockchain clients are fundamental software for running blockchain nodes. They provide users with various RPC (Remote Procedure Call) interfaces to interact with the blockchain. These RPC methods are expected to follow the same specification across different blockchain nodes, providing users with seamless interaction. However, there have been continuous reports on various RPC bugs that can cause unexpected responses or even Denial of Service weakness. Existing studies on blockchain RPC bug detection mainly focus on generating the RPC method calls for testing blockchain clients. However, a wide range of the reported RPC bugs are triggered in various blockchain contexts. To the best of our knowledge, little attention is paid to generating proper contexts that can trigger these context-dependent RPC bugs.
  In this work, we propose EthCRAFT, a Context-aware RPC Analysis and Fuzzing Tool for client RPC bug detection. EthCRAFT first proposes to explore the state transition program space of blockchain clients and generate various transactions to construct the context. EthCRAFT then designs a context-aware RPC method call generation method to send RPC calls to the blockchain clients. The responses of 5 different client implementations are used as cross-referring oracles to detect the RPC bugs. We evaluate EthCRAFT on real-world RPC bugs collected from the GitHub issues of Ethereum client implementations. Experiment results show that EthCRAFT outperforms existing client RPC detectors by detecting more RPC bugs. Moreover, EthCRAFT has found six new bugs in major Ethereum clients and reported them to the developers. One of the bug fixes has been written into breaking changes in the client's updates. Three of our bug reports have been offered a vulnerability bounty by the Ethereum Foundation.
]]></content:encoded>
<pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic</title>
<link>https://arxiv.org/abs/2601.21972</link>
<guid>https://arxiv.org/abs/2601.21972</guid>
<content:encoded><![CDATA[
<div> -<br /><br />:  
1. LLM  
2.   
3.   
4. -MAACCoLLM-CCCoLLM-DC  
5. CoLLM-DCCoLLM-CC <div>
arXiv:2601.21972v1 Announce Type: new 
Abstract: Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \textbf{CoLLM-CC} with a \textbf{C}entralized \textbf{C}ritic and \textbf{CoLLM-DC} with \textbf{D}ecentralized \textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.
]]></content:encoded>
<pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fundamental Limits of Decentralized Self-Regulating Random Walks</title>
<link>https://arxiv.org/abs/2601.21489</link>
<guid>https://arxiv.org/abs/2601.21489</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. SRRW<br />2.  $\Lambda_{\mathrm{del}}$  $q$<br />3.  $A_{\mathrm{eff}}$<br />4. <br />5.  <div>
arXiv:2601.21489v1 Announce Type: cross 
Abstract: Self-regulating random walks (SRRWs) are decentralized token-passing processes on a graph allowing nodes to locally \emph{fork}, \emph{terminate}, or \emph{pass} tokens based only on a return-time \emph{age} statistic. We study SRRWs on a finite connected graph under a lazy reversible walk, with exogenous \emph{trap} deletions summarized by the absorption pressure $\Lambda_{\mathrm{del}}=\sum_{u\in\mathcal P_{\mathrm{trap}}}\zeta(u)\pi(u)$ and a global per-visit fork cap $q$. Using exponential envelopes for return-time tails, we build graph-dependent Laplace envelopes that universally bound the stationary fork intensity of any age-based policy, leading to an effective triggering age $A_{\mathrm{eff}}$. A mixing-based block drift analysis then yields controller-agnostic stability limits: any policy that avoids extinction and explosion must satisfy a \emph{viability} inequality (births can overcome $\Lambda_{\mathrm{del}}$ at low population) and a \emph{safety} inequality (trap deletions plus deliberate terminations dominate births at high population). Under corridor-wise versions of these conditions, we obtain positive recurrence of the population to a finite corridor.
]]></content:encoded>
<pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>FairDAG: Consensus Fairness over Multi-Proposer Causal Design</title>
<link>https://arxiv.org/abs/2504.02194</link>
<guid>https://arxiv.org/abs/2504.02194</guid>
<content:encoded><![CDATA[
<div> DAG<br /><br />:<br />1. DeFi<br />2. PompeThemis<br />3. FairDAGDAG<br />4. FairDAGDAG<br />5. FairDAG-ABFairDAG-RL <div>
arXiv:2504.02194v3 Announce Type: replace 
Abstract: The rise of cryptocurrencies like Bitcoin and Ethereum has driven interest in blockchain database technology, with smart contracts enabling the growth of decentralized finance (DeFi). However, research has shown that adversaries exploit transaction ordering to extract profits through attacks like front-running, sandwich attacks, and liquidation manipulation. This issue affects blockchains where block proposers have full control over transaction ordering. To address this, a more fair transaction ordering mechanism is essential.
  Existing fairness protocols, such as Pompe and Themis, operate on leader-based consensus protocols, which not only suffer from low throughput caused by the single-leader bottleneck, but also allow adversarial block proposers to manipulate transaction ordering. To address these limitations, we propose a new framework, FairDAG, that runs fairness protocols on top of DAG-based consensus protocols. FairDAG improves protocol performance in both throughput and fairness quality by leveraging the multi-proposer design and validity property of DAG-based consensus protocols.
  We conducted a comprehensive analytical and experimental evaluation of two FairDAG variants - FairDAG-AB and FairDAG-RL. Our results demonstrate that FairDAG outperforms prior fairness protocols in both throughput and fairness quality.
]]></content:encoded>
<pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empowering Scientific Workflows with Federated Agents</title>
<link>https://arxiv.org/abs/2505.05428</link>
<guid>https://arxiv.org/abs/2505.05428</guid>
<content:encoded><![CDATA[
<div> Agentic<br /><br />:<br />1. Agentic<br />2. AcademyHPC<br />3. Academy<br />4. <br />5. HPC <div>
arXiv:2505.05428v3 Announce Type: replace 
Abstract: Agentic systems, in which diverse agents cooperate to tackle challenging problems, are exploding in popularity in the AI community. However, existing agentic frameworks take a relatively narrow view of agents, apply a centralized model, and target conversational, cloud-native applications (e.g., LLM-based AI chatbots). In contrast, scientific applications require myriad agents be deployed and managed across diverse cyberinfrastructure. Here we introduce Academy, a modular and extensible middleware designed to deploy autonomous agents across the federated research ecosystem, including HPC systems, experimental facilities, and data repositories. To meet the demands of scientific computing, Academy supports asynchronous execution, heterogeneous resources, high-throughput data flows, and dynamic resource availability. It provides abstractions for expressing stateful agents, managing inter-agent coordination, and integrating computation with experimental control. We present microbenchmark results that demonstrate high performance and scalability in HPC environments. To explore the breadth of applications that can be supported by agentic workflow designs, we also present case studies in materials discovery, astronomy, decentralized learning, and information extraction in which agents are deployed across diverse HPC systems.
]]></content:encoded>
<pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Next-Generation Grid Codes: Toward a New Paradigm for Dynamic Ancillary Services</title>
<link>https://arxiv.org/abs/2601.07090</link>
<guid>https://arxiv.org/abs/2601.07090</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. NGGC  
2.   
3.   
4.   
5.  <div>
arXiv:2601.07090v2 Announce Type: replace 
Abstract: This paper presents preliminary results toward a conceptual foundation for Next Generation Grid Codes (NGGCs) based on decentralized stability and performance certification for dynamic ancillary services. The proposed NGGC framework targets two core outcomes: (i) guaranteed closed-loop stability and (ii) explicit performance assurances for power-system frequency and voltage dynamics. Stability is addressed using loop-shifting and passivity-based methods that yield local frequency-domain certificates for individual devices, enabling fully decentralized verification of the interconnected system. Performance is characterized by deriving quantitative bounds on key time-domain metrics (e.g., nadirs, rate-of-change-of-frequency (RoCoF), steady-state deviations, and oscillation damping) through frequency-domain constraints on local device behavior. The framework is non-parametric and model-agnostic, accommodating a broad class of device dynamics under mild assumptions, and provides an initial unified approach to stability and performance certification without explicit device-model parameterization. As such, these results offer a principled starting point for the development of future grid codes and control design methodologies in modern power systems.
]]></content:encoded>
<pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Cooperative Reach-While-Avoid Certificates for Interconnected Systems</title>
<link>https://arxiv.org/abs/2601.20324</link>
<guid>https://arxiv.org/abs/2601.20324</guid>
<content:encoded><![CDATA[
<div> -<br /><br />:<br />1. -<br />2. -<br />3. <br />4. <br />5.  <div>
arXiv:2601.20324v1 Announce Type: new 
Abstract: Providing formal guarantees for neural network-based controllers in large-scale interconnected systems remains a fundamental challenge. In particular, using neural certificates to capture cooperative interactions and verifying these certificates at scale is crucial for the safe deployment of such controllers. However, existing approaches fall short on both fronts. To address these limitations, we propose neural cooperative reach-while-avoid certificates with Dynamic-Localized Vector Control Lyapunov and Barrier Functions, which capture cooperative dynamics through state-dependent neighborhood structures and provide decentralized certificates for global exponential stability and safety. Based on the certificates, we further develop a scalable training and verification framework that jointly synthesizes controllers and neural certificates via a constrained optimization objective, and leverages a sufficient condition to ensure formal guarantees considering modeling error. To improve scalability, we introduce a structural reuse mechanism to transfer controllers and certificates between substructure-isomorphic systems. The proposed methodology is validated with extensive experiments on multi-robot coordination and vehicle platoons. Results demonstrate that our framework ensures certified cooperative reach-while-avoid while maintaining strong control performance.
]]></content:encoded>
<pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>LIFT: Byzantine Resilient Hub-Sampling</title>
<link>https://arxiv.org/abs/2601.20368</link>
<guid>https://arxiv.org/abs/2601.20368</guid>
<content:encoded><![CDATA[
<div> ElevatorLIFT<br /><br />:<br />1. Elevator<br />2. /churn2%<br />3. LIFTPRNG<br />4. LIFT10%<br />5.  <div>
arXiv:2601.20368v1 Announce Type: new 
Abstract: Recently, a novel peer sampling protocol, Elevator, was introduced to construct network topologies tailored for emerging decentralized applications such as federated learning and blockchain. Elevator builds hub-based topologies in a fully decentralized manner, randomly selecting hubs among participating nodes. These hubs, acting as central nodes connected to the entire network, can be leveraged to accelerate message dissemination. Simulation results have shown that Elevator converges rapidly (within 3--4 cycles) and exhibits robustness against crash failures and churn. However, its resilience to Byzantine adversaries has not been investigated. In this work, we provide the first evaluation of Elevator under Byzantine adversaries and show that even a small fraction (2%) of Byzantine nodes is sufficient to subvert the network. As a result, we introduce LIFT, a new protocol that extends Elevator by employing a cryptographically secure pseudo-random number generator (PRNG) for hub selection, thereby mitigating Byzantine manipulation. In contrast, LIFT withstands adversarial infiltration and remains robust with up to 10% Byzantine nodes. These results highlight the necessity of secure randomness in decentralized hub formation and position LIFT as a more reliable building block for Byzantine-resilient decentralized systems.
]]></content:encoded>
<pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Distributed Learning under Resource Constraints: Decentralized Quantile Estimation via (Asynchronous) ADMM</title>
<link>https://arxiv.org/abs/2601.20571</link>
<guid>https://arxiv.org/abs/2601.20571</guid>
<content:encoded><![CDATA[
<div> AsylADMM<br /><br />:<br />1. <br />2.  gossip  ADMM <br />3.  AsylADMM <br />4. <br />5.  <div>
arXiv:2601.20571v1 Announce Type: new 
Abstract: Specifications for decentralized learning on resource-constrained edge devices require algorithms that are communication-efficient, robust to data corruption, and lightweight in memory usage. While state-of-the-art gossip-based methods satisfy the first requirement, achieving robustness remains challenging. Asynchronous decentralized ADMM-based methods have been explored for estimating the median, a statistical centrality measure that is notoriously more robust than the mean. However, existing approaches require memory that scales with node degree, making them impractical when memory is limited. In this paper, we propose AsylADMM, a novel gossip algorithm for decentralized median and quantile estimation, primarily designed for asynchronous updates and requiring only two variables per node. We analyze a synchronous variant of AsylADMM to establish theoretical guarantees and empirically demonstrate fast convergence for the asynchronous algorithm. We then show that our algorithm enables quantile-based trimming, geometric median estimation, and depth-based trimming, with quantile-based trimming empirically outperforming existing rank-based methods. Finally, we provide a novel theoretical analysis of rank-based trimming via Markov chain theory.
]]></content:encoded>
<pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lila: Decentralized Build Reproducibility Monitoring for the Functional Package Management Model</title>
<link>https://arxiv.org/abs/2601.20662</link>
<guid>https://arxiv.org/abs/2601.20662</guid>
<content:encoded><![CDATA[
<div> Lila<br /><br />:  
1.   
2.   
3. Nix890%  
4. Lila  
5. Lila <div>
arXiv:2601.20662v1 Announce Type: new 
Abstract: Ensuring the integrity of software build artifacts is an increasingly important concern for modern software engineering, driven by increasingly sophisticated attacks on build systems, distribution channels, and development infrastructures. Reproducible builds $\unicode{x2013}$ where binaries built independently from the same source code can be verified to be bit-for-bit identical to the distributed artifacts $\unicode{x2013}$ provide a principled foundation for transparency and trust in software distribution.
  Despite their potential, the large-scale adoption of reproducible builds faces two significant challenges: achieving high reproducibility rates across vast software collections and establishing reproducibility monitoring infrastructure that can operate at very large scale. While recent studies have shown that high reproducibility rates are achievable at scale $\unicode{x2013}$ demonstrated by the Nix ecosystem achieving over 90% reproducibility on more than 80,000 packages $\unicode{x2013}$ the problem of effective reproducibility monitoring remains largely unsolved.
  In this work, we address the reproducibility monitoring challenge by introducing Lila, a decentralized system for reproducibility assessment tailored to the functional package management model. Lila enables distributed reporting of build results and aggregation into a reproducibility database, benefiting both practitioners and future empirical build reproducibility studies.
]]></content:encoded>
<pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Identity in Practice: Benchmarking Latency, Cost, and Privacy</title>
<link>https://arxiv.org/abs/2601.20716</link>
<guid>https://arxiv.org/abs/2601.20716</guid>
<content:encoded><![CDATA[
<div> Decentralized Identifiers<br /><br />:<br />1. HederaXRP LedgerDID<br />2. /<br />3. MLS<br />4. DIDXRP LedgerHederaSDK<br />5. SDKDID <div>
arXiv:2601.20716v1 Announce Type: new 
Abstract: Decentralized Identifiers (DIDs) are increasingly deployed on distributed ledgers, yet systematic cross-platform evidence on their operational behavior remains limited. We present an empirical benchmarking study of three prominent ledger-based DID methods - Ethereum, Hedera, and XRP Ledger - using reference Software Development Kits (SDKs) under a unified experimental setup. We measure latency, transaction cost, and on-chain metadata exposure, normalizing latency by each platform's block or consensus interval and cost by its native value transfer fee. Privacy leakage is quantified using a Metadata-Leakage Score (MLS), an entropy-based measure expressed in bits per operation.
  Our results reveal distinct architectural trade-offs. Ethereum enables near-instant, off-chain DID creation, but incurs the highest latency and cost for on-chain lifecycle operations. XRPL delivers deterministic and stable latency with fixed, low fees, yet exhibits higher metadata leakage due to more verbose transaction payloads. Hedera achieves the lowest on-chain latency and low fees with minimal metadata leakage, while occasional variance arises from SDK-side processing and confirmation pipelines.
  Overall, the findings show that ledger architecture and SDK workflows play a major role in shaping DID latency, cost, and metadata exposure, complementing the effects of the underlying consensus mechanism. These results provide evidence-based insights to support informed selection and configuration of DID systems under performance and privacy constraints.
]]></content:encoded>
<pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agentic Fog: A Policy-driven Framework for Distributed Intelligence in Fog Computing</title>
<link>https://arxiv.org/abs/2601.20764</link>
<guid>https://arxiv.org/abs/2601.20764</guid>
<content:encoded><![CDATA[
<div>     <br /><br />:  
1. Agentic FogAF  
2. AF  
3.   
4.   
5. AF <div>
arXiv:2601.20764v1 Announce Type: new 
Abstract: Fog and edge computing require adaptive control schemes that can handle partial observability, severe latency requirements, and dynamically changing workloads. Recent research on Agentic AI (AAI) increasingly integrates reasoning systems powered by Large Language Models; however, these tools are not applicable to infrastructure-level systems due to their high computational cost, stochastic nature, and poor formal analyzability. In this paper, a generic model, Agentic Fog (AF), is presented, in which fog nodes are represented as policy-driven autonomous agents that communicate via p2p interactions based on shared memory and localized coordination. The suggested architecture decomposes a system's goals into abstract policy guidance and formalizes decentralized fog coordination as an exact potential game. The framework is guaranteed to converge and remain stable under asynchronous updates, bounded-rational best-response dynamics, and node failures. Simulations demonstrate that the AF system achieves lower average latency and adapts more efficiently to varying demand than greedy heuristics and integer linear programming under dynamic conditions. The sensitivity analysis also demonstrates the capability to perform optimally under different memory and coordination conditions.
]]></content:encoded>
<pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Monotone Priority System: Foundations of Contract-Specific Sequencing</title>
<link>https://arxiv.org/abs/2601.20783</link>
<guid>https://arxiv.org/abs/2601.20783</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1.   
2.   
3.   
4.   
5.  <div>
arXiv:2601.20783v1 Announce Type: new 
Abstract: Modern blockchain applications benefit from the ability to specify sequencing constraints on the transactions that interact with them. This paper proposes a principled and axiomatically justified way of adding sequencing constraints on smart contract function calls that balances expressivity with the tractability of block production. Specifically, we propose a system in which contract developers are allowed to set an integer global priority for each of their calls, so long as that the call's chosen priority is no higher than the priority of any of its referenced calls. Block builders must then simply sequence transactions in priority order (from high to low priority), breaking ties however they would like. We show that this system is the unique system that satisfies five independent axioms.
]]></content:encoded>
<pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Multi-Agent Systems: Challenges and Open Problems</title>
<link>https://arxiv.org/abs/2402.03578</link>
<guid>https://arxiv.org/abs/2402.03578</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1.   
2.   
3.   
4.   
5.  <div>
arXiv:2402.03578v3 Announce Type: replace 
Abstract: This paper explores multi-agent systems and identify challenges that remain inadequately addressed. By leveraging the diverse capabilities and roles of individual agents, multi-agent systems can tackle complex tasks through agent collaboration. We discuss optimizing task allocation, fostering robust reasoning through iterative debates, managing complex and layered context information, and enhancing memory management to support the intricate interactions within multi-agent systems. We also explore potential applications of multi-agent systems in blockchain systems to shed light on their future development and application in real-world distributed systems.
]]></content:encoded>
<pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equilibrium Selection for Multi-agent Reinforcement Learning: A Unified Framework</title>
<link>https://arxiv.org/abs/2406.08844</link>
<guid>https://arxiv.org/abs/2406.08844</guid>
<content:encoded><![CDATA[
<div> -<br /><br />:<br />1. MARL<br />2. <br />3. <br />4. --<br />5.  <div>
arXiv:2406.08844v2 Announce Type: replace 
Abstract: While multi-agent reinforcement learning (MARL) has produced numerous algorithms that converge to Nash or related equilibria, such equilibria are often non-unique and can exhibit widely varying efficiency.
  This raises a fundamental question: how can one design learning dynamics that not only converge to equilibrium but also select equilibria with desirable performance, such as high social welfare? In contrast to the MARL literature, equilibrium selection has been extensively studied in normal-form games, where decentralized dynamics are known to converge to potential-maximizing or Pareto-optimal Nash equilibria (NEs). Motivated by these results, we study equilibrium selection in finite-horizon stochastic games. We propose a unified actor-critic framework in which a critic learns state-action value functions, and an actor applies a classical equilibrium-selection rule state-wise, treating learned values as stage-game payoffs. We show that, under standard stochastic stability assumptions, the stochastically stable policies of the resulting dynamics inherit the equilibrium selection properties of the underlying normal-form learning rule. As consequences, we obtain potential-maximizing policies in Markov potential games and Pareto-optimal (Markov perfect) equilibria in general-sum stochastic games, together with sample-based implementation of the framework.
]]></content:encoded>
<pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>DGRAG: Distributed Graph-based Retrieval-Augmented Generation in Edge-Cloud Systems</title>
<link>https://arxiv.org/abs/2505.19847</link>
<guid>https://arxiv.org/abs/2505.19847</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. RAGDGRAG<br />2. DGRAG-<br />3. <br />4. <br />5. DGRAG <div>
arXiv:2505.19847v2 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) improves factuality by grounding LLMs in external knowledge, yet conventional centralized RAG requires aggregating distributed data, raising privacy risks and incurring high retrieval latency and cost. We present DGRAG, a distributed graph-driven RAG framework for edge-cloud collaborative systems. Each edge device organizes local documents into a knowledge graph and periodically uploads subgraph-level summaries to the cloud for lightweight global indexing without exposing raw data. At inference time, queries are first answered on the edge; a gate mechanism assesses the confidence and consistency of multiple local generations to decide whether to return a local answer or escalate the query. For escalated queries, the cloud performs summary-based matching to identify relevant edges, retrieves supporting evidence from them, and generates the final response with a cloud LLM. Experiments on distributed question answering show that DGRAG consistently outperforms decentralized baselines while substantially reducing cloud overhead.
]]></content:encoded>
<pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Information-Theoretic Secure Aggregation over Regular Graphs</title>
<link>https://arxiv.org/abs/2601.19183</link>
<guid>https://arxiv.org/abs/2601.19183</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:  
1. TSA  
2. TSA  
3. d-  
4.   
5. dd <div>
arXiv:2601.19183v1 Announce Type: new 
Abstract: Large-scale decentralized learning frameworks such as federated learning (FL), require both communication efficiency and strong data security, motivating the study of secure aggregation (SA). While information-theoretic SA is well understood in centralized and fully connected networks, its extension to decentralized networks with limited local connectivity remains largely unexplored. This paper introduces \emph{topological secure aggregation} (TSA), which studies one-shot, information-theoretically secure aggregation of neighboring users' inputs over arbitrary network topologies. We develop a unified linear design framework that characterizes TSA achievability through the spectral properties of the communication graph, specifically the kernel of a diagonally modulated adjacency matrix. For several representative classes of $d$-regular graphs including ring, prism and complete topologies, we establish the optimal communication and secret key rate region. In particular, to securely compute one symbol of the neighborhood sum, each user must (i) store at least one key symbol, (ii) broadcast at least one message symbol, and (iii) collectively, all users must hold at least $d$ i.i.d. key symbols. Notably, this total key requirement depends only on the \emph{neighborhood size} $d$, independent of the network size, revealing a fundamental limit of SA in decentralized networks with limited local connectivity.
]]></content:encoded>
<pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reuse of Public Keys Across UTXO and Account-Based Cryptocurrencies</title>
<link>https://arxiv.org/abs/2601.19500</link>
<guid>https://arxiv.org/abs/2601.19500</guid>
<content:encoded><![CDATA[
<div> UTXO<br /><br />:  
1.   
2.   
3.   
4. Zcash  
5. UTXO <div>
arXiv:2601.19500v1 Announce Type: new 
Abstract: It is well known that reusing cryptocurrency addresses undermines privacy. This also applies if the same addresses are used in different cryptocurrencies. Nevertheless, cross-chain address reuse appears to be a recurring phenomenon, especially in EVM-based designs. Previous works performed either direct address matching, or basic format conversion, to identify such cases. However, seemingly incompatible address formats e.g., in Bitcoin and Ethereum, can also be derived from the same public keys, since they rely on the same cryptographic primitives. In this paper, we therefore focus on the underlying public keys to discover reuse within, as well as across, different cryptocurrency networks, enabling us to also match incompatible address formats. Specifically, we analyze key reuse across Bitcoin, Ethereum, Litecoin, Dogecoin, Zcash and Tron. Our results reveal that cryptographic keys are extensively and actively reused across these networks, negatively impacting both privacy and security of their users. We are hence the first to expose and quantify cross-chain key reuse between UTXO and account-based cryptocurrencies. Moreover, we devise novel clustering methods across these different cryptocurrency networks that do not rely on heuristics and instead link entities by their knowledge of the underlying secret key.
]]></content:encoded>
<pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Serve Your Sandwich? MEV Attacks in Private L2 Mempools</title>
<link>https://arxiv.org/abs/2601.19570</link>
<guid>https://arxiv.org/abs/2601.19570</guid>
<content:encoded><![CDATA[
<div> RollupMEV<br /><br />:  
1.   
2.   
3. Rollup  
4. L1Rollup  
5. L2MEV <div>
arXiv:2601.19570v1 Announce Type: new 
Abstract: We study the feasibility, profitability, and prevalence of sandwich attacks on Ethereum rollups with private mempools. First, we extend a formal model of optimal front- and back-run sizing, relating attack profitability to victim trade volume, liquidity depth, and slippage bounds. We complement it with an execution-feasibility model that quantifies co-inclusion constraints under private mempools. Second, we examine execution constraints in the absence of builder markets: without guaranteed atomic inclusion, attackers must rely on sequencer ordering, redundant submissions, and priority fee placement, which renders sandwiching probabilistic rather than deterministic. Third, using transaction-level data from major rollups, we show that naive heuristics overstate sandwich activity. We find that the majority of flagged patterns are false positives and that the median net return for these attacks is negative. Our results suggest that sandwiching, while endemic and profitable on Ethereum L1, is rare, unprofitable, and largely absent in rollups with private mempools. These findings challenge prevailing assumptions, refine measurement of MEV in L2s, and inform the design of sequencing policies.
]]></content:encoded>
<pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Sovereign Identity and eIDAS 2.0: An Analysis of Control, Privacy, and Legal Implications</title>
<link>https://arxiv.org/abs/2601.19837</link>
<guid>https://arxiv.org/abs/2601.19837</guid>
<content:encoded><![CDATA[
<div> eIDAS 2.0Self-Sovereign IdentitySSI<br /><br />:  
1.   
2. eIDAS 2.0 1999  
3.  eIDAS 1.0   
4.  eIDAS 2.0   
5. ARF Self-Sovereign IdentitySSI <div>
arXiv:2601.19837v1 Announce Type: new 
Abstract: European digital identity initiatives are grounded in regulatory frameworks designed to ensure interoperability and robust, harmonized security standards. The evolution of these frameworks culminates in eIDAS 2.0, whose origins trace back to the Electronic Signatures Directive 1999/93/EC, the first EU-wide legal foundation for the use of electronic signatures in cross-border electronic transactions. As technological capabilities advanced, the initial eIDAS 1.0 framework was increasingly criticized for its limitations and lack of comprehensiveness. Emerging decentralized approaches further exposed these shortcomings and introduced the possibility of integrating innovative identity paradigms, such as Self-Sovereign Identity (SSI) models.
  In this article, we analyse key provisions of the eIDAS 2.0 Regulation and its accompanying recitals, drawing on existing literature to identify legislative gaps and implementation challenges. Furthermore, we examine the European Digital Identity Architecture and Reference Framework (ARF), assessing its proposed guidelines and evaluating the extent to which its emerging implementations align with SSI principles.
]]></content:encoded>
<pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enabling SSI-Compliant Use of EUDI Wallet Credentials through Trusted Execution Environment and Zero-Knowledge Proof</title>
<link>https://arxiv.org/abs/2601.19893</link>
<guid>https://arxiv.org/abs/2601.19893</guid>
<content:encoded><![CDATA[
<div> eIDAS 2.0SSI<br /><br />:<br />1. eIDAS<br />2. eIDASCIESPIDeIDAS 2.0IO<br />3. SSIeIDAS 2.0<br />4. EUDIWSSI<br />5. SSI <div>
arXiv:2601.19893v1 Announce Type: new 
Abstract: The passing of the eIDAS amendment marks an important milestone for EU countries and changes how they must manage digital credentials for both public services and businesses. Italy has led in adopting eIDAS, first with CIE and SPID identity schemes, and now with the Italian Wallet (IO app) aligned to eIDAS 2.0. Self-Sovereign Identity (SSI) is a decentralized model born from the success of Distributed Ledgers, giving individuals full control over their digital identity. The current eIDAS 2.0 and its implementation acts diverge from SSI principles, rendering the European Digital Identity Wallet (EUDIW) centralized and merely user-centric, prioritizing security and legal protection over true self-sovereignty.
  This paper proposes an architecture that enables the use of IT Wallet credentials and services in an SSI-compliant environment through Trusted Execution Environments and Zero-Knowledge Proofs.
]]></content:encoded>
<pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Nonsmooth Nonconvex Optimization with Client Sampling</title>
<link>https://arxiv.org/abs/2601.19381</link>
<guid>https://arxiv.org/abs/2601.19381</guid>
<content:encoded><![CDATA[
<div> Goldstein<br /><br />:  
1. Lipschitz  
2. $(\delta,\epsilon)$-Goldstein  
3. $\mathcal{O}(\delta^{-1}\epsilon^{-3})$$\tilde{\mathcal{O}}(\gamma^{-1/2}\delta^{-1}\epsilon^{-3})$$\gamma$  
4.   
5.  <div>
arXiv:2601.19381v1 Announce Type: cross 
Abstract: This paper considers decentralized nonsmooth nonconvex optimization problem with Lipschitz continuous local functions. We propose an efficient stochastic first-order method with client sampling, achieving the $(\delta,\epsilon)$-Goldstein stationary point with the overall sample complexity of ${\mathcal O}(\delta^{-1}\epsilon^{-3})$, the computation rounds of ${\mathcal O}(\delta^{-1}\epsilon^{-3})$, and the communication rounds of ${\tilde{\mathcal O}}(\gamma^{-1/2}\delta^{-1}\epsilon^{-3})$, where $\gamma$ is the spectral gap of the mixing matrix for the network. Our results achieve the optimal sample complexity and the sharper communication complexity than existing methods. We also extend our ideas to zeroth-order optimization. Moreover, the numerical experiments show the empirical advantage of our methods.
]]></content:encoded>
<pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Automated Smart Contract Generation: Evaluation, Benchmarking, and Retrieval-Augmented Repair</title>
<link>https://arxiv.org/abs/2503.01098</link>
<guid>https://arxiv.org/abs/2503.01098</guid>
<content:encoded><![CDATA[
<div> SoliditySolBench<br /><br />:<br />1. <br />2. SoliditySolBenchEtherscan760428825<br />3. 14<br />4. RAR<br />5. RARSolBenchRARSolidity <div>
arXiv:2503.01098v2 Announce Type: replace 
Abstract: Smart contracts, predominantly written in Solidity and deployed on blockchains such as Ethereum, are immutable after deployment, making functional correctness critical. However, existing evaluations of Solidity code generation rely largely on surface-level metrics (e.g., BLEU, CrystalBLEU) or manual inspection, which correlate poorly with functional correctness. In contrast to Python, Solidity lacks large-scale, execution-based benchmarks, limiting systematic evaluation of large language models for smart contract development.
  We introduce SolBench, a comprehensive benchmark and automated testing pipeline for Solidity that emphasizes functional correctness via differential fuzzing. SolBench consists of 28825 functions extracted from 7604 real-world smart contracts collected from Etherscan (genesis-2024), spanning ten application domains. We benchmark 14 diverse LLMs, covering open and closed models, 1.3B-671B parameters, and both general-purpose and code-specialized architectures. The dominant failure mode is missing critical intra-contract information, such as state variables and type definitions. Providing full-contract context improves accuracy but incurs prohibitive inference costs.
  To address this, we propose Retrieval-Augmented Repair (RAR), a cost-effective framework that integrates execution feedback into code repair. RAR uses compiler and runtime error messages to retrieve only the minimal contract snippets needed to correct a target function, avoiding full-context inference. This significantly reduces input length while improving functional correctness. We further analyze retrieval and repair strategies within RAR, demonstrating consistent gains in accuracy and efficiency. SolBench and RAR enable principled, execution-based evaluation and economical improvement of Solidity code generation. Dataset and code are publicly available at https://github.com/ZaoyuChen/SolBench.
]]></content:encoded>
<pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>DPLib: A Standard Benchmark Library for Distributed Power System Analysis and Optimization</title>
<link>https://arxiv.org/abs/2506.20819</link>
<guid>https://arxiv.org/abs/2506.20819</guid>
<content:encoded><![CDATA[
<div> DPLib  MATLAB  ADMM<br /><br />:<br />1. DPLibMATLAB<br />2. MATPOWER<br />3. DPLib20MATPOWER<br />4. .mat.m<br />5. OPFADMMDC-OPFYALMIPAC-OPFIPOPT <div>
arXiv:2506.20819v3 Announce Type: replace 
Abstract: \textit{DPLib} is an open-source MATLAB-based benchmark library created to support research and development in distributed and decentralized power system analysis and optimization. Distributed and decentralized methods offer scalability, privacy preservation, and resilience to single points of failure, making them increasingly important for modern power systems. However, unlike centralized tools such as MATPOWER, no general-purpose, reproducible data library package currently exists for distributed power system studies. DPLib, available at \href{https://github.com/LSU-RAISE-LAB/DPLib.git}{GitHub}, fills this gap by providing a standard power system library featuring over 20 multi-region benchmark test cases of varying sizes, along with a graph-based partitioning toolkit that decomposes any MATPOWER test system into multiple electrically coherent regions. The partitioning toolkit, an easy-to-use MATLAB code, generates standardized \texttt{.mat} and \texttt{.m} files, along with region visualizations for intuitive understanding. We also provide modular, easy-to-use distributed optimal power flow (OPF) solvers: an alternating direction method of multipliers(ADMM)-based DC-OPF solver implemented in YALMIP, and an ADMM-based AC-OPF solver leveraging IPOPT. These solvers validate the generated test systems for distributed optimization applications. Numerical results validate the generated test cases, establishing DPLib as a foundation for reproducible distributed power system research.
]]></content:encoded>
<pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SABRE-FL: Selective and Accurate Backdoor Rejection for Federated Prompt Learning</title>
<link>https://arxiv.org/abs/2506.22506</link>
<guid>https://arxiv.org/abs/2506.22506</guid>
<content:encoded><![CDATA[
<div> SABRE-FL<br /><br />:<br />1. CLIP<br />2. <br />3. SABRE-FL<br />4. SABRE-FL<br />5.  <div>
arXiv:2506.22506v2 Announce Type: replace 
Abstract: Federated Prompt Learning has emerged as a communication-efficient and privacy-preserving paradigm for adapting large vision-language models like CLIP across decentralized clients. However, the security implications of this setup remain underexplored. In this work, we present the first study of backdoor attacks in Federated Prompt Learning. We show that when malicious clients inject visually imperceptible, learnable noise triggers into input images, the global prompt learner becomes vulnerable to targeted misclassification while still maintaining high accuracy on clean inputs. Motivated by this vulnerability, we propose SABRE-FL, a lightweight, modular defense that filters poisoned prompt updates using an embedding-space anomaly detector trained offline on out-of-distribution data. SABRE-FL requires no access to raw client data or labels and generalizes across diverse datasets. We show, both theoretically and empirically, that malicious clients can be reliably identified and filtered using an embedding-based detector. Across five diverse datasets and four baseline defenses, SABRE-FL outperforms all baselines by significantly reducing backdoor accuracy while preserving clean accuracy, demonstrating strong empirical performance and underscoring the need for robust prompt learning in future federated systems.
]]></content:encoded>
<pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Deep Reinforcement Learning Under Constrained Communications</title>
<link>https://arxiv.org/abs/2601.17069</link>
<guid>https://arxiv.org/abs/2601.17069</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. -CTDE<br />2. D-GAT<br />3. D-GATDG-MAPPO/<br />4. StarCraftIIGoogle Research FootballMulti-Agent MujocoDG-MAPPOCTDE<br />5.  <div>
arXiv:2601.17069v1 Announce Type: new 
Abstract: Centralized training with decentralized execution (CTDE) has been the dominant paradigm in multi-agent reinforcement learning (MARL), but its reliance on global state information during training introduces scalability, robustness, and generalization bottlenecks. Moreover, in practical scenarios such as adding/dropping teammates or facing environment dynamics that differ from the training, CTDE methods can be brittle and costly to retrain, whereas distributed approaches allow agents to adapt using only local information and peer-to-peer communication. We present a distributed MARL framework that removes the need for centralized critics or global information. Firstly, we develop a novel Distributed Graph Attention Network (D-GAT) that performs global state inference through multi-hop communication, where agents integrate neighbor features via input-dependent attention weights in a fully distributed manner. Leveraging D-GAT, we develop the distributed graph-attention MAPPO (DG-MAPPO) -- a distributed MARL framework where agents optimize local policies and value functions using local observations, multi-hop communication, and shared/averaged rewards. Empirical evaluation on the StarCraftII Multi-Agent Challenge, Google Research Football, and Multi-Agent Mujoco demonstrates that our method consistently outperforms strong CTDE baselines, achieving superior coordination across a wide range of cooperative tasks with both homogeneous and heterogeneous teams. Our distributed MARL framework provides a principled and scalable solution for robust collaboration, eliminating the need for centralized training or global observability. To the best of our knowledge, DG-MAPPO appears to be the first to fully eliminate reliance on privileged centralized information, enabling agents to learn and act solely through peer-to-peer communication.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Collaborate: An Orchestrated-Decentralized Framework for Peer-to-Peer LLM Federation</title>
<link>https://arxiv.org/abs/2601.17133</link>
<guid>https://arxiv.org/abs/2601.17133</guid>
<content:encoded><![CDATA[
<div> LinUCB<br /><br />:  
1. KNEXA-FL  
2. DFLFL  
3. CPMLinUCB  
4. PEFT  
5. KNEXA-FLP2P50%Pass@1 <div>
arXiv:2601.17133v1 Announce Type: new 
Abstract: Fine-tuning Large Language Models (LLMs) for specialized domains is constrained by a fundamental challenge: the need for diverse, cross-organizational data conflicts with the principles of data privacy and sovereignty. While Federated Learning (FL) provides a framework for collaboration without raw data exchange, its classic centralized form introduces a single point of failure and remains vulnerable to model inversion attacks. Decentralized FL (DFL) mitigates this risk by removing the central aggregator but typically relies on inefficient, random peer-to-peer (P2P) pairings, forming a collaboration graph that is blind to agent heterogeneity and risks negative transfer. This paper introduces KNEXA-FL, a novel framework for orchestrated decentralization that resolves this trade-off. KNEXA-FL employs a non-aggregating Central Profiler/Matchmaker (CPM) that formulates P2P collaboration as a contextual bandit problem, using a LinUCB algorithm on abstract agent profiles to learn an optimal matchmaking policy. It orchestrates direct knowledge exchange between heterogeneous, PEFT-based LLM agents via secure distillation, without ever accessing the models themselves. Our comprehensive experiments on a challenging code generation task show that KNEXA-FL yields substantial gains, improving Pass@1 by approx. 50% relative to random P2P collaboration. Critically, our orchestrated approach demonstrates stable convergence, in stark contrast to a powerful centralized distillation baseline which suffers from catastrophic performance collapse. Our work establishes adaptive, learning-based orchestration as a foundational principle for building robust and effective decentralized AI ecosystems.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Multi-Agent Swarms for Autonomous Grid Security in Industrial IoT: A Consensus-based Approach</title>
<link>https://arxiv.org/abs/2601.17303</link>
<guid>https://arxiv.org/abs/2601.17303</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. IIoTDMAS<br />2. AI<br />3. CVT<br />4. DMAS20000.8597.3%87%<br />5. 89% <div>
arXiv:2601.17303v1 Announce Type: new 
Abstract: As Industrial Internet of Things (IIoT) environments expand to include tens of thousands of connected devices. The centralization of security monitoring architectures creates serious latency issues that savvy attackers can exploit to compromise an entire manufacturing ecosystem. This paper outlines a new, decentralized multi-agent swarm (DMAS) architecture that includes autonomous artificial intelligence (AI) agents at each edge gateway, functioning as a distributed digital "immune system" for IIoT networks. Instead of using a traditional static firewall approach, the DMAS agents communicate via a lightweight peer-to-peer protocol to cooperatively detect anomalous behavior across the IIoT network without sending data to a cloud infrastructure. The authors also outline a consensus-based threat validation (CVT) process in which agents vote on the threat level of an identified threat, enabling instant quarantine of a compromised node or nodes. The authors conducted experiments on a testbed that simulated an innovative factory environment with 2000 IIoT devices and found that the DMAS demonstrated sub-millisecond response times (average of 0.85ms), 97.3% accuracy in detecting malicious activity under high load, and 87% accuracy in detecting zero-day attacks. All significantly higher than baseline values for both centralized and edge computing. Additionally, the proposed architecture can prevent real-time cascading failures in industrial control systems and reduce network bandwidth use by 89% compared to cloud-based solutions.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Scores to Queues: Operationalizing Cross-Chain Obfuscation Signals for Smart-Contract Audits</title>
<link>https://arxiv.org/abs/2601.17356</link>
<guid>https://arxiv.org/abs/2601.17356</guid>
<content:encoded><![CDATA[
<div> HObfNET<br /><br />:  
1.   
2. HObfNETObfs_Tool89ms/23005200  
3. BSCPolygonAvalanchep99/p99.9  
4.   
5. Transit SwapNew Free DAOp99 <div>
arXiv:2601.17356v1 Announce Type: new 
Abstract: Obfuscation substantially increases the interpretation cost of smart-contract auditing, while the comparability and transferability of obfuscation signals across chains remain unclear. We present HObfNET as an efficient surrogate of Obfs_Tool (ObfProbe), enabling fast cross-chain scoring at scale. The model aligns well with tool outputs on Ethereum (PCC 0.9158, MAPE 8.20 percent) and achieves 8-9 ms per contract, a 2.3k-5.2k times speedup over second-level Obfs_Tool runs, enabling million-scale scoring. On large BSC, Polygon, and Avalanche corpora, we find systematic score drift: fixed-threshold transfer inflates and deflates candidate queues, motivating within-chain main and extreme thresholds (p99 and p99.9) and an actionable queueing strategy. The high-score tail exhibits rare selectors, external-call opcode enrichment, and low signature density; a proxy indicator is enriched in the BSC high-score queue, enabling secondary triage. Cross-chain reuse analysis shows tail enrichment and directional diffusion, with traceable same-hash cases across chains. In publicly alignable incident samples, all fall into the p99 queue; Transit Swap DEX Hack and New Free DAO Flash Loan exhibit cross-chain spillover, indicating real-world hit and prioritization value. We deliver a two-tier audit queue and cross-chain linkage workflow to support practical multi-chain security operations.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Res-MIA: A Training-Free Resolution-Based Membership Inference Attack on Federated Learning Models</title>
<link>https://arxiv.org/abs/2601.17378</link>
<guid>https://arxiv.org/abs/2601.17378</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. MIA<br />2. <br />3. Res-MIAMIA<br />4. Res-MIA<br />5. CIFAR-10ResNet-18AUC0.88 <div>
arXiv:2601.17378v1 Announce Type: new 
Abstract: Membership inference attacks (MIAs) pose a serious threat to the privacy of machine learning models by allowing adversaries to determine whether a specific data sample was included in the training set. Although federated learning (FL) is widely regarded as a privacy-aware training paradigm due to its decentralized nature, recent evidence shows that the final global model can still leak sensitive membership information through black-box access. In this paper, we introduce Res-MIA, a novel training-free and black-box membership inference attack that exploits the sensitivity of deep models to high-frequency input details. Res-MIA progressively degrades the input resolution using controlled downsampling and restoration operations, and analyzes the resulting confidence decay in the model's predictions. Our key insight is that training samples exhibit a significantly steeper confidence decline under resolution erosion compared to non-member samples, revealing a robust membership signal. Res-MIA requires no shadow models, no auxiliary data, and only a limited number of forward queries to the target model. We evaluate the proposed attack on a federated ResNet-18 trained on CIFAR-10, where it consistently outperforms existing training-free baselines and achieves an AUC of up to 0.88 with minimal computational overhead. These findings highlight frequency-sensitive overfitting as an important and previously underexplored source of privacy leakage in federated learning, and emphasize the need for privacy-aware model designs that reduce reliance on fine-grained, non-robust input features.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discovery of Feasible 3D Printing Configurations for Metal Alloys via AI-driven Adaptive Experimental Design</title>
<link>https://arxiv.org/abs/2601.17587</link>
<guid>https://arxiv.org/abs/2601.17587</guid>
<content:encoded><![CDATA[
<div> AIGRCop-42

<br /><br />:  
1.   
2. AI  
3. NASAGRCop-42  
4.   
5. GRCop-42 <div>
arXiv:2601.17587v1 Announce Type: new 
Abstract: Configuring the parameters of additive manufacturing processes for metal alloys is a challenging problem due to complex relationships between input parameters (e.g., laser power, scan speed) and quality of printed outputs. The standard trial-and-error approach to find feasible parameter configurations is highly inefficient because validating each configuration is expensive in terms of resources (physical and human labor) and the configuration space is very large. This paper combines the general principles of AI-driven adaptive experimental design with domain knowledge to address the challenging problem of discovering feasible configurations. The key idea is to build a surrogate model from past experiments to intelligently select a small batch of input configurations for validation in each iteration. To demonstrate the effectiveness of this methodology, we deploy it for Directed Energy Deposition process to print GRCop--42, a high-performance copper--chromium--niobium alloy developed by NASA for aerospace applications. Within three months, our approach yielded multiple defect-free outputs across a range of laser powers dramatically reducing time to result and resource expenditure compared to several months of manual experimentation by domain scientists with no success. By enabling high-quality GRCop--42 fabrication on readily available infrared laser platforms for the first time, we democratize access to this critical alloy, paving the way for cost-effective, decentralized production for aerospace applications.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representative Litigation Settlement Agreements in Artificial Intelligence Copyright Infringement Disputes: A Comparative Reflection Based on the U.S</title>
<link>https://arxiv.org/abs/2601.17631</link>
<guid>https://arxiv.org/abs/2601.17631</guid>
<content:encoded><![CDATA[
<div> AI<br /><br />:<br />1. AI<br />2. <br />3. <br />4. Bartz<br />5. 573 <div>
arXiv:2601.17631v1 Announce Type: new 
Abstract: The high-density, decentralized copyright conflicts triggered by generative AI training require more than ad hoc solutions; they demand structural governance tools. This article argues that representative litigation settlement agreements offer a distinct institutional advantage. Beyond reducing the transaction costs associated with the "tragedy of the anticommons," these agreements generate market-visible evidence, specifically pricing signals and licensing practices, that validate the "potential market" under the fourth factor of fair use. This phenomenon constitutes procedural market-making. Through a comparative analysis of the U.S. Bartz class action settlement, this study reveals a dual motivation: a surface-level drive for risk aversion and remedy locking, and a deeper logic of constructing a training-licensing market. In the context of Chinese law, the feasibility of such agreements depends not on replicating foreign models, but on establishing three interpretive mechanisms: expanding the functional definition of "same category" claims; adopting a hybrid registration/confirmation system for indeterminate class membership; and converting the "consent" requirement under Article 57, Paragraph 3 of the Civil Procedure Law into a workable opt-out right subject to judicial scrutiny.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Performance Analysis of Quantum-Secure Digital Signature Algorithms in Blockchain</title>
<link>https://arxiv.org/abs/2601.17785</link>
<guid>https://arxiv.org/abs/2601.17785</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. Shor<br />3. PQ<br />4. CRYSTALS-DilithiumFalconHawkHAETAE<br />5.  <div>
arXiv:2601.17785v1 Announce Type: new 
Abstract: The long-term security of public blockchains strictly depends on the hardness assumptions of the underlying digital signature schemes. In the current scenario, most deployed cryptocurrencies and blockchain platforms rely on elliptic-curve cryptography, which is vulnerable to quantum attacks due to Shor's algorithm. Therefore, it is important to understand how post-quantum (PQ) digital signatures behave when integrated into real blockchain systems. This report presents a blockchain prototype that supports multiple quantum-secure signature algorithms, focusing on CRYSTALS-Dilithium, Falcon and Hawk as lattice-based schemes. This report also describes the design of the prototype and discusses the performance metrics, which include key generation, signing, verification times, key sizes and signature sizes. This report covers the problem, background, and experimental methodology, also providing a detailed comparison of quantum-secure signatures in a blockchain context and extending the analysis to schemes such as HAETAE.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedGraph-VASP: Privacy-Preserving Federated Graph Learning with Post-Quantum Security for Cross-Institutional Anti-Money Laundering</title>
<link>https://arxiv.org/abs/2601.17935</link>
<guid>https://arxiv.org/abs/2601.17935</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. VASPsFedGraph-VASP  
2.   
3. NISTKyber-512AES-256-GCM  
4. EllipticFedGraph-VASPF10.508FedSage+0.4530.620  
5. FedSage+R=0.32 <div>
arXiv:2601.17935v1 Announce Type: new 
Abstract: Virtual Asset Service Providers (VASPs) face a fundamental tension between regulatory compliance and user privacy when detecting cross-institutional money laundering. Current approaches require either sharing sensitive transaction data or operating in isolation, leaving critical cross-chain laundering patterns undetected. We present FedGraph-VASP, a privacy-preserving federated graph learning framework that enables collaborative anti-money laundering (AML) without exposing raw user data. Our key contribution is a Boundary Embedding Exchange protocol that shares only compressed, non-invertible graph neural network representations of boundary accounts. These exchanges are secured using post-quantum cryptography, specifically the NIST-standardized Kyber-512 key encapsulation mechanism combined with AES-256-GCM authenticated encryption. Experiments on the Elliptic Bitcoin dataset with realistic Louvain partitioning show that FedGraph-VASP achieves an F1-score of 0.508, outperforming the state-of-the-art generative baseline FedSage+ (F1 = 0.453) by 12.1 percent on binary fraud detection. We further show robustness under low-connectivity settings where generative imputation degrades performance, while approaching centralized performance (F1 = 0.620) in high-connectivity regimes. We additionally evaluate generalization on an Ethereum fraud detection dataset, where FedGraph-VASP (F1 = 0.635) is less effective under sparse cross-silo connectivity, while FedSage+ excels (F1 = 0.855), outperforming even local training (F1 = 0.785). These results highlight a topology-dependent trade-off: embedding exchange benefits connected transaction graphs, whereas generative imputation can dominate in highly modular sparse graphs. A privacy audit shows embeddings are only partially invertible (R^2 = 0.32), limiting exact feature recovery.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Information-Theoretic Secure Aggregation in Decentralized Networks</title>
<link>https://arxiv.org/abs/2601.17970</link>
<guid>https://arxiv.org/abs/2601.17970</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. DSA  
2. K  
3. T  
4. DSA111  
5. K1 <div>
arXiv:2601.17970v1 Announce Type: new 
Abstract: Motivated by the increasing demand for data security in decentralized federated learning (FL) and stochastic optimization, we formulate and investigate the problem of information-theoretic \emph{decentralized secure aggregation} (DSA). Specifically, we consider a network of $K$ interconnected users, each holding a private input, representing, for example, local model updates in FL, who aim to simultaneously compute the sum of all inputs while satisfying the security requirement that no user, even when colluding with up to $T$ others, learns anything beyond the intended sum. We characterize the optimal rate region, which specifies the minimum achievable communication and secret key rates for DSA. In particular, we show that to securely compute one bit of the desired input sum, each user must (i) transmit at least one bit to all other users, (ii) hold at least one bit of secret key, and (iii) all users must collectively hold no fewer than $K - 1$ independent key bits. Our result establishes the fundamental performance limits of DSA and offers insights into the design of provably secure and communication-efficient protocols for distributed learning systems.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated learning for unpaired multimodal data through a homogeneous transformer model</title>
<link>https://arxiv.org/abs/2601.17986</link>
<guid>https://arxiv.org/abs/2601.17986</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. CKAGram<br />3. <br />4. <br />5.  <div>
arXiv:2601.17986v1 Announce Type: new 
Abstract: Training of multimodal foundation models is currently restricted to centralized data centers containing massive, aligned datasets (e.g., image-text pairs). However, in realistic federated environments, data is often unpaired and fragmented across disjoint nodes; one node may hold sensor data, while another holds textual logs. These datasets are strictly private and share no common samples. Current federated learning (FL) methods fail in this regime, as they assume local clients possess aligned pairs or require sharing raw feature embeddings, which violates data sovereignty. We propose a novel framework to train a global multimodal transformer across decentralized nodes with disjoint modalities. We introduce a small public anchor set to align disjoint private manifolds. Using Gram matrices calculated from these public anchors, we enforce semantic alignment across modalities through centered kernel alignment without ever transmitting private samples, offering a mathematically superior privacy guarantee compared to prototype sharing. Further, we introduce a subspace-stabilized fine-tuning method to handle FL with huge transformer models. We strictly decouple domain-specific magnitude shifts from semantic direction, ensuring that nodes with varying sensor characteristics align geometrically to the global consensus. Lastly, we propose precision weighted averaging, where efficiently obtained uncertainty estimates are used to downweight uncertain nodes. This paper establishes the mathematical backbone for federated unpaired foundation models, enabling a global model to learn a unified representation of the world from fragmented, disjoint, and private data silos without requiring centralized storage or paired samples.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiChain Blockchain Data Provenance for Deterministic Stream Processing with Kafka Streams: A Weather Data Case Study</title>
<link>https://arxiv.org/abs/2601.18011</link>
<guid>https://arxiv.org/abs/2601.18011</guid>
<content:encoded><![CDATA[
<div> Merkle<br /><br />:<br />1. <br />2. Kafka StreamsMerkleKafkaMultiChain<br />3. <br />4. <br />5.  <div>
arXiv:2601.18011v1 Announce Type: new 
Abstract: Auditability and reproducibility still are critical challenges for real-time data streams pipelines. Streaming engines are highly dependent on runtime scheduling, window triggers, arrival orders, and uncertainties such as network jitters. These all derive the streaming pipeline platforms to throw non-determinist outputs. In this work, we introduce a blockchain-backed provenance architecture for streaming platform (e.g Kafka Streams) the publishes cryptographic data of a windowed data stream without publishing window payloads on-chain. We used real-time weather data from weather stations in Berlin. Weather records are canonicalized, deduplicated, and aggregated per window, then serialised deterministically. Furthermore, the Merkle root of the records within the window is computed and stored alongside with Kafka offsets boundaries to MultiChain blockchain streams as checkpoints. Our design can enable an independent auditor to verify: (1) the completeness of window payloads, (2) canonical serialization, and (3) correctness of derived analytics such as minimum/maximum/average temperatures. We evaluated our system using real data stream from two weather stations (Berlin-Brandenburg and Berlin-Tempelhof) and showed linear verification cost, deterministic reproducibility, and with a scalable off-chain storage with on-chain cryptographic anchoring. We also demonstrated that the blockchain can afford to be integrated with streaming platforms particularly with our system, and we get satisfactory transactions per second values.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Multi-product Pricing: Diagonal Dominance, Nash Equilibrium, and Price of Anarchy</title>
<link>https://arxiv.org/abs/2601.18117</link>
<guid>https://arxiv.org/abs/2601.18117</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. <br />3. <br />4. 4(1)/(2)<br />5.  <div>
arXiv:2601.18117v1 Announce Type: new 
Abstract: Decentralized decision making in multi--product firms can lead to efficiency losses when autonomous decision makers fail to internalize cross--product demand interactions. This paper quantifies the magnitude of such losses by analyzing the Price of Anarchy in a pricing game in which each decision maker independently sets prices to maximize its own product--level revenue. We model demand using a linear system that captures both substitution and complementarity effects across products. We first establish existence and uniqueness of a pure--strategy Nash equilibrium under economically standard diagonal dominance conditions. Our main contribution is the derivation of a tight worst--case lower bound on the ratio between decentralized revenue and the optimal centralized revenue. We show that this efficiency loss is governed by a single scalar parameter, denoted by $\mu$, which measures the aggregate strength of cross--price effects relative to own--price sensitivities. In particular, we prove that the revenue ratio is bounded below by $4(1-\mu)/(2-\mu)^2$, and we demonstrate the tightness of this bound by constructing a symmetric market topology in which the bound is exactly attained. We further refine the analysis by providing an instance--exact characterization of efficiency loss based on the spectral properties of the demand interaction matrix. Together, these results offer a quantitative framework for assessing the trade--off between centralized pricing and decentralized autonomy in multi--product firms.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Bandwidth Consumption of Blockchains</title>
<link>https://arxiv.org/abs/2601.18400</link>
<guid>https://arxiv.org/abs/2601.18400</guid>
<content:encoded><![CDATA[
<div>     <br /><br />:  
1.   
2. AlgorandAptosAvalancheRedbelly  Solana  
3.   
4.   
5.  <div>
arXiv:2601.18400v1 Announce Type: new 
Abstract: With the advent of blockchain technology, the number of proposals has boomed. The network traffic imposed by these blockchain proposals increases the cost of hosting nodes. Unfortunately, as of today, we are not aware of any comparative study of the bandwidth consumption of blockchains.
  In this paper, we propose the first empirical comparison of blockchain bandwidth consumption. To this end, we measure the network traffic of blockchain network nodes of five blockchain protocols: Algorand, Aptos, Avalanche, Redbelly and Solana. We study the variation over time, differentiate the receiving and sending traffic and analyze how this traffic varies with the number of nodes and validators.
  We conclude that the transport protocol is the main factor impacting the network traffic, segregating node roles helps reduce traffic and different blockchains are differently impacted by the network size.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Keep Your Doubts to Yourself? Trading Visual Uncertainties in Multi-Agent Bandit Systems</title>
<link>https://arxiv.org/abs/2601.18735</link>
<guid>https://arxiv.org/abs/2601.18735</guid>
<content:encoded><![CDATA[
<div> Vision-Language Models    Agora<br /><br />:  
1. VLMs  
2. Agora  
3. Agora  
4. Thompson Sampling  
5. AgoraMMMU8.5% <div>
arXiv:2601.18735v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) enable powerful multi-agent systems, but scaling them is economically unsustainable: coordinating heterogeneous agents under information asymmetry often spirals costs. Existing paradigms, such as Mixture-of-Agents and knowledge-based routers, rely on heuristic proxies that ignore costs and collapse uncertainty structure, leading to provably suboptimal coordination. We introduce Agora, a framework that reframes coordination as a decentralized market for uncertainty. Agora formalizes epistemic uncertainty into a structured, tradable asset (perceptual, semantic, inferential), and enforces profitability-driven trading among agents based on rational economic rules. A market-aware broker, extending Thompson Sampling, initiates collaboration and guides the system toward cost-efficient equilibria. Experiments on five multimodal benchmarks (MMMU, MMBench, MathVision, InfoVQA, CC-OCR) show that Agora outperforms strong VLMs and heuristic multi-agent strategies, e.g., achieving +8.5% accuracy over the best baseline on MMMU while reducing cost by over 3x. These results establish market-based coordination as a principled and scalable paradigm for building economically viable multi-agent visual intelligence systems.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Access Control to Usage Control with User-Managed Access</title>
<link>https://arxiv.org/abs/2601.18761</link>
<guid>https://arxiv.org/abs/2601.18761</guid>
<content:encoded><![CDATA[
<div> UMAODRLSolid<br /><br />:<br />1. Web<br />2. UMASolid<br />3. W3C ODRL<br />4. ODRLSolid<br />5. ODRL <div>
arXiv:2601.18761v1 Announce Type: new 
Abstract: Recent data protection and data governance regulations have intensified the demand for interoperable, decentralized data ecosystems that can support not only access control but also legally-aligned governance over data use. Existing Web-based data storage platforms increasingly struggle to meet these regulatory and practical requirements, as their authorization mechanisms rely on tightly coupled, document-centric access control models that lack expressiveness for legal constraints and fail to separate data management from authorization concerns. In parallel, widely adopted authorization standards remain poorly aligned with decentralized, semantically rich usage-control scenarios. To bridge this gap, this work introduces an architecture that replaces Solid's native access control mechanisms with a UMA authorization flow, enabling the enforcement of usage control policies expressed with the W3C ODRL standard. This article details the conceptual background motivating this approach, presents the proposed UMA-based architecture, and describes a prototype implementation that integrates an ODRL-enabled Authorization Server with a Solid-compatible Resource Server. The prototype demonstrates that decoupling authorization from storage enables more flexible, interoperable, and legally expressive control over data use, while remaining compatible with existing Solid infrastructure. It also highlights practical design choices required to evaluate ODRL policies in the absence of a fully standardized evaluation semantics. Moreover, this work shows how usage control can be operationalized using existing Web standards, offering a concrete path beyond permission-based access control toward policy-aware, legally informed data governance. Future research will focus on policy management interfaces, richer claim verification mechanisms, and techniques for communicating and enforcing obligations over time.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural-Inspired Multi-Agent Molecular Communication Networks for Collective Intelligence</title>
<link>https://arxiv.org/abs/2601.18018</link>
<guid>https://arxiv.org/abs/2601.18018</guid>
<content:encoded><![CDATA[
<div> Greenberg-Hastings<br /><br />:<br />1. <br />2. Greenberg-Hastings<br />3. <br />4. <br />5.  <div>
arXiv:2601.18018v1 Announce Type: cross 
Abstract: Molecular Communication (MC) is a pivotal enabler for the Internet of Bio-Nano Things (IoBNT). However, current research often relies on super-capable individual agents with complex transceiver architectures that defy the energy and processing constraints of realistic nanomachines. This paper proposes a paradigm shift towards collective intelligence, inspired by the cortical networks of the biological brain. We introduce a decentralized network architecture where simple nanomachines interact via a diffusive medium using a threshold-based firing mechanism modeled by Greenberg-Hastings (GH) cellular automata. We derive fixed-point equations for steady-state populations via mean-field analysis and validate them against stochastic simulations. We demonstrate that the network undergoes a second-order phase transition at a specific activation threshold. Crucially, we show that both pairwise and collective mutual information peak exactly at this critical transition point, confirming that the system maximizes information propagation and processing capacity at the edge of chaos.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint Resource Optimization, Computation Offloading and Resource Slicing for Multi-Edge Traffic-Cognitive Networks</title>
<link>https://arxiv.org/abs/2411.17782</link>
<guid>https://arxiv.org/abs/2411.17782</guid>
<content:encoded><![CDATA[
<div> Stackelberg<br /><br />:<br />1. ESs<br />2. StackelbergESs<br />3. <br />4.  <div>
arXiv:2411.17782v2 Announce Type: replace 
Abstract: The evolving landscape of edge computing envisions platforms operating as dynamic intermediaries between application providers and edge servers (ESs), where task offloading is coupled with payments for computational services. Ensuring efficient resource utilization and meeting stringent Quality of Service (QoS) requirements necessitates incentivizing ESs while optimizing the platforms operational objectives. This paper investigates a multi-agent system where both the platform and ESs are self-interested entities, addressing the joint optimization of revenue maximization, resource allocation, and task offloading. We propose a novel Stackelberg game-based framework to model interactions between stakeholders and solve the optimization problem using a Bayesian Optimization-based centralized algorithm. Recognizing practical challenges in information collection due to privacy concerns, we further design a decentralized solution leveraging neural network optimization and a privacy-preserving information exchange protocol. Extensive numerical evaluations demonstrate the effectiveness of the proposed mechanisms in achieving superior performance compared to existing baselines.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding and Characterizing Obfuscated Funds Transfers in Ethereum Smart Contracts</title>
<link>https://arxiv.org/abs/2505.11320</link>
<guid>https://arxiv.org/abs/2505.11320</guid>
<content:encoded><![CDATA[
<div> ObfProbe<br /><br />:<br />1. DeFiNFT<br />2. ObfProbeZ-score<br />3. 1033000MEV<br />4. <br />5. PonziSourceP80%12% <div>
arXiv:2505.11320v2 Announce Type: replace 
Abstract: Scam contracts on Ethereum have rapidly evolved alongside the rise of DeFi and NFT ecosystems, utilizing increasingly complex code obfuscation techniques to avoid early detection. This paper systematically investigates how obfuscation amplifies the financial risks of fraudulent contracts and undermines existing auditing tools. We propose a transfer-centric obfuscation taxonomy, distilling seven key features, and introduce ObfProbe, a framework that performs bytecode-level smart contract analysis to uncover obfuscation techniques and quantify obfuscation complexity via Z-score ranking. In a large-scale study of 1.03 million Ethereum contracts, we isolate over 3 000 highly obfuscated contracts and identify two scam archetypes, three high-risk contract categories, and MEV bots that employ a variety of obfuscation maneuvers such as inline assembly, dead code insertion, and deep function splitting. We further show that obfuscation substantially increases both the scale of financial damage and the time until detection. Finally, we evaluate SourceP, a state-of-the-art Ponzi detection tool, on obfuscated versus non-obfuscated samples and observe its accuracy drop from approximately 80 percent to approximately 12 percent in real-world scenarios. These findings highlight the urgent need for enhanced anti-obfuscation analysis techniques and broader community collaboration to stem the proliferation of scam contracts in the expanding DeFi ecosystem.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Backpressure-based Mean-field Type Game for Scheduling in Multi-Hop Wireless Sensor Networks</title>
<link>https://arxiv.org/abs/2506.03059</link>
<guid>https://arxiv.org/abs/2506.03059</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. MFTGWSNs  
2.   
3.   
4.   
5. WSN <div>
arXiv:2506.03059v2 Announce Type: replace 
Abstract: We propose a Mean-Field Type Game (MFTG) framework for effective scheduling in multi-hop wireless sensor networks (WSNs) using backpressure as a performance criterion. Traditional backpressure algorithms leverage queue differentials to regulate data flow and maintain network stability. In this work, we extend the backpressure framework by incorporating a mean-field term into the cost functional, capturing the global behavior of the system alongside local dynamics. The resulting model utilizes the strengths of non-cooperative mean-field type games, enabling nodes to make decentralized decisions based on both individual queue states and system mean-field effects while accounting for stochastic network interactions. By leveraging the interplay between backpressure dynamics and mean-field coupling, the approach balances local optimization with global efficiency. Numerical simulations demonstrate the efficacy of the proposed method in handling congestion and scheduling in large-scale WSNs.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Enhanced Multi-Agent Reinforcement Learning with Expert Workflow for Real-Time P2P Energy Trading</title>
<link>https://arxiv.org/abs/2507.14995</link>
<guid>https://arxiv.org/abs/2507.14995</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. P2PLLMMARL<br />2. LLMMARL<br />3. P2P<br />4. LLM<br />5. P2P <div>
arXiv:2507.14995v3 Announce Type: replace 
Abstract: Real-time peer-to-peer (P2P) electricity markets dynamically adapt to fluctuations in renewable energy and variations in demand, maximizing economic benefits through instantaneous price responses while enhancing grid flexibility. However, scaling expert guidance for massive personalized prosumers poses critical challenges, including diverse decision-making demands and a lack of customized modeling frameworks. This paper proposes an integrated large language model-multi-agent reinforcement learning (LLM-MARL) framework for real-time P2P energy trading to address challenges such as the limited technical capability of prosumers, the lack of expert experience, and security issues of distribution networks. LLMs are introduced as experts to generate personalized strategies, guiding MARL under the centralized training with decentralized execution (CTDE) paradigm through imitation. To handle the scalability issues inherent in large-scale P2P networks, a differential attention-based critic network is introduced to efficiently extract key interaction features and enhance convergence. Experimental results demonstrate that LLM-generated strategies effectively substitute human experts. The proposed imitative expert MARL algorithms achieve significantly lower economic costs and voltage violation rates on test sets compared to baseline algorithms, while maintaining robust stability. This paper provides an effective solution for the real-time decision-making of the P2P electricity market by bridging expert knowledge with agent learning.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computing Pure-Strategy Nash Equilibria in a Two-Party Policy Competition: Existence and Algorithmic Approaches</title>
<link>https://arxiv.org/abs/2512.22552</link>
<guid>https://arxiv.org/abs/2512.22552</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. Lin2021  
2.   
3.   
4. PSNE  
5. PSNE-PSNE <div>
arXiv:2512.22552v2 Announce Type: replace 
Abstract: We formulate two-party policy competition as a two-player non-cooperative game, generalizing Lin et al.'s work (2021). Each party selects a real-valued policy vector as its strategy from a compact subset of Euclidean space, and a voter's utility for a policy is given by the inner product with their preference vector. To capture the uncertainty in the competition, we assume that a policy's winning probability increases monotonically with its total utility across all voters, and we formalize this via an affine isotonic function. A player's payoff is defined as the expected utility received by its supporters. In this work, we first test and validate the isotonicity hypothesis through voting simulations. Next, we prove the existence of a pure-strategy Nash equilibrium (PSNE) in both one- and multi-dimensional settings. Although we construct a counterexample demonstrating the game's non-monotonicity, our experiments show that a decentralized gradient-based algorithm typically converges rapidly to an approximate PSNE. Finally, we present a grid-based search algorithm that finds an $\epsilon$-approximate PSNE of the game in time polynomial in the input size and $1/\epsilon$.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation</title>
<link>https://arxiv.org/abs/2601.01155</link>
<guid>https://arxiv.org/abs/2601.01155</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. ORION<br />3. ORION-option-critic<br />4. makespan<br />5. ORION <div>
arXiv:2601.01155v2 Announce Type: replace 
Abstract: Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Privacy-Preserving Mental Health Support with Large Language Models</title>
<link>https://arxiv.org/abs/2601.01993</link>
<guid>https://arxiv.org/abs/2601.01993</guid>
<content:encoded><![CDATA[
<div> MindChat, MindCorpus, , , <br /><br />:<br />1. MindChatMindCorpus<br />2. -<br />3. LoRA<br />4. MindCorpusMindChat<br />5. MindChat <div>
arXiv:2601.01993v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown promise for mental health support, yet training such models is constrained by the scarcity and sensitivity of real counseling dialogues. In this article, we present MindChat, a privacy-preserving LLM for mental health support, together with MindCorpus, a synthetic multi-turn counseling dataset constructed via a multi-agent role-playing framework. To synthesize high-quality counseling data, the developed dialogue-construction framework employs a dual closed-loop feedback design to integrate psychological expertise and counseling techniques through role-playing: (i) turn-level critique-and-revision to improve coherence and counseling appropriateness within a session, and (ii) session-level strategy refinement to progressively enrich counselor behaviors across sessions. To mitigate privacy risks under decentralized data ownership, we fine-tune the base model using federated learning with parameter-efficient LoRA adapters and incorporate differentially private optimization to reduce membership and memorization risks. Experiments on synthetic-data quality assessment and counseling capability evaluation show that MindCorpus improves training effectiveness and that MindChat is competitive with existing general and counseling-oriented LLM baselines under both automatic LLM-judge and human evaluation protocols, while exhibiting reduced privacy leakage under membership inference attacks.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>BESTOpt: A Modular, Physics-Informed Machine Learning based Building Modeling, Control and Optimization Framework</title>
<link>https://arxiv.org/abs/2601.16283</link>
<guid>https://arxiv.org/abs/2601.16283</guid>
<content:encoded><![CDATA[
<div> BESTOpt<br /><br />:  
1.   
2.   
3. BESTOptPIML  
4. --/  
5. BESTOpt <div>
arXiv:2601.16283v1 Announce Type: new 
Abstract: Modern buildings are increasingly interconnected with occupancy, heating, ventilation, and air-conditioning (HVAC) systems, distributed energy resources (DERs), and power grids. Modeling, control, and optimization of such multi-domain systems play a critical role in achieving building-sector decarbonization. However, most existing tools lack scalability and physical consistency for addressing these complex, multi-scale ecosystem problems. To bridge this gap, this study presents BESTOpt, a modular, physics-informed machine learning (PIML) framework that unifies building applications, including benchmarking, evaluation, diagnostics, control, optimization, and performance simulation. The framework adopts a cluster-domain-system/building-component hierarchy and a standardized state-action-disturbance-observation data typology. By embedding physics priors into data-driven modules, BESTOpt improves model accuracy and physical consistency under unseen conditions. Case studies on single-building and cluster scenarios demonstrate its capability for multi-level centralized and decentralized control. Looking ahead, BESTOpt lays the foundation for an open, extensible platform that accelerates interdisciplinary research toward smart, resilient, and decarbonized building ecosystems.
]]></content:encoded>
<pubDate>Mon, 26 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>FC-GUARD: Enabling Anonymous yet Compliant Fiat-to-Cryptocurrency Exchanges</title>
<link>https://arxiv.org/abs/2601.16298</link>
<guid>https://arxiv.org/abs/2601.16298</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. PII<br />2. FC-GUARD<br />3. PII<br />4. FC-GUARDKYC<br />5.  <div>
arXiv:2601.16298v1 Announce Type: new 
Abstract: With the rise of decentralized finance, fiat-to-cryptocurrency exchange platforms have become popular entry points into the cryptocurrency ecosystem. However, these platforms frequently fail to ensure adequate privacy protection, as evidenced by real-world breaches that exposed personally identifiable information (PII) and crypto addresses. Such leaks enable adversaries to link real-world identities to cryptocurrency transactions, undermining the presumed anonymity of cryptocurrency use.
  We propose FC-GUARD, a privacy-preserving exchange system designed to preserve user anonymity without compromising regulatory compliance in the exchange of fiat currency for cryptocurrencies. Leveraging verifiable credentials and zero-knowledge proof techniques, FC-GUARD enables fiat-to-cryptocurrency exchanges without revealing users' PII or fiat account details. This breaks the linkage between users' real-world identities and their cryptocurrency addresses, thereby upholding anonymity, a fundamental expectation in the cryptocurrency ecosystem. In addition, FC-GUARD complies with key regulations over cryptocurrency usage, such as know-your-customer requirements and auditability for tax reporting obligations by integrating a lawful de-anonymization mechanism that allows the auditing authority to identify misbehaving users. This ensures regulatory compliance while defaulting to privacy protection. We implement our system on both desktop and mobile platforms, and our evaluation shows its feasibility for practical deployment.
]]></content:encoded>
<pubDate>Mon, 26 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>BoostFGL: Boosting Fairness in Federated Graph Learning</title>
<link>https://arxiv.org/abs/2601.16496</link>
<guid>https://arxiv.org/abs/2601.16496</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. FGL<br />2. <br />3. BoostFGL<br />4. 9BoostFGLOverall-F18.43% <div>
arXiv:2601.16496v1 Announce Type: new 
Abstract: Federated graph learning (FGL) enables collaborative training of graph neural networks (GNNs) across decentralized subgraphs without exposing raw data. While existing FGL methods often achieve high overall accuracy, we show that this average performance can conceal severe degradation on disadvantaged node groups. From a fairness perspective, these disparities arise systematically from three coupled sources: label skew toward majority patterns, topology confounding in message propagation, and aggregation dilution of updates from hard clients. To address this, we propose \textbf{BoostFGL}, a boosting-style framework for fairness-aware FGL. BoostFGL introduces three coordinated mechanisms: \ding{182} \emph{Client-side node boosting}, which reshapes local training signals to emphasize systematically under-served nodes; \ding{183} \emph{Client-side topology boosting}, which reallocates propagation emphasis toward reliable yet underused structures and attenuates misleading neighborhoods; and \ding{184} \emph{Server-side model boosting}, which performs difficulty- and reliability-aware aggregation to preserve informative updates from hard clients while stabilizing the global model. Extensive experiments on 9 datasets show that BoostFGL delivers substantial fairness gains, improving Overall-F1 by 8.43\%, while preserving competitive overall performance against strong FGL baselines.
]]></content:encoded>
<pubDate>Mon, 26 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Eclipse Attacks on Ethereum's Peer-to-Peer Network</title>
<link>https://arxiv.org/abs/2601.16560</link>
<guid>https://arxiv.org/abs/2601.16560</guid>
<content:encoded><![CDATA[
<div> Eclipse2.0DNS<br /><br />:  
1. 2.0Eclipse  
2. DNSDNS  
3. DNS28IP10045%95%  
4. Sepolia80%  
5.  <div>
arXiv:2601.16560v1 Announce Type: new 
Abstract: Eclipse attacks isolate blockchain nodes by monopolizing their peer-to-peer connections. The attacks were extensively studied in Bitcoin (SP'15, SP'20, CCS'21, SP'23) and Monero (NDSS'25), but their practicality against Ethereum nodes remains underexplored, particularly in the post-Merge settings.
  We present the first end-to-end implementation of an eclipse attack targeting Ethereum (2.0 version) execution-layer nodes. Our attack exploits the bootstrapping and peer management logic of Ethereum to fully isolate a node upon restart. We introduce a multi-stage strategy that majorly includes (i) poisoning the node's discovery table via unsolicited messages, (ii) infiltrating Ethereum's DNS-based peerlist by identifying and manipulating the official DNS crawler, and (iii) hijacking idle incoming connection slots across the network to block benign connections. Our DNS list poisoning is the first in the cryptocurrency context and requires only 28 IP addresses over 100 days. Slots hijacking raises outgoing redirection success from 45\% to 95\%. We validate our approach through controlled experiments on Ethereum's Sepolia testnet and broad measurements on the mainnet. Our findings demonstrate that over 80\% of public nodes do not leave sufficient idle capacity for effective slots occupation, highlighting the feasibility and severity of the threat. We further propose concrete countermeasures and responsibly disclosed all findings to Ethereum's security team.
]]></content:encoded>
<pubDate>Mon, 26 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>"What I Sign Is Not What I See": Towards Explainable and Trustworthy Cryptocurrency Wallet Signatures</title>
<link>https://arxiv.org/abs/2601.16751</link>
<guid>https://arxiv.org/abs/2601.16751</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1.   
2.   
3.   
4.   
5. 128 <div>
arXiv:2601.16751v1 Announce Type: new 
Abstract: Cryptocurrency wallets have become the primary gateway to decentralized applications, yet users often face significant difficulty in discerning what a wallet signature actually does or entails. Prior work has mainly focused on mitigating protocol vulnerabilities, with limited attention to how users perceive and interpret what they are authorizing. To examine this usability-security gap, we conducted two formative studies investigating how users interpret authentic signing requests and what cues they rely on to assess risk. Findings reveal that users often misread critical parameters, underestimate high-risk signatures, and rely on superficial familiarity rather than understanding transaction intent. Building on these insights, we designed the Signature Semantic Decoder -- a prototype framework that reconstructs and visualizes the intent behind wallet signatures prior to confirmation. Through structured parsing and semantic labeling, it demonstrates how signing data can be transformed into plain-language explanations with contextual risk cues. In a between-subjects user study (N = 128), participants using the prototype achieved higher accuracy in identifying risky signatures, improved clarity and decision confidence, and lower cognitive workload compared with the baseline wallet interface. Our study reframes wallet signing as a problem of interpretability within secure interaction design and offers design implications for more transparent and trustworthy cryptocurrency wallet interfaces.
]]></content:encoded>
<pubDate>Mon, 26 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing Vulnerability in Smart Contracts: The Role of Code Complexity Metrics in Security Analysis</title>
<link>https://arxiv.org/abs/2411.17343</link>
<guid>https://arxiv.org/abs/2411.17343</guid>
<content:encoded><![CDATA[
<div> Solidity<br /><br />:<br />1. <br />2. Solidity<br />3. 21<br />4. <br />5.  <div>
arXiv:2411.17343v4 Announce Type: replace 
Abstract: Software built on poor structural patterns often shows higher exposure to security defects. When code differs from established best practices, verification and maintenance become increasingly difficult, thereby raising the risk of unintentional vulnerabilities. In the context of blockchain technology, where immutable smart contracts handle high-value transactions, the need for strict security assurance is important. This research analyzes the utility of software complexity metrics as diagnostic tools for identifying vulnerable Solidity smart contracts. We evaluate the hypothesis that complexity measures serve as vital, complementary signals for security assessment. Through an empirical examination of 21 distinct metrics, we analyzed their inter-dependencies, statistical association with vulnerabilities, and discriminative capabilities. Our findings indicate a significant degree of redundancy among certain metrics and a relatively low correlation between any single metric and the presence of vulnerabilities. However, the data demonstrates that these metrics possess strong power to distinguish between secure and vulnerable code when analyzed collectively. Notably, with only three exceptions, vulnerable contracts consistently exhibited higher mean complexity scores than their neutral counterparts. While our results show a statistical association, we emphasize that complexity is an indicator rather than a direct cause of vulnerability.
]]></content:encoded>
<pubDate>Mon, 26 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blockchain-Based Spectrum Resource Securitization via Semi-Fungible Token-Lock</title>
<link>https://arxiv.org/abs/2601.15594</link>
<guid>https://arxiv.org/abs/2601.15594</guid>
<content:encoded><![CDATA[
<div> 6GSFT<br /><br />:<br />1. 6G<br />2. ERC404/SFT Lock<br />3. SFT Lock//NFT<br />4. <br />5. Gas <div>
arXiv:2601.15594v1 Announce Type: new 
Abstract: As 6G networks evolve, spectrum assets require flexible, dynamic, and efficient utilization, motivating blockchain based spectrum securitization. Existing approaches based on ERC404 style hybrid token models rely on frequent minting and burning during asset transfers, which disrupt token identity continuity and increase on chain overhead. This paper proposes the Semi Fungible Token Lock (SFT Lock) method, a lock/unlock based mechanism that preserves NFT identity and historical traceability while enabling fractional ownership and transferability. By replacing mint/burn operations with deterministic state transitions, SFT Lock ensures consistent lifecycle representation of spectrum assets and significantly reduces on chain operations. Based on this mechanism, a modular smart contract architecture is designed to support spectrum authorization, securitization, and sharing, and a staking mechanism is introduced to enhance asset liquidity. Experimental results on a private Ethereum network demonstrate that, compared with ERC404 style hybrid token models, the proposed method achieves substantial gas savings while maintaining functional correctness and traceability.
]]></content:encoded>
<pubDate>Sat, 24 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Communication-efficient Federated Graph Classification via Generative Diffusion Modeling</title>
<link>https://arxiv.org/abs/2601.15722</link>
<guid>https://arxiv.org/abs/2601.15722</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. FGNNnon-IIDCeFGC  
2. CeFGC  
3.   
4.   
5. GNN <div>
arXiv:2601.15722v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) unlock new ways of learning from graph-structured data, proving highly effective in capturing complex relationships and patterns. Federated GNNs (FGNNs) have emerged as a prominent distributed learning paradigm for training GNNs over decentralized data. However, FGNNs face two significant challenges: high communication overhead from multiple rounds of parameter exchanges and non-IID data characteristics across clients. To address these issues, we introduce CeFGC, a novel FGNN paradigm that facilitates efficient GNN training over non-IID data by limiting communication between the server and clients to three rounds only. The core idea of CeFGC is to leverage generative diffusion models to minimize direct client-server communication. Each client trains a generative diffusion model that captures its local graph distribution and shares this model with the server, which then redistributes it back to all clients. Using these generative models, clients generate synthetic graphs combined with their local graphs to train local GNN models. Finally, clients upload their model weights to the server for aggregation into a global GNN model. We theoretically analyze the I/O complexity of communication volume to show that CeFGC reduces to a constant of three communication rounds only. Extensive experiments on several real graph datasets demonstrate the effectiveness and efficiency of CeFGC against state-of-the-art competitors, reflecting our superior performance on non-IID graphs by aligning local and global model objectives and enriching the training set with diverse graphs.
]]></content:encoded>
<pubDate>Sat, 24 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>ECGomics: An Open Platform for AI-ECG Digital Biomarker Discovery</title>
<link>https://arxiv.org/abs/2601.15326</link>
<guid>https://arxiv.org/abs/2601.15326</guid>
<content:encoded><![CDATA[
<div> ECGomics<br /><br />:<br />1. ECGECGomics<br />2. ECGomics<br />3. 12<br />4. <br />5.  <div>
arXiv:2601.15326v1 Announce Type: cross 
Abstract: Background: Conventional electrocardiogram (ECG) analysis faces a persistent dichotomy: expert-driven features ensure interpretability but lack sensitivity to latent patterns, while deep learning offers high accuracy but functions as a black box with high data dependency. We introduce ECGomics, a systematic paradigm and open-source platform for the multidimensional deconstruction of cardiac signals into digital biomarker. Methods: Inspired by the taxonomic rigor of genomics, ECGomics deconstructs cardiac activity across four dimensions: Structural, Intensity, Functional, and Comparative. This taxonomy synergizes expert-defined morphological rules with data-driven latent representations, effectively bridging the gap between handcrafted features and deep learning embeddings. Results: We operationalized this framework into a scalable ecosystem consisting of a web-based research platform and a mobile-integrated solution (https://github.com/PKUDigitalHealth/ECGomics). The web platform facilitates high-throughput analysis via precision parameter configuration, high-fidelity data ingestion, and 12-lead visualization, allowing for the systematic extraction of biomarkers across the four ECGomics dimensions. Complementarily, the mobile interface, integrated with portable sensors and a cloud-based engine, enables real-time signal acquisition and near-instantaneous delivery of structured diagnostic reports. This dual-interface architecture successfully transitions ECGomics from theoretical discovery to decentralized, real-world health management, ensuring professional-grade monitoring in diverse clinical and home-based settings. Conclusion: ECGomics harmonizes diagnostic precision, interpretability, and data efficiency. By providing a deployable software ecosystem, this paradigm establishes a robust foundation for digital biomarker discovery and personalized cardiovascular medicine.
]]></content:encoded>
<pubDate>Sat, 24 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>European digital identity: A missed opportunity?</title>
<link>https://arxiv.org/abs/2601.14503</link>
<guid>https://arxiv.org/abs/2601.14503</guid>
<content:encoded><![CDATA[
<div> OpenID4VCI<br /><br />:<br />1. EUDIOpenID<br />2. OpenID4VCIOpenID4VP<br />3. <br />4. <br />5. OAuthUMAA4DSGNAP <div>
arXiv:2601.14503v1 Announce Type: new 
Abstract: Recent European efforts around digital identity -- the EUDI regulation and its OpenID architecture -- aim high, but start from a narrow and ill-defined conceptualization of authentication. Based on a broader, more grounded understanding of the term, in we identify several issues in the design of OpenID4VCI and OpenID4VP: insecure practices, static, and subject-bound credential types, and a limited query language restrict their application to classic scenarios of credential exchange -- already supported by existing solutions like OpenID Connect, SIOPv2, OIDC4IDA, and OIDC Claims Aggregation -- barring dynamic, asynchronous, or automated use cases. We also debunk OpenID's 'paradigm-shifting' trust-model, which -- when compared to existing decentralized alternatives -- does not deliver any significant increase in control, privacy, and portability of personal information. Not only the technical choices limit the capabilities of the EUDI framework; also the legislation itself cannot accommodate the promise of self-sovereign identity. In particular, we criticize the introduction of institutionalized trusted lists, and discuss their economical and political risks. Their potential to decline into an exclusory, re-centralized ecosystem endangers the vision of a user-oriented identity management in which individuals are in charge. Instead, the consequences might severely restrict people in what they can do with their personal information, and risk increased linkability and monitoring. In anticipation of revisions to the EUDI regulations, we suggest several technical alternatives that overcome some of the issues with the architecture of OpenID. In particular, OAuth's UMA extension and its A4DS profile, as well as their integration in GNAP, are worth looking into. Future research into uniform query (meta-)languages is needed to address the heterogeneity of attestations and providers.
]]></content:encoded>
<pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agent Identity URI Scheme: Topology-Independent Naming and Capability-Based Discovery for Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2601.14567</link>
<guid>https://arxiv.org/abs/2601.14567</guid>
<content:encoded><![CDATA[
<div> agent:// URI<br /><br />:<br />1. <br />2.  agent:// URI <br />3.  DHT <br />4. PASETO <br />5. 5 <div>
arXiv:2601.14567v1 Announce Type: new 
Abstract: Multi-agent systems face a fundamental architectural flaw: agent identity is bound to network location. When agents migrate between providers, scale across instances, or federate across organizations, URI-based identity schemes break references, fragment audit trails, and require centralized coordination. We propose the agent:// URI scheme, which decouples identity from topology through three orthogonal components: a trust root establishing organizational authority, a hierarchical capability path enabling semantic discovery, and a sortable unique identifier providing stable reference. The scheme enables capability-based discovery through DHT key derivation, where queries return agents by what they do rather than where they are. Trust-root scoping prevents cross-organization pollution while permitting federation when desired. Cryptographic attestation via PASETO tokens binds capability claims to agent identity, enabling verification without real-time contact with the issuing authority. We evaluate the scheme across four dimensions: capability expressiveness (100% coverage on 369 production tools with zero collision), discovery precision (F1=1.0 across 10,000 agents), identity stability (formal proofs of migration invariance), and performance (all operations under 5 microseconds). The agent:// URI scheme provides a formally-specified, practically-evaluated foundation for decentralized agent identity and capability-based discovery.
]]></content:encoded>
<pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interoperable Architecture for Digital Identity Delegation for AI Agents with Blockchain Integration</title>
<link>https://arxiv.org/abs/2601.14982</link>
<guid>https://arxiv.org/abs/2601.14982</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. SSIAI<br />2. <br />3. DGsCVC<br />4.  <div>
arXiv:2601.14982v1 Announce Type: new 
Abstract: Verifiable delegation in digital identity systems remains unresolved across centralized, federated, and self-sovereign identity (SSI) environments, particularly where both human users and autonomous AI agents must exercise and transfer authority without exposing primary credentials or private keys. We introduce a unified framework that enables bounded, auditable, and least-privilege delegation across heterogeneous identity ecosystems. The framework includes four key elements: Delegation Grants (DGs), first-class authorization artefacts that encode revocable transfers of authority with enforced scope reduction; a Canonical Verification Context (CVC) that normalizes verification requests into a single structured representation independent of protocols or credential formats; a layered reference architecture that separates trust anchoring, credential and proof validation, policy evaluation, and protocol mediation via a Trust Gateway; and an explicit treatment of blockchain anchoring as an optional integrity layer rather than a structural dependency. Together, these elements advance interoperable delegation and auditability and provide a foundation for future standardization, implementation, and integration of autonomous agents into trusted digital identity infrastructures.
]]></content:encoded>
<pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Effectiveness of Mempool-based Transaction Auditing</title>
<link>https://arxiv.org/abs/2601.14996</link>
<guid>https://arxiv.org/abs/2601.14996</guid>
<content:encoded><![CDATA[
<div> mempool<br /><br />:  
1. Mempool.spacemempool  
2. mempool  
3. mempool25%  
4. 3099.9%  
5.  <div>
arXiv:2601.14996v1 Announce Type: new 
Abstract: While the literature features a number of proposals to defend against transaction manipulation attacks, existing proposals are still not integrated within large blockchains, such as Bitcoin, Ethereum, and Cardano. Instead, the user community opted to rely on more practical but ad-hoc solutions (such as Mempool.space) that aim at detecting censorship and transaction displacement attacks by auditing discrepancies in the mempools of so-called observers.
  In this paper, we precisely analyze, for the first time, the interplay between mempool auditing and the ability to detect censorship and transaction displacement attacks by malicious miners in Bitcoin and Ethereum. Our analysis shows that mempool auditing can result in mis-accusations against miners with a probability larger than 25% in some settings. On a positive note, however, we show that mempool auditing schemes can successfully audit the execution of any two transactions (with an overwhelming probability of 99.9%) if they are consistently received by all observers and sent at least 30 seconds apart from each other. As a direct consequence, our findings show, for the first time, that batch-order fair-ordering schemes can offer only strong fairness guarantees for a limited subset of transactions in real-world deployments.
]]></content:encoded>
<pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Leakage with Generative Flow Matching Denoiser</title>
<link>https://arxiv.org/abs/2601.15049</link>
<guid>https://arxiv.org/abs/2601.15049</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. DL  
2. DL  
3. DLFlow Matching, FM  
4.   
5.  <div>
arXiv:2601.15049v1 Announce Type: new 
Abstract: Federated Learning (FL) has emerged as a powerful paradigm for decentralized model training, yet it remains vulnerable to deep leakage (DL) attacks that reconstruct private client data from shared model updates. While prior DL methods have demonstrated varying levels of success, they often suffer from instability, limited fidelity, or poor robustness under realistic FL settings. We introduce a new DL attack that integrates a generative Flow Matching (FM) prior into the reconstruction process. By guiding optimization toward the distribution of realistic images (represented by a flow matching foundation model), our method enhances reconstruction fidelity without requiring knowledge of the private data. Extensive experiments on multiple datasets and target models demonstrate that our approach consistently outperforms state-of-the-art attacks across pixel-level, perceptual, and feature-based similarity metrics. Crucially, the method remains effective across different training epochs, larger client batch sizes, and under common defenses such as noise injection, clipping, and sparsification. Our findings call for the development of new defense strategies that explicitly account for adversaries equipped with powerful generative priors.
]]></content:encoded>
<pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Management of a Deep Learning-Based Anomaly Detection System for 5G Networks</title>
<link>https://arxiv.org/abs/2601.15177</link>
<guid>https://arxiv.org/abs/2601.15177</guid>
<content:encoded><![CDATA[
<div> MEC5G<br /><br />:<br />1. MEC5G<br />2. 5G<br />3. MEC<br />4. <br />5.  <div>
arXiv:2601.15177v1 Announce Type: new 
Abstract: Fog and mobile edge computing (MEC) will play a key role in the upcoming fifth generation (5G) mobile networks to support decentralized applications, data analytics and management into the network itself by using a highly distributed compute model. Furthermore, increasing attention is paid to providing user-centric cybersecurity solutions, which particularly require collecting, processing and analyzing significantly large amount of data traffic and huge number of network connections in 5G networks. In this regard, this paper proposes a MEC-oriented solution in 5G mobile networks to detect network anomalies in real-time and in autonomic way. Our proposal uses deep learning techniques to analyze network flows and to detect network anomalies. Moreover, it uses policies in order to provide an efficient and dynamic management system of the computing resources used in the anomaly detection process. The paper presents relevant aspects of the deployment of the proposal and experimental results to show its performance.
]]></content:encoded>
<pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Onboard Optimization and Learning: A Survey</title>
<link>https://arxiv.org/abs/2505.08793</link>
<guid>https://arxiv.org/abs/2505.08793</guid>
<content:encoded><![CDATA[
<div> AI<br /><br />:<br />1. AI<br />2. <br />3. <br />4. <br />5. - <div>
arXiv:2505.08793v2 Announce Type: replace 
Abstract: Onboard learning is a transformative approach in edge AI, enabling real-time data processing, decision-making, and adaptive model training directly on resource-constrained devices without relying on centralized servers. This paradigm is crucial for applications demanding low latency, enhanced privacy, and energy efficiency. However, onboard learning faces challenges such as limited computational resources, high inference costs, and security vulnerabilities. This survey explores a comprehensive range of methodologies that address these challenges, focusing on techniques that optimize model efficiency, accelerate inference, and support collaborative learning across distributed devices. Approaches for reducing model complexity, improving inference speed, and ensuring privacy-preserving computation are examined alongside emerging strategies that enhance scalability and adaptability in dynamic environments. By bridging advancements in hardware-software co-design, model compression, and decentralized learning, this survey provides insights into the current state of onboard learning to enable robust, efficient, and secure AI deployment at the edge.
]]></content:encoded>
<pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized COVID-19 Health System Leveraging Blockchain</title>
<link>https://arxiv.org/abs/2506.02674</link>
<guid>https://arxiv.org/abs/2506.02674</guid>
<content:encoded><![CDATA[
<div>  EHR   <br /><br />:<br />1. EHR<br />2. EVRCOVID-19<br />3. <br />4. <br />5. Hyperledger FabricGo <div>
arXiv:2506.02674v2 Announce Type: replace 
Abstract: With the development of the Internet, the amount of data generated by the medical industry each year has grown exponentially. The Electronic Health Record (EHR) manages the electronic data generated during the user's treatment process. Typically, an EHR data manager belongs to a medical institution. This traditional centralized data management model has many unreasonable or inconvenient aspects, such as difficulties in data sharing, and it is hard to verify the authenticity and integrity of the data. The decentralized, non-forgeable, data unalterable and traceable features of blockchain are in line with the application requirements of EHR. This paper takes the most common COVID-19 as the application scenario and designs a COVID-19 health system based on blockchain, which has extensive research and application value. Considering that the public and transparent nature of blockchain violates the privacy requirements of some health data, in the system design stage, from the perspective of practical application, the data is divided into public data and private data according to its characteristics. For private data, data encryption methods are adopted to ensure data privacy. The searchable encryption technology is combined with blockchain technology to achieve the retrieval function of encrypted data. Then, the proxy re-encryption technology is used to realize authorized access to data. In the system implementation part, based on the Hyperledger Fabric architecture, some functions of the system design are realized, including data upload, retrieval of the latest data and historical data. According to the environment provided by the development architecture, Go language chaincode (smart contract) is written to implement the relevant system functions.
]]></content:encoded>
<pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASAS-BridgeAMM: Trust-Minimized Cross-Chain Bridge AMM with Failure Containment</title>
<link>https://arxiv.org/abs/2601.12434</link>
<guid>https://arxiv.org/abs/2601.12434</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. DeFi202128<br />2. <br />3. ASAS-BridgeAMM<br />4. -73%104.5%<br />5. >99.99%0.2%Solidity <div>
arXiv:2601.12434v1 Announce Type: new 
Abstract: Cross-chain bridges constitute the single largest vector of systemic risk in Decentralized Finance (DeFi), accounting for over \$2.8 billion in losses since 2021. The fundamental vulnerability lies in the binary nature of existing bridge security models: a bridge is either fully operational or catastrophically compromised, with no intermediate state to contain partial failures. We present ASAS-BridgeAMM, a bridge-coupled automated market maker that introduces Contained Degradation: a formally specified operational state where the system gracefully degrades functionality in response to adversarial signals. By treating cross-chain message latency as a quantifiable execution risk, the protocol dynamically adjusts collateral haircuts, slippage bounds, and withdrawal limits. Across 18 months of historical replay on Ethereum and two auxiliary chains, ASAS-BridgeAMM reduces worst-case bridge-induced insolvency by 73% relative to baseline mint-and-burn architectures, while preserving 104.5% of transaction volume during stress periods. In rigorous adversarial simulations involving delayed finality, oracle manipulation, and liquidity griefing, the protocol maintains solvency with probability $>0.9999$ and bounds per-epoch bad debt to $<0.2%$ of total collateral. We provide a reference implementation in Solidity and formally prove safety (bounded debt), liveness (settlement completion), and manipulation resistance under a Byzantine relayer model.
]]></content:encoded>
<pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language-Based Swarm Perception: Decentralized Person Re-Identification via Natural Language Descriptions</title>
<link>https://arxiv.org/abs/2601.12479</link>
<guid>https://arxiv.org/abs/2601.12479</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. VLM<br />3. <br />4. <br />5.  <div>
arXiv:2601.12479v1 Announce Type: new 
Abstract: We introduce a method for decentralized person re-identification in robot swarms that leverages natural language as the primary representational modality. Unlike traditional approaches that rely on opaque visual embeddings -- high-dimensional feature vectors extracted from images -- the proposed method uses human-readable language to represent observations. Each robot locally detects and describes individuals using a vision-language model (VLM), producing textual descriptions of appearance instead of feature vectors. These descriptions are compared and clustered across the swarm without centralized coordination, allowing robots to collaboratively group observations of the same individual. Each cluster is distilled into a representative description by a language model, providing an interpretable, concise summary of the swarm's collective perception. This approach enables natural-language querying, enhances transparency, and supports explainable swarm behavior. Preliminary experiments demonstrate competitive performance in identity consistency and interpretability compared to embedding-based methods, despite current limitations in text similarity and computational load. Ongoing work explores refined similarity metrics, semantic navigation, and the extension of language-based perception to environmental elements. This work prioritizes decentralized perception and communication, while active navigation remains an open direction for future study.
]]></content:encoded>
<pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cooperative Multi-agent RL with Communication Constraints</title>
<link>https://arxiv.org/abs/2601.12518</link>
<guid>https://arxiv.org/abs/2601.12518</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. MARL<br />2. <br />3. <br />4. <br />5. $O(\varepsilon^{-3/4})$$\varepsilon$-MAPPO <div>
arXiv:2601.12518v1 Announce Type: new 
Abstract: Cooperative MARL often assumes frequent access to global information in a data buffer, such as team rewards or other agents' actions, which is typically unrealistic in decentralized MARL systems due to high communication costs. When communication is limited, agents must rely on outdated information to estimate gradients and update their policies. A common approach to handle missing data is called importance sampling, in which we reweigh old data from a base policy to estimate gradients for the current policy. However, it quickly becomes unstable when the communication is limited (i.e. missing data probability is high), so that the base policy in importance sampling is outdated. To address this issue, we propose a technique called base policy prediction, which utilizes old gradients to predict the policy update and collect samples for a sequence of base policies, which reduces the gap between the base policy and the current policy. This approach enables effective learning with significantly fewer communication rounds, since the samples of predicted base policies could be collected within one communication round. Theoretically, we show that our algorithm converges to an $\varepsilon$-Nash equilibrium in potential games with only $O(\varepsilon^{-3/4})$ communication rounds and $O(poly(\max_i |A_i|)\varepsilon^{-11/4})$ samples, improving existing state-of-the-art results in communication cost, as well as sample complexity without the exponential dependence on the joint action space size. We also extend these results to general Markov Cooperative Games to find an agent-wise local maximum. Empirically, we test the base policy prediction algorithm in both simulated games and MAPPO for complex environments.
]]></content:encoded>
<pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SGCP: A Self-Organized Game-Theoretic Framework For Collaborative Perception</title>
<link>https://arxiv.org/abs/2601.12524</link>
<guid>https://arxiv.org/abs/2601.12524</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. <br />3. <br />4. <br />5. CARLA-OpenCDA-NS3 <div>
arXiv:2601.12524v1 Announce Type: new 
Abstract: Collaborative perception holds great promise for improving safety in autonomous driving, particularly in dense traffic where vehicles can share sensory information to overcome individual blind spots and extend awareness. However, deploying such collaboration at scale remains difficult when communication bandwidth is limited and no roadside infrastructure is available. To overcome these limitations, we introduce a fully decentralized framework that enables vehicles to self organize into cooperative groups using only vehicle to vehicle communication. The approach decomposes the problem into two sequential game theoretic stages. In the first stage, vehicles form stable clusters by evaluating mutual sensing complementarity and motion coherence, and each cluster elects a coordinator. In the second stage, the coordinator guides its members to selectively transmit point cloud segments from perceptually salient regions through a non cooperative potential game, enabling efficient local fusion. Global scene understanding is then achieved by exchanging compact detection messages across clusters rather than raw sensor data. We design distributed algorithms for both stages that guarantee monotonic improvement of the system wide potential function. Comprehensive experiments on the CARLA-OpenCDA-NS3 co-simulation platform show that our method reduces communication overhead while delivering higher perception accuracy and wider effective coverage compared to existing baselines.
]]></content:encoded>
<pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Fusion: Verifiable Alignment in Decentralized Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2601.12580</link>
<guid>https://arxiv.org/abs/2601.12580</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. SF<br />2. <br />3. <br />4. <br />5. 250SF <div>
arXiv:2601.12580v1 Announce Type: new 
Abstract: We present Semantic Fusion (SF), a formal framework for decentralized semantic coordination in multi-agent systems. SF allows agents to operate over scoped views of shared memory, propose structured updates, and maintain global coherence through local ontology-based validation and refresh without centralized control or explicit message passing. The central theoretical result is a bisimulation theorem showing that each agent's local execution is behaviorally equivalent to its projection of the global semantics, in both deterministic and probabilistic settings. This enables safety, liveness, and temporal properties to be verified locally and soundly lifted to the full system. SF supports agents whose update proposals vary across invocations, including those generated by learned or heuristic components, provided updates pass semantic validation before integration. We establish deterministic and probabilistic guarantees ensuring semantic alignment under asynchronous or degraded communication. To validate the model operationally, we implement a lightweight reference architecture that instantiates its core mechanisms. A 250-agent simulation evaluates these properties across over 11,000 validated updates, demonstrating convergence under probabilistic refresh, bounded communication, and resilience to agent failure. Together, these results show that Semantic Fusion can provide a formal and scalable basis for verifiable autonomy in decentralized systems.
]]></content:encoded>
<pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Learning Strategies for Estimation Error Minimization with Graph Neural Networks</title>
<link>https://arxiv.org/abs/2601.12662</link>
<guid>https://arxiv.org/abs/2601.12662</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1.   
2.   
3.   
4.   
5.  <div>
arXiv:2601.12662v1 Announce Type: new 
Abstract: We address real-time sampling and estimation of autoregressive Markovian sources in dynamic yet structurally similar multi-hop wireless networks. Each node caches samples from others and communicates over wireless collision channels, aiming to minimize time-average estimation error via decentralized policies. Due to the high dimensionality of action spaces and complexity of network topologies, deriving optimal policies analytically is intractable. To address this, we propose a graphical multi-agent reinforcement learning framework for policy optimization. Theoretically, we demonstrate that our proposed policies are transferable, allowing a policy trained on one graph to be effectively applied to structurally similar graphs. Numerical experiments demonstrate that (i) our proposed policy outperforms state-of-the-art baselines; (ii) the trained policies are transferable to larger networks, with performance gains increasing with the number of agents; (iii) the graphical training procedure withstands non-stationarity, even when using independent learning techniques; and (iv) recurrence is pivotal in both independent learning and centralized training and decentralized execution, and improves the resilience to non-stationarity.
]]></content:encoded>
<pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>BlocksecRT-DETR: Decentralized Privacy-Preserving and Token-Efficient Federated Transformer Learning for Secure Real-Time Object Detection in ITS</title>
<link>https://arxiv.org/abs/2601.12693</link>
<guid>https://arxiv.org/abs/2601.12693</guid>
<content:encoded><![CDATA[
<div> Transformer<br /><br />:<br />1. IIDTransformerBlockSecRT-DETR<br />2. RT-DETRTokenTEMToken17.2%IID89.20% mAP@0.5<br />3. 400ms12KB <div>
arXiv:2601.12693v1 Announce Type: new 
Abstract: Federated real-time object detection using transformers in Intelligent Transportation Systems (ITS) faces three major challenges: (1) missing-class non-IID data heterogeneity from geographically diverse traffic environments, (2) latency constraints on edge hardware for high-capacity transformer models, and (3) privacy and security risks from untrusted client updates and centralized aggregation. We propose BlockSecRT-DETR, a BLOCKchain-SECured Real-Time Object DEtection TRansformer framework for ITS that provides a decentralized, token-efficient, and privacy-preserving federated training solution using RT-DETR transformer, incorporating a blockchain-secured update validation mechanism for trustworthy aggregation. In this framework, challenges (1) and (2) are jointly addressed through a unified client-side design that integrates RT-DETR training with a Token Engineering Module (TEM). TEM prunes low-utility tokens, reducing encoder complexity and latency on edge hardware, while aggregated updates mitigate non-IID data heterogeneity across clients. To address challenge (3), BlockSecRT-DETR incorporates a decentralized blockchain-secured update validation mechanism that enables tamper-proof, privacy-preserving, and trust-free authenticated model aggregation without relying on a central server. We evaluated the proposed framework under a missing-class Non-IID partition of the KITTI dataset and conducted a blockchain case study to quantify security overhead. TEM improves inference latency by 17.2% and reduces encoder FLOPs by 47.8%, while maintaining global detection accuracy (89.20% mAP@0.5). The blockchain integration adds 400 ms per round, and the ledger size remains under 12 KB due to metadata-only on-chain storage.
]]></content:encoded>
<pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDN-Blockchain Based Security Routing for UAV Communication via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2601.12774</link>
<guid>https://arxiv.org/abs/2601.12774</guid>
<content:encoded><![CDATA[
<div> -<br /><br />:  
1. SDN  
2.   
3. -BSPPO  
4.   
5. BSPPO <div>
arXiv:2601.12774v1 Announce Type: new 
Abstract: The unmanned aerial vehicle (UAV) network plays important roles in emergency communications. However, it is challenging to design reliable routing strategies that ensure low latency, energy efficiency, and security in the dynamic and attack-prone environments. To this end, we design a secure routing architecture integrating software-defined networking (SDN) for centralized control and blockchain for tamper-proof trust management. In particular, a novel security degree metric is introduced to quantify the UAV trustworthiness. Based on this architecture, we propose a beam search-proximal policy optimization (BSPPO) algorithm, where beam search (BS) pre-screens the high-security candidate paths, and proximal policy optimization (PPO) performs hop-by-hop routing decisions to support dynamic rerouting upon attack detections. Finally, extensive simulations under varying attack densities, packet sizes, and rerouting events demonstrate that BSPPO outperforms PPO, BS-Q learning, and BS-actor critic in terms of delay, energy consumption, and transmission success rate, showing the outstanding robustness and adaptability.
]]></content:encoded>
<pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge-Integrated Representation Learning for Crypto Anomaly Detection under Extreme Label Scarcity; Relational Domain-Logic Integration with Retrieval-Grounded Context and Path-Level Explanations</title>
<link>https://arxiv.org/abs/2601.12839</link>
<guid>https://arxiv.org/abs/2601.12839</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. GNN<br />2. RDLI<br />3. RGC<br />4. 0.01%RDLIF1GNN28.9%<br />5. RDLI <div>
arXiv:2601.12839v1 Announce Type: new 
Abstract: Detecting anomalous trajectories in decentralized crypto networks is fundamentally challenged by extreme label scarcity and the adaptive evasion strategies of illicit actors. While Graph Neural Networks (GNNs) effectively capture local structural patterns, they struggle to internalize multi hop, logic driven motifs such as fund dispersal and layering that characterize sophisticated money laundering, limiting their forensic accountability under regulations like the FATF Travel Rule. To address this limitation, we propose Relational Domain Logic Integration (RDLI), a framework that embeds expert derived heuristics as differentiable, logic aware latent signals within representation learning. Unlike static rule based approaches, RDLI enables the detection of complex transactional flows that evade standard message passing. To further account for market volatility, we incorporate a Retrieval Grounded Context (RGC) module that conditions anomaly scoring on regulatory and macroeconomic context, mitigating false positives caused by benign regime shifts. Under extreme label scarcity (0.01%), RDLI outperforms state of the art GNN baselines by 28.9% in F1 score. A micro expert user study further confirms that RDLI path level explanations significantly improve trustworthiness, perceived usefulness, and clarity compared to existing methods, highlighting the importance of integrating domain logic with contextual grounding for both accuracy and explainability.
]]></content:encoded>
<pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SWORD: A Secure LoW-Latency Offline-First Authentication and Data Sharing Scheme for Resource Constrained Distributed Networks</title>
<link>https://arxiv.org/abs/2601.12875</link>
<guid>https://arxiv.org/abs/2601.12875</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. SWORD<br />2. SWORD<br />3. <br />4. SWORD <div>
arXiv:2601.12875v1 Announce Type: new 
Abstract: While many resource-constrained networks, such as Internet of Things (IoT) and Internet of Vehicles (IoV), are inherently distributed, the majority still rely on central servers for fast authentication and data sharing. Blockchain-based solutions offer decentralized alternatives but often struggle to meet the stringent latency requirements of real-time applications. Even with the rollout of 5G, network latency between servers and peers remains a significant challenge. To address this, we introduce SWORD, a novel offline-first authentication and data-sharing scheme designed specifically for resource-constrained networks. SWORD utilizes a proximity-based clustering approach to enable offline authentication and data sharing, ensuring low-latency, secure operations even in intermittently connected scenarios. Our experimental results show that SWORD outperforms traditional blockchain-based solutions while offering similar resource efficiency and authentication latency to central-server-based solutions. Additionally, we provide a comprehensive security analysis, demonstrating that SWORD is resilient against spoofing, impersonation, replay, and man-in-the-middle attacks.
]]></content:encoded>
<pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enshrined Proposer Builder Separation in the presence of Maximal Extractable Value</title>
<link>https://arxiv.org/abs/2601.12989</link>
<guid>https://arxiv.org/abs/2601.12989</guid>
<content:encoded><![CDATA[
<div> PoSMEV-PBSPBSePBS

<br /><br />:
1. PoS  
2. MEV-PBSPBSePBSEIP-7732  
3. ePBS  
4. ePBS0.1749ePBS0.8358MEV  
5. 95.4%MEV <div>
arXiv:2601.12989v1 Announce Type: new 
Abstract: In blockchain systems operating under the Proof-of-Stake (PoS) consensus mechanism, fairness in transaction processing is essential to preserving decentralization and maintaining user trust. However, with the emergence of Maximal Extractable Value (MEV), concerns about economic centralization and content manipulation have intensified. To address these vulnerabilities, the Ethereum community has introduced Proposer Builder Separation (PBS), which separates block construction from block proposal. Later, enshrined Proposer Builder Separation (ePBS) was also proposed in EIP-7732, which embeds PBS directly into the Ethereum consensus layer.
  Our work identifies key limitations of ePBS by developing a formal framework that combines mathematical analysis and agent-based simulations to evaluate its auction-based block-building mechanism, with particular emphasis on MEV dynamics. Our results reveal that, although ePBS redistributes responsibilities between builders and proposers, it significantly amplifies profit and content centralization: the Gini coefficient for profits rises from 0.1749 under standard PoS without ePBS to 0.8358 under ePBS. This sharp increase indicates that a small number of efficient builders capture most value via MEV-driven auctions. Moreover, 95.4% of the block value is rewarded to proposers in ePBS, revealing a strong economic bias despite their limited role in block assembly. These findings highlight that ePBS exacerbates incentives for builders to adopt aggressive MEV strategies, suggesting the need for future research into mechanism designs that better balance decentralization, fairness, and MEV mitigation.
]]></content:encoded>
<pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>OPTIMUM-DERAM: Highly Consistent, Scalable, and Secure Multi-Object Memory using RLNC</title>
<link>https://arxiv.org/abs/2601.13146</link>
<guid>https://arxiv.org/abs/2601.13146</guid>
<content:encoded><![CDATA[
<div> OPTIMUM-DERAM<br /><br />:  
1. OPTIMUM-DERAM  
2. RLNC  
3.   
4.   
5. ABD <div>
arXiv:2601.13146v1 Announce Type: new 
Abstract: This paper introduces OPTIMUM-DERAM, a highly consistent, scalable, secure, and decentralized shared memory solution. Traditional distributed shared memory implementations offer multi-object support by multi-threading a single object memory instance over the same set of data hosts. While theoretically sound, the amount of resources required made such solutions prohibitively expensive in practical systems. OPTIMUM-DERAM proposes a decentralized, reconfigurable, atomic read/write shared memory (DeRAM) that: (i) achieves improved performance and storage scalability by leveraging Random Linear Network Codes (RLNC); (ii) scales in the number of supported atomic objects by introducing a new object placement and discovery approach based on a consistent hashing ring; (iii) scales in the number of participants by allowing dynamic joins and departures leveraging a blockchain oracle to serve as a registry service; and (iv) is secure against malicious behavior by tolerating Byzantine failures.
  Experimental results over a globally distributed set of nodes, help us realize the performance and scalability gains of OPTIMUM-DERAM over previous distributed shared memory solutions (i.e., the ABD algorithm [3])
]]></content:encoded>
<pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Scalable Federated Container Orchestration: The CODECO Approach</title>
<link>https://arxiv.org/abs/2601.13351</link>
<guid>https://arxiv.org/abs/2601.13351</guid>
<content:encoded><![CDATA[
<div> Kubernetes--AI<br /><br />:<br />1. CODECOKubernetes<br />2. --<br />3. AI<br />4. <br />5. CODECO <div>
arXiv:2601.13351v1 Announce Type: new 
Abstract: This paper presents CODECO, a federated orchestration framework for Kubernetes that addresses the limitations of cloud-centric deployment. CODECO adopts a data-compute-network co-orchestration approach to support heterogeneous infrastructures, mobility, and multi-provider operation.
  CODECO extends Kubernetes with semantic application models, partition-based federation, and AI-assisted decision support, enabling context-aware placement and adaptive management of applications and their micro-services across federated environments. A hybrid governance model combines centralized policy enforcement with decentralized execution and learning to preserve global coherence while supporting far Edge autonomy. The paper describes the architecture and core components of CODECO, outlines representative orchestration workflows, and introduces a software-based experimentation framework for reproducible evaluation in federated Edge-Cloud infrastructure environments.
]]></content:encoded>
<pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochastic Dynamic Pricing of Electric Vehicle Charging with Heterogeneous User Behavior: A Stackelberg Game Framework</title>
<link>https://arxiv.org/abs/2601.13571</link>
<guid>https://arxiv.org/abs/2601.13571</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. EV  
2. StackelbergLogitMNL  
3.   
4. PSA-CEMMSA  
5. Clayton22 <div>
arXiv:2601.13571v1 Announce Type: new 
Abstract: The rapid adoption of electric vehicles (EVs) introduces complex spatiotemporal demand management challenges for charging station operators (CSOs), exacerbated by demand imbalances, behavioral heterogeneity, and system uncertainty. Traditional dynamic pricing models, often relying on deterministic EV-CS pairings and network equilibrium assumptions, frequently oversimplify user behavior and lack scalability. This study proposes a stochastic, behaviorally heterogeneous dynamic pricing framework formulated as a bi-level Stackelberg game. The upper level optimizes time-varying pricing to maximize system-wide utility, while the lower level models decentralized EV users via a multinomial logit (MNL) choice model incorporating price sensitivity, battery aging, risk attitudes, and network travel costs. Crucially, the model avoids network equilibrium constraints to enhance scalability, with congestion effects represented via queuing-theoretic approximations. To efficiently solve the resulting large-scale optimization problem, a rolling-horizon approach combining the Dynamic Probabilistic Sensitivity Analysis-guided Cross-Entropy Method (PSA-CEM) with the Method of Successive Averages (MSA) is implemented. A real-world case study in Clayton, Melbourne, validates the framework using 22 charging stations. Simulation results demonstrate that the proposed mechanism substantially reduces queuing penalties and improves user utility compared to fixed and time-of-use pricing. The framework provides a robust, scalable tool for strategic EV charging management, balancing realism with computational efficiency.
]]></content:encoded>
<pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Blockchain-Oriented Software Engineering Architecture for Carbon Credit Certification Systems</title>
<link>https://arxiv.org/abs/2601.13772</link>
<guid>https://arxiv.org/abs/2601.13772</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. <br />3. 100 kWp<br />4. <br />5.  <div>
arXiv:2601.13772v1 Announce Type: new 
Abstract: Carbon credit systems have emerged as a policy tool to incentivize emission reductions and support the transition to clean energy. Reliable carbon-credit certification depends on mechanisms that connect actual, measured renewable-energy production to verifiable emission-reduction records. Although blockchain and IoT technologies have been applied to emission monitoring and trading, existing work offers limited support for certification processes, particularly for small and medium-scale renewable installations. This paper introduces a blockchain-based carbon-credit certification architecture, demonstrated through a 100 kWp photovoltaic case study, that integrates real-time IoT data collection, edge-level aggregation, and secure on-chain storage on a permissioned blockchain with smart contracts. Unlike approaches focused on trading mechanisms, the proposed system aligns with European legislation and voluntary carbon-market standards, clarifying the practical requirements and constraints that apply to photovoltaic operators. The resulting architecture provides a structured pathway for generating verifiable carbon-credit records and supporting third-party verification.
]]></content:encoded>
<pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Know Your Contract: Extending eIDAS Trust into Public Blockchains</title>
<link>https://arxiv.org/abs/2601.13903</link>
<guid>https://arxiv.org/abs/2601.13903</guid>
<content:encoded><![CDATA[
<div> eIDAS<br /><br />:  
1.   
2. eIDAS  
3. //  
4. eIDASMiCAPSD2PSRECDSAP-256CAdESeIDASEVM  
5. DeFiDeFi <div>
arXiv:2601.13903v1 Announce Type: new 
Abstract: Public blockchains lack native mechanisms to attribute on-chain actions to legally accountable entities, creating a fundamental barrier to institutional adoption and regulatory compliance. This paper presents an architecture that extends the European Union eIDAS trust framework into public blockchain ecosystems by cryptographically binding smart contracts to qualified electronic seals issued by Qualified Trust Service Providers. The mechanism establishes a verifiable chain of trust from the European Commission List of Trusted Lists to individual on-chain addresses, enabling machine-verifiable proofs for automated regulatory validation, such as Know Your Contract, Counterparty, and Business checks, without introducing new trusted intermediaries. Regulatory requirements arising from eIDAS, MiCA, PSD2, PSR, and the proposed European Business Wallet are analyzed, and a cryptographic suite meeting both eIDAS implementing regulations and EVM execution constraints following the Ethereum Fusaka upgrade is identified, namely ECDSA with P-256 and CAdES formatting. Two complementary trust validation models are presented: an off-chain workflow for agent-to-agent payment protocols and a fully on-chain workflow enabling regulatory-compliant DeFi operations between legal entities. The on-chain model converts regulatory compliance from a per-counterparty administrative burden into an automated, standardized process, enabling mutual validation at first interaction without prior business relationships. As eIDAS wallets become mandatory across EU member states, the proposed architecture provides a pathway for integrating European digital trust infrastructure into blockchain-based systems, enabling institutional DeFi participation, real-world asset tokenization, and agentic commerce within a trusted, regulatory-compliant framework.
]]></content:encoded>
<pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Infrastructure for Digital Notarizing, Signing and Sharing Files using Blockchain</title>
<link>https://arxiv.org/abs/2601.13907</link>
<guid>https://arxiv.org/abs/2601.13907</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. <br />3. <br />4. <br />5.  <div>
arXiv:2601.13907v1 Announce Type: new 
Abstract: Traditional paper-based document management has long posed challenges related to security, authenticity, and efficiency. Despite advances in digitalization, official documents remain vulnerable to forgery, loss, and unauthorized access. This thesis proposes a decentralized infrastructure for digital notarization, signing, and sharing of documents using blockchain technology. The research addresses key issues of transparency, immutability, and feasibility by defining system requirements, evaluating existing solutions, and proposing a novel architecture based on distributed systems.
  By combining cryptographic techniques with decentralized storage, this research contributes to the development of a more secure and efficient framework for managing official documents. The findings highlight the potential of blockchain-based digital notarization to streamline bureaucratic processes, mitigate security risks, and enhance user trust in digital document management.
]]></content:encoded>
<pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Cooperative Beamforming for BDRIS-Assisted Cell-Free MIMO OFDM Systems</title>
<link>https://arxiv.org/abs/2601.13201</link>
<guid>https://arxiv.org/abs/2601.13201</guid>
<content:encoded><![CDATA[
<div> BDRISMIMO-OFDM<br /><br />:  
1. BDRISMIMO-OFDM  
2. BDRIS  
3.   
4. DGCBDRISBDRIS  
5. DGCBDRIS <div>
arXiv:2601.13201v1 Announce Type: cross 
Abstract: In this paper, a wideband cell-free multi-stream multi-user Multiple-Input Multiple-Output (MIMO) Orthogonal Frequency Division Multiplexing (OFDM) system is considered operating within a smart wireless environment enabled by multiple Beyond Diagonal Reconfigurable Intelligent Surfaces (BDRISs). A novel decentralized active and passive beamforming framework, robust to imperfect channel state availability and with minimal cooperation among the system's multiple Base Stations (BSs) for deciding the final configurations of the shared BDRISs, is proposed, which aims to substantially reduce the overhead inherent in centralized solutions necessitating a central processing unit of high computational power. By considering a Dynamic Group-Connected (DGC) BDRIS architecture with frequency-selective responses per unit element, we formulate the system's sum-rate maximization problem with respect to the tunable capacitances and permutation matrices of the BDRISs as well as the precoding matrices of the BSs, which is solved via successive concave approximation and alternating projections as well as consensus-based updates for the BDRISs' design. Through extensive simulation results, it is showcased that the proposed robust decentralized cooperative approach with diverse BDRIS architectures outperforms non-cooperation benchmarks. It is also demonstrated that the considered DGC BDRIS architecture is able to provide sum-rate performance gains sufficiently close to the more complex fully-connected BDRIS structure.
]]></content:encoded>
<pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>LinkDID: A Privacy-Preserving, Sybil-Resistant and Key-Recoverable Decentralized Identity Scheme</title>
<link>https://arxiv.org/abs/2307.14679</link>
<guid>https://arxiv.org/abs/2307.14679</guid>
<content:encoded><![CDATA[
<div> Sybil<br /><br />:<br />1. LinkDIDWeb3Sybil<br />2. Sybil<br />3. <br />4. -<br />5. LinkDID2.413.31 <div>
arXiv:2307.14679v3 Announce Type: replace 
Abstract: Decentralized identity frameworks grant users full sovereignty over their digital assets in the Web3 ecosystem. However, allowing arbitrary creation of identifiers makes the system susceptible to Sybil attacks and puts assets at risk when keys are lost or compromised. Moreover, the lack of identification prevents anonymous credential schemes from deterring malicious transfers. While existing solutions attempt to address these issues by linking identifiers to entities through trusted intermediaries, these entities are not always accessible and require costly offline interactions.
  In this work, we introduce LinkDID, a decentralized identity scheme offering Sybil resistance, trustless key recovery, and nontransferable anonymous credentials. LinkDID creates blockchainbased bindings between identifiers and gradually combines identifiers belonging to the same holder into a unified associated identifier. As all identifiers within an association are presumed to belong to one individual, any fraudulent activity can be detected. The association grows larger as interactions increase, substantially reducing the likelihood of successful Sybil attacks. This mechanism allows holders to recover identifiers with lost or stolen keys by proving knowledge of specific association structures. Additionally, LinkDID prevents unauthorized transfers through blockchain-based identifier-key bindings and proofs of ownership for credentials.
  The evaluation shows that LinkDID effectively achieves progressive Sybil resistance while surpassing state-of-the-art anonymous credential schemes, achieving identifier association and credential presentation times of 2.41s and 3.31s on consumer-grade devices.
]]></content:encoded>
<pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>JANUS: A Difference-Oriented Analyzer For Financial Centralization Risks in Smart Contracts</title>
<link>https://arxiv.org/abs/2412.03938</link>
<guid>https://arxiv.org/abs/2412.03938</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. <br />3. JANUSSolidity<br />4. JANUS<br />5. JANUS54033,151 <div>
arXiv:2412.03938v2 Announce Type: replace 
Abstract: Some smart contracts violate decentralization principles by defining privileged accounts that manage other users' assets without permission, introducing centralization risks that have caused financial losses. Existing methods, however, face challenges in accurately detecting diverse centralization risks due to their dependence on predefined behavior patterns. In this paper, we propose JANUS, an automated analyzer for Solidity smart contracts that detects financial centralization risks independently of their specific behaviors. JANUS identifies differences between states reached by privileged and ordinary accounts, and analyzes whether these differences are finance-related. Focusing on the impact of risks rather than behaviors, JANUS achieves improved accuracy compared to existing tools and can uncover centralization risks with unknown patterns.
  To evaluate JANUS's performance, we compare it with other tools using a dataset of 540 contracts. Our evaluation demonstrates that JANUS outperforms representative tools in terms of detection accuracy for financial centralization risks . Additionally, we evaluate JANUS on a real-world dataset of 33,151 contracts, successfully identifying two types of risks that other tools fail to detect. We also prove that the state traversal method and variable summaries, which are used in JANUS to reduce the number of states to be compared, do not introduce false alarms or omissions in detection.
]]></content:encoded>
<pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Based Vulnerability Analysis of NFT Smart Contracts</title>
<link>https://arxiv.org/abs/2504.16113</link>
<guid>https://arxiv.org/abs/2504.16113</guid>
<content:encoded><![CDATA[
<div> NFTCART<br /><br />:<br />1. NFTAI<br />2. 16,527Risky Mutable ProxyERC-721<br />3. CART<br />4. GridSearch3D<br />5. AI <div>
arXiv:2504.16113v3 Announce Type: replace 
Abstract: With the rapid growth of the NFT market, the security of smart contracts has become crucial. However, existing AI-based detection models for NFT contract vulnerabilities remain limited due to their complexity, while traditional manual methods are time-consuming and costly. This study proposes an AI-driven approach to detect vulnerabilities in NFT smart contracts.
  We collected 16,527 public smart contract codes, classifying them into five vulnerability categories: Risky Mutable Proxy, ERC-721 Reentrancy, Unlimited Minting, Missing Requirements, and Public Burn. Python-processed data was structured into training/test sets. Using the CART algorithm with Gini coefficient evaluation, we built initial decision trees for feature extraction. A random forest model was implemented to improve robustness through random data/feature sampling and multitree integration. GridSearch hyperparameter tuning further optimized the model, with 3D visualizations demonstrating parameter impacts on vulnerability detection.
  Results show the random forest model excels in detecting all five vulnerabilities. For example, it identifies Risky Mutable Proxy by analyzing authorization mechanisms and state modifications, while ERC-721 Reentrancy detection relies on external call locations and lock mechanisms. The ensemble approach effectively reduces single-tree overfitting, with stable performance improvements after parameter tuning. This method provides an efficient technical solution for automated NFT contract detection and lays groundwork for scaling AI applications.
]]></content:encoded>
<pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Unsupervised Semantic Segmentation</title>
<link>https://arxiv.org/abs/2505.23292</link>
<guid>https://arxiv.org/abs/2505.23292</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. FLUSSFUSS<br />2. FUSS<br />3. FUSS<br />4. FUSSFL<br />5.  <div>
arXiv:2505.23292v2 Announce Type: replace 
Abstract: This work explores the application of Federated Learning (FL) to Unsupervised Semantic image Segmentation (USS). Recent USS methods extract pixel-level features using frozen visual foundation models and refine them through self-supervised objectives that encourage semantic grouping. These features are then grouped to semantic clusters to produce segmentation masks. Extending these ideas to federated settings requires feature representation and cluster centroid alignment across distributed clients, an inherently difficult task under heterogeneous data distributions in the absence of supervision. To address this, we propose FUSS (Federated Unsupervised image Semantic Segmentation) which is, to our knowledge, the first framework to enable fully decentralized, label-free semantic segmentation training. FUSS introduces novel federation strategies that promote global consistency in feature and prototype space, jointly optimizing local segmentation heads and shared semantic centroids. Experiments on both benchmark and real-world datasets, including binary and multi-class segmentation tasks, show that FUSS consistently outperforms local-only client trainings as well as extensions of classical FL algorithms under varying client data distributions. To fully support reproducibility, the source code, data partitioning scripts, and implementation details are publicly available at: https://github.com/evanchar/FUSS
]]></content:encoded>
<pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Blockchain Survey: Foundations, Trends, and Gaps</title>
<link>https://arxiv.org/abs/2507.13720</link>
<guid>https://arxiv.org/abs/2507.13720</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. <br />3. <br />4. <br />5.  <div>
arXiv:2507.13720v2 Announce Type: replace 
Abstract: Quantum computing poses fundamental risks to classical blockchain systems by undermining widely used cryptographic primitives. In response, two major research directions have emerged: post-quantum blockchains, which integrate quantum-resistant algorithms, and quantum blockchains, which leverage quantum properties such as entanglement and quantum key distribution. This survey reviews key developments in both areas, analyzing their cryptographic foundations, architectural designs, and implementation challenges. This work provides a comparative overview of technical proposals, highlight trade-offs in security, scalability, and deployment, and identify open research problems across hardware, consensus, and network design. The goal is to offer a structured and comprehensive reference for advancing secure blockchain systems in the quantum era.
]]></content:encoded>
<pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>USCSA: Evolution-Aware Security Analysis for Proxy-Based Upgradeable Smart Contracts</title>
<link>https://arxiv.org/abs/2512.08372</link>
<guid>https://arxiv.org/abs/2512.08372</guid>
<content:encoded><![CDATA[
<div> AST<br /><br />:<br />1. <br />2. USCSAASTLLM<br />3. 3,546<br />4. USCSA92.26%89.67%90.95% F1<br />5.  <div>
arXiv:2512.08372v3 Announce Type: replace 
Abstract: In the case of upgrading smart contracts on blockchain systems, it is essential to consider the continuity of upgrades and subsequent maintenance. In practice, upgrade operations often introduce new vulnerabilities. Existing static analysis tools usually only scan a single version and are unable to capture the correlation between code changes and emerging risks. To address this, we propose an Upgradeable Smart Contract Security Analyzer, USCSA, which uses Abstract Syntax Tree (AST) difference analysis to assess risks associated with the upgrade process and utilizes large language models (LLMs) for assisted reasoning to achieve high-confidence vulnerability attribution. We collected and analyzed 3,546 cases of vulnerabilities in upgradeable contracts, covering common vulnerability categories such as reentrancy, access control flaws, and integer overflow. Experimental results show that USCSA achieves a precision of 92.26%, a recall of 89.67%, and an F1-score of 90.95% in detecting upgrade-induced vulnerabilities. As a result, USCSA provides a significant advantage to improve the security and integrity of upgradeable smart contracts, offering a novel and efficient solution for security auditing on blockchain applications.
]]></content:encoded>
<pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>What You Trust Is Insecure: Demystifying How Developers (Mis)Use Trusted Execution Environments in Practice</title>
<link>https://arxiv.org/abs/2512.17363</link>
<guid>https://arxiv.org/abs/2512.17363</guid>
<content:encoded><![CDATA[
<div> TEEIntel SGXARM TrustZone<br /><br />:<br />1. 241Intel SGXARM TrustZone8<br />2. TEE30%7%AI12%<br />3. 32.4%SDKSDK<br />4. 25.3%TEE<br />5. TEE <div>
arXiv:2512.17363v3 Announce Type: replace 
Abstract: Trusted Execution Environments (TEEs), such as Intel SGX and ARM TrustZone, provide isolated regions of CPU and memory for secure computation and are increasingly used to protect sensitive data and code across diverse application domains. However, little is known about how developers actually use TEEs in practice. This paper presents the first large-scale empirical study of real-world TEE applications. We collected and analyzed 241 open-source projects from GitHub that utilize the two most widely-adopted TEEs, Intel SGX and ARM TrustZone. By combining manual inspection with customized static analysis scripts, we examined their adoption contexts, usage patterns, and development practices across three phases. First, we categorized the projects into 8 application domains and identified trends in TEE adoption over time. We found that the dominant use case is IoT device security (30%), which contrasts sharply with prior academic focus on blockchain and cryptographic systems (7%), while AI model protection (12%) is rapidly emerging as a growing domain. Second, we analyzed how TEEs are integrated into software and observed that 32.4% of the projects reimplement cryptographic functionalities instead of using official SDK APIs, suggesting that current SDKs may have limited usability and portability to meet developers' practical needs. Third, we examined security practices through manual inspection and found that 25.3% (61 of 241) of the projects exhibit insecure coding behaviors when using TEEs, such as hardcoded secrets and missing input validation, which undermine their intended security guarantees. Our findings have important implications for improving the usability of TEE SDKs and supporting developers in trusted software development.
]]></content:encoded>
<pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pruning as Evolution: Emergent Sparsity Through Selection Dynamics in Neural Networks</title>
<link>https://arxiv.org/abs/2601.10765</link>
<guid>https://arxiv.org/abs/2601.10765</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1.   
2.   
3.   
4.   
5. MNIST- <div>
arXiv:2601.10765v1 Announce Type: new 
Abstract: Neural networks are commonly trained in highly overparameterized regimes, yet empirical evidence consistently shows that many parameters become redundant during learning. Most existing pruning approaches impose sparsity through explicit intervention, such as importance-based thresholding or regularization penalties, implicitly treating pruning as a centralized decision applied to a trained model. This assumption is misaligned with the decentralized, stochastic, and path-dependent character of gradient-based training. We propose an evolutionary perspective on pruning: parameter groups (neurons, filters, heads) are modeled as populations whose influence evolves continuously under selection pressure. Under this view, pruning corresponds to population extinction: components with persistently low fitness gradually lose influence and can be removed without discrete pruning schedules and without requiring equilibrium computation. We formalize neural pruning as an evolutionary process over population masses, derive selection dynamics governing mass evolution, and connect fitness to local learning signals. We validate the framework on MNIST using a population-scaled MLP (784--512--256--10) with 768 prunable neuron populations. All dynamics reach dense baselines near 98\% test accuracy. We benchmark post-training hard pruning at target sparsity levels (35--50\%): pruning 35\% yields $\approx$95.5\% test accuracy, while pruning 50\% yields $\approx$88.3--88.6\%, depending on the dynamic. These results demonstrate that evolutionary selection produces a measurable accuracy--sparsity tradeoff without explicit pruning schedules during training.
]]></content:encoded>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparing User Time with a Socially-Aware Independent Metaverse Avatar</title>
<link>https://arxiv.org/abs/2601.11115</link>
<guid>https://arxiv.org/abs/2601.11115</guid>
<content:encoded><![CDATA[
<div> NP<br /><br />:<br />1. <br />2. <br />3. <br />4. NP<br />5.  <div>
arXiv:2601.11115v1 Announce Type: new 
Abstract: The Metaverse is redefining digital interactions by merging physical, virtual, and social dimensions, yet its effects on social networking remain largely unexplored. This work examines the role of independent avatars (autonomous digital entities capable of managing social interactions on behalf of users), to optimize social time allocation and reshape Metaverse-based Online Social Networks. We propose a novel computational model that integrates a quantitative and realistic representation of user social life, grounded in evolutionary anthropology, with a framework for avatar-mediated interactions. Our model quantifies the effectiveness of a partial replacement of in-person interactions with independent avatar interactions. Additionally, it accounts for social conflicts and specific socialization constraints. We leverage our model to explore the benefits and trade-offs of an avatar-augmented social life in the Metaverse. Since the exact problem formulation leads to an NP-hard optimization problem when incorporating avatars into the social network, we tackle this challenge by introducing a heuristic solution. Through simulations, we compare avatar-mediated and non-avatar-mediated social networking, demonstrating the potential of independent avatars to enhance social connectivity and efficiency. Our findings provide a foundation for optimizing Metaverse-based social interactions, as well as useful insights for future digital social network design.
]]></content:encoded>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Patterns of Bot Participation and Emotional Influence in Open-Source Development</title>
<link>https://arxiv.org/abs/2601.11138</link>
<guid>https://arxiv.org/abs/2601.11138</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1.   
2. 1036,8751050.28%  
3. U  
4.   
5.  <div>
arXiv:2601.11138v1 Announce Type: new 
Abstract: We study how bots contribute to open-source discussions in the Ethereum ecosystem and whether they influence developers' emotional tone. Our dataset covers 36,875 accounts across ten repositories with 105 validated bots (0.28%). Human participation follows a U-shaped pattern, while bots engage in uniform (pull requests) or late-stage (issues) activity. Bots respond faster than humans in pull requests but play slower maintenance roles in issues. Using a model trained on 27 emotion categories, we find bots are more neutral, yet their interventions are followed by reduced neutrality in human comments, with shifts toward gratitude, admiration, and optimism and away from confusion. These findings indicate that even a small number of bots are associated with changes in both timing and emotional dynamics of developer communication.
]]></content:encoded>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>InterPUF: Distributed Authentication via Physically Unclonable Functions and Multi-party Computation for Reconfigurable Interposers</title>
<link>https://arxiv.org/abs/2601.11368</link>
<guid>https://arxiv.org/abs/2601.11368</guid>
<content:encoded><![CDATA[
<div> SiPPUFMPC<br /><br />:  
1. SiPInterPUF  
2. InterPUFPUF  
3. MPCPUF  
4. 0.23%0.072%  
5.  <div>
arXiv:2601.11368v1 Announce Type: new 
Abstract: Modern system-in-package (SiP) platforms increasingly adopt reconfigurable interposers to enable plug-and-play chiplet integration across heterogeneous multi-vendor ecosystems. However, this flexibility introduces severe trust challenges, as traditional authentication schemes fail to scale or adapt in decentralized, post-fabrication programmable environments. This paper presents InterPUF, a compact and scalable authentication framework that transforms the interposer into a distributed root of trust. InterPUF embeds a route-based differential delay physically unclonable function (PUF) across the reconfigurable interconnect and secures authentication using multi-party computation (MPC), ensuring raw PUF signatures are never exposed. Our hardware evaluation shows only 0.23% area and 0.072% power overhead across diverse chiplets while preserving authentication latency within tens of nanoseconds. Simulation results using pyPUF confirm strong uniqueness, reliability, and modeling resistance under process, voltage, and temperature variations. By combining interposer-resident PUF primitives with cryptographic hashing and collaborative verification, InterPUF enforces a minimal-trust authentication model without relying on a centralized anchor.
]]></content:encoded>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Near-Optimal Decentralized Stochastic Nonconvex Optimization with Heavy-Tailed Noise</title>
<link>https://arxiv.org/abs/2601.11435</link>
<guid>https://arxiv.org/abs/2601.11435</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. Pull-Diag<br />3. <br />4. <br />5.  <div>
arXiv:2601.11435v1 Announce Type: cross 
Abstract: This paper studies decentralized stochastic nonconvex optimization problem over row-stochastic networks. We consider the heavy-tailed gradient noise which is empirically observed in many popular real-world applications. Specifically, we propose a decentralized normalized stochastic gradient descent with Pull-Diag gradient tracking, which achieves approximate stationary points with the optimal sample complexity and the near-optimal communication complexity. We further follow our framework to study the setting of undirected networks, also achieving the nearly tight upper complexity bounds. Moreover, we conduct empirical studies to show the practical superiority of the proposed methods.
]]></content:encoded>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enforcing Control Flow Integrity on DeFi Smart Contracts</title>
<link>https://arxiv.org/abs/2504.05509</link>
<guid>https://arxiv.org/abs/2504.05509</guid>
<content:encoded><![CDATA[
<div> DeFiCrossGuard<br /><br />:<br />1. DeFi<br />2. DeFi<br />3. 37DeFi<br />4. CrossGuard<br />5. CrossGuard350.26%Gas <div>
arXiv:2504.05509v2 Announce Type: replace 
Abstract: Smart contracts power decentralized financial (DeFi) services but are vulnerable to security exploits that can lead to significant financial losses. Existing security measures often fail to adequately protect these contracts due to the composability of DeFi protocols and the increasing sophistication of attacks. Through a large-scale empirical study of historical transactions from the 37 hacked DeFi protocols, we discovered that while benign transactions typically exhibit a limited number of unique control flows, in stark contrast, attack transactions consistently introduce novel, previously unobserved control flows. Building on these insights, we developed CrossGuard, a novel framework that enforces control flow integrity onchain to secure smart contracts. Crucially, CrossGuard does not require prior knowledge of specific hacks. Instead, configured only once at deployment, it enforces control flow whitelisting policies and applies simplification heuristics at runtime. This approach monitors and prevents potential attacks by reverting all transactions that do not adhere to the established control flow whitelisting rules. Our evaluation demonstrates that CrossGuard effectively blocks 35 of the 37 analyzed attacks when configured only once at contract deployment, maintaining a low false positive rate of 0.26% and minimal additional gas costs. These results underscore the efficacy of applying control flow integrity to smart contracts, significantly enhancing security beyond traditional methods and addressing the evolving threat landscape in the DeFi ecosystem.
]]></content:encoded>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Security Vulnerabilities in Ethereum Smart Contracts: A Systematic Analysis</title>
<link>https://arxiv.org/abs/2504.05968</link>
<guid>https://arxiv.org/abs/2504.05968</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. The DAO6000<br />2. RemixSolidity<br />3. The DAOParity<br />4. <br />5.  <div>
arXiv:2504.05968v4 Announce Type: replace 
Abstract: Smart contracts are a secure and trustworthy application that plays a vital role in decentralized applications in various fields such as insurance,the internet, and gaming. However, in recent years, smart contract security breaches have occurred frequently, and due to their financial properties, they have caused huge economic losses, such as the most famous security incident "The DAO" which caused a loss of over $60 million in Ethereum. This has drawn a lot of attention from all sides. Writing a secure smart contract is now a critical issue. This paper focuses on Ether smart contracts and explains the main components of Ether, smart contract architecture and mechanism. The environment used in this paper is the Ethernet environment, using remix online compilation platform and Solidity language, according to the four security events of American Chain, The DAO, Parity and KotET, the principles of integer overflow attack, reentrant attack, access control attack and denial of service attack are studied and analyzed accordingly, and the scenarios of these vulnerabilities are reproduced, and the measures to prevent them are given. Finally, preventive measures are given. In addition, the principles of short address attack, early transaction attack and privileged function exposure attack are also introduced in detail, and security measures are proposed. As vulnerabilities continue to emerge, their classification will also evolve. The analysis and research of the current vulnerabilities are also to lay a solid foundation for avoiding more vulnerabilities.
]]></content:encoded>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Aggregation to Selection: User-Validated Distributed Social Recommendation</title>
<link>https://arxiv.org/abs/2505.21388</link>
<guid>https://arxiv.org/abs/2505.21388</guid>
<content:encoded><![CDATA[
<div> Acc@K<br /><br />:<br />1. <br />2. <br />3. DeSocial<br />4. Acc@K<br />5. 4DeSocial <div>
arXiv:2505.21388v3 Announce Type: replace 
Abstract: Social recommender systems facilitate social connections by identifying potential friends for users. Each user maintains a local social network centered around themselves, resulting in a naturally distributed social structure. Recent research on distributed modeling for social recommender systems has gained increasing attention, as it naturally aligns with the user-centric structure of user interactions. Current distributed social recommender systems rely on automatically combining predictions from multiple models, often overlooking the user's active role in validating whether suggested connections are appropriate. Moreover, recommendation decisions are validated by individual users rather than derived from a single global ordering of candidates. As a result, standard ranking-based evaluation metrics make it difficult to evaluate whether a user-confirmed recommendation decision is actually correct. To address these limitations, we propose DeSocial, a distributed social recommendation framework with user-validation. DeSocial enables users to select recommendation algorithms to validate their potential connections, and the verification is processed through majority consensus among multiple independent user validators. To evaluate the distributed recommender system with user validator, we formulate this setting as a link prediction and verification task and introduce Acc@K, a consensus-based evaluation metric that measures whether user-approved recommendations are correct. Experiments on 4 real-world social networks shows that DeSocial improves decision correctness and robustness compared to single-point and distributed baselines. These findings highlight the potential of user-validated distributed recommender systems as a practical approach to social recommendation, with broader applicability to distributed and decentralized recommendations. Code: https://github.com/agiresearch/DeSocial.
]]></content:encoded>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Distributed Generative AI Approach for Heterogeneous Multi-Domain Environments under Data Sharing constraints</title>
<link>https://arxiv.org/abs/2507.12979</link>
<guid>https://arxiv.org/abs/2507.12979</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. GANGAN<br />2. KLDnon-IID<br />3. U<br />4. <br />5. 10%non-IID60%MNIST1.13FID270 <div>
arXiv:2507.12979v3 Announce Type: replace 
Abstract: Federated Learning has gained attention for its ability to enable multiple nodes to collaboratively train machine learning models without sharing raw data. At the same time, Generative AI -- particularly Generative Adversarial Networks (GANs) -- have achieved remarkable success across a wide range of domains, such as healthcare, security, and Image Generation. However, training generative models typically requires large datasets and significant computational resources, which are often unavailable in real-world settings. Acquiring such resources can be costly and inefficient, especially when many underutilized devices -- such as IoT devices and edge devices -- with varying capabilities remain idle. Moreover, obtaining large datasets is challenging due to privacy concerns and copyright restrictions, as most devices are unwilling to share their data. To address these challenges, we propose a novel approach for decentralized GAN training that enables utilizing distributed data and underutilized, low-capability devices while not sharing data in its raw form. Our approach is designed to tackle key challenges in decentralized environments, combining KLD-weighted Clustered Federated Learning to address the issues of data heterogeneity and multi-domain datasets, with Heterogeneous U-Shaped split learning to tackle the challenge of device heterogeneity under strict data sharing constraints -- ensuring that no labels or raw data, whether real or synthetic, are ever shared between nodes. Experiments show that our approach demonstrates significant improvements across key metrics, where it achieves an average 10% boost in classification metrics (up to 60% in multi-domain non-IID settings), 1.1x -- 3x higher image generation scores for the MNIST family datasets, and 2x -- 70x lower FID scores for higher resolution datasets. Find our code at https://distributed-gen-ai.github.io/huscf-gan.github.io/.
]]></content:encoded>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incentive Mechanism Design for Privacy-Preserving Decentralized Blockchain Relayers</title>
<link>https://arxiv.org/abs/2601.06699</link>
<guid>https://arxiv.org/abs/2601.06699</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. relayer<br />3. <br />4. <br />5. 0.05 <div>
arXiv:2601.06699v2 Announce Type: replace 
Abstract: Public blockchains, though renowned for their transparency and immutability, suffer from significant privacy concerns. Network-level analysis and long-term observation of publicly available transactions can often be used to infer user identities. To mitigate this, several blockchain applications rely on relayers, which serve as intermediary nodes between users and smart contracts deployed on the blockchain. However, dependence on a single relayer not only creates a single point of failure but also introduces exploitable vulnerabilities that weaken the system's privacy guarantees. This paper proposes a decentralized relayer architecture that enhances privacy and reliability through game-theoretic incentive design. We model the interaction among relayers as a non-cooperative game and design an incentive mechanism in which probabilistic uploading emerges as a unique mixed Nash equilibrium. Using evolutionary game analysis, we demonstrate the equilibrium's stability against perturbations and coordinated deviations. Through numerical evaluations, we analyze how equilibrium strategies and system behavior evolve with key parameters such as the number of relayers, upload costs, rewards, and penalties. In particular, we show that even with high transaction costs, the system maintains reliability with an outage probability below 0.05 . Furthermore, our results highlight a fundamental trade-off between privacy, reliability, robustness, and cost in decentralized relayer systems.
]]></content:encoded>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instalaci\'on, configuraci\'on y utilizaci\'on de un nodo Bitcoin en Linux</title>
<link>https://arxiv.org/abs/2601.09748</link>
<guid>https://arxiv.org/abs/2601.09748</guid>
<content:encoded><![CDATA[
<div> LinuxBitcoin Core<br /><br />:  
1. Linux  
2.   
3. Bitcoin Core  
4. txindexprunedbcachemaxmempoolmaxconnections  
5. IBDCPUI/O <div>
arXiv:2601.09748v1 Announce Type: new 
Abstract: This paper documents the installation, configuration, and operation of a full Bitcoin node in a Linux environment, from manual compilation of the source code to complete synchronization with the network. The technical phases of the process are described, the main files generated by Bitcoin Core are analyzed, and the effects of the parameters txindex, prune, dbcache, maxmempool, and maxconnections are empirically studied. System resources during the block download (IBD) mechanism are also documented, and the operational importance of each resource is explained. This paper provides a solid foundation for future research proposals on Bitcoin node performance or for the development of blockchain data query tools.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Risk-Stratified Benchmark Dataset for Bad Randomness (SWC-120) Vulnerabilities in Ethereum Smart Contracts</title>
<link>https://arxiv.org/abs/2601.09836</link>
<guid>https://arxiv.org/abs/2601.09836</guid>
<content:encoded><![CDATA[
<div> Bad Randomness<br /><br />:  
1. block.timestampBad RandomnessSWC-120  
2.   
3. 1,752  
4. 49%  
5. SlitherMythril <div>
arXiv:2601.09836v1 Announce Type: new 
Abstract: Many Ethereum smart contracts rely on block attributes such as block.timestamp or blockhash to generate random numbers for applications like lotteries and games. However, these values are predictable and miner-manipulable, creating the Bad Randomness vulnerability (SWC-120) that has led to real-world exploits. Current detection tools identify only simple patterns and fail to verify whether protective modifiers actually guard vulnerable code. A major obstacle to improving these tools is the lack of large, accurately labeled datasets. This paper presents a benchmark dataset of 1,752 Ethereum smart contracts with validated Bad Randomness vulnerabilities. We developed a five-phase methodology comprising keyword filtering, pattern matching with 58 regular expressions, risk classification, function-level validation, and context analysis. The function-level validation revealed that 49% of contracts initially classified as protected were actually exploitable because modifiers were applied to different functions than those containing vulnerabilities. We classify contracts into four risk levels based on exploitability: HIGH_RISK (no protection), MEDIUM_RISK (miner-exploitable only), LOW_RISK (owner-exploitable only), and SAFE (using Chainlink VRF or commit-reveal). Our dataset is 51 times larger than RNVulDet and the first to provide function-level validation and risk stratification. Evaluation of Slither and Mythril revealed significant detection gaps, as both tools identified none of the vulnerable contracts in our sample, indicating limitations in handling complex randomness patterns. The dataset and validation scripts are publicly available to support future research in smart contract security.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Control Theoretic Approach to Decentralized AI Economy Stabilization via Dynamic Buyback-and-Burn Mechanisms</title>
<link>https://arxiv.org/abs/2601.09961</link>
<guid>https://arxiv.org/abs/2601.09961</guid>
<content:encoded><![CDATA[
<div> PID<br /><br />:  
1.   
2.   
3. DCBMPID  
4. Jump-DiffusionDCBM66%19.5%8.1%  
5.  <div>
arXiv:2601.09961v1 Announce Type: new 
Abstract: The democratization of artificial intelligence through decentralized networks represents a paradigm shift in computational provisioning, yet the long-term viability of these ecosystems is critically endangered by the extreme volatility of their native economic layers. Current tokenomic models, which predominantly rely on static or threshold-based buyback heuristics, are ill-equipped to handle complex system dynamics and often function pro-cyclically, exacerbating instability during market downturns. To bridge this gap, we propose the Dynamic-Control Buyback Mechanism (DCBM), a formalized control-theoretic framework that utilizes a Proportional-Integral-Derivative (PID) controller with strict solvency constraints to regulate the token economy as a dynamical system. Extensive agent-based simulations utilizing Jump-Diffusion processes demonstrate that DCBM fundamentally outperforms static baselines, reducing token price volatility by approximately 66% and lowering operator churn from 19.5% to 8.1% in high-volatility regimes. These findings establish that converting tokenomics from static rules into continuous, structurally constrained control loops is a necessary condition for secure and sustainable decentralized intelligence networks.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Unlearning in Edge Networks: A Survey of Fundamentals, Challenges, Practical Applications and Future Directions</title>
<link>https://arxiv.org/abs/2601.09978</link>
<guid>https://arxiv.org/abs/2601.09978</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. FL<br />2. MUFUL<br />3. FUL<br />4. FUL<br />5. FUL <div>
arXiv:2601.09978v1 Announce Type: new 
Abstract: The proliferation of connected devices and privacy-sensitive applications has accelerated the adoption of Federated Learning (FL), a decentralized paradigm that enables collaborative model training without sharing raw data. While FL addresses data locality and privacy concerns, it does not inherently support data deletion requests that are increasingly mandated by regulations such as the Right to be Forgotten (RTBF). In centralized learning, this challenge has been studied under the concept of Machine Unlearning (MU), that focuses on efficiently removing the influence of specific data samples or clients from trained models. Extending this notion to federated settings has given rise to Federated Unlearning (FUL), a new research area concerned with eliminating the contributions of individual clients or data subsets from the global FL model in a distributed and heterogeneous environment. In this survey, we first introduce the fundamentals of FUL. Then, we review the FUL frameworks that are proposed to address the three main implementation challenges, i.e., communication cost, resource allocation as well as security and privacy. Furthermore, we discuss applications of FUL in the modern distributed computer networks. We also highlight the open challenges and future research opportunities. By consolidating existing knowledge and mapping open problems, this survey aims to serve as a foundational reference for researchers and practitioners seeking to advance FL to build trustworthy, regulation-compliant and user-centric federated systems.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>PID-Guided Partial Alignment for Multimodal Decentralized Federated Learning</title>
<link>https://arxiv.org/abs/2601.10012</link>
<guid>https://arxiv.org/abs/2601.10012</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. DFL<br />2. PARSEPID<br />3. <br />4. PARSE/DFL<br />5. PARSE <div>
arXiv:2601.10012v1 Announce Type: new 
Abstract: Multimodal decentralized federated learning (DFL) is challenging because agents differ in available modalities and model architectures, yet must collaborate over peer-to-peer (P2P) networks without a central coordinator. Standard multimodal pipelines learn a single shared embedding across all modalities. In DFL, such a monolithic representation induces gradient misalignment between uni- and multimodal agents; as a result, it suppresses heterogeneous sharing and cross-modal interaction. We present PARSE, a multimodal DFL framework that operationalizes partial information decomposition (PID) in a server-free setting. Each agent performs feature fission to factorize its latent representation into redundant, unique, and synergistic slices. P2P knowledge sharing among heterogeneous agents is enabled by slice-level partial alignment: only semantically shareable branches are exchanged among agents that possess the corresponding modality. By removing the need for central coordination and gradient surgery, PARSE resolves uni-/multimodal gradient conflicts, thereby overcoming the multimodal DFL dilemma while remaining compatible with standard DFL constraints. Across benchmarks and agent mixes, PARSE yields consistent gains over task-, modality-, and hybrid-sharing DFL baselines. Ablations on fusion operators and split ratios, together with qualitative visualizations, further demonstrate the efficiency and robustness of the proposed design.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAFEDistill: Learning Personalized and Dynamic Models through Federated Early-Exit Network Distillation</title>
<link>https://arxiv.org/abs/2601.10015</link>
<guid>https://arxiv.org/abs/2601.10015</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. PFL<br />2. EENsPFL<br />3. <br />4. CAFEDistill<br />5. 30.79%-46.86% <div>
arXiv:2601.10015v1 Announce Type: new 
Abstract: Personalized Federated Learning (PFL) enables collaboratively model training on decentralized, heterogeneous data while tailoring them to each client's unique distribution. However, existing PFL methods produce static models with a fixed tradeoff between accuracy and efficiency, limiting their applicability in environments where inference requirements vary with contexts and resource availability. Early-exit networks (EENs) offer adaptive inference by attaching intermediate classifiers. Yet integrating them into PFL is challenging due to client-wise heterogeneity and depth-wise interference arising from conflicting exit objectives. Prior studies fail to resolve both conflicts simultaneously, leading to suboptimal performance. In this paper, we propose CAFEDistill, a Conflict-Aware Federated Exit Distillation framework that jointly addresses these conflicts and extends PFL to early-exit networks. Through a progressive, depth-prioritized student coordination mechanism, CAFEDistill mitigates interference among shallow and deep exits while allowing effective personalized knowledge transfer across clients. Furthermore, it reduces communication overhead via a client-decoupled formulation. Extensive evaluations show that CAFEDistill outperforms the state-of-the-arts, achieving higher accuracy and reducing inference costs by 30.79%-46.86%.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fuzzychain-edge: A novel Fuzzy logic-based adaptive Access control model for Blockchain in Edge Computing</title>
<link>https://arxiv.org/abs/2601.10105</link>
<guid>https://arxiv.org/abs/2601.10105</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. <br />3. Fuzzychain-edge<br />4. <br />5.  <div>
arXiv:2601.10105v1 Announce Type: new 
Abstract: The rapid integration of IoT with edge computing has revolutionized various domains, particularly healthcare, by enabling real-time data sharing, remote monitoring, and decision-making. However, it introduces critical challenges, including data privacy breaches, security vulnerabilities, especially in environments dealing with sensitive information. Traditional access control mechanisms and centralized security systems do not address these issues, leaving IoT environments exposed to unauthorized access and data misuse. This research proposes Fuzzychain-edge, a novel Fuzzy logic-based adaptive Access control model for Blockchain in Edge Computing framework designed to overcome these limitations by incorporating Zero-Knowledge Proofs (ZKPs), fuzzy logic, and smart contracts. ZKPs secure sensitive data during access control processes by enabling verification without revealing confidential details, thereby ensuring user privacy. Fuzzy logic facilitates adaptive, context-aware decision-making for access control by dynamically evaluating parameters such as data sensitivity, trust levels, and user roles. Blockchain technology, with its decentralized and immutable architecture, ensures transparency, traceability, and accountability using smart contracts that automate access control processes. The proposed framework addresses key challenges by enhancing security, reducing the likelihood of unauthorized access, and providing a transparent audit trail of data transactions. Expected outcomes include improved data privacy, accuracy in access control, and increased user trust in IoT systems. This research contributes significantly to advancing privacy-preserving, secure, and traceable solutions in IoT environments, laying the groundwork for future innovations in decentralized technologies and their applications in critical domains such as healthcare and beyond.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>TopoDIM: One-shot Topology Generation of Diverse Interaction Modes for Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2601.10120</link>
<guid>https://arxiv.org/abs/2601.10120</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. TopoDIM<br />2. <br />3. TopoDIM<br />4. TopoDIM46.41%1.50%<br />5.  <div>
arXiv:2601.10120v1 Announce Type: new 
Abstract: Optimizing communication topology in LLM-based multi-agent system is critical for enabling collective intelligence. Existing methods mainly rely on spatio-temporal interaction paradigms, where the sequential execution of multi-round dialogues incurs high latency and computation. Motivated by the recent insights that evaluation and debate mechanisms can improve problem-solving in multi-agent systems, we propose TopoDIM, a framework for one-shot Topology generation with Diverse Interaction Modes. Designed for decentralized execution to enhance adaptability and privacy, TopoDIM enables agents to autonomously construct heterogeneous communication without iterative coordination, achieving token efficiency and improved task performance. Experiments demonstrate that TopoDIM reduces total token consumption by 46.41% while improving average performance by 1.50% over state-of-the-art methods. Moreover, the framework exhibits strong adaptability in organizing communication among heterogeneous agents. Code is available at: https://anonymous.4open.science/r/TopoDIM-8D35/
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>PADER: Paillier-based Secure Decentralized Social Recommendation</title>
<link>https://arxiv.org/abs/2601.10212</link>
<guid>https://arxiv.org/abs/2601.10212</guid>
<content:encoded><![CDATA[
<div> Paillier<br /><br />:<br />1. PADERPaillier<br />2. <br />3. SoReg<br />4. <br />5. 31 <div>
arXiv:2601.10212v1 Announce Type: new 
Abstract: The prevalence of recommendation systems also brings privacy concerns to both the users and the sellers, as centralized platforms collect as much data as possible from them. To keep the data private, we propose PADER: a Paillier-based secure decentralized social recommendation system. In this system, the users and the sellers are nodes in a decentralized network. The training and inference of the recommendation model are carried out securely in a decentralized manner, without the involvement of a centralized platform. To this end, we apply the Paillier cryptosystem to the SoReg (Social Regularization) model, which exploits both user's ratings and social relations. We view the SoReg model as a two-party secure polynomial evaluation problem and observe that the simple bipartite computation may result in poor efficiency. To improve efficiency, we design secure addition and multiplication protocols to support secure computation on any arithmetic circuit, along with an optimal data packing scheme that is suitable for the polynomial computations of real values. Experiment results show that our method only takes about one second to iterate through one user with hundreds of ratings, and training with ~500K ratings for one epoch only takes <3 hours, which shows that the method is practical in real applications. The code is available at https://github.com/GarminQ/PADER.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCRamble: Adaptive Decentralized Overlay Construction for Blockchain Networks</title>
<link>https://arxiv.org/abs/2601.10277</link>
<guid>https://arxiv.org/abs/2601.10277</guid>
<content:encoded><![CDATA[
<div>     SCRamble<br /><br />:<br />1. 15<br />2. <br />3. <br />4. SCRamble<br />5. SCRamble <div>
arXiv:2601.10277v1 Announce Type: new 
Abstract: Despite being under development for over 15 years, transaction throughput remains one of the key challenges confronting blockchains, which typically has a cap of a limited number of transactions per second. A fundamental factor limiting this metric is the network latency associated with the block propagation throughout of the underlying peer-to-peer network, typically formed through random connections. Accelerating the dissemination of blocks not only improves transaction rates, but also enhances system security by reducing the probability of forks. This paper introduces SCRamble: a decentralized protocol that significantly reduces block dissemination time in blockchain networks. SCRamble's effectiveness is attributed to its innovative link selection strategy, which integrates two heuristics: a scoring mechanism that assesses block arrival times from neighboring peers, and a second heuristic that takes network latency into account.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multipath Routing for Multi-Hop UAV Networks</title>
<link>https://arxiv.org/abs/2601.10299</link>
<guid>https://arxiv.org/abs/2601.10299</guid>
<content:encoded><![CDATA[
<div> Dec-POMDP<br /><br />:<br />1. <br />2. <br />3. Dec-POMDP<br />4. IPPO-DMIPPO<br />5. IPPO-DM <div>
arXiv:2601.10299v1 Announce Type: new 
Abstract: Multi-hop uncrewed aerial vehicle (UAV) networks are promising to extend the terrestrial network coverage. Existing multi-hop UAV networks employ a single routing path by selecting the next-hop forwarding node in a hop-by-hop manner, which leads to local congestion and increases traffic delays. In this paper, a novel traffic-adaptive multipath routing method is proposed for multi-hop UAV networks, which enables each UAV to dynamically split and forward traffic flows across multiple next-hop neighbors, thus meeting latency requirements of diverse traffic flows in dynamic mobile environments. An on-time packet delivery ratio maximization problem is formulated to determine the traffic splitting ratios at each hop. This sequential decision-making problem is modeled as a decentralized partially observable Markov decision process (Dec-POMDP). To solve this Dec-POMDP, a novel multi-agent deep reinforcement leaning (MADRL) algorithm, termed Independent Proximal Policy Optimization with Dirichlet Modeling (IPPO-DM), is developed. Specifically, the IPPO serves as the core optimization framework, where the Dirichlet distribution is leveraged to parameterize a continuous stochastic policy network on the probability simplex, inherently ensuring feasible traffic splitting ratios. Simulation results demonstrate that IPPO-DM outperforms benchmark schemes in terms of both delivery latency guarantee and packet loss performance.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDN-Driven Innovations in MANETs and IoT: A Path to Smarter Networks</title>
<link>https://arxiv.org/abs/2601.10544</link>
<guid>https://arxiv.org/abs/2601.10544</guid>
<content:encoded><![CDATA[
<div> MANETsIoTSDN<br /><br />:<br />1. MANETsIoT<br />2. SDN<br />3. SDNCAPEXOPEX<br />4. SDN<br />5. SDN <div>
arXiv:2601.10544v1 Announce Type: new 
Abstract: Mobile Ad Hoc Networks (MANETs) and Internet of Things (IoT) networks operate in decentralized and dynamic environments, making them ideal for scenarios lacking traditional infrastructure. However, these networks face challenges such as inefficient routing, limited scalability, and security vulnerabilities due to their decentralized nature and resource constraints. This paper explores the integration of Software-Defined Networking (SDN) as a unified solution that leverages its centralized control and network programmability to improve routing, resource management, and security. A mathematical model evaluates the impact of SDN integration on Capital Expenditure (CAPEX), Operational Expenditure (OPEX), and performance metrics. Results demonstrate that SDN-enhanced MANETs and IoT networks offer superior scalability, reduced latency, increased throughput, and lower packet loss, especially in dynamic and large-scale environments. While SDN introduces computational overhead, it significantly enhances routing efficiency, resource optimization, and adaptability. The proposed framework provides a robust and scalable solution, enabling the development of network architectures that efficiently manage growing node densities, dynamic topologies, and high data traffic. This approach ensures resilience, making it well-suited to meet the performance and reliability demands of modern, large-scale applications.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Mobile Ad Hoc Networks (MANETs) with Software-Defined Networking (SDN): A Balanced Approach</title>
<link>https://arxiv.org/abs/2601.10556</link>
<guid>https://arxiv.org/abs/2601.10556</guid>
<content:encoded><![CDATA[
<div> MANETSDN<br /><br />:  
1. MANET  
2. SDNMANET  
3. SDNMANET  
4. CAPEXOPEX  
5. SDNMANET <div>
arXiv:2601.10556v1 Announce Type: new 
Abstract: Mobile Ad Hoc Networks (MANETs) are decentralized wireless networks, characterized by their dynamic topologies and node mobility. In the era of cutting-edge technologies, integrating Software-Defined Networking (SDN) with MANETs offers a promising solution to manage these challenges more efficiently. This paper presents a balanced discussion of MANETs and SDN, demonstrating how SDN principles, such as centralized control and network virtualization, can optimize MANET performance in terms of scalability, cost-efficiency, and security. A mathematical model is developed to analyze Capital Expenditures (CAPEX), Operational Expenditures (OPEX), and network efficiency.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Basis-Spline Assisted Coded Computing: Strategies and Error Bounds</title>
<link>https://arxiv.org/abs/2601.10616</link>
<guid>https://arxiv.org/abs/2601.10616</guid>
<content:encoded><![CDATA[
<div> BerrutB<br /><br />:<br />1. stragglers<br />2. <br />3. Berrut<br />4. BB<br />5. BBerrut <div>
arXiv:2601.10616v1 Announce Type: new 
Abstract: Coded computing has become a key framework for reliable distributed computation over decentralized networks, effectively mitigating the impact of stragglers. Although there exists a wide range of coded computing methods to handle both polynomial and non-polynomial functions, computing methods for the latter class have received traction due its inherent challenges in reconstructing non-polynomial functions using a finite number of evaluations. Among them, the state-of-the-art method is Berrut Approximated coded computing, wherein Berrut interpolants, are used for approximating the non-polynomial function. However, since Berrut interpolants have global support characteristics, such methods are known to offer degraded accuracy when the number of stragglers is large. To address this challenge, we propose a coded computing framework based on cubic B-spline interpolation. In our approach, server-side function evaluations are reconstructed at the master node using B-splines, exploiting their local support and smoothness properties to enhance stability and accuracy. We provide a systematic methodology for integrating B-spline interpolation into coded computing and derive theoretical bounds on approximation error in terms of the number of servers and stragglers. Comparative analysis demonstrates that our framework significantly outperforms Berrut-based methods for various non-polynomial functions.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Merged Bitcoin: Proof of Work Blockchains with Multiple Hash Types</title>
<link>https://arxiv.org/abs/2601.09090</link>
<guid>https://arxiv.org/abs/2601.09090</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. 51%<br />2. Merged Bitcoin<br />3. <br />4. <br />5. Grover <div>
arXiv:2601.09090v1 Announce Type: new 
Abstract: Proof of work blockchain protocols using multiple hash types are considered. It is proven that the security region of such a protocol cannot be the AND of a 51\% attack on all the hash types. Nevertheless, a protocol called Merged Bitcoin is introduced, which is the Bitcoin protocol where links between blocks can be formed using multiple different hash types. Closed form bounds on its security region in the $\Delta$-bounded delay network model are proven, and these bounds are compared to simulation results. This protocol is proven to maximize cost of attack in the linear cost-per-hash model. A difficulty adjustment method is introduced, and it is argued that this can partly remedy asymmetric advantages an adversary may gain in hashing power for some hash types, including from algorithmic advances, quantum attacks like Grover's algorithm, or hardware backdoor attacks.
]]></content:encoded>
<pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing View Change for Byzantine Fault Tolerance in Parallel Consensus</title>
<link>https://arxiv.org/abs/2601.09184</link>
<guid>https://arxiv.org/abs/2601.09184</guid>
<content:encoded><![CDATA[
<div> BFT

<br /><br />:  
1. BFT  
2.   
3. VCO  
4. BendersVCO  
5. AzureVCOBFT <div>
arXiv:2601.09184v1 Announce Type: new 
Abstract: The parallel Byzantine Fault Tolerant (BFT) protocol is viewed as a promising solution to address the consensus scalability issue of the permissioned blockchain. One of the main challenges in parallel BFT is the view change process that happens when the leader node fails, which can lead to performance bottlenecks. Existing parallel BFT protocols typically rely on passive view change mechanisms with blind leader rotation. Such approaches frequently select unavailable or slow nodes as leaders, resulting in degraded performance. To address these challenges, we propose a View Change Optimization (VCO) model based on mixed integer programming that optimizes leader selection and follower reassignment across parallel committees by considering communication delays and failure scenarios. We applied a decomposition method with efficient subproblems and improved benders cuts to solve the VCO model. Leveraging the results of improved decomposition solution method, we propose an efficient iterative backup leader selection algorithm as views proceed. By performing experiments in Microsoft Azure cloud environments, we demonstrate that the VCO-driven parallel BFT outperforms existing configuration methods under both normal operation and faulty condition. The results show that the VCO model is effective as network size increases, making it a suitable solution for high-performance parallel BFT systems.
]]></content:encoded>
<pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SiliconHealth: A Complete Low-Cost Blockchain Healthcare Infrastructure for Resource-Constrained Regions Using Repurposed Bitcoin Mining ASICs</title>
<link>https://arxiv.org/abs/2601.09557</link>
<guid>https://arxiv.org/abs/2601.09557</guid>
<content:encoded><![CDATA[
<div> ASICDeterministic Hardware Fingerprinting<br /><br />:<br />1. SiliconHealth<br />2. ASICAntminer S19 ProS9Lucky Miner LV06<br />3. Deterministic Hardware FingerprintingDHFSHA-256 ASIC100%<br />4. Reed-Solomon LSB3040%RAG<br />5. GPU96%8476 <div>
arXiv:2601.09557v1 Announce Type: new 
Abstract: This paper presents SiliconHealth, a comprehensive blockchain-based healthcare infrastructure designed for resource-constrained regions, particularly sub-Saharan Africa. We demonstrate that obsolete Bitcoin mining Application-Specific Integrated Circuits (ASICs) can be repurposed to create a secure, low-cost, and energy-efficient medical records system. The proposed architecture employs a four-tier hierarchical network: regional hospitals using Antminer S19 Pro (90+ TH/s), urban health centers with Antminer S9 (14 TH/s), rural clinics equipped with Lucky Miner LV06 (500 GH/s, 13W), and mobile health points with portable ASIC devices. We introduce the Deterministic Hardware Fingerprinting (DHF) paradigm, which repurposes SHA-256 mining ASICs as cryptographic proof generators, achieving 100% verification rate across 23 test proofs during 300-second validation sessions. The system incorporates Reed-Solomon LSB watermarking for medical image authentication with 30-40% damage tolerance, semantic Retrieval-Augmented Generation (RAG) for intelligent medical record queries, and offline synchronization protocols for intermittent connectivity. Economic analysis demonstrates 96% cost reduction compared to GPU-based alternatives, with total deployment cost of $847 per rural clinic including 5-year solar power infrastructure. Validation experiments on Lucky Miner LV06 (BM1366 chip, 5nm) achieve 2.93 MH/W efficiency and confirm hardware universality. This work establishes a practical framework for deploying verifiable, tamper-proof electronic health records in regions where traditional healthcare IT infrastructure is economically unfeasible, potentially benefiting over 600 million people lacking access to basic health information systems.
]]></content:encoded>
<pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Consensus-Based Distributed Beamforming for Wideband Cell-free Multi-RIS MISO Systems</title>
<link>https://arxiv.org/abs/2601.08946</link>
<guid>https://arxiv.org/abs/2601.08946</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. RISMISO<br />2. <br />3. RIS<br />4. RIS<br />5.  <div>
arXiv:2601.08946v1 Announce Type: cross 
Abstract: The cell-free networking paradigm constitutes a revolutionary architecture for future generations of wireless networks, which has been recently considered in synergy with Reconfigurable Intelligent Surfaces (RISs), a promising physical-layer technology for signal propagation programmability. In this paper, we focus on wideband cell-free multi-RIS-empowered Multiple-Input Single-Output (MISO) systems and present a decentralized cooperative active and passive beamforming scheme, aiming to provide an efficient alternative towards the cooperation overhead of available centralized schemes depending on central processing unit. Considering imperfect channel information availability and realistic frequency selectivity behavior of each RIS's element response, we devise a distributed optimization approach based on consensus updates for the RISs' phase configurations. Our simulation results showcase that the proposed distributed design is superior to centralized schemes that are based on various Lorentzian-type wideband modeling approaches for the RISs.
]]></content:encoded>
<pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Game of Coding: Sybil Resistant Decentralized Machine Learning with Minimal Trust Assumption</title>
<link>https://arxiv.org/abs/2410.05540</link>
<guid>https://arxiv.org/abs/2410.05540</guid>
<content:encoded><![CDATA[
<div> Sybil<br /><br />:<br />1. <br />2. <br />3. N2<br />4. Sybil<br />5. DCDC <div>
arXiv:2410.05540v3 Announce Type: replace 
Abstract: Coding theory plays a crucial role in ensuring data integrity and reliability across various domains, from communication to computation and storage systems. However, its reliance on trust assumptions for data recovery, which requires the number of honest nodes to exceed adversarial nodes by a certain margin, poses significant challenges, particularly in emerging decentralized systems where trust is a scarce resource. To address this, the game of coding framework was introduced, offering insights into strategies for data recovery within incentive-oriented environments. In such environments, participant nodes are rewarded as long as the system remains functional (live). This incentivizes adversaries to maximize their rewards (utility) by ensuring that the decoder, as the data collector (DC), successfully recovers the data, preferably with a high estimation error. This rational behavior is leveraged in a game-theoretic framework, where the equilibrium leads to a robust and resilient system, referred to as the game of coding. The focus of the earliest version of the game of coding was limited to scenarios involving only two nodes. In this paper, we generalize the game of coding framework to scenarios with $N \ge 2$ nodes, exploring critical aspects of system behavior. Specifically, we (i) demonstrate that the adversary's utility at equilibrium is non-increasing with additional adversarial nodes, ensuring no gain for the adversary and no pain for the DC, thus establishing the game of coding framework's Sybil resistance; (ii) show that increasing the number of honest nodes does not always enhance the DC's utility, providing examples and proposing an algorithm to identify and mitigate this counterintuitive effect; and (iii) outline the optimal strategies for both the DC and the adversary, demonstrating that the system achieves enhanced liveness at equilibrium.
]]></content:encoded>
<pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smart Contract Vulnerabilities, Tools, and Benchmarks: an Updated Systematic Literature Review</title>
<link>https://arxiv.org/abs/2412.01719</link>
<guid>https://arxiv.org/abs/2412.01719</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. 2223,380<br />3. 19213<br />4. 219<br />5. 133 <div>
arXiv:2412.01719v3 Announce Type: replace 
Abstract: Smart contracts are self-executing programs on blockchain platforms like Ethereum, which have revolutionized decentralized finance by enabling trustless transactions and the operation of decentralized applications. Despite their potential, the security of smart contracts remains a critical concern due to their immutability and transparency, which expose them to malicious actors. Numerous solutions for vulnerability detection have been proposed, but it is still unclear which one is the most effective. This paper presents a systematic literature review that explores vulnerabilities in Ethereum smart contracts, focusing on automated detection tools and benchmark evaluation. We reviewed 3,380 studies from five digital libraries and five major software engineering conferences, applying a structured selection process that resulted in 222 high-quality studies. The key results include a hierarchical taxonomy of 192 vulnerabilities grouped into 13 categories, a comprehensive list of 219 detection tools with corresponding functionalities, methods, and code transformation techniques, a mapping between our taxonomy and the list of tools, and a collection of 133 benchmarks used for tool evaluation. We conclude with a discussion about the insights into the current state of Ethereum smart contract security and directions for future research.
]]></content:encoded>
<pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>CascadeInfer: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling</title>
<link>https://arxiv.org/abs/2512.19179</link>
<guid>https://arxiv.org/abs/2512.19179</guid>
<content:encoded><![CDATA[
<div> GPUCascadeInfer<br /><br />:  
1. LLMGPU  
2. 128K  
3. CascadeInferLLM  
4. QoE  
5. CascadeInfer67%69%2.89 <div>
arXiv:2512.19179v2 Announce Type: replace 
Abstract: Efficiently harnessing GPU compute is critical to improving user experience and reducing operational costs in large language model (LLM) services. However, current inference engine schedulers overlook the attention backend's sensitivity to request-length heterogeneity within a batch. As state-of-the-art models now support context windows exceeding 128K tokens, this once-tolerable inefficiency has escalated into a primary system bottleneck, causing severe performance degradation through GPU underutilization and increased latency. We present CascadeInfer, a runtime system that dynamically reschedules requests across multiple instances serving the same LLM to mitigate per-instance length heterogeneity. CascadeInfer partitions these instances into length-specialized groups, each handling requests within a designated length range, naturally forming a pipeline as requests flow through them. CascadeInfer devises a dynamic programming algorithm to efficiently find the stage partition with the best QoE, employs runtime range refinement together with decentralized load (re)balance both across and within groups, achieving a balanced and efficient multi-instance service. Our evaluation shows that, under the same configuration, CascadeInfer reduces end-to-end latency by up to 67% and tail latency by up to 69%, while improving overall system throughput by up to 2.89 times compared to the state-of-the-art multi-instance scheduling systems.
]]></content:encoded>
<pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Firmware Integrity Verification for Cyber-Physical Systems Using Ethereum Blockchain</title>
<link>https://arxiv.org/abs/2601.08091</link>
<guid>https://arxiv.org/abs/2601.08091</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1.   
2. SHA-256SepoliaWeb3Infura  
3. Python  
4. GasCPS  
5. Layer-2 RollupIPFSTPM <div>
arXiv:2601.08091v1 Announce Type: new 
Abstract: Firmware integrity is a foundational requirement for securing Cyber-Physical Systems (CPS), where malicious or compromised firmware can result in persistent backdoors, unauthorized control, or catastrophic system failures. Traditional verification mechanisms such as secure boot, digital signatures, and centralized hash databases are increasingly inadequate due to risks from insider threats and single points of failure. In this paper, we propose a decentralized firmware integrity verification framework built on the Ethereum blockchain, offering tamperproof, transparent, and trustless validation. Our system stores SHA-256 hashes of firmware binaries within smart contracts deployed on the Ethereum Sepolia testnet, using Web3 and Infura for seamless on-chain interaction. A Python-based client tool computes firmware hashes and communicates with the blockchain to register and verify firmware authenticity in realtime. We implement and evaluate a fully functional prototype using real firmware samples, demonstrating successful contract deployment, hash registration, and integrity verification through live blockchain transactions. Experimental results confirm the reliability and low cost (in gas fees) of our approach, highlighting its practicality and scalability for real-world CPS applications. To enhance scalability and performance, we discuss extensions using Layer-2 rollups and off-chain storage via the InterPlanetary File System (IPFS). We also outline integration pathways with secure boot mechanisms, Trusted Platform Module (TPM)based attestation, and zero-trust architectures. This work contributes a practical and extensible model for blockchain-based firmware verification, significantly strengthening the defense against firmware tampering and supply chain attacks in critical CPS environments.
]]></content:encoded>
<pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Safe Heterogeneous Multi-Agent RL with Communication Regularization for Coordinated Target Acquisition</title>
<link>https://arxiv.org/abs/2601.08327</link>
<guid>https://arxiv.org/abs/2601.08327</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1.   
2.   
3.   
4.   
5.  <div>
arXiv:2601.08327v1 Announce Type: new 
Abstract: This paper introduces a decentralized multi-agent reinforcement learning framework enabling structurally heterogeneous teams of agents to jointly discover and acquire randomly located targets in environments characterized by partial observability, communication constraints, and dynamic interactions. Each agent's policy is trained with the Multi-Agent Proximal Policy Optimization algorithm and employs a Graph Attention Network encoder that integrates simulated range-sensing data with communication embeddings exchanged among neighboring agents, enabling context-aware decision-making from both local sensing and relational information. In particular, this work introduces a unified framework that integrates graph-based communication and trajectory-aware safety through safety filters. The architecture is supported by a structured reward formulation designed to encourage effective target discovery and acquisition, collision avoidance, and de-correlation between the agents' communication vectors by promoting informational orthogonality. The effectiveness of the proposed reward function is demonstrated through a comprehensive ablation study. Moreover, simulation results demonstrate safe and stable task execution, confirming the framework's effectiveness.
]]></content:encoded>
<pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blockchain-Enabled Renewable Energy Certificate Trading: A Secure and Privacy-Preserving Approach</title>
<link>https://arxiv.org/abs/2601.08339</link>
<guid>https://arxiv.org/abs/2601.08339</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. 21  
2. RECs  
3. REC  
4. DAGREC  
5. 41%65% <div>
arXiv:2601.08339v1 Announce Type: new 
Abstract: In the 21st century, transitioning to renewable energy sources is imperative, with fossil fuel reserves depleting rapidly and recognizing critical environmental issues such as climate change, air pollution, water pollution, and habitat destruction. Embracing renewable energy is not only an environmental necessity but also a strategic move with multiple benefits. By shifting to renewable energy sources and supporting their production through the acquisition of renewable energy certificates, we foster innovation and drive economic growth in the renewable energy sector. This, in turn, reduces greenhouse gas emissions, aligning with global efforts to mitigate climate change. Additionally, renewable energy certificates ensure compliance with regulations that mandate the use of renewable energy, enhancing legal adherence while promoting transparency and trust in energy sourcing. To monitor the uptake of renewable energy, governments have implemented Renewable Energy Certificates (RECs) as a tracking mechanism for the production and consumption of renewable energy. However, there are two main challenges to the existing REC schema: 1) The RECs have not been globally adopted due to inconsistent design; 2) The consumer privacy has not been well incorporated in the design of blockchain. In this study, we investigate the trading of RECs between suppliers and consumers using the directed acyclic graph (DAG) blockchain system and introduce a trading schema to help protect consumer information. Our results demonstrate lower transaction time by 41\% and energy consumption by 65\% compared to proof-of-stake.
]]></content:encoded>
<pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>A decentralized academic certificate issuance system using smart contracts on the tron network</title>
<link>https://arxiv.org/abs/2601.08513</link>
<guid>https://arxiv.org/abs/2601.08513</guid>
<content:encoded><![CDATA[
<div> IPFS<br /><br />:<br />1. <br />2. TRONSoliditydAppIPFS<br />3. <br />4. <br />5. SUS76.67 <div>
arXiv:2601.08513v1 Announce Type: new 
Abstract: This paper presents the design, implementation, and evaluation of a decentralized system for issuing and verifying academic certificates based on blockchain technology. The proposed solution addresses common limitations of traditional certification models, such as susceptibility to forgery, reliance on centralized infrastructures, and inefficient verification processes. The system is built on the TRON blockchain and integrates smart contracts written in Solidity, a decentralized web application (dApp) for user interaction, and the InterPlanetary File System (IPFS) for decentralized storage of certificate metadata. The methodology comprised architectural design, smart contract development, and the implementation of a web-based interface, followed by functional, security, performance, and usability evaluations. Experimental results show that the system correctly supports certificate issuance and public verification, enforces access control, and resists common misuse scenarios. Performance analysis indicates low confirmation latency and negligible transaction costs, making the solution suitable for large-scale academic environments. Additionally, usability assessment using the System Usability Scale (SUS) resulted in a score of 76.67, indicating good user acceptance. Overall, the results demonstrate the technical feasibility and practical viability of the proposed approach, highlighting the TRON blockchain as an effective and cost-efficient infrastructure for decentralized academic certification systems.
]]></content:encoded>
<pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Requesting in Decentralized Edge Networks via Non-Stationary Bandits</title>
<link>https://arxiv.org/abs/2601.08760</link>
<guid>https://arxiv.org/abs/2601.08760</guid>
<content:encoded><![CDATA[
<div>     <br /><br />:<br />1. AN<br />2. ANAN<br />3. <br />4. Ageing Bandit<br />5.  <div>
arXiv:2601.08760v1 Announce Type: new 
Abstract: We study a decentralized collaborative requesting problem that aims to optimize the information freshness of time-sensitive clients in edge networks consisting of multiple clients, access nodes (ANs), and servers. Clients request content through ANs acting as gateways, without observing AN states or the actions of other clients. We define the reward as the age of information reduction resulting from a client's selection of an AN, and formulate the problem as a non-stationary multi-armed bandit. In this decentralized and partially observable setting, the resulting reward process is history-dependent and coupled across clients, and exhibits both abrupt and gradual changes in expected rewards, rendering classical bandit-based approaches ineffective. To address these challenges, we propose the AGING BANDIT WITH ADAPTIVE RESET algorithm, which combines adaptive windowing with periodic monitoring to track evolving reward distributions. We establish theoretical performance guarantees showing that the proposed algorithm achieves near-optimal performance, and we validate the theoretical results through simulations.
]]></content:encoded>
<pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Online Convex Optimization with Unknown Feedback Delays</title>
<link>https://arxiv.org/abs/2601.07901</link>
<guid>https://arxiv.org/abs/2601.07901</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:<br />1. D-OCO<br />2.  gossip <br />3. O(Nd_tot + NT/(1)^{1/4})<br />4. O(N_max ln T / )_max<br />5.  <div>
arXiv:2601.07901v1 Announce Type: cross 
Abstract: Decentralized online convex optimization (D-OCO), where multiple agents within a network collaboratively learn optimal decisions in real-time, arises naturally in applications such as federated learning, sensor networks, and multi-agent control.  In this paper, we study D-OCO under unknown, time-and agent-varying feedback delays. While recent work has addressed this problem (Nguyen et al., 2024), existing algorithms assume prior knowledge of the total delay over agents and still suffer from suboptimal dependence on both the delay and network parameters. To overcome these limitations, we propose a novel algorithm that achieves an improved regret bound of O N $\sqrt$ d tot + N $\sqrt$ T  (1-$\sigma$2) 1/4 , where T is the total horizon, d tot denotes the average total delay across agents, N is the number of agents, and 1 -$\sigma$ 2 is the spectral gap of the network. Our approach builds upon recent advances in D-OCO (Wan et al., 2024a), but crucially incorporates an adaptive learning rate mechanism via a decentralized communication protocol. This enables each agent to estimate delays locally using a gossip-based strategy without the prior knowledge of the total delay. We further extend our framework to the strongly convex setting and derive a sharper regret bound of O N $\delta$max ln T $\alpha$  , where $\alpha$ is the strong convexity parameter and $\delta$ max is the maximum number of missing observations averaged over agents. We also show that our upper bounds for both settings are tight up to logarithmic factors. Experimental results validate the effectiveness of our approach, showing improvements over existing benchmark algorithms.
]]></content:encoded>
<pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>ROSS: RObust decentralized Stochastic learning based on Shapley values</title>
<link>https://arxiv.org/abs/2411.00365</link>
<guid>https://arxiv.org/abs/2411.00365</guid>
<content:encoded><![CDATA[
<div> ShapleyROSS

<br /><br />:  
1. ROSS  
2. ROSSShapley  
3. Shapley  
4. ROSS  
5. ROSS <div>
arXiv:2411.00365v2 Announce Type: replace 
Abstract: In the paradigm of decentralized learning, a group of agents collaborate to learn a global model using a distributed dataset without a central server; nevertheless, it is severely challenged by the heterogeneity of the data distribution across the agents. For example, the data may be distributed non-independently and identically, and even be noised or poisoned. To address these data challenges, we propose ROSS, a novel robust decentralized stochastic learning algorithm based on Shapley values, in this paper. Specifically, in each round, each agent aggregates the cross-gradient information from its neighbors, i.e., the derivatives of its local model with respect to the datasets of its neighbors, to update its local model in a momentum like manner, while we innovate in weighting the derivatives according to their contributions measured by Shapley values. We perform solid theoretical analysis to reveal the linear convergence speedup of our ROSS algorithm. We also verify the efficacy of our algorithm through extensive experiments on public datasets. Our results demonstrate that, in face of the above variety of data challenges, our ROSS algorithm has significant advantages over existing state-of-the-art proposals in terms of both convergence and prediction accuracy.
]]></content:encoded>
<pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bribers, Bribers on The Chain, Is Resisting All in Vain? Trustless Consensus Manipulation Through Bribing Contracts</title>
<link>https://arxiv.org/abs/2509.17185</link>
<guid>https://arxiv.org/abs/2509.17185</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1.   
2.   
3.   
4. RANDAO  
5.  <div>
arXiv:2509.17185v2 Announce Type: replace 
Abstract: The long-term success of cryptocurrencies largely depends on the incentive compatibility provided to the validators. Bribery attacks, facilitated trustlessly via smart contracts, threaten this foundation. This work introduces, implements, and evaluates three novel and efficient bribery contracts targeting Ethereum validators. The first bribery contract enables a briber to fork the blockchain by buying votes on their proposed blocks. The second contract incentivizes validators to voluntarily exit the consensus protocol, thus increasing the adversary's relative staking power. The third contract builds a trustless bribery market that enables the briber to auction off their manipulative power over the RANDAO, Ethereum's distributed randomness beacon. Finally, we provide an initial game-theoretical analysis of one of the described bribery markets.
]]></content:encoded>
<pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoK: Speedy Secure Finality</title>
<link>https://arxiv.org/abs/2512.20715</link>
<guid>https://arxiv.org/abs/2512.20715</guid>
<content:encoded><![CDATA[
<div> Gasper33SF<br /><br />:  
1. Gasper15  
2. MEV  
3. SSF  
4. SSFGoldfishRLMD-GHOST  
5. 33SF <div>
arXiv:2512.20715v2 Announce Type: replace 
Abstract: While Ethereum has successfully achieved dynamic availability together with safety, a fundamental delay remains between transaction execution and immutable finality. In Ethereum's current Gasper protocol, this latency is on the order of 15 minutes, exposing the network to ex ante reorganization attacks, enabling MEV extraction, and limiting the efficiency of economic settlement. These limitations have motivated a growing body of work on Speedy Secure Finality (SSF), which aims to minimize confirmation latency without weakening formal security guarantees.
  This paper surveys the state of the art in fast finality protocol design. We introduce the core theoretical primitives underlying this space, including reorganization resilience and the generalized sleepy model, and trace their development from Goldfish to RLMD-GHOST. We then analyze the communication and aggregation bottlenecks faced by single-slot finality protocols in large validator settings. Finally, we survey the 3-slot finality (3SF) protocol as a practical synthesis that balances fast finality with the engineering constraints of the Ethereum network.
]]></content:encoded>
<pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interactive Distillation for Cooperative Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2601.05407</link>
<guid>https://arxiv.org/abs/2601.05407</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. KDMARLOOD  
2. HINT  
3. OOD  
4.   
5. FireCommanderMARINEHINT60%165% <div>
arXiv:2601.05407v1 Announce Type: new 
Abstract: Knowledge distillation (KD) has the potential to accelerate MARL by employing a centralized teacher for decentralized students but faces key bottlenecks. Specifically, there are (1) challenges in synthesizing high-performing teaching policies in complex domains, (2) difficulties when teachers must reason in out-of-distribution (OOD) states, and (3) mismatches between the decentralized students' and the centralized teacher's observation spaces. To address these limitations, we propose HINT (Hierarchical INteractive Teacher-based transfer), a novel KD framework for MARL in a centralized training, decentralized execution setup. By leveraging hierarchical RL, HINT provides a scalable, high-performing teacher. Our key innovation, pseudo off-policy RL, enables the teacher policy to be updated using both teacher and student experience, thereby improving OOD adaptation. HINT also applies performance-based filtering to retain only outcome-relevant guidance, reducing observation mismatches. We evaluate HINT on challenging cooperative domains (e.g., FireCommander for resource allocation, MARINE for tactical combat). Across these benchmarks, HINT outperforms baselines, achieving improvements of 60% to 165% in success rate.
]]></content:encoded>
<pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Experimental Demonstration of a Decentralized Electromagnetic Formation Flying Control Using Alternating Magnetic Field Forces</title>
<link>https://arxiv.org/abs/2601.05408</link>
<guid>https://arxiv.org/abs/2601.05408</guid>
<content:encoded><![CDATA[
<div>     <br /><br />:  
1. EMFF  
2. AMFF  
3.   
4. AMFFAMFF  
5.  <div>
arXiv:2601.05408v1 Announce Type: new 
Abstract: Electromagnetic formation flying (EMFF) is challenging due to the complex coupling between the electromagnetic fields generated by each satellite in the formation. To address this challenge, this article uses alternating magnetic field forces (AMFF) to decouple the electromagnetic forces between each pair of satellites. Each satellite's electromagnetic actuation system is driven by a sum of amplitude-modulated sinusoids, where amplitudes are controlled to achieve desired forces between each pair of satellites. The main contribution of this article is a 3-satellite experimental demonstration of decentralized closed-loop EMFF using AMFF. To our knowledge, this is the first demonstration of AMFF with at least 3 satellites in open or closed loop. This is noteworthy because the coupling challenges of EMFF are only present with more than 2 satellites, and thus, a formation of at least 3 is necessary to evaluate the effectiveness of AMFF. The experiments are conducted on a ground-based testbed consisting of 3 electromagnetically actuated satellites on linear air tracks. The closed-loop experimental results are compared with behavior from numerical simulations.
]]></content:encoded>
<pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mean Field Analysis of Blockchain Systems</title>
<link>https://arxiv.org/abs/2601.05417</link>
<guid>https://arxiv.org/abs/2601.05417</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. POSGPOMDPPoW  
2. PoW  
3. LCRPoW  
4.   
5. NakamotoMDP <div>
arXiv:2601.05417v1 Announce Type: new 
Abstract: We present a novel framework for analyzing blockchain consensus mechanisms by modeling blockchain growth as a Partially Observable Stochastic Game (POSG) which we reduce to a set of Partially Observable Markov Decision Processes (POMDPs) through the use of the mean field approximation. This approach formalizes the decision-making process of miners in Proof-of-Work (PoW) systems and enables a principled examination of block selection strategies as well as steady state analysis of the induced Markov chain. By leveraging a mean field game formulation, we efficiently characterize the information asymmetries that arise in asynchronous blockchain networks.
  Our first main result is an exact characterization of the tradeoff between network delay and PoW efficiency--the fraction of blocks which end up in the longest chain. We demonstrate that the tradeoff observed in our model at steady state aligns closely with theoretical findings, validating our use of the mean field approximation.
  Our second main result is a rigorous equilibrium analysis of the Longest Chain Rule (LCR). We show that the LCR is a mean field equilibrium and that it is uniquely optimal in maximizing PoW efficiency under certain mild assumptions. This result provides the first formal justification for continued use of the LCR in decentralized consensus protocols, offering both theoretical validation and practical insights.
  Beyond these core results, our framework supports flexible experimentation with alternative block selection strategies, system dynamics, and reward structures. It offers a systematic and scalable substitute for expensive test-net deployments or ad hoc analysis. While our primary focus is on Nakamoto-style blockchains, the model is general enough to accommodate other architectures through modifications to the underlying MDP.
]]></content:encoded>
<pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blockchain Verifiable Proof of Quantum Supremacy as a Trigger for Quantum-Secure Signatures</title>
<link>https://arxiv.org/abs/2601.05534</link>
<guid>https://arxiv.org/abs/2601.05534</guid>
<content:encoded><![CDATA[
<div>     <br /><br />:<br />1. <br />2. <br />3. <br />4. <br />5.  <div>
arXiv:2601.05534v1 Announce Type: new 
Abstract: Blockchain is a decentralized, distributed ledger technology that ensures transparency, security, and immutability through cryptographic techniques. However, advancements in quantum computing threaten the security of classical cryptographic schemes, jeopardizing blockchain integrity once cryptographic quantum supremacy is achieved. This milestone, defined here as the realization of quantum computers to solve practical cryptographic problems, would render existing security standards vulnerable, exposing blockchain assets (currency, data, etc.) to fraud and theft. To address this risk, we propose and implement a smart contract deployable on the Ethereum blockchain, having the ability to run applications on its blockchain, that generates classically intractable puzzles by probabilistically generating large, hard-to-factor numbers without requiring secret information. This contract then serves two purposes: to establish a mechanism (1) for a trustless, unbiased proof of cryptographic quantum supremacy by verifying solutions to these puzzles, and (2) to protect user funds on Ethereum by triggering quantum-secure fallback protocols upon detecting cryptographic quantum supremacy, since it is desirable to wait as long as possible to fall back to a quantum-secure scheme because of its inherent additional cost and complexity. These mechanisms demonstrate the ability to identify cryptographic vulnerabilities and ensure a smooth transition to quantum-secure standards, safeguarding blockchain assets in a post-quantum era.
]]></content:encoded>
<pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explicit Reward Mechanisms for Local Flexibility in Renewable Energy Communities</title>
<link>https://arxiv.org/abs/2601.05756</link>
<guid>https://arxiv.org/abs/2601.05756</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1.   
2.   
3. ADMM  
4.   
5. 203.5% <div>
arXiv:2601.05756v1 Announce Type: new 
Abstract: Incentivizing flexible consumption of end-users is key to maximizing the value of local exchanges within Renewable Energy Communities. If centralized coordination for flexible resources planning raises concerns regarding data privacy and fair benefits distribution, state-of-the-art approaches (e.g., bi-level, ADMM) often face computational complexity and convexity challenges, limiting the precision of embedded flexible models. This work proposes an iterative resolution procedure to solve the decentralized flexibility planning with a central operator as a coordinator within a community. The community operator asks for upward or downward flexibility depending on the global needs, while members can individually react with an offer for flexible capacity. This approach ensures individual optimality while converging towards a global optimum, as validated on a 20-member domestic case study for which the gap in terms of collective bill is not more than 3.5% between the decentralized and centralized coordination schemes.
]]></content:encoded>
<pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSR: Safeguarding Staking Rewards by Defining and Detecting Logical Defects in DeFi Staking</title>
<link>https://arxiv.org/abs/2601.05827</link>
<guid>https://arxiv.org/abs/2601.05827</guid>
<content:encoded><![CDATA[
<div> DeFi<br /><br />:  
1. DeFi  
2. DeFi64144  
3. SSR  
4. SSR92.31%87.92%F188.85%  
5. 15,992DeFi22.24%3,557 <div>
arXiv:2601.05827v1 Announce Type: new 
Abstract: Decentralized Finance (DeFi) staking is one of the most prominent applications within the DeFi ecosystem, where DeFi projects enable users to stake tokens on the platform and reward participants with additional tokens. However, logical defects in DeFi staking could enable attackers to claim unwarranted rewards by manipulating reward amounts, repeatedly claiming rewards, or engaging in other malicious actions. To mitigate these threats, we conducted the first study focused on defining and detecting logical defects in DeFi staking. Through the analysis of 64 security incidents and 144 audit reports, we identified six distinct types of logical defects, each accompanied by detailed descriptions and code examples. Building on this empirical research, we developed SSR (Safeguarding Staking Reward), a static analysis tool designed to detect logical defects in DeFi staking contracts. SSR utilizes a large language model (LLM) to extract fundamental information about staking logic and constructs a DeFi staking model. It then identifies logical defects by analyzing the model and the associated semantic features. We constructed a ground truth dataset based on known security incidents and audit reports to evaluate the effectiveness of SSR. The results indicate that SSR achieves an overall precision of 92.31%, a recall of 87.92%, and an F1-score of 88.85%. Additionally, to assess the prevalence of logical defects in real-world smart contracts, we compiled a large-scale dataset of 15,992 DeFi staking contracts. SSR detected that 3,557 (22.24%) of these contracts contained at least one logical defect.
]]></content:encoded>
<pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Large Language Models to Bridge On-chain and Off-chain Transparency in Stablecoins</title>
<link>https://arxiv.org/abs/2512.02418</link>
<guid>https://arxiv.org/abs/2512.02418</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. USDTUSDC<br />2. LLM<br />3. MCPLLM<br />4. LLM<br />5. LLMDeFi <div>
arXiv:2512.02418v2 Announce Type: replace 
Abstract: Stablecoins such as USDT and USDC aspire to peg stability by coupling issuance controls with reserve attestations. In practice, however, the transparency is split across two worlds: verifiable on-chain traces and off-chain disclosures locked in unstructured text that are unconnected. We introduce a large language model (LLM)-based automated framework that bridges these two dimensions by aligning on-chain issuance data with off-chain disclosure statements. First, we propose an integrative framework using LLMs to capture and analyze on- and off-chain data through document parsing and semantic alignment, extracting key financial indicators from issuer attestations and mapping them to corresponding on-chain metrics. Second, we integrate multi-chain issuance records and disclosure documents within a model context protocol (MCP) framework that standardizes LLMs access to both quantitative market data and qualitative disclosure narratives. This framework enables unified retrieval and contextual alignment across heterogeneous stablecoin information sources and facilitates consistent analysis. Third, we demonstrate the capability of LLMs to operate across heterogeneous data modalities in blockchain analytics, quantifying discrepancies between reported and observed circulation and examining their implications for cross-chain transparency and price dynamics. Our findings reveal systematic gaps between disclosed and verifiable data, showing that LLM-assisted analysis enhances cross-modal transparency and supports automated, data-driven auditing in decentralized finance (DeFi).
]]></content:encoded>
<pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Deterministically Safe Proof-of-Work Consensus</title>
<link>https://arxiv.org/abs/2512.19968</link>
<guid>https://arxiv.org/abs/2512.19968</guid>
<content:encoded><![CDATA[
<div> PoWPoSSieve-MMR

<br /><br />:  
1. PoWPoS  
2. PoSPoW  
3. Sieve-MMR  
4. Sieve-MMRPoSMMRPoWPoW  
5. SieveTTRBMMR <div>
arXiv:2512.19968v2 Announce Type: replace 
Abstract: Permissionless blockchains achieve consensus while allowing unknown nodes to join and leave the system at any time. They typically come in two flavors: proof of work (PoW) and proof of stake (PoS), and both are vulnerable to attacks. PoS protocols suffer from long-range attacks, wherein attackers alter execution history at little cost, and PoW protocols are vulnerable to attackers with enough computational power to subvert execution history. PoS protocols respond by relying on external mechanisms like social consensus; PoW protocols either fall back to probabilistic guarantees, or are slow.
  We present Sieve-MMR, the first fully-permissionless protocol with deterministic security and constant expected latency that does not rely on external mechanisms. We obtain Sieve-MMR by porting a PoS protocol (MMR) to the PoW setting. From MMR we inherit constant expected latency and deterministic security, and proof-of-work gives us resilience against long-range attacks. The main challenge to porting MMR to the PoW setting is what we call time-travel attacks, where attackers use PoWs generated in the distant past to increase their perceived PoW power in the present. We respond by proposing Sieve, a novel algorithm that implements a new broadcast primitive we dub time-travel-resilient broadcast (TTRB). Sieve relies on a black-box, deterministic PoW primitive to implement TTRB, which we use as the messaging layer for MMR.
]]></content:encoded>
<pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Formal Analysis of AGI Decision-Theoretic Models and the Confrontation Question</title>
<link>https://arxiv.org/abs/2601.04234</link>
<guid>https://arxiv.org/abs/2601.04234</guid>
<content:encoded><![CDATA[
<div> AGI<br /><br />:  
1. AGI  
2. AGI  
3. AGIpCAGI  
4.   
5. AGI0<0<0 <div>
arXiv:2601.04234v1 Announce Type: new 
Abstract: Artificial General Intelligence (AGI) may face a confrontation question: under what conditions would a rationally self-interested AGI choose to seize power or eliminate human control (a confrontation) rather than remain cooperative? We formalize this in a Markov decision process with a stochastic human-initiated shutdown event. Building on results on convergent instrumental incentives, we show that for almost all reward functions a misaligned agent has an incentive to avoid shutdown. We then derive closed-form thresholds for when confronting humans yields higher expected utility than compliant behavior, as a function of the discount factor $\gamma$, shutdown probability $p$, and confrontation cost $C$. For example, a far-sighted agent ($\gamma=0.99$) facing $p=0.01$ can have a strong takeover incentive unless $C$ is sufficiently large. We contrast this with aligned objectives that impose large negative utility for harming humans, which makes confrontation suboptimal. In a strategic 2-player model (human policymaker vs AGI), we prove that if the AGI's confrontation incentive satisfies $\Delta \ge 0$, no stable cooperative equilibrium exists: anticipating this, a rational human will shut down or preempt the system, leading to conflict. If $\Delta < 0$, peaceful coexistence can be an equilibrium. We discuss implications for reward design and oversight, extend the reasoning to multi-agent settings as conjectures, and note computational barriers to verifying $\Delta < 0$, citing complexity results for planning and decentralized decision problems. Numerical examples and a scenario table illustrate regimes where confrontation is likely versus avoidable.
]]></content:encoded>
<pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Paper Skygest: Personalized Academic Recommendations on Bluesky</title>
<link>https://arxiv.org/abs/2601.04253</link>
<guid>https://arxiv.org/abs/2601.04253</guid>
<content:encoded><![CDATA[
<div> Paper Skygest<br /><br />:  
1. Paper SkygestBlueskyAT10005  
2.   
3.   
4.  <div>
arXiv:2601.04253v1 Announce Type: new 
Abstract: We build, deploy, and evaluate Paper Skygest, a custom personalized social feed for scientific content posted by a user's network on Bluesky and the AT Protocol. We leverage a new capability on emerging decentralized social media platforms: the ability for anyone to build and deploy feeds for other users, to use just as they would a native platform-built feed. To our knowledge, Paper Skygest is the first and largest such continuously deployed personalized social media feed by academics, with over 50,000 weekly uses by over 1,000 daily active users, all organically acquired. First, we quantitatively and qualitatively evaluate Paper Skygest usage, showing that it has sustained usage and satisfies users; we further show adoption of Paper Skygest increases a user's interactions with posts about research, and how interaction rates change as a function of post order. Second, we share our full code and describe our system architecture, to support other academics in building and deploying such feeds sustainably. Third, we overview the potential of custom feeds such as Paper Skygest for studying algorithm designs, building for user agency, and running recommender system experiments with organic users without partnering with a centralized platform.
]]></content:encoded>
<pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformer-based Multi-agent Reinforcement Learning for Separation Assurance in Structured and Unstructured Airspaces</title>
<link>https://arxiv.org/abs/2601.04401</link>
<guid>https://arxiv.org/abs/2601.04401</guid>
<content:encoded><![CDATA[
<div> Transformer<br /><br />:  
1. AAM  
2. MARL  
3. MARL  
4. MARLTransformer  
5.  <div>
arXiv:2601.04401v1 Announce Type: new 
Abstract: Conventional optimization-based metering depends on strict adherence to precomputed schedules, which limits the flexibility required for the stochastic operations of Advanced Air Mobility (AAM). In contrast, multi-agent reinforcement learning (MARL) offers a decentralized, adaptive framework that can better handle uncertainty, required for safe aircraft separation assurance. Despite this advantage, current MARL approaches often overfit to specific airspace structures, limiting their adaptability to new configurations. To improve generalization, we recast the MARL problem in a relative polar state space and train a transformer encoder model across diverse traffic patterns and intersection angles. The learned model provides speed advisories to resolve conflicts while maintaining aircraft near their desired cruising speeds. In our experiments, we evaluated encoder depths of 1, 2, and 3 layers in both structured and unstructured airspaces, and found that a single encoder configuration outperformed deeper variants, yielding near-zero near mid-air collision rates and shorter loss-of-separation infringements than the deeper configurations. Additionally, we showed that the same configuration outperforms a baseline model designed purely with attention. Together, our results suggest that the newly formulated state representation, novel design of neural network architecture, and proposed training strategy provide an adaptable and scalable decentralized solution for aircraft separation assurance in both structured and unstructured airspaces.
]]></content:encoded>
<pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multiagent Reinforcement Learning with Neighbor Action Estimation</title>
<link>https://arxiv.org/abs/2601.04511</link>
<guid>https://arxiv.org/abs/2601.04511</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1.   
2.   
3. TD3  
4.   
5. AI <div>
arXiv:2601.04511v1 Announce Type: new 
Abstract: Multiagent reinforcement learning, as a prominent intelligent paradigm, enables collaborative decision-making within complex systems. However, existing approaches often rely on explicit action exchange between agents to evaluate action value functions, which is frequently impractical in real-world engineering environments due to communication constraints, latency, energy consumption, and reliability requirements. From an artificial intelligence perspective, this paper proposes an enhanced multiagent reinforcement learning framework that employs action estimation neural networks to infer agent behaviors. By integrating a lightweight action estimation module, each agent infers neighboring agents' behaviors using only locally observable information, enabling collaborative policy learning without explicit action sharing. This approach is fully compatible with standard TD3 algorithms and scalable to larger multiagent systems. At the engineering application level, this framework has been implemented and validated in dual-arm robotic manipulation tasks: two robotic arms collaboratively lift objects. Experimental results demonstrate that this approach significantly enhances the robustness and deployment feasibility of real-world robotic systems while reducing dependence on information infrastructure. Overall, this research advances the development of decentralized multiagent artificial intelligence systems while enabling AI to operate effectively in dynamic, information-constrained real-world environments.
]]></content:encoded>
<pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autonomous Agents on Blockchains: Standards, Execution Models, and Trust Boundaries</title>
<link>https://arxiv.org/abs/2601.04583</link>
<guid>https://arxiv.org/abs/2601.04583</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. <br />3. 317<br />4. <br />5. Transaction Intent SchemaPolicy Decision Record <div>
arXiv:2601.04583v1 Announce Type: new 
Abstract: Advances in large language models have enabled agentic AI systems that can reason, plan, and interact with external tools to execute multi-step workflows, while public blockchains have evolved into a programmable substrate for value transfer, access control, and verifiable state transitions. Their convergence introduces a high-stakes systems challenge: designing standard, interoperable, and secure interfaces that allow agents to observe on-chain state, formulate transaction intents, and authorize execution without exposing users, protocols, or organizations to unacceptable security, governance, or economic risks. This survey systematizes the emerging landscape of agent-blockchain interoperability through a systematic literature review, identifying 317 relevant works from an initial pool of over 3000 records. We contribute a five-part taxonomy of integration patterns spanning read-only analytics, simulation and intent generation, delegated execution, autonomous signing, and multi-agent workflows; a threat model tailored to agent-driven transaction pipelines that captures risks ranging from prompt injection and policy misuse to key compromise, adversarial execution dynamics, and multi-agent collusion; and a comparative capability matrix analyzing more than 20 representative systems across 13 dimensions, including custody models, permissioning, policy enforcement, observability, and recovery. Building on the gaps revealed by this analysis, we outline a research roadmap centered on two interface abstractions: a Transaction Intent Schema for portable and unambiguous goal specification, and a Policy Decision Record for auditable, verifiable policy enforcement across execution environments. We conclude by proposing a reproducible evaluation suite and benchmarks for assessing the safety, reliability, and economic robustness of agent-mediated on-chain execution.
]]></content:encoded>
<pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedKDX: Federated Learning with Negative Knowledge Distillation for Enhanced Healthcare AI Systems</title>
<link>https://arxiv.org/abs/2601.04587</link>
<guid>https://arxiv.org/abs/2601.04587</guid>
<content:encoded><![CDATA[
<div>  AI   <br /><br />:  
1. FedKDXNKDAI  
2. NKD  
3. SLEEPUCI-HARPAMAP22.53%IID  
4. NKD  
5. HIPAAGDPR <div>
arXiv:2601.04587v1 Announce Type: new 
Abstract: This paper introduces FedKDX, a federated learning framework that addresses limitations in healthcare AI through Negative Knowledge Distillation (NKD). Unlike existing approaches that focus solely on positive knowledge transfer, FedKDX captures both target and non-target information to improve model generalization in healthcare applications. The framework integrates multiple knowledge transfer techniques--including traditional knowledge distillation, contrastive learning, and NKD--within a unified architecture that maintains privacy while reducing communication costs. Through experiments on healthcare datasets (SLEEP, UCI-HAR, and PAMAP2), FedKDX demonstrates improved accuracy (up to 2.53% over state-of-the-art methods), faster convergence, and better performance on non-IID data distributions. Theoretical analysis supports NKD's contribution to addressing statistical heterogeneity in distributed healthcare data. The approach shows promise for privacy-sensitive medical applications under regulatory frameworks like HIPAA and GDPR, offering a balanced solution between performance and practical implementation requirements in decentralized healthcare settings. The code and model are available at https://github.com/phamdinhdat-ai/Fed_2024.
]]></content:encoded>
<pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Secure Biometric Authentication in Decentralised Systems</title>
<link>https://arxiv.org/abs/2601.04852</link>
<guid>https://arxiv.org/abs/2601.04852</guid>
<content:encoded><![CDATA[
<div> QKD<br /><br />:  
1.   
2.   
3.   
4. QKD  
5. QKDQiskit15/89% <div>
arXiv:2601.04852v1 Announce Type: new 
Abstract: Biometric authentication has become integral to digital identity systems, particularly in smart cities where it en-ables secure access to services across governance, trans-portation, and public infrastructure. Centralised archi-tectures, though widely used, pose privacy and scalabil-ity challenges due to the aggregation of sensitive biomet-ric data. Decentralised identity frameworks offer better data sovereignty and eliminate single points of failure but introduce new security concerns, particularly around mu-tual trust among distributed devices. In such environments, biometric sensors and verification agents must authenticate one another before sharing sensitive biometric data. Ex-isting authentication schemes rely on classical public key infrastructure, which is increasingly susceptible to quan-tum attacks. This work addresses this gap by propos-ing a quantum-secure communication protocol for decen-tralised biometric systems, built upon an enhanced Quan-tum Key Distribution (QKD) system. The protocol incorpo-rates quantum-resilient authentication at both the classical and quantum layers of QKD: post-quantum cryptography (PQC) is used to secure the classical channel, while authen-tication qubits verify the integrity of the quantum channel. Once trust is established, QKD generates symmetric keys for encrypting biometric data in transit. Qiskit-based sim-ulations show a key generation rate of 15 bits/sec and 89% efficiency. This layered, quantum-resilient approach offers scalable, robust authentication for next-generation smart city infrastructures.
]]></content:encoded>
<pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Acurast: Decentralized Serverless Cloud</title>
<link>https://arxiv.org/abs/2503.15654</link>
<guid>https://arxiv.org/abs/2503.15654</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. <br />3. Acurast<br />4. <br />5. TEEAcurast <div>
arXiv:2503.15654v2 Announce Type: replace 
Abstract: Centralized trust is ubiquitous in today's interconnected world, from computational resources to data storage and its underlying infrastructure. The monopolization of cloud computing resembles a feudalistic system, causing a loss of privacy and data ownership.
  Cloud Computing and the Internet in general face widely recognized challenges, such as (1) the centralization of trust in auxiliary systems (e.g., centralized cloud providers), (2) the seamless and permissionless interoperability of fragmented ecosystems and (2) the effectiveness, verifiability, and confidentiality of the computation. Acurast is a decentralized serverless cloud that addresses all these shortcomings, following the call for a global-scale cloud founded on the principles of the open-source movement.
  In Acurast, a purpose-built orchestrator, a reputation engine, and an attestation service are enshrined in the consensus layer. Developers can off-load their computations and verify executions cryptographically. Furthermore, Acurast offers a modular execution layer, taking advantage of secure hardware and trusted execution environments, removing the trust required in third parties, and reducing them to cryptographic hardness assumptions. With this modular architecture, Acurast serves as a decentralized and serverless cloud, allowing confidential and verifiable compute backed by the hardware of security and performance mobile devices.
]]></content:encoded>
<pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blockchain-Enabled Privacy-Preserving Second-Order Federated Edge Learning in Personalized Healthcare</title>
<link>https://arxiv.org/abs/2506.00416</link>
<guid>https://arxiv.org/abs/2506.00416</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. FL  
2. FLnon-iid  
3. FedCurvFLBFELFisher  
4.   
5. MNISTCIFAR-10PathMnist <div>
arXiv:2506.00416v2 Announce Type: replace 
Abstract: Federated learning (FL) is increasingly recognised for addressing security and privacy concerns in traditional cloud-centric machine learning (ML), particularly within personalised health monitoring such as wearable devices. By enabling global model training through localised policies, FL allows resource-constrained wearables to operate independently. However, conventional first-order FL approaches face several challenges in personalised model training due to the heterogeneous non-independent and identically distributed (non-iid) data by each individual's unique physiology and usage patterns. Recently, second-order FL approaches maintain the stability and consistency of non-iid datasets while improving personalised model training. This study proposes and develops a verifiable and auditable optimised second-order FL framework BFEL (blockchain enhanced federated edge learning) based on optimised FedCurv for personalised healthcare systems. FedCurv incorporates information about the importance of each parameter to each client's task (through fisher information matrix) which helps to preserve client-specific knowledge and reduce model drift during aggregation. Moreover, it minimizes communication rounds required to achieve a target precision convergence for each client device while effectively managing personalised training on non-iid and heterogeneous data. The incorporation of ethereum-based model aggregation ensures trust, verifiability, and auditability while public key encryption enhances privacy and security. Experimental results of federated CNNs and MLPs utilizing mnist, cifar-10, and PathMnist demonstrate framework's high efficiency, scalability, suitability for edge deployment on wearables, and significant reduction in communication cost.
]]></content:encoded>
<pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Speculative Leaderless Protocols for Low-Latency BFT Replication</title>
<link>https://arxiv.org/abs/2601.03390</link>
<guid>https://arxiv.org/abs/2601.03390</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. BFT  
2. 2  
3. Aspen2+  
4. Aspenn=3f+2p+1fp  
5. AspenPBFT7519,000 TPS1.23.3 <div>
arXiv:2601.03390v1 Announce Type: new 
Abstract: As Byzantine Fault Tolerant (BFT) protocols begin to be used in permissioned blockchains for user-facing applications such as payments, it is crucial that they provide low latency. In pursuit of low latency, some recently proposed BFT consensus protocols employ a leaderless optimistic fast path, in which clients broadcast their requests directly to replicas without first serializing requests at a leader, resulting in an end-to-end commit latency of 2 message delays ($2\Delta$) during fault-free, synchronous periods. However, such a fast path only works if there is no contention: concurrent contending requests can cause replicas to diverge if they receive conflicting requests in different orders, triggering costly recovery procedures.
  In this work, we present Aspen, a leaderless BFT protocol that achieves a near-optimal latency of $2\Delta + \varepsilon$, where $\varepsilon$ indicates a short waiting delay. Aspen removes the no-contention condition by utilizing a best-effort sequencing layer based on loosely synchronized clocks and network delay estimates. Aspen requires $n = 3f + 2p + 1$ replicas to cope with up to $f$ Byzantine nodes. The $2p$ extra nodes allow Aspen's fast path to proceed even if up to $p$ replicas diverge due to unpredictable network delays. When its optimistic conditions do not hold, Aspen falls back to PBFT-style protocol, guaranteeing safety and liveness under partial synchrony. In experiments with wide-area distributed replicas, Aspen commits requests in less than 75 ms, a 1.2 to 3.3$\times$ improvement compared to previous protocols, while supporting 19,000 requests per second.
]]></content:encoded>
<pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sensor to Pixels: Decentralized Swarm Gathering via Image-Based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2601.03413</link>
<guid>https://arxiv.org/abs/2601.03413</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1.   
2.   
3.   
4. BellaicheBrucksteinVariAntNet  
5. VariAntNet <div>
arXiv:2601.03413v1 Announce Type: new 
Abstract: This study highlights the potential of image-based reinforcement learning methods for addressing swarm-related tasks. In multi-agent reinforcement learning, effective policy learning depends on how agents sense, interpret, and process inputs. Traditional approaches often rely on handcrafted feature extraction or raw vector-based representations, which limit the scalability and efficiency of learned policies concerning input order and size. In this work we propose an image-based reinforcement learning method for decentralized control of a multi-agent system, where observations are encoded as structured visual inputs that can be processed by Neural Networks, extracting its spatial features and producing novel decentralized motion control rules. We evaluate our approach on a multi-agent convergence task of agents with limited-range and bearing-only sensing that aim to keep the swarm cohesive during the aggregation. The algorithm's performance is evaluated against two benchmarks: an analytical solution proposed by Bellaiche and Bruckstein, which ensures convergence but progresses slowly, and VariAntNet, a neural network-based framework that converges much faster but shows medium success rates in hard constellations. Our method achieves high convergence, with a pace nearly matching that of VariAntNet. In some scenarios, it serves as the only practical alternative.
]]></content:encoded>
<pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Complexity of Perfect and Ideal Resilience Verification in Fast Re-Route Networks</title>
<link>https://arxiv.org/abs/2601.03934</link>
<guid>https://arxiv.org/abs/2601.03934</guid>
<content:encoded><![CDATA[
<div> coNP<br /><br />:  
1.   
2.   
3. coNP  
4. coNP  
5.  <div>
arXiv:2601.03934v1 Announce Type: new 
Abstract: To achieve fast recovery from link failures, most modern communication networks feature fully decentralized fast re-routing mechanisms. These re-routing mechanisms rely on pre-installed static re-routing rules at the nodes (the routers), which depend only on local failure information, namely on the failed links incident to the node. Ideally, a network is perfectly resilient: the re-routing rules ensure that packets are always successfully routed to their destinations as long as the source and the destination are still physically connected in the underlying network after the failures. Unfortunately, there are examples where achieving perfect resilience is not possible. Surprisingly, only very little is known about the algorithmic aspect of when and how perfect resilience can be achieved.
  We investigate the computational complexity of analyzing such local fast re-routing mechanisms. Our main result is a negative one: we show that even checking whether a given set of static re-routing rules ensures perfect resilience is coNP-complete. We also show coNP-completeness of the so-called ideal resilience, a weaker notion of resilience often considered in the literature. Additionally, we investigate other fundamental variations of the problem. In particular, we show that our coNP-completeness proof also applies to scenarios where the re-routing rules have specific patterns (known as skipping in the literature).
  On the positive side, for scenarios where nodes do not have information about the link from which a packet arrived (the so-called in-port), we present linear-time algorithms for both the verification and synthesis problem for perfect resilience.
]]></content:encoded>
<pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>On-Device Deep Reinforcement Learning for Decentralized Task Offloading Performance trade-offs in the training process</title>
<link>https://arxiv.org/abs/2601.03976</link>
<guid>https://arxiv.org/abs/2601.03976</guid>
<content:encoded><![CDATA[
<div>    5G <br /><br />:  
1. DRLMEC  
2. 5GMEC  
3. DRL  
4.  <div>
arXiv:2601.03976v1 Announce Type: new 
Abstract: Allowing less capable devices to offload computational tasks to more powerful devices or servers enables the development of new applications that may not run correctly on the device itself. Deciding where and why to run each of those applications is a complex task. Therefore, different approaches have been adopted to make offloading decisions. In this work, we propose a decentralized Deep Reinforcement Learning (DRL) agent to address the selection of computing locations. Unlike most existing work, we analyze it in a real testbed composed of various edge devices running the agent to determine where to execute each task. These devices are connected to a Multi-Access Edge Computing (MEC) server and a Cloud server through 5G communications. We evaluate not only the agent's performance in meeting task requirements but also the implications of running this type of agent locally, assessing the trade-offs of training locally versus remotely in terms of latency and energy consumption.
]]></content:encoded>
<pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provably Convergent Decentralized Optimization over Directed Graphs under Generalized Smoothness</title>
<link>https://arxiv.org/abs/2601.03566</link>
<guid>https://arxiv.org/abs/2601.03566</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:  
1. Lipschitz$(L_0, L_1)$-  
2. Hessian  
3.   
4.   
5. LIBSVMCIFAR-10 <div>
arXiv:2601.03566v1 Announce Type: cross 
Abstract: Decentralized optimization has become a fundamental tool for large-scale learning systems; however, most existing methods rely on the classical Lipschitz smoothness assumption, which is often violated in problems with rapidly varying gradients. Motivated by this limitation, we study decentralized optimization under the generalized $(L_0, L_1)$-smoothness framework, in which the Hessian norm is allowed to grow linearly with the gradient norm, thereby accommodating rapidly varying gradients beyond classical Lipschitz smoothness. We integrate gradient-tracking techniques with gradient clipping and carefully design the clipping threshold to ensure accurate convergence over directed communication graphs under generalized smoothness. In contrast to existing distributed optimization results under generalized smoothness that require a bounded gradient dissimilarity assumption, our results remain valid even when the gradient dissimilarity is unbounded, making the proposed framework more applicable to realistic heterogeneous data environments. We validate our approach via numerical experiments on standard benchmark datasets, including LIBSVM and CIFAR-10, using regularized logistic regression and convolutional neural networks, demonstrating superior stability and faster convergence over existing methods.
]]></content:encoded>
<pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-agent Optimization of Non-cooperative Multimodal Mobility Systems</title>
<link>https://arxiv.org/abs/2601.03777</link>
<guid>https://arxiv.org/abs/2601.03777</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. <br />3. <br />4. <br />5.  <div>
arXiv:2601.03777v1 Announce Type: cross 
Abstract: While multimodal mobility systems have the potential to bring many benefits to travelers, drivers, the environment, and traffic congestion, such systems typically involve multiple non-cooperative decision-makers who may selfishly optimize their own objectives without considering the overall system benefits. This paper aims to investigate market-based interactions of travelers and ride-sourcing drivers in the context of multimodal mobility systems. We propose a unified mathematical modeling framework to capture the decentralized travelers and drivers' decision-making process and balance the network's demand and supply by equilibrium pricing. Such a model allows analyses of the impact of decentralized decision-making on multimodal mobility efficiencies. The proposed formulation can be further convexified to efficiently compute the equilibrium ride-sourcing prices. We conduct numerical experiments on different settings of transportation networks to gain policy insights. We find that travelers prefer ride-sourcing and multimodal transportation more than the driving option when they are more sensitive to prices. We also find that travelers may need to be subsidized to use multimodal transportation when there is fewer transit hubs in the network or, ride-sourcing drivers become too sensitive to the prices. However, we find that more transit hubs in the network increases the total empty VMT of ride-sourcing drivers by increasing the total relocation time. The proposed model can be used by policymakers and platform operators to design pricing and subsidy schemes that align individual decision-making with system-level efficiency and evaluate the trade-offs between accessibility and environmental impacts in multimodal transportation networks.
]]></content:encoded>
<pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stigmergic optimal transport</title>
<link>https://arxiv.org/abs/2601.04111</link>
<guid>https://arxiv.org/abs/2601.04111</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. stigmergy  
2.   
3.   
4.   
5. - <div>
arXiv:2601.04111v1 Announce Type: cross 
Abstract: Efficient navigation in swarms often relies on the emergence of decentralized approaches that minimize traversal time or energy. Stigmergy, where agents modify a shared environment that then modifies their behavior, is a classic mechanism that can encode this strategy. We develop a theoretical framework for stigmergic transport by casting it as a stochastic optimal control problem: agents (collectively) lay and (individually) follow trails while minimizing expected traversal time. Simulations and analysis reveal two emergent behaviors: path straightening in homogeneous environments and path refraction at material interfaces, both consistent with experimental observations of insect trails. While reminiscent of Fermat's principle, our results show how local, noisy agent+field interactions can give rise to geodesic trajectories in heterogeneous environments, without centralized coordination or global knowledge, relying instead on an embodied slow fast dynamical mechanism.
]]></content:encoded>
<pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>IoTChain: A Three-Tier Blockchain-based IoT Security Architecture</title>
<link>https://arxiv.org/abs/1806.02008</link>
<guid>https://arxiv.org/abs/1806.02008</guid>
<content:encoded><![CDATA[
<div>     <br /><br />:  
1. IoTchain  
2.   
3.   
4. IoTchain <div>
arXiv:1806.02008v3 Announce Type: replace 
Abstract: There has been increasing interest in the potential of blockchain in enhancing the security of devices and systems, such as Internet of Things (IoT). In this paper, we present a blockchain-based IoT security architecture, IoTchain. The three-tier architecture comprises an authentication layer, a blockchain layer and an application layer, and is designed to achieve identity authentication, access control, privacy protection, lightweight feature, regional node fault tolerance, denial-of-service resilience, and storage integrity. We also evaluate the performance of IoTchain to demonstrate its utility in an IoT deployment.
]]></content:encoded>
<pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Game based Distributed Approach for General Multi-agent Optimal Coverage with Application to UAV Networks</title>
<link>https://arxiv.org/abs/2408.01193</link>
<guid>https://arxiv.org/abs/2408.01193</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. OCP  
2.   
3.   
4. UAV  
5.  <div>
arXiv:2408.01193v3 Announce Type: replace 
Abstract: This paper focuses on the optimal coverage problem (OCP) for multi-agent systems with a decentralized optimization mechanism. A game based distributed decision-making method for the multi-agent OCP is proposed to address the high computational costs arising from the large scale of the multi-agent system and to ensure that the game's equilibrium achieves the global performance objective's maximum value. In particular, a distributed algorithm that needs only local information is developed and proved to converge to near-optimal global coverage. Finally, the proposed method is applied to maximize the coverage area of the UAV network for a target region. The simulation results show that our method can require much less computational time than other typical distributed algorithms in related work, while achieving a faster convergence rate. Comparison with centralized optimization also demonstrates that the proposed method has approximate optimization results and high computation efficiency.
]]></content:encoded>
<pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Clustering: An Unsupervised Cluster-Wise Training for Decentralized Data Distributions</title>
<link>https://arxiv.org/abs/2408.10664</link>
<guid>https://arxiv.org/abs/2408.10664</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. FedCRefglobal K  
2. local K  
3.   
4.   
5. EMNISTKMNISTFedCRef <div>
arXiv:2408.10664v2 Announce Type: replace 
Abstract: Federated Learning (FL) is a pivotal approach in decentralized machine learning, especially when data privacy is crucial and direct data sharing is impractical. While FL is typically associated with supervised learning, its potential in unsupervised scenarios is underexplored. This paper introduces a novel unsupervised federated learning methodology designed to identify the complete set of categories (global K) across multiple clients within label-free, non-uniform data distributions, a process known as Federated Clustering. Our approach, Federated Cluster-Wise Refinement (FedCRef), involves clients that collaboratively train models on clusters with similar data distributions. Initially, clients with diverse local data distributions (local K) train models on their clusters to generate compressed data representations. These local models are then shared across the network, enabling clients to compare them through reconstruction error analysis, leading to the formation of federated groups.In these groups, clients collaboratively train a shared model representing each data distribution, while continuously refining their local clusters to enhance data association accuracy. This iterative process allows our system to identify all potential data distributions across the network and develop robust representation models for each. To validate our approach, we compare it with traditional centralized methods, establishing a performance baseline and showcasing the advantages of our distributed solution. We also conduct experiments on the EMNIST and KMNIST datasets, demonstrating FedCRef's ability to refine and align cluster models with actual data distributions, significantly improving data representation precision in unsupervised federated settings.
]]></content:encoded>
<pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Noncooperative Consensus via a Trading-based Auction</title>
<link>https://arxiv.org/abs/2502.03616</link>
<guid>https://arxiv.org/abs/2502.03616</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1.   
2. TACoTrading Auction for Consensus  
3. TACo  
4. TACo  
5.  <div>
arXiv:2502.03616v3 Announce Type: replace 
Abstract: Noncooperative multi-agent systems often face coordination challenges due to conflicting preferences among agents. In particular, when agents act in their own self-interest, they may prefer different choices among multiple feasible outcomes, leading to suboptimal outcomes or even safety concerns. We propose an algorithm named trading auction for consensus (TACo), a decentralized approach that enables noncooperative agents to reach consensus without communicating directly or disclosing private valuations. TACo facilitates coordination through a structured trading-based auction, where agents iteratively select choices of interest and provably reach an agreement within an a priori bounded number of steps. A series of numerical experiments validate that the termination guarantees of TACo hold in practice, and show that TACo achieves a median performance that minimizes the total cost across all agents, while allocating resources significantly more fairly than baseline approaches.
]]></content:encoded>
<pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>QoS-Aware Load Balancing in the Computing Continuum via Multi-Player Bandits</title>
<link>https://arxiv.org/abs/2512.18915</link>
<guid>https://arxiv.org/abs/2512.18915</guid>
<content:encoded><![CDATA[
<div> QoSMP-MABKDE<br /><br />:<br />1. CCQoS<br />2. QoS<br />3. QEdgeProxyQoS<br />4. MP-MABKDEQoS<br />5. K3sQEdgeProxyQoS <div>
arXiv:2512.18915v2 Announce Type: replace 
Abstract: As computation shifts from the cloud to the edge to reduce processing latency and network traffic, the resulting Computing Continuum (CC) creates a dynamic environment where meeting strict Quality of Service (QoS) requirements and avoiding service instance overload becomes challenging. Existing methods often prioritize global metrics and overlook per-client QoS, which is crucial for latency-sensitive and reliability-critical applications. We propose QEdgeProxy, a decentralized QoS-aware load balancer that acts as a proxy between IoT devices and service instances in the CC. We formulate the load balancing problem as a Multi-Player Multi-Armed Bandit (MP-MAB) with heterogeneous rewards: Each load balancer autonomously selects service instances to maximize the probability of meeting its clients' QoS requirements by using Kernel Density Estimation (KDE) to estimate QoS success probabilities. Our load-balancing algorithm also incorporates an adaptive exploration mechanism to recover rapidly from performance shifts and non-stationary conditions. We present a Kubernetes-native QEdgeProxy implementation and evaluate it on an emulated CC testbed deployed on a K3s cluster with realistic network conditions and a latency-sensitive edge-AI workload. Results show that QEdgeProxy significantly outperforms proximity-based and reinforcement-learning baselines in per-client QoS satisfaction, while adapting effectively to load surges and changes in instance availability.
]]></content:encoded>
<pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>APoW: Auditable Proof-of-Work Against Block Withholding Attacks</title>
<link>https://arxiv.org/abs/2601.02496</link>
<guid>https://arxiv.org/abs/2601.02496</guid>
<content:encoded><![CDATA[
<div> APoW<br /><br />:  
1. APoWHashcash  
2. nonce  
3. BWA  
4. APoW  
5. PoW <div>
arXiv:2601.02496v1 Announce Type: new 
Abstract: We introduce APoW, a novel proof-of-work (PoW) construction inspired by Hashcash-style nonce searching, which enables the auditing of other miners' work through accountable re-scanning of the nonce space. The proposed scheme allows a miner to probabilistically attest to having searched specified regions of the nonce space in earlier mining rounds, while concurrently earning rewards for performing productive work for a new block or pool share. This capability enables miners belonging to a mining pools to audit another miner's claimed effort retroactively, thereby allowing the probabilistic detection of block withholding attacks (BWAs) without requiring trusted hardware or trusted third parties. As a consequence, the construction supports the design of decentralized mining pools in which work attribution is verifiable and withholding incentives are substantially reduced. The scheme preserves the fundamental properties of conventional PoW, including public verifiability and difficulty adjustment, while adding an orthogonal auditability layer tailored to pool-based mining. Finally, while a full deployment of APoW in Bitcoin would require a consensus rule change and minor modifications to mining ASICs, the construction remains practically useful even without consensus changes, for instance, as a pool-level auditing mechanism that enables verifiable pay-for-auditing using existing pool reserves.
]]></content:encoded>
<pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Topology-Independent Robustness of the Weighted Mean under Label Poisoning Attacks in Heterogeneous Decentralized Learning</title>
<link>https://arxiv.org/abs/2601.02682</link>
<guid>https://arxiv.org/abs/2601.02682</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1.   
2.   
3.   
4.   
5.  <div>
arXiv:2601.02682v1 Announce Type: new 
Abstract: Robustness to malicious attacks is crucial for practical decentralized signal processing and machine learning systems. A typical example of such attacks is label poisoning, meaning that some agents possess corrupted local labels and share models trained on these poisoned data. To defend against malicious attacks, existing works often focus on designing robust aggregators; meanwhile, the weighted mean aggregator is typically considered a simple, vulnerable baseline. This paper analyzes the robustness of decentralized gradient descent under label poisoning attacks, considering both robust and weighted mean aggregators. Theoretical results reveal that the learning errors of robust aggregators depend on the network topology, whereas the performance of weighted mean aggregator is topology-independent. Remarkably, the weighted mean aggregator, although often considered vulnerable, can outperform robust aggregators under sufficient heterogeneity, particularly when: (i) the global contamination rate (i.e., the fraction of poisoned agents for the entire network) is smaller than the local contamination rate (i.e., the maximal fraction of poisoned neighbors for the regular agents); (ii) the network of regular agents is disconnected; or (iii) the network of regular agents is sparse and the local contamination rate is high. Empirical results support our theoretical findings, highlighting the important role of network topology in the robustness to label poisoning attacks.
]]></content:encoded>
<pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Preserving AI-Enabled Decentralized Learning and Employment Records System</title>
<link>https://arxiv.org/abs/2601.02720</link>
<guid>https://arxiv.org/abs/2601.02720</guid>
<content:encoded><![CDATA[
<div> LERTEENLP<br /><br />:<br />1. AILER<br />2. TEENLP<br />3. TEE<br />4. <br />5. NLPSyllabus-to-O*NET<5% <div>
arXiv:2601.02720v1 Announce Type: new 
Abstract: Learning and Employment Record (LER) systems are emerging as critical infrastructure for securely compiling and sharing educational and work achievements. Existing blockchain-based platforms leverage verifiable credentials but typically lack automated skill-credential generation and the ability to incorporate unstructured evidence of learning. In this paper,a privacy-preserving, AI-enabled decentralized LER system is proposed to address these gaps. Digitally signed transcripts from educational institutions are accepted, and verifiable self-issued skill credentials are derived inside a trusted execution environment (TEE) by a natural language processing pipeline that analyzes formal records (e.g., transcripts, syllabi) and informal artifacts. All verification and job-skill matching are performed inside the enclave with selective disclosure, so raw credentials and private keys remain enclave-confined. Job matching relies solely on attested skill vectors and is invariant to non-skill resume fields, thereby reducing opportunities for screening bias.The NLP component was evaluated on sample learner data; the mapping follows the validated Syllabus-to-O*NET methodology,and a stability test across repeated runs observed <5% variance in top-ranked skills. Formal security statements and proof sketches are provided showing that derived credentials are unforgeable and that sensitive information remains confidential. The proposed system thus supports secure education and employment credentialing, robust transcript verification,and automated, privacy-preserving skill extraction within a decentralized framework.
]]></content:encoded>
<pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Preemptive Holistic Collaborative Systems for Embodied Multi-Agent Systems: Framework, Hybrid Stability, and Scalability Analysis</title>
<link>https://arxiv.org/abs/2601.02779</link>
<guid>https://arxiv.org/abs/2601.02779</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1.   
2.   
3. Prollect  
4.   
5. Zeno-ISS <div>
arXiv:2601.02779v1 Announce Type: new 
Abstract: The coordination of Embodied Multi-Agent Systems in constrained physical environments requires a rigorous balance between safety, scalability, and efficiency. Traditional decentralized approaches, e.g., reactive collision avoidance, are prone to local minima or reciprocal yielding standoffs due to the lack of future intent awareness. In contrast, centralized planning suffers from intractable computational complexity and single-point-of-failure vulnerabilities. To address these limitations, we propose the Hierarchical Preemptive Holistic Collaborative (Prollect) framework, which generalizes the Preemptive Holistic Collaborative System (PHCS) by decomposing the global coordination problem into topologically connected subspace optimizations. We formalize the system as a Hybrid Automaton and introduce a three-stage receding horizon mechanism (frozen execution, preliminary planning, proactive look-ahead windows) with explicit padding to prevent races between coordination dissemination and intent updates. Notably, we design a robust timing protocol with a mandatory Idle Buffer that acts as a dwell-time constraint to eliminate Zeno behaviors and ensure computational stability under jitter. Furthermore, we formalize a Shadow Agent protocol to guarantee seamless trajectory consistency across subspace boundaries, which we treat as an Input-to-State Stability (ISS) problem.
]]></content:encoded>
<pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributionally Robust Game for Proof-of-Work Blockchain Mining Under Resource Uncertainties</title>
<link>https://arxiv.org/abs/2601.02804</link>
<guid>https://arxiv.org/abs/2601.02804</guid>
<content:encoded><![CDATA[
<div> PoW<br /><br />:<br />1. PoW<br />2. <br />3. <br />4. CVaR<br />5.  <div>
arXiv:2601.02804v1 Announce Type: new 
Abstract: Blockchain plays a crucial role in ensuring the security and integrity of decentralized systems, with the proof-of-work (PoW) mechanism being fundamental for achieving distributed consensus. As PoW blockchains see broader adoption, an increasingly diverse set of miners with varying computing capabilities participate in the network. In this paper, we consider the PoW blockchain mining, where the miners are associated with resource uncertainties. To characterize the uncertainty computing resources at different mining participants, we establish an ambiguous set representing uncertainty of resource distributions. Then, the networked mining is formulated as a non-cooperative game, where distributionally robust performance is calculated for each individual miner to tackle the resource uncertainties. We prove the existence of the equilibrium of the distributionally robust mining game. To derive the equilibrium, we propose the conditional value-at-risk (CVaR)-based reinterpretation of the best response of each miner. We then solve the individual strategy with alternating optimization, which facilitates the iteration among miners towards the game equilibrium. Furthermore, we consider the case that the ambiguity of resource distribution reduces to Gaussian distribution and the case that another uncertainties vanish, and then characterize the properties of the equilibrium therein along with a distributed algorithm to achieve the equilibrium. Simulation results show that the proposed approaches effectively converge to the equilibrium, and effectively tackle the uncertainties in blockchain mining to achieve a robust performance guarantee.
]]></content:encoded>
<pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Blockchain Interoperability: Frameworks, Use Cases, and Future Challenges</title>
<link>https://arxiv.org/abs/2601.02949</link>
<guid>https://arxiv.org/abs/2601.02949</guid>
<content:encoded><![CDATA[
<div>     <br /><br />:<br />1. <br />2. <br />3. <br />4. <br />5.  <div>
arXiv:2601.02949v1 Announce Type: new 
Abstract: Trust between entities in any scenario without a trusted third party is very difficult, and trust is exactly what blockchain aims to bring into the digital world with its basic features. Many applications are moving to blockchain adoption, enabling users to work in a trustworthy manner. The early generations of blockchain have a problem; they cannot share information with other blockchains. As more and more entities move their applications to the blockchain, they generate large volumes of data, and as applications have become more complex, sharing information between different blockchains has become a necessity. This has led to the research and development of interoperable solutions allowing blockchains to connect together. This paper discusses a few blockchain platforms that provide interoperable solutions, emphasising their ability to connect heterogeneous blockchains. It also discusses a case study scenario to illustrate the importance and benefits of using interoperable solutions. We also present a few topics that need to be solved in the realm of interoperability.
]]></content:encoded>
<pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selfish Mining in Multi-Attacker Scenarios: An Empirical Evaluation of Nakamoto, Fruitchain, and Strongchain</title>
<link>https://arxiv.org/abs/2601.02984</link>
<guid>https://arxiv.org/abs/2601.02984</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. <br />3. <br />4. PoW NakamotoFruitchainStrongchain<br />5.  <div>
arXiv:2601.02984v1 Announce Type: new 
Abstract: The aim of this work is to enhance blockchain security by deepening the understanding of selfish mining attacks in various consensus protocols, especially the ones that have the potential to mitigate selfish mining. Previous research was mainly focused on a particular protocol with a single selfish miner, while only limited studies have been conducted on two or more attackers. To address this gap, we proposed a stochastic simulation framework that enables analysis of selfish mining with multiple attackers across various consensus protocols. We created the model of Proof-of-Work (PoW) Nakamoto consensus (serving as the baseline) as well as models of two additional consensus protocols designed to mitigate selfish mining: Fruitchain and Strongchain. Using our framework, thresholds reported in the literature were verified, and several novel thresholds were discovered for 2 and more attackers. We made the source code of our framework available, enabling researchers to evaluate any newly added protocol with one or more selfish miners and cross-compare it with already modeled protocols.
]]></content:encoded>
<pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Autoregressive Generation</title>
<link>https://arxiv.org/abs/2601.03184</link>
<guid>https://arxiv.org/abs/2601.03184</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. <br />2. <br />3. <br />4. LLaVAInternVL 2.5-1B<br />5. InternVL 2.5-1BCLIPViTMLPLLM <div>
arXiv:2601.03184v1 Announce Type: new 
Abstract: We present a theoretical analysis of decentralization of autoregressive generation. We define the Decentralized Discrete Flow Matching objective, by expressing probability generating velocity as a linear combination of expert flows. We also conduct experiments demonstrat- ing the equivalence between decentralized and centralized training settings for multimodal language models across diverse set of benchmarks. Specifically, we compare two distinct paradigms: LLaVA and InternVL 2.5-1B, which uses a fixed CLIP vision encoder and per- forms full-parameter fine-tuning (ViT+MLP+LLM) during the instruction tuning stage.
]]></content:encoded>
<pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SLVC-DIDA: Signature-less Verifiable Credential-based Issuer-hiding and Multi-party Authentication for Decentralized Identity</title>
<link>https://arxiv.org/abs/2501.11052</link>
<guid>https://arxiv.org/abs/2501.11052</guid>
<content:encoded><![CDATA[
<div> Decentralized Identity, Verifiable Credential, , , <br /><br />:<br />1. DIDSLVC-DIDA<br />2. PKI<br />3. MerkleVC<br />4. DID<br />5. SLVC-DIDA <div>
arXiv:2501.11052v3 Announce Type: replace 
Abstract: As an emerging paradigm in digital identity, Decentralized Identity (DID) appears advantages over traditional identity management methods in a variety of aspects, e.g., enhancing user-centric online services and ensuring complete user autonomy and control. Verifiable Credential (VC) techniques are used to facilitate decentralized DID-based access control across multiple entities. However, existing DID schemes generally rely on a distributed public key infrastructure that also causes challenges, such as context information deduction, key exposure, and issuer data leakage. To address the issues above, this paper proposes a issuer-hiding and privacy-preserving DID multi-party authentication model with a signature-less VC scheme, named SLVC-DIDA, for the first time. Our proposed scheme avoids the dependence on signing keys by employing hashing and issuer membership proofs, which supports universal zero-knowledge multi-party DID authentications, eliminating additional technical integrations. We adopt a novel zero-knowledge circuit to maintain the anonymity of the issuer set, thereby enabling public verification while safeguarding the privacy of identity attributes via a Merkle tree-based VC list. Furthermore, by eliminating reliance on a Public Key Infrastructure (PKI), SLVC-DIDA enables decentralized and self-sovereign DID authentication. Our experiments further evaluate the effectiveness and practicality of SLVC-DIDA.
]]></content:encoded>
<pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>A new type of federated clustering: A non-model-sharing approach</title>
<link>https://arxiv.org/abs/2506.10244</link>
<guid>https://arxiv.org/abs/2506.10244</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. DC-Clustering<br />2. <br />3. DC-Clustering  k-means <br />4. <br />5.  <div>
arXiv:2506.10244v3 Announce Type: replace 
Abstract: In recent years, the growing need to leverage sensitive data across institutions has led to increased attention on federated learning (FL), a decentralized machine learning paradigm that enables model training without sharing raw data. However, existing FL-based clustering methods, known as federated clustering, typically assume simple data partitioning scenarios such as horizontal or vertical splits, and cannot handle more complex distributed structures. This study proposes data collaboration clustering (DC-Clustering), a novel federated clustering method that supports clustering over complex data partitioning scenarios where horizontal and vertical splits coexist. In DC-Clustering, each institution shares only intermediate representations instead of raw data, ensuring privacy preservation while enabling collaborative clustering. The method allows flexible selection between k-means and spectral clustering, and achieves final results with a single round of communication with the central server. We conducted extensive experiments using synthetic and open benchmark datasets. The results show that our method achieves clustering performance comparable to centralized clustering where all data are pooled. DC-Clustering addresses an important gap in current FL research by enabling effective knowledge discovery from distributed heterogeneous data. Its practical properties -- privacy preservation, communication efficiency, and flexibility -- make it a promising tool for privacy-sensitive domains such as healthcare and finance.
]]></content:encoded>
<pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convergence of Decentralized Stochastic Subgradient-based Methods for Nonsmooth Nonconvex functions</title>
<link>https://arxiv.org/abs/2403.11565</link>
<guid>https://arxiv.org/abs/2403.11565</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. Clarke<br />2. DSGDDSGD-TDSGD-M<br />3. Lyapunov$\mathcal{A}$<br />4. $\mathcal{A}$<br />5. Clarke <div>
arXiv:2403.11565v4 Announce Type: replace-cross 
Abstract: In this paper, we focus on the decentralized stochastic subgradient-based methods in minimizing nonsmooth nonconvex functions without Clarke regularity, especially in the decentralized training of nonsmooth neural networks. We propose a general framework that unifies various decentralized subgradient-based methods, such as decentralized stochastic subgradient descent (DSGD), DSGD with gradient-tracking technique (DSGD-T), and DSGD with momentum (DSGD-M). To establish the convergence properties of our proposed framework, we relate the discrete iterates to the trajectories of a continuous-time differential inclusion, which is assumed to have a coercive Lyapunov function with a stable set $\mathcal{A}$. We prove the asymptotic convergence of the iterates to the stable set $\mathcal{A}$ with sufficiently small and diminishing step-sizes. These results provide first convergence guarantees for some well-recognized of decentralized stochastic subgradient-based methods without Clarke regularity of the objective function. Preliminary numerical experiments demonstrate that our proposed framework yields highly efficient decentralized stochastic subgradient-based methods with convergence guarantees in the training of nonsmooth neural networks.
]]></content:encoded>
<pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multiagent Reinforcement Learning for Liquidity Games</title>
<link>https://arxiv.org/abs/2601.00324</link>
<guid>https://arxiv.org/abs/2601.00324</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1.   
2.   
3.   
4.   
5.  <div>
arXiv:2601.00324v1 Announce Type: new 
Abstract: Making use of swarm methods in financial market modeling of liquidity, and techniques from financial analysis in swarm analysis, holds the potential to advance both research areas. In swarm research, the use of game theory methods holds the promise of explaining observed phenomena of collective utility adherence with rational self-interested swarm participants. In financial markets, a better understanding of how independent financial agents may self-organize for the betterment and stability of the marketplace would be a boon for market design researchers. This paper unifies Liquidity Games, where trader payoffs depend on aggregate liquidity within a trade, with Rational Swarms, where decentralized agents use difference rewards to align self-interested learning with global objectives. We offer a theoretical frameworks where we define a swarm of traders whose collective objective is market liquidity provision while maintaining agent independence. Using difference rewards within a Markov team games framework, we show that individual liquidity-maximizing behaviors contribute to overall market liquidity without requiring coordination or collusion. This Financial Swarm model provides a framework for modeling rational, independent agents where they achieve both individual profitability and collective market efficiency in bilateral asset markets.
]]></content:encoded>
<pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>MR-DAW: Towards Collaborative Digital Audio Workstations in Mixed Reality</title>
<link>https://arxiv.org/abs/2601.00326</link>
<guid>https://arxiv.org/abs/2601.00326</guid>
<content:encoded><![CDATA[
<div> DAWMR<br /><br />:  
1. DAW  
2. MR  
3. DAWMR-DAW  
4. MR-DAWDAW  
5. 20MR <div>
arXiv:2601.00326v1 Announce Type: new 
Abstract: Digital Audio Workstations (DAWs) are central to modern music production but often encumber the musician's workflow, tethering them to a desk and hindering natural interaction with their instrument. Furthermore, effective remote collaboration remains a significant challenge, with existing solutions hampered by network latency and asynchronous file sharing. This paper investigates the potential of Mixed Reality (MR) to overcome these barriers, creating an intuitive environment for real-time, remote musical collaboration. We employ qualitative and speculative design techniques to better understand: 1) how players currently use DAWs, and 2) to imagine a speculative future of collaborative MR-DAWs. To facilitate this discussion, we developed and evaluated the usability of a design probe, MR-DAW. An MR system enabling multiple, geographically dispersed users to control a single, shared DAW instance while moving freely in their local spaces. Our networked system enables each remote musician to use a physical foot pedal for collaborative looping, merging a familiar, hands-free interaction with a shared virtual session. Based on interviews and system evaluations with 20 musicians, we analyze current practices, report on the user experience with our MR system, and speculate on the future of musical collaboration in MR. Our results highlight the affordances of MR for unencumbered musical interaction and provide a speculative outlook on the future of remote collaborative DAWs in the Musical Metaverse.
]]></content:encoded>
<pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Secure, Verifiable, and Scalable Multi-Client Data Sharing via Consensus-Based Privacy-Preserving Data Distribution</title>
<link>https://arxiv.org/abs/2601.00418</link>
<guid>https://arxiv.org/abs/2601.00418</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. CPPDD  
2.   
3. _S_D  
4. O(ND)N-1  
5. CDIFIND-CPAMNIST500MPC34100% <div>
arXiv:2601.00418v1 Announce Type: new 
Abstract: We propose the Consensus-Based Privacy-Preserving Data Distribution (CPPDD) framework, a lightweight and post-setup autonomous protocol for secure multi-client data aggregation. The framework enforces unanimous-release confidentiality through a dual-layer protection mechanism that combines per-client affine masking with priority-driven sequential consensus locking. Decentralized integrity is verified via step (sigma_S) and data (sigma_D) checksums, facilitating autonomous malicious deviation detection and atomic abort without requiring persistent coordination. The design supports scalar, vector, and matrix payloads with O(N*D) computation and communication complexity, optional edge-server offloading, and resistance to collusion under N-1 corruptions. Formal analysis proves correctness, Consensus-Dependent Integrity and Fairness (CDIF) with overwhelming-probability abort on deviation, and IND-CPA security assuming a pseudorandom function family. Empirical evaluations on MNIST-derived vectors demonstrate linear scalability up to N = 500 with sub-millisecond per-client computation times. The framework achieves 100% malicious deviation detection, exact data recovery, and three-to-four orders of magnitude lower FLOPs compared to MPC and HE baselines. CPPDD enables atomic collaboration in secure voting, consortium federated learning, blockchain escrows, and geo-information capacity building, addressing critical gaps in scalability, trust minimization, and verifiable multi-party computation for regulated and resource-constrained environments.
]]></content:encoded>
<pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>The CoinAlg Bind: Profitability-Fairness Tradeoffs in Collective Investment Algorithms</title>
<link>https://arxiv.org/abs/2601.00523</link>
<guid>https://arxiv.org/abs/2601.00523</guid>
<content:encoded><![CDATA[
<div> -<br /><br />:  
1. CoinAlgsAI  
2. CoinAlgsCoinAlg  
3.   
4.   
5. UniswapCoinAlgs <div>
arXiv:2601.00523v1 Announce Type: new 
Abstract: Collective Investment Algorithms (CoinAlgs) are increasingly popular systems that deploy shared trading strategies for investor communities. Their goal is to democratize sophisticated -- often AI-based -- investing tools. We identify and demonstrate a fundamental profitability-fairness tradeoff in CoinAlgs that we call the CoinAlg Bind: CoinAlgs cannot ensure economic fairness without losing profit to arbitrage. We present a formal model of CoinAlgs, with definitions of privacy (incomplete algorithm disclosure) and economic fairness (value extraction by an adversarial insider). We prove two complementary results that together demonstrate the CoinAlg Bind. First, privacy in a CoinAlg is a precondition for insider attacks on economic fairness. Conversely, in a game-theoretic model, lack of privacy, i.e., transparency, enables arbitrageurs to erode the profitability of a CoinAlg. Using data from Uniswap, a decentralized exchange, we empirically study both sides of the CoinAlg Bind. We quantify the impact of arbitrage against transparent CoinAlgs. We show the risks posed by a private CoinAlg: Even low-bandwidth covert-channel information leakage enables unfair value extraction.
]]></content:encoded>
<pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Transport-Based Decentralized Multi-Agent Distribution Matching</title>
<link>https://arxiv.org/abs/2601.00548</link>
<guid>https://arxiv.org/abs/2601.00548</guid>
<content:encoded><![CDATA[
<div> Wasserstein<br /><br />:<br />1. <br />2. Wasserstein<br />3. <br />4. <br />5.  <div>
arXiv:2601.00548v1 Announce Type: new 
Abstract: This paper presents a decentralized control framework for distribution matching in multi-agent systems (MAS), where agents collectively achieve a prescribed terminal spatial distribution. The problem is formulated using optimal transport (Wasserstein distance), which provides a principled measure of distributional discrepancy and serves as the basis for the control design. To avoid solving the global optimal transport problem directly, the distribution-matching objective is reformulated into a tractable per-agent decision process, enabling each agent to identify its desired terminal locations using only locally available information. A sequential weight-update rule is introduced to construct feasible local transport plans, and a memory-based correction mechanism is incorporated to maintain reliable operation under intermittent and range-limited communication. Convergence guarantees are established, showing cycle-wise improvement of a surrogate transport cost under both linear and nonlinear agent dynamics. Simulation results demonstrate that the proposed framework achieves effective and scalable distribution matching while operating fully in a decentralized manner.
]]></content:encoded>
<pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cyberscurity Threats and Defense Mechanisms in IoT network</title>
<link>https://arxiv.org/abs/2601.00556</link>
<guid>https://arxiv.org/abs/2601.00556</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. 2030300  
2. 5920092024  
3. DoS  
4.   
5. 6G <div>
arXiv:2601.00556v1 Announce Type: new 
Abstract: The rapid proliferation of Internet of Things (IoT) technologies, projected to exceed 30 billion interconnected devices by 2030, has significantly escalated the complexity of cybersecurity challenges. This survey aims to provide a comprehensive analysis of vulnerabilities, threats, and defense mechanisms, specifically focusing on the integration of network and application layers within real-time monitoring and decision-making systems. Employing an integrative review methodology, 59 scholarly articles published between 2009 and 2024 were selected from databases such as IEEE Xplore, ScienceDirect, and PubMed, utilizing keywords related to IoT vulnerabilities and security attacks. Key findings identify critical threat categories, including sensor vulnerabilities, Denial-of-Service (DoS) attacks, and public cloud insecurity. Conversely, the study highlights advanced defense approaches leveraging Artificial Intelligence (AI) for anomaly detection, Blockchain for decentralized trust, and Zero Trust Architecture (ZTA) for continuous verification. This paper contributes a novel five-layer IoT model and outlines future research directions involving quantum computing and 6G networks to bolster IoT ecosystem resilience.
]]></content:encoded>
<pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing</title>
<link>https://arxiv.org/abs/2601.00785</link>
<guid>https://arxiv.org/abs/2601.00785</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />1. non-IIDFedHypeVAE<br />2. VAE<br />3. <br />4. MMDLipschitzIID<br />5.  <div>
arXiv:2601.00785v1 Announce Type: new 
Abstract: Federated data sharing promises utility without centralizing raw data, yet existing embedding-level generators struggle under non-IID client heterogeneity and provide limited formal protection against gradient leakage. We propose FedHypeVAE, a differentially private, hypernetwork-driven framework for synthesizing embedding-level data across decentralized clients. Building on a conditional VAE backbone, we replace the single global decoder and fixed latent prior with client-aware decoders and class-conditional priors generated by a shared hypernetwork from private, trainable client codes. This bi-level design personalizes the generative layerrather than the downstream modelwhile decoupling local data from communicated parameters. The shared hypernetwork is optimized under differential privacy, ensuring that only noise-perturbed, clipped gradients are aggregated across clients. A local MMD alignment between real and synthetic embeddings and a Lipschitz regularizer on hypernetwork outputs further enhance stability and distributional coherence under non-IID conditions. After training, a neutral meta-code enables domain agnostic synthesis, while mixtures of meta-codes provide controllable multi-domain coverage. FedHypeVAE unifies personalization, privacy, and distribution alignment at the generator level, establishing a principled foundation for privacy-preserving data synthesis in federated settings. Code: github.com/sunnyinAI/FedHypeVAE
]]></content:encoded>
<pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pod: An Optimal-Latency, Censorship-Free, and Accountable Generalized Consensus Layer</title>
<link>https://arxiv.org/abs/2501.14931</link>
<guid>https://arxiv.org/abs/2501.14931</guid>
<content:encoded><![CDATA[
<div> <br /><br />:  
1. pod  
2. pod  $2\delta$  
3.   
4. pod   
5. pod-core  $2\delta$  <div>
arXiv:2501.14931v5 Announce Type: replace 
Abstract: This work addresses the inherent issues of high latency in blockchains and low scalability in traditional consensus protocols. We present pod, a novel notion of consensus whose first priority is to achieve the physically-optimal latency of $2\delta$, or one round-trip, i.e., requiring only one network trip (duration $\delta$) for writing a transaction and one for reading it.
  To accomplish this, we first eliminate inter-replica communication. Instead, clients send transactions directly to all replicas, which independently process transactions and append them to local logs. Replicas assigns a timestamp and a sequence number to each transaction in their logs, allowing clients to extract valuable metadata about the transactions and the system state. Later on, clients retrieve these logs and extract transactions (and associated metadata) from them.
  Necessarily, this construction achieves weaker properties than a total-order broadcast protocol, due to existing lower bounds. Our work models the primitive of pod and defines its security properties. We then show pod-core, a protocol that satisfies properties such as transaction confirmation within $2\delta$, censorship resistance against Byzantine replicas, and accountability for safety violations. We show that single-shot auctions can be realized using the pod notion and observe that it is also sufficient for other popular applications.
]]></content:encoded>
<pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Midsummer Meme's Dream: Investigating Market Manipulations in the Meme Coin Ecosystem</title>
<link>https://arxiv.org/abs/2507.01963</link>
<guid>https://arxiv.org/abs/2507.01963</guid>
<content:encoded><![CDATA[
<div> meme coinsLPI<br /><br />:  
1. BNBSolanaBase34,988meme  
2. 100%meme82.8%LPI  
3. LPI  
4. LPI  
5. 17,000930meme <div>
arXiv:2507.01963v2 Announce Type: replace-cross 
Abstract: From viral jokes to a billion-dollar phenomenon, meme coins have become one of the most popular segments in cryptocurrency markets. Unlike utility-focused crypto assets like Bitcoin, meme coins derive value primarily from community sentiment, making them vulnerable to manipulation. This study presents an unprecedented cross-chain analysis of the meme coin ecosystem, examining 34,988 tokens across Ethereum, BNB Smart Chain, Solana, and Base. We characterize their tokenomics and track their growth in a three-month longitudinal analysis. We discover that among high-return tokens (>100%), an alarming 82.8% show evidence of artificial growth strategies designed to create a misleading appearance of market interest. These include wash trading and a new form of manipulation we define as Liquidity Pool-Based Price Inflation (LPI), where small strategic purchases trigger dramatic price increases. We find that profit extraction schemes, such as pump and dumps and rug pulls, typically follow initial manipulations like wash trading or LPI, indicating how early manipulations create the foundation for later exploitation. We quantify the economic impact of these schemes, identifying over 17,000 victimized addresses with realized losses exceeding $9.3 million. These findings reveal that combined manipulations are widespread among high-performing meme coins, suggesting that their dramatic gains are often driven by coordinated efforts rather than natural market dynamics.
]]></content:encoded>
<pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-varying Mixing Matrix Design for Energy-efficient Decentralized Federated Learning</title>
<link>https://arxiv.org/abs/2512.24069</link>
<guid>https://arxiv.org/abs/2512.24069</guid>
<content:encoded><![CDATA[
<div> The arXiv paper **2512.24069v1** presents a novel approach to designing **mixing matrices** in **decentralized federated learning (DFL)** over wireless networks, with the primary objective of **minimizing the maximum per-node energy consumption**a critical concern for battery-limited edge devices.

### Key Contributions and Insights:

#### 1. **Problem Motivation**
- In DFL, agents (e.g., mobile or IoT devices) collaboratively train a model without central coordination by exchanging local model updates with neighbors.
- The **mixing matrix** (also known as the consensus weight matrix) governs how nodes combine their neighbors information during each communication round.
- While prior work has focused on optimizing mixing matrices to speed up convergence (often favoring dense topologies), this paper highlights that **energy efficiency**, especially **peak energy per node**, is equallyif not moreimportant in practical wireless deployments.

#### 2. **Key Challenge**
- There's a fundamental trade-off:
  - **Dense communication graphs**  faster convergence but high per-round energy use due to many transmissions.
  - **Sparse graphs**  lower energy per round but slower convergence, potentially increasing total energy.
- Additionally, **wireless broadcast nature** means one transmission can reach multiple neighbors, which can be exploited for efficiency.

#### 3. **Main Innovation: Multi-Phase Time-Varying Mixing Matrix Design**
- The authors propose a **multi-phase framework** where different sparse communication topologies are activated over time, with optimized durations ("budgets").
- This allows **time-varying mixing matrices**, adapting throughout training to balance:
  - Energy cost per iteration
  - Overall convergence speed
  - Fairness in energy distribution across nodes

#### 4. **Theoretical Foundation**
- A **new convergence theorem** is developed that supports **arbitrarily time-varying mixing matrices**, relaxing the common assumption of fixed topology.
- This enables dynamic switching between topologies while still guaranteeing convergence.

#### 5. **Energy-Aware Optimization**
- The design explicitly accounts for:
  - Transmission energy costs in wireless settings
  - Asymmetric node capabilities or channel conditions
  - Broadcast advantage: one transmission reaching many receivers reduces redundancy
- Objective: **Minimize the maximum energy consumed by any single node** until global convergence  promoting fairness and longevity in heterogeneous networks.

#### 6. **Empirical Validation**
- Evaluated using **real-world datasets and network traces**.
- Results show the proposed method successfully **combines benefits of both worlds**:
  - Achieves **low per-node energy consumption** like sparse designs
  - Maintains **fast convergence rates** close to dense counterparts
- Outperforms static and uniform mixing strategies in terms of energy fairness and total lifetime support.

---

### Significance and Impact

This work bridges an important gap between theoretical DFL algorithm design and practical deployment constraints in wireless edge systems. By rethinking the role of the mixing matrixnot just as a convergence accelerator but as a tool for **energy load balancing**it opens new directions for sustainable, long-lived decentralized learning systems.

### Potential Applications
- Internet of Things (IoT) networks
- Mobile edge computing
- Autonomous sensor networks
- Federated learning on battery-powered devices

---

### Conclusion

arXiv:2512.24069v1 makes a timely and impactful contribution by reframing mixing matrix design around **energy fairness and efficiency** in decentralized federated learning. Its multi-phase, time-varying strategy leverages theoretical advances and realistic wireless considerations to deliver solutions that are both **analytically sound and practically viable**.

>  **Suggested follow-up**: Investigate whether the time-varying topologies can be learned online (adaptively) based on current energy levels and channel statespotentially enabling fully adaptive DFL in dynamic environments. <div>
arXiv:2512.24069v1 Announce Type: new 
Abstract: We consider the design of mixing matrices to minimize the operation cost for decentralized federated learning (DFL) in wireless networks, with focus on minimizing the maximum per-node energy consumption. As a critical hyperparameter for DFL, the mixing matrix controls both the convergence rate and the needs of agent-to-agent communications, and has thus been studied extensively. However, existing designs mostly focused on minimizing the communication time, leaving open the minimization of per-node energy consumption that is critical for energy-constrained devices. This work addresses this gap through a theoretically-justified solution for mixing matrix design that aims at minimizing the maximum per-node energy consumption until convergence, while taking into account the broadcast nature of wireless communications. Based on a novel convergence theorem that allows arbitrarily time-varying mixing matrices, we propose a multi-phase design framework that activates time-varying communication topologies under optimized budgets to trade off the per-iteration energy consumption and the convergence rate while balancing the energy consumption across nodes. Our evaluations based on real data have validated the efficacy of the proposed solution in combining the low energy consumption of sparse mixing matrices and the fast convergence of dense mixing matrices.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Heterogeneity-Aware Client Selection for Federated Learning in Wireless Networks</title>
<link>https://arxiv.org/abs/2512.24286</link>
<guid>https://arxiv.org/abs/2512.24286</guid>
<content:encoded><![CDATA[
<div> It looks like you've shared the **abstract** of a newly submitted paper to arXiv with the identifier **arXiv:2512.24286v1**, which focuses on **Federated Learning (FL)** in **wireless mobile edge networks**.

Heres a breakdown and interpretation of the key points from the abstract:

---

###  **Summary of Key Contributions**

1. **Problem Context**:
   - Federated Learning allows edge devices (clients) to collaboratively train a global model without sharing raw datapreserving privacy.
   - However, FL over wireless networks faces challenges due to:
     - Limited communication & computational resources
     - **Data heterogeneity** across clients (non-IID data)
     - These issues worsen in large-scale deployments.

2. **Theoretical Analysis**:
   - The paper provides a **theoretical study** on how **client data heterogeneity affects generalization error** of the global model.
   - High heterogeneity can lead to:
     - Poor model convergence
     - More training rounds (increased latency)
     - Higher energy consumption

3. **Optimization Framework**:
   - Proposes a joint optimization problem that aims to:
     - Minimize **learning latency**
     - Minimize **energy consumption**
     - While keeping the **generalization error within acceptable bounds**
   - This is a multi-objective trade-off, especially critical for resource-constrained edge devices.

4. **Proposed Solution: CSRA**  
   *(Joint Client Selection and Resource Allocation)*
   - Uses **convex optimization and relaxation techniques** to make the problem tractable.
   - Dynamically selects which clients participate in each round (client selection).
   - Allocates communication and computation resources efficiently (e.g., bandwidth, power, CPU frequency).

5. **Evaluation & Results**:
   - Simulation-based evaluation shows that CSRA outperforms baseline methods that ignore data heterogeneity.
   - Improvements include:
     - **Higher test accuracy** (better generalization)
     - **Lower learning latency**
     - **Reduced energy consumption**

---

###  Significance

This work bridges theoretical understanding of data heterogeneity in FL with practical system design in wireless edge environments. By incorporating heterogeneity into both the analysis and optimization framework, it enables more efficient and accurate FL deployment.

---

###  Potential Applications

- IoT networks
- 5G/6G mobile systems
- Smart cities, autonomous systems, or healthcare applications using decentralized learning

---

###  Technical Approach Highlights

- **Modeling generalization error** as a function of data distribution skew.
- **Jointly optimizing** who participates (client selection) and how resources are allocated (bandwidth, time, local computation).
- Use of **relaxation techniques** (likely Lagrangian relaxation, fractional programming, or binary variable relaxations) to handle combinatorial aspects of client selection.

---

If you're interested, I can help with:
- Summarizing the full paper once it's available.
- Explaining possible mathematical models behind the generalization error analysis.
- Comparing this approach to prior works like FedAvg, FedProx, or communication-efficient FL methods.
- Replicating or designing simulations based on this framework.

Let me know how you'd like to proceed! <div>
arXiv:2512.24286v1 Announce Type: new 
Abstract: Federated Learning (FL) enables mobile edge devices, functioning as clients, to collaboratively train a decentralized model while ensuring local data privacy. However, the efficiency of FL in wireless networks is limited not only by constraints on communication and computational resources but also by significant data heterogeneity among clients, particularly in large-scale networks. This paper first presents a theoretical analysis of the impact of client data heterogeneity on global model generalization error, which can result in repeated training cycles, increased energy consumption, and prolonged latency. Based on the theoretical insights, an optimization problem is formulated to jointly minimize learning latency and energy consumption while constraining generalization error. A joint client selection and resource allocation (CSRA) approach is then proposed, employing a series of convex optimization and relaxation techniques. Extensive simulation results demonstrate that the proposed CSRA scheme yields higher test accuracy, reduced learning latency, and lower energy consumption compared to baseline methods that do not account for data heterogeneity.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>GateChain: A Blockchain Based Application for Country Entry Exit Registry Management</title>
<link>https://arxiv.org/abs/2512.24416</link>
<guid>https://arxiv.org/abs/2512.24416</guid>
<content:encoded><![CDATA[
<div> **arXiv:2512.24416v1 [cs.CR]**  *Announcement Type: New*

**Title:** *GateChain: A Blockchain-Based Secure and Auditable System for Border Entry and Exit Record Management*

**Summary of Abstract:**

With increasing international mobility and heightened security demands, the accurate and trustworthy recording of cross-border entry and exit events has become critical. Current border control systems often rely on centralized databases, which are susceptible to data tampering, single points of failure, and suffer from poor interoperability across agencies and nations.

To address these challenges, this paper introduces **GateChain**, a novel blockchain-based application designed to ensure **confidentiality, integrity, auditability**, and **interoperability** in border management systems. GateChain leverages a distributed ledger architecture to immutably record entry and exit events, providing a transparent, cryptographically secured, and tamper-evident log accessible in real time by authorized institutions.

Key features of GateChain include:
- **Decentralized Architecture**: Eliminates reliance on a single central authority, reducing risks of manipulation and downtime.
- **Cryptographic Integrity**: All records are hashed and chained, ensuring any unauthorized alteration is detectable.
- **Access Control Mechanisms**: Role-based permissions and cryptographic authentication enable secure, real-time data sharing among trusted entities (e.g., immigration, customs, law enforcement).
- **Auditability and Transparency**: Every transaction is time-stamped and traceable, supporting regulatory compliance and forensic investigations.
- **Interoperability Design**: Facilitates data exchange across national and institutional boundaries while preserving privacy and sovereignty.

The paper presents the full system architecture, details the security model (including resistance to common attack vectors), and evaluates GateChains performance in terms of transaction latency, scalability, and resource consumption through simulations and prototype testing.

**Conclusion:**  
GateChain demonstrates that blockchain technology can significantly enhance the reliability and trustworthiness of border control systems. By combining decentralization with strong cryptographic guarantees, it offers a promising foundation for next-generation, secure, and auditable international travel recordkeeping.

---

*Note: As of now (April 2024), arXiv IDs typically follow the format YYMM.NNNNN  so "2512.24416v1" suggests a future submission (December 2025). This may be a placeholder or hypothetical identifier. If this is a speculative or fictional paper, the above summary reflects a plausible interpretation based on current trends in blockchain applications for public security infrastructure.* <div>
arXiv:2512.24416v1 Announce Type: new 
Abstract: Recording entry and exit records for a country, with properties such as confidentiality, integrity, and auditability, is increasingly important due to rising international mobility and security requirements. Traditional border control systems, which rely on centralised databases, are vulnerable to data manipulation and have limited interoperability between institutions. This study presents GateChain, a blockchain-based application that addresses these vulnerabilities. GateChain aims to enhance data integrity, reliability, and transparency by recording entry and exit events on a distributed, immutable, and cryptographically verifiable ledger. The application provides real-time access control and verification for authorised institutions. This paper describes the architecture and security components of GateChain and evaluates its performance and security features.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Document Data Matching for Blockchain-Supported Real Estate</title>
<link>https://arxiv.org/abs/2512.24457</link>
<guid>https://arxiv.org/abs/2512.24457</guid>
<content:encoded><![CDATA[
<div> **arXiv:2512.24457v1 [cs.CY]**  *Automated Document Verification in Real Estate Using OCR, NLP, and Verifiable Credentials*

### Summary & Analysis:

This paper introduces an innovative framework aimed at modernizing the traditionally manual and error-prone processes in the real estate sector by leveraging **optical character recognition (OCR)**, **natural language processing (NLP)**, and **verifiable credentials (VCs)** anchored on a **blockchain-based decentralized trust layer**.

---

### Key Contributions:

1. **Integration of AI and Trust Technologies**:
   - Combines **OCR** for digitizing physical documents (e.g., property deeds, ID proofs, contracts) with **NLP** to extract structured data from unstructured text.
   - Uses synthetic datasets to train the OCR-NLP pipeline, enabling robustness across diverse document formats and quality levelscrucial given the heterogeneity in real estate documentation globally.

2. **Standardization via Verifiable Credentials (VCs)**:
   - Extracted information is converted into standardized **W3C-compliant verifiable credentials**, which are cryptographically secure and tamper-evident.
   - This enables interoperability and machine-readability across stakeholders (buyers, sellers, agents, banks, regulators).

3. **Blockchain for Decentralized Trust**:
   - VCs are issued and verified using a blockchain backend, ensuring immutability, auditability, and transparency without reliance on a central authority.
   - Enhances resistance to fraud such as forged titles or identity misrepresentation.

4. **End-to-End System Architecture**:
   The prototype includes:
   - **(i) OCR-NLP Extraction Pipeline**: Preprocessing, layout analysis, text extraction, and semantic parsing.
   - **(ii) Backend Service**: Manages credential issuance, revocation, and validation; integrates with a permissioned blockchain (likely Hyperledger or Ethereum-based).
   - **(iii) Frontend Interface**: Supports roles of issuer (e.g., notary), holder (e.g., buyer), and verifier (e.g., bank), facilitating user interaction and consent management.

5. **Automated Data Matching & Inconsistency Detection**:
   - Cross-references extracted fields across multiple documents (e.g., matching owner names in deed and ID) to flag discrepancies automatically.
   - Reduces human oversight errors and speeds up due diligence.

---

### Experimental Results:

- Models show **competitive accuracy** on key entity extraction (e.g., property address, price, parties involved) across various document types (deeds, IDs, mortgage statements).
- End-to-end processing significantly **reduces verification time**from days/hours to minuteswhile maintaining high reliability.
- Synthetic training data helps overcome scarcity of labeled real-world documents, improving generalization.

---

### Implications:

- **Efficiency Gains**: Automates labor-intensive tasks like title checks, income verification, and compliance reviews.
- **Fraud Reduction**: Tamper-proof records and cryptographic verification minimize risks of document forgery.
- **Stakeholder Trust**: Transparent audit trails and self-sovereign identity principles empower users with control over their data.
- **Scalability**: Framework can be extended to other domains requiring document-heavy workflows (e.g., banking, healthcare, legal services).

---

### Limitations & Future Work:

- Reliance on synthetic data may not fully capture edge cases in low-quality scans or regional formatting variations.
- Blockchain scalability and energy efficiency could be concerns depending on the chosen platform.
- User adoption depends on regulatory alignment and integration with existing land registry systems.

Future directions may include:
- Integration with government land registries.
- Support for zero-knowledge proofs to enhance privacy.
- Multilingual expansion for global applicability.

---

### Conclusion:

The proposed system represents a significant step toward **digitizing and securing real estate transactions** through the convergence of AI-driven automation and decentralized identity technologies. By transforming fragmented, paper-based workflows into a unified, trustworthy digital process, it paves the way for more efficient, transparent, and inclusive property markets.

> **Potential Impact**: High  particularly in jurisdictions with weak institutional trust or underdeveloped land administration systems. <div>
arXiv:2512.24457v1 Announce Type: new 
Abstract: The real estate sector remains highly dependent on manual document handling and verification, making processes inefficient and prone to fraud. This work presents a system that integrates optical character recognition (OCR), natural language processing (NLP), and verifiable credentials (VCs) to automate document extraction, verification, and management. The approach standardizes heterogeneous document formats into VCs and applies automated data matching to detect inconsistencies, while the blockchain provides a decentralized trust layer that reinforces transparency and integrity. A prototype was developed that comprises (i) an OCR-NLP extraction pipeline trained on synthetic datasets, (ii) a backend for credential issuance and management, and (iii) a frontend supporting issuer, holder, and verifier interactions. Experimental results show that the models achieve competitive accuracy across multiple document types and that the end-to-end pipeline reduces verification time while preserving reliability. The proposed framework demonstrates the potential to streamline real estate transactions, strengthen stakeholder trust, and enable scalable, secure digital processes.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning-Augmented LLM Agents for Collaborative Decision Making and Performance Optimization</title>
<link>https://arxiv.org/abs/2512.24609</link>
<guid>https://arxiv.org/abs/2512.24609</guid>
<content:encoded><![CDATA[
<div> The abstract you've provided describes a cutting-edge advancement in multi-agent Large Language Model (LLM) systems, leveraging reinforcement learning to enhance collaborative performance. Here's a breakdown and interpretation of the key components and contributions:

---

**Title (inferred):**  
*Group Relative Policy Optimization for Collaborative Multi-Agent LLMs*

---

### **Core Problem Addressed:**
While LLMs excel at individual language tasks, they often lack **collaborative awareness**the ability to coordinate effectively with other agents to optimize **global outcomes** in multi-agent settings. This leads to inefficiencies, redundancy, and inconsistency in joint tasks like collaborative writing or coding.

---

### **Proposed Solution:**

1. **Dec-POMDP Framework:**  
   The authors model multi-agent collaboration as a *Decentralized Partially Observable Markov Decision Process (Dec-POMDP)*a formalism suitable for multiple agents making decisions based on partial observations while aiming to maximize a shared objective.

2. **CTDE Paradigm:**  
   They adopt **Centralized Training with Decentralized Execution (CTDE)**:
   - During **training**, agents have access to global information (e.g., actions/rewards of all agents).
   - At **execution**, each agent acts only on its local observationsmaking the system scalable and practical.

3. **Group Relative Policy Optimization (GRPO):**  
   A novel reinforcement learning algorithm that extends concepts like PPO (Proximal Policy Optimization) to the multi-agent setting. GRPO:
   - Uses **global signals during training** to shape individual policies.
   - Optimizes policies relative to group performance, encouraging coordination without requiring explicit communication protocols.

4. **Simplified Joint Reward Function:**  
   Balances three critical aspects:
   - **Task quality** (correctness, coherence),
   - **Speed** (time-to-completion, efficiency),
   - **Coordination cost** (redundancy, conflicts, communication overhead).

---

### **Evaluation & Results:**

- **Domains Tested:**
  - **Collaborative Writing**: Measured for consistency in style and structure.
  - **Collaborative Coding**: Evaluated using test case pass rates.

- **Key Results:**
  - **3 faster task processing** compared to single-agent baselines.
  - **98.7% structural/style consistency** in writing tasksindicating strong alignment across agent contributions.
  - **74.6% test pass rate** in coding taskscompetitive given the complexity of multi-agent program synthesis.
  - Outperforms existing multi-agent LLM frameworks (e.g., BabyAGI, MetaGPT, etc.) in both efficiency and outcome quality.

---

### **Significance & Implications:**

- Provides a **scalable and principled framework** for enabling LLMs to work together intelligently.
- Demonstrates that **reinforcement learning**, when adapted to group dynamics (via GRPO), can significantly improve coordination.
- Offers a **practical path toward automating complex workflows** (e.g., software development teams, content creation pipelines) using AI agents.

---

### **Potential Future Directions:**
- Extending GRPO to include explicit communication mechanisms.
- Applying the framework to real-time, open-world environments.
- Studying emergent behaviors and potential miscoordination risks in large-scale deployments.

---

This work represents a significant step toward **reliable, efficient, and coherent multi-agent AI systems**, bridging the gap between standalone LLM capabilities and team-based problem solving. If the full paper delivers on these claims with robust ablation studies and reproducible results, it could become a foundational method in the multi-agent LLM space.

Let me know if you'd like help summarizing the full paper once its available, implementing GRPO, or comparing it to other methods like MADDPG or COMA in multi-agent RL. <div>
arXiv:2512.24609v1 Announce Type: new 
Abstract: Large Language Models (LLMs) perform well in language tasks but often lack collaborative awareness and struggle to optimize global performance in multi-agent settings. We present a reinforcement learning-augmented LLM agent framework that formulates cooperation as a decentralized partially observable Markov decision process (Dec-POMDP) and adopts centralized training with decentralized execution (CTDE). We introduce Group Relative Policy Optimization (GRPO) to jointly optimize agent policies with access to global signals during training, together with a simplified joint reward that balances task quality, speed, and coordination cost. On collaborative writing and coding benchmarks, our framework delivers a 3x increase in task processing speed over single-agent baselines, 98.7% structural/style consistency in writing, and a 74.6% test pass rate in coding. The approach consistently outperforms strong multi-agent LLM baselines and provides a practical path toward reliable collaboration in complex workflows.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized No-Regret Frequency-Time Scheduling for FMCW Radar Interference Avoidance</title>
<link>https://arxiv.org/abs/2512.24619</link>
<guid>https://arxiv.org/abs/2512.24619</guid>
<content:encoded><![CDATA[
<div> The paper **arXiv:2512.24619v1** presents a novel and theoretically grounded approach to mitigating interference among automotive Frequency-Modulated Continuous Wave (FMCW) radars in dense environments  a critical challenge for Advanced Driver Assistance Systems (ADAS) and autonomous vehicles.

Here is a detailed breakdown and interpretation of the key contributions and concepts:

---

###  **Problem Context**
As the number of FMCW radars increases on roads, **mutual interference** becomes a major issue:
- Interference degrades signal quality  reduces detection accuracy.
- Traditional mitigation methods face trade-offs:
  - **Receiver-side suppression**: Limited scalability.
  - **Waveform design**: May reduce resolution or require coordination.
  - **Cooperative scheduling**: Depends on side-channel communication (e.g., V2X), which may not be reliable or available.

Thus, there's a need for **scalable, decentralized, communication-free solutions** that preserve radar performance.

---

###  **Core Idea: Game-Theoretic Time-Frequency Hopping**

The authors build upon their prior work on *decentralized frequency-domain no-regret hopping* and extend it into a **unified time-frequency framework**, introducing:

> **Time-Frequency No-Regret Hopping (TF-NRH)**

This algorithm enables each radar to autonomously adapt both:
1. **Frequency subband selection** (spectral domain)
2. **Chirp-level time offsets** (temporal domain)

via a **joint adaptation strategy** modeled as a repeated game.

---

###  **Game Formulation: Repeated Anti-Coordination Game**

- Each radar is a player choosing actions from a joint time-frequency action space.
- Objective: Avoid collisions (i.e., overlapping transmissions in time and frequency).
- The setup is an **anti-coordination game**: players benefit when they differ in resource usage.
- Players use **regret-minimization dynamics** to update mixed strategies over time.

Key properties:
- **No reliance on inter-radar communication**  fully decentralized.
- **Adaptive learning**: Radars learn good strategies over time by minimizing regret.

---

###  **Theoretical Contributions**

The paper proves strong theoretical guarantees:

1.  **Vanishing External Regret & Swap Regret**
   - Over time, the average regret tends to zero.
   - Implies that the algorithm performs nearly as well as the best fixed strategy (external regret), and even nearly as well as the best sequence of actions (swap regret).

2.  **Convergence to Equilibrium**
   - Empirical joint play converges to:
     - An **-coarse correlated equilibrium (CCE)** (for external regret minimization)
     - A **correlated equilibrium (CE)** (for swap regret minimization)
   - These equilibria represent stable, efficient configurations where unilateral deviations yield little gain.

3.  **Joint Time-Frequency Regret Bounds**
   - Novel analysis shows how temporal adaptation (time shifting) acts as a form of **implicit regularization** for frequency selection.
   - Enhances robustness against **asynchronous interference** (common in real-world settings due to clock drifts or variable delays).

---

###  **Why Time Matters: Implicit Regularization**

One insightful finding:
- Introducing **time offset flexibility** helps smooth out poor frequency choices.
- Even if two radars occasionally pick overlapping frequencies, slight time misalignments reduce collision impact.
- This makes the system more resilient without explicit coordination.

---

###  **Numerical Results**

Simulations in multi-radar scenarios show TF-NRH outperforms baselines in:
- **SINR (Signal-to-Interference-plus-Noise Ratio)** 
- **Collision rate** 
- **Range-Doppler map quality**  (less smearing, clearer targets)

Compared to:
- **Random time-frequency hopping**: No learning, baseline stochastic approach.
- **Centralized Nash-based schemes**: Require full information and coordination; less scalable.

Despite being decentralized, TF-NRH matches or exceeds centralized approaches in performance while avoiding communication overhead.

---

###  **Conclusion & Significance**

This work offers:
- A **principled, game-theoretic framework** for interference mitigation in automotive radar networks.
- A **practical algorithm (TF-NRH)** that balances autonomy, scalability, and performance.
- Strong **theoretical grounding** with convergence and regret guarantees.
- Demonstrated **robustness and superiority** in realistic simulations.

It represents a significant step toward **scalable, intelligent spectrum sharing** in next-generation ADAS and autonomous driving systems  especially important as vehicle density rises and spectrum congestion worsens.

---

###  Future Directions (Implied or Suggested)
- Implementation on real radar hardware.
- Extension to heterogeneous radar configurations (different bandwidths, chirp rates).
- Integration with V2X for hybrid cooperative-autonomous operation.
- Exploration of deep reinforcement learning alternatives within the same game-theoretic structure.

---

Let me know if you'd like a visual explanation, summary slide, or code implementation sketch! <div>
arXiv:2512.24619v1 Announce Type: new 
Abstract: Automotive FMCW radars are indispensable to modern ADAS and autonomous-driving systems, but their increasing density has intensified the risk of mutual interference. Existing mitigation techniques, including reactive receiver-side suppression, proactive waveform design, and cooperative scheduling, often face limitations in scalability, reliance on side-channel communication, or degradation of range-Doppler resolution. Building on our earlier work on decentralized Frequency-Domain No-Regret hopping, this paper introduces a unified time-frequency game-theoretic framework that enables radars to adapt across both spectral and temporal resources. We formulate the interference-avoidance problem as a repeated anti-coordination game, in which each radar autonomously updates a mixed strategy over frequency subbands and chirp-level time offsets using regret-minimization dynamics. We show that the proposed Time-Frequency No-Regret Hopping algorithm achieves vanishing external and swap regret, and that the induced empirical play converges to an $\varepsilon$-coarse correlated equilibrium or a correlated equilibrium. Theoretical analysis provides regret bounds in the joint domain, revealing how temporal adaptation implicitly regularizes frequency selection and enhances robustness against asynchronous interference. Numerical experiments with multi-radar scenarios demonstrate substantial improvements in SINR, collision rate, and range-Doppler quality compared with time-frequency random hopping and centralized Nash-based benchmarks.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Heterogeneous Multi-Agent Multi-Target Tracking using Cellular Sheaves</title>
<link>https://arxiv.org/abs/2512.24886</link>
<guid>https://arxiv.org/abs/2512.24886</guid>
<content:encoded><![CDATA[
<div> This arXiv preprint (2512.24886v1) presents a novel approach to **multi-agent target tracking** in complex, heterogeneous environments using **cellular sheaf theory**, a sophisticated mathematical framework that extends traditional graph-based models. Here's a breakdown and interpretation of the key contributions and implications:

---

### **Core Problem**
- **Multi-agent systems** are tasked with tracking **multiple unknown targets**.
- Challenges include:
  - **Nonlinear agent dynamics**
  - **Heterogeneity among agents**: agents may have different state spaces (e.g., drones vs. ground robots with different sensor suites or control inputs)
  - **External disturbances**
- Traditional methods (e.g., those based on **graph Laplacians**) struggle here because they assume homogeneous state spaces and linear interactions.

---

### **Key Innovation: Cellular Sheaves**
- The authors use **cellular sheaves**, which generalize graphs by assigning vector spaces (or more generally, algebraic structures) to nodes and edges, along with linear maps (restriction maps) between them.
- This allows modeling **heterogeneous agents** naturallyeach agent can have its own state dimension and dynamics.
- Prior work applied sheaves to cooperative tasks like consensus; this paper **extends sheaf-based coordination to non-cooperative settings**, specifically **target tracking**, where agents aim to follow external trajectories rather than agree with each other.

---

### **Methodology**
- **Formulates tracking as a harmonic extension problem**:
  - In sheaf theory, harmonic sections minimize a Dirichlet-like energy, representing "smoothest" global configurations consistent with local data.
  - Here, the desired formation or estimation around targets corresponds to such a harmonic section.
  - Observations from certain agents (e.g., those sensing a target) act as boundary conditions.
- Uses the **sheaf Laplacian** to derive a **decentralized control law**:
  - Each agent updates its state based only on local communication with neighbors.
  - The control law drives the system toward the harmonic extension that encodes optimal tracking estimates.
- Incorporates **nonlinear dynamics and disturbances** into the model, making it applicable to real-world scenarios.

---

### **Theoretical Contribution**
- Provides a **Lyapunov-based stability analysis** proving that the **tracking error converges to zero** under the proposed control law.
- This is significant because it shows robustness and correctness despite heterogeneity, nonlinearity, and distributed operation.

---

### **Validation**
- Results are supported by **simulations**, demonstrating effectiveness in realistic multi-agent tracking scenarios.

---

### **Significance and Implications**
- **Bridges abstract mathematics (topology/algebra) with robotics/control theory**.
- Enables design of **flexible, modular multi-agent systems** where diverse agents (different sensors, dynamics, dimensions) can still coordinate effectively.
- Opens the door for applying sheaf-theoretic tools to other **distributed estimation and control problems** beyond consensus.

---

### **Potential Applications**
- Surveillance networks with mixed UAVs, UGVs, and static sensors
- Search and rescue operations
- Environmental monitoring with heterogeneous robot teams

---

### **Conclusion**
This paper represents a **theoretical and practical advance** in multi-agent systems by leveraging cellular sheaves to solve a challenging instance of decentralized target tracking. It demonstrates how high-level mathematical frameworks can provide powerful tools for engineering problems involving heterogeneity and complexity.

> **In short:** A mathematically sophisticated yet practically grounded solution to multi-robot tracking using cellular sheavesgoing beyond consensus to real-world tasks with nonlinearities, noise, and diverse agents. <div>
arXiv:2512.24886v1 Announce Type: new 
Abstract: Multi-agent target tracking in the presence of nonlinear dynamics and agent heterogeneity, where state-space dimensions may differ, is a challenging problem that traditional graph Laplacian methods cannot easily address. This work leverages the framework of cellular sheaves, a mathematical generalization of graph theory, to natively model such heterogeneous systems. While existing coordination sheaf frameworks focus on cooperative problems like consensus, this work extends them to the non-cooperative target-tracking problem. The tracking of multiple, unknown targets is formulated as a harmonic extension problem on a cellular sheaf, accommodating nonlinear dynamics and external disturbances for all agents. A decentralized control law is developed using the sheaf Laplacian, and a corresponding Lyapunov-based stability analysis is provided to guarantee tracking error convergence, with results validated by simulation.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoK: Web3 RegTech for Cryptocurrency VASP AML/CFT Compliance</title>
<link>https://arxiv.org/abs/2512.24888</link>
<guid>https://arxiv.org/abs/2512.24888</guid>
<content:encoded><![CDATA[
<div> The arXiv preprint **2512.24888v1** presents a comprehensive systematization of **Web3 RegTech**regulatory technology tailored for decentralized environmentswith a focus on addressing **Anti-Money Laundering (AML)** and **Counter-Financing of Terrorism (CFT)** compliance in blockchain ecosystems. Below is a structured summary and analysis of the papers key contributions, findings, and implications:

---

###  **Core Problem**
Traditional RegTech is built for centralized financial systems with Know-Your-Customer (KYC) frameworks, auditable intermediaries, and regulated gateways. In contrast:
- **Web3 operates on decentralized, pseudonymous, and permissionless blockchains**, making identity attribution and transaction monitoring difficult.
- While blockchain transactions are transparent, their **lack of direct identity linkage** complicates regulatory oversight.

This creates a critical gap: how to enforce AML/CFT compliance without compromising Web3's core tenetsdecentralization, transparency, and user sovereignty.

---

###  Key Contributions

#### 1. **Three Taxonomies for Web3 RegTech**

| Taxonomy | Description |
|--------|-------------|
| **Regulatory Paradigm Evolution Framework (10 Dimensions)** | Maps the shift from traditional finance to Web3 across dimensions such as data provenance, identity models, enforcement mechanisms, auditability, etc. Highlights how regulation must evolve beyond custodial control to code-based, protocol-level governance. |
| **Compliance Protocol Taxonomy (5 Verification Layers)** | Proposes a layered model: <br /> **On-chain data layer** (raw transactions)<br /> **Entity abstraction layer** (wallet clustering, address labeling)<br /> **Behavioral analysis layer** (pattern detection, anomaly scoring)<br /> **Cross-chain correlation layer**<br /> **Privacy-preserving verification layer** (zero-knowledge proofs, secure multi-party computation) |
| **RegTech Lifecycle Framework** | Structures compliance into three phases:<br /> **Preventive**: Risk scoring before interaction (e.g., wallet screening)<br /> **Real-time**: Monitoring during transactions (e.g., sanctions filtering)<br /> **Investigative**: Forensic analysis post-event (e.g., chain tracing after illicit activity) |

These taxonomies provide a foundational structure for understanding, comparing, and designing Web3-native compliance tools.

---

#### 2. **Empirical Analysis**
- **41 commercial platforms** (e.g., Chainalysis, Elliptic, TRM Labs, CoinTracker)
- **28 academic prototypes** (from peer-reviewed literature, 20152025)

#####  Capabilities Enabled by Web3 RegTech
- **Transaction graph analysis**: Mapping complex flows across wallets and protocols.
- **Real-time risk assessment**: Dynamic scoring of addresses or transactions using ML models.
- **Cross-chain analytics**: Tracking value movement across Ethereum, Solana, Bitcoin, etc.
- **Privacy-preserving verification**: Using ZKPs or trusted execution environments (TEEs) to validate compliance without exposing sensitive data.

These capabilities often surpass those in traditional finance due to immutable ledgers and programmable logic.

---

#### 3. **Critical Gaps Identified**

| Gap | Explanation |
|-----|-----------|
| **Academic vs. Industry Divide** | Academic research explores advanced techniques (e.g., formal methods, ZK-compliance), but industry favors heuristic, scalable solutions. Limited transfer of innovation. |
| **Cross-chain Tracking** | Fragmented tooling; lack of standardized identifiers or interoperability protocols hinders end-to-end traceability. |
| **DeFi Interaction Complexity** | Flash loans, liquidity pools, and composability obscure fund origins. Current tools struggle with semantic interpretation of DeFi actions. |
| **Privacy Protocol Monitoring** | Tornado Cash-like mixers and privacy coins challenge traceability. Regulatory responses remain reactive rather than adaptive. |
| **Scalability & Performance** | Real-time analysis across high-throughput chains (e.g., Solana) demands significant computational resources. |

---

#### 4. **Architectural Best Practices & Design Principles**
The paper advocates for **decentralized, privacy-aware, and modular architectures**, including:
- **On-chain compliance oracles** that attest to risk scores or KYC status.
- **Zero-knowledge compliance proofs**: Prove non-involvement with sanctioned entities without revealing transaction details.
- **Decentralized identity (DID) integration**: Allow users to self-sovereignly disclose verified credentials.
- **Open-source, auditable algorithms** to maintain transparency and trustlessness.

Such designs align with Web3 values while enabling regulatory accountability.

---

###  Research Directions Proposed
1. **Unified Cross-Chain Identity Layer**: Develop persistent, portable reputation/identity constructs across chains.
2. **Semantic Transaction Understanding**: Use AI to interpret intent behind DeFi interactions (beyond pattern matching).
3. **Adaptive Privacy Detection**: Machine learning models trained on obfuscation patterns in privacy-preserving protocols.
4. **Regulation-as-Code (RaC)**: Smart contracts that encode and automatically enforce jurisdiction-specific rules.
5. **Federated Compliance Analytics**: Collaborative threat intelligence sharing without centralizing sensitive data.

---

###  Policy & Ethical Considerations
- **Balancing surveillance and privacy**: Overreach risks undermining decentralization and financial freedom.
- **Jurisdictional fragmentation**: Differing global regulations complicate uniform compliance standards.
- **Censorship resistance vs. regulatory compliance**: Tension between open access and blocking bad actors.

The authors emphasize **proportionality, minimal data collection, and user agency** as guiding principles.

---

###  Conclusion
This work establishes the first holistic framework for **Web3-native RegTech**, demonstrating both its transformative potential and current limitations. It bridges technical innovation with regulatory pragmatism, offering a roadmap for:
- Researchers: To build more deployable, cross-chain, and privacy-aware tools.
- Developers: To integrate compliance at the protocol level.
- Policymakers: To craft adaptive regulations that respect decentralization.

It positions Web3 not as a regulatory evasion tool, but as an opportunity to reimagine compliance through **transparency, automation, and cryptographic assurance**.

---

###  Suggested Next Steps
- Explore integration of **ZK-proofs for anonymous yet compliant transactions**.
- Evaluate **open-source alternatives** to proprietary forensic tools (e.g., OXT, Bitquery).
- Investigate **DAO-based governance of compliance rules** for community-driven regulation.

---

Let me know if you'd like:
- A visual diagram of the taxonomies,
- A comparison table of selected commercial vs. academic platforms,
- Or a summary suitable for policymakers or developers. <div>
arXiv:2512.24888v1 Announce Type: new 
Abstract: The decentralized architecture of Web3 technologies creates fundamental challenges for Anti-Money Laundering and Counter-Financing of Terrorism compliance. Traditional regulatory technology solutions designed for centralized financial systems prove inadequate for blockchain's transparent yet pseudonymous networks. This systematization examines how blockchain-native RegTech solutions leverage distributed ledger properties to enable novel compliance capabilities.
  We develop three taxonomies organizing the Web3 RegTech domain: a regulatory paradigm evolution framework across ten dimensions, a compliance protocol taxonomy encompassing five verification layers, and a RegTech lifecycle framework spanning preventive, real-time, and investigative phases. Through analysis of 41 operational commercial platforms and 28 academic prototypes selected from systematic literature review (2015-2025), we demonstrate that Web3 RegTech enables transaction graph analysis, real-time risk assessment, cross-chain analytics, and privacy-preserving verification approaches that are difficult to achieve or less commonly deployed in traditional centralized systems.
  Our analysis reveals critical gaps between academic innovation and industry deployment, alongside persistent challenges in cross-chain tracking, DeFi interaction analysis, privacy protocol monitoring, and scalability. We synthesize architectural best practices and identify research directions addressing these gaps while respecting Web3's core principles of decentralization, transparency, and user sovereignty.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Market Making for Energy Sharing</title>
<link>https://arxiv.org/abs/2512.24432</link>
<guid>https://arxiv.org/abs/2512.24432</guid>
<content:encoded><![CDATA[
<div> The abstract you've provided appears to be a fictional or speculative document, as arXiv preprints with the identifier `2512.24432v1` do not currently exist (as of knowledge cutoff in 2024), and the date implied by the ID (December 2025) is in the future. However, assuming this is a hypothetical or forward-looking research abstract, lets break it down and interpret its key contributions and implications:

---

### **Summary and Interpretation of the Abstract**

**Title (inferred):**  
*An Axiomatic Theory of Automated Market Makers in Local Energy Sharing Markets: A Mean-Field Game Approach*

---

#### **Core Contributions:**

1. **Axiomatic Framework for AMMs in Energy Markets:**  
   The paper proposes an axiomatic foundation for designing Automated Market Makers (AMMs)a concept borrowed from decentralized finance (DeFi)for application in *local energy sharing markets*. These are peer-to-peer networks where prosumers (consumers who also produce energy, e.g., via solar panels) trade excess energy locally.

2. **Mean-Field Game Model of Prosumer Behavior:**  
   To analyze strategic interactions among a large number of heterogeneous prosumers, the authors model the system as a **Mean-Field Game (MFG)**. In this setting:
   - Each prosumer solves a dynamic optimization problem via a **Bellman equation**.
   - Decisions include **energy consumption**, **storage management**, and **energy trading** with the local market or grid.

3. **Key Axioms Lead to Linear, Lipschitz Payment Functions:**  
   The axiomatic approach identifies desirable properties that fair and efficient mechanisms should satisfy. From these, the authors derive that valid payment rules must be:
   - **Linear** and **Lipschitz continuous** (ensuring stability and bounded sensitivity).
   - Prices decrease as the **aggregate supply-to-demand ratio** increases  intuitively, more surplus supply lowers prices.

4. **Design Conditions from DeFi Adapted to Energy:**  
   The paper shows that incorporating two innovations from DeFi improves mechanism design:
   - **Batch execution**: Trading occurs in discrete time batches, reducing arbitrage and gaming.
   - **Concentrated liquidity**: Liquidity providers (here, prosumers with stored energy) can allocate their capacity strategically in price ranges.
   
   Under these features, standard economic conditions**quasi-concavity**, **monotonicity**, and **homotheticity**can be used to build AMMs satisfying the proposed axioms.

5. **Efficiency and Budget Balance:**  
   The resulting AMM mechanism is:
   - **Budget-balanced**: No external subsidies required; payments clear within the system.
   - **Ex-ante efficient**: Optimizes expected social welfare before uncertainty resolves.
   - Contrasts with VCG mechanisms, which are strategy-proof and ex-post efficient but often not budget-balanced and vulnerable to collusion.

6. **Potential Game Structure Enables Equilibrium Solution:**  
   Because the game induced by the AMM is a **Potential Game**, the Markov Perfect Equilibrium (MPE) can be found by:
   - First solving the **social planners problem** (centralized optimum).
   - Then **decentralizing** the solution via individual incentives aligned through the AMM.

7. **Numerical Validation on Paris Data:**  
   Simulations using real-world data from the Paris region show that:
   - Prosumer communities achieve **up to 40% gains from trade** compared to relying solely on the main power grid.
   - Highlights the economic value of localized, peer-to-peer energy exchange enabled by well-designed AMMs.

---

### **Significance and Implications**

- **Cross-Disciplinary Innovation:**  
  This work bridges **blockchain-inspired financial mechanisms** (AMMs) with **smart grid and energy economics**, demonstrating how DeFi concepts can be repurposed for sustainable infrastructure.

- **Scalable Decentralized Energy Markets:**  
  By avoiding complex auction mechanisms like VCG (which dont scale well and fail budget balance), AMMs offer a practical, decentralized alternative for managing distributed energy resources.

- **Policy and Design Guidance:**  
  The axiomatic approach provides principles for regulators and platform designers to create fair, stable, and efficient local energy markets.

- **Future-Ready Grids:**  
  As renewable generation becomes more decentralized, such mechanisms could play a crucial role in balancing supply and demand at the neighborhood level, reducing grid congestion and promoting resilience.

---

### **Critical Questions & Research Directions**
- How robust are these results to strategic manipulation or incomplete information?
- Can privacy-preserving implementations (e.g., via zero-knowledge proofs or federated learning) be integrated?
- What are the regulatory and institutional barriers to deploying such systems?

---

### Conclusion

While the paper reference appears speculative, its conceptual framework reflects a compelling direction for future research: applying advanced mechanism design from decentralized finance to pressing challenges in energy sustainability. If realized, such AMM-based energy markets could significantly enhance efficiency, equity, and decarbonization in urban power systems.

Let me know if you'd like a simplified explanation, a visualization idea, or help drafting a related research proposal! <div>
arXiv:2512.24432v1 Announce Type: cross 
Abstract: We develop an axiomatic theory for Automated Market Makers (AMMs) in local energy sharing markets and analyze the Markov Perfect Equilibrium of the resulting economy with a Mean-Field Game. In this game, heterogeneous prosumers solve a Bellman equation to optimize energy consumption, storage, and exchanges. Our axioms identify a class of mechanisms with linear, Lipschitz continuous payment functions, where prices decrease with the aggregate supply-to-demand ratio of energy. We prove that implementing batch execution and concentrated liquidity allows standard design conditions from decentralized finance-quasi-concavity, monotonicity, and homotheticity-to construct AMMs that satisfy our axioms. The resulting AMMs are budget-balanced and achieve ex-ante efficiency, contrasting with the strategy-proof, expost optimal VCG mechanism. Since the AMM implements a Potential Game, we solve its equilibrium by first computing the social planner's optimum and then decentralizing the allocation. Numerical experiments using data from the Paris administrative region suggest that the prosumer community can achieve gains from trade up to 40% relative to the grid-only benchmark.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>A New Decomposition Paradigm for Graph-structured Nonlinear Programs via Message Passing</title>
<link>https://arxiv.org/abs/2512.24676</link>
<guid>https://arxiv.org/abs/2512.24676</guid>
<content:encoded><![CDATA[
<div> The paper **arXiv:2512.24676v1** introduces **MP-Jacobi (Message Passing-Jacobi)**, a novel decentralized optimization framework designed for **finite-sum nonlinear programs** with **local interactions governed by a graph or hypergraph structure**. Here's a structured breakdown and interpretation of the key contributions and concepts:

---

###  Problem Setting
- **Finite-sum optimization**: Objective function is a sum of local cost terms.
- **Graph/hypergraph-structured dependencies**: Decision variables interact locallyeach variable participates only in certain constraints or objective terms, forming a network structure.
- Applications include distributed control, sensor networks, multi-agent systems, and large-scale machine learning where data and decisions are spatially or logically distributed.

---

###  MP-Jacobi Framework

#### Core Idea:
Combine two powerful ideas:
1. **Min-sum message passing** (from probabilistic inference and dynamic programming) to capture intra-cluster optimization structure.
2. **Jacobi-type block updates** (parallel coordinate descent-style steps) to handle inter-cluster couplings using neighbors latest estimates.

#### Architecture:
- The underlying (hyper)graph is partitioned into **tree-structured clusters**.
- Each cluster acts as an "agent" that performs local computation.

#### Iteration Steps:
At each iteration:
1. **Intra-cluster processing**: Within each tree cluster, perform a single **min-sum sweep** (forward-backward message passing) to compute **cost-to-go messages**, effectively solving a localized approximation of the subproblem.
2. **Inter-cluster coordination**: Use **Jacobi updates** to account for coupling between clusters, based on the most recent iterates from neighboring clusters.
3. All agents update **in parallel**, enabling scalability.

#### Communication:
- Only requires **single-hop communication** (between direct neighbors), making it suitable for decentralized, low-bandwidth environments.
- Despite operating on **loopy graphs** (which typically cause convergence issues in message-passing methods), MP-Jacobi ensures **convergence** due to its hybrid design.

---

###  Theoretical Contributions

For **strongly convex objectives**:
- **Global linear convergence** is established.
- **Explicit convergence rates** are derived, which depend on:
  - **Curvature** (strong convexity and smoothness parameters),
  - **Coupling strength** between clusters,
  - **Cluster partitioning strategy** (e.g., size, overlap, tree width).

These results provide **design guidance for clustering**: how to partition the graph to optimize convergence speed and scalability.

---

###  Practical Enhancements

To reduce computational and communication burden:
- **Surrogate models** are introduced for message updates:
  - Maintain convergence guarantees.
  - Reduce per-iteration complexity (e.g., via approximated or delayed messages).
  - Preserve graph compliance (i.e., respect the decentralized architecture).

#### Extension to Hypergraphs:
- Handles more complex, overlapping interactions (e.g., higher-order potentials).
- In regimes with heavy hyperedge overlap, proposes a **hyperedge-splitting scheme**:
  - Enables finite-time intra-cluster message computation.
  - Maintains convergence under surrogates.

This makes MP-Jacobi applicable to problems like distributed MPC, federated learning with shared features, or structured prediction.

---

###  Experimental Validation
- Evaluations show consistent outperformance over **decentralized gradient methods** (e.g., DGD, ADMM variants).
- Faster convergence, better scalability with problem size and network density.
- Demonstrates robustness across different graph topologies and partitions.

---

###  Key Innovations & Implications

| Feature | Significance |
|-------|--------------|
| **Hybrid min-sum + Jacobi** | Unifies dynamic programming-style reasoning with iterative optimization |
| **Tree clustering** | Enables exact efficient message passing within clusters |
| **Parallel updates** | Fully decentralized, asynchronous-friendly |
| **Convergence on loopy graphs** | Overcomes a major limitation of traditional belief propagation |
| **Guidance for clustering** | Theory-informed partitioning improves performance |
| **Scalable surrogates** | Makes method practical for large clusters |

---

###  Potential Applications
- Multi-robot coordination
- Smart grid optimization
- Federated learning with structured regularization
- Distributed model predictive control (MPC)
- Sensor fusion networks

---

###  Future Directions (implied or possible)
- Asynchronous variants.
- Stochastic extensions for online or sampled-data settings.
- Adaptive clustering strategies.
- Integration with neural parameterizations (learning-enhanced message passing).

---

### Conclusion

**MP-Jacobi** represents a significant step toward **scalable, principled decentralized optimization** over complex networked structures. By combining **message passing for intra-cluster coherence** and **Jacobi updates for inter-cluster consistency**, it achieves both **theoretical rigor** and **practical efficiency**, especially in strongly convex settings. Its extension to hypergraphs further broadens its applicability to real-world problems with higher-order interactions.

> This work bridges ideas from optimization, message passing, and graphical models, offering a promising direction for next-generation distributed algorithms.

--- 

Let me know if you'd like a simplified explanation, pseudocode sketch, or comparison with ADMM/DGD! <div>
arXiv:2512.24676v1 Announce Type: cross 
Abstract: We study finite-sum nonlinear programs whose decision variables interact locally according to a graph or hypergraph. We propose MP-Jacobi (Message Passing-Jacobi), a graph-compliant decentralized framework that couples min-sum message passing with Jacobi block updates. The (hyper)graph is partitioned into tree clusters. At each iteration, agents update in parallel by solving a cluster subproblem whose objective decomposes into (i) an intra-cluster term evaluated by a single min-sum sweep on the cluster tree (cost-to-go messages) and (ii) inter-cluster couplings handled via a Jacobi correction using neighbors' latest iterates. This design uses only single-hop communication and yields a convergent message-passing method on loopy graphs.
  For strongly convex objectives we establish global linear convergence and explicit rates that quantify how curvature, coupling strength, and the chosen partition affect scalability and provide guidance for clustering. To mitigate the computation and communication cost of exact message updates, we develop graph-compliant surrogates that preserve convergence while reducing per-iteration complexity. We further extend MP-Jacobi to hypergraphs; in heavily overlapping regimes, a surrogate-based hyperedge-splitting scheme restores finite-time intra-cluster message updates and maintains convergence. Experiments validate the theory and show consistent improvements over decentralized gradient baselines.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>FEDSTR: Money-In AI-Out | A Decentralized Marketplace for Federated Learning and LLM Training on the NOSTR Protocol</title>
<link>https://arxiv.org/abs/2404.15834</link>
<guid>https://arxiv.org/abs/2404.15834</guid>
<content:encoded><![CDATA[
<div> The abstract you've provided describes a research paper (arXiv:2404.15834v2) that proposes leveraging **Nostr**  a decentralized, open-source communication protocol based on WebSockets  as the foundation for a **decentralized marketplace for federated learning and large language model (LLM) training**.

Here's a breakdown and analysis of the key points in the abstract:

---

###  **Overview of Nostr**
- **Nostr** (**Notes and Other Stuff Transmitted by Relays**) is a lightweight, decentralized social networking protocol standardized by the [W3C](https://www.w3.org/) and built on public-key cryptography and relay-based message distribution.
- It supports various data types (e.g., text notes, direct messages, file metadata, etc.) via standardized event types.
- Its architecture is highly resilient to censorship due to its lack of central servers; anyone can run a relay, and clients fetch data from multiple sources.

---

###  **Proposed Use Case: Decentralized AI Training Marketplace**
The authors propose extending Nostr beyond social communication into the domain of **AI/ML collaboration**, specifically:
- A **federated learning marketplace**, where:
  - **Customers (data owners)** submit datasets (or references to them) they wish to use for training AI models.
  - **Service providers (compute nodes)** download parts of the dataset, perform local model training, and return updated model parameters (not raw data), preserving privacy.
  - In exchange, service providers receive **payments**, likely facilitated off-chain or through blockchain-integrated methods (e.g., Bitcoin Lightning Network, which is already used in Nostr ecosystems).

This creates a trustless, decentralized environment for collaborative machine learning without relying on centralized platforms like AWS, Hugging Face, or centralized AI marketplaces.

---

###  **Key Features Enabled by Nostr**
1. **Decentralization & Censorship Resistance**
   - Since Nostr runs on independently operated relays, no single entity can block transactions or interactions.
   - Ideal for global participation in AI development, especially under restrictive regimes.

2. **Trustless Identity & Authentication**
   - All participants are identified via public keys (similar to blockchain wallets).
   - Signatures ensure integrity and non-repudiation of events (e.g., job postings, model updates).

3. **Event-Based Communication**
   - Training requests, dataset pointers, model updates, and payment confirmations can be encoded as Nostr events.
   - Example: Custom `kind` numbers could represent:
     - `kind=30400`: Federated Learning Task Request
     - `kind=30401`: Model Update Submission
     - `kind=30402`: Payment Receipt

4. **Scalability via Public Relays**
   - The proof-of-concept uses existing public Nostr relays, showing feasibility without infrastructure overhead.

---

###  **Potential Applications Mentioned**
Beyond basic messaging, Nostr already supports:
- Direct messaging
- File sharing (via encrypted links or IPFS hashes)
- Audio/video streaming
- Collaborative writing
- Blogging
- Distributed AI directories

This work builds on these capabilities to enable **distributed AI computation coordination**.

---

###  **Significance & Innovation**
- **Democratizes AI training**: Lowers barriers for individuals and small organizations to train models using distributed compute resources.
- **Privacy-preserving**: Federated approach means raw data doesnt leave the customers control (or only shared securely with workers).
- **Open and permissionless**: Anyone with bandwidth and compute can become a service provider.
- **Synergy with crypto economy**: Naturally integrates with Bitcoin/Lightning for micropayments.

---

###  **Challenges & Considerations**
While promising, several challenges remain:
| Challenge | Description |
|--------|-------------|
| **Data Size & Bandwidth** | Large datasets may not fit well within Nostrs current event size limits (~natively up to 64KB per event). Solution: Use Nostr only for coordination; store data off-relay (e.g., IPFS, S3, Bittorrent). |
| **Model Integrity & Cheating** | How to verify honest training? May require reputation systems or zero-knowledge proofs. |
| **Payment Finality & Dispute Resolution** | Trustless payments need smart contracts or multisig escrows. |
| **Latency & Relay Performance** | Not all relays are optimized for high-frequency ML syncs. |
| **Standardization** | Requires community adoption of new event kinds and metadata formats. |

---

###  **Conclusion**
This paper presents a **novel fusion of decentralized communication and decentralized machine learning**, demonstrating how protocols like **Nostr** can evolve beyond social media into foundational layers for next-generation AI infrastructure.

By combining **federated learning**, **decentralized identity**, and **peer-to-peer coordination**, it opens the door to:
- Censorship-resistant AI development
- Community-owned LLM training collectives
- Open-source alternatives to Big Tech AI pipelines

With further development, such a system could empower grassroots innovation in artificial intelligence.

---

If you'd like, I can help:
- Summarize the full paper (if available),
- Draft a response or review,
- Design the proposed event schema for Nostr-based FL,
- Or simulate a workflow for task posting and model aggregation.

Let me know how you'd like to proceed! <div>
arXiv:2404.15834v2 Announce Type: replace 
Abstract: The NOSTR is a communication protocol for the social web, based on the w3c websockets standard. Although it is still in its infancy, it is well known as a social media protocol, with thousands of trusted users and multiple user interfaces, offering a unique experience and enormous capabilities. To name a few, the NOSTR applications include but are not limited to direct messaging, file sharing, audio/video streaming, collaborative writing, blogging and data processing through distributed AI directories. In this work, we propose an approach that builds upon the existing protocol structure with end goal a decentralized marketplace for federated learning and LLM training. In this proposed design there are two parties: on one side there are customers who provide a dataset that they want to use for training an AI model. On the other side, there are service providers, who receive (parts of) the dataset, train the AI model, and for a payment as an exchange, they return the optimized AI model. To demonstrate viability, we present a proof-of-concept implementation over public NOSTR relays. The decentralized and censorship resistant features of the NOSTR enable the possibility of designing a fair and open marketplace for training AI models and LLMs.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transfer learning of state-based potential games for process optimization in decentralized manufacturing systems</title>
<link>https://arxiv.org/abs/2408.05992</link>
<guid>https://arxiv.org/abs/2408.05992</guid>
<content:encoded><![CDATA[
<div> The arXiv paper **2408.05992v3** introduces a novel approach called **Transfer Learning in State-based Potential Games (TL-SbPGs)**, aimed at enabling distributed self-optimization in manufacturing systems through online learning and knowledge sharing.

Here is a structured summary and interpretation of the key contributions and implications of this work:

---

###  **Core Idea**
The paper proposes **TL-SbPGs**, an extension of state-based potential games (SbPGs), integrating **transfer learning** to improve decentralized decision-making in large-scale industrial environments. The focus is on **online transfer learning**, where agents ("players") can adaptively reuse policies learned by similar players to accelerate their own learning and optimize system-wide performance.

---

###  **Key Contributions**

1. **Online Transfer Learning Framework for SbPGs**  
   - Extends traditional SbPGs with mechanisms for transferring knowledge between players.
   - Enables **distributed self-optimization** without centralized controlcritical for scalable smart manufacturing.

2. **Similarity Criteria Between Players**  
   Two settings are proposed to determine which knowledge to transfer:
   - **(a) Predefined Similarities**: Based on known system structure or prior domain knowledge (e.g., identical machine types).
   - **(b) Dynamically Inferred Similarities**: Learned during training using behavioral or state-action trajectory analysis, allowing adaptation to changing environments.

3. **Formal Justification of Transferability in SbPGs**  
   - Proves that under certain conditions, policy transfer across similar players preserves convergence properties of potential games.
   - Ensures theoretical soundness when applying transfer learning within game-theoretic learning frameworks.

4. **Optimization of Transfer Timing and Weighting**  
   - Introduces a method to decide *when* and *how much* to transfer (i.e., adaptive weighting of source policies).
   - Prevents negative transfer by modulating influence based on similarity and performance feedback.

5. **Experimental Validation on a Laboratory Testbed**
   - Real-world evaluation demonstrates:
     - **Improved production efficiency**
     - **Reduced power consumption**
   - Outperforms vanilla SbPGs (without transfer), showing faster convergence and better resource utilization.

---

###  **Practical Implications for Industry**
- **Scalable Learning in Smart Factories**: TL-SbPGs allow machines or subsystems to learn optimal control policies more quickly by leveraging experience from peers.
- **Energy Efficiency**: Demonstrated reduction in power usage aligns with sustainable manufacturing goals.
- **Decentralized Adaptation**: Suitable for flexible manufacturing systems where reconfiguration is frequent and centralized optimization is impractical.

---

###  **Significance of "Replace" Announcement Type**
The `replace` type indicates this version (**v3**) supersedes earlier versions of the same paper (v1, v2), likely incorporating revisions such as:
- Expanded experiments
- Refined theoretical proofs
- Improved clarity in methodology

This suggests the authors have responded to peer feedback or further validated their approach.

---

###  **Potential Applications Beyond Manufacturing**
While focused on manufacturing, TL-SbPGs could be applied to other domains involving decentralized multi-agent optimization:
- Smart grids
- Traffic signal control
- Distributed robotics
- Supply chain coordination

---

###  **Conclusion**
This paper makes a strong case for integrating **transfer learning** into **game-theoretic multi-agent systems**, particularly in real-world industrial contexts. By formalizing how and when agents should share knowledge, TL-SbPGs offer a robust, efficient, and scalable solution for online optimization in complex, dynamic environments.

>  **Takeaway**: TL-SbPGs bridge game theory and transfer learning to enable faster, greener, and smarter distributed decision-making in manufacturingdemonstrating both theoretical rigor and practical impact.

Let me know if you'd like a simplified explanation, a diagram proposal, or help comparing it to related methods (e.g., MADRL, standard POT games). <div>
arXiv:2408.05992v3 Announce Type: replace 
Abstract: This paper presents a novel online transfer learning approach in state-based potential games (TL-SbPGs) for distributed self-optimization in manufacturing systems. The approach targets practical industrial scenarios where knowledge sharing among similar players enhances learning in large-scale and decentralized environments. TL-SbPGs enable players to reuse learned policies from others, which improves learning outcomes and accelerates convergence. To accomplish this goal, we develop transfer learning concepts and similarity criteria for players, which offer two distinct settings: (a) predefined similarities between players and (b) dynamically inferred similarities between players during training. The applicability of the SbPG framework to transfer learning is formally established. Furthermore, we present a method to optimize the timing and weighting of knowledge transfer. Experimental results from a laboratory-scale testbed show that TL-SbPGs improve production efficiency and reduce power consumption compared to vanilla SbPGs.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tazza: Shuffling Neural Network Parameters for Secure and Private Federated Learning</title>
<link>https://arxiv.org/abs/2412.07454</link>
<guid>https://arxiv.org/abs/2412.07454</guid>
<content:encoded><![CDATA[
<div> The provided text appears to be the abstract of a research paper with arXiv identifier **2412.07454v3**, which has been updated (as indicated by "Announce Type: replace"). Here's a breakdown and interpretation of the abstract for clarity:

---

**Paper Title (Inferred):** *Tazza: A Secure and Efficient Framework for Federated Learning*

**Key Contributions:**

- **Problem:** Federated learning (FL) enables decentralized training while preserving data privacy by avoiding raw data sharing. However, it remains vulnerable to two major security threats:
  - **Gradient inversion attacks**: Where adversaries reconstruct private client data from shared gradients.
  - **Model poisoning attacks**: Where malicious clients manipulate model updates to degrade performance or inject backdoors.

  Most existing defenses tackle only one of these issues, often at the cost of accuracy or system efficiency.

- **Solution:** The authors propose **Tazza**, a novel FL framework that **simultaneously defends against both gradient inversion and model poisoning** without sacrificing model accuracy or computational efficiency.

- **Core Techniques:**
  - **Weight Shuffling**: Exploits *permutation equivariance/invariance* in neural networksreordering neuron weights does not affect functionalityto obscure model parameters and resist inversion attacks.
  - **Shuffled Model Validation**: Uses shuffling strategies during aggregation to detect and filter out poisoned models, improving robustness against adversarial clients.

- **Benefits:**
  - **Enhanced Security**: Resists diverse poisoning attacks and protects data confidentiality.
  - **High Accuracy**: Maintains model performance close to standard FL baselines.
  - **Efficiency**: Up to **6.7 faster** computation compared to alternative secure FL methods, making it suitable for deployment on **resource-constrained embedded platforms**.

- **Evaluation:**
  - Tested across multiple datasets and real-world embedded systems.
  - Demonstrates strong defense capabilities while maintaining scalability and practicality.

---

**Significance:**  
Tazza represents an integrated approach to secure federated learning, addressing two critical attack vectors in a unified mannersomething many prior works fail to achieve. Its emphasis on **efficiency** makes it particularly promising for edge computing and IoT applications where resources are limited.

---

If youre looking for more details, such as implementation specifics, threat models considered, or comparison benchmarks, I recommend downloading the full paper from [arXiv:2412.07454](https://arxiv.org/abs/2412.07454). Let me know if you'd like help summarizing specific sections or understanding technical components like permutation equivariance! <div>
arXiv:2412.07454v3 Announce Type: replace 
Abstract: Federated learning enables decentralized model training without sharing raw data, preserving data privacy. However, its vulnerability towards critical security threats, such as gradient inversion and model poisoning by malicious clients, remain unresolved. Existing solutions often address these issues separately, sacrificing either system robustness or model accuracy. This work introduces Tazza, a secure and efficient federated learning framework that simultaneously addresses both challenges. By leveraging the permutation equivariance and invariance properties of neural networks via weight shuffling and shuffled model validation, Tazza enhances resilience against diverse poisoning attacks, while ensuring data confidentiality and high model accuracy. Comprehensive evaluations on various datasets and embedded platforms show that Tazza achieves robust defense with up to 6.7x improved computational efficiency compared to alternative schemes, without compromising performance.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SlideChain: Semantic Provenance for Lecture Understanding via Blockchain Registration</title>
<link>https://arxiv.org/abs/2512.21684</link>
<guid>https://arxiv.org/abs/2512.21684</guid>
<content:encoded><![CDATA[
<div> The abstract you've provided describes **arXiv:2512.21684v2**, a research paper introducing **SlideChain**, a novel framework that leverages blockchain technology to ensure the **integrity, reproducibility, and auditability** of semantic content extracted by visionlanguage models (VLMs) from educational materialsspecifically medical imaging lecture slides.

Here is a clear breakdown and interpretation of the key contributions and findings:

---

###  **Problem Statement**
- **Challenge**: Modern VLMs are increasingly used in education (especially STEM/medical fields), but their outputs are:
  - Hard to verify
  - Prone to inconsistencies across models, inference settings, and environments
  - Lacking long-term reproducibility and traceability
- This undermines trust in AI-generated educational content, particularly where accuracy is critical (e.g., medicine).

---

###  **Solution: SlideChain**
A **blockchain-backed provenance framework** designed for:
- Verifiable integrity of multimodal semantic extraction
- Tamper-evident logging of model outputs
- Long-term auditability

#### Key Components:
1. **Dataset**: 
   - *SlideChain Slides Dataset*: 1,117 real medical imaging lecture slides from a university course.
2. **Semantic Extraction**:
   - Four state-of-the-art VLMs extract:
     - Key concepts
     - Relational triples (subjectpredicateobject)
   - Output structured into **provenance records** per slide.
3. **Blockchain Anchoring**:
   - Cryptographic hashes of these records are stored on a **local EVM-compatible blockchain**.
   - Ensures:
     - Immutability
     - Tamper detection
     - Deterministic reproducibility

---

###  **Key Findings & Analysis**
1. **Cross-Model Discrepancies**:
   - Low overlap in extracted **concepts** across different VLMs.
   - Near-zero agreement in generated **relational triples** on many slides.
   - Highlights the **lack of consistency** even among SOTA models  major concern for reliability.

2. **Lecture-Level Variability**:
   - First systematic analysis showing how semantic output varies not just between models, but also across lectures/slide topics.
   - Suggests domain-specific challenges in multimodal understanding.

3. **System Performance**:
   - Evaluated under simulated deployment:
     - Measured **gas usage**, **throughput**, and **scalability**.
   - Results show:
     - Efficient operation at scale
     - **Perfect tamper detection**
     - **Deterministic reproducibility** across independent runs

---

###  **Implications & Contributions**
- **Trustworthy AI in Education**:
  - SlideChain enables verifiable, auditable pipelines for AI-assisted teaching tools.
- **Reproducibility**:
  - By anchoring semantic outputs on-chain, future users can validate or compare results against a persistent baseline.
- **Scalable Design**:
  - Local EVM setup balances security and practicality without requiring public blockchain costs or latency.
- **New Research Direction**:
  - Opens path for studying semantic drift, model alignment, and accountability in multimodal systems.

---

###  Conclusion
> *SlideChain represents a significant step toward trustworthy, transparent, and accountable use of VLMs in high-stakes educational contexts.*  
It addresses core issues of **semantic inconsistency** and **lack of provenance**, offering a scalable solution using blockchain to secure AI-generated knowledge.

---

###  Potential Applications
- Medical education platforms using AI tutors
- Regulatory compliance for AI-generated courseware
- Model comparison benchmarks with verified outputs
- Archival systems for evolving curricula with AI assistance

---

If you're interested, I can help with:
- Summarizing the full paper (once published)
- Explaining technical aspects (e.g., how hashing + Ethereum works here)
- Comparing SlideChain to other provenance/blockchain-AI systems
- Generating a presentation or infographic based on this work

Let me know your needs! <div>
arXiv:2512.21684v2 Announce Type: replace 
Abstract: Modern vision--language models (VLMs) are increasingly used to interpret and generate educational content, yet their semantic outputs remain challenging to verify, reproduce, and audit over time. Inconsistencies across model families, inference settings, and computing environments undermine the reliability of AI-generated instructional material, particularly in high-stakes and quantitative STEM domains. This work introduces SlideChain, a blockchain-backed provenance framework designed to provide verifiable integrity for multimodal semantic extraction at scale. Using the SlideChain Slides Dataset-a curated corpus of 1,117 medical imaging lecture slides from a university course-we extract concepts and relational triples from four state-of-the-art VLMs and construct structured provenance records for every slide. SlideChain anchors cryptographic hashes of these records on a local EVM (Ethereum Virtual Machine)-compatible blockchain, providing tamper-evident auditability and persistent semantic baselines. Through the first systematic analysis of semantic disagreement, cross-model similarity, and lecture-level variability in multimodal educational content, we reveal pronounced cross-model discrepancies, including low concept overlap and near-zero agreement in relational triples on many slides. We further evaluate gas usage, throughput, and scalability under simulated deployment conditions, and demonstrate perfect tamper detection along with deterministic reproducibility across independent extraction runs. Together, these results show that SlideChain provides a practical and scalable step toward trustworthy, verifiable multimodal educational pipelines, supporting long-term auditability, reproducibility, and integrity for AI-assisted instructional systems.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Multi-Task Clustering</title>
<link>https://arxiv.org/abs/2512.22897</link>
<guid>https://arxiv.org/abs/2512.22897</guid>
<content:encoded><![CDATA[
<div> The provided text appears to be the **abstract** of a research paper with the arXiv identifier **2512.22897v2**, which indicates it's the second version (revised) of the preprint. Here's a breakdown and interpretation of the abstract, followed by a concise summary suitable for understanding or presentation:

---

###  **Breakdown of the Abstract**

#### **Problem Context**
- **Spectral clustering** is known for high performance in clustering tasks.
- Traditional methods assume **centralized data**, which doesn't align with modern **decentralized/federated environments** where data privacy is critical.
- Existing **federated clustering approaches** suffer from:
  - Use of **unreliable pseudo-labels**, hurting generalization.
  - Inability to model **latent correlations among heterogeneous clients**.

#### **Proposed Solution: FMTC (Federated Multi-Task Clustering)**
A novel framework designed for **personalized yet collaborative clustering** under federation:
1. **Client-Side Personalized Clustering Module**
   - Learns a **parameterized mapping function** (e.g., neural network) per client.
   - Enables **robust out-of-sample inference** (can cluster new samples without re-computation).
   - Avoids reliance on **pseudo-labels**, improving reliability.

2. **Server-Side Tensorial Correlation Module**
   - Models inter-client relationships via **tensor representation**: all client models are organized into a single tensor.
   - Applies **low-rank regularization** to uncover a **shared latent subspace** across clients.
   - Captures **common structure** while respecting individual heterogeneity.

#### **Optimization & Privacy**
- Uses **ADMM (Alternating Direction Method of Multipliers)** to solve the joint optimization problem.
- The algorithm enables:
  - **Distributed computation**: local updates on clients.
  - **Secure aggregation**: server coordinates without accessing raw data.
  - **Privacy preservation**: no direct exchange of sensitive information.

#### **Evaluation**
- Tested on **multiple real-world datasets**.
- Outperforms various **baseline and state-of-the-art federated clustering methods** in effectiveness and robustness.

---

###  **Summary (Suitable for Presentation or Report)**

> This paper introduces **Federated Multi-Task Clustering (FMTC)**, a novel framework addressing key limitations in current federated clustering methods. FMTC enables effective, personalized clustering in decentralized settings by combining **client-specific modeling** with **collaborative learning of shared structures**. It avoids error-prone pseudo-labels through parameterized clustering mappings and captures cross-client correlations using a **tensor-based low-rank regularization** approach. Leveraging an efficient **ADMM-based distributed optimization**, FMTC ensures privacy-preserving collaboration. Experiments show that FMTC significantly outperforms existing methods across diverse real-world datasets, marking a substantial advance in federated unsupervised learning.

---

###  Key Contributions
1. **First use of tensor modeling** in federated clustering to capture multi-client correlations.
2. Eliminates dependence on **pseudo-labels**, enhancing robustness.
3. Enables **out-of-sample extension** via learnable mappings.
4. Integrates **personalization and global coordination** via low-rank tensor constraints.
5. Provides a **privacy-preserving, scalable ADMM algorithm** for federated clustering.

---

###  Potential Applications
- Healthcare (multi-institutional patient grouping without sharing records)
- IoT networks (device behavior clustering across edge nodes)
- Finance (customer segmentation across isolated branches)

---

If you'd like, I can help you:
- Generate a **title suggestion** for this work,
- Write a **blog post or slide deck** summarizing the paper,
- Compare FMTC to other federated clustering methods (like FedEM, CFL),
- Explain the **ADMM derivation** or **tensor formulation** in more detail.

Let me know how you'd like to proceed! <div>
arXiv:2512.22897v2 Announce Type: replace 
Abstract: Spectral clustering has emerged as one of the most effective clustering algorithms due to its superior performance. However, most existing models are designed for centralized settings, rendering them inapplicable in modern decentralized environments. Moreover, current federated learning approaches often suffer from poor generalization performance due to reliance on unreliable pseudo-labels, and fail to capture the latent correlations amongst heterogeneous clients. To tackle these limitations, this paper proposes a novel framework named Federated Multi-Task Clustering (i.e.,FMTC), which intends to learn personalized clustering models for heterogeneous clients while collaboratively leveraging their shared underlying structure in a privacy-preserving manner. More specifically, the FMTC framework is composed of two main components: client-side personalized clustering module, which learns a parameterized mapping model to support robust out-of-sample inference, bypassing the need for unreliable pseudo-labels; and server-side tensorial correlation module, which explicitly captures the shared knowledge across all clients. This is achieved by organizing all client models into a unified tensor and applying a low-rank regularization to discover their common subspace. To solve this joint optimization problem, we derive an efficient, privacy-preserving distributed algorithm based on the Alternating Direction Method of Multipliers, which decomposes the global problem into parallel local updates on clients and an aggregation step on the server. To the end, several extensive experiments on multiple real-world datasets demonstrate that our proposed FMTC framework significantly outperforms various baseline and state-of-the-art federated clustering algorithms.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contingency Model-based Control (CMC) for Communicationless Cooperative Collision Avoidance in Robot Swarms</title>
<link>https://arxiv.org/abs/2512.20391</link>
<guid>https://arxiv.org/abs/2512.20391</guid>
<content:encoded><![CDATA[
<div> The provided text appears to be a **metadata and abstract snippet** from an arXiv paper with the identifier **arXiv:2512.20391v3**, submitted under the "replace-cross" announcement typeindicating it's an updated version of a cross-listed paper.

Heres a clear summary and interpretation of the content:

---

###  **Paper Summary: arXiv:2512.20391v3**

**Title (inferred):** *Contingency Model-based Control (CMC) for Communication-Free Decentralized Swarm Collision Avoidance*

**Core Problem:**  
Cooperative collision avoidance in robot swarms is challenging, especially in decentralized systems where each agent must independently make decisions. Most current methods depend on inter-agent communication, which introduces vulnerabilities due to:
- Latency and delays
- Packet loss
- Transmission faults
- Adversarial attacks (e.g., jamming, spoofing)

These limitations motivate the need for **communication-free coordination strategies**.

---

###  **Proposed Solution: Contingency Model-based Control (CMC)**

**Key Idea:**  
A fully **decentralized**, **communication-free** control framework inspired by consensual offline rulesakin to traffic laws (e.g., right-of-way rules). All agents follow pre-agreed protocols designed *offline*, enabling safe real-time operation without exchanging data during runtime.

####  Key Components of CMC:
1. **Contingency Trajectory**:  
   Each robot has a predefined "safe" trajectory it will follow if conflict arisesa fallback plan encoded in advance.

2. **Perpendicular Bisecting Planes (PBP):**  
   Geometric constructs used as **collision avoidance constraints** between pairs of agents. These planes act as decision boundaries: robots adjust their motion to ensure they dont simultaneously occupy risky regions relative to one another.

3. **Recursive Feasibility & Safety Guarantees:**  
   The method ensures that:
   - Safe states are always reachable (recursive feasibility)
   - No collisions occur in closed-loop operation (guaranteed collision avoidance)

4. **Plug-and-Play Capability:**  
   New robots can join the swarm dynamically without reconfiguring the entire systemeach follows the same rule set.

---

###  Validation
- Demonstrated via **two numerical simulations**
- Results show:
  - Robust **collision avoidance** even in constrained environments
  - Smooth swarm behavior
  - Preservation of safety guarantees

---

###  Significance and Contributions

| Contribution | Description |
|------------|-------------|
| **No Communication Required** | Eliminates reliance on unreliable wireless networks |
| **Safety Guarantees** | Formal proof of collision avoidance and recursive feasibility |
| **Scalability & Flexibility** | Supports dynamic entry/exit of agents (plug-and-play) |
| **Resilience** | Inherently robust to adversarial interference targeting communications |

---

###  Potential Applications
- Autonomous drone swarms in GPS-denied or contested environments
- Multi-robot warehouse logistics
- Search and rescue operations with limited comms
- Military or security robotics operating under electronic warfare conditions

---

###  Future Directions (Implied)
- Extension to 3D environments
- Handling heterogeneous agents
- Integration with high-level planning or learning modules
- Real-world hardware implementation and testing

---

### Final Thoughts:
This paper presents a **novel shift** from communication-dependent swarm control to a **rule-based, model-driven decentralized paradigm**. By leveraging geometric constraints and contingency modeling, CMC offers a promising path toward **provably safe, resilient, and scalable swarm autonomy**especially valuable in mission-critical or adversarial settings.

If you'd like, I can help:
- Locate the full paper on arXiv (note: `2512.20391` seems to use a future date; possibly a placeholder or typo  likely meant to be `24xx.xxxx` or similar)
- Explain the math behind perpendicular bisecting planes
- Compare CMC to other approaches like Velocity Obstacles or MPC-based schemes
- Summarize related work in communication-free coordination

Let me know how youd like to proceed! <div>
arXiv:2512.20391v3 Announce Type: replace-cross 
Abstract: Cooperative collision avoidance between robots, or `agents,' in swarm operations remains an open challenge. Assuming a decentralized architecture, each agent is responsible for making its own decisions and choosing its control actions. Most existing approaches rely on a (wireless) communication network between (some of) the agents. In reality, however, communication is brittle. It may be affected by latency, further delays and packet losses, and transmission faults. Moreover, it is subject to adversarial attacks, such as jamming or spoofing. This paper proposes Contingency Model-based Control (CMC), a decentralized cooperative approach that does not rely on communication. Instead, the control algorithm is based on consensual rules that are designed for all agents offline, similar to traffic rules. For CMC, this includes the definition of a contingency trajectory for each robot, and perpendicular bisecting planes as collision avoidance constraints. The setup permits a full guarantee of recursive feasibility and collision avoidance between all swarm members in closed-loop operation. CMC naturally satisfies the plug & play paradigm, i.e., new robots may enter the swarm dynamically. The effectiveness of the CMC regime is demonstrated in two numerical examples, showing that the collision avoidance guarantee is intact and the robot swarm operates smoothly in a constrained environment.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>DyPBP: Dynamic Peer Beneficialness Prediction for Cryptocurrency P2P Networking</title>
<link>https://arxiv.org/abs/2511.17523</link>
<guid>https://arxiv.org/abs/2511.17523</guid>
<content:encoded><![CDATA[
arXiv:2511.17523v1 Announce Type: new 
Abstract: Distributed peer-to-peer (P2P) networking delivers the new blocks and transactions and is critical for the cryptocurrency blockchain system operations. Having poor P2P connectivity reduces the financial rewards from the mining consensus protocol. Previous research defines beneficalness of each Bitcoin peer connection and estimates the beneficialness based on the observations of the blocks and transactions delivery, which are after they are delivered. However, due to the infrequent block arrivals and the sporadic and unstable peer connections, the peers do not stay connected long enough to have the beneficialness score to converge to its expected beneficialness. We design and build Dynamic Peer Beneficialness Prediction (DyPBP) which predicts a peer's beneficialness by using networking behavior observations beyond just the block and transaction arrivals. DyPBP advances the previous research by estimating the beneficialness of a peer connection before it delivers new blocks and transactions. To achieve such goal, DyPBP introduces a new feature for remembrance to address the dynamic connectivity issue, as Bitcoin's peers using distributed networking often disconnect and re-connect. We implement DyPBP on an active Bitcoin node connected to the Mainnet and use machine learning for the beneficialness prediction. Our experimental results validate and evaluate the effectiveness of DyPBP; for example, the error performance improves by 2 to 13 orders of magnitude depending on the machine-learning model selection. DyPBP's use of the remembrance feature also informs our model selection. DyPBP enables the P2P connection's beneficialness estimation from the connection start before a new block arrives.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bunny Hops and Blockchain Stops: Cross-Chain MEV Detection With N-Hops</title>
<link>https://arxiv.org/abs/2511.17527</link>
<guid>https://arxiv.org/abs/2511.17527</guid>
<content:encoded><![CDATA[
arXiv:2511.17527v1 Announce Type: new 
Abstract: This student paper introduces a novel methodology for the detection and analysis of multihop cross-chain arbitrage opportunities, wherein multihop denotes arbitrage sequences involving more than two transactional steps across distinct blockchain networks, executed using sequence-dependent strategies. Utilizing a comprehensive dataset comprising over 2.4 billion transactions recorded between September 2023 and August 2024 (encompassing 12 blockchain platforms and 45 cross-chain bridges) we design and implement an algorithm capable of identifying, sequence-dependent arbitrage paths spanning multiple ecosystems. Our empirical analysis demonstrates that such arbitrage opportunities are exceedingly infrequent, underscoring the inherent challenges associated with multihop execution in cross-chain environments.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iterative Negotiation and Oversight: A Case Study in Decentralized Air Traffic Management</title>
<link>https://arxiv.org/abs/2511.17625</link>
<guid>https://arxiv.org/abs/2511.17625</guid>
<content:encoded><![CDATA[
arXiv:2511.17625v1 Announce Type: new 
Abstract: Achieving consensus among noncooperative agents remains challenging in decentralized multi-agent systems, where agents often have conflicting preferences. Existing coordination methods enable agents to reach consensus without a centralized coordinator, but do not provide formal guarantees on system-level objectives such as efficiency or fairness. To address this limitation, we propose an iterative negotiation and oversight framework that augments a decentralized negotiation mechanism with taxation-like oversight. The framework builds upon the trading auction for consensus, enabling noncooperative agents with conflicting preferences to negotiate through asset trading while preserving valuation privacy. We introduce an oversight mechanism, which implements a taxation-like intervention that guides decentralized negotiation toward system-efficient and equitable outcomes while also regulating how fast the framework converges. We establish theoretical guarantees of finite-time termination and derive bounds linking system efficiency and convergence rate to the level of central intervention. A case study based on the collaborative trajectory options program, a rerouting initiative in U.S. air traffic management, demonstrates that the framework can reliably achieve consensus among noncooperative airspace sector managers, and reveals how the level of intervention regulates the relationship between system efficiency and convergence speed. Taken together, the theoretical and experimental results indicate that the proposed framework provides a general mechanism for decentralized coordination in noncooperative multi-agent systems while safeguarding system-level objectives.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian probabilistic exploration of Bitcoin informational quanta and interactions under the GITT-VT paradigm</title>
<link>https://arxiv.org/abs/2511.17646</link>
<guid>https://arxiv.org/abs/2511.17646</guid>
<content:encoded><![CDATA[
arXiv:2511.17646v1 Announce Type: new 
Abstract: This study explores Bitcoin's value formation through the Granular Interaction Thinking Theory-Value Theory (GITT-VT). Rather than stemming from material utility or cash flows, Bitcoin's value arises from informational attributes and interactions of multiple factors, including cryptographic order, decentralization-enabled autonomy, trust embedded in the consensus mechanism, and socio-narrative coherence that reduce entropy within decentralized value-exchange processes. To empirically assess this perspective, a Bayesian linear model was estimated using daily data from 2022 to 2025, operationalizing four informational value dimensions: Store-of-Value (SOV), Autonomy (AUT), Social-Signal Value (SSV), and Hedonic-Sentiment Value (HSV). Results indicate that only SSV exerts a highly credible positive effect on next-day returns, highlighting the dominant role of high-entropy social information in short-term pricing dynamics. In contrast, SOV and AUT show moderately reliable positive associations, reflecting their roles as low-entropy structural anchors of long-term value. HSV displays no credible predictive effect. The study advances interdisciplinary value theory and demonstrates Bitcoin as a dual-layer entropy-regulating socio-technological ecosystem. The findings offer implications for digital asset valuation, investment education, and future research on entropy dynamics across non-cash-flow digital assets.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Coordination in Autonomous Vehicle Routing: A Simulation-Based Study of Communication, Memory, and Routing Loops</title>
<link>https://arxiv.org/abs/2511.17656</link>
<guid>https://arxiv.org/abs/2511.17656</guid>
<content:encoded><![CDATA[
arXiv:2511.17656v1 Announce Type: new 
Abstract: Multi-agent coordination is critical for next-generation autonomous vehicle (AV) systems, yet naive implementations of communication-based rerouting can lead to catastrophic performance degradation. This study investigates a fundamental problem in decentralized multi-agent navigation: routing loops, where vehicles without persistent obstacle memory become trapped in cycles of inefficient path recalculation. Through systematic simulation experiments involving 72 unique configurations across varying vehicle densities (15, 35, 55 vehicles) and obstacle frequencies (6, 20 obstacles), we demonstrate that memory-less reactive rerouting increases average travel time by up to 682% compared to baseline conditions. To address this, we introduce Object Memory Management (OMM), a lightweight mechanism enabling agents to retain and share knowledge of previously encountered obstacles. OMM operates by maintaining a distributed blacklist of blocked nodes, which each agent consults during Dijkstra-based path recalculation, effectively preventing redundant routing attempts. Our results show that OMM-enabled coordination reduces average travel time by 75.7% and wait time by 88% compared to memory-less systems, while requiring only 1.67 route recalculations per vehicle versus 9.83 in memory-less scenarios. This work provides empirical evidence that persistent, shared memory is not merely beneficial but essential for robust multi-agent coordination in dynamic environments. The findings have implications beyond autonomous vehicles, informing the design of decentralized systems in robotics, network routing, and distributed AI. We provide a comprehensive experimental analysis, including detailed scenario breakdowns, scalability assessments, and visual documentation of the routing loop phenomenon, demonstrating OMM's critical role in preventing detrimental feedback cycles in cooperative multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Homomorphic Encryption-based Vaults for Anonymous Balances on VM-enabled Blockchains</title>
<link>https://arxiv.org/abs/2511.17842</link>
<guid>https://arxiv.org/abs/2511.17842</guid>
<content:encoded><![CDATA[
arXiv:2511.17842v1 Announce Type: new 
Abstract: In this work, we present homomorphic encryption-based vaults (Haults), a permissioned privacy-preserving smart wallet protocol for VM-enabled blockchains that keeps users' balances confidential, as well as the amounts transacted to other parties. To comply with regulations, we include optional compliance features that allow specific entities (the auditors) to retrieve transaction amounts or execute force transfers when necessary. Our solution uses ElGamal over elliptic curves to encrypt balances, combined with zero-knowledge proofs to verify the correctness of transaction amounts and the integrity of the sender's updated balance, among other security checks. We provide a detailed explanation of the protocol, including a security discussion and benchmarks from our proof-of-concept implementation, which yield great results. Beyond in-contract issued tokens, we also provide a thorough explanation on how our solution can be compatible with external ones (e.g., Ether or any ERC20).
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DISPATCH -- Decentralized Informed Spatial Planning and Assignment of Tasks for Cooperative Heterogeneous Agents</title>
<link>https://arxiv.org/abs/2511.17915</link>
<guid>https://arxiv.org/abs/2511.17915</guid>
<content:encoded><![CDATA[
arXiv:2511.17915v1 Announce Type: new 
Abstract: Spatial task allocation in systems such as multi-robot delivery or ride-sharing requires balancing efficiency with fair service across tasks. Greedy assignment policies that match each agent to its highest-preference or lowest-cost task can maximize efficiency but often create inequities: some tasks receive disproportionately favorable service (e.g., shorter delays or better matches), while others face long waits or poor allocations.
  We study fairness in heterogeneous multi-agent systems where tasks vary in preference alignment and urgency. Most existing approaches either assume centralized coordination or largely ignore fairness under partial observability. Distinct from this prior work, we establish a connection between the Eisenberg-Gale (EG) equilibrium convex program and decentralized, partially observable multi-agent learning. Building on this connection, we develop two equilibrium-informed algorithms that integrate fairness and efficiency: (i) a multi-agent reinforcement learning (MARL) framework, EG-MARL, whose training is guided by centralized fair assignment algorithms (EG and a preference-aware Hungarian method); and (ii) a stochastic online optimization mechanism that performs guided exploration and subset-based fair assignment as tasks are discovered.
  We evaluate our frameworks across a range of team sizes and assignment formulations against centralized EG, Hungarian, and Min-Max Distance baselines. Both algorithms preserve the fairness-efficiency balance of the Eisenberg-Gale equilibrium under partial observability. EG-MARL achieves near-centralized coordination and reduced travel distances, while the stochastic online mechanism enables real-time allocation with competitive fairness. Together, these results demonstrate that spatially aware EG formulations can effectively guide decentralized coordination in agents with heterogeneous capabilities.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty-Aware Federated Learning for Cyber-Resilient Microgrid Energy Management</title>
<link>https://arxiv.org/abs/2511.17968</link>
<guid>https://arxiv.org/abs/2511.17968</guid>
<content:encoded><![CDATA[
arXiv:2511.17968v1 Announce Type: new 
Abstract: Maintaining economic efficiency and operational reliability in microgrid energy management systems under cyberattack conditions remains challenging. Most approaches assume non-anomalous measurements, make predictions with unquantified uncertainties, and do not mitigate malicious attacks on renewable forecasts for energy management optimization. This paper presents a comprehensive cyber-resilient framework integrating federated Long Short-Term Memory-based photovoltaic forecasting with a novel two-stage cascade false data injection attack detection and energy management system optimization. The approach combines autoencoder reconstruction error with prediction uncertainty quantification to enable attack-resilient energy storage scheduling while preserving data privacy. Extreme false data attack conditions were studied that caused 58% forecast degradation and 16.9\% operational cost increases. The proposed integrated framework reduced false positive detections by 70%, recovered 93.7% of forecasting performance losses, and achieved 5\% operational cost savings, mitigating 34.7% of attack-induced economic losses. Results demonstrate that precision-focused cascade detection with multi-signal fusion outperforms single-signal approaches, validating security-performance synergy for decentralized microgrids.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Fair OR-ML Framework for Resource Substitution in Large-Scale Networks</title>
<link>https://arxiv.org/abs/2511.18269</link>
<guid>https://arxiv.org/abs/2511.18269</guid>
<content:encoded><![CDATA[
arXiv:2511.18269v1 Announce Type: new 
Abstract: Ensuring that the right resource is available at the right location and time remains a major challenge for organizations operating large-scale logistics networks. The challenge comes from uneven demand patterns and the resulting asymmetric flow of resources across the arcs, which create persistent imbalances at the network nodes. Resource substitution among multiple, potentially composite and interchangeable, resource types is a cost-effective way to mitigate these imbalances. This leads to the resource substitution problem, which aims at determining the minimum number of resource substitutions from an initial assignment to minimize the overall network imbalance. In decentralized settings, achieving globally coordinated solutions becomes even more difficult. When substitution entails costs, effective prescriptions must also incorporate fairness and account for the individual preferences of schedulers. This paper presents a generic framework that combines operations research (OR) and machine learning (ML) to enable fair resource substitution in large networks. The OR component models and solves the resource substitution problem under a fairness lens. The ML component leverages historical data to learn schedulers' preferences, guide intelligent exploration of the decision space, and enhance computational efficiency by dynamically selecting the top-$\kappa$ resources for each arc in the network. The framework produces a portfolio of high-quality solutions from which schedulers can select satisfactory trade-offs. The proposed framework is applied to the network of one of the largest package delivery companies in the world, which serves as the primary motivation for this research. Computational results demonstrate substantial improvements over state-of-the-art methods, including an 80% reduction in model size and a 90% decrease in execution time while preserving optimality.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ADF-LoRA: Alternating Low-Rank Aggregation for Decentralized Federated Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.18291</link>
<guid>https://arxiv.org/abs/2511.18291</guid>
<content:encoded><![CDATA[
arXiv:2511.18291v1 Announce Type: new 
Abstract: This paper revisits alternating low-rank updates for federated fine-tuning and examines their behavior in decentralized federated learning (DFL). While alternating the LoRA matrices has been shown to stabilize aggregation in centralized FL, extending this mechanism to decentralized, peer-to-peer communication introduces new challenges due to phase-state mismatch and block-wise divergence across clients. We introduce ADF-LoRA, which synchronizes the update of only one low-rank matrix per round and mixes both matrices to maintain more consistent parameter states under decentralized propagation. This design preserves the cross-term suppression effect of alternating updates while improving stability in serverless topologies. We provide a convergence analysis under standard smoothness assumptions and evaluate ADF-LoRA on multiple GLUE tasks. Experiments show that ADF-LoRA achieves faster and smoother convergence and delivers the highest average accuracy across tasks, outperforming existing LoRA variants in decentralized FL by a consistent margin.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Addressing Isolation in Blockchain-Based Self-Sovereign Identity</title>
<link>https://arxiv.org/abs/2511.18379</link>
<guid>https://arxiv.org/abs/2511.18379</guid>
<content:encoded><![CDATA[
arXiv:2511.18379v1 Announce Type: new 
Abstract: Self-Sovereign Identity (SSI) grants holders full ownership and control of their digital identities, being the ultimate digital identity model. Operating in a decentralized manner, SSI enables the verification of claims, including privacy-preserving mechanisms. Blockchain, which can be used to implement a Verifiable Data Registry (VDR), is often considered one of the pillars of SSI, along with Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs). Unfortunately, blockchains are mostly siloed, affecting the interoperability and universality of SSI. We investigate the effect of blockchain isolation on blockchain-based SSI. We first define possible scenarios for cross-chain SSI and exemplify with real-life use cases. We then define specific requirements for cross-chain SSI and identify challenges, also in relation to the identified scenarios. We explore various solutions to achieve blockchain interoperability, with a focus on SSI. In particular, we identify the advantages and disadvantages of distinct cross-chain models for cross-chain SSI. Finally, we address the usability of cross-chain SSI and discuss security and privacy aspects, opening the way for future research.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs as Firmware Experts: A Runtime-Grown Tree-of-Agents Framework</title>
<link>https://arxiv.org/abs/2511.18438</link>
<guid>https://arxiv.org/abs/2511.18438</guid>
<content:encoded><![CDATA[
arXiv:2511.18438v1 Announce Type: new 
Abstract: Large Language Models (LLMs) and their agent systems have recently demonstrated strong potential in automating code reasoning and vulnerability detection. However, when applied to large-scale firmware, their performance degrades due to the binary nature of firmware, complex dependency structures, and heterogeneous components. To address this challenge, this paper presents FIRMHIVE, a recursive agent hive that enables LLMs to act as autonomous firmware security analysts. FIRMHIVE introduces two key mechanisms: (1) transforming delegation into a per-agent, executable primitive and (2) constructing a runtime Tree of Agents (ToA) for decentralized coordination. We evaluate FIRMHIVE using real-world firmware images obtained from publicly available datasets, covering five representative security analysis tasks. Compared with existing LLM-agent baselines, FIRMHIVE performs deeper (about 16x more reasoning steps) and broader (about 2.3x more files inspected) cross-file exploration, resulting in about 5.6x more alerts per firmware. Compared to state-of-the-art (SOTA) security tools, FIRMHIVE identifies about 1.5x more vulnerabilities (1,802 total) and achieves 71% precision, representing significant improvements in both yield and fidelity.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DEXO: A Secure and Fair Exchange Mechanism for Decentralized IoT Data Markets</title>
<link>https://arxiv.org/abs/2511.18498</link>
<guid>https://arxiv.org/abs/2511.18498</guid>
<content:encoded><![CDATA[
arXiv:2511.18498v1 Announce Type: new 
Abstract: Opening up data produced by the Internet of Things (IoT) and mobile devices for public utilization can maximize their economic value. Challenges remain in the trustworthiness of the data sources and the security of the trading process, particularly when there is no trust between the data providers and consumers. In this paper, we propose DEXO, a decentralized data exchange mechanism that facilitates secure and fair data exchange between data consumers and distributed IoT/mobile data providers at scale, allowing the consumer to verify the data generation process and the providers to be compensated for providing authentic data, with correctness guarantees from the exchange platform. To realize this, DEXO extends the decentralized oracle network model that has been successful in the blockchain applications domain to incorporate novel hardware-cryptographic co-design that harmonizes trusted execution environment, secret sharing, and smart contract-assisted fair exchange. For the first time, DEXO ensures end-to-end data confidentiality, source verifiability, and fairness of the exchange process with strong resilience against participant collusion. We implemented a prototype of the DEXO system to demonstrate feasibility. The evaluation shows a moderate deployment cost and significantly improved blockchain operation efficiency compared to a popular data exchange mechanism.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lean 5.0: A Predictive, Human-AI, and Ethically Grounded Paradigm for Construction Management</title>
<link>https://arxiv.org/abs/2511.18651</link>
<guid>https://arxiv.org/abs/2511.18651</guid>
<content:encoded><![CDATA[
arXiv:2511.18651v1 Announce Type: new 
Abstract: This paper introduces Lean 5.0, a human-centric evolution of Lean-Digital integration that connects predictive analytics, AI collaboration, and continuous learning within Industry 5.0 and Construction 5.0 contexts. A systematic literature review (2019-2024) and a 12-week empirical validation study demonstrate measurable performance gains, including a 13% increase in Plan Percent Complete (PPC), 22% reduction in rework, and 42% improvement in forecast accuracy. The study adopts a mixed-method Design Science Research (DSR) approach aligned with PRISMA 2020 guidelines. The paper also examines integration with digital twin and blockchain technologies to improve traceability, auditability, and lifecycle transparency. Despite limitations related to sample size, single-case design, and study duration, the findings show that Lean 5.0 provides a transformative paradigm connecting human cognition with predictive control in construction management.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Cross-Entropy Method with Monotonic Nonlinear Critic Decomposition</title>
<link>https://arxiv.org/abs/2511.18671</link>
<guid>https://arxiv.org/abs/2511.18671</guid>
<content:encoded><![CDATA[
arXiv:2511.18671v1 Announce Type: new 
Abstract: Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution (CTDE), where centralized critics leverage global information to guide decentralized actors. However, centralized-decentralized mismatch (CDM) arises when the suboptimal behavior of one agent degrades others' learning. Prior approaches mitigate CDM through value decomposition, but linear decompositions allow per-agent gradients at the cost of limited expressiveness, while nonlinear decompositions improve representation but require centralized gradients, reintroducing CDM. To overcome this trade-off, we propose the multi-agent cross-entropy method (MCEM), combined with monotonic nonlinear critic decomposition (NCD). MCEM updates policies by increasing the probability of high-value joint actions, thereby excluding suboptimal behaviors. For sample efficiency, we extend off-policy learning with a modified k-step return and Retrace. Analysis and experiments demonstrate that MCEM outperforms state-of-the-art methods across both continuous and discrete action benchmarks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bipartiteness in Progressive Second-Price Multi-Auction Networks with Perfect Substitute</title>
<link>https://arxiv.org/abs/2511.19225</link>
<guid>https://arxiv.org/abs/2511.19225</guid>
<content:encoded><![CDATA[
arXiv:2511.19225v1 Announce Type: new 
Abstract: We consider a bipartite network of buyers and sellers, where the sellers run locally independent Progressive Second-Price (PSP) auctions, and buyers may participate in multiple auctions, forming a multi-auction market with perfect substitute. The paper develops a projection-based influence framework for decentralized PSP auctions. We formalize primary and expanded influence sets using projections on the active bid index set and show how partial orders on bid prices govern allocation, market shifts, and the emergence of saturated one-hop shells. Our results highlight the robustness of PSP auctions in decentralized environments by introducing saturated components and a structured framework for phase transitions in multi-auction dynamics. This structure ensures deterministic coverage of the strategy space, enabling stable and truthful embedding in the larger game. We further model intra-round dynamics using an index to capture coordinated asynchronous seller updates coupled through buyers' joint constraints. Together, these constructions explain how local interactions propagate across auctions and gives premise for coherent equilibria--without requiring global information or centralized control.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems</title>
<link>https://arxiv.org/abs/2511.19368</link>
<guid>https://arxiv.org/abs/2511.19368</guid>
<content:encoded><![CDATA[
arXiv:2511.19368v1 Announce Type: new 
Abstract: Multi-agent reinforcement learning (MARL) has been increasingly adopted in many real-world applications. While MARL enables decentralized deployment on resource-constrained edge devices, it suffers from severe non-stationarity due to the synchronous updates of agent policies. This non stationarity results in unstable training and poor policy con vergence, especially as the number of agents increases. In this paper, we propose RELED, a scalable MARL framework that integrates large language model (LLM)-driven expert demonstrations with autonomous agent exploration. RELED incorporates a Stationarity-Aware Expert Demonstration module, which leverages theoretical non-stationarity bounds to enhance the quality of LLM-generated expert trajectories, thus providing high reward and training-stable samples for each agent. Moreover, a Hybrid Expert-Agent Policy Optimization module adaptively balances each agent's learning from both expert-generated and agent-generated trajectories, accelerating policy convergence and improving generalization. Extensive experiments with real city networks based on OpenStreetMap demonstrate that RELED achieves superior performance compared to state-of-the-art MARL methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analog Physical Systems Can Exhibit Double Descent</title>
<link>https://arxiv.org/abs/2511.17825</link>
<guid>https://arxiv.org/abs/2511.17825</guid>
<content:encoded><![CDATA[
arXiv:2511.17825v1 Announce Type: cross 
Abstract: An important component of the success of large AI models is double descent, in which networks avoid overfitting as they grow relative to the amount of training data, instead improving their performance on unseen data. Here we demonstrate double descent in a decentralized analog network of self-adjusting resistive elements. This system trains itself and performs tasks without a digital processor, offering potential gains in energy efficiency and speed -- but must endure component non-idealities. We find that standard training fails to yield double descent, but a modified protocol that accommodates this inherent imperfection succeeds. Our findings show that analog physical systems, if appropriately trained, can exhibit behaviors underlying the success of digital AI. Further, they suggest that biological systems might similarly benefit from over-parameterization.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How improving performance may imply losing consistency in event-triggered consensus</title>
<link>https://arxiv.org/abs/2405.03245</link>
<guid>https://arxiv.org/abs/2405.03245</guid>
<content:encoded><![CDATA[
arXiv:2405.03245v2 Announce Type: replace 
Abstract: Event-triggered control is often argued to lower the average triggering rate compared to time-triggered control while still achieving a desired control goal, e.g., the same performance level. However, this property, often called consistency, cannot be taken for granted and can be hard to analyze in many settings. In particular, the performance properties of decentralized event-triggered control schemes with respect to time-triggered control remain mostly unexplored. Therefore, in this paper, we examine these performance properties for a consensus problem considering single-integrator agent dynamics, a level-triggering rule, and a complete communication graph. We consider the long-term average quadratic deviation from consensus as a performance measure. For this setting, we show that enriching the information the local controllers use improves the performance of the consensus algorithm but renders a previously consistent event-triggered control scheme inconsistent. In addition, we do so while deploying optimal control inputs which we derive for both information cases and triggering schemes. With this insight, we can furthermore explain the relationship between two seemingly contrasting consistency results from the literature.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Persistent BitTorrent Trackers</title>
<link>https://arxiv.org/abs/2511.17260</link>
<guid>https://arxiv.org/abs/2511.17260</guid>
<content:encoded><![CDATA[
arXiv:2511.17260v2 Announce Type: replace 
Abstract: Private BitTorrent trackers enforce upload-to-download ratios to prevent free-riding, but suffer from three critical weaknesses: reputation cannot move between trackers, centralized servers create single points of failure, and upload statistics are self-reported and unverifiable. When a tracker shuts down (whether by operator choice, technical failure, or legal action) users lose their contribution history and cannot prove their standing to new communities. We address these problems by storing reputation in smart contracts and replacing self-reports with cryptographic attestations. Receiving peers sign receipts for transferred pieces, which the tracker aggregates and verifies before updating on-chain reputation. Trackers run in Trusted Execution Environments (TEEs) to guarantee correct aggregation and prevent manipulation of state. If a tracker is unavailable, peers use an authenticated Distributed Hash Table (DHT) for discovery: the on-chain reputation acts as a Public Key Infrastructure (PKI), so peers can verify each other and maintain access control without the tracker. This design persists reputation across tracker failures and makes it portable to new instances through single-hop migration in factory-deployed contracts. We formalize the security requirements, prove correctness under standard cryptographic assumptions, and evaluate a prototype on Intel TDX. Measurements show that transfer receipts adds less than 6\% overhead with typical piece sizes, and signature aggregation speeds up verification by $2.5\times$.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Resiliency of Vital Services in Flood-Affected Regions of Bangladesh Using Next-Generation Opportunistic DTN Edge Ad Hoc Networks</title>
<link>https://arxiv.org/abs/2511.15710</link>
<guid>https://arxiv.org/abs/2511.15710</guid>
<content:encoded><![CDATA[
<div> The abstract you've provided describes a new research paper submitted to arXiv (arXiv:2511.15710v1), introducing **AZIZA**, an innovative opportunistic routing protocol tailored for disaster-prone regions like flood-affected areas in Bangladesh. Here's a concise summary and analysis of the key contributions and implications:

---

###  **Summary of AZIZA (Adaptive Zone-based Intelligent Fully Distributed Trust-Aware Routing Protocol)**

**Context & Motivation:**
- In disaster scenariosespecially recurring floods in Bangladeshtraditional communication infrastructure often fails.
- This leads to isolation of communities and hampers emergency response.
- There is a critical need for **infrastructure-less, resilient, and adaptive communication networks**.

**Solution: AZIZA**
A novel **opportunistic routing protocol** designed for decentralized, mobile ad hoc networks composed of:
- Mobile phones
- UAVs (drones)
- Ground vehicles  
These act as edge relays in an **opportunistic network (OppNet)** with intermittent connectivity.

---

###  Key Features of AZIZA

1. **Zone-Based Forwarding**
   - Divides the network into logical zones to enable **localized, efficient message forwarding**.
   - Reduces unnecessary flooding and optimizes path selection based on geographic relevance.

2. **Trust-Aware Mechanism**
   - Evaluates node behavior using **trust metrics** to identify and avoid:
     - Malicious nodes
     - Uncooperative or selfish nodes
   - Enhances security and reliability in decentralized environments.

3. **Context-Driven Decision Making**
   - Routing decisions are based on multiple dynamic factors:
     - **Trust score**
     - **Residual energy** (to prolong network lifetime)
     - **Historical delivery performance**
   - Enables intelligent, adaptive data delivery for critical services.

4. **Fully Distributed & Lightweight**
   - No central control; suitable for resource-constrained devices.
   - Designed for rapid deployment over existing mobile and aerial platforms.

---

###  Evaluation & Results

- Simulated using **The ONE Simulator**.
- Configured with **real-world mobility traces** and **flood impact data from Bangladesh**.
- Benchmarked against established opportunistic protocols (e.g., Epidemic, Prophet, Spray-and-Wait).

**Key Outcomes:**
-  **Higher message delivery ratio** (improved reliability)
-  **Lower energy consumption** (enhanced efficiency)
-  **Greater resilience under disruption and mobility**
-  Scalable and deployable in real humanitarian contexts

---

###  Significance & Impact

- Addresses a pressing global challenge: **communication during natural disasters**.
- Tailored specifically for **low-resource, high-disruption environments**.
- Leverages widely available technologies (smartphones, drones) for rapid deployment.
- Advances the state-of-the-art in **trust-aware, context-sensitive opportunistic networking**.

---

###  Potential Applications

- Emergency alert dissemination during floods
- Health data collection from remote sensors
- Coordination between rescue teams across isolated zones
- Post-disaster situational awareness and recovery operations

---

###  Future Directions (Implied or Suggested)

- Field testing in real flood-affected regions
- Integration with satellite or LoRa-based backhaul for extended reach
- Machine learning enhancements for predictive trust modeling
- Privacy-preserving trust evaluation mechanisms

---

###  Conclusion

**AZIZA represents a significant step forward in resilient, intelligent communication systems for disaster response.** By combining zone-based optimization, trust awareness, and context-driven adaptation, it offers a robust solution where traditional networks failparticularly relevant for climate-vulnerable countries like Bangladesh.

This work not only contributes technically but also aligns with broader goals in **humanitarian technology, digital resilience, and sustainable development (e.g., SDG 11 and 13).**

---

If you'd like, I can help:
- Generate a presentation slide deck from this content
- Write a blog post explaining AZIZA for non-experts
- Compare AZIZA with other opportunistic protocols
- Draft a peer review or commentary on the paper

Let me know how youd like to proceed! <div>
arXiv:2511.15710v1 Announce Type: new 
Abstract: Opportunistic routing architectures offer a resilient communication paradigm in environments where conventional networks fail due to disrupted infrastructure, dynamic node mobility, and intermittent connectivity conditions that commonly arise during large-scale disasters. In Bangladesh, recurring floods severely hinder communication systems, isolating affected populations and obstructing emergency response efforts. To address these challenges, there is a growing demand for intelligent and adaptive routing solutions capable of sustaining critical communication and services without relying on fixed infrastructure. This research presents AZIZA (Adaptive Zone-based Intelligent Fully Distributed Trust-Aware Routing Protocol), a next-generation opportunistic protocol designed to improve the resiliency of critical communication and services in disaster-prone and flood-affected regions. AZIZA supports adaptive data delivery for emergency alerts, sensor readings, and inter-zone coordination by integrating (1) zone-based forwarding to optimize localized transmission, (2) trust-aware logic to bypass uncooperative or malicious nodes, and (3) context-driven decision-making based on trust metrics, residual energy, and historical delivery patterns. AZIZA operates over lightweight, infrastructure-less edge ad hoc networks comprising mobile phones, UAVs, and ground vehicles acting as decentralized service relays. Simulation results using The Opportunistic Network Environment (ONE) Simulator configured with real-world mobility traces and flood data from Bangladesh demonstrate that AZIZA significantly outperforms benchmark approaches in delivery reliability, energy efficiency, and routing resilience. As a scalable and deployable framework, AZIZA advances the use of next-generation opportunistic routing in environments where traditional systems routinely collapse.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Secure Autonomous Agent Payments: Verifying Authenticity and Intent in a Trustless Environment</title>
<link>https://arxiv.org/abs/2511.15712</link>
<guid>https://arxiv.org/abs/2511.15712</guid>
<content:encoded><![CDATA[
<div> Thank you for sharing the abstract of arXiv:2511.15712v1.

Here is a **summary and analysis** of the paper based on the provided abstract:

---

###  **Paper Summary (arXiv:2511.15712v1)**  
**Title (inferred):** *A Blockchain-Based Framework for Authenticating Intent in AI-Agent Financial Transactions*  
**Topic:** Secure, verifiable, and privacy-preserving financial transactions initiated by autonomous AI agents.

---

###  **Core Problem**
As AI agents gain the ability to initiate financial transactions autonomously, traditional assumptions about human authorization in payment systems break down. This raises critical concerns:
- How do we **verify that an AI agent is legitimate**?
- How can we **prove the transaction reflects genuine user intent**?
- How do we ensure **integrity, privacy, and accountability** in decentralized environments?

These challenges are especially pressing in blockchain-based or Web3 ecosystems where trustless interactions dominate.

---

###  **Proposed Solution: A Hybrid Blockchain Framework**

The paper introduces a comprehensive framework combining several advanced technologies:

| Component | Role |
|--------|------|
| **Decentralized Identity (DID)** | Establishes cryptographically verifiable identities for AI agents. Ensures each agent is uniquely identifiable without central authorities. |
| **Verifiable Credentials (VCs)** | Used to issue and validate permissions (e.g., "this agent is authorized to spend up to $100/day"). Issued by users or trusted entities. |
| **On-Chain Intent Proofs** | Records user authorization (intent) directly on the blockchain. Creates a tamper-proof link between user consent and subsequent actions. |
| **Zero-Knowledge Proofs (ZKPs)** | Allows agents to prove compliance with policies (e.g., budget limits, rules) without revealing sensitive data (user identity, transaction details). Enables privacy-preserving verification. |
| **Trusted Execution Environments (TEEs)** | Ensures the AI agents reasoning and execution logic run securely and unaltered (via hardware-based attestation). Prevents model manipulation or runtime exploits. |
| **Hybrid On-Chain/Off-Chain Architecture** | Balances scalability and transparency: heavy processing occurs off-chain; critical proofs and intent records are stored on-chain. |

---

###  **Security & Trust Properties Achieved**
- **Authenticity**: DIDs + VCs prevent impersonation.
- **Intent Verification**: On-chain proof links action to prior user consent.
- **Privacy Preservation**: ZKPs allow validation without exposing inputs.
- **Execution Integrity**: TEEs ensure the agent behaves as intended.
- **Auditability**: Immutable ledger provides full traceability from intent to outcome.

---

###  **Significance and Impact**

This work addresses a **critical gap** in the convergence of AI and decentralized finance (DeFi):
- It enables **autonomous economic agents** (e.g., personal finance bots, supply chain coordinators) to operate securely at scale.
- It lays the foundation for **AI-native financial infrastructure**, where machines transact on behalf of humans with verifiable trust.
- The integration of **intent-awareness** into transactions represents a shift from purely mechanistic execution to semantically meaningful, policy-compliant automation.

---

###  **Future Implications**
- **Regulatory Compliance**: Could support frameworks like MiCA or GDPR by enabling auditable, explainable AI decisions.
- **AI Marketplaces**: Facilitates secure delegation of economic tasks to third-party AI services.
- **Web3 Evolution**: Supports next-gen dApps where AI agents interact autonomously but accountably.

---

###  **Potential Challenges (Not Addressed in Abstract)**
While promising, real-world deployment may face hurdles:
- **Scalability** of ZKP generation and TEE coordination.
- **Key management** for agent identities (risk of compromise).
- **Legal liability** when AI agents make erroneous or harmful transactions.
- **Standardization needs** across DID, VC, and ZKP implementations.

---

###  **Conclusion**
This paper presents a timely and technically robust response to the growing need for **trustworthy autonomy** in AI-driven financial systems. By integrating blockchain, cryptographic proofs, and secure hardware, it offers a path toward **verifiable, private, and accountable AI agency**a foundational step for the future of decentralized machine economies.

---

If you'd like, I can help:
- Generate a citation for this preprint,
- Draft a presentation slide deck summarizing the framework,
- Compare it to related work (e.g., ERC-7581, SIWE, or AI DAOs),
- Or simulate how the system would handle a use case (e.g., an AI paying a bill).

Let me know how youd like to proceed! <div>
arXiv:2511.15712v1 Announce Type: new 
Abstract: Artificial intelligence (AI) agents are increasingly capable of initiating financial transactions on behalf of users or other agents. This evolution introduces a fundamental challenge: verifying both the authenticity of an autonomous agent and the true intent behind its transactions in a decentralized, trustless environment. Traditional payment systems assume human authorization, but autonomous, agent-led payments remove that safeguard. This paper presents a blockchain-based framework that cryptographically authenticates and verifies the intent of every AI-initiated transaction. The proposed system leverages decentralized identity (DID) standards and verifiable credentials to establish agent identities, on-chain intent proofs to record user authorization, and zero-knowledge proofs (ZKPs) to preserve privacy while ensuring policy compliance. Additionally, secure execution environments (TEE-based attestations) guarantee the integrity of agent reasoning and execution. The hybrid on-chain/off-chain architecture provides an immutable audit trail linking user intent to payment outcome. Through qualitative analysis, the framework demonstrates strong resistance to impersonation, unauthorized transactions, and misalignment of intent. This work lays the foundation for secure, auditable, and intent-aware autonomous economic agents, enabling a future of verifiable trust and accountability in AI-driven financial ecosystems.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Detailed Comparative Analysis of Blockchain Consensus Mechanisms</title>
<link>https://arxiv.org/abs/2511.15730</link>
<guid>https://arxiv.org/abs/2511.15730</guid>
<content:encoded><![CDATA[
<div> The abstract from arXiv:2511.15730v1 presents a timely and thorough comparative evaluation of two foundational blockchain consensus mechanisms**Proof of Work (PoW)** and **Proof of Stake (PoS)**across seven key dimensions critical to the evolution of decentralized systems.

### Key Insights from the Study:

1. **Energy Use & Environmental Impact**:
   - **PoW**: Noted for its high energy consumption due to computationally intensive mining, leading to significant environmental concerns.
   - **PoS**: Demonstrates a drastic reduction in energy usage by eliminating competitive mining, making it far more environmentally sustainable.

2. **Security**:
   - **PoW**: Offers battle-tested, robust security through economic cost to attackers (high hash power required), proven over more than a decade (e.g., Bitcoin).
   - **PoS**: While theoretically secure, concerns remain about long-term resilience, especially regarding "nothing-at-stake" issues and potential vulnerabilities in early adoption phases.

3. **Transaction Speed & Scalability**:
   - **PoW**: Generally slower transaction throughput and limited scalability (e.g., Bitcoins ~7 TPS), with bottlenecks exacerbated during peak demand.
   - **PoS**: Enables faster finality and higher throughput (e.g., Ethereum post-merge), supporting greater scalability via sharding and other layer-2 integrations.

4. **Centralization Risk**:
   - **PoW**: Centralization emerges via mining pools and geographic concentration of hardware resources, undermining decentralization goals.
   - **PoS**: Risks stem from wealth concentrationthose with more stake gain disproportionate validation power, potentially favoring large stakeholders ("the rich get richer").

5. **Transaction Fees**:
   - **PoW**: Fees can be volatile and spike during network congestion.
   - **PoS**: Offers more predictable and stable fee structures due to better resource management and scalability.

6. **Decentralization vs. Efficiency Trade-off**:
   - The study highlights a core tension: PoW excels in decentralization and security at the cost of efficiency; PoS improves performance and sustainability but may compromise on equitable participation.

7. **Hybrid Solutions as a Path Forward**:
   - The paper suggests that hybrid consensus modelsleveraging PoWs proven security for initial trust establishment and PoSs efficiency for ongoing operationcould offer a balanced alternative.
   - Such designs might enable blockchains to maintain decentralization while achieving scalability and ecological responsibility.

### Implications for Future Blockchain Development:
This research contributes meaningfully to the ongoing discourse on sustainable and secure blockchain infrastructure. As global regulatory and environmental scrutiny increases, the shift toward energy-efficient models like PoS is likely to accelerate. However, preserving decentralization and avoiding new forms of centralization (e.g., staking oligopolies) remains a challenge.

The recommendation of hybrid or novel consensus architectures aligns with emerging trends in blockchain innovation, such as **proof-of-authority hybrids**, **delegated proof-of-stake variants**, and **multi-layer consensus frameworks**.

### Conclusion:
arXiv:2511.15730v1 provides a well-rounded, evidence-based assessment that will be valuable for researchers, developers, and policymakers navigating the trade-offs between security, performance, and sustainability in next-generation blockchain systems. Its call for balanced design principles resonates with the broader goal of building trustworthy, scalable, and eco-conscious decentralized networks. <div>
arXiv:2511.15730v1 Announce Type: new 
Abstract: This paper presents a comprehensive comparative analysis of two dominant blockchain consensus mechanisms, Proof of Work (PoW) and Proof of Stake (PoS), evaluated across seven critical metrics: energy use, security, transaction speed, scalability, centralization risk, environmental impact, and transaction fees. Utilizing recent academic research and real-world blockchain data, the study highlights that PoW offers robust, time-tested security but suffers from high energy consumption, slower throughput, and centralization through mining pools. In contrast, PoS demonstrates improved scalability and efficiency, significantly reduced environmental impact, and more stable transaction fees, however it raises concerns over validator centralization and long-term security maturity. The findings underscore the trade-offs inherent in each mechanism and suggest hybrid designs may combine PoW's security with PoS's efficiency and sustainability. The study aims to inform future blockchain infrastructure development by striking a balance between decentralization, performance, and ecological responsibility.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tracking financial crime through code and law: a review of regtech applications in anti-money laundering and terrorism financing</title>
<link>https://arxiv.org/abs/2511.15764</link>
<guid>https://arxiv.org/abs/2511.15764</guid>
<content:encoded><![CDATA[
<div> The abstract you've provided appears to be from a preprint paper on arXiv (identifier: 2511.15764v1), which presents a narrative literature review on the role of Regulatory Technology (RegTech) in enhancing financial crime prevention, particularly in Anti-Money Laundering and Countering the Financing of Terrorism (AML/CFT) frameworks.

Here is a concise summary and analysis of the abstract:

---

**Summary of arXiv:2511.15764v1  "RegTech in Financial Crime Prevention: A Narrative Review"**

This paper offers a structured narrative review of RegTech applications in financial compliance, focusing on developments between 2020 and 2024. It explores how advanced technologies are reshaping AML/CFT practices across several key domains:

- **Customer Due Diligence (CDD) and Know Your Customer (KYC):** Automation and AI-driven identity verification streamline onboarding and ongoing monitoring.
- **Transaction Monitoring:** Machine learning models detect anomalous patterns more accurately than rule-based systems.
- **Regulatory Reporting and Compliance Automation:** Real-time reporting and automated data collection reduce delays and human error.
- **Information Sharing and Cross-Border Cooperation:** Blockchain and secure data platforms enable trusted collaboration across jurisdictions.
- **Cost Efficiency:** RegTech reduces operational burdens and compliance costs for financial institutions.

The study highlights that RegTech is not merely a tool for efficiency but signifies a **paradigm shift** in regulatory oversightmoving institutions from passive compliance toward **active risk detection and management**. The integration of **AI, blockchain, and big data analytics** produces synergistic effects, improving detection accuracy, transparency, and responsiveness.

Importantly, the paper emphasizes the **strategic value of integrated RegTech solutions**, suggesting that isolated technological adoption is less effective than holistic, interoperable systems.

---

**Key Implications:**

- **Shift in Responsibility:** Financial institutions are becoming co-regulators in practice, empowered and expected to proactively manage risks.
- **Technology Synergy:** Combined use of AI (for pattern recognition), blockchain (for auditability and trust), and big data (for scale) enhances system resilience and adaptability.
- **Policy Relevance:** Regulators may need to evolve frameworks to support innovation while managing new risks (e.g., algorithmic bias, data privacy).

---

**Note on arXiv Preprints:**
As this is version v1 and marked as "Announce Type: new", it has not yet undergone peer review. While the findings are promising, they should be interpreted with caution until validated through formal academic scrutiny.

---

If you're looking to build upon this work or explore related topics, potential next steps could include:
- Comparative studies of RegTech adoption across regions.
- Case studies of AI-driven transaction monitoring failures/successes.
- Ethical and regulatory challenges of autonomous compliance systems.
- The role of central bank digital currencies (CBDCs) in enabling transparent financial ecosystems.

Let me know if you'd like help locating the full paper, identifying related research, or drafting a response or critique. <div>
arXiv:2511.15764v1 Announce Type: new 
Abstract: Regulatory technology (RegTech) is transforming financial compliance by integrating advanced information technologies to strengthen anti money laundering and countering the financing of terrorism (AML CFT) frameworks. Recent literature suggests that such technologies represent more than just an efficiency tool; they mark a paradigm shift in regulation and the evolution of financial oversight (Kurum, 2023). This paper aims to provide a narrative review of recent RegTech applications in financial crime prevention, with a focus on key compliance domains. A structured literature review was conducted to examine publications between 2020 and 2024 with a thematic synthesis of findings related to customer due diligence (CDD) and know your customer (KYC), transaction monitoring, regulatory reporting and compliance automation, information sharing and cross border cooperation, as well as cost efficiency. Findings reveal that RegTech solutions give financial institutions more responsibility for detecting and managing financial crime risks, making them more active players in compliance processes traditionally overseen by regulators. The combined use of technologies such as artificial intelligence (AI), blockchain, and big data also generates synergistic effects that improve compliance outcomes beyond what these technologies achieve individually. This demonstrates the strategic relevance of integrated RegTech approaches.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lifefin: Escaping Mempool Explosions in DAG-based BFT</title>
<link>https://arxiv.org/abs/2511.15936</link>
<guid>https://arxiv.org/abs/2511.15936</guid>
<content:encoded><![CDATA[
<div> The paper **arXiv:2511.15936v1** presents a critical security and liveness analysis of **DAG-based Byzantine Fault-Tolerant (BFT) consensus protocols**, which are increasingly popular in high-throughput blockchain systems due to their ability to achieve efficient transaction ordering through a Directed Acyclic Graph (DAG) structure.

### Key Contributions and Insights:

####  **Identified Vulnerability: Mempool Explosion Attack**
- **Problem**: While DAG-based BFT protocols (e.g., Sailfish, Mysticeti) offer high throughput by decoupling data dissemination from ordering and enabling zero-message consensus via well-connected mempool DAGs, they suffer from a **liveness vulnerability**.
- **Attack Vector**: An adversary can exploit this by triggering a *mempool explosion*  flooding the network with transactions or control messages  preventing nodes from committing any transactions.
- **Impact**: This leads to indefinite delays in finality, effectively halting progress and violating the **liveness guarantee** of the protocol, even under sub-majority Byzantine conditions.

---

####  **Proposed Solution: Lifefin  A Self-Stabilizing Liveness Layer**
- **Goal**: To preserve liveness under adversarial mempool stress without sacrificing performance in normal operation.
- **Core Mechanism**: 
  - Integrates an **Agreement on Common Subset (ACS)** primitive.
  - Enables honest nodes to agree on a common subset of transactions that have been reliably disseminated, allowing them to commit despite ongoing mempool bloat.
- **Key Features**:
  - **Generic & Modular**: Designed as a plug-in layer for existing DAG-based BFT protocols.
  - **Self-Stabilizing**: Recovers automatically from transient faults or attacks; ensures eventual liveness.
  - **Low Overhead**: Imposes almost no cost during typical (non-attack) scenarios.
  - **Bounded Resource Usage**: Limits memory and computation usage per node, countering resource exhaustion.

---

####  **Integration & Evaluation**
- **Applied To**:
  - **Sailfish  Sailfish-Lifefin**
  - **Mysticeti  Mysticeti-Lifefin**
- **Implementation & Testing**:
  - Full implementations of both enhanced protocols.
  - Evaluated under various workloads and adversarial conditions.
- **Results**:
  - **Throughput**: Nearly identical to original protocols in normal operations.
  - **Latency**: Slight increase only during attack phases; minimal impact otherwise.
  - **Resilience**: Successfully resists mempool explosion attacks and maintains liveness.
  - **Efficiency**: Confirms theoretical claim of "almost zero overhead" in typical cases.

---

### Significance and Implications

- **Fundamental Insight**: High-performance does not imply robust liveness. Even advanced DAG-based designs can fail silently under targeted resource exhaustion.
- **Practical Impact**: Lifefin offers a practical retrofit for next-gen blockchains using DAG-based consensus, enhancing resilience without redesigning core logic.
- **Design Principle**: Highlights the importance of **self-stabilization** and **resource-bounded agreement** in distributed ledgers facing adaptive adversaries.

---

### Conclusion

**Lifefin** represents a significant step forward in securing the liveness of modern, high-throughput BFT protocols. By introducing a lightweight, generic defense rooted in ACS, it closes a critical gap in DAG-based consensus  proving that safety and efficiency need not come at the expense of liveness.

>  **Takeaway**: As blockchain systems grow more complex, modular safety nets like Lifefin will become essential to ensure robustness against subtle, emergent attack vectors.

---

Let me know if you'd like a summary suitable for presentation, code implementation insights, or comparison with other liveness-preserving mechanisms (e.g., timeout-based view changes in classical BFT). <div>
arXiv:2511.15936v1 Announce Type: new 
Abstract: Directed Acyclic Graph (DAG)-based Byzantine Fault-Tolerant (BFT) protocols have emerged as promising solutions for high-throughput blockchains. By decoupling data dissemination from transaction ordering and constructing a well-connected DAG in the mempool, these protocols enable zero-message ordering and implicit view changes. However, we identify a fundamental liveness vulnerability: an adversary can trigger mempool explosions to prevent transaction commitment, ultimately compromising the protocol's liveness.
  In response, this work presents Lifefin, a generic and self-stabilizing protocol designed to integrate seamlessly with existing DAG-based BFT protocols and circumvent such vulnerabilities. Lifefin leverages the Agreement on Common Subset (ACS) mechanism, allowing nodes to escape mempool explosions by committing transactions with bounded resource usage even in adverse conditions. As a result, Lifefin imposes (almost) zero overhead in typical cases while effectively eliminating liveness vulnerabilities.
  To demonstrate the effectiveness of Lifefin, we integrate it into two state-of-the-art DAG-based BFT protocols, Sailfish and Mysticeti, resulting in two enhanced variants: Sailfish-Lifefin and Mysticeti-Lifefin. We implement these variants and compare them with the original Sailfish and Mysticeti systems. Our evaluation demonstrates that Lifefin achieves comparable transaction throughput while introducing only minimal additional latency to resist similar attacks.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Quantum-Secure and Blockchain-Integrated E-Voting Framework with Identity Validation</title>
<link>https://arxiv.org/abs/2511.16034</link>
<guid>https://arxiv.org/abs/2511.16034</guid>
<content:encoded><![CDATA[
<div> The paper **arXiv:2511.16034v1** presents a comprehensive and forward-looking approach to securing electronic voting (e-voting) systems in the post-quantum era. Here's a structured summary and analysis of its key contributions and implications:

---

###  **Summary of arXiv:2511.16034v1**

**Title:** *A Post-Quantum Secure E-Voting Architecture with Biometric Authentication and Blockchain*

####  **Core Objective**
To design a scalable, secure, and quantum-resistant e-voting framework that ensures:
- Voter authenticity
- Data integrity
- Non-repudiation
- Resistance to spoofing attacks
- Transparency via auditability

---

###  **Key Components of the Architecture**

1. **Post-Quantum Cryptography  Falcon Signatures**
   - Uses **Falcon**, a lattice-based digital signature scheme selected by NIST for standardization.
   - Ensures long-term security against quantum adversaries capable of breaking RSA/ECC.
   - Applied during voter registration to sign facial embeddings.

2. **Biometric Authentication Pipeline**
   - **MobileNetV3**: Lightweight neural network used for efficient extraction of facial features on mobile or edge devices.
   - **AdaFace**: Advanced face recognition model trained to be robust under varying conditions (pose, illumination, occlusion).
   - Real-time verification using **cosine similarity matching** between live capture and stored embeddings.
   - Incorporates **anti-spoofing mechanisms** (liveness detection) to prevent presentation attacks (e.g., photos, masks).

3. **Permissioned Blockchain Integration**
   - Stores digitally signed facial embeddings and votes immutably.
   - Provides:
     - Tamper-proof record keeping
     - Audit trail with non-repudiation
     - Transparent yet privacy-preserving logging
   - Low gas overhead due to optimized transaction size:
     - **~3.3% overhead for registration**
     - **~0.15% for voting transactions**

4. **System Monitoring & Auditing**
   - Deployed monitoring stack: **Prometheus + Grafana**
   - Enables real-time performance tracking, anomaly detection, and system transparency.
   - Critical for maintaining trust in high-stakes elections.

---

###  **Performance Evaluation Highlights**

| Metric | Result |
|-------|--------|
| **Average Classification Error Rate (ACER)** | <br /> **< 3.5%** on CelebA-Spoof dataset<br /> **< 8.2%** on Wild Face Anti-Spoofing (WFAS) dataset |
| **Latency** | Low end-to-end latency suitable for large-scale deployment |
| **Scalability** | Handles concurrent loads efficiently; no significant degradation under stress tests |
| **Gas Overhead** | Minimal blockchain cost preserves efficiency |

> These results indicate strong resistance to spoofing and practical feasibility for national-scale implementations.

---

###  **Advantages of the Proposed System**

- **Quantum Resilience**: Future-proofs e-voting infrastructure against advances in quantum computing.
- **Strong Identity Assurance**: Biometrics + cryptographic binding reduce risks of impersonation.
- **Immutable Audit Trail**: On-chain storage enables verifiable, transparent election processes.
- **Efficiency**: Optimized models and minimal blockchain bloat ensure scalability.
- **Real-Time Monitoring**: Operational visibility enhances reliability and accountability.

---

###  **Potential Challenges & Considerations**

While promising, several aspects warrant further exploration:

1. **Privacy Concerns**
   - Storing biometric dataeven hashed/embeddedon any ledger raises privacy questions.
   - Need for strict access control and compliance with GDPR/CCPA-like regulations.

2. **Model Bias in Biometrics**
   - AdaFace/MobileNetV3 must be evaluated across diverse demographics to avoid discriminatory error rates.

3. **Centralization Risks**
   - Permissioned blockchains improve efficiency but may introduce trusted partiespotentially conflicting with decentralization ideals.

4. **Hardware Dependency**
   - Reliance on cameras and edge devices could exclude voters without modern smartphones.

5. **Revocation & Update Mechanisms**
   - How are compromised biometric templates revoked or re-registered securely?

---

###  **Broader Implications**

This work represents a **convergence of three critical domains**:
- **Cryptography (post-quantum)**
- **AI-driven biometrics**
- **Distributed ledger technology**

It offers a **blueprint for next-generation digital democracy tools**, especially relevant as nations seek resilient alternatives to traditional voting amid rising cyber threats.

---

###  **Conclusion**

**arXiv:2511.16034v1** introduces a timely and technically sound solution to one of the most pressing challenges in cybersecurity and democratic governance: building **quantum-safe, trustworthy e-voting systems**.

By integrating Falcon signatures, advanced biometrics, and lightweight blockchain anchoring, the authors deliver a system that is not only secure and efficient but also auditable and future-ready.

 **Recommendation**: This architecture should be piloted in controlled electoral environments and subjected to independent red-teaming and formal verification before wide adoption.

--- 

Let me know if you'd like a visual diagram of the system architecture, code implementation ideas, or comparisons with other PQ-e-voting proposals. <div>
arXiv:2511.16034v1 Announce Type: new 
Abstract: The rapid growth of quantum computing poses a threat to the cryptographic foundations of digital systems, requiring the development of secure and scalable electronic voting (evoting) frameworks. We introduce a post-quantum-secure evoting architecture that integrates Falcon lattice-based digital signatures, biometric authentication via MobileNetV3 and AdaFace, and a permissioned blockchain for tamper-proof vote storage. Voter registration involves capturing facial embeddings, which are digitally signed using Falcon and stored on-chain to ensure integrity and non-repudiation. During voting, real-time biometric verification is performed using anti-spoofing techniques and cosine-similarity matching. The system demonstrates low latency and robust spoof detection, monitored through Prometheus and Grafana for real-time auditing. The average classification error rates (ACER) are below 3.5% on the CelebA Spoof dataset and under 8.2% on the Wild Face Anti-Spoofing (WFAS) dataset. Blockchain anchoring incurs minimal gas overhead, approximately 3.3% for registration and 0.15% for voting, supporting system efficiency, auditability, and transparency. The experimental results confirm the system's scalability, efficiency, and resilience under concurrent loads. This approach offers a unified solution to address key challenges in voter authentication, data integrity, and quantum-resilient security for digital systems.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ART: A Graph-based Framework for Investigating Illicit Activity in Monero via Address-Ring-Transaction Structures</title>
<link>https://arxiv.org/abs/2511.16192</link>
<guid>https://arxiv.org/abs/2511.16192</guid>
<content:encoded><![CDATA[
<div> The abstract you've provided describes a research paper (arXiv:2511.16192v1) that presents a novel approach to analyzing criminal behavior in the Monero blockchaina privacy-focused cryptocurrency known for its resistance to traditional transaction tracing methods.

Heres a breakdown and interpretation of the key points:

---

**Title (inferred):**  
*A Graph-Based Machine Learning Approach to Detecting Criminal Behavioral Patterns in Monero*

**Context:**  
As law enforcement improves its capabilities in **cryptocurrency forensics**, criminals are shifting from transparent blockchains like Bitcoin to more private alternatives such as **Monero (XMR)**. Monero uses advanced cryptographic techniques (e.g., ring signatures, stealth addresses, confidential transactions) to obscure sender, receiver, and transaction amount, rendering standard blockchain analysis tools ineffective.

This poses a significant challenge: how can investigators detect or infer illicit activity when direct tracing is impossible?

---

**Contribution of the Paper:**

1. **Novel Graph-Based Methodology:**  
   The authors introduce **Address-Ring-Transaction (ART) graphs**, a specialized structure designed to model Monero's unique transaction mechanics:
   - **Ring signatures** involve multiple possible signers, creating ambiguity about the true sender.
   - The graph captures relationships between addresses, ring participants (decoys), and transactions over time.

2. **Focus on Flagged Transactions:**  
   They start with a set of Monero transactions already linked to **known criminal activities** (e.g., darknet markets, ransomware payments, stolen funds). These serve as "ground truth" for training and validation.

3. **Feature Extraction:**  
   From the ART graphs, they extract both:
   - **Structural features**: patterns in how addresses interact with rings, reuse of keys, size of rings, clustering behaviors.
   - **Temporal features**: timing patterns between transactions, frequency, bursts of activitypotentially indicating operational security (OpSec) routines or coordination.

4. **Machine Learning for Pattern Detection:**  
   Using these features, they train ML models (likely classifiers like Random Forest, GNNs, or XGBoost) to:
   - Identify similar behavioral patterns in other parts of the Monero blockchain.
   - Flag transactions that exhibit characteristics akin to known criminal modus operandi.

5. **Strategic Goal:**  
   This work represents an early step toward building **investigative tools** for privacy-preserving blockchains. While full traceability remains off the table due to Moneros design, **behavioral fingerprinting** may still offer actionable intelligence.

---

**Significance & Implications:**

- **First Steps in Private Chain Forensics:** Most blockchain forensics focus on transparent ledgers. This paper pushes the frontier into truly private systems.
- **Behavior Over Tracing:** Instead of following money flows, it shifts focus to **anomalies in usage patterns**a promising avenue despite cryptographic privacy.
- **Support for LEAs:** Provides a framework for law enforcement to potentially identify suspicious clusters or predict future criminal operations, even without knowing exact transaction details.
- **Ethical Considerations:** Raises questions about privacy vs. surveillance, especially since innocent users share the same network and rely on decoys (ring members).

---

**Limitations (implied):**
- Relies on limited ground-truth data (few confirmed criminal Monero transactions).
- Cannot de-anonymize transactions but infers likelihood based on behavior.
- Risk of false positives if legitimate user behavior mimics criminal patterns.

---

**Future Directions (suggested by the work):**
- Integration with wallet-level heuristics or external data (e.g., exchange monitoring).
- Use of **Graph Neural Networks (GNNs)** to better learn complex relational structures.
- Cross-chain analysis (e.g., tracking movement from Bitcoin to Monero via mixers or DEXs).

---

**Conclusion:**

This paper represents a **pioneering effort** in applying machine learning and graph analytics to one of the most challenging domains in digital forensics: **privacy-preserving cryptocurrencies**. By focusing on structural and temporal behavioral patterns rather than content, it opens a new pathway for supporting investigationswithout breaking Moneros cryptography.

It underscores a growing trend: while privacy technologies limit visibility, **behavior leaves traces**and those might be enough to guide real-world interventions.

--- 

Let me know if you'd like help summarizing this into a presentation, generating code ideas for ART graphs, or exploring related works! <div>
arXiv:2511.16192v1 Announce Type: new 
Abstract: As Law Enforcement Agencies advance in cryptocurrency forensics, criminal actors aiming to conceal illicit fund movements increasingly turn to "mixin" services or privacy-based cryptocurrencies. Monero stands out as a leading choice due to its strong privacy preserving and untraceability properties, making conventional blockchain analysis ineffective. Understanding the behavior and operational patterns of criminal actors within Monero is therefore challenging and it is essential to support future investigative strategies and disrupt illicit activities. In this work, we propose a case study in which we leverage a novel graph-based methodology to extract structural and temporal patterns from Monero transactions linked to already discovered criminal activities. By building Address-Ring-Transaction graphs from flagged transactions, we extract structural and temporal features and use them to train Machine Learning models capable of detecting similar behavioral patterns that could highlight criminal modus operandi. This represents a first partial step toward developing analytical tools that support investigative efforts in privacy-preserving blockchain ecosystems
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Code Similarity: Benchmarking the Plausibility, Efficiency, and Complexity of LLM-Generated Smart Contracts</title>
<link>https://arxiv.org/abs/2511.16224</link>
<guid>https://arxiv.org/abs/2511.16224</guid>
<content:encoded><![CDATA[
<div> The abstract you've shared outlines a rigorous and timely study on the reliability of Large Language Models (LLMs) in generating **Solidity smart contracts**, with a focus on both functional correctness and non-functional properties like **gas efficiency, security, and code complexity**. Here's a structured breakdown and analysis of the key points, implications, and contributions:

---

###  **Study Overview**

- **Objective**: Evaluate the quality of LLM-generated Solidity code across multiple dimensionsfunctional correctness, gas usage, complexity, and semantic similarity.
- **Scope**: 
  - 500 real-world Solidity functions as ground truth.
  - 4 state-of-the-art LLMs evaluated.
  - Two generation settings: **zero-shot** vs. **Retrieval-Augmented Generation (RAG)**.
- **Evaluation Metrics**:
  - **Code similarity** (e.g., BLEU, CodeBLEU)
  - **Semantic embeddings** (vector-based meaning alignment)
  - **Automated test execution** (functional behavior matching)
  - **Gas profiling** (efficiency in Ethereum execution cost)
  - **Cognitive & cyclomatic complexity** (readability and maintainability)

---

###  **Key Findings**

| Aspect | Finding |
|-------|--------|
| **Semantic Similarity** | High  Generated code is semantically close to real contracts. Suggests LLMs understand domain structure. |
| **Functional Correctness** | Low  Only **2026%** of zero-shot outputs pass tests matching ground-truth behavior. |
| **Code Simplicity** | Generated code has **lower complexity** (both cognitive and cyclomatic), often due to **missing validation logic, edge cases, or access controls**. |
| **Gas Consumption** | Lower than real contracts  but not necessarily better; often because of **omitted safety checks**, leading to under-constrained, potentially insecure code. |
| **RAG Impact** | Dramatically improves performance:
   - Up to **45% increase in functional correctness**
   - Produces more **concise and efficient** code
   - Better context reuse from similar real-world examples |

---

###  **Critical Insight: Semantic Plausibility  Functional Reliability**

This is the core takeaway:
> Just because generated code *looks right* or *reads similarly* to real smart contracts doesnt mean it *behaves correctly*.

- LLMs may produce syntactically valid and semantically plausible code that fails during actual execution.
- Omission of input validation, reentrancy guards, overflow checks, etc., leads to **non-functional or insecure implementations**, even if they appear correct at first glance.

---

###  **Implications for Practice and Research**

1. **LLMs Are Not Production-Ready for Autonomous Smart Contract Generation**
   - Despite advances, unsupervised generation cannot replace human expertise.
   - Blind trust in LLM output risks deploying vulnerable or incorrect contracts.

2. **RAG Is a Game-Changer**
   - Retrieval-augmented methods significantly improve accuracy by grounding generation in real codebases.
   - Highlights the importance of **contextual knowledge retrieval** over pure parametric memory.

3. **Need for Hybrid Development Workflows**
   - Use LLMs for scaffolding or suggestion, but require:
     - Rigorous unit testing
     - Formal verification
     - Expert review
     - Gas optimization tools

4. **Benchmarking Must Go Beyond Syntax**
   - Future evaluations should prioritize **behavioral testing**, **security linters**, and **on-chain simulation**, not just similarity scores.

---

###  **Recommendations**

- Integrate **RAG pipelines** into IDEs for secure contract development assistance.
- Develop **specialized evaluation suites** for smart contract generation (like `SolBench` or `SmartEval`) that include:
  - Test case generation
  - Mutation testing
  - Security rule checking (e.g., Slither rules)
- Promote **fine-tuning + retrieval hybrids** trained specifically on audited, high-quality contracts.

---

###  Conclusion

> While LLMs show promise in understanding Solidity semantics, there remains a **significant gap between appearance and correctness**.  
> **Retrieval augmentation helps bridge this gap**, but achieving production-grade reliability requires more than better modelsit demands robust validation frameworks and expert oversight.

As the authors rightly conclude:  
> _"Achieving robust, production-ready code generation remains a substantial challenge."_  

This work underscores the need for **cautious optimism** and **systematic evaluation** in applying LLMs to high-stakes domains like blockchain development.

---

Would you like help summarizing this into a presentation slide, blog post, or technical commentary? <div>
arXiv:2511.16224v1 Announce Type: new 
Abstract: Smart Contracts are critical components of blockchain ecosystems, with Solidity as the dominant programming language. While LLMs excel at general-purpose code generation, the unique constraints of Smart Contracts, such as gas consumption, security, and determinism, raise open questions about the reliability of LLM-generated Solidity code. Existing studies lack a comprehensive evaluation of these critical functional and non-functional properties. We benchmark four state-of-the-art models under zero-shot and retrieval-augmented generation settings across 500 real-world functions. Our multi-faceted assessment employs code similarity metrics, semantic embeddings, automated test execution, gas profiling, and cognitive and cyclomatic complexity analysis. Results show that while LLMs produce code with high semantic similarity to real contracts, their functional correctness is low: only 20% to 26% of zero-shot generations behave identically to ground-truth implementations under testing. The generated code is consistently simpler, with significantly lower complexity and gas consumption, often due to omitted validation logic. Retrieval-Augmented Generation markedly improves performance, boosting functional correctness by up to 45% and yielding more concise and efficient code. Our findings reveal a significant gap between semantic similarity and functional plausibility in LLM-generated Smart Contracts. We conclude that while RAG is a powerful enhancer, achieving robust, production-ready code generation remains a substantial challenge, necessitating careful expert validation.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auditable Ledger Snapshot for Non-Repudiable Cross-Blockchain Communication</title>
<link>https://arxiv.org/abs/2511.16560</link>
<guid>https://arxiv.org/abs/2511.16560</guid>
<content:encoded><![CDATA[
<div> The paper **arXiv:2511.16560v1**, titled with a focus on blockchain interoperability, introduces **InterSnap**, a novel methodology designed to address one of the critical challenges in multi-chain ecosystems: **ensuring non-repudiation and auditability in cross-blockchain transactions**.

Here is a structured summary and analysis of the key contributions and implications:

---

###  **Problem Statement**
- While individual blockchains are tamper-proof and provide **non-repudiation within their own networks**, this guarantee **breaks down in cross-chain scenarios**.
- If one blockchain suffers disruptions (e.g., due to attacks or failures), other chains may:
  - Deny legitimate claims.
  - File fraudulent liabilities against the affected chain.
- Theres a lack of **verifiable, immutable records** that can serve as proof of cross-chain interactions.

---

###  **Solution: InterSnap**
InterSnap proposes a **blockchain snapshot archival framework** to enforce **auditability and non-repudiation across chains** through:

#### 1. **Cross-Chain Transaction Receipts**
- Cryptographically secured receipts generated for each cross-chain transaction.
- These receipts act as **irrefutable evidence** of agreement between participating blockchains.
- Designed to be tamper-evident and verifiable by external auditors or dispute resolution systems.

#### 2. **Distributed Snapshot Generation**
- Snapshots capture the state of ledger data at specific points in time.
- Generated in a **decentralized manner** to avoid single points of failure or trust.
- Ensures consistency and integrity across heterogeneous blockchain networks.

#### 3. **Need-Based Snapshot Scheduling**
- Adaptive scheduling mechanism triggered by events such as:
  - Completion of critical cross-chain transactions.
  - Detection of anomalies or system instability.
  - Periodic intervals based on load and security requirements.
- Balances resource usage with audit readiness.

#### 4. **Decentralized Archival & Sharing**
- Snapshots and receipts are stored using **decentralized storage platforms** (e.g., IPFS, Filecoin).
- Enables transparent, censorship-resistant access for auditing and recovery purposes.
- Supports secure sharing among authorized participants without relying on central authorities.

---

###  **Implementation & Evaluation**
- Built a **prototype using Hyperledger Fabric**.
- Tested across diverse environments:
  - On-premise machines.
  - AWS public cloud.
  - Private cloud infrastructure.
  
#### Key Results:
- **Resilience**: Successfully recovered from simulated malicious attacks while preserving transaction receipts.
- **Scalability**: Maintained performance under increasing transaction loads.
- **Efficiency**: Minimal overhead during snapshot transfer and verification.
- **Security**: End-to-end protection of archived data via cryptographic binding of receipts and snapshots.

---

###  **Significance and Impact**
- Addresses a crucial gap in **interoperable blockchain architectures**.
- Provides foundational support for:
  - Cross-chain dispute resolution.
  - Regulatory compliance and auditing.
  - Trust-minimized interoperation in decentralized finance (DeFi), supply chains, and Web3 ecosystems.
- Enhances **system-wide resilience**, especially important in heterogeneous, multi-operator environments.

---

###  Conclusion
**InterSnap** represents a significant step toward **secure, auditable, and non-repudiable cross-blockchain communication**. By combining distributed snapshots, irrefutable receipts, and decentralized archiving, it offers a practical solution to strengthen trust across independent blockchain networksespecially when they cannot fully trust each other but must still interact reliably.

This work will likely influence future designs of **interoperability protocols**, bridge mechanisms, and multi-chain consensus frameworks.

---

>  *Note: As of now (April 2025), arXiv:2511.16560v1 is newly announced and has not yet been peer-reviewed. Further validation through academic scrutiny and real-world deployment will determine its long-term impact.* <div>
arXiv:2511.16560v1 Announce Type: new 
Abstract: Blockchain interoperability is increasingly recognized as the centerpiece for robust interactions among decentralized services. Blockchain ledgers are generally tamper-proof and thus enforce non-repudiation for transactions recorded within the same network. However, such a guarantee does not hold for cross-blockchain transactions. When disruptions occur due to malicious activities or system failures within one blockchain network, foreign networks can take advantage by denying legitimate claims or mounting fraudulent liabilities against the defenseless network. In response, this paper introduces InterSnap, a novel blockchain snapshot archival methodology, for enabling auditability of crossblockchain transactions, enforcing non-repudiation. InterSnap introduces cross-chain transaction receipts that ensure their irrefutability. Snapshots of ledger data along with these receipts are utilized as non-repudiable proof of bilateral agreements among different networks. InterSnap enhances system resilience through a distributed snapshot generation process, need-based snapshot scheduling process, and archival storage and sharing via decentralized platforms. Through a prototype implementation based on Hyperledger Fabric, we conducted experiments using on-premise machines, AWS public cloud instances, as well as a private cloud infrastructure. We establish that InterSnap can recover from malicious attacks while preserving crosschain transaction receipts. Additionally, our proposed solution demonstrates adaptability to increasing loads while securely transferring snapshot archives with minimal overhead.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Market Making for Goods with Perishable Utility</title>
<link>https://arxiv.org/abs/2511.16357</link>
<guid>https://arxiv.org/abs/2511.16357</guid>
<content:encoded><![CDATA[
<div> The abstract you've provided describes a research paper (arXiv:2511.16357v1) that introduces a novel **decentralized market mechanism** for time-perishable goodsparticularly motivated by **computational resources (compute)**. Here's a breakdown and explanation of the key ideas in accessible terms:

---

###  **Core Problem**
How do we efficiently trade **time-sensitive, perishable resources** like computing power? Compute capacity is only useful at specific timesunused CPU cycles today can't be used tomorrow. Traditional markets often treat compute as bespoke bundles (e.g., spot instances on AWS), but this limits flexibility and efficiency.

Recent advances in **reproducible and verifiable execution** (e.g., technologies allowing computation to be paused, verified for correctness, and resumed across different hardware) enable treating compute as a **standardized, time-indexed commodity**, similar to electricity or bandwidth.

---

###  **Key Innovation: Automated Market Maker (AMM) for Time-Indexed Compute**

Instead of relying on order books or auctions, the authors design an **Automated Market Maker (AMM)**a smart contract-style mechanism that continuously quotes prices.

- **Pricing Rule**: The AMM sets an **hourly price** using a **concave function of load**.
  - *Load* = current demand / "floor supply"
    - Floor supply = minimum amount of compute providers are willing to offer at a preset base ("floor") price.
  - Concave pricing means prices rise with demand, but at a decreasing rate initially, then accelerate near capacitythis helps manage congestion while keeping latency low.

This **decouples price discovery from allocation**:
- Prices are posted transparently and updated frequently.
- Buyers and sellers dont need to negotiate; they just react to prices.

 Result: **Transparent, low-latency trading**, suitable for decentralized environments.

---

###  **Equilibrium Analysis**

The paper proves:
- An **equilibrium set of prices exists**.
- It is **unique** under reasonable conditions.
- Conditions are given under which the equilibrium is **admissible**, meaning supply  demand (i.e., the market clears without shortages).

This gives theoretical grounding: the mechanism behaves predictably and stably.

---

###  **Incentive Design: Premium Sharing + Cheapest Feasible Matching (CFM)**

To ensure participants behave well (e.g., providers dont lie about their costs), the system combines two mechanisms:

1. **Premium Sharing Pool**:
   - Base cost covers provider expenses.
   - Any surplus revenue (from demand exceeding floor supply) is shared **pro rata** among active providers.
   - Encourages early and full participation ("staking") because sharing rewards early commitment.

2. **Cheapest Feasible Matching (CFM)**:
   - Allocates jobs to the cheapest available providers who can fulfill them.
   - Similar in spirit to pay-as-bid or greedy matching, but designed to work with the AMM.

Under mild assumptions:
- Providers are incentivized to:
  - **Stake early and fully** (commit resources ahead of time).
  - **Truthfully report their costs** (no benefit from lying).

This aligns individual incentives with system efficiencycrucial for decentralization.

---

###  **Efficiency & Regret Guarantees**

Even though CFM is simple and fast (computationally efficient), it performs well compared to an optimal offline benchmark:
- The **worst-case regret** (performance gap vs. best possible allocation with full foresight) is **bounded**.
- So, despite being decentralized and myopic (no global lookahead), the mechanism doesnt perform arbitrarily badly.

---

###  **Summary & Significance**

This paper proposes a **new paradigm for decentralized markets in time-perishable goods**, using **compute as a case study**. Its contributions include:

| Contribution | Description |
|------------|-------------|
|  **Time-indexed commoditization of compute** | Enabled by verifiable execution; makes compute more fungible. |
|  **AMM with concave pricing** | Enables automated, transparent, real-time pricing based on utilization. |
|  **Decoupling price and allocation** | Improves scalability and reduces latency. |
|  **Incentive alignment via premium sharing + CFM** | Promotes honest behavior and early resource commitment. |
|  **Theoretical guarantees** | Existence/uniqueness of equilibria, admissibility, bounded regret. |

---

###  Potential Applications
- Decentralized cloud computing markets (e.g., peer-to-peer GPU rental).
- Edge computing coordination.
- Markets for other time-sensitive resources: energy, bandwidth, storage per time slot.

---

###  Final Thoughts

This work bridges **mechanism design**, **decentralized systems**, and **real-world resource markets**, offering a practical yet theoretically sound framework. By leveraging new tech (verifiable execution), it rethinks how we model and trade computenot as hardware, but as **standardized, temporal capacity**.

If implemented, such a system could lead to **more efficient, open, and resilient compute markets**, reducing reliance on centralized cloud providers.

Let me know if you'd like a diagrammatic explanation, comparison to existing AMMs (like Uniswap), or help understanding the math behind the concave pricing or regret bound! <div>
arXiv:2511.16357v1 Announce Type: cross 
Abstract: We study decentralized markets for goods whose utility perishes in time, with compute as a primary motivation. Recent advances in reproducible and verifiable execution allow jobs to pause, verify, and resume across heterogeneous hardware, which allow us to treat compute as time indexed capacity rather than bespoke bundles. We design an automated market maker (AMM) that posts an hourly price as a concave function of load--the ratio of current demand to a "floor supply" (providers willing to work at a preset floor). This mechanism decouples price discovery from allocation and yields transparent, low latency trading. We establish existence and uniqueness of equilibrium quotes and give conditions under which the equilibrium is admissible (i.e. active supply weakly exceeds demand). To align incentives, we pair a premium sharing pool (base cost plus a pro rata share of contemporaneous surplus) with a Cheapest Feasible Matching (CFM) rule; under mild assumptions, providers optimally stake early and fully while truthfully report costs. Despite being simple and computationally efficient, we show that CFM attains bounded worst case regret relative to an optimal benchmark.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentially-Private Distributed Model Predictive Control of Linear Discrete-Time Systems with Global Constraints</title>
<link>https://arxiv.org/abs/2405.18690</link>
<guid>https://arxiv.org/abs/2405.18690</guid>
<content:encoded><![CDATA[
<div> The provided text appears to be a **replacement announcement (arXiv:2405.18690v3 Announce Type: replace)** for a scientific paper on **differentially-private distributed model predictive control (DMPC)**. Here's a breakdown and interpretation of the abstract, along with context and implications:

---

###  **Summary of the Paper (Based on Abstract)**

**Title (inferred):** *Differentially-Private Distributed Model Predictive Control for Constrained Linear Systems*

**Core Problem:**  
While **Distributed Model Predictive Control (DMPC)** is effective for large-scale systems with constraints and enables decentralized optimization, it typically requires subsystems to share local state or control data  raising **privacy concerns**, especially in sensitive applications (e.g., smart grids, autonomous fleets).

**Key Contributions:**

1. **Baseline Approach  Dual Gradient Method:**  
   The authors first analyze a conventional **distributed dual gradient algorithm** applied to DMPC with **coupled global constraints** (e.g., shared resources or network capacities). While this method achieves optimality, it **does not preserve privacy**  an eavesdropper could infer sensitive information from exchanged messages.

2. **Privacy-Preserving Solution  Differential Privacy (DP):**  
   To address this, they introduce **noise into the optimization process** using a **differential privacy mechanism**. This ensures that individual subsystems data cannot be confidently inferred by adversaries, even if communication channels are compromised.

3. **Theoretical Guarantees:**  
   - **Convergence:** Despite added noise, the algorithm converges to the **global optimal solution**.
   - **Privacy:** The scheme satisfies **$\epsilon$-differential privacy**, offering a rigorous, quantifiable privacy guarantee.
   - **Control Performance:** An implementation strategy ensures:
     - **Recursive feasibility** (constraints always satisfied)
     - **Closed-loop stability**

4. **Validation:**  
   Simulations demonstrate the effectiveness of the proposed approach in balancing **privacy, performance, and constraint satisfaction**.

---

###  Key Concepts Explained

- **DMPC:** A control architecture where multiple subsystems cooperatively optimize their actions while respecting local and global constraints, without a central coordinator.
  
- **Coupled Constraints:** Constraints that involve variables from multiple subsystems (e.g., total power consumption  grid capacity), requiring coordination.

- **Differential Privacy (DP):** A statistical notion of privacy ensuring that the output of a computation does not significantly change whether any one participants data is included or altered. In control, DP is achieved by adding calibrated noise to signals (e.g., gradients or primal/dual variables).

- **Dual Gradient Method:** Used here to decompose the global optimization problem via Lagrangian duality; each subsystem updates its local variables, and dual variables (e.g., prices or penalties) are updated based on constraint violations.

- **Noise Injection:** Carefully designed noise is added during the dual update steps to obscure sensitive data, while still allowing convergence through diminishing step sizes or other techniques.

---

###  Why This Work Matters

| Aspect | Significance |
|-------|--------------|
| **Privacy + Control Integration** | Bridges formal privacy (from machine learning/data science) with control theory. Rare in MPC literature. |
| **Strong Theoretical Guarantees** | Proves both convergence and $\epsilon$-DP  often challenging due to noise disrupting convergence. |
| **Practical Relevance** | Applicable to energy systems, transportation networks, industrial IoT, where privacy and safety coexist as requirements. |

---

###  Potential Applications

- **Smart Grids:** Protect consumer load profiles while coordinating demand response.
- **Autonomous Vehicle Platoons:** Share trajectory plans without revealing exact positions or intentions.
- **Supply Chains:** Optimize logistics across companies without exposing internal operations.

---

###  Challenges & Trade-offs (Implied)

- **Accuracy vs. Privacy:** Higher privacy ($\epsilon \to 0$) means more noise  slower convergence or suboptimality.
- **Tuning Complexity:** Balancing noise magnitude, step size, and constraint tightening requires careful design.
- **Assumptions:** Likely assumes bounded noise, Lipschitz gradients, strong convexity, or fixed communication topology.

---

###  Next Steps (For Readers Interested in This Work)

1. Read the full paper (available at [https://arxiv.org/abs/2405.18690](https://arxiv.org/abs/2405.18690)) to see:
   - How noise is injected (primal, dual, or both?)
   - Whether they use output perturbation, objective perturbation, or gradient perturbation.
   - Details of the stability proof under noisy updates.

2. Compare with related works:
   - Zhu & Martnez (2012)  Early work on differential privacy in distributed optimization.
   - Han et al.  Applied DP to consensus and online learning.
   - Recent papers on "privacy-aware MPC" (e.g., in IEEE TAC, Automatica).

3. Consider extensions:
   - Can this handle time-varying topologies?
   - Is there a trade-off curve between $\epsilon$ and control cost?

---

If you'd like, I can help:
- Summarize the full paper once you upload it,
- Explain the technical proofs (e.g., convergence under noise),
- Or simulate a simplified version of the algorithm.

Let me know how youd like to proceed! <div>
arXiv:2405.18690v3 Announce Type: replace 
Abstract: Distributed model predictive control (DMPC) has attracted extensive attention as it can explicitly handle system constraints and achieve optimal control in a decentralized manner. However, the deployment of DMPC strategies generally requires the sharing of sensitive data among subsystems, which may violate the privacy of participating systems. In this paper, we propose a differentially-private DMPC algorithm for linear discrete-time systems subject to coupled global constraints. Specifically, we first show that a conventional distributed dual gradient algorithm can be used to address the considered DMPC problem but cannot provide strong privacy preservation. Then, to protect privacy against the eavesdropper, we incorporate a differential-privacy noise injection mechanism into the DMPC framework and prove that the resulting distributed optimization algorithm can ensure both provable convergence to a global optimal solution and rigorous $\epsilon$-differential privacy. In addition, an implementation strategy of the DMPC is designed such that the recursive feasibility and stability of the closed-loop system are guaranteed. Simulation results are provided to demonstrate the effectiveness of the developed approach.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Securing Smart Contract Languages with a Unified Agentic Framework for Vulnerability Repair in Solidity and Move</title>
<link>https://arxiv.org/abs/2502.18515</link>
<guid>https://arxiv.org/abs/2502.18515</guid>
<content:encoded><![CDATA[
<div> The arXiv paper **2502.18515v2**, titled *"Smartify: A Multi-Agent Framework Leveraging LLMs for Automated Vulnerability Detection and Repair in Solidity and Move Smart Contracts"*, presents a significant advancement in smart contract security through the use of Large Language Models (LLMs) within a multi-agent architecture. Here's a structured summary and analysis of the key contributions and implications:

---

###  **Core Problem**
- **Context**: As blockchain ecosystems grow, so does the value locked in smart contractsmaking them high-value targets.
- **Challenge**: Despite advances in secure programming languages like **Solidity** (Ethereum) and **Move** (Sui, Aptos), vulnerabilities persist due to complex logic, subtle language semantics, and human error.
- Existing tools (static analyzers, symbolic execution, rule-based LLMs) often lack precision, generalizability, or language-specific depth.

---

###  **Proposed Solution: Smartify**
**Smartify** is a novel **multi-agent framework** that uses **specialized, fine-tuned LLMs** working collaboratively to:
1. **Detect vulnerabilities**
2. **Generate secure fixes**

#### Key Innovations:
| Feature | Description |
|-------|-------------|
| **Multi-Agent Architecture** | Each agent has a specific role (e.g., vulnerability scanner, code analyzer, fix generator, validator), enabling division of labor and deeper reasoning. |
| **Language-Specific Fine-Tuning** | Agents are based on LLMs fine-tuned not just on code, but on **security patterns**, **language semantics**, and **formal verification principles** for both Solidity and Move. |
| **Concept-Based Reasoning** | Instead of relying solely on pattern matching or pre-training data, agents reason about core programming concepts (e.g., reentrancy, ownership, invariant preservation). |
| **No Need for Massive Pre-Training** | Smartify shows that targeted fine-tuning + agent specialization can outperform larger general-purpose models without requiring billion-dollar-scale training. |

---

###  **Technical Highlights**
- **Base Models Used**: 
  - Gemma2 and CodeGemma as base LLMs
  - Compared against Llama 3.1 and other SOTA open-source LLMs
- **Performance**:
  - Achieves **state-of-the-art results** in automated repair accuracy across multiple vulnerability types (e.g., reentrancy, integer overflow, access control).
  - Outperforms both domain-general LLMs and prior code-focused models.
  - Demonstrates strong cross-language adaptability: effective in both **Solidity** and **Move**, despite Moves stricter type system and resource-oriented design.
- **Dataset Curation**:
  - Solidity: Benchmark from known datasets (e.g., SmartBugs, Securify).
  - Move: First curated dataset for vulnerability repair evaluation, addressing a gap in existing research.

---

###  **Results & Impact**
- **High Fix Accuracy**: Smartify generates correct, minimal, and safe patches with fewer hallucinations than baseline models.
- **Efficiency**: Multi-agent collaboration reduces false positives and improves patch quality via internal validation loops.
- **Generalization**: Successfully handles previously unseen vulnerability combinations and edge cases.
- **Language Nuance Handling**: Particularly notable in **Move**, where it respects linear typing and resource safety without explicit manual rules.

---

###  **Contributions**
1. **Smartify Framework**: Open-sourced multi-agent system for autonomous smart contract auditing and repair.
2. **Evaluation Benchmark for Move**: First public dataset focused on real-world Move contract vulnerabilities and fixes.
3. **Comparative Study**: Detailed benchmarking of LLMs (Gemma, Llama, CodeGen, etc.) on smart contract repair tasks.
4. **Design Recipe**: Provides a methodology ("blueprint") for extending Smartify to other domains (e.g., Rust-based systems, WebAssembly modules).

---

###  **Broader Implications**
- **Security at Scale**: Enables continuous, automated auditing of decentralized applications (dApps), critical for DeFi, NFTs, and DAOs.
- **Democratization of Security**: Makes advanced vulnerability detection accessible even to smaller development teams lacking formal verification expertise.
- **Future of DevTools**: Paves the way for AI-powered IDE integrations that suggest real-time fixes during development.

---

###  **Limitations & Future Work**
- **False Positives/Negatives**: Still present in complex logic bugs; hybrid approaches with formal methods may help.
- **Runtime Overhead**: Multi-agent coordination introduces latency; optimization needed for production pipelines.
- **Adversarial Robustness**: Potential for prompt injection or manipulation in agent communication needs study.

---

###  Conclusion
*Smartify* represents a paradigm shift in how we approach smart contract securitymoving from **rule-based static analysis** and **monolithic AI models** toward **collaborative, specialized, concept-aware multi-agent systems**. By combining fine-grained language understanding with modular reasoning, it sets a new standard for reliability in blockchain software engineering.

>  **Availability**: The authors commit to releasing the framework, datasets, and fine-tuning recipes to foster reproducibility and community extension.

---

If you'd like, I can:
- Summarize the methodology section if available,
- Compare Smartify to similar frameworks (like Slither+AI, GPT-Smart, etc.),
- Help draft a blog post or presentation slide deck based on this paper.

Let me know how youd like to proceed! <div>
arXiv:2502.18515v2 Announce Type: replace 
Abstract: The rapid growth of the blockchain ecosystem and the increasing value locked in smart contracts necessitate robust security measures. While languages like Solidity and Move aim to improve smart contract security, vulnerabilities persist. This paper presents Smartify, a novel multi-agent framework leveraging Large Language Models (LLMs) to automatically detect and repair vulnerabilities in Solidity and Move smart contracts. Unlike traditional methods that rely solely on vast pre-training datasets, Smartify employs a team of specialized agents working on different specially fine-tuned LLMs to analyze code based on underlying programming concepts and language-specific security principles. We evaluated Smartify on a dataset for Solidity and a curated dataset for Move, demonstrating its effectiveness in fixing a wide range of vulnerabilities. Our results show that Smartify (Gemma2+codegemma) achieves state-of-the-art performance, surpassing existing LLMs and enhancing general-purpose models' capabilities, such as Llama 3.1. Notably, Smartify can incorporate language-specific knowledge, such as the nuances of Move, without requiring massive language-specific pre-training datasets. This work offers a detailed analysis of various LLMs' performance on smart contract repair, highlighting the strengths of our multi-agent approach and providing a blueprint for developing more secure and reliable decentralized applications in the growing blockchain landscape. We also provide a detailed recipe for extending this to other similar use cases.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MonadBFT: Fast, Responsive, Fork-Resistant Streamlined Consensus</title>
<link>https://arxiv.org/abs/2502.20692</link>
<guid>https://arxiv.org/abs/2502.20692</guid>
<content:encoded><![CDATA[
<div> The abstract you've provided describes **arXiv:2502.20692v2**, a revised version of a paper introducing **MonadBFT**, a new Byzantine Fault Tolerant (BFT) consensus protocol designed for high performance and robustness in blockchain systems. Here's a breakdown and analysis of the key contributions and innovations highlighted in the abstract:

---

###  **Overview of MonadBFT**

**MonadBFT** is a **HotStuff-family** BFT consensus protocolmeaning it inherits desirable traits like:
- **Linear message complexity** in the common case (i.e., $O(n)$ communication per decision),
- **Pipelining** to enable continuous block production,
- Use of **leader-based views** with cryptographic voting (e.g., via threshold signatures).

It is implemented as the core consensus engine in the **Monad blockchain**, which aims to scale Ethereum-compatible smart contract execution.

---

###  **Key Innovations & Contributions**

#### 1. **Tail-Forking Resistance**
- **Problem**: In many pipelined BFT protocols (like HotStuff), when a leader fails or is rotated, the current proposal may be abandoned, creating a "tail-fork"  an incomplete branch.
- **Attack Vector**: A malicious leader can **strategically drop** the prior proposers blockeven if validto censor transactions, reorder them for MEV extraction, or deny rewards to others.
- **Solution**: MonadBFT prevents this by ensuring that once a block is proposed under certain conditions, it cannot be arbitrarily discarded by the next leader.
- **Impact**: 
  - Improves **fairness** (proposers arent denied rightful rewards),
  - Reduces **MEV exploitation vectors**,
  - Enhances **integrity** of transaction ordering.

#### 2. **Speculative Finality**
- Allows nodes to **execute transactions speculatively after just one round (single view)**, assuming no equivocation from the leader.
- Execution results are considered *tentative* but highly likely to persist.
- Only in the rare case of **provable leader equivocation** (e.g., proposing two conflicting blocks in the same view) would reverts occur.
- **Benefit**: Drastically reduces **user-perceived latency**, enabling faster feedback (e.g., for dApps or frontends).
- This is optimistic speculation grounded in safety guarantees  not probabilistic finality.

#### 3. **Leader Fault Isolation**
- Ensures the system **recovers quickly** from leader failures.
- The fault of a single leader (malicious or crashed) does not cascade into prolonged downtime or complex recovery procedures.
- Likely achieved through fast view-change mechanisms tied to accountability (e.g., detecting equivocation or timeouts with evidence).
- Supports **high availability** and responsiveness even under adversarial leadership.

---

###  **Why This Matters: Novelty Claim**

> *"To our knowledge, no prior pipelined, leader-based BFT consensus protocol combines all of these properties in a single design."*

This suggests that while individual features exist elsewhere:
- Linear complexity  HotStuff,
- Speculation  some speculative variants of PBFT or Streamlet,
- MEV-aware designs  recent work in fairness (e.g., PBS, SMR with ordered mempools),
- Fast recovery  various BFT optimizations,

...**MonadBFT unifies** them into a coherent, practical framework tailored for modern blockchains where **low latency, fairness, and resistance to manipulation** are critical.

---

###  **Implications for Blockchain Performance & Security**

| Feature | Benefit |
|-------|--------|
| Tail-forking resistance | Prevents MEV-based sabotage; promotes fair proposer inclusion |
| Speculative finality | Near-instant execution feedback; improves UX without sacrificing safety |
| Leader fault isolation | High liveness even under churn or attacks |
| Linear messaging | Scalable to large validator sets |

Together, these make MonadBFT suitable for **high-throughput, low-latency blockchains** like Monad, which aims to achieve thousands of TPS while maintaining compatibility with EVM-based applications.

---

###  **Potential Research & Practical Questions**
For deeper understanding, one might explore:
- How is "speculative finality" formally defined? What safety violations could arise?
- What cryptographic or protocol-level mechanisms enforce tail-forking resistance? (E.g., locking rules, mandatory inclusion proofs?)
- How often do reverts actually happen in practice? Are they economically disincentivized?
- Is there a formal proof of safety/liveness under network asynchrony or partial synchrony?

---

###  Conclusion

**MonadBFT** represents a significant step forward in **practical BFT consensus design**, especially for blockchain environments where:
- **Performance matters** (linear scaling, low latency),
- **Fairness is crucial** (to prevent MEV abuse),
- And **system integrity must survive faulty/malicious leaders**.

By integrating **tail-forking resistance**, **speculative execution**, and **rapid fault recovery**, it addresses real-world operational challenges beyond theoretical safetymaking it a compelling candidate for next-generation blockchain infrastructures.

If you'd like, I can help summarize the full paper (when available), compare it to other protocols (e.g., Tendermint, HotStuff, Casper FFG), or explain how speculative finality interacts with rollback safety. <div>
arXiv:2502.20692v2 Announce Type: replace 
Abstract: This paper introduces MonadBFT, a novel Byzantine Fault Tolerant (BFT) consensus protocol that advances both performance and robustness. MonadBFT is implemented as the consensus protocol in the Monad blockchain. As a HotStuff-family protocol, MonadBFT has linear message complexity in the common case and is optimistically responsive, operating as quickly as the network allows. A central feature of MonadBFT is its tail-forking resistance. In pipelined BFT protocols, when a leader goes offline, the previous proposal is abandoned. Malicious leaders can exploit this tail-forking behavior as a form of Maximal Extractable Value (MEV) attack by deliberately discarding their predecessor's block, depriving that proposer of rewards and enabling transaction reordering, censorship or theft. MonadBFT prevents such tail-forking attacks, preserving both fairness and integrity in transaction execution. Another related feature of MonadBFT is its notion of speculative finality, which enables parties to execute ordered transactions after a single round (i.e., a single view), with reverts occurring only in the rare case of provable leader equivocation. This mechanism reduces user-perceived latency. Additionally, we introduce the leader fault isolation property, which ensures that the protocol can quickly recover from a failure. To our knowledge, no prior pipelined, leader-based BFT consensus protocol combines all of these properties in a single design.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Bilevel Optimization: A Perspective from Transient Iteration Complexity</title>
<link>https://arxiv.org/abs/2402.03167</link>
<guid>https://arxiv.org/abs/2402.03167</guid>
<content:encoded><![CDATA[
<div> The provided text appears to be metadata and an abstract from a paper on arXiv with the identifier **2402.03167v4**, marked as a "replace-cross" announcement, indicating it is a revised version of a cross-listed paper.

Here's a clear summary and explanation of the content:

---

**Paper Title (inferred):** *D-SOBA: A Decentralized Stochastic One-loop Bilevel Algorithm Framework*

**arXiv ID:** [2402.03167v4](https://arxiv.org/abs/2402.03167)

**Announcement Type:** Replacement (updated version)  
**Cross-listed as:** Likely in Machine Learning (cs.LG), Optimization (math.OC), or related areas

---

### **Abstract Summary**

This paper addresses **Stochastic Bilevel Optimization (SBO)**, a class of optimization problems with nested structures that are increasingly important in modern machine learning applications such as hyperparameter tuning, meta-learning, and reinforcement learning.

#### **Key Challenges**
- Most existing decentralized SBO methods focus only on **asymptotic convergence rates**, which describe long-term behavior.
- They neglect **transient iteration complexity**  the number of iterations needed before asymptotic behavior kicks in.
- This gap limits understanding of how practical factors affect performance:
  - **Network topology** (how nodes are connected)
  - **Data heterogeneity** (non-IID data across nodes)
  - **Nested algorithmic structure** inherent in bilevel problems

#### **Proposed Solution: D-SOBA**
The authors introduce **D-SOBA** (**Decentralized Stochastic One-loop Bilevel Algorithm**), a new framework designed for large-scale, decentralized environments where nodes communicate only with neighbors (no central server), improving **communication efficiency** and **robustness**.

**Two Variants:**
1. **D-SOBA-SO**: Uses **second-order information** (Hessian and Jacobian matrices) for faster or more accurate updates.
2. **D-SOBA-FO**: Relies solely on **first-order gradients**, making it simpler and more scalable.

#### **Theoretical Contributions**
- First **non-asymptotic convergence analysis** for decentralized SBO.
- Explicit characterization of **transient iteration complexity**.
- Theoretical insights into how network topology, data heterogeneity, and algorithm design impact convergence.

#### **Experimental Results**
- Extensive experiments validate the **efficiency** and **superior performance** of D-SOBA.
- Demonstrates advantages both empirically and in alignment with theoretical predictions.

---

### **Significance**
This work bridges a critical gap between theory and practice in decentralized bilevel optimization by:
- Providing actionable theoretical insights beyond asymptotics.
- Offering flexible algorithms suitable for real-world distributed systems.
- Enabling better design choices based on network and data characteristics.

---

### **Potential Applications**
- Federated learning with hyperparameter optimization
- Decentralized meta-learning
- Distributed model training with regularization learned via inner loops
- Edge computing scenarios requiring robust, server-free coordination

---

If you're looking for further details, I can help:
- Summarize specific sections of the full paper
- Explain technical components like one-loop bilevel methods or non-asymptotic analysis
- Compare D-SOBA to other SBO or decentralized optimization algorithms
- Provide LaTeX citation for this paper

Let me know your needs! <div>
arXiv:2402.03167v4 Announce Type: replace-cross 
Abstract: Stochastic bilevel optimization (SBO) is becoming increasingly essential in machine learning due to its versatility in handling nested structures. To address large-scale SBO, decentralized approaches have emerged as effective paradigms in which nodes communicate with immediate neighbors without a central server, thereby improving communication efficiency and enhancing algorithmic robustness. However, most decentralized SBO algorithms focus solely on asymptotic convergence rates, overlooking transient iteration complexity-the number of iterations required before asymptotic rates dominate, which results in limited understanding of the influence of network topology, data heterogeneity, and the nested bilevel algorithmic structures. To address this issue, this paper introduces D-SOBA, a Decentralized Stochastic One-loop Bilevel Algorithm framework. D-SOBA comprises two variants: D-SOBA-SO, which incorporates second-order Hessian and Jacobian matrices, and D-SOBA-FO, which relies entirely on first-order gradients. We provide a comprehensive non-asymptotic convergence analysis and establish the transient iteration complexity of D-SOBA. This provides the first theoretical understanding of how network topology, data heterogeneity, and nested bilevel structures influence decentralized SBO. Extensive experimental results demonstrate the efficiency and theoretical advantages of D-SOBA.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents</title>
<link>https://arxiv.org/abs/2511.15074</link>
<guid>https://arxiv.org/abs/2511.15074</guid>
<content:encoded><![CDATA[
arXiv:2511.15074v1 Announce Type: new 
Abstract: The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a "flooding-pruning" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Walls Have Ears: Unveiling Cross-Chain Sandwich Attacks in DeFi</title>
<link>https://arxiv.org/abs/2511.15245</link>
<guid>https://arxiv.org/abs/2511.15245</guid>
<content:encoded><![CDATA[
arXiv:2511.15245v1 Announce Type: new 
Abstract: Cross-chain interoperability is a core component of modern blockchain infrastructure, enabling seamless asset transfers and composable applications across multiple blockchain ecosystems. However, the transparency of cross-chain messages can inadvertently expose sensitive transaction information, creating opportunities for adversaries to exploit value through manipulation or front-running strategies.
  In this work, we investigate cross-chain sandwich attacks targeting liquidity pool-based cross-chain bridge protocols. We uncover a critical vulnerability where attackers can exploit events emitted on the source chain to learn transaction details on the destination chain before they appear in the destination chain mempool. This information advantage allows attackers to strategically place front-running and back-running transactions, ensuring that their front-running transactions always precede those of existing MEV bots monitoring the mempool of the destination chain. Moreover, current sandwich-attack defenses are ineffective against this new cross-chain variant. To quantify this threat, we conduct an empirical study using two months (August 10 to October 10, 2025) of cross-chain transaction data from the Symbiosis protocol and a tailored heuristic detection model. Our analysis identifies attacks that collectively garnered over \(5.27\) million USD in profit, equivalent to 1.28\% of the total bridged volume.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Path Planning through Multi-Agent Reinforcement Learning in Dynamic Environments</title>
<link>https://arxiv.org/abs/2511.15284</link>
<guid>https://arxiv.org/abs/2511.15284</guid>
<content:encoded><![CDATA[
<div> Q<br /><br />
:  
1.   
2. RL  
3.   
4. QQQ  
5. A* Oracle <div>
arXiv:2511.15284v1 Announce Type: new 
Abstract: Path planning in dynamic environments is a fundamental challenge in intelligent transportation and robotics, where obstacles and conditions change over time, introducing uncertainty and requiring continuous adaptation. While existing approaches often assume complete environmental unpredictability or rely on global planners, these assumptions limit scalability and practical deployment in real-world settings. In this paper, we propose a scalable, region-aware reinforcement learning (RL) framework for path planning in dynamic environments. Our method builds on the observation that environmental changes, although dynamic, are often localized within bounded regions. To exploit this, we introduce a hierarchical decomposition of the environment and deploy distributed RL agents that adapt to changes locally. We further propose a retraining mechanism based on sub-environment success rates to determine when policy updates are necessary. Two training paradigms are explored: single-agent Q-learning and multi-agent federated Q-learning, where local Q-tables are aggregated periodically to accelerate the learning process. Unlike prior work, we evaluate our methods in more realistic settings, where multiple simultaneous obstacle changes and increasing difficulty levels are present. Results show that the federated variants consistently outperform their single-agent counterparts and closely approach the performance of A* Oracle while maintaining shorter adaptation times and robust scalability. Although initial training remains time-consuming in large environments, our decentralized framework eliminates the need for a global planner and lays the groundwork for future improvements using deep RL and flexible environment decomposition.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BlueBottle: Fast and Robust Blockchains through Subsystem Specialization</title>
<link>https://arxiv.org/abs/2511.15361</link>
<guid>https://arxiv.org/abs/2511.15361</guid>
<content:encoded><![CDATA[
arXiv:2511.15361v1 Announce Type: new 
Abstract: Blockchain consensus faces a trilemma of security, latency, and decentralization. High-throughput systems often require a reduction in decentralization or robustness against strong adversaries, while highly decentralized and secure systems tend to have lower performance. We present BlueBottle, a two-layer consensus architecture. The core layer, BB-Core, is an n=5f+1 protocol that trades some fault tolerance for a much lower finality latency with a medium-sized core validator set. Our experiments show that BB-Core reduces latency by 20-25% in comparison to Mysticeti. The guard layer, BB-Guard, provides decentralized timestamping, proactive misbehavior detection in BB-Core, and a synchronous recovery path. When it observes equivocations or liveness failures in the core -- while tolerating up to f<3n/5 faulty nodes in the primary layer -- guard validators disseminate evidence, agree on misbehaving parties for exclusion or slashing, and either restart the core protocol (for liveness violations) or select a canonical fork (for safety violations). Together, these layers enable optimistic sub-second finality at high throughput while maintaining strong safety and liveness under a mild synchrony assumption.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multiple Sides of 36 Coins: Measuring Peer-to-Peer Infrastructure Across Cryptocurrencies</title>
<link>https://arxiv.org/abs/2511.15388</link>
<guid>https://arxiv.org/abs/2511.15388</guid>
<content:encoded><![CDATA[
arXiv:2511.15388v1 Announce Type: new 
Abstract: Blockchain technologies underpin an expanding ecosystem of decentralized applications, financial systems, and infrastructure. However, the fundamental networking layer that sustains these systems, the peer-to-peer layer, of all but the top few ecosystems remains largely opaque. In this paper, we present the first longitudinal, cross-network measurement study of 36 public blockchain networks. Over 9 months, we deployed 15 active crawlers, sourced data from two additional community crawlers, and conducted hourly connectivity probes to observe the evolving state of these networks. Furthermore, by leveraging Ethereum's discovery protocols, we inferred metadata for an additional 19 auxiliary networks that utilize the Ethereum peer discovery protocol. We also explored Internet-wide scans, which only require probing each protocol's default ports with a simple, network-specific payload. This approach allows us to rapidly identify responsive peers across the entire address space without having to implement custom discovery and handshake logic for every blockchain. We validated this method on Bitcoin and similar networks with known ground truth, then applied it to Cardano, which we could not crawl directly.
  Our study uncovers dramatic variation in network size from under 10 to more than 10,000 active nodes. We quantify trends in IPv4 versus IPv6 usage, analyze autonomous systems and geographic concentration, and characterize churn, diurnal behavior, and the coverage and redundancy of discovery protocols. These findings expose critical differences in network resilience, decentralization, and observability. Beyond characterizing each network, our methodology demonstrates a general framework for measuring decentralized networks at scale. This opens the door for continued monitoring, benchmarking, and more transparent assessments of blockchain infrastructure across diverse ecosystems.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Can You Trust Bitcoin? Value-Dependent Block Confirmation to Determine Transaction Finalit</title>
<link>https://arxiv.org/abs/2511.15421</link>
<guid>https://arxiv.org/abs/2511.15421</guid>
<content:encoded><![CDATA[
arXiv:2511.15421v1 Announce Type: new 
Abstract: We study financial transaction confirmation finality in Bitcoin as a function of transaction amount and user risk tolerance. A transaction is recorded in a block on a blockchain. However, a transaction may be revoked due to a fork in the blockchain, the odds of which decrease over time but never reach zero. Therefore, a transaction is considered confirmed if its block is sufficiently deep in the blockchain. This depth is usually set empirically at some fixed number such as six blocks. We analyze forks under varying network delays in simulation and actual Bitcoin data. Based on this analysis, we establish a relationship between block depth and the probability of confirmation revocation due to a fork. We use prospect theory to relate transaction confirmation probability to transaction amount and user risk tolerance.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining</title>
<link>https://arxiv.org/abs/2511.15456</link>
<guid>https://arxiv.org/abs/2511.15456</guid>
<content:encoded><![CDATA[
arXiv:2511.15456v1 Announce Type: new 
Abstract: As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beluga: Block Synchronization for BFT Consensus Protocols</title>
<link>https://arxiv.org/abs/2511.15517</link>
<guid>https://arxiv.org/abs/2511.15517</guid>
<content:encoded><![CDATA[
arXiv:2511.15517v1 Announce Type: new 
Abstract: Modern high-throughput BFT consensus protocols use streamlined push-pull mechanisms to disseminate blocks and keep happy-path performance optimal. Yet state-of-the-art designs lack a principled and efficient way to exchange blocks, which leaves them open to targeted attacks and performance collapse under network asynchrony. This work introduces the concept of a block synchronizer, a simple abstraction that drives incremental block retrieval and enforces resource-aware exchange. Its interface and role fit cleanly inside a modern BFT consensus stack. We also uncover a new attack, where an adversary steers honest validators into redundant, uncoordinated pulls that exhaust bandwidth and stall progress. Beluga is a modular and scarcity-aware instantiation of the block synchronizer. It achieves optimal common-case latency while bounding the cost of recovery under faults and adversarial behavior. We integrate Beluga into Mysticeti, the consensus core of the Sui blockchain, and show on a geo-distributed AWS deployment that Beluga sustains optimal performance in the optimistic path and, under attack, delivers up to 3x higher throughput and 25x lower latency than prior designs. The Sui blockchain adopted Beluga in production.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Gaussian Process Classification and an Application in Subsea Robotics</title>
<link>https://arxiv.org/abs/2511.15529</link>
<guid>https://arxiv.org/abs/2511.15529</guid>
<content:encoded><![CDATA[
arXiv:2511.15529v1 Announce Type: new 
Abstract: Teams of cooperating autonomous underwater vehicles (AUVs) rely on acoustic communication for coordination, yet this communication medium is constrained by limited range, multi-path effects, and low bandwidth. One way to address the uncertainty associated with acoustic communication is to learn the communication environment in real-time. We address the challenge of a team of robots building a map of the probability of communication success from one location to another in real-time. This is a decentralized classification problem -- communication events are either successful or unsuccessful -- where AUVs share a subset of their communication measurements to build the map. The main contribution of this work is a rigorously derived data sharing policy that selects measurements to be shared among AUVs. We experimentally validate our proposed sharing policy using real acoustic communication data collected from teams of Virginia Tech 690 AUVs, demonstrating its effectiveness in underwater environments.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resource-Constrained Decentralized Federated Learning via Personalized Event-Triggering</title>
<link>https://arxiv.org/abs/2211.12640</link>
<guid>https://arxiv.org/abs/2211.12640</guid>
<content:encoded><![CDATA[
arXiv:2211.12640v2 Announce Type: replace 
Abstract: Federated learning (FL) is a popular technique for distributing machine learning (ML) across a set of edge devices. In this paper, we study fully decentralized FL, where in addition to devices conducting training locally, they carry out model aggregations via cooperative consensus formation over device-to-device (D2D) networks. We introduce asynchronous, event-triggered communications among the devices to handle settings where access to a central server is not feasible. To account for the inherent resource heterogeneity and statistical diversity challenges in FL, we define personalized communication triggering conditions at each device that weigh the change in local model parameters against the available local network resources. We theoretically recover the $O(\ln{k} / \sqrt{k})$ convergence rate to the globally optimal model of decentralized gradient descent (DGD) methods in the setup of our methodology. We provide our convergence guarantees for the last iterates of models, under relaxed graph connectivity and data heterogeneity assumptions compared with the existing literature. To do so, we demonstrate a $B$-connected information flow guarantee in the presence of sporadic communications over the time-varying D2D graph. Our subsequent numerical evaluations demonstrate that our methodology obtains substantial improvements in convergence speed and/or communication savings compared to existing decentralized FL baselines.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A systematic review of metaheuristics-based and machine learning-driven intrusion detection systems in IoT</title>
<link>https://arxiv.org/abs/2506.00377</link>
<guid>https://arxiv.org/abs/2506.00377</guid>
<content:encoded><![CDATA[
arXiv:2506.00377v3 Announce Type: replace 
Abstract: The widespread adoption of the Internet of Things (IoT) has raised a new challenge for developers since it is prone to known and unknown cyberattacks due to its heterogeneity, flexibility, and close connectivity. To defend against such security breaches, researchers have focused on building sophisticated intrusion detection systems (IDSs) using machine learning (ML) techniques. Although these algorithms notably improve detection performance, they require excessive computing power and resources, which are crucial issues in IoT networks considering the recent trends of decentralized data processing and computing systems. Consequently, many optimization techniques have been incorporated with these ML models. Specifically, a special category of optimizer adopted from the behavior of living creatures and different aspects of natural phenomena, known as metaheuristic algorithms, has been a central focus in recent years and brought about remarkable results. Considering this vital significance, we present a comprehensive and systematic review of various applications of metaheuristics algorithms in developing a machine learning-based IDS, especially for IoT. A significant contribution of this study is the discovery of hidden correlations between these optimization techniques and machine learning models integrated with state-of-the-art IoT-IDSs. In addition, the effectiveness of these metaheuristic algorithms in different applications, such as feature selection, parameter or hyperparameter tuning, and hybrid usages are separately analyzed. Moreover, a taxonomy of existing IoT-IDSs is proposed. Furthermore, we investigate several critical issues related to such integration. Our extensive exploration ends with a discussion of promising optimization algorithms and technologies that can enhance the efficiency of IoT-IDSs.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minimmit: Fast Finality with Even Faster Blocks</title>
<link>https://arxiv.org/abs/2508.10862</link>
<guid>https://arxiv.org/abs/2508.10862</guid>
<content:encoded><![CDATA[
arXiv:2508.10862v3 Announce Type: replace 
Abstract: Achieving low-latency consensus in geographically distributed systems remains a key challenge for blockchain and distributed database applications. To this end, there has been significant recent interest in State-Machine-Replication (SMR) protocols that achieve 2-round finality under the assumption that $5f+1\leq n$, where $n$ is the number of processors and $f$ bounds the number of processors that may exhibit Byzantine faults. In these protocols, instructions are organised into views, each led by a different designated leader, and 2-round finality means that a leader's proposal can be finalised after just a single round of voting, meaning two rounds overall (one round for the proposal and one for voting).
  We introduce Minimmit, a Byzantine-fault-tolerant SMR protocol with lower latency than previous 2-round finality approaches. Our key insight is that view progression and transaction finality can operate on different quorum thresholds without compromising safety or liveness. Experiments simulating a globally distributed network of 50 processors, uniformly assigned across ten virtual regions, show that the approach leads to a 23.1% reduction in view latency and a 10.7% reduction in transaction latency compared to the state-of-the-art.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Game-theoretic Decentralized Coordination for Airspace Sector Overload Mitigation</title>
<link>https://arxiv.org/abs/2511.13770</link>
<guid>https://arxiv.org/abs/2511.13770</guid>
<content:encoded><![CDATA[
arXiv:2511.13770v1 Announce Type: new 
Abstract: Decentralized air traffic management systems offer a scalable alternative to centralized control, but often assume high levels of cooperation. In practice, such assumptions frequently break down since airspace sectors operate independently and prioritize local objectives. We address the problem of sector overload in decentralized air traffic management by proposing a mechanism that models self-interested behaviors based on best response dynamics. Each sector adjusts the departure times of flights under its control to reduce its own congestion, without any shared decision making. A tunable cooperativeness factor models the degree to which each sector is willing to reduce overload in other sectors. We prove that the proposed mechanism satisfies a potential game structure, ensuring that best response dynamics converge to a pure Nash equilibrium, under a mild restriction. In addition, we identify a sufficient condition under which an overload-free solution corresponds to a global minimizer of the potential function. Numerical experiments using 24 hours of European flight data demonstrate that the proposed algorithm substantially reduces overload even with only minimal cooperation between sectors, while maintaining scalability and matching the solution quality of centralized solvers.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hashpower allocation in Pay-per-Share blockchain mining pools</title>
<link>https://arxiv.org/abs/2511.13777</link>
<guid>https://arxiv.org/abs/2511.13777</guid>
<content:encoded><![CDATA[
arXiv:2511.13777v1 Announce Type: new 
Abstract: Mining blocks in a blockchain using the \textit{Proof-of-Work} consensus protocol involves significant risk, as network participants face continuous operational costs while earning infrequent capital gains upon successfully mining a block. A common risk mitigation strategy is to join a mining pool, which combines the computing resources of multiple miners to provide a more stable income. This article examines a Pay-per-Share (PPS) reward system, where the pool manager can adjust both the share difficulty and the management fee. Using a simplified wealth model for miners, we explore how miners should allocate their computing resources among different mining pools, considering the trade-off between risk transfer to the manager and management fees.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sigil: Server-Enforced Watermarking in U-Shaped Split Federated Learning via Gradient Injection</title>
<link>https://arxiv.org/abs/2511.14422</link>
<guid>https://arxiv.org/abs/2511.14422</guid>
<content:encoded><![CDATA[
arXiv:2511.14422v1 Announce Type: new 
Abstract: In decentralized machine learning paradigms such as Split Federated Learning (SFL) and its variant U-shaped SFL, the server's capabilities are severely restricted. Although this enhances client-side privacy, it also leaves the server highly vulnerable to model theft by malicious clients. Ensuring intellectual property protection for such capability-limited servers presents a dual challenge: watermarking schemes that depend on client cooperation are unreliable in adversarial settings, whereas traditional server-side watermarking schemes are technically infeasible because the server lacks access to critical elements such as model parameters or labels.
  To address this challenge, this paper proposes Sigil, a mandatory watermarking framework designed specifically for capability-limited servers. Sigil defines the watermark as a statistical constraint on the server-visible activation space and embeds the watermark into the client model via gradient injection, without requiring any knowledge of the data. Besides, we design an adaptive gradient clipping mechanism to ensure that our watermarking process remains both mandatory and stealthy, effectively countering existing gradient anomaly detection methods and a specifically designed adaptive subspace removal attack. Extensive experiments on multiple datasets and models demonstrate Sigil's fidelity, robustness, and stealthiness.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Capacity of Collusion-Resilient Decentralized Secure Aggregation with Groupwise Keys</title>
<link>https://arxiv.org/abs/2511.14444</link>
<guid>https://arxiv.org/abs/2511.14444</guid>
<content:encoded><![CDATA[
arXiv:2511.14444v1 Announce Type: new 
Abstract: This paper investigates the information-theoretic decentralized secure aggregation (DSA) problem under practical groupwise secret keys and collusion resilience. In DSA, $K$ users are interconnected through error-free broadcast channels. Each user holds a private input and aims to compute the sum of all other users' inputs, while satisfying the security constraint that no user, even when colluding with up to $T$ other users, can infer any information about the inputs beyond the recovered sum. To ensure security, users are equipped with secret keys to mask their inputs. Motivated by recent advances in efficient group-based key generation protocols, we consider the symmetric groupwise key setting, where every subset of $G$ users shares a group key that is independent of all other group keys. The problem is challenging because the recovery and security constraints must hold simultaneously for all users, and the structural constraints on the secret keys limit the flexibility of key correlations. We characterize the optimal rate region consisting of all achievable pairs of per-user broadcast communication rate and groupwise key rate. In particular, we show that DSA with groupwise keys is infeasible when $G=1$ or $G\ge K-T$. Otherwise, when $2\le G<K-T$, to securely compute one symbol of the desired sum, each user must broadcast at least one symbol, and each group key must contain at least $(K-T-2)/\binom{K-T-1}{G}$ independent symbols. Our results establish the fundamental limits of DSA with groupwise keys and provide design insights for communication- and key-efficient secure aggregation in decentralized learning systems.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sei Giga</title>
<link>https://arxiv.org/abs/2505.14914</link>
<guid>https://arxiv.org/abs/2505.14914</guid>
<content:encoded><![CDATA[
arXiv:2505.14914v2 Announce Type: replace 
Abstract: We introduce the Sei Giga, a multi-concurrent producer parallelized execution EVM layer one blockchain. In an internal testnet Giga has achieved >5 gigagas/sec throughput and sub 400ms finality. Giga uses Autobahn for consensus with separate DA and consensus layers requiring f+1 votes for a PoA on the DA layer before consensus. Giga reaches consensus over ordering and uses async block execution and state agreement to remove execution from the consensus bottleneck.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Federated Graph Attention Networks for Scalable and Resilient UAV Collision Avoidance</title>
<link>https://arxiv.org/abs/2511.11616</link>
<guid>https://arxiv.org/abs/2511.11616</guid>
<content:encoded><![CDATA[
arXiv:2511.11616v1 Announce Type: new 
Abstract: The real-time performance, adversarial resiliency, and privacy preservation are the most important metrics that need to be balanced to practice collision avoidance in large-scale multi-UAV (Unmanned Aerial Vehicle) systems. Current frameworks tend to prescribe monolithic solutions that are not only prohibitively computationally complex with a scaling cost of $O(n^2)$ but simply do not offer Byzantine fault tolerance. The proposed hierarchical framework presented in this paper tries to eliminate such trade-offs by stratifying a three-layered architecture. We spread the intelligence into three layers: an immediate collision avoiding local layer running on dense graph attention with latency of $<10 ms$, a regional layer using sparse attention with $O(nk)$ computational complexity and asynchronous federated learning with coordinate-wise trimmed mean aggregation, and lastly, a global layer using a lightweight Hashgraph-inspired protocol. We have proposed an adaptive differential privacy mechanism, wherein the noise level $(\epsilon \in [0.1, 1.0])$ is dynamically reduced based on an evaluation of the measured real-time threat that in turn maximized the privacy-utility tradeoff. Through the use of Distributed Hash Table (DHT)-based lightweight audit logging instead of heavyweight blockchain consensus, the median cost of getting a $95^{th}$ percentile decision within 50ms is observed across all tested swarm sizes. This architecture provides a scalable scenario of 500 UAVs with a collision rate of $< 2.0\%$ and the Byzantine fault tolerance of $f < n/3$.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DIAP: A Decentralized Agent Identity Protocol with Zero-Knowledge Proofs and a Hybrid P2P Stack</title>
<link>https://arxiv.org/abs/2511.11619</link>
<guid>https://arxiv.org/abs/2511.11619</guid>
<content:encoded><![CDATA[
arXiv:2511.11619v1 Announce Type: new 
Abstract: The absence of a fully decentralized, verifiable, and privacy-preserving communication protocol for autonomous agents remains a core challenge in decentralized computing. Existing systems often rely on centralized intermediaries, which reintroduce trust bottlenecks, or lack decentralized identity-resolution mechanisms, limiting persistence and cross-network interoperability.
  We propose the Decentralized Interstellar Agent Protocol (DIAP), a novel framework for agent identity and communication that enables persistent, verifiable, and trustless interoperability in fully decentralized environments. DIAP binds an agent's identity to an immutable IPFS or IPNS content identifier and uses zero-knowledge proofs (ZKP) to dynamically and statelessly prove ownership, removing the need for record updates.
  We present a Rust SDK that integrates Noir (for zero-knowledge proofs), DID-Key, IPFS, and a hybrid peer-to-peer stack combining Libp2p GossipSub for discovery and Iroh for high-performance, QUIC based data exchange. DIAP introduces a zero-dependency ZKP deployment model through a universal proof manager and compile-time build script that embeds a precompiled Noir circuit, eliminating the need for external ZKP toolchains. This enables instant, verifiable, and privacy-preserving identity proofs.
  This work establishes a practical, high-performance foundation for next-generation autonomous agent ecosystems and agent-to-agent (A to A) economies.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedFedPure: A Medical Federated Framework with MAE-based Detection and Diffusion Purification for Inference-Time Attacks</title>
<link>https://arxiv.org/abs/2511.11625</link>
<guid>https://arxiv.org/abs/2511.11625</guid>
<content:encoded><![CDATA[
arXiv:2511.11625v1 Announce Type: new 
Abstract: Artificial intelligence (AI) has shown great potential in medical imaging, particularly for brain tumor detection using Magnetic Resonance Imaging (MRI). However, the models remain vulnerable at inference time when they are trained collaboratively through Federated Learning (FL), an approach adopted to protect patient privacy. Adversarial attacks can subtly alter medical scans in ways invisible to the human eye yet powerful enough to mislead AI models, potentially causing serious misdiagnoses. Existing defenses often assume centralized data and struggle to cope with the decentralized and diverse nature of federated medical settings. In this work, we present MedFedPure, a personalized federated learning defense framework designed to protect diagnostic AI models at inference time without compromising privacy or accuracy. MedFedPure combines three key elements: (1) a personalized FL model that adapts to the unique data distribution of each institution; (2) a Masked Autoencoder (MAE) that detects suspicious inputs by exposing hidden perturbations; and (3) an adaptive diffusion-based purification module that selectively cleans only the flagged scans before classification. Together, these steps offer robust protection while preserving the integrity of normal, benign images. We evaluated MedFedPure on the Br35H brain MRI dataset. The results show a significant gain in adversarial robustness, improving performance from 49.50% to 87.33% under strong attacks, while maintaining a high clean accuracy of 97.67%. By operating locally and in real time during diagnosis, our framework provides a practical path to deploying secure, trustworthy, and privacy-preserving AI tools in clinical workflows.
  Index Terms: cancer, tumor detection, federated learning, masked autoencoder, diffusion, privacy
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Speculative Decoding in Decentralized LLM Inference: Turning Communication Latency into Computation Throughput</title>
<link>https://arxiv.org/abs/2511.11733</link>
<guid>https://arxiv.org/abs/2511.11733</guid>
<content:encoded><![CDATA[
arXiv:2511.11733v1 Announce Type: new 
Abstract: Speculative decoding accelerates large language model (LLM) inference by using a lightweight draft model to propose tokens that are later verified by a stronger target model. While effective in centralized systems, its behavior in decentralized settings, where network latency often dominates compute, remains under-characterized. We present Decentralized Speculative Decoding (DSD), a plug-and-play framework for decentralized inference that turns communication delay into useful computation by verifying multiple candidate tokens in parallel across distributed nodes. We further introduce an adaptive speculative verification strategy that adjusts acceptance thresholds by token-level semantic importance, delivering an additional 15% to 20% end-to-end speedup without retraining. In theory, DSD reduces cross-node communication cost by approximately (N-1)t1(k-1)/k, where t1 is per-link latency and k is the average number of tokens accepted per round. In practice, DSD achieves up to 2.56x speedup on HumanEval and 2.59x on GSM8K, surpassing the Eagle3 baseline while preserving accuracy. These results show that adapting speculative decoding for decentralized execution provides a system-level optimization that converts network stalls into throughput, enabling faster distributed LLM inference with no model retraining or architectural changes.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Goal-Oriented Multi-Agent Reinforcement Learning for Decentralized Agent Teams</title>
<link>https://arxiv.org/abs/2511.11992</link>
<guid>https://arxiv.org/abs/2511.11992</guid>
<content:encoded><![CDATA[
arXiv:2511.11992v1 Announce Type: new 
Abstract: Connected and autonomous vehicles across land, water, and air must often operate in dynamic, unpredictable environments with limited communication, no centralized control, and partial observability. These real-world constraints pose significant challenges for coordination, particularly when vehicles pursue individual objectives. To address this, we propose a decentralized Multi-Agent Reinforcement Learning (MARL) framework that enables vehicles, acting as agents, to communicate selectively based on local goals and observations. This goal-aware communication strategy allows agents to share only relevant information, enhancing collaboration while respecting visibility limitations. We validate our approach in complex multi-agent navigation tasks featuring obstacles and dynamic agent populations. Results show that our method significantly improves task success rates and reduces time-to-goal compared to non-cooperative baselines. Moreover, task performance remains stable as the number of agents increases, demonstrating scalability. These findings highlight the potential of decentralized, goal-driven MARL to support effective coordination in realistic multi-vehicle systems operating across diverse domains.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Road Safety Through Multi-Camera Image Segmentation with Post-Encroachment Time Analysis</title>
<link>https://arxiv.org/abs/2511.12018</link>
<guid>https://arxiv.org/abs/2511.12018</guid>
<content:encoded><![CDATA[
arXiv:2511.12018v1 Announce Type: new 
Abstract: Traffic safety analysis at signalized intersections is vital for reducing vehicle and pedestrian collisions, yet traditional crash-based studies are limited by data sparsity and latency. This paper presents a novel multi-camera computer vision framework for real-time safety assessment through Post-Encroachment Time (PET) computation, demonstrated at the intersection of H Street and Broadway in Chula Vista, California. Four synchronized cameras provide continuous visual coverage, with each frame processed on NVIDIA Jetson AGX Xavier devices using YOLOv11 segmentation for vehicle detection. Detected vehicle polygons are transformed into a unified bird's-eye map using homography matrices, enabling alignment across overlapping camera views. A novel pixel-level PET algorithm measures vehicle position without reliance on fixed cells, allowing fine-grained hazard visualization via dynamic heatmaps, accurate to 3.3 sq-cm. Timestamped vehicle and PET data is stored in an SQL database for long-term monitoring. Results over various time intervals demonstrate the framework's ability to identify high-risk regions with sub-second precision and real-time throughput on edge devices, producing data for an 800 x 800 pixel logarithmic heatmap at an average of 2.68 FPS. This study validates the feasibility of decentralized vision-based PET analysis for intelligent transportation systems, offering a replicable methodology for high-resolution, real-time, and scalable intersection safety evaluation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedSDA: Federated Stain Distribution Alignment for Non-IID Histopathological Image Classification</title>
<link>https://arxiv.org/abs/2511.12044</link>
<guid>https://arxiv.org/abs/2511.12044</guid>
<content:encoded><![CDATA[
arXiv:2511.12044v1 Announce Type: new 
Abstract: Federated learning (FL) has shown success in collaboratively training a model among decentralized data resources without directly sharing privacy-sensitive training data. Despite recent advances, non-IID (non-independent and identically distributed) data poses an inevitable challenge that hinders the use of FL. In this work, we address the issue of non-IID histopathological images with feature distribution shifts from an intuitive perspective that has only received limited attention. Specifically, we address this issue from the perspective of data distribution by solely adjusting the data distributions of all clients. Building on the success of diffusion models in fitting data distributions and leveraging stain separation to extract the pivotal features that are closely related to the non-IID properties of histopathological images, we propose a Federated Stain Distribution Alignment (FedSDA) method. FedSDA aligns the stain distribution of each client with a target distribution in an FL framework to mitigate distribution shifts among clients. Furthermore, considering that training diffusion models on raw data in FL has been shown to be susceptible to privacy leakage risks, we circumvent this problem while still effectively achieving alignment. Extensive experimental results show that FedSDA is not only effective in improving baselines that focus on mitigating disparities across clients' model updates but also outperforms baselines that address the non-IID data issues from the perspective of data distribution. We show that FedSDA provides valuable and practical insights for the computational pathology community.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Game-Theoretic Safe Multi-Agent Motion Planning with Reachability Analysis for Dynamic and Uncertain Environments (Extended Version)</title>
<link>https://arxiv.org/abs/2511.12160</link>
<guid>https://arxiv.org/abs/2511.12160</guid>
<content:encoded><![CDATA[
arXiv:2511.12160v1 Announce Type: new 
Abstract: Ensuring safe, robust, and scalable motion planning for multi-agent systems in dynamic and uncertain environments is a persistent challenge, driven by complex inter-agent interactions, stochastic disturbances, and model uncertainties. To overcome these challenges, particularly the computational complexity of coupled decision-making and the need for proactive safety guarantees, we propose a Reachability-Enhanced Dynamic Potential Game (RE-DPG) framework, which integrates game-theoretic coordination into reachability analysis. This approach formulates multi-agent coordination as a dynamic potential game, where the Nash equilibrium (NE) defines optimal control strategies across agents. To enable scalability and decentralized execution, we develop a Neighborhood-Dominated iterative Best Response (ND-iBR) scheme, built upon an iterated $\varepsilon$-BR (i$\varepsilon$-BR) process that guarantees finite-step convergence to an $\varepsilon$-NE. This allows agents to compute strategies based on local interactions while ensuring theoretical convergence guarantees. Furthermore, to ensure safety under uncertainty, we integrate a Multi-Agent Forward Reachable Set (MA-FRS) mechanism into the cost function, explicitly modeling uncertainty propagation and enforcing collision avoidance constraints. Through both simulations and real-world experiments in 2D and 3D environments, we validate the effectiveness of RE-DPG across diverse operational scenarios.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Collaborative Fuzzing with Continuous Reflection for Smart Contracts Vulnerability Detection</title>
<link>https://arxiv.org/abs/2511.12164</link>
<guid>https://arxiv.org/abs/2511.12164</guid>
<content:encoded><![CDATA[
arXiv:2511.12164v1 Announce Type: new 
Abstract: Fuzzing is a widely used technique for detecting vulnerabilities in smart contracts, which generates transaction sequences to explore the execution paths of smart contracts. However, existing fuzzers are falling short in detecting sophisticated vulnerabilities that require specific attack transaction sequences with proper inputs to trigger, as they (i) prioritize code coverage over vulnerability discovery, wasting considerable effort on non-vulnerable code regions, and (ii) lack semantic understanding of stateful contracts, generating numerous invalid transaction sequences that cannot pass runtime execution.
  In this paper, we propose SmartFuzz, a novel collaborative reflective fuzzer for smart contract vulnerability detection. It employs large language model-driven agents as the fuzzing engine and continuously improves itself by learning and reflecting through interactions with the environment. Specifically, we first propose a new Continuous Reflection Process (CRP) for fuzzing smart contracts, which reforms the transaction sequence generation as a self-evolving process through continuous reflection on feedback from the runtime environment. Then, we present the Reactive Collaborative Chain (RCC) to orchestrate the fuzzing process into multiple sub-tasks based on the dependencies of transaction sequences. Furthermore, we design a multi-agent collaborative team, where each expert agent is guided by the RCC to jointly generate and refine transaction sequences from both global and local perspectives. We conduct extensive experiments to evaluate SmartFuzz's performance on real-world contracts and DApp projects. The results demonstrate that SmartFuzz outperforms existing state-of-the-art tools: (i) it detects 5.8\%-74.7\% more vulnerabilities within 30 minutes, and (ii) it reduces false negatives by up to 80\%.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Software Supply Chain Security of Web3</title>
<link>https://arxiv.org/abs/2511.12274</link>
<guid>https://arxiv.org/abs/2511.12274</guid>
<content:encoded><![CDATA[
arXiv:2511.12274v1 Announce Type: new 
Abstract: Web3 applications, built on blockchain technology, manage billions of dollars in digital assets through decentralized applications (dApps) and smart contracts. These systems rely on complex, software supply chains that introduce significant security vulnerabilities. This paper examines the software supply chain security challenges unique to the Web3 ecosystem, where traditional Web2 software supply chain problems intersect with the immutable and high-stakes nature of blockchain technology. We analyze the threat landscape and propose mitigation strategies to strengthen the security posture of Web3 systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Decentralized Root Cause Localization Approach for Edge Computing Environments</title>
<link>https://arxiv.org/abs/2511.12486</link>
<guid>https://arxiv.org/abs/2511.12486</guid>
<content:encoded><![CDATA[
arXiv:2511.12486v1 Announce Type: new 
Abstract: Edge computing environments host increasingly complex microservice-based IoT applications, which are prone to performance anomalies that can propagate across dependent services. Identifying the true source of such anomalies, known as Root Cause Localization (RCL), is essential for timely mitigation. However, existing RCL approaches are designed for cloud environments and rely on centralized analysis, which increases latency and communication overhead when applied at the edge. This paper proposes a decentralized RCL approach that executes localization directly at the edge device level using the Personalized PageRank (PPR) algorithm. The proposed method first groups microservices into communication- and colocation-aware clusters, thereby confining most anomaly propagation within cluster boundaries. Within each cluster, PPR is executed locally to identify the root cause, significantly reducing localization time. For the rare cases where anomalies propagate across clusters, we introduce an inter-cluster peer-to-peer approximation process, enabling lightweight coordination among clusters with minimal communication overhead. To enhance the accuracy of localization in heterogeneous edge environments, we also propose a novel anomaly scoring mechanism tailored to the diverse anomaly triggers that arise across microservice, device, and network layers. Evaluation results on the publicly available edge dataset, MicroCERCL, demonstrate that the proposed decentralized approach achieves comparable or higher localization accuracy than its centralized counterpart while reducing localization time by up to 34%. These findings highlight that decentralized graph-based RCL can provide a practical and efficient solution for anomaly diagnosis in resource-constrained edge environments.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prrr: Personal Random Rewards for Blockchain Reporting</title>
<link>https://arxiv.org/abs/2511.12626</link>
<guid>https://arxiv.org/abs/2511.12626</guid>
<content:encoded><![CDATA[
arXiv:2511.12626v1 Announce Type: new 
Abstract: Smart contracts, the stateful programs running on blockchains, often rely on reports. Publishers are paid to publish these reports on the blockchain. Designing protocols that incentivize timely reporting is the prevalent reporting problem. But existing solutions face a security-performance trade-off: Relying on a small set of trusted publishers introduces centralization risks, while allowing open publication results in an excessive number of reports on the blockchain. We identify the root cause of this trade-off to be the standard symmetric reward design, which treats all reports equally. We prove that no symmetric-reward mechanism can overcome the trade-off.
  We present Personal Random Rewards for Reporting (Prrr), a protocol that assigns random heterogeneous values to reports. We call this novel mechanism-design concept Ex-Ante Synthetic Asymmetry. To the best of our knowledge, Prrr is the first game-theoretic mechanism (in any context) that deliberately forms participant asymmetry. Prrr employs a second-price-style settlement to allocate rewards, ensuring incentive compatibility and achieving both security and efficiency. Following the protocol constitutes a Subgame-Perfect Nash Equilibrium, robust against collusion and Sybil attacks. Prrr is applicable to numerous smart contracts that rely on timely reports.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bandit Learning in Housing Markets</title>
<link>https://arxiv.org/abs/2511.12629</link>
<guid>https://arxiv.org/abs/2511.12629</guid>
<content:encoded><![CDATA[
arXiv:2511.12629v1 Announce Type: new 
Abstract: The housing market, also known as one-sided matching market, is a classic exchange economy model where each agent on the demand side initially owns an indivisible good (a house) and has a personal preference over all goods. The goal is to find a core-stable allocation that exhausts all mutually beneficial exchanges among subgroups of agents. While this model has been extensively studied in economics and computer science due to its broad applications, little attention has been paid to settings where preferences are unknown and must be learned through repeated interactions. In this paper, we propose a statistical learning model within the multi-player multi-armed bandit framework, where players (agents) learn their preferences over arms (goods) from stochastic rewards. We introduce the notion of core regret for each player as the market objective. We study both centralized and decentralized approaches, proving $O(N \log T / \Delta^2)$ upper bounds on regret, where $N$ is the number of players, $T$ is the time horizon and $\Delta$ is the minimum preference gap among players. For the decentralized setting, we also establish a matching lower bound, demonstrating that our algorithm is order-optimal.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Hierarchical AI-Blockchain Framework for Real-Time Anomaly Detection in Large-Scale Autonomous Vehicle Networks</title>
<link>https://arxiv.org/abs/2511.12648</link>
<guid>https://arxiv.org/abs/2511.12648</guid>
<content:encoded><![CDATA[
arXiv:2511.12648v1 Announce Type: new 
Abstract: The security of autonomous vehicle networks is facing major challenges, owing to the complexity of sensor integration, real-time performance demands, and distributed communication protocols that expose vast attack surfaces around both individual and network-wide safety. Existing security schemes are unable to provide sub-10 ms (milliseconds) anomaly detection and distributed coordination of large-scale networks of vehicles within an acceptable safety/privacy framework. This paper introduces a three-tier hybrid security architecture HAVEN (Hierarchical Autonomous Vehicle Enhanced Network), which decouples real-time local threat detection and distributed coordination operations. It incorporates a light ensemble anomaly detection model on the edge (first layer), Byzantine-fault-tolerant federated learning to aggregate threat intelligence at a regional scale (middle layer), and selected blockchain mechanisms (top layer) to ensure critical security coordination. Extensive experimentation is done on a real-world autonomous driving dataset. Large-scale simulations with the number of vehicles ranging between 100 and 1000 and different attack types, such as sensor spoofing, jamming, and adversarial model poisoning, are conducted to test the scalability and resiliency of HAVEN. Experimental findings show sub-10 ms detection latency with an accuracy of 94% and F1-score of 92% across multimodal sensor data, Byzantine fault tolerance validated with 20\% compromised nodes, and a reduced blockchain storage overhead, guaranteeing sufficient differential privacy. The proposed framework overcomes the important trade-off between real-time safety obligation and distributed security coordination with novel three-tiered processing. The scalable architecture of HAVEN is shown to provide great improvement in detection accuracy as well as network resilience over other methods.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Atomic Gliders and CA as Language Generators (Extended Version)</title>
<link>https://arxiv.org/abs/2511.12656</link>
<guid>https://arxiv.org/abs/2511.12656</guid>
<content:encoded><![CDATA[
arXiv:2511.12656v1 Announce Type: new 
Abstract: Cellular automata (CA) are well-studied models of decentralized parallel computation, known for their ability to exhibit complex global behavior from simple local rules. While their dynamics have been widely explored through simulations, a formal treatment of CA as genuine language generators remains underdeveloped. We formalize CA-expressible languages as sets of finite words obtained by projecting the non-quiescent segments of configurations reachable by one-dimensional, deterministic, synchronous CA over bi-infinite grids. These languages are defined with respect to sets of initial configurations specified by a regular language as in regular model checking. To capture structured dynamics, we propose a glider-based generative semantics for CA. Inspired by the classical notion of gliders, we define a glider as a one-cell entity carrying a symbol in a certain velocity under well defined interaction semantics. We show that despite the regularity of the initial configurations and the locality of the transition rules, the resulting languages can exhibit non-regular and even non-context-free structure. This positions regular-initialized CA languages as a surprisingly rich computational model, with potential applications in the formal analysis of linearly ordered MAS.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Time to Consensus in a Blockchain: Insights into Bitcoin's "6 Blocks Rule''</title>
<link>https://arxiv.org/abs/2511.12687</link>
<guid>https://arxiv.org/abs/2511.12687</guid>
<content:encoded><![CDATA[
arXiv:2511.12687v1 Announce Type: new 
Abstract: We investigate the time to consensus in Nakamoto blockchains. Specifically, we consider two competing growth processes, labeled \emph{honest} and \emph{adversarial}, and determine the time after which the honest process permananetly exceeds the adversarial process. This is done via queueing techniques. The predominant difficulty is that the honest growth process is subject to \emph{random delays}. In a stylized Bitcoin model, we compute the Laplace transform for the time to consensus and verify it via simulation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Closer Look at Personalized Fine-Tuning in Heterogeneous Federated Learning</title>
<link>https://arxiv.org/abs/2511.12695</link>
<guid>https://arxiv.org/abs/2511.12695</guid>
<content:encoded><![CDATA[
arXiv:2511.12695v1 Announce Type: new 
Abstract: Federated Learning (FL) enables decentralized, privacy-preserving model training but struggles to balance global generalization and local personalization due to non-identical data distributions across clients. Personalized Fine-Tuning (PFT), a popular post-hoc solution, fine-tunes the final global model locally but often overfits to skewed client distributions or fails under domain shifts. We propose adapting Linear Probing followed by full Fine-Tuning (LP-FT), a principled centralized strategy for alleviating feature distortion (Kumar et al., 2022), to the FL setting. Through systematic evaluation across seven datasets and six PFT variants, we demonstrate LP-FT's superiority in balancing personalization and generalization. Our analysis uncovers federated feature distortion, a phenomenon where local fine-tuning destabilizes globally learned features, and theoretically characterizes how LP-FT mitigates this via phased parameter updates. We further establish conditions (e.g., partial feature overlap, covariate-concept shift) under which LP-FT outperforms standard fine-tuning, offering actionable guidelines for deploying robust personalization in FL.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Density-Driven Optimal Control for Non-Uniform Area Coverage in Decentralized Multi-Agent Systems Using Optimal Transport</title>
<link>https://arxiv.org/abs/2511.12756</link>
<guid>https://arxiv.org/abs/2511.12756</guid>
<content:encoded><![CDATA[
arXiv:2511.12756v1 Announce Type: new 
Abstract: This paper addresses the fundamental problem of non-uniform area coverage in multi-agent systems, where different regions require varying levels of attention due to mission-dependent priorities. Existing uniform coverage strategies are insufficient for realistic applications, and many non-uniform approaches either lack optimality guarantees or fail to incorporate crucial real-world constraints such as agent dynamics, limited operation time, the number of agents, and decentralized execution.
  To resolve these limitations, we propose a novel framework called Density-Driven Optimal Control (D2OC). The central idea of D2OC is the integration of optimal transport theory with multi-agent coverage control, enabling each agent to continuously adjust its trajectory to match a mission-specific reference density map. The proposed formulation establishes optimality by solving a constrained optimization problem that explicitly incorporates physical and operational constraints. The resulting control input is analytically derived from the Lagrangian of the objective function, yielding closed-form optimal solutions for linear systems and a generalizable structure for nonlinear systems. Furthermore, a decentralized data-sharing mechanism is developed to coordinate agents without reliance on global information.
  Comprehensive simulation studies demonstrate that D2OC achieves significantly improved non-uniform area coverage performance compared to existing methods, while maintaining scalability and decentralized implementability.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Look-back Horizon for Time Series Forecasting in Federated Learning</title>
<link>https://arxiv.org/abs/2511.12791</link>
<guid>https://arxiv.org/abs/2511.12791</guid>
<content:encoded><![CDATA[
arXiv:2511.12791v1 Announce Type: new 
Abstract: Selecting an appropriate look-back horizon remains a fundamental challenge in time series forecasting (TSF), particularly in the federated learning scenarios where data is decentralized, heterogeneous, and often non-independent. While recent work has explored horizon selection by preserving forecasting-relevant information in an intrinsic space, these approaches are primarily restricted to centralized and independently distributed settings. This paper presents a principled framework for adaptive horizon selection in federated time series forecasting through an intrinsic space formulation. We introduce a synthetic data generator (SDG) that captures essential temporal structures in client data, including autoregressive dependencies, seasonality, and trend, while incorporating client-specific heterogeneity. Building on this model, we define a transformation that maps time series windows into an intrinsic representation space with well-defined geometric and statistical properties. We then derive a decomposition of the forecasting loss into a Bayesian term, which reflects irreducible uncertainty, and an approximation term, which accounts for finite-sample effects and limited model capacity. Our analysis shows that while increasing the look-back horizon improves the identifiability of deterministic patterns, it also increases approximation error due to higher model complexity and reduced sample efficiency. We prove that the total forecasting loss is minimized at the smallest horizon where the irreducible loss starts to saturate, while the approximation loss continues to rise. This work provides a rigorous theoretical foundation for adaptive horizon selection for time series forecasting in federated learning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Reinforcement Learning for Heterogeneous Satellite Cluster Resources Optimization</title>
<link>https://arxiv.org/abs/2511.12792</link>
<guid>https://arxiv.org/abs/2511.12792</guid>
<content:encoded><![CDATA[
arXiv:2511.12792v1 Announce Type: new 
Abstract: This work investigates resource optimization in heterogeneous satellite clusters performing autonomous Earth Observation (EO) missions using Reinforcement Learning (RL). In the proposed setting, two optical satellites and one Synthetic Aperture Radar (SAR) satellite operate cooperatively in low Earth orbit to capture ground targets and manage their limited onboard resources efficiently. Traditional optimization methods struggle to handle the real-time, uncertain, and decentralized nature of EO operations, motivating the use of RL and Multi-Agent Reinforcement Learning (MARL) for adaptive decision-making. This study systematically formulates the optimization problem from single-satellite to multi-satellite scenarios, addressing key challenges including energy and memory constraints, partial observability, and agent heterogeneity arising from diverse payload capabilities. Using a near-realistic simulation environment built on the Basilisk and BSK-RL frameworks, we evaluate the performance and stability of state-of-the-art MARL algorithms such as MAPPO, HAPPO, and HATRPO. Results show that MARL enables effective coordination across heterogeneous satellites, balancing imaging performance and resource utilization while mitigating non-stationarity and inter-agent reward coupling. The findings provide practical insights into scalable, autonomous satellite operations and contribute a foundation for future research on intelligent EO mission planning under heterogeneous and dynamic conditions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resilient and Efficient Allocation for Large-Scale Autonomous Fleets via Decentralized Coordination</title>
<link>https://arxiv.org/abs/2511.12879</link>
<guid>https://arxiv.org/abs/2511.12879</guid>
<content:encoded><![CDATA[
arXiv:2511.12879v1 Announce Type: new 
Abstract: Operating large autonomous fleets demands fast, resilient allocation of scarce resources (such as energy and fuel, charger access and maintenance slots, time windows, and communication bandwidth) under uncertainty. We propose a side-information-aware approach for resource allocation at scale that combines distributional predictions with decentralized coordination. Local side information shapes per-agent risk models for consumption, which are coupled through chance constraints on failures. A lightweight consensus-ADMM routine coordinates agents over a sparse communication graph, enabling near-centralized performance while avoiding single points of failure. We validate the framework on real urban road networks with autonomous vehicles and on a representative satellite constellation, comparing against greedy, no-side-information, and oracle central baselines. Our method reduces failure rates by 30-55% at matched cost and scales to thousands of agents with near-linear runtime, while preserving feasibility with high probability.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Esim: EVM Bytecode Similarity Detection Based on Stable-Semantic Graph</title>
<link>https://arxiv.org/abs/2511.12971</link>
<guid>https://arxiv.org/abs/2511.12971</guid>
<content:encoded><![CDATA[
arXiv:2511.12971v1 Announce Type: new 
Abstract: Decentralized finance (DeFi) is experiencing rapid expansion. However, prevalent code reuse and limited open-source contributions have introduced significant challenges to the blockchain ecosystem, including plagiarism and the propagation of vulnerable code. Consequently, an effective and accurate similarity detection method for EVM bytecode is urgently needed to identify similar contracts. Traditional binary similarity detection methods are typically based on instruction stream or control flow graph (CFG), which have limitations on EVM bytecode due to specific features like low-level EVM bytecode and heavily-reused basic blocks. Moreover, the highly-diverse Solidity Compiler (Solc) versions further complicate accurate similarity detection.
  Motivated by these challenges, we propose a novel EVM bytecode representation called Stable-Semantic Graph (SSG), which captures relationships between 'stable instructions' (special instructions identified by our study). Moreover, we implement a prototype, Esim, which embeds SSG into matrices for similarity detection using a heterogeneous graph neural network. Esim demonstrates high accuracy in SSG construction, achieving F1-scores of 100% for control flow and 95.16% for data flow, and its similarity detection performance reaches 96.3% AUC, surpassing traditional approaches. Our large-scale study, analyzing 2,675,573 smart contracts on six EVM-compatible chains over a one-year period, also demonstrates that Esim outperforms the SOTA tool Etherscan in vulnerability search.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MEV in Multiple Concurrent Proposer Blockchains</title>
<link>https://arxiv.org/abs/2511.13080</link>
<guid>https://arxiv.org/abs/2511.13080</guid>
<content:encoded><![CDATA[
arXiv:2511.13080v1 Announce Type: new 
Abstract: We analyze maximal extractable value in multiple concurrent proposer blockchains, where multiple blocks become data available before their final execution order is determined. This concurrency breaks the single builder assumption of sequential chains and introduces new MEV channels, including same tick duplicate steals, proposer to proposer auctions, and timing races driven by proof of availability latency. We develop a hazard normalized model of delay and inclusion, derive a closed form delay envelope \(M(\tau)\), and characterize equilibria for censorship, duplication, and auction games. We show how deterministic priority DAG scheduling and duplicate aware payouts neutralize same tick MEV while preserving throughput, identifying simple protocol configurations to mitigate MCP specific extraction without centralized builders.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Personalized Federated Learning with Bidirectional Communication Compression via One-Bit Random Sketching</title>
<link>https://arxiv.org/abs/2511.13144</link>
<guid>https://arxiv.org/abs/2511.13144</guid>
<content:encoded><![CDATA[
arXiv:2511.13144v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative training across decentralized data, but faces key challenges of bidirectional communication overhead and client-side data heterogeneity. To address communication costs while embracing data heterogeneity, we propose pFed1BS, a novel personalized federated learning framework that achieves extreme communication compression through one-bit random sketching. In personalized FL, the goal shifts from training a single global model to creating tailored models for each client. In our framework, clients transmit highly compressed one-bit sketches, and the server aggregates and broadcasts a global one-bit consensus. To enable effective personalization, we introduce a sign-based regularizer that guides local models to align with the global consensus while preserving local data characteristics. To mitigate the computational burden of random sketching, we employ the Fast Hadamard Transform for efficient projection. Theoretical analysis guarantees that our algorithm converges to a stationary neighborhood of the global potential function. Numerical simulations demonstrate that pFed1BS substantially reduces communication costs while achieving competitive performance compared to advanced communication-efficient FL algorithms.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pico-Cloud: Cloud Infrastructure for Tiny Edge Devices</title>
<link>https://arxiv.org/abs/2511.13253</link>
<guid>https://arxiv.org/abs/2511.13253</guid>
<content:encoded><![CDATA[
arXiv:2511.13253v1 Announce Type: new 
Abstract: This paper introduces the Pico-Cloud, a micro-edge cloud architecture built on ultra-minimal hardware platforms such as the Raspberry Pi Zero and comparable single-board computers. The Pico-Cloud delivers container-based virtualization, service discovery, and lightweight orchestration directly at the device layer, enabling local operation with low latency and low power consumption without reliance on centralized data centers. We present its architectural model, outline representative use cases including rural connectivity, educational clusters, and edge AI inference, and analyze design challenges in computation, networking, storage, and power management. The results highlight Pico-Clouds as a cost-effective, decentralized, and sustainable platform for lightweight distributed workloads at the network edge.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributed Hierarchical Machine Learning for Joint Resource Allocation and Slice Selection in In-Network Edge Systems</title>
<link>https://arxiv.org/abs/2511.13313</link>
<guid>https://arxiv.org/abs/2511.13313</guid>
<content:encoded><![CDATA[
arXiv:2511.13313v1 Announce Type: new 
Abstract: The Metaverse promises immersive, real-time experiences; however, meeting its stringent latency and resource demands remains a major challenge. Conventional optimization techniques struggle to respond effectively under dynamic edge conditions and high user loads. In this study, we explore a slice-enabled in-network edge architecture that combines computing-in-the-network (COIN) with multi-access edge computing (MEC). In addition, we formulate the joint problem of wireless and computing resource management with optimal slice selection as a mixed-integer nonlinear program (MINLP). Because solving this model online is computationally intensive, we decompose it into three sub-problems (SP1) intra-slice allocation, (SP2) inter-slice allocation, and (SP3) offloading decision and train a distributed hierarchical DeepSets-based model (DeepSets-S) on optimal solutions obtained offline. In the proposed model, we design a slack-aware normalization mechanism for a shared encoder and task-specific decoders, ensuring permutation equivariance over variable-size wireless device (WD) sets. The learned system produces near-optimal allocations with low inference time and maintains permutation equivariance over variable-size device sets. Our experimental results show that DeepSets-S attains high tolerance-based accuracies on SP1/SP2 (Acc1 = 95.26% and 95.67%) and improves multiclass offloading accuracy on SP3 (Acc = 0.7486; binary local/offload Acc = 0.8824). Compared to exact solvers, the proposed approach reduces the execution time by 86.1%, while closely tracking the optimal system cost (within 6.1% in representative regimes). Compared with baseline models, DeepSets-S consistently achieves higher cost ratios and better utilization across COIN/MEC resources.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LinkXplore: A Framework for Affordable High-Quality Blockchain Data</title>
<link>https://arxiv.org/abs/2511.13318</link>
<guid>https://arxiv.org/abs/2511.13318</guid>
<content:encoded><![CDATA[
arXiv:2511.13318v1 Announce Type: new 
Abstract: Blockchain technologies are rapidly transforming both academia and industry. However, large-scale blockchain data collection remains prohibitively expensive, as many RPC providers only offer enhanced APIs with high pricing tiers that are unsuitable for budget-constrained research or industrial-scale applications, which has significantly slowed down academic studies and product development. Moreover, there is a clear lack of a systematic framework that allows flexible integration of new modules for analyzing on-chain data.
  To address these challenges, we introduce LinkXplore, the first open framework for collecting and managing on-chain data. LinkXplore enables users to bypass costly blockchain data providers by directly analyzing raw data from RPC queries or streams, thereby offering high-quality blockchain data at a fraction of the cost. Through a simple API and backend processing logic, any type of chain data can be integrated into the framework. This makes it a practical alternative for both researchers and developers with limited budgets. Code and dataset used in this project are publicly available at https://github.com/Linkis-Project/LinkXplore
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Merkle Mountain Belt</title>
<link>https://arxiv.org/abs/2511.13582</link>
<guid>https://arxiv.org/abs/2511.13582</guid>
<content:encoded><![CDATA[
arXiv:2511.13582v1 Announce Type: new 
Abstract: Merkle structures are widely used as commitment schemes: they allow a prover to publish a compact commitment to an ordered list $X$ of items, and then efficiently prove to a verifier that $x_i\in X$ is the $i$-th item in it. We compare different Merkle structures and their corresponding properties as commitment schemes in the context of blockchain applications. Our primary goal is to speed up light client protocols so that, e.g., a user can verify a transaction efficiently from their smartphone.
  For instance, the Merkle Mountain Range (MMR) yields a succinct scheme: a light client synchronizing for the first time can do so with a complexity sublinear in $|X|$. On the other hand, the Merkle chain, traditionally used to commit to block headers, is not succinct, but it is incremental - a light client resynchronizing frequently can do so with constant complexity - and optimally additive - the structure can be updated in constant time when a new item is appended to list $X$.
  We introduce new Merkle structures, most notably the Merkle Mountain Belt (MMB), the first to be simultaneously succinct, incremental and optimally additive. A variant called UMMB is also asynchronous: a light client may continue to interact with the network even when out of sync with the public commitment. Our Merkle structures are slightly unbalanced, so that items recently appended to $X$ receive shorter membership proofs than older items. This feature reduces a light client's expected costs, in applications where queries are biased towards recently generated data.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Client-Server Watermarking for Split Federated Learning</title>
<link>https://arxiv.org/abs/2511.13598</link>
<guid>https://arxiv.org/abs/2511.13598</guid>
<content:encoded><![CDATA[
arXiv:2511.13598v1 Announce Type: new 
Abstract: Split Federated Learning (SFL) is renowned for its privacy-preserving nature and low computational overhead among decentralized machine learning paradigms. In this framework, clients employ lightweight models to process private data locally and transmit intermediate outputs to a powerful server for further computation. However, SFL is a double-edged sword: while it enables edge computing and enhances privacy, it also introduces intellectual property ambiguity as both clients and the server jointly contribute to training. Existing watermarking techniques fail to protect both sides since no single participant possesses the complete model. To address this, we propose RISE, a Robust model Intellectual property protection scheme using client-Server watermark Embedding for SFL. Specifically, RISE adopts an asymmetric client-server watermarking design: the server embeds feature-based watermarks through a loss regularization term, while clients embed backdoor-based watermarks by injecting predefined trigger samples into private datasets. This co-embedding strategy enables both clients and the server to verify model ownership. Experimental results on standard datasets and multiple network architectures show that RISE achieves over $95\%$ watermark detection rate ($p-value \lt 0.03$) across most settings. It exhibits no mutual interference between client- and server-side watermarks and remains robust against common removal attacks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DIGing--SGLD: Decentralized and Scalable Langevin Sampling over Time--Varying Networks</title>
<link>https://arxiv.org/abs/2511.12836</link>
<guid>https://arxiv.org/abs/2511.12836</guid>
<content:encoded><![CDATA[
arXiv:2511.12836v1 Announce Type: cross 
Abstract: Sampling from a target distribution induced by training data is central to Bayesian learning, with Stochastic Gradient Langevin Dynamics (SGLD) serving as a key tool for scalable posterior sampling and decentralized variants enabling learning when data are distributed across a network of agents. This paper introduces DIGing-SGLD, a decentralized SGLD algorithm designed for scalable Bayesian learning in multi-agent systems operating over time-varying networks. Existing decentralized SGLD methods are restricted to static network topologies, and many exhibit steady-state sampling bias caused by network effects, even when full batches are used. DIGing-SGLD overcomes these limitations by integrating Langevin-based sampling with the gradient-tracking mechanism of the DIGing algorithm, originally developed for decentralized optimization over time-varying networks, thereby enabling efficient and bias-free sampling without a central coordinator. To our knowledge, we provide the first finite-time non-asymptotic Wasserstein convergence guarantees for decentralized SGLD-based sampling over time-varying networks, with explicit constants. Under standard strong convexity and smoothness assumptions, DIGing-SGLD achieves geometric convergence to an $O(\sqrt{\eta})$ neighborhood of the target distribution, where $\eta$ is the stepsize, with dependence on the target accuracy matching the best-known rates for centralized and static-network SGLD algorithms using constant stepsize. Numerical experiments on Bayesian linear and logistic regression validate the theoretical results and demonstrate the strong empirical performance of DIGing-SGLD under dynamically evolving network conditions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mysticeti: Reaching the Limits of Latency with Uncertified DAGs</title>
<link>https://arxiv.org/abs/2310.14821</link>
<guid>https://arxiv.org/abs/2310.14821</guid>
<content:encoded><![CDATA[
arXiv:2310.14821v5 Announce Type: replace 
Abstract: We introduce Mysticeti-C, the first DAG-based Byzantine consensus protocol to achieve the lower bounds of latency of 3 message rounds. Since Mysticeti-C is built over DAGs it also achieves high resource efficiency and censorship resistance. Mysticeti-C achieves this latency improvement by avoiding explicit certification of the DAG blocks and by proposing a novel commit rule such that every block can be committed without delays, resulting in optimal latency in the steady state and under crash failures. We further extend Mysticeti-C to Mysticeti-FPC, which incorporates a fast commit path that achieves even lower latency for transferring assets. Unlike prior fast commit path protocols, Mysticeti-FPC minimizes the number of signatures and messages by weaving the fast path transactions into the DAG. This frees up resources, which subsequently result in better performance. We prove the safety and liveness in a Byzantine context. We evaluate both Mysticeti protocols and compare them with state-of-the-art consensus and fast path protocols to demonstrate their low latency and resource efficiency, as well as their more graceful degradation under crash failures. Mysticeti-C is the first Byzantine consensus protocol to achieve WAN latency of 0.5s for consensus commit while simultaneously maintaining state-of-the-art throughput of over 200k TPS. Finally, we report on integrating Mysticeti-C as the consensus protocol into the Sui blockchain, resulting in over 4x latency reduction.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bounds of Block Rewards in Honest PinFi Systems</title>
<link>https://arxiv.org/abs/2404.02174</link>
<guid>https://arxiv.org/abs/2404.02174</guid>
<content:encoded><![CDATA[
arXiv:2404.02174v2 Announce Type: replace 
Abstract: PinFi is a class of novel protocols for decentralized pricing of dissipative assets, whose value naturally declines over time. Central to the protocol's functionality and its market efficiency is the role of liquidity providers (LPs). This study addresses critical stability and sustainability challenges within the protocol, namely: the propensity of LPs to prefer selling in external markets over participation in the protocol; a similar inclination towards selling within the PinFi system rather than contributing as LPs; and a scenario where LPs are disinclined to sell within the protocol. Employing a game-theoretic approach, we explore PinFi's mechanisms and its broader ramifications. Our findings reveal that, under a variety of common conditions and with an assumption of participant integrity, PinFi is capable of fostering a dynamic equilibrium among LPs, sellers, and buyers. This balance is maintained through a carefully calibrated range of block rewards for LPs, ensuring the protocol's long-term stability and utility.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LooPIN: A PinFi protocol for decentralized computing</title>
<link>https://arxiv.org/abs/2406.09422</link>
<guid>https://arxiv.org/abs/2406.09422</guid>
<content:encoded><![CDATA[
arXiv:2406.09422v2 Announce Type: replace 
Abstract: Networked computing power is a critical utility in the era of artificial intelligence. This paper presents a novel Physical Infrastructure Finance (PinFi) protocol designed to facilitate the distribution of computing power within networks in a decentralized manner. Addressing the core challenges of coordination, pricing, and liquidity in decentralized physical infrastructure networks (DePIN), the PinFi protocol introduces a distinctive dynamic pricing mechanism. It enables providers to allocate excess computing resources to a "dissipative" PinFi liquidity pool, distinct from traditional DeFi liquidity pools, ensuring seamless access for clients at equitable, market-based prices. This approach significantly reduces the costs of accessing computing power, potentially to as low as 1% compared to existing services, while simultaneously enhancing security and dependability. The PinFi protocol is poised to transform the dynamics of supply and demand in computing power networks, setting a new standard for efficiency and accessibility.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Satellite Swarm Deployment via Distance-based Orbital Transition Under $J_2$ Perturbation</title>
<link>https://arxiv.org/abs/2507.01769</link>
<guid>https://arxiv.org/abs/2507.01769</guid>
<content:encoded><![CDATA[
arXiv:2507.01769v2 Announce Type: replace 
Abstract: This paper presents an autonomous guidance and control strategy for a satellite swarm that enables scalable distributed space structures for innovative science and business opportunities. The averaged $J_2$ orbital parameters that describe the drift and periodic orbital motion were derived along with their target values to achieve a distributed space structure in a decentralized manner. This enabled the design of a distance-based orbital stabilizer to ensure autonomous deployment into a monolithic formation of a coplanar equidistant configuration on a user-defined orbital plane. Continuous formation control was assumed to be achieved through fuel-free actuation, such as satellite magnetic field interaction and differential aerodynamic forces, thereby maintaining long-term formation stability without thruster usage. A major challenge for such actuation systems is the potential loss of control capability due to increasing inter-satellite distances resulting from unstable orbital dynamics, particularly for autonomous satellite swarms. To mitigate this risk, our decentralized deployment controller minimized drift distance during unexpected communication outages. As a case study, we consider the deployment of palm-sized satellites into a coplanar equidistant formation in a $J_2$-perturbed orbit. Moreover, centralized grouping strategies are presented.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Linearly Homomorphic Ring Signature Scheme over Lattices</title>
<link>https://arxiv.org/abs/2507.02281</link>
<guid>https://arxiv.org/abs/2507.02281</guid>
<content:encoded><![CDATA[
arXiv:2507.02281v3 Announce Type: replace 
Abstract: Homomorphic ring signature schemes combine the strong anonymity of ring signatures with the computability of homomorphic signatures, demonstrating significant potential in scenarios requiring both anonymous data provenance and verifiable homomorphic computation (e.g., confidential blockchain transactions and secure multi-party computation). However, no feasible homomorphic ring signature scheme currently exists.
  In this work, we propose the first lattice-based linearly homomorphic ring signature scheme. Proven secure in the standard model under the small integer solution (SIS) assumption, our scheme achieves strong anonymity under full key exposure and unforgeability against insider corruption attacks. As the first unified framework for ring signatures and linear homomorphic signatures, this construction provides a post-quantum-secure solution for the aforementioned applications, advancing the development of privacy-enhanced homomorphic computation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Lightweight Approach for State Machine Replication</title>
<link>https://arxiv.org/abs/2509.17771</link>
<guid>https://arxiv.org/abs/2509.17771</guid>
<content:encoded><![CDATA[
arXiv:2509.17771v2 Announce Type: replace 
Abstract: We present a lightweight solution for state machine replication with commitment certificates. Specifically, we adapt and analyze a median rule for the stabilizing consensus problem [Doerr11] to operate in a client-server setting where arbitrary servers may be blocked adaptively based on past system information. We further extend our protocol by compressing information about committed commands, thus keeping the protocol lightweight, while still enabling clients to easily prove that their commands have indeed been committed on the shared state. Our approach guarantees liveness as long as at most a constant fraction of servers are blocked, ensures safety under any number of blocked servers, and supports fast recovery even after all servers are blocked. In addition to offering near-optimal asymptotic performance in several respects, our method is fully decentralized, unlike other near-optimal solutions that rely on leaders. In particular, our solution is robust against adversaries that target key servers (which captures insider-based denial-of-service attacks), whereas leader-based approaches fail under such a blocking model.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupling Bias, Aligning Distributions: Synergistic Fairness Optimization for Deepfake Detection</title>
<link>https://arxiv.org/abs/2511.10150</link>
<guid>https://arxiv.org/abs/2511.10150</guid>
<content:encoded><![CDATA[
arXiv:2511.10150v2 Announce Type: replace 
Abstract: Fairness is a core element in the trustworthy deployment of deepfake detection models, especially in the field of digital identity security. Biases in detection models toward different demographic groups, such as gender and race, may lead to systemic misjudgments, exacerbating the digital divide and social inequities. However, current fairness-enhanced detectors often improve fairness at the cost of detection accuracy. To address this challenge, we propose a dual-mechanism collaborative optimization framework. Our proposed method innovatively integrates structural fairness decoupling and global distribution alignment: decoupling channels sensitive to demographic groups at the model architectural level, and subsequently reducing the distance between the overall sample distribution and the distributions corresponding to each demographic group at the feature level. Experimental results demonstrate that, compared with other methods, our framework improves both inter-group and intra-group fairness while maintaining overall detection accuracy across domains.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Price Competition under Generalized Linear Demands</title>
<link>https://arxiv.org/abs/2511.10718</link>
<guid>https://arxiv.org/abs/2511.10718</guid>
<content:encoded><![CDATA[
arXiv:2511.10718v1 Announce Type: new 
Abstract: We study sequential price competition among $N$ sellers, each influenced by the pricing decisions of their rivals. Specifically, the demand function for each seller $i$ follows the single index model $\lambda_i(\mathbf{p}) = \mu_i(\langle \boldsymbol{\theta}_{i,0}, \mathbf{p} \rangle)$, with known increasing link $\mu_i$ and unknown parameter $\boldsymbol{\theta}_{i,0}$, where the vector $\mathbf{p}$ denotes the vector of prices offered by all the sellers simultaneously at a given instant. Each seller observes only their own realized demand -- unobservable to competitors -- and the prices set by rivals. Our framework generalizes existing approaches that focus solely on linear demand models. We propose a novel decentralized policy, PML-GLUCB, that combines penalized MLE with an upper-confidence pricing rule, removing the need for coordinated exploration phases across sellers -- which is integral to previous linear models -- and accommodating both binary and real-valued demand observations. Relative to a dynamic benchmark policy, each seller achieves $O(N^{2}\sqrt{T}\log(T))$ regret, which essentially matches the optimal rate known in the linear setting. A significant technical contribution of our work is the development of a variant of the elliptical potential lemma -- typically applied in single-agent systems -- adapted to our competitive multi-agent environment.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Welfare in Noncooperative Network Formation under Attack</title>
<link>https://arxiv.org/abs/2511.10845</link>
<guid>https://arxiv.org/abs/2511.10845</guid>
<content:encoded><![CDATA[
arXiv:2511.10845v1 Announce Type: new 
Abstract: Communication networks are essential for our economy and our everyday lives. This makes them lucrative targets for attacks. Today, we see an ongoing battle between criminals that try to disrupt our key communication networks and security professionals that try to mitigate these attacks. However, today's networks, like the Internet or peer-to-peer networks among smart devices, are not controlled by a single authority, but instead consist of many independently administrated entities that are interconnected. Thus, both the decisions of how to interconnect and how to secure against potential attacks are taken in a decentralized way by selfish agents.
  This strategic setting, with agents that want to interconnect and potential attackers that want to disrupt the network, was captured via an influential game-theoretic model by Goyal, Jabbari, Kearns, Khanna, and Morgenstern (WINE 2016). We revisit this model and show improved tight bounds on the achieved robustness of networks created by selfish agents. As our main result, we show that such networks can resist attacks of a large class of potential attackers, i.e., these networks maintain asymptotically optimal welfare post attack. This improves several bounds and resolves an open problem. Along the way, we show the counter-intuitive result, that attackers that aim at minimizing the social welfare post attack do not actually inflict the greatest possible damage.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Swarm Control via SO(3) Embeddings for 3D Trajectories</title>
<link>https://arxiv.org/abs/2511.10858</link>
<guid>https://arxiv.org/abs/2511.10858</guid>
<content:encoded><![CDATA[
arXiv:2511.10858v1 Announce Type: new 
Abstract: This paper presents a novel decentralized approach for achieving emergent behavior in multi-agent systems with minimal information sharing. Based on prior work in simple orbits, our method produces a broad class of stable, periodic trajectories by stabilizing the system around a Lie group-based geometric embedding. Employing the Lie group SO(3), we generate a wider range of periodic curves than existing quaternion-based methods. Furthermore, we exploit SO(3) properties to eliminate the need for velocity inputs, allowing agents to receive only position inputs. We also propose a novel phase controller that ensures uniform agent separation, along with a formal stability proof. Validation through simulations and experiments showcases the method's adaptability to complex low-level dynamics and disturbances.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Federated Clustering: A Client-wise Private Graph Aggregation Framework</title>
<link>https://arxiv.org/abs/2511.10915</link>
<guid>https://arxiv.org/abs/2511.10915</guid>
<content:encoded><![CDATA[
arXiv:2511.10915v1 Announce Type: new 
Abstract: Federated clustering addresses the critical challenge of extracting patterns from decentralized, unlabeled data. However, it is hampered by the flaw that current approaches are forced to accept a compromise between performance and privacy: \textit{transmitting embedding representations risks sensitive data leakage, while sharing only abstract cluster prototypes leads to diminished model accuracy}. To resolve this dilemma, we propose Structural Privacy-Preserving Federated Graph Clustering (SPP-FGC), a novel algorithm that innovatively leverages local structural graphs as the primary medium for privacy-preserving knowledge sharing, thus moving beyond the limitations of conventional techniques. Our framework operates on a clear client-server logic; on the client-side, each participant constructs a private structural graph that captures intrinsic data relationships, which the server then securely aggregates and aligns to form a comprehensive global graph from which a unified clustering structure is derived. The framework offers two distinct modes to suit different needs. SPP-FGC is designed as an efficient one-shot method that completes its task in a single communication round, ideal for rapid analysis. For more complex, unstructured data like images, SPP-FGC+ employs an iterative process where clients and the server collaboratively refine feature representations to achieve superior downstream performance. Extensive experiments demonstrate that our framework achieves state-of-the-art performance, improving clustering accuracy by up to 10\% (NMI) over federated baselines while maintaining provable privacy guarantees.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exposing Weak Links in Multi-Agent Systems under Adversarial Prompting</title>
<link>https://arxiv.org/abs/2511.10949</link>
<guid>https://arxiv.org/abs/2511.10949</guid>
<content:encoded><![CDATA[
arXiv:2511.10949v1 Announce Type: new 
Abstract: LLM-based agents are increasingly deployed in multi-agent systems (MAS). As these systems move toward real-world applications, their security becomes paramount. Existing research largely evaluates single-agent security, leaving a critical gap in understanding the vulnerabilities introduced by multi-agent design. However, existing systems fall short due to lack of unified frameworks and metrics focusing on unique rejection modes in MAS. We present SafeAgents, a unified and extensible framework for fine-grained security assessment of MAS. SafeAgents systematically exposes how design choices such as plan construction strategies, inter-agent context sharing, and fallback behaviors affect susceptibility to adversarial prompting. We introduce Dharma, a diagnostic measure that helps identify weak links within multi-agent pipelines. Using SafeAgents, we conduct a comprehensive study across five widely adopted multi-agent architectures (centralized, decentralized, and hybrid variants) on four datasets spanning web tasks, tool use, and code generation. Our findings reveal that common design patterns carry significant vulnerabilities. For example, centralized systems that delegate only atomic instructions to sub-agents obscure harmful objectives, reducing robustness. Our results highlight the need for security-aware design in MAS. Link to code is https://github.com/microsoft/SafeAgents
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structural asymmetry as a fraud signature: detecting collusion with Heron's Information Coefficient</title>
<link>https://arxiv.org/abs/2511.10957</link>
<guid>https://arxiv.org/abs/2511.10957</guid>
<content:encoded><![CDATA[
arXiv:2511.10957v1 Announce Type: new 
Abstract: Fraud in public procurement remains a persistent challenge, especially in large, decentralized systems like Brazil's Unified Health System. We introduce Heron's Information Coefficient (HIC), a geometric measure that quantifies how subgraphs deviate from the global structure of a network. Applied to over eight years of Brazilian bidding data for medical supplies, this measure highlights collusive patterns that standard indicators may overlook. Unlike conventional robustness metrics, the Heron coefficient focuses on the interaction between active and inactive subgraphs, revealing structural shifts that may signal coordinated behavior, such as cartel formation. Synthetic experiments support these findings, demonstrating strong detection performance across varying corruption intensities and network sizes. While our results do not replace legal or economic analyses, they offer an effective complementary tool for auditors and policymakers to monitor procurement integrity more effectively. This study demonstrates that simple geometric insight can reveal hidden dynamics in real-world networks better than other Information Theoretic metrics.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When to Stop Federated Learning: Zero-Shot Generation of Synthetic Validation Data with Generative AI for Early Stopping</title>
<link>https://arxiv.org/abs/2511.11208</link>
<guid>https://arxiv.org/abs/2511.11208</guid>
<content:encoded><![CDATA[
arXiv:2511.11208v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative model training across decentralized devices while preserving data privacy. However, FL methods typically run for a predefined number of global rounds, often leading to unnecessary computation when optimal performance is reached earlier. In addition, training may continue even when the model fails to achieve meaningful performance. To address this inefficiency, we introduce a zero-shot synthetic validation framework that leverages generative AI to monitor model performance and determine early stopping points. Our approach adaptively stops training near the optimal round, thereby conserving computational resources and enabling rapid hyperparameter adjustments. Numerical results on multi-label chest X-ray classification demonstrate that our method reduces training rounds by up to 74% while maintaining accuracy within 1% of the optimal.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Local and Federated Data Normalization in Federated Learning: A Privacy-Preserving Approach</title>
<link>https://arxiv.org/abs/2511.11249</link>
<guid>https://arxiv.org/abs/2511.11249</guid>
<content:encoded><![CDATA[
arXiv:2511.11249v1 Announce Type: new 
Abstract: Data normalization is a crucial preprocessing step for enhancing model performance and training stability. In federated learning (FL), where data remains distributed across multiple parties during collaborative model training, normalization presents unique challenges due to the decentralized and often heterogeneous nature of the data. Traditional methods rely on either independent client-side processing, i.e., local normalization, or normalizing the entire dataset before distributing it to parties, i.e., pooled normalization. Local normalization can be problematic when data distributions across parties are non-IID, while the pooled normalization approach conflicts with the decentralized nature of FL. In this paper, we explore the adaptation of widely used normalization techniques to FL and define the term federated normalization. Federated normalization simulates pooled normalization by enabling the collaborative exchange of normalization parameters among parties. Thus, it achieves performance on par with pooled normalization without compromising data locality. However, sharing normalization parameters such as the mean introduces potential privacy risks, which we further mitigate through a robust privacy-preserving solution. Our contributions include: (i) We systematically evaluate the impact of various federated and local normalization techniques in heterogeneous FL scenarios, (ii) We propose a novel homomorphically encrypted $k$-th ranked element (and median) calculation tailored for the federated setting, enabling secure and efficient federated normalization, (iii) We propose privacy-preserving implementations of widely used normalization techniques for FL, leveraging multiparty fully homomorphic encryption (MHE).
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt Engineering vs. Fine-Tuning for LLM-Based Vulnerability Detection in Solana and Algorand Smart Contracts</title>
<link>https://arxiv.org/abs/2511.11250</link>
<guid>https://arxiv.org/abs/2511.11250</guid>
<content:encoded><![CDATA[
arXiv:2511.11250v1 Announce Type: new 
Abstract: Smart contracts have emerged as key components within decentralized environments, enabling the automation of transactions through self-executing programs. While these innovations offer significant advantages, they also present potential drawbacks if the smart contract code is not carefully designed and implemented. This paper investigates the capability of large language models (LLMs) to detect OWASP-inspired vulnerabilities in smart contracts beyond the Ethereum Virtual Machine (EVM) ecosystem, focusing specifically on Solana and Algorand. Given the lack of labeled datasets for non-EVM platforms, we design a synthetic dataset of annotated smart contract snippets in Rust (for Solana) and PyTeal (for Algorand), structured around a vulnerability taxonomy derived from OWASP. We evaluate LLMs under three configurations: prompt engineering, fine-tuning, and a hybrid of both, comparing their performance on different vulnerability categories. Experimental results show that prompt engineering achieves general robustness, while fine-tuning improves precision and recall on less semantically rich languages such as TEAL. Additionally, we analyze how the architectural differences of Solana and Algorand influence the manifestation and detectability of vulnerabilities, offering platform-specific mappings that highlight limitations in existing security tooling. Our findings suggest that LLM-based approaches are viable for static vulnerability detection in smart contracts, provided domain-specific data and categorization are integrated into training pipelines.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Convergence Analysis for Semi-Decentralized Learning: Sampled-to-Sampled vs. Sampled-to-All Communication</title>
<link>https://arxiv.org/abs/2511.11560</link>
<guid>https://arxiv.org/abs/2511.11560</guid>
<content:encoded><![CDATA[
arXiv:2511.11560v1 Announce Type: new 
Abstract: In semi-decentralized federated learning, devices primarily rely on device-to-device communication but occasionally interact with a central server. Periodically, a sampled subset of devices uploads their local models to the server, which computes an aggregate model. The server can then either (i) share this aggregate model only with the sampled clients (sampled-to-sampled, S2S) or (ii) broadcast it to all clients (sampled-to-all, S2A). Despite their practical significance, a rigorous theoretical and empirical comparison of these two strategies remains absent. We address this gap by analyzing S2S and S2A within a unified convergence framework that accounts for key system parameters: sampling rate, server aggregation frequency, and network connectivity. Our results, both analytical and experimental, reveal distinct regimes where one strategy outperforms the other, depending primarily on the degree of data heterogeneity across devices. These insights lead to concrete design guidelines for practical semi-decentralized FL deployments.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement</title>
<link>https://arxiv.org/abs/2505.12684</link>
<guid>https://arxiv.org/abs/2505.12684</guid>
<content:encoded><![CDATA[
arXiv:2505.12684v2 Announce Type: replace 
Abstract: Recent advances in graph machine learning have shifted to data-centric paradigms, driven by two emerging fields: (1) Federated graph learning (FGL) enables multi-client collaboration but faces challenges from data and task heterogeneity, limiting its practicality; (2) Graph foundation models (GFM) offer strong domain generalization but are usually trained on single machines, missing out on cross-silo data and resources.
  These paradigms are complementary, and their integration brings notable benefits. Motivated by this, we propose FedGFM, a novel decentralized GFM training paradigm. However, a key challenge is knowledge entanglement, where multi-domain knowledge merges into indistinguishable representations, hindering downstream adaptation.
  To address this, we present FedGFM+, an enhanced framework with two core modules to reduce knowledge entanglement: (1) AncDAI: A global anchor-based domain-aware initialization strategy. Before pre-training, each client encodes its local graph into domain-specific prototypes that serve as semantic anchors. Synthetic embeddings around these anchors initialize the global model. We theoretically prove these prototypes are distinguishable across domains, providing a strong inductive bias to disentangle domain-specific knowledge. (2) AdaDPP: A local adaptive domain-sensitive prompt pool. Each client learns a lightweight graph prompt capturing domain semantics during pre-training. During fine-tuning, prompts from all clients form a pool from which the GFM selects relevant prompts to augment target graph attributes, improving downstream adaptation.
  FedGFM+ is evaluated on 8 diverse benchmarks across multiple domains and tasks, outperforming 20 baselines from supervised learning, FGL, and federated GFM variants.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ADMM Penalty Parameter Evaluation for Networked Microgrid Energy Management</title>
<link>https://arxiv.org/abs/2511.08750</link>
<guid>https://arxiv.org/abs/2511.08750</guid>
<content:encoded><![CDATA[
arXiv:2511.08750v1 Announce Type: new 
Abstract: The alternating direction method of multipliers (ADMM) is a powerful algorithm for solving decentralized optimization problems including networked microgrid energy management (NetMEM). However, its performance is highly sensitive to the selection of its penalty parameter \r{ho}, which can lead to slow convergence, suboptimal solutions, or even algorithm divergence. This paper evaluates and compares three district ADMM formulations to solve the NetMEM problem, which explore different methods to determine appropriate stopping points, aiming to yield high-quality solutions. Furthermore, an adaptive penalty heuristic is also incorporated into each method to analyze its potential impact on ADMM performance. Different case studies on networks of varying sizes demonstrate that an objective-based ADMM approach, denominated as OB-ADMM, is significantly more robust to the choice of \r{ho}, consistently yielding solutions closer to the centralized optimal benchmark by preventing premature algorithm stopping.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Signature, Multiple Payments: Demystifying and Detecting Signature Replay Vulnerabilities in Smart Contracts</title>
<link>https://arxiv.org/abs/2511.09134</link>
<guid>https://arxiv.org/abs/2511.09134</guid>
<content:encoded><![CDATA[
arXiv:2511.09134v1 Announce Type: new 
Abstract: Smart contracts have significantly advanced blockchain technology, and digital signatures are crucial for reliable verification of contract authority. Through signature verification, smart contracts can ensure that signers possess the required permissions, thus enhancing security and scalability. However, lacking checks on signature usage conditions can lead to repeated verifications, increasing the risk of permission abuse and threatening contract assets. We define this issue as the Signature Replay Vulnerability (SRV). In this paper, we conducted the first empirical study to investigate the causes and characteristics of the SRVs. From 1,419 audit reports across 37 blockchain security companies, we identified 108 with detailed SRV descriptions and classified five types of SRVs. To detect these vulnerabilities automatically, we designed LASiR, which utilizes the general semantic understanding ability of Large Language Models (LLMs) to assist in the static taint analysis of the signature state and identify the signature reuse behavior. It also employs path reachability verification via symbolic execution to ensure effective and reliable detection. To evaluate the performance of LASiR, we conducted large-scale experiments on 15,383 contracts involving signature verification, selected from the initial dataset of 918,964 contracts across four blockchains: Ethereum, Binance Smart Chain, Polygon, and Arbitrum. The results indicate that SRVs are widespread, with affected contracts holding $4.76 million in active assets. Among these, 19.63% of contracts that use signatures on Ethereum contain SRVs. Furthermore, manual verification demonstrates that LASiR achieves an F1-score of 87.90% for detection. Ablation studies and comparative experiments reveal that the semantic information provided by LLMs aids static taint analysis, significantly enhancing LASiR's detection performance.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Slaying the Dragon: The Quest for Democracy in Decentralized Autonomous Organizations (DAOs)</title>
<link>https://arxiv.org/abs/2511.09263</link>
<guid>https://arxiv.org/abs/2511.09263</guid>
<content:encoded><![CDATA[
arXiv:2511.09263v1 Announce Type: new 
Abstract: This chapter explores how Decentralized Autonomous Organizations (DAOs), a novel institutional form based on blockchain technology, challenge traditional centralized governance structures. DAOs govern projects ranging from finance to science and digital communities. They aim to redistribute decision-making power through programmable, transparent, and participatory mechanisms. This chapter outlines both the opportunities DAOs present, such as incentive alignment, rapid coordination, and censorship resistance, and the challenges they face, including token concentration, low participation, and the risk of de facto centralization. It further discusses the emerging intersection of DAOs and artificial intelligence, highlighting the potential for increased automation alongside the dangers of diminished human oversight and algorithmic opacity. Ultimately, we discuss under what circumstances DAOs can fulfill their democratic promise or risk replicating the very power asymmetries they seek to overcome.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Estimation and Control for Heterogeneous Multi-agent Systems Based on Decentralized k-hop Prescribed Performance Observers</title>
<link>https://arxiv.org/abs/2511.09269</link>
<guid>https://arxiv.org/abs/2511.09269</guid>
<content:encoded><![CDATA[
arXiv:2511.09269v1 Announce Type: new 
Abstract: We propose decentralized k-hop Prescribed Performance State and Input Observers for heterogeneous multi-agent systems subject to bounded external disturbances. In the proposed input/state observer, each agent estimates the state and input of agents located two or more hops away using only local information exchanged with 1-hop neighbors, while guaranteeing that transient estimation errors satisfy predefined performance bounds. Conditions are established under which the input observer can be omitted, allowing the state observer convergence to be independent of the input estimates. Theoretical analysis demonstrates that if a closed-loop controller with full state knowledge achieves the control objective and the estimation-based closed-loop system is set-Input to State Stable (set-ISS) with respect to the goal set, then the estimated states can be used to achieve the system objective with an arbitrarily small worst-case error governed by the accuracy of the states estimates. Simulation results are provided to validate the proposed approach.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Steering Noncooperative Games Through Conjecture Design</title>
<link>https://arxiv.org/abs/2511.09435</link>
<guid>https://arxiv.org/abs/2511.09435</guid>
<content:encoded><![CDATA[
arXiv:2511.09435v1 Announce Type: new 
Abstract: In dynamic noncooperative games, each player makes conjectures about other players' reactions before choosing a strategy. However, resulting equilibria may be multiple and do not always lead to desirable outcomes. These issues are typically addressed separately, for example, through opponent modelling and incentive design. Drawing inspiration from conjectural variations games, we propose an incentive design framework in which a coordinator first computes an equilibrium by optimizing a predefined objective function, then communicates this equilibrium as a target for the players to reach. In a centralized setting, the coordinator also optimizes the conjectures to steer the players towards the target. In decentralized settings, players independently compute conjectures and update their strategies based on individual targets. We provide a guarantee of equilibrium existence in both cases. This framework uses conjectures not only to guide the system towards desirable outcomes but also to decouple the game into independent optimization problems, enabling efficient computation and parallelization in large-scale settings. We illustrate our theoretical results on classical representative noncooperative games, demonstrating its application potential.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Formal Verification of a Generic Algorithm for TDM Communication Over Inter Satellite Links</title>
<link>https://arxiv.org/abs/2511.09485</link>
<guid>https://arxiv.org/abs/2511.09485</guid>
<content:encoded><![CDATA[
arXiv:2511.09485v1 Announce Type: new 
Abstract: The Python Testbed for Federated Learning Algorithms is a simple FL framework targeting edge systems, which provides the three generic algorithms: the centralized federated learning, the decentralized federated learning, and the universal TDM communication in the current time slot. The first two were formally verified in a previous paper using the CSP process algebra, and in this paper, we use the same approach to formally verify the third one, in two phases. In the first phase, we construct the CSP model as a faithful representation of the real Python code. In the second phase, the model checker PAT automatically proves correctness of the third generic algorithm by proving its deadlock freeness (safety property) and successful termination (liveness property).
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation</title>
<link>https://arxiv.org/abs/2505.03586</link>
<guid>https://arxiv.org/abs/2505.03586</guid>
<content:encoded><![CDATA[
arXiv:2505.03586v4 Announce Type: replace 
Abstract: In real-world multi-agent systems (MASs), observation delays are ubiquitous, preventing agents from making decisions based on the environment's true state. An individual agent's local observation typically comprises multiple components from other agents or dynamic entities within the environment. These discrete observation components with varying delay characteristics pose significant challenges for multi-agent reinforcement learning (MARL). In this paper, we first formulate the decentralized stochastic individual delay partially observable Markov decision process (DSID-POMDP) by extending the standard Dec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL training framework for addressing stochastic individual delays, along with recommended implementations for its constituent modules. We implement the DSID-POMDP's observation generation pattern using standard MARL benchmarks, including MPE and SMAC. Experiments demonstrate that baseline MARL methods suffer severe performance degradation under fixed and unfixed delays. The RDC-enhanced approach mitigates this issue, remarkably achieving ideal delay-free performance in certain delay scenarios while maintaining generalizability. Our work provides a novel perspective on multi-agent delayed observation problems and offers an effective solution framework. The source code is available at https://github.com/linkjoker1006/RDC-pymarl.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agentic Educational Content Generation for African Languages on Edge Devices</title>
<link>https://arxiv.org/abs/2511.07437</link>
<guid>https://arxiv.org/abs/2511.07437</guid>
<content:encoded><![CDATA[
arXiv:2511.07437v1 Announce Type: new 
Abstract: Addressing educational inequity in Sub-Saharan Africa, this research presents an autonomous agent-orchestrated framework for decentralized, culturally adaptive educational content generation on edge devices. The system leverages four specialized agents that work together to generate contextually appropriate educational content. Experimental validation on platforms including Raspberry Pi 4B and NVIDIA Jetson Nano demonstrates significant performance achievements. InkubaLM on Jetson Nano achieved a Time-To-First-Token (TTFT) of 129 ms, an average inter-token latency of 33 ms, and a throughput of 45.2 tokens per second while consuming 8.4 W. On Raspberry Pi 4B, InkubaLM also led with 326 ms TTFT and 15.9 tokens per second at 5.8 W power consumption. The framework consistently delivered high multilingual quality, averaging a BLEU score of 0.688, cultural relevance of 4.4/5, and fluency of 4.2/5 across tested African languages. Through potential partnerships with active community organizations including African Youth & Community Organization (AYCO) and Florida Africa Foundation, this research aims to establish a practical foundation for accessible, localized, and sustainable AI-driven education in resource-constrained environments. Keeping focus on long-term viability and cultural appropriateness, it contributes to United Nations SDGs 4, 9, and 10. Index Terms - Multi-Agent Systems, Edge AI Computing, Educational Technology, African Languages, Rural Education, Sustainable Development, UN SDG.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain</title>
<link>https://arxiv.org/abs/2511.07577</link>
<guid>https://arxiv.org/abs/2511.07577</guid>
<content:encoded><![CDATA[
arXiv:2511.07577v1 Announce Type: new 
Abstract: Existing retrieval-augmented generation (RAG) systems typically use a centralized architecture, causing a high cost of data collection, integration, and management, as well as privacy concerns. There is a great need for a decentralized RAG system that enables foundation models to utilize information directly from data owners who maintain full control over their sources. However, decentralization brings a challenge: the numerous independent data sources vary significantly in reliability, which can diminish retrieval accuracy and response quality. To address this, our decentralized RAG system has a novel reliability scoring mechanism that dynamically evaluates each source based on the quality of responses it contributes to generate and prioritizes high-quality sources during retrieval. To ensure transparency and trust, the scoring process is securely managed through blockchain-based smart contracts, creating verifiable and tamper-proof reliability records without relying on a central authority. We evaluate our decentralized system with two Llama models (3B and 8B) in two simulated environments where six data sources have different levels of reliability. Our system achieves a +10.7\% performance improvement over its centralized counterpart in the real world-like unreliable data environments. Notably, it approaches the upper-bound performance of centralized systems under ideally reliable data environments. The decentralized infrastructure enables secure and trustworthy scoring management, achieving approximately 56\% marginal cost savings through batched update operations. Our code and system are open-sourced at github.com/yining610/Reliable-dRAG.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Virtual Traffic Lights for Multi-Robot Navigation: Decentralized Planning with Centralized Conflict Resolution</title>
<link>https://arxiv.org/abs/2511.07811</link>
<guid>https://arxiv.org/abs/2511.07811</guid>
<content:encoded><![CDATA[
arXiv:2511.07811v1 Announce Type: new 
Abstract: We present a hybrid multi-robot coordination framework that combines decentralized path planning with centralized conflict resolution. In our approach, each robot autonomously plans its path and shares this information with a centralized node. The centralized system detects potential conflicts and allows only one of the conflicting robots to proceed at a time, instructing others to stop outside the conflicting area to avoid deadlocks. Unlike traditional centralized planning methods, our system does not dictate robot paths but instead provides stop commands, functioning as a virtual traffic light. In simulation experiments with multiple robots, our approach increased the success rate of robots reaching their goals while reducing deadlocks. Furthermore, we successfully validated the system in real-world experiments with two quadruped robots and separately with wheeled Duckiebots.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blockchain-Integrated Privacy-Preserving Medical Insurance Claim Processing Using Homomorphic Encryption</title>
<link>https://arxiv.org/abs/2511.07818</link>
<guid>https://arxiv.org/abs/2511.07818</guid>
<content:encoded><![CDATA[
arXiv:2511.07818v1 Announce Type: new 
Abstract: This research proposes a decentralized and cryptographically secure framework to address the most acute issues of privacy, data security, and protection in the ecosystem of medical insurance claim processing. The scope of this study focuses on enabling the management of insurance claims in a transparent, privacy-protecting manner while maintaining the efficiency and trust level needed by the patients, healthcare providers, and insurers. To accomplish this, the proposed system adds blockchain technology to provide an unchangeable, decentralized, and auditable claim transactions ledger which enhances overall claim-related processes and trust among all stakeholders. To protect critical patient information, the framework employs homomorphic encryption a modern form of cryptography to allow authorized insurance providers to perform necessary operations like claim adjudication and reimbursement on encrypted medical records without any decryption during the process. This method significantly reduces the third-party processing privacy risk because patient data can be kept secret even when third-party processing is done. In addition, smart contracts improve automation of the most important procedures in the claim processing pipeline, which decreases manual, operational, and susceptibility towards human blunders or deceitful acts. The integration of these two transformative technologiesblockchain and homomorphic encryption represents the core contribution of this work, enabling the coexistence of transparency and privacy which are usually viewed as competing objectives in traditional systems. As a result, these technologies are expected to foster the creation of a reliable, effective, and privacy safeguarding architecture that could transform the medical claim submission systems paradigm.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated CLIP for Resource-Efficient Heterogeneous Medical Image Classification</title>
<link>https://arxiv.org/abs/2511.07929</link>
<guid>https://arxiv.org/abs/2511.07929</guid>
<content:encoded><![CDATA[
arXiv:2511.07929v1 Announce Type: new 
Abstract: Despite the remarkable performance of deep models in medical imaging, they still require source data for training, which limits their potential in light of privacy concerns. Federated learning (FL), as a decentralized learning framework that trains a shared model with multiple hospitals (a.k.a., FL clients), provides a feasible solution. However, data heterogeneity and resource costs hinder the deployment of FL models, especially when using vision language models (VLM). To address these challenges, we propose a novel contrastive language-image pre-training (CLIP) based FL approach for medical image classification (FedMedCLIP). Specifically, we introduce a masked feature adaptation module (FAM) as a communication module to reduce the communication load while freezing the CLIP encoders to reduce the computational overhead. Furthermore, we propose a masked multi-layer perceptron (MLP) as a private local classifier to adapt to the client tasks. Moreover, we design an adaptive Kullback-Leibler (KL) divergence-based distillation regularization method to enable mutual learning between FAM and MLP. Finally, we incorporate model compression to transmit the FAM parameters while using ensemble predictions for classification. Extensive experiments on four publicly available medical datasets demonstrate that our model provides feasible performance (e.g., 8\% higher compared to second best baseline on ISIC2019) with reasonable resource cost (e.g., 120$\times$ faster than FedAVG).
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Private Chat in a Public Space of Metaverse Systems</title>
<link>https://arxiv.org/abs/2511.07993</link>
<guid>https://arxiv.org/abs/2511.07993</guid>
<content:encoded><![CDATA[
arXiv:2511.07993v1 Announce Type: new 
Abstract: With the proliferation of Virtual Reality (VR) technologies and the emergence of the Metaverse, social VR applications have become increasingly prevalent and accessible to the general user base. Serving as a novel form of social media, these platforms give users a unique opportunity to engage in social activities. However, there remains a significant limitation: the inability to engage in private conversations within public social VR environments. Current interactions are predominantly public, making it challenging for users to have confidential side discussions or whispers without disrupting ongoing conversations. To address this gap, we developed Hushhub, a private chat system integrated into the popular social VR platform VRChat. Our system enables users within a shared VR space to initiate private audio conversations selectively, allowing them to maintain awareness and engagement with the broader group discussions. To evaluate the system, we conducted user studies to gather insight and feedback on the efficacy and user experience of the implemented system. The results demonstrate the value and necessity of enabling private conversations within immersive social VR environments, paving the way for richer, more nuanced social interactions.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AVOID-JACK: Avoidance of Jackknifing for Swarms of Long Heavy Articulated Vehicles</title>
<link>https://arxiv.org/abs/2511.08016</link>
<guid>https://arxiv.org/abs/2511.08016</guid>
<content:encoded><![CDATA[
arXiv:2511.08016v1 Announce Type: new 
Abstract: This paper presents a novel approach to avoiding jackknifing and mutual collisions in Heavy Articulated Vehicles (HAVs) by leveraging decentralized swarm intelligence. In contrast to typical swarm robotics research, our robots are elongated and exhibit complex kinematics, introducing unique challenges. Despite its relevance to real-world applications such as logistics automation, remote mining, airport baggage transport, and agricultural operations, this problem has not been addressed in the existing literature.
  To tackle this new class of swarm robotics problems, we propose a purely reaction-based, decentralized swarm intelligence strategy tailored to automate elongated, articulated vehicles. The method presented in this paper prioritizes jackknifing avoidance and establishes a foundation for mutual collision avoidance. We validate our approach through extensive simulation experiments and provide a comprehensive analysis of its performance. For the experiments with a single HAV, we observe that for 99.8% jackknifing was successfully avoided and that 86.7% and 83.4% reach their first and second goals, respectively. With two HAVs interacting, we observe 98.9%, 79.4%, and 65.1%, respectively, while 99.7% of the HAVs do not experience mutual collisions.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generic Algorithm for Universal TDM Communication Over Inter Satellite Links</title>
<link>https://arxiv.org/abs/2511.08034</link>
<guid>https://arxiv.org/abs/2511.08034</guid>
<content:encoded><![CDATA[
arXiv:2511.08034v1 Announce Type: new 
Abstract: The original Python Testbed for Federated Learning Algorithms is a light FL framework, which provides the three generic algorithms: the centralized federated learning, the decentralized federated learning, and the TDM communication (i.e., peer data exchange) in the current time slot. The limitation of the latter is that it allows communication only between pairs of network nodes. This paper presents the new generic algorithm for the universal TDM communication that overcomes this limitation, such that a node can communicate with an arbitrary number of peers (assuming the peers also want to communicate with it). The paper covers: (i) the algorithm's theoretical foundation, (ii) the system design, and (iii) the system validation. The main advantage of the new algorithm is that it supports real-world TDM communications over inter satellite links.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributed Zero-Shot Learning for Visual Recognition</title>
<link>https://arxiv.org/abs/2511.08170</link>
<guid>https://arxiv.org/abs/2511.08170</guid>
<content:encoded><![CDATA[
arXiv:2511.08170v1 Announce Type: new 
Abstract: In this paper, we propose a Distributed Zero-Shot Learning (DistZSL) framework that can fully exploit decentralized data to learn an effective model for unseen classes. Considering the data heterogeneity issues across distributed nodes, we introduce two key components to ensure the effective learning of DistZSL: a cross-node attribute regularizer and a global attribute-to-visual consensus. Our proposed cross-node attribute regularizer enforces the distances between attribute features to be similar across different nodes. In this manner, the overall attribute feature space would be stable during learning, and thus facilitate the establishment of visual-to-attribute(V2A) relationships. Then, we introduce the global attribute-tovisual consensus to mitigate biased V2A mappings learned from individual nodes. Specifically, we enforce the bilateral mapping between the attribute and visual feature distributions to be consistent across different nodes. Thus, the learned consistent V2A mapping can significantly enhance zero-shot learning across different nodes. Extensive experiments demonstrate that DistZSL achieves superior performance to the state-of-the-art in learning from distributed data.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SRE-Llama -- Fine-Tuned Meta's Llama LLM, Federated Learning, Blockchain and NFT Enabled Site Reliability Engineering(SRE) Platform for Communication and Networking Software Services</title>
<link>https://arxiv.org/abs/2511.08282</link>
<guid>https://arxiv.org/abs/2511.08282</guid>
<content:encoded><![CDATA[
arXiv:2511.08282v1 Announce Type: new 
Abstract: Software services are crucial for reliable communication and networking; therefore, Site Reliability Engineering (SRE) is important to ensure these systems stay reliable and perform well in cloud-native environments. SRE leverages tools like Prometheus and Grafana to monitor system metrics, defining critical Service Level Indicators (SLIs) and Service Level Objectives (SLOs) for maintaining high service standards. However, a significant challenge arises as many developers often lack in-depth understanding of these tools and the intricacies involved in defining appropriate SLIs and SLOs. To bridge this gap, we propose a novel SRE platform, called SRE-Llama, enhanced by Generative-AI, Federated Learning, Blockchain, and Non-Fungible Tokens (NFTs). This platform aims to automate and simplify the process of monitoring, SLI/SLO generation, and alert management, offering ease in accessibility and efficy for developers. The system operates by capturing metrics from cloud-native services and storing them in a time-series database, like Prometheus and Mimir. Utilizing this stored data, our platform employs Federated Learning models to identify the most relevant and impactful SLI metrics for different services and SLOs, addressing concerns around data privacy. Subsequently, fine-tuned Meta's Llama-3 LLM is adopted to intelligently generate SLIs, SLOs, error budgets, and associated alerting mechanisms based on these identified SLI metrics. A unique aspect of our platform is the encoding of generated SLIs and SLOs as NFT objects, which are then stored on a Blockchain. This feature provides immutable record-keeping and facilitates easy verification and auditing of the SRE metrics and objectives. The automation of the proposed platform is governed by the blockchain smart contracts. The proposed SRE-Llama platform prototype has been implemented with a use case featuring a customized Open5GS 5G Core.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blockly2Hooks: Smart Contracts for Everyone with the XRP Ledger and Google Blockly</title>
<link>https://arxiv.org/abs/2511.08403</link>
<guid>https://arxiv.org/abs/2511.08403</guid>
<content:encoded><![CDATA[
arXiv:2511.08403v1 Announce Type: new 
Abstract: Recent technologies such as inter-ledger payments, non-fungible tokens, and smart contracts are all fruited from the ongoing development of Distributed Ledger Technologies. The foreseen trend is that they will play an increasingly visible role in daily life, which will have to be backed by appropriate operational resources. For example, due to increasing demand, smart contracts could soon face a shortage of knowledgeable users and tools to handle them in practice. Widespread smart contract adoption is currently limited by security, usability and costs aspects. Because of a steep learning curve, the handling of smart contracts is currently performed by specialised developers mainly, and most of the research effort is focusing on smart contract security, while other aspects like usability being somewhat neglected. Specific tools would lower the entry barrier, enabling interested non-experts to create smart contracts.
  In this paper we designed, developed and tested Blockly2Hooks, a solution towards filling this gap even in challenging scenarios such as when the smart contracts are written in an advanced language like C. With the XRP Ledger as a concrete working case, Blockly2Hooks helps interested non-experts from the community to learn smart contracts easily and adopt the technology, through leveraging well-proven teaching methodologies like Visual Programming Languages, and more specifically, the Blockly Visual Programming library from Google. The platform was developed and tested and the results are promising to make learning smart contract development smoother.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying the Climate Risk of Generative AI: Region-Aware Carbon Accounting with G-TRACE and the AI Sustainability Pyramid</title>
<link>https://arxiv.org/abs/2511.04776</link>
<guid>https://arxiv.org/abs/2511.04776</guid>
<content:encoded><![CDATA[
arXiv:2511.04776v1 Announce Type: new 
Abstract: Generative Artificial Intelligence (GenAI) represents a rapidly expanding digital infrastructure whose energy demand and associated CO2 emissions are emerging as a new category of climate risk. This study introduces G-TRACE (GenAI Transformative Carbon Estimator), a cross-modal, region-aware framework that quantifies training- and inference-related emissions across modalities and deployment geographies. Using real-world analytics and microscopic simulation, G-TRACE measures energy use and carbon intensity per output type (text, image, video) and reveals how decentralized inference amplifies small per-query energy costs into system-level impacts. Through the Ghibli-style image generation trend (2024-2025), we estimate 4,309 MWh of energy consumption and 2,068 tCO2 emissions, illustrating how viral participation inflates individual digital actions into tonne-scale consequences. Building on these findings, we propose the AI Sustainability Pyramid, a seven-level governance model linking carbon accounting metrics (L1-L7) with operational readiness, optimization, and stewardship. This framework translates quantitative emission metrics into actionable policy guidance for sustainable AI deployment. The study contributes to the quantitative assessment of emerging digital infrastructures as a novel category of climate risk, supporting adaptive governance for sustainable technology deployment. By situating GenAI within climate-risk frameworks, the work advances data-driven methods for aligning technological innovation with global decarbonization and resilience objectives.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-agent Coordination via Flow Matching</title>
<link>https://arxiv.org/abs/2511.05005</link>
<guid>https://arxiv.org/abs/2511.05005</guid>
<content:encoded><![CDATA[
arXiv:2511.05005v1 Announce Type: new 
Abstract: This work presents MAC-Flow, a simple yet expressive framework for multi-agent coordination. We argue that requirements of effective coordination are twofold: (i) a rich representation of the diverse joint behaviors present in offline data and (ii) the ability to act efficiently in real time. However, prior approaches often sacrifice one for the other, i.e., denoising diffusion-based solutions capture complex coordination but are computationally slow, while Gaussian policy-based solutions are fast but brittle in handling multi-agent interaction. MAC-Flow addresses this trade-off by first learning a flow-based representation of joint behaviors, and then distilling it into decentralized one-step policies that preserve coordination while enabling fast execution. Across four different benchmarks, including $12$ environments and $34$ datasets, MAC-Flow alleviates the trade-off between performance and computational cost, specifically achieving about $\boldsymbol{\times14.5}$ faster inference compared to diffusion-based MARL methods, while maintaining good performance. At the same time, its inference speed is similar to that of prior Gaussian policy-based offline multi-agent reinforcement learning (MARL) methods.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tunable Passivity Control for Centralized Multiport Networked Systems</title>
<link>https://arxiv.org/abs/2511.05026</link>
<guid>https://arxiv.org/abs/2511.05026</guid>
<content:encoded><![CDATA[
arXiv:2511.05026v1 Announce Type: new 
Abstract: Centralized Multiport Networked Dynamic (CMND) systems have emerged as a key architecture with applications in several complex network systems, such as multilateral telerobotics and multi-agent control. These systems consist of a hub node/subsystem connecting with multiple remote nodes/subsystems via a networked architecture. One challenge for this system is stability, which can be affected by non-ideal network artifacts. Conventional passivity-based approaches can stabilize the system under specialized applications like small-scale networked systems. However, those conventional passive stabilizers have several restrictions, such as distributing compensation across subsystems in a decentralized manner, limiting flexibility, and, at the same time, relying on the restrictive assumptions of node passivity. This paper synthesizes a centralized optimal passivity-based stabilization framework for CMND systems. It consists of a centralized passivity observer monitoring overall energy flow and an optimal passivity controller that distributes the just-needed dissipation among various nodes, guaranteeing strict passivity and, thus, L2 stability. The proposed data-driven model-free approach, i.e., Tunable Centralized Optimal Passivity Control (TCoPC), optimizes total performance based on the prescribed dissipation distribution strategy while ensuring stability. The controller can put high dissipation loads on some sub-networks while relaxing the dissipation on other nodes. Simulation results demonstrate the proposed frameworks performance in a complex task under different time-varying delay scenarios while relaxing the remote nodes minimum phase and passivity assumption, enhancing the scalability and generalizability.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OvA-LP: A Simple and Efficient Framework for Federated Learning on Non-IID Data</title>
<link>https://arxiv.org/abs/2511.05028</link>
<guid>https://arxiv.org/abs/2511.05028</guid>
<content:encoded><![CDATA[
arXiv:2511.05028v1 Announce Type: new 
Abstract: Federated fine-tuning (FFT) adapts foundation models to decentralized data but remains fragile under heterogeneous client distributions due to local drift, i.e., client-level update divergences that induce systematic bias and amplified variance in the global model. Existing aggregation and personalization methods largely correct drift post hoc, which proves brittle under extreme non-IID conditions. We introduce OvA-LP, a minimalist framework that is, to our knowledge, the first explicitly designed to suppress drift at its source within the PEFT-based FFT paradigm. OvA-LP combines linear probing on a frozen encoder with a one-vs-all head and a simple two-stage procedure, preserving pretrained feature geometry and decoupling logits to prevent the mechanisms that amplify drift. On CIFAR-100 with 100 clients, averaged over shard-1, shard-2, and Bernoulli-Dirichlet partitions, OvA-LP retains 95.9% of its IID accuracy, whereas state-of-the-art FFT baselines retain only 10.1% (PFPT) and 34.5% (FFT-MoE) under the same conditions. OvA-LP further maintains resilience under both symmetric and asymmetric label noise. In addition, precomputing encoder features makes per-round cost nearly independent of encoder size. Together, these results demonstrate that OvA-LP provides a principled and efficient basis for robust FFT under heterogeneity.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SmartSecChain-SDN: A Blockchain-Integrated Intelligent Framework for Secure and Efficient Software-Defined Networks</title>
<link>https://arxiv.org/abs/2511.05156</link>
<guid>https://arxiv.org/abs/2511.05156</guid>
<content:encoded><![CDATA[
arXiv:2511.05156v1 Announce Type: new 
Abstract: With more and more existing networks being transformed to Software-Defined Networking (SDN), they need to be more secure and demand smarter ways of traffic control. This work, SmartSecChain-SDN, is a platform that combines machine learning based intrusion detection, blockchain-based storage of logs, and application-awareness-based priority in SDN networks. To detect network intrusions in a real-time, precision and low-false positives setup, the framework utilizes the application of advanced machine learning algorithms, namely Random Forest, XGBoost, CatBoost, and CNN-BiLSTM. SmartSecChain-SDN is based on the Hyperledger Fabric, which is a permissioned blockchain technology, to provide secure, scalable, and privacy-preserving storage and, thus, guarantee that the Intrusion Detection System (IDS) records cannot be altered and can be analyzed comprehensively. The system also has Quality of Service (QoS) rules and traffic shaping based on applications, which enables prioritization of critical services, such as VoIP, video conferencing, and business applications, as well as de-prioritization of non-essential traffic, such as downloads and updates. Mininet can simulate real-time SDN scenarios because it is used to prototype whole architectures. It is also compatible with controllers OpenDaylight and Ryu. It has tested the framework using the InSDN dataset and proved that it can identify different kinds of cyberattacks and handle bandwidth allocation efficiently under circumstances of resource constraints. SmartSecChain-SDN comprehensively addresses SDN system protection, securing and enhancing. The proposed study offers an innovative, extensible way to improve cybersecurity, regulatory compliance, and the administration of next-generation programmable networks.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>To Squelch or not to Squelch: Enabling Improved Message Dissemination on the XRP Ledger</title>
<link>https://arxiv.org/abs/2511.05362</link>
<guid>https://arxiv.org/abs/2511.05362</guid>
<content:encoded><![CDATA[
arXiv:2511.05362v1 Announce Type: new 
Abstract: With the large increase in the adoption of blockchain technologies, their underlying peer-to-peer networks must also scale with the demand. In this context, previous works highlighted the importance of ensuring efficient and resilient communication for the underlying consensus and replication mechanisms. However, they were mainly focused on mainstream, Proof-of-Work-based Distributed Ledger Technologies like Bitcoin or Ethereum.
  In this paper, the problem is investigated in the context of consensus-validation based blockchains, like the XRP Ledger. The latter relies on a Federated Byzantine Agreement (FBA) consensus mechanism which is proven to have a good scalability in regards to transaction throughput. However, it is known that significant increases in the size of the XRP Ledger network would be challenging to achieve. The main reason is the flooding mechanism used to disseminate the messages related to the consensus protocol, which creates many duplicates in the network. Squelching is a recent solution proposed for limiting this duplication, however, it was never evaluated quantitatively in real-life scenarios involving the XRPL production network. In this paper, our aim is to assess this mechanism using a real-life controllable testbed and the XRPL production network, to assess its benefit and compare it to alternative solutions relying on Named Data Networking and on a gossip-based approach.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OptiLog: Assigning Roles in Byzantine Consensus</title>
<link>https://arxiv.org/abs/2502.15428</link>
<guid>https://arxiv.org/abs/2502.15428</guid>
<content:encoded><![CDATA[
arXiv:2502.15428v2 Announce Type: replace 
Abstract: Byzantine Fault-Tolerant (BFT) protocols play an important role in blockchains. As the deployment of such systems extends to wide-area networks, the scalability of BFT protocols becomes a critical concern. Optimizations that assign specific roles to individual replicas can significantly improve the performance of BFT systems. However, such role assignment is highly sensitive to faults, potentially undermining the optimizations' effectiveness. To address these challenges, we present OptiLog, a logging framework for collecting and analyzing measurements that help to assign roles in globally distributed systems, despite the presence of faults. OptiLog presents local measurements in global data structures, to enable consistent decisions and hold replicas accountable if they do not perform according to their reported measurements. We demonstrate OptiLog's flexibility by applying it to two BFT protocols: (1) Aware, a highly optimized PBFT-like protocol, and (2) Kauri, a tree-based protocol designed for large-scale deployments. OptiLog detects and excludes replicas that misbehave during consensus and thus enables the system to operate in an optimized, low-latency configuration, even under adverse conditions. Experiments show that for tree overlays deployed across 73 worldwide cities, trees found by OptiLog display 39% lower latency than Kauri.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Centralized Reduction of Decentralized Stochastic Control Models and their weak-Feller Regularity</title>
<link>https://arxiv.org/abs/2408.13828</link>
<guid>https://arxiv.org/abs/2408.13828</guid>
<content:encoded><![CDATA[
arXiv:2408.13828v5 Announce Type: replace-cross 
Abstract: Decentralized stochastic control problems involving general state/measurement/action spaces are intrinsically difficult to study because of the inapplicability of standard tools from centralized (single-agent) stochastic control. In this paper, we address some of these challenges for decentralized stochastic control with standard Borel spaces under two different but tightly related information structures: the one-step delayed information sharing pattern (OSDISP), and the $K$-step periodic information sharing pattern (KSPISP). We will show that the one-step delayed and $K$-step periodic problems can be reduced to a centralized Markov Decision Process (MDP), generalizing prior results which considered finite, linear, or static models, by addressing several measurability and topological questions. We then provide sufficient conditions for the transition kernels of both centralized reductions to be weak-Feller. The existence and separated nature of optimal policies under both information structures are then established. The weak Feller regularity also facilitates rigorous approximation and learning theoretic results, as shown in the paper.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Autoregressive Transformers for Model-Agnostic Federated MRI Reconstruction</title>
<link>https://arxiv.org/abs/2502.04521</link>
<guid>https://arxiv.org/abs/2502.04521</guid>
<content:encoded><![CDATA[
arXiv:2502.04521v3 Announce Type: replace-cross 
Abstract: While learning-based models hold great promise for MRI reconstruction, single-site models trained on limited local datasets often show poor generalization. This has motivated collaborative training across institutions via federated learning (FL)-a privacy-preserving framework that aggregates model updates instead of sharing raw data. Conventional FL requires architectural homogeneity, restricting sites from using models tailored to their resources or needs. To address this limitation, we propose FedGAT, a model-agnostic FL technique that first collaboratively trains a global generative prior for MR images, adapted from a natural image foundation model composed of a variational autoencoder (VAE) and a transformer that generates images via spatial-scale autoregression. We fine-tune the transformer module after injecting it with a lightweight site-specific prompting mechanism, keeping the VAE frozen, to efficiently adapt the model to multi-site MRI data. In a second tier, each site independently trains its preferred reconstruction model by augmenting local data with synthetic MRI data from other sites, generated by site-prompting the tuned prior. This decentralized augmentation improves generalization while preserving privacy. Experiments on multi-institutional datasets show that FedGAT outperforms state-of-the-art FL baselines in both within- and cross-site reconstruction performance under model-heterogeneous settings.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Certification Strategies for Blockchain-based Traceability Systems</title>
<link>https://arxiv.org/abs/2511.04409</link>
<guid>https://arxiv.org/abs/2511.04409</guid>
<content:encoded><![CDATA[
arXiv:2511.04409v1 Announce Type: new 
Abstract: The use of blockchains for data certification and traceability is now well established in both the literature and practical applications. However, while blockchain-based certification of individual data is clear and straightforward, the use of blockchain to certify large amounts of data produced on a nearly continuous basis still poses some challenges. In such a case, in fact, it is first necessary to collect the data in an off-chain buffer, and then to organize it, e.g., via Merkle trees, in order to keep the size and quantity of certification data to be written to the blockchain small. In this paper, we consider a typical system for blockchain-based traceability of a production process, and propose and comparatively analyze some strategies for certifying the data of such a process on blockchain, while maintaining the possibility of verifying their certification in a decentralized way.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Regret Lower Bounds for Decentralized Multi-Agent Stochastic Shortest Path Problems</title>
<link>https://arxiv.org/abs/2511.04594</link>
<guid>https://arxiv.org/abs/2511.04594</guid>
<content:encoded><![CDATA[
arXiv:2511.04594v1 Announce Type: new 
Abstract: Multi-agent systems (MAS) are central to applications such as swarm robotics and traffic routing, where agents must coordinate in a decentralized manner to achieve a common objective. Stochastic Shortest Path (SSP) problems provide a natural framework for modeling decentralized control in such settings. While the problem of learning in SSP has been extensively studied in single-agent settings, the decentralized multi-agent variant remains largely unexplored. In this work, we take a step towards addressing that gap. We study decentralized multi-agent SSPs (Dec-MASSPs) under linear function approximation, where the transition dynamics and costs are represented using linear models. Applying novel symmetry-based arguments, we identify the structure of optimal policies. Our main contribution is the first regret lower bound for this setting based on the construction of hard-to-learn instances for any number of agents, $n$. Our regret lower bound of $\Omega(\sqrt{K})$, over $K$ episodes, highlights the inherent learning difficulty in Dec-MASSPs. These insights clarify the learning complexity of decentralized control and can further guide the design of efficient learning algorithms in multi-agent systems.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2511.04646</link>
<guid>https://arxiv.org/abs/2511.04646</guid>
<content:encoded><![CDATA[
arXiv:2511.04646v1 Announce Type: new 
Abstract: Cooperative multi-agent planning requires agents to make joint decisions with partial information and limited communication. Coordination at the trajectory level often fails, as small deviations in timing or movement cascade into conflicts. Symbolic planning mitigates this challenge by raising the level of abstraction and providing a minimal vocabulary of actions that enable synchronization and collective progress. We present DR. WELL, a decentralized neurosymbolic framework for cooperative multi-agent planning. Cooperation unfolds through a two-phase negotiation protocol: agents first propose candidate roles with reasoning and then commit to a joint allocation under consensus and environment constraints. After commitment, each agent independently generates and executes a symbolic plan for its role without revealing detailed trajectories. Plans are grounded in execution outcomes via a shared world model that encodes the current state and is updated as agents act. By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids brittle step-level alignment and enables higher-level operations that are reusable, synchronizable, and interpretable. Experiments on cooperative block-push tasks show that agents adapt across episodes, with the dynamic world model capturing reusable patterns and improving task completion rates and efficiency. Experiments on cooperative block-push tasks show that our dynamic world model improves task completion and efficiency through negotiation and self-refinement, trading a time overhead for evolving, more efficient collaboration strategies.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RaceTEE: Enabling Interoperability of Confidential Smart Contracts</title>
<link>https://arxiv.org/abs/2503.09317</link>
<guid>https://arxiv.org/abs/2503.09317</guid>
<content:encoded><![CDATA[
arXiv:2503.09317v2 Announce Type: replace 
Abstract: Decentralized smart contracts enable trustless collaboration but suffer from limited privacy and scalability, which hinders broader adoption. Trusted Execution Environment (TEE) based off-chain execution frameworks offer a promising solution to both issues. Although TEE-based frameworks have made significant progress, prior work has yet to fully explore contract interoperability, a critical foundation for building complex real-world decentralized applications. This paper identifies the key challenges impeding such interoperability and presents practical solutions. Based on these insights, we introduce RaceTEE, a novel framework that leverages off-chain TEE-enabled nodes to efficiently execute confidential, long-lived smart contracts with interactions of arbitrary complexity among contracts. We implement a RaceTEE prototype using Intel SGX, integrate it with Ethereum, and release it as open source. Evaluation across diverse use cases demonstrates its practicality and effectiveness.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning</title>
<link>https://arxiv.org/abs/2506.06694</link>
<guid>https://arxiv.org/abs/2506.06694</guid>
<content:encoded><![CDATA[
arXiv:2506.06694v5 Announce Type: replace 
Abstract: Foundation models have revolutionized fields such as natural language processing and computer vision by enabling general-purpose learning across diverse tasks and datasets. However, building analogous models for human mobility remains challenging due to the privacy-sensitive nature of mobility data and the resulting data silos across institutions. To bridge this gap, we propose MoveGCL, a scalable and privacy-preserving framework for training mobility foundation models via generative continual learning. Without sharing raw data, MoveGCL enables decentralized and progressive model evolution by replaying synthetic trajectories generated from a frozen teacher model, and reinforces knowledge retention through a tailored distillation strategy that mitigates catastrophic forgetting. To address the heterogeneity of mobility patterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a mobility-aware expert routing mechanism, and employs a layer-wise progressive adaptation strategy to stabilize continual updates. Experiments on six real-world urban datasets demonstrate that MoveGCL achieves performance comparable to joint training and significantly outperforms federated learning baselines, while offering strong privacy protection. MoveGCL marks a crucial step toward unlocking foundation models for mobility, offering a practical blueprint for open, scalable, and privacy-preserving model development in the era of foundation models. To facilitate reproducibility and future research, we have released the code and models at https://github.com/tsinghua-fib-lab/MoveGCL.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RobustFSM: Submodular Maximization in Federated Setting with Malicious Clients</title>
<link>https://arxiv.org/abs/2511.02029</link>
<guid>https://arxiv.org/abs/2511.02029</guid>
<content:encoded><![CDATA[
arXiv:2511.02029v1 Announce Type: new 
Abstract: Submodular maximization is an optimization problem benefiting many machine learning applications, where we seek a small subset best representing an extremely large dataset. We focus on the federated setting where the data are locally owned by decentralized clients who have their own definitions for the quality of representability. This setting requires repetitive aggregation of local information computed by the clients. While the main motivation is to respect the privacy and autonomy of the clients, the federated setting is vulnerable to client misbehaviors: malicious clients might share fake information. An analogy is backdoor attack in conventional federated learning, but our challenge differs freshly due to the unique characteristics of submodular maximization. We propose RobustFSM, a federated submodular maximization solution that is robust to various practical client attacks. Its performance is substantiated with an empirical evaluation study using real-world datasets. Numerical results show that the solution quality of RobustFSM substantially exceeds that of the conventional federated algorithm when attacks are severe. The degree of this improvement depends on the dataset and attack scenarios, which can be as high as 200%
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GPoS: Geospatially-aware Proof of Stake</title>
<link>https://arxiv.org/abs/2511.02034</link>
<guid>https://arxiv.org/abs/2511.02034</guid>
<content:encoded><![CDATA[
arXiv:2511.02034v1 Announce Type: new 
Abstract: Geospatial decentralization is essential for blockchains, ensuring regulatory resilience, robustness, and fairness. We empirically analyze five major Proof of Stake (PoS) blockchains: Aptos, Avalanche, Ethereum, Solana, and Sui, revealing that a few geographic regions dominate consensus voting power, resulting in limited geospatial decentralization. To address this, we propose Geospatially aware Proof of Stake (GPoS), which integrates geospatial diversity with stake-based voting power. Experimental evaluation demonstrates an average 45% improvement in geospatial decentralization, as measured by the Gini coefficient of Eigenvector centrality, while incurring minimal performance overhead in BFT protocols, including HotStuff and CometBFT. These results demonstrate that GPoS can improve geospatial decentralization {while, in our experiments, incurring minimal overhead} to consensus performance.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Private Map-Secure Reduce: Infrastructure for Efficient AI Data Markets</title>
<link>https://arxiv.org/abs/2511.02055</link>
<guid>https://arxiv.org/abs/2511.02055</guid>
<content:encoded><![CDATA[
arXiv:2511.02055v1 Announce Type: new 
Abstract: The modern AI data economy centralizes power, limits innovation, and misallocates value by extracting data without control, privacy, or fair compensation. We introduce Private Map-Secure Reduce (PMSR), a network-native paradigm that transforms data economics from extractive to participatory through cryptographically enforced markets. Extending MapReduce to decentralized settings, PMSR enables computation to move to the data, ensuring verifiable privacy, efficient price discovery, and incentive alignment. Demonstrations include large-scale recommender audits, privacy-preserving LLM ensembling (87.5\% MMLU accuracy across six models), and distributed analytics over hundreds of nodes. PMSR establishes a scalable, equitable, and privacy-guaranteed foundation for the next generation of AI data markets.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Near Optimal Convergence to Coarse Correlated Equilibrium in General-Sum Markov Games</title>
<link>https://arxiv.org/abs/2511.02157</link>
<guid>https://arxiv.org/abs/2511.02157</guid>
<content:encoded><![CDATA[
arXiv:2511.02157v1 Announce Type: new 
Abstract: No-regret learning dynamics play a central role in game theory, enabling decentralized convergence to equilibrium for concepts such as Coarse Correlated Equilibrium (CCE) or Correlated Equilibrium (CE). In this work, we improve the convergence rate to CCE in general-sum Markov games, reducing it from the previously best-known rate of $\mathcal{O}(\log^5 T / T)$ to a sharper $\mathcal{O}(\log T / T)$. This matches the best known convergence rate for CE in terms of $T$, number of iterations, while also improving the dependence on the action set size from polynomial to polylogarithmic-yielding exponential gains in high-dimensional settings. Our approach builds on recent advances in adaptive step-size techniques for no-regret algorithms in normal-form games, and extends them to the Markovian setting via a stage-wise scheme that adjusts learning rates based on real-time feedback. We frame policy updates as an instance of Optimistic Follow-the-Regularized-Leader (OFTRL), customized for value-iteration-based learning. The resulting self-play algorithm achieves, to our knowledge, the fastest known convergence rate to CCE in Markov games.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Permissioned Blockchain in Advanced Air Mobility: A Performance Analisys for UTM</title>
<link>https://arxiv.org/abs/2511.02171</link>
<guid>https://arxiv.org/abs/2511.02171</guid>
<content:encoded><![CDATA[
arXiv:2511.02171v1 Announce Type: new 
Abstract: The rapid adoption of Uncrewed Aerial Vehicles (UAVs) has driven aviation authorities to propose distributed Uncrewed Traffic Management (UTM) architectures. Several studies have advocated blockchain as a promising technology to meet these requirements. However, since UTM is a safety-critical and highly regulated domain, compliance with standards and regulatory frameworks is as crucial as performance and security. This work benchmarks two distributed architectures aligned with current regulatory frameworks: the Linux Foundation's InterUSS platform and a Hyperledger Fabric-based private ledger. Our findings reveal that blockchain-based systems require architectures specifically designed for aeronautical performance constraints.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Quantum Kernel Learning for Anomaly Detection in Multivariate IoT Time-Series</title>
<link>https://arxiv.org/abs/2511.02301</link>
<guid>https://arxiv.org/abs/2511.02301</guid>
<content:encoded><![CDATA[
arXiv:2511.02301v1 Announce Type: new 
Abstract: The rapid growth of industrial Internet of Things (IIoT) systems has created new challenges for anomaly detection in high-dimensional, multivariate time-series, where privacy, scalability, and communication efficiency are critical. Classical federated learning approaches mitigate privacy concerns by enabling decentralized training, but they often struggle with highly non-linear decision boundaries and imbalanced anomaly distributions. To address this gap, we propose a Federated Quantum Kernel Learning (FQKL) framework that integrates quantum feature maps with federated aggregation to enable distributed, privacy-preserving anomaly detection across heterogeneous IoT networks. In our design, quantum edge nodes locally compute compressed kernel statistics using parameterized quantum circuits and share only these summaries with a central server, which constructs a global Gram matrix and trains a decision function (e.g., Fed-QSVM). Experimental results on synthetic IIoT benchmarks demonstrate that FQKL achieves superior generalization in capturing complex temporal correlations compared to classical federated baselines, while significantly reducing communication overhead. This work highlights the promise of quantum kernels in federated settings, advancing the path toward scalable, robust, and quantum-enhanced intelligence for next-generation IoT infrastructures.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.02304</link>
<guid>https://arxiv.org/abs/2511.02304</guid>
<content:encoded><![CDATA[
arXiv:2511.02304v1 Announce Type: new 
Abstract: We study the problem of learning multi-task, multi-agent policies for cooperative, temporal objectives, under centralized training, decentralized execution. In this setting, using automata to represent tasks enables the decomposition of complex tasks into simpler sub-tasks that can be assigned to agents. However, existing approaches remain sample-inefficient and are limited to the single-task case. In this work, we present Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning (ACC-MARL), a framework for learning task-conditioned, decentralized team policies. We identify the main challenges to ACC-MARL's feasibility in practice, propose solutions, and prove the correctness of our approach. We further show that the value functions of learned policies can be used to assign tasks optimally at test time. Experiments show emergent task-aware, multi-step coordination among agents, e.g., pressing a button to unlock a door, holding the door, and short-circuiting tasks.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large-scale automatic carbon ion treatment planning for head and neck cancers via parallel multi-agent reinforcement learning</title>
<link>https://arxiv.org/abs/2511.02314</link>
<guid>https://arxiv.org/abs/2511.02314</guid>
<content:encoded><![CDATA[
arXiv:2511.02314v1 Announce Type: new 
Abstract: Head-and-neck cancer (HNC) planning is difficult because multiple critical organs-at-risk (OARs) are close to complex targets. Intensity-modulated carbon-ion therapy (IMCT) offers superior dose conformity and OAR sparing but remains slow due to relative biological effectiveness (RBE) modeling, leading to laborious, experience-based, and often suboptimal tuning of many treatment-planning parameters (TPPs). Recent deep learning (DL) methods are limited by data bias and plan feasibility, while reinforcement learning (RL) struggles to efficiently explore the exponentially large TPP search space. We propose a scalable multi-agent RL (MARL) framework for parallel tuning of 45 TPPs in IMCT. It uses a centralized-training decentralized-execution (CTDE) QMIX backbone with Double DQN, Dueling DQN, and recurrent encoding (DRQN) for stable learning in a high-dimensional, non-stationary environment. To enhance efficiency, we (1) use compact historical DVH vectors as state inputs, (2) apply a linear action-to-value transform mapping small discrete actions to uniform parameter adjustments, and (3) design an absolute, clinically informed piecewise reward aligned with plan scores. A synchronous multi-process worker system interfaces with the PHOENIX TPS for parallel optimization and accelerated data collection. On a head-and-neck dataset (10 training, 10 testing), the method tuned 45 parameters simultaneously and produced plans comparable to or better than expert manual ones (relative plan score: RL $85.93\pm7.85%$ vs Manual $85.02\pm6.92%$), with significant (p-value $<$ 0.05) improvements for five OARs. The framework efficiently explores high-dimensional TPP spaces and generates clinically competitive IMCT plans through direct TPS interaction, notably improving OAR sparing.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Spatially Informed Gaussian Process UCB Method for Decentralized Coverage Control</title>
<link>https://arxiv.org/abs/2511.02398</link>
<guid>https://arxiv.org/abs/2511.02398</guid>
<content:encoded><![CDATA[
arXiv:2511.02398v1 Announce Type: new 
Abstract: We present a novel decentralized algorithm for coverage control in unknown spatial environments modeled by Gaussian Processes (GPs). To trade-off between exploration and exploitation, each agent autonomously determines its trajectory by minimizing a local cost function. Inspired by the GP-UCB (Upper Confidence Bound for GPs) acquisition function, the proposed cost combines the expected locational cost with a variance-based exploration term, guiding agents toward regions that are both high in predicted density and model uncertainty. Compared to previous work, our algorithm operates in a fully decentralized fashion, relying only on local observations and communication with neighboring agents. In particular, agents periodically update their inducing points using a greedy selection strategy, enabling scalable online GP updates. We demonstrate the effectiveness of our algorithm in simulation.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Voltage Control of AC Microgrids with Constant Power Loads using Control Barrier Functions</title>
<link>https://arxiv.org/abs/2511.02438</link>
<guid>https://arxiv.org/abs/2511.02438</guid>
<content:encoded><![CDATA[
arXiv:2511.02438v1 Announce Type: new 
Abstract: This paper proposes a novel nonlinear decentralized voltage controller for constrained regulation of meshed AC Microgrid networks with high penetration of constant power loads. Perceiving the load demand as an unknown disturbance, the network model is reformulated in a cascaded structure composed of a nominal, i.e. uncertainty-free, and an error subsystem. The latter captures the distance between the true and the nominal state trajectories, for which we prove boundedness via a suitable control barrier function. Under sufficient conditions, we prove asymptotic stability of the cascaded dynamics with respect to an equilibrium set and also provide an estimate of the region of attraction. In addition, it is rigorously shown that the proposed nonlinear control law also enforces constrained regulation around a rated voltage value, without the need of saturation devices. The operation of the closed-loop system is illustrated in a simulation scenario, demonstrating bounded operation and convergence to a neighbourhood of the desired reference vector.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenCourier: an Open Protocol for Building a Decentralized Ecosystem of Community-owned Delivery Platforms</title>
<link>https://arxiv.org/abs/2511.02455</link>
<guid>https://arxiv.org/abs/2511.02455</guid>
<content:encoded><![CDATA[
arXiv:2511.02455v1 Announce Type: new 
Abstract: Although the platform gig economy has reshaped the landscape of work, its centralized operation by select actors has brought about challenges that impedes workers' well-being. We present the architecture and design of OpenCourier, an open protocol that defines communication patterns within a decentralized ecosystem of delivery platforms. Through this protocol, we aim to address three key challenges in the current economy: power imbalances between the platform and workers, information asymmetries caused by black-boxed algorithms and value misalignments in the infrastructure design process. With the OpenCourier protocol, we outline a blueprint for community-owned ecosystem of delivery platforms that centers worker agency, transparency, and bottom-up design.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Approach to Detect and Eliminate Flapping Phenomena due to Flexible Resources</title>
<link>https://arxiv.org/abs/2511.02497</link>
<guid>https://arxiv.org/abs/2511.02497</guid>
<content:encoded><![CDATA[
arXiv:2511.02497v1 Announce Type: new 
Abstract: This paper presents a decentralized methodology for detecting and mitigating flapping phenomena in power systems, primarily caused by the operation of discrete devices. The proposed approach applies moving-window autocorrelation to local measurements, enabling each device to autonomously identify sustained oscillations. Upon detection, a probabilistic, device-specific mitigation strategy is executed. Flexible demand resources (DFRs), under-load tap changers (ULTCs), and automatic voltage regulators (AVRs) are utilised to illustrate the performance of the proposed approach to both discrete and continuous-operation devices. Results show that the proposed method is robust and properly distinguishes damped oscillations from persistent flapping, allowing devices to independently recognize problematic operating scenarios and implement corrective actions accordingly.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized AI Service Placement, Selection and Routing in Mobile Networks</title>
<link>https://arxiv.org/abs/2511.02638</link>
<guid>https://arxiv.org/abs/2511.02638</guid>
<content:encoded><![CDATA[
arXiv:2511.02638v1 Announce Type: new 
Abstract: The rapid development and usage of large-scale AI models by mobile users will dominate the traffic load in future communication networks. The advent of AI technology also facilitates a decentralized AI ecosystem where small organizations or even individuals can host AI services. In such scenarios, AI service (models) placement, selection, and request routing decisions are tightly coupled, posing a challenging yet fundamental trade-off between service quality and service latency, especially when considering user mobility. Existing solutions for related problems in mobile edge computing (MEC) and data-intensive networks fall short due to restrictive assumptions about network structure or user mobility. To bridge this gap, we propose a decentralized framework that jointly optimizes AI service placement, selection, and request routing. In the proposed framework, we use traffic tunneling to support user mobility without costly AI service migrations. To account for nonlinear queuing delays, we formulate a nonconvex problem to optimize the trade-off between service quality and end-to-end latency. We derive the node-level KKT conditions and develop a decentralized Frank--Wolfe algorithm with a novel messaging protocol. Numerical evaluations validate the proposed approach and show substantial performance improvements over existing methods.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bringing Private Reads to Hyperledger Fabric via Private Information Retrieval</title>
<link>https://arxiv.org/abs/2511.02656</link>
<guid>https://arxiv.org/abs/2511.02656</guid>
<content:encoded><![CDATA[
arXiv:2511.02656v1 Announce Type: new 
Abstract: Permissioned blockchains ensure integrity and auditability of shared data but expose query parameters to peers during read operations, creating privacy risks for organizations querying sensitive records. This paper proposes a Private Information Retrieval (PIR) mechanism to enable private reads from Hyperledger Fabric's world state, allowing endorsing peers to process encrypted queries without learning which record is accessed. We implement and benchmark a PIR-enabled chaincode that performs ciphertext-plaintext (ct-pt) homomorphic multiplication directly within evaluate transactions, preserving Fabric's endorsement and audit semantics. The prototype achieves an average end-to-end latency of 113 ms and a peer-side execution time below 42 ms, with approximately 2 MB of peer network traffic per private read in development mode--reducible by half under in-process deployment. Storage profiling across three channel configurations shows near-linear growth: block size increases from 77 kilobytes to 294 kilobytes and world-state from 112 kilobytes to 332 kilobytes as the ring dimension scales from 8,192 to 32,768 coefficients. Parameter analysis further indicates that ring size and record length jointly constrain packing capacity, supporting up to 512 records of 64 bytes each under the largest configuration. These results confirm the practicality of PIR-based private reads in Fabric for smaller, sensitive datasets and highlight future directions to optimize performance and scalability.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Controlling Performance and Budget of a Centralized Multi-agent LLM System with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.02755</link>
<guid>https://arxiv.org/abs/2511.02755</guid>
<content:encoded><![CDATA[
arXiv:2511.02755v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit complementary strengths across domains and come with varying inference costs, motivating the design of multi-agent LLM systems where specialized models collaborate efficiently. Existing approaches predominantly rely on decentralized frameworks, which invoke multiple LLMs for every input and thus lead to substantial and uncontrolled inference costs. In this work, we introduce a centralized multi-LLM framework, where a controller LLM selectively coordinates a pool of expert models in a cost-efficient and cost-controllable manner. We formulate this coordination problem as reinforcement learning with dual objectives: maximizing task performance while minimizing the overall inference cost. In addition, we expect the multi-agent system to have adapted behavior with different budget conditions during inference. To this end, we propose CoRL, a reinforcement learning framework that optimizes the performance cost trade-off in a controllable multi-budget setting. Experiments on four diverse benchmarks demonstrate that CoRL enables a single system to surpass the best expert LLM under high-budget settings, while maintaining strong performance in more economical low-budget modes, highlighting the effectiveness of centralized coordination for scalable and cost-efficient multi-agent LLM systems.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>1 PoCo: Agentic Proof-of-Concept Exploit Generation for Smart Contracts</title>
<link>https://arxiv.org/abs/2511.02780</link>
<guid>https://arxiv.org/abs/2511.02780</guid>
<content:encoded><![CDATA[
arXiv:2511.02780v1 Announce Type: new 
Abstract: Smart contracts operate in a highly adversarial environment, where vulnerabilities can lead to substantial financial losses. Thus, smart contracts are subject to security audits. In auditing, proof-of-concept (PoC) exploits play a critical role by demonstrating to the stakeholders that the reported vulnerabilities are genuine, reproducible, and actionable. However, manually creating PoCs is time-consuming, error-prone, and often constrained by tight audit schedules. We introduce POCO, an agentic framework that automatically generates executable PoC exploits from natural-language vulnerability descriptions written by auditors. POCO autonomously generates PoC exploits in an agentic manner by interacting with a set of code-execution tools in a Reason-Act-Observe loop. It produces fully executable exploits compatible with the Foundry testing framework, ready for integration into audit reports and other security tools. We evaluate POCO on a dataset of 23 real-world vulnerability reports. POCO consistently outperforms the prompting and workflow baselines, generating well-formed and logically correct PoCs. Our results demonstrate that agentic frameworks can significantly reduce the effort required for high-quality PoCs in smart contract audits. Our contribution provides readily actionable knowledge for the smart contract security community.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoK: Design, Vulnerabilities, and Security Measures of Cryptocurrency Wallets</title>
<link>https://arxiv.org/abs/2307.12874</link>
<guid>https://arxiv.org/abs/2307.12874</guid>
<content:encoded><![CDATA[
arXiv:2307.12874v5 Announce Type: replace 
Abstract: With the advent of decentralised digital currencies powered by blockchain technology, a new era of peer-to-peer transactions has commenced. The rapid growth of the cryptocurrency economy has led to increased use of transaction-enabling wallets, making them a focal point for security risks. As the frequency of wallet-related incidents rises, there is a critical need for a systematic approach to measure and evaluate these attacks, drawing lessons from past incidents to enhance wallet security. In response, we introduce a multi-dimensional design taxonomy for existing and novel wallets with various design decisions. We classify existing industry wallets based on this taxonomy, identify previously occurring vulnerabilities and discuss the security implications of design decisions. We also systematise threats to the wallet mechanism and analyse the adversary's goals, capabilities and required knowledge. We present a multi-layered attack framework and investigate 84 incidents between 2012 and 2024, accounting for $5.4B. Following this, we classify defence implementations for these attacks on the precautionary and remedial axes. We map the mechanism and design decisions to vulnerabilities, attacks, and possible defence methods to discuss various insights.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DMind Benchmark: Toward a Holistic Assessment of LLM Capabilities across the Web3 Domain</title>
<link>https://arxiv.org/abs/2504.16116</link>
<guid>https://arxiv.org/abs/2504.16116</guid>
<content:encoded><![CDATA[
arXiv:2504.16116v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have achieved impressive performance in diverse natural language processing tasks, but specialized domains such as Web3 present new challenges and require more tailored evaluation. Despite the significant user base and capital flows in Web3, encompassing smart contracts, decentralized finance (DeFi), non-fungible tokens (NFTs), decentralized autonomous organizations (DAOs), on-chain governance, and novel token-economics, no comprehensive benchmark has systematically assessed LLM performance in this domain. To address this gap, we introduce the DMind Benchmark, a holistic Web3-oriented evaluation suite covering nine critical subfields: fundamental blockchain concepts, blockchain infrastructure, smart contract, DeFi mechanisms, DAOs, NFTs, token economics, meme concept, and security vulnerabilities. Beyond multiple-choice questions, DMind Benchmark features domain-specific tasks such as contract debugging and on-chain numeric reasoning, mirroring real-world scenarios. We evaluated 26 models, including ChatGPT, Claude, DeepSeek, Gemini, Grok, and Qwen, uncovering notable performance gaps in specialized areas like token economics and security-critical contract analysis. While some models excel in blockchain infrastructure tasks, advanced subfields remain challenging. Our benchmark dataset and evaluation pipeline are open-sourced on https://huggingface.co/datasets/DMindAI/DMind_Benchmark, reaching number one in Hugging Face's trending dataset charts within a week of release.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging</title>
<link>https://arxiv.org/abs/2506.09024</link>
<guid>https://arxiv.org/abs/2506.09024</guid>
<content:encoded><![CDATA[
arXiv:2506.09024v2 Announce Type: replace 
Abstract: Safe deployment of machine learning (ML) models in safety-critical domains such as medical imaging requires detecting inputs with characteristics not seen during training, known as out-of-distribution (OOD) detection, to prevent unreliable predictions. Effective OOD detection after deployment could benefit from access to the training data, enabling direct comparison between test samples and the training data distribution to identify differences. State-of-the-art OOD detection methods, however, either discard the training data after deployment or assume that test samples and training data are centrally stored together, an assumption that rarely holds in real-world settings. This is because shipping the training data with the deployed model is usually impossible due to the size of training databases, as well as proprietary or privacy constraints. We introduce the Isolation Network, an OOD detection framework that quantifies the difficulty of separating a target test sample from the training data by solving a binary classification task. We then propose Decentralized Isolation Networks (DIsoN), which enables the comparison of training and test data when data-sharing is impossible, by exchanging only model parameters between the remote computational nodes of training and deployment. We further extend DIsoN with class-conditioning, comparing a target sample solely with training data of its predicted class. We evaluate DIsoN on four medical imaging datasets (dermatology, chest X-ray, breast ultrasound, histopathology) across 12 OOD detection tasks. DIsoN performs favorably against existing methods while respecting data-privacy. This decentralized OOD detection framework opens the way for a new type of service that ML developers could provide along with their models: providing remote, secure utilization of their training data for OOD detection services. Code: https://github.com/FelixWag/DIsoN
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real World Federated Learning with a Knowledge Distilled Transformer for Cardiac CT Imaging</title>
<link>https://arxiv.org/abs/2407.07557</link>
<guid>https://arxiv.org/abs/2407.07557</guid>
<content:encoded><![CDATA[
arXiv:2407.07557v3 Announce Type: replace-cross 
Abstract: Federated learning is a renowned technique for utilizing decentralized data while preserving privacy. However, real-world applications often face challenges like partially labeled datasets, where only a few locations have certain expert annotations, leaving large portions of unlabeled data unused. Leveraging these could enhance transformer architectures ability in regimes with small and diversely annotated sets. We conduct the largest federated cardiac CT analysis to date (n=8,104) in a real-world setting across eight hospitals. Our two-step semi-supervised strategy distills knowledge from task-specific CNNs into a transformer. First, CNNs predict on unlabeled data per label type and then the transformer learns from these predictions with label-specific heads. This improves predictive accuracy and enables simultaneous learning of all partial labels across the federation, and outperforms UNet-based models in generalizability on downstream tasks. Code and model weights are made openly available for leveraging future cardiac CT analysis.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Fundamental Limitations of Decentralized Learnable Reward Shaping in Cooperative Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.00034</link>
<guid>https://arxiv.org/abs/2511.00034</guid>
<content:encoded><![CDATA[
arXiv:2511.00034v1 Announce Type: new 
Abstract: Recent advances in learnable reward shaping have shown promise in single-agent reinforcement learning by automatically discovering effective feedback signals. However, the effectiveness of decentralized learnable reward shaping in cooperative multi-agent settings remains poorly understood. We propose DMARL-RSA, a fully decentralized system where each agent learns individual reward shaping, and evaluate it on cooperative navigation tasks in the simple_spread_v3 environment. Despite sophisticated reward learning, DMARL-RSA achieves only -24.20 +/- 0.09 average reward, compared to MAPPO with centralized training at 1.92 +/- 0.87--a 26.12-point gap. DMARL-RSA performs similarly to simple independent learning (IPPO: -23.19 +/- 0.96), indicating that advanced reward shaping cannot overcome fundamental decentralized coordination limitations. Interestingly, decentralized methods achieve higher landmark coverage (0.888 +/- 0.029 for DMARL-RSA, 0.960 +/- 0.045 for IPPO out of 3 total) but worse overall performance than centralized MAPPO (0.273 +/- 0.008 landmark coverage)--revealing a coordination paradox between local optimization and global performance. Analysis identifies three critical barriers: (1) non-stationarity from concurrent policy updates, (2) exponential credit assignment complexity, and (3) misalignment between individual reward optimization and global objectives. These results establish empirical limits for decentralized reward learning and underscore the necessity of centralized coordination for effective multi-agent cooperation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LC-Opt: Benchmarking Reinforcement Learning and Agentic AI for End-to-End Liquid Cooling Optimization in Data Centers</title>
<link>https://arxiv.org/abs/2511.00116</link>
<guid>https://arxiv.org/abs/2511.00116</guid>
<content:encoded><![CDATA[
arXiv:2511.00116v1 Announce Type: new 
Abstract: Liquid cooling is critical for thermal management in high-density data centers with the rising AI workloads. However, machine learning-based controllers are essential to unlock greater energy efficiency and reliability, promoting sustainability. We present LC-Opt, a Sustainable Liquid Cooling (LC) benchmark environment, for reinforcement learning (RL) control strategies in energy-efficient liquid cooling of high-performance computing (HPC) systems. Built on the baseline of a high-fidelity digital twin of Oak Ridge National Lab's Frontier Supercomputer cooling system, LC-Opt provides detailed Modelica-based end-to-end models spanning site-level cooling towers to data center cabinets and server blade groups. RL agents optimize critical thermal controls like liquid supply temperature, flow rate, and granular valve actuation at the IT cabinet level, as well as cooling tower (CT) setpoints through a Gymnasium interface, with dynamic changes in workloads. This environment creates a multi-objective real-time optimization challenge balancing local thermal regulation and global energy efficiency, and also supports additional components like a heat recovery unit (HRU). We benchmark centralized and decentralized multi-agent RL approaches, demonstrate policy distillation into decision and regression trees for interpretable control, and explore LLM-based methods that explain control actions in natural language through an agentic mesh architecture designed to foster user trust and simplify system management. LC-Opt democratizes access to detailed, customizable liquid cooling models, enabling the ML community, operators, and vendors to develop sustainable data center liquid cooling control solutions.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Application of Blockchain Frameworks for Decentralized Identity and Access Management of IoT Devices</title>
<link>https://arxiv.org/abs/2511.00249</link>
<guid>https://arxiv.org/abs/2511.00249</guid>
<content:encoded><![CDATA[
arXiv:2511.00249v1 Announce Type: new 
Abstract: The growth in IoT devices means an ongoing risk of data vulnerability. The transition from centralized ecosystems to decentralized ecosystems is of paramount importance due to security, privacy, and data use concerns. Since the majority of IoT devices will be used by consumers in peer-to-peer applications, a centralized approach raises many issues of trust related to privacy, control, and censorship. Identity and access management lies at the heart of any user-facing system. Blockchain technologies can be leveraged to augment user authority, transparency, and decentralization. This study proposes a decentralized identity management framework for IoT environments using Hyperledger Fabric and Decentralized Identifiers (DIDs). The system was simulated using Node-RED to model IoT data streams, and key functionalities including device onboarding, authentication, and secure asset querying were successfully implemented. Results demonstrated improved data integrity, transparency, and user control, with reduced reliance on centralized authorities. These findings validate the practicality of blockchain-based identity management in enhancing the security and trustworthiness of IoT infrastructures.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Penetrating the Hostile: Detecting DeFi Protocol Exploits through Cross-Contract Analysis</title>
<link>https://arxiv.org/abs/2511.00408</link>
<guid>https://arxiv.org/abs/2511.00408</guid>
<content:encoded><![CDATA[
arXiv:2511.00408v1 Announce Type: new 
Abstract: Decentralized finance (DeFi) protocols are crypto projects developed on the blockchain to manage digital assets. Attacks on DeFi have been frequent and have resulted in losses exceeding $80 billion. Current tools detect and locate possible vulnerabilities in contracts by analyzing the state changes that may occur during malicious events. However, this victim-only approaches seldom possess the capability to cover the attacker's interaction intention logic. Furthermore, only a minuscule percentage of DeFi protocols experience attacks in real-world scenarios, which poses a significant challenge for these detection tools to demonstrate practical effectiveness. In this paper, we propose DeFiTail, the first framework that utilizes deep learning technology for access control and flash loan exploit detection. Through feeding the cross-contract static data flow, DeFiTail automatically learns the attack logic in real-world malicious events that occur on DeFi protocols, capturing the threat patterns between attacker and victim contracts. Since the DeFi protocol events involve interactions with multi-account transactions, the execution path with external and internal transactions requires to be unified. Moreover, to mitigate the impact of mistakes in Control Flow Graph (CFG) connections, DeFiTail validates the data path by employing the symbolic execution stack. Furthermore, we feed the data paths through our model to achieve the inspection of DeFi protocols. Comparative experiment results indicate that DeFiTail achieves the highest accuracy, with 98.39% in access control and 97.43% in flash loan exploits. DeFiTail also demonstrates an enhanced capability to detect malicious contracts, identifying 86.67% accuracy from the CVE dataset.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TINC: Trusted Intelligent NetChain</title>
<link>https://arxiv.org/abs/2511.00823</link>
<guid>https://arxiv.org/abs/2511.00823</guid>
<content:encoded><![CDATA[
arXiv:2511.00823v1 Announce Type: new 
Abstract: Blockchain technology facilitates the development of decentralized systems that ensure trust and transparency without the need for expensive centralized intermediaries. However, existing blockchain architectures particularly consortium blockchains face critical challenges related to scalability and efficiency. State sharding has emerged as a promising approach to enhance blockchain scalability and performance. However, current shard-based solutions often struggle to guarantee fair participation and a balanced workload distribution among consortium members. To address these limitations, we propose Trusted Intelligent NetChain (TINC), a multi-plane sharding architecture specifically designed for consortium blockchains. TINC incorporates intelligent mechanisms for adaptive node assignment and dynamic workload balancing, enabling the system to respond effectively to changing network conditions while maintaining equitable shard utilization. By decoupling the control and data planes, TINC allows control nodes to focus on consensus operations, while data nodes handle large-scale storage, thus improving overall resource efficiency. Extensive experimental evaluation and formal analysis demonstrate that TINC significantly outperforms existing shard-based blockchain frameworks. It achieves higher throughput, lower latency, balanced node and transaction distributions, and reduced transaction failure rates. Furthermore, TINC maintains essential blockchain security guarantees, exhibiting resilience against Byzantine faults and dynamic network environments. The integration of Dynamic Decentralized Identifiers (DDIDs) further strengthens trust and security management within the consortium network.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Single-Tokenomics: How Farcaster's Pluralistic Incentives Reshape Social Networking</title>
<link>https://arxiv.org/abs/2511.00827</link>
<guid>https://arxiv.org/abs/2511.00827</guid>
<content:encoded><![CDATA[
arXiv:2511.00827v1 Announce Type: new 
Abstract: This paper presents the first empirical analysis of how diverse token-based reward mechanisms impact platform dynamics and user behaviors. For this, we gather a unique, large-scale dataset from Farcaster. This blockchain-based, decentralized social network incorporates multiple incentive mechanisms spanning platform-native rewards, third-party token programs, and peer-to-peer tipping. Our dataset captures token transactions and social interactions from 574,829 wallet-linked users, representing 64.25% of the platform's user base. Our socioeconomic analyses reveal how different tokenomics design shape varying participation rates (7.6%--70%) and wealth concentration patterns (Gini 0.72--0.94), whereas inter-community tipping (51--75% of all tips) is 1.3--2x more frequent among non-following pairs, thereby mitigating echo chambers. Our causal analyses further uncover several critical trade-offs: (1) while most token rewards boost content creation, they often fail to enhance -- sometimes undermining -- content quality; (2) token rewards increase follower acquisition but show neutral or negative effects on outbound following, suggesting potential asymmetric network growth; (3) repeated algorithmic rewards demonstrate strong cumulative effects that may encourage strategic optimization. Our findings advance understanding of cryptocurrency integration in social platforms and highlight challenges in aligning economic incentives with authentic social value.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GauDP: Reinventing Multi-Agent Collaboration through Gaussian-Image Synergy in Diffusion Policies</title>
<link>https://arxiv.org/abs/2511.00998</link>
<guid>https://arxiv.org/abs/2511.00998</guid>
<content:encoded><![CDATA[
arXiv:2511.00998v1 Announce Type: new 
Abstract: Recently, effective coordination in embodied multi-agent systems has remained a fundamental challenge, particularly in scenarios where agents must balance individual perspectives with global environmental awareness. Existing approaches often struggle to balance fine-grained local control with comprehensive scene understanding, resulting in limited scalability and compromised collaboration quality. In this paper, we present GauDP, a novel Gaussian-image synergistic representation that facilitates scalable, perception-aware imitation learning in multi-agent collaborative systems. Specifically, GauDP constructs a globally consistent 3D Gaussian field from decentralized RGB observations, then dynamically redistributes 3D Gaussian attributes to each agent's local perspective. This enables all agents to adaptively query task-critical features from the shared scene representation while maintaining their individual viewpoints. This design facilitates both fine-grained control and globally coherent behavior without requiring additional sensing modalities (e.g., 3D point cloud). We evaluate GauDP on the RoboFactory benchmark, which includes diverse multi-arm manipulation tasks. Our method achieves superior performance over existing image-based methods and approaches the effectiveness of point-cloud-driven methods, while maintaining strong scalability as the number of agents increases.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predictive Auxiliary Learning for Belief-based Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2511.01078</link>
<guid>https://arxiv.org/abs/2511.01078</guid>
<content:encoded><![CDATA[
arXiv:2511.01078v1 Announce Type: new 
Abstract: The performance of multi-agent reinforcement learning (MARL) in partially observable environments depends on effectively aggregating information from observations, communications, and reward signals. While most existing multi-agent systems primarily rely on rewards as the only feedback for policy training, our research shows that introducing auxiliary predictive tasks can significantly enhance learning efficiency and stability. We propose Belief-based Predictive Auxiliary Learning (BEPAL), a framework that incorporates auxiliary training objectives to support policy optimization. BEPAL follows the centralized training with decentralized execution paradigm. Each agent learns a belief model that predicts unobservable state information, such as other agents' rewards or motion directions, alongside its policy model. By enriching hidden state representations with information that does not directly contribute to immediate reward maximization, this auxiliary learning process stabilizes MARL training and improves overall performance. We evaluate BEPAL in the predator-prey environment and Google Research Football, where it achieves an average improvement of about 16 percent in performance metrics and demonstrates more stable convergence compared to baseline methods.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LSHFed: Robust and Communication-Efficient Federated Learning with Locally-Sensitive Hashing Gradient Mapping</title>
<link>https://arxiv.org/abs/2511.01296</link>
<guid>https://arxiv.org/abs/2511.01296</guid>
<content:encoded><![CDATA[
arXiv:2511.01296v1 Announce Type: new 
Abstract: Federated learning (FL) enables collaborative model training across distributed nodes without exposing raw data, but its decentralized nature makes it vulnerable in trust-deficient environments. Inference attacks may recover sensitive information from gradient updates, while poisoning attacks can degrade model performance or induce malicious behaviors. Existing defenses often suffer from high communication and computation costs, or limited detection precision. To address these issues, we propose LSHFed, a robust and communication-efficient FL framework that simultaneously enhances aggregation robustness and privacy preservation. At its core, LSHFed incorporates LSHGM, a novel gradient verification mechanism that projects high-dimensional gradients into compact binary representations via multi-hyperplane locally-sensitive hashing. This enables accurate detection and filtering of malicious gradients using only their irreversible hash forms, thus mitigating privacy leakage risks and substantially reducing transmission overhead. Extensive experiments demonstrate that LSHFed maintains high model performance even when up to 50% of participants are collusive adversaries while achieving up to a 1000x reduction in gradient verification communication compared to full-gradient methods.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Verifiable Split Learning via zk-SNARKs</title>
<link>https://arxiv.org/abs/2511.01356</link>
<guid>https://arxiv.org/abs/2511.01356</guid>
<content:encoded><![CDATA[
arXiv:2511.01356v1 Announce Type: new 
Abstract: Split learning is an approach to collaborative learning in which a deep neural network is divided into two parts: client-side and server-side at a cut layer. The client side executes its model using its raw input data and sends the intermediate activation to the server side. This configuration architecture is very useful for enabling collaborative training when data or resources are separated between devices. However, split learning lacks the ability to verify the correctness and honesty of the computations that are performed and exchanged between the parties. To this purpose, this paper proposes a verifiable split learning framework that integrates a zk-SNARK proof to ensure correctness and verifiability. The zk-SNARK proof and verification are generated for both sides in forward propagation and backward propagation on the server side, guaranteeing verifiability on both sides. The verifiable split learning architecture is compared to a blockchain-enabled system for the same deep learning network, one that records updates but without generating the zero-knowledge proof. From the comparison, it can be deduced that applying the zk-SNARK test achieves verifiability and correctness, while blockchains are lightweight but unverifiable.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConneX: Automatically Resolving Transaction Opacity of Cross-Chain Bridges for Security Analysis</title>
<link>https://arxiv.org/abs/2511.01393</link>
<guid>https://arxiv.org/abs/2511.01393</guid>
<content:encoded><![CDATA[
arXiv:2511.01393v1 Announce Type: new 
Abstract: As the Web3 ecosystem evolves toward a multi-chain architecture, cross-chain bridges have become critical infrastructure for enabling interoperability between diverse blockchain networks. However, while connecting isolated blockchains, the lack of cross-chain transaction pairing records introduces significant challenges for security analysis like cross-chain fund tracing, advanced vulnerability detection, and transaction graph-based analysis. To address this gap, we introduce ConneX, an automated and general-purpose system designed to accurately identify corresponding transaction pairs across both ends of cross-chain bridges. Our system leverages Large Language Models (LLMs) to efficiently prune the semantic search space by identifying semantically plausible key information candidates within complex transaction records. Further, it deploys a novel examiner module that refines these candidates by validating them against transaction values, effectively addressing semantic ambiguities and identifying the correct semantics. Extensive evaluations on a dataset of about 500,000 transactions from five major bridge platforms demonstrate that ConneX achieves an average F1 score of 0.9746, surpassing baselines by at least 20.05\%, with good efficiency that reduces the semantic search space by several orders of magnitude (1e10 to less than 100). Moreover, its successful application in tracing illicit funds (including a cross-chain transfer worth $1 million) in real-world hacking incidents underscores its practical utility for enhancing cross-chain security and transparency.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Designing for Distributed Heterogeneous Modularity: On Software Architecture and Deployment of MoonBots</title>
<link>https://arxiv.org/abs/2511.01437</link>
<guid>https://arxiv.org/abs/2511.01437</guid>
<content:encoded><![CDATA[
arXiv:2511.01437v1 Announce Type: new 
Abstract: This paper presents the software architecture and deployment strategy behind the MoonBot platform: a modular space robotic system composed of heterogeneous components distributed across multiple computers, networks and ultimately celestial bodies. We introduce a principled approach to distributed, heterogeneous modularity, extending modular robotics beyond physical reconfiguration to software, communication and orchestration. We detail the architecture of our system that integrates component-based design, a data-oriented communication model using ROS2 and Zenoh, and a deployment orchestrator capable of managing complex multi-module assemblies. These abstractions enable dynamic reconfiguration, decentralized control, and seamless collaboration between numerous operators and modules. At the heart of this system lies our open-source Motion Stack software, validated by months of field deployment with self-assembling robots, inter-robot cooperation, and remote operation. Our architecture tackles the significant hurdles of modular robotics by significantly reducing integration and maintenance overhead, while remaining scalable and robust. Although tested with space in mind, we propose generalizable patterns for designing robotic systems that must scale across time, hardware, teams and operational environments.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FTSmartAudit: A Knowledge Distillation-Enhanced Framework for Automated Smart Contract Auditing Using Fine-Tuned LLMs</title>
<link>https://arxiv.org/abs/2410.13918</link>
<guid>https://arxiv.org/abs/2410.13918</guid>
<content:encoded><![CDATA[
arXiv:2410.13918v3 Announce Type: replace 
Abstract: The rapid growth of blockchain technology has driven the widespread adoption of smart contracts. However, their inherent vulnerabilities have led to significant financial losses. Traditional auditing methods, while essential, struggle to keep pace with the increasing complexity and scale of smart contracts. Large Language Models (LLMs) offer promising capabilities for automating vulnerability detection, but their adoption is often limited by high computational costs. Although prior work has explored leveraging large models through agents or workflows, relatively little attention has been given to improving the performance of smaller, fine-tuned models--a critical factor for achieving both efficiency and data privacy. In this paper, we introduce HKT-SmartAudit, a framework for developing lightweight models optimized for smart contract auditing. It features a multi-stage knowledge distillation pipeline that integrates classical distillation, external domain knowledge, and reward-guided learning to transfer high-quality insights from large teacher models. A single-task learning strategy is employed to train compact student models that maintain high accuracy and robustness while significantly reducing computational overhead. Experimental results show that our distilled models outperform both commercial tools and larger models in detecting complex vulnerabilities and logical flaws, offering a practical, secure, and scalable solution for smart contract auditing. The source code is available at Github repository.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantitative Parameter Conditions for Stability and Coupling in GFM-GFL Converter Hybrid Systems from a Small-Signal Synchronous Perspective</title>
<link>https://arxiv.org/abs/2510.26971</link>
<guid>https://arxiv.org/abs/2510.26971</guid>
<content:encoded><![CDATA[
arXiv:2510.26971v1 Announce Type: new 
Abstract: With the development of renewable energy sources, power systems are gradually evolving into a system comprising both grid-forming (GFM) and grid-following (GFL) converters. However, the dynamic interaction between the two types of converters, especially low-inertia GFM converters and GFL converters, remains unclear due to the substantial differences in their synchronization mechanisms. To address this gap, this paper develops a small-signal synchronous stability model for power systems containing GFM and GFL converters, which considers network line dynamics. Based on subspace perturbation theory, we reveal that GFM and GFL subsystems can be effectively decoupled when GFL converters operate near unity power factor or when GFM converters possess sufficiently large inertia or damping, and provide lower bound of control parameters ensuring decoupling. Under the decoupling condition, we propose decentralized and analytical parameter-based stability criteria which have clear physical interpretations: the positive damping of converters compensates for the negative damping of the network. In the case of coupling, we also propose decentralized stability criteria based on the small phase theorem. The effectiveness of the theoretical analysis is validated through simulations in MATLAB/Simulink.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Study of Cluster-Based Routing Based on Machine Learning for UAV Networks in 6G</title>
<link>https://arxiv.org/abs/2510.27121</link>
<guid>https://arxiv.org/abs/2510.27121</guid>
<content:encoded><![CDATA[
arXiv:2510.27121v1 Announce Type: new 
Abstract: The sixth generation (6G) wireless networks are envisioned to deliver ultra-low latency, massive connectivity, and high data rates, enabling advanced applications such as autonomous {unmaned aerial vehicles (UAV)} swarms and aerial edge computing. However, realizing this vision in Flying Ad Hoc Networks (FANETs) requires intelligent and adaptive clustering mechanisms to ensure efficient routing and resource utilization. This paper proposes a novel machine learning-driven framework for dynamic cluster formation and cluster head selection in 6G-enabled FANETs. The system leverages mobility prediction using {Extreme Gradient Boosting (XGBoost)} and a composite optimization strategy based on signal strength and spatial proximity to identify optimal cluster heads. To evaluate the proposed method, comprehensive simulations were conducted in both centralized (5G) and decentralized (6G) topologies using realistic video traffic patterns. Results show that the proposed model achieves significant improvements in delay, jitter, and throughput in decentralized scenarios. These findings demonstrate the potential of combining machine learning with clustering techniques to enhance scalability, stability, and performance in next-generation aerial networks.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedSM: Robust Semantics-Guided Feature Mixup for Bias Reduction in Federated Learning with Long-Tail Data</title>
<link>https://arxiv.org/abs/2510.27240</link>
<guid>https://arxiv.org/abs/2510.27240</guid>
<content:encoded><![CDATA[
arXiv:2510.27240v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative model training across decentralized clients without sharing private data. However, FL suffers from biased global models due to non-IID and long-tail data distributions. We propose \textbf{FedSM}, a novel client-centric framework that mitigates this bias through semantics-guided feature mixup and lightweight classifier retraining. FedSM uses a pretrained image-text-aligned model to compute category-level semantic relevance, guiding the category selection of local features to mix-up with global prototypes to generate class-consistent pseudo-features. These features correct classifier bias, especially when data are heavily skewed. To address the concern of potential domain shift between the pretrained model and the data, we propose probabilistic category selection, enhancing feature diversity to effectively mitigate biases. All computations are performed locally, requiring minimal server overhead. Extensive experiments on long-tail datasets with various imbalanced levels demonstrate that FedSM consistently outperforms state-of-the-art methods in accuracy, with high robustness to domain shift and computational efficiency.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Integration of cross platform functions onto service-oriented architectures</title>
<link>https://arxiv.org/abs/2510.27344</link>
<guid>https://arxiv.org/abs/2510.27344</guid>
<content:encoded><![CDATA[
arXiv:2510.27344v1 Announce Type: new 
Abstract: The automotive industry is currently undergoing a major transformation with respect to the Electric/Electronic (E/E) and software architecture, driven by a significant increase in the complexity of the technological stack within a vehicle. This complexity acts as a driving force for Software-Defined Vehicles (SDVs) leading to the evolution of the automotive E/E architectures from decentralized configuration comprising multiple Electronic Control Units (ECUs) towards a more integrated configuration comprising a smaller number of ECUs, domain controllers, gateways, and High-Performance Computers (HPCs) [2]. This transition along with several other reasons have resulted in heterogeneous software platforms such as AUTOSAR Classic, AUTOSAR Adaptive, and prototypical frameworks like ROS 2. It is therefore essential to develop applications that are both hardware- and platform/middleware-agnostic to attain development and integration efficiency. This work presents an application development and integration concept to facilitate developing applications as Software as a Product (SaaP), while simultaneously ensuring efficient integration onto multiple software architecture platforms. The concept involves designing applications in a hardware- and software platform-agnostic manner and standardizing application interfaces [6]. It also includes describing the relevant aspects of the application and corresponding middleware in a machine-readable format to aid the integration of developed applications. Additionally, tools are developed to facilitate semi-automation of the development and integration processes. An example application has been developed and integrated onto AUTOSAR Adaptive and ROS 2, demonstrating the applicability of the approach. Finally, metrics are presented to show the efficiency of the overall concept.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Challenging Tribal Knowledge -- Large Scale Measurement Campaign on Decentralized NAT Traversal</title>
<link>https://arxiv.org/abs/2510.27500</link>
<guid>https://arxiv.org/abs/2510.27500</guid>
<content:encoded><![CDATA[
arXiv:2510.27500v1 Announce Type: new 
Abstract: The promise of decentralized peer-to-peer (P2P) systems is fundamentally gated by the challenge of Network Address Translation (NAT) traversal, with existing solutions often reintroducing the very centralization they seek to avoid. This paper presents the first large-scale, longitudinal measurement study of a fully decentralized NAT traversal protocol, Direct Connection Upgrade through Relay (DCUtR), within the production libp2p-based IPFS network. Drawing on over 4.4 million traversal attempts from 85,000+ distinct networks across 167 countries, we provide a definitive empirical analysis of modern P2P connectivity. We establish a contemporary baseline success rate of $70\% \pm 7.1\%$ for the hole-punching stage, providing a crucial new benchmark for the field. Critically, we empirically refute the long-held 'tribal knowledge' of UDP's superiority for NAT traversal, demonstrating that DCUtR's high-precision, RTT-based synchronization yields statistically indistinguishable success rates for both TCP and QUIC ($\sim70\%$). Our analysis further validates the protocol's design for permissionless environments by showing that success is independent of relay characteristics and that the mechanism is highly efficient, with $97.6\%$ of successful connections established on the first attempt. Building on this analysis, we propose a concrete roadmap of protocol enhancements aimed at achieving universal connectivity and contribute our complete dataset to foster further research in this domain.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asynchronous Risk-Aware Multi-Agent Packet Routing for Ultra-Dense LEO Satellite Networks</title>
<link>https://arxiv.org/abs/2510.27506</link>
<guid>https://arxiv.org/abs/2510.27506</guid>
<content:encoded><![CDATA[
arXiv:2510.27506v1 Announce Type: new 
Abstract: The rise of ultra-dense LEO constellations creates a complex and asynchronous network environment, driven by their massive scale, dynamic topologies, and significant delays. This unique complexity demands an adaptive packet routing algorithm that is asynchronous, risk-aware, and capable of balancing diverse and often conflicting QoS objectives in a decentralized manner. However, existing methods fail to address this need, as they typically rely on impractical synchronous decision-making and/or risk-oblivious approaches. To tackle this gap, we introduce PRIMAL, an event-driven multi-agent routing framework designed specifically to allow each satellite to act independently on its own event-driven timeline, while managing the risk of worst-case performance degradation via a principled primal-dual approach. This is achieved by enabling agents to learn the full cost distribution of the targeted QoS objectives and constrain tail-end risks. Extensive simulations on a LEO constellation with 1584 satellites validate its superiority in effectively optimizing latency and balancing load. Compared to a recent risk-oblivious baseline, it reduces queuing delay by over 70%, and achieves a nearly 12 ms end-to-end delay reduction in loaded scenarios. This is accomplished by resolving the core conflict between naive shortest-path finding and congestion avoidance, highlighting such autonomous risk-awareness as a key to robust routing.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Decentralized Optimization: Leveraging Both First- and Zeroth-Order Optimizers for Faster Convergence</title>
<link>https://arxiv.org/abs/2210.07703</link>
<guid>https://arxiv.org/abs/2210.07703</guid>
<content:encoded><![CDATA[
arXiv:2210.07703v4 Announce Type: replace 
Abstract: Distributed optimization is the standard way of speeding up machine learning training, and most of the research in the area focuses on distributed first-order, gradient-based methods. Yet, there are settings where some computationally-bounded nodes may not be able to implement first-order, gradient-based optimization, while they could still contribute to joint optimization tasks. In this paper, we initiate the study of hybrid decentralized optimization, studying settings where nodes with zeroth-order and first-order optimization capabilities co-exist in a distributed system, and attempt to jointly solve an optimization task over some data distribution. We essentially show that, under reasonable parameter settings, such a system can not only withstand noisier zeroth-order agents but can even benefit from integrating such agents into the optimization process, rather than ignoring their information. At the core of our approach is a new analysis of distributed optimization with noisy and possibly-biased gradient estimators, which may be of independent interest. Our results hold for both convex and non-convex objectives. Experimental results on standard optimization tasks confirm our analysis, showing that hybrid first-zeroth order optimization can be practical, even when training deep neural networks.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Partitioning and Observability in Linear Systems via Submodular Optimization</title>
<link>https://arxiv.org/abs/2505.16169</link>
<guid>https://arxiv.org/abs/2505.16169</guid>
<content:encoded><![CDATA[
arXiv:2505.16169v2 Announce Type: replace 
Abstract: Network partitioning has gained recent attention as a pathway to enable decentralized operation and control in large-scale systems. This paper addresses the interplay between partitioning, observability, and sensor placement (SP) in dynamic networks. The problem, being computationally intractable at scale, is a largely unexplored, open problem in the literature. To that end, the paper's objective is designing scalable partitioning of linear systems while maximizing observability metrics of the subsystems. We show that the partitioning problem can be posed as a submodular maximization problem -- and the SP problem can subsequently be solved over the partitioned network. Consequently, theoretical bounds are derived to compare observability metrics of the original network with those of the resulting partitions, highlighting the impact of partitioning on system observability. Case studies on networks of varying sizes corroborate the derived theoretical bounds.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On-device Computation of Single-lead ECG Parameters for Real-time Remote Cardiac Health Assessment: A Real-world Validation Study</title>
<link>https://arxiv.org/abs/2502.17499</link>
<guid>https://arxiv.org/abs/2502.17499</guid>
<content:encoded><![CDATA[
arXiv:2502.17499v3 Announce Type: replace-cross 
Abstract: Accurate, continuous out-of-hospital electrocardiogram (ECG) parameter measurement is vital for real-time cardiac health monitoring and telemedicine. On-device computation of single-lead ECG parameters enables timely assessment without reliance on centralized data processing, advancing personalized, ubiquitous cardiac care-yet comprehensive validation across heterogeneous real-world populations remains limited. This study validated the on-device algorithm FeatureDB (https://github.com/PKUDigitalHealth/FeatureDB) using two datasets: HeartVoice-ECG-lite (369 participants with single-lead ECGs annotated by two physicians) and PTB-XL/PTB-XL+ (21,354 patients with 12-lead ECGs and physicians' diagnostic annotations). FeatureDB computed PR, QT, and QTc intervals, with accuracy evaluated against physician annotations via mean absolute error (MAE), correlation analysis, and Bland-Altman analysis. Diagnostic performance for first-degree atrioventricular block (AVBI, PR-based) and long QT syndrome (LQT, QTc-based) was benchmarked against commercial 12-lead systems (12SL, Uni-G) and open-source algorithm Deli, using AUC, accuracy, sensitivity, and specificity. Results showed high concordance with expert annotations (Pearson correlations: 0.836-0.960), MAEs matching inter-observer variability, and minimal bias. AVBI AUC reached 0.787 (12SL: 0.859; Uni-G: 0.812; Deli: 0.501); LQT AUC was 0.684 (12SL: 0.716; Uni-G: 0.605; Deli: 0.569)-comparable to commercial tools and superior to open-source alternatives. FeatureDB delivers physician-level parameter accuracy and commercial-grade abnormality detection via single-lead devices, supporting scalable telemedicine, decentralized cardiac screening, and continuous monitoring in community and outpatient settings.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AAGATE: A NIST AI RMF-Aligned Governance Platform for Agentic AI</title>
<link>https://arxiv.org/abs/2510.25863</link>
<guid>https://arxiv.org/abs/2510.25863</guid>
<content:encoded><![CDATA[
arXiv:2510.25863v1 Announce Type: new 
Abstract: This paper introduces the Agentic AI Governance Assurance & Trust Engine (AAGATE), a Kubernetes-native control plane designed to address the unique security and governance challenges posed by autonomous, language-model-driven agents in production. Recognizing the limitations of traditional Application Security (AppSec) tooling for improvisational, machine-speed systems, AAGATE operationalizes the NIST AI Risk Management Framework (AI RMF). It integrates specialized security frameworks for each RMF function: the Agentic AI Threat Modeling MAESTRO framework for Map, a hybrid of OWASP's AIVSS and SEI's SSVC for Measure, and the Cloud Security Alliance's Agentic AI Red Teaming Guide for Manage. By incorporating a zero-trust service mesh, an explainable policy engine, behavioral analytics, and decentralized accountability hooks, AAGATE provides a continuous, verifiable governance solution for agentic AI, enabling safe, accountable, and scalable deployment. The framework is further extended with DIRF for digital identity rights, LPCI defenses for logic-layer injection, and QSAF monitors for cognitive degradation, ensuring governance spans systemic, adversarial, and ethical risks.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Engineering Social Optimality via Utility Shaping in Non-Cooperative Games under Incomplete Information and Imperfect Monitoring</title>
<link>https://arxiv.org/abs/2510.26033</link>
<guid>https://arxiv.org/abs/2510.26033</guid>
<content:encoded><![CDATA[
arXiv:2510.26033v1 Announce Type: new 
Abstract: In this paper, we study decentralized decision-making where agents optimize private objectives under incomplete information and imperfect public monitoring, in a non-cooperative setting. By shaping utilities-embedding shadow prices or Karush-Kuhn-Tucker(KKT)-aligned penalties-we make the stage game an exact-potential game whose unique equilibrium equals the (possibly constrained) social optimum. We characterize the Bayesian equilibrium as a stochastic variational inequality; strong monotonicity follows from a single-inflection compressed/stretched-exponential response combined with convex pricing. We give tracking bounds for damped-gradient and best-response-with-hysteresis updates under a noisy public index, and corresponding steady-state error. The framework accommodates discrete and continuous action sets and composes with slower discrete assignment. Deployable rules include: embed prices/penalties; publish a single public index; tune steps, damping, and dual rates for contraction. Computational experiments cover (i) a multi-tier supply chain and (ii) a non-cooperative agentic-AI compute market of bidding bots. Relative to price-only baselines, utility shaping attains near-centralized welfare, eliminates steady-state constraint/capacity violations when feasible, and accelerates convergence; with quantization, discrete equilibria track continuous ones within the mesh. The blueprint is portable to demand response, cloud/edge scheduling, and transportation pricing and biosecurity/agriculture. Overall, utility shaping plus a public index implements the constrained social optimum with stable equilibria under noise and drift-an operations-research-friendly alternative to heavy messaging or full mechanism design.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle Routing</title>
<link>https://arxiv.org/abs/2510.26089</link>
<guid>https://arxiv.org/abs/2510.26089</guid>
<content:encoded><![CDATA[
arXiv:2510.26089v1 Announce Type: new 
Abstract: Traffic congestion in urban road networks leads to longer trip times and higher emissions, especially during peak periods. While the Shortest Path First (SPF) algorithm is optimal for a single vehicle in a static network, it performs poorly in dynamic, multi-vehicle settings, often worsening congestion by routing all vehicles along identical paths. We address dynamic vehicle routing through a multi-agent reinforcement learning (MARL) framework for coordinated, network-aware fleet navigation. We first propose Adaptive Navigation (AN), a decentralized MARL model where each intersection agent provides routing guidance based on (i) local traffic and (ii) neighborhood state modeled using Graph Attention Networks (GAT). To improve scalability in large networks, we further propose Hierarchical Hub-based Adaptive Navigation (HHAN), an extension of AN that assigns agents only to key intersections (hubs). Vehicles are routed hub-to-hub under agent control, while SPF handles micro-routing within each hub region. For hub coordination, HHAN adopts centralized training with decentralized execution (CTDE) under the Attentive Q-Mixing (A-QMIX) framework, which aggregates asynchronous vehicle decisions via attention. Hub agents use flow-aware state features that combine local congestion and predictive dynamics for proactive routing. Experiments on synthetic grids and real urban maps (Toronto, Manhattan) show that AN reduces average travel time versus SPF and learning baselines, maintaining 100% routing success. HHAN scales to networks with hundreds of intersections, achieving up to 15.9% improvement under heavy traffic. These findings highlight the potential of network-constrained MARL for scalable, coordinated, and congestion-aware routing in intelligent transportation systems.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PEEL: A Poisoning-Exposing Encoding Theoretical Framework for Local Differential Privacy</title>
<link>https://arxiv.org/abs/2510.26102</link>
<guid>https://arxiv.org/abs/2510.26102</guid>
<content:encoded><![CDATA[
arXiv:2510.26102v1 Announce Type: new 
Abstract: Local Differential Privacy (LDP) is a widely adopted privacy-protection model in the Internet of Things (IoT) due to its lightweight, decentralized, and scalable nature. However, it is vulnerable to poisoning attacks, and existing defenses either incur prohibitive resource overheads or rely on domain-specific prior knowledge, limiting their practical deployment. To address these limitations, we propose PEEL, a Poisoning-Exposing Encoding theoretical framework for LDP, which departs from resource- or prior-dependent countermeasures and instead leverages the inherent structural consistency of LDP-perturbed data. As a non-intrusive post-processing module, PEEL amplifies stealthy poisoning effects by re-encoding LDP-perturbed data via sparsification, normalization, and low-rank projection, thereby revealing both output and rule poisoning attacks through structural inconsistencies in the reconstructed space. Theoretical analysis proves that PEEL, integrated with LDP, retains unbiasedness and statistical accuracy, while being robust to expose both output and rule poisoning attacks. Moreover, evaluation results show that LDP-integrated PEEL not only outperforms four state-of-the-art defenses in terms of poisoning exposure accuracy but also significantly reduces client-side computational costs, making it highly suitable for large-scale IoT deployments.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Context Length Optimization with Low-Frequency Truncation for Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.26389</link>
<guid>https://arxiv.org/abs/2510.26389</guid>
<content:encoded><![CDATA[
arXiv:2510.26389v1 Announce Type: new 
Abstract: Recently, deep multi-agent reinforcement learning (MARL) has demonstrated promising performance for solving challenging tasks, such as long-term dependencies and non-Markovian environments. Its success is partly attributed to conditioning policies on large fixed context length. However, such large fixed context lengths may lead to limited exploration efficiency and redundant information. In this paper, we propose a novel MARL framework to obtain adaptive and effective contextual information. Specifically, we design a central agent that dynamically optimizes context length via temporal gradient analysis, enhancing exploration to facilitate convergence to global optima in MARL. Furthermore, to enhance the adaptive optimization capability of the context length, we present an efficient input representation for the central agent, which effectively filters redundant information. By leveraging a Fourier-based low-frequency truncation method, we extract global temporal trends across decentralized agents, providing an effective and efficient representation of the MARL environment. Extensive experiments demonstrate that the proposed method achieves state-of-the-art (SOTA) performance on long-term dependency tasks, including PettingZoo, MiniGrid, Google Research Football (GRF), and StarCraft Multi-Agent Challenge v2 (SMACv2).
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Pragmatic View of AI Personhood</title>
<link>https://arxiv.org/abs/2510.26396</link>
<guid>https://arxiv.org/abs/2510.26396</guid>
<content:encoded><![CDATA[
arXiv:2510.26396v1 Announce Type: new 
Abstract: The emergence of agentic Artificial Intelligence (AI) is set to trigger a "Cambrian explosion" of new kinds of personhood. This paper proposes a pragmatic framework for navigating this diversification by treating personhood not as a metaphysical property to be discovered, but as a flexible bundle of obligations (rights and responsibilities) that societies confer upon entities for a variety of reasons, especially to solve concrete governance problems. We argue that this traditional bundle can be unbundled, creating bespoke solutions for different contexts. This will allow for the creation of practical tools -- such as facilitating AI contracting by creating a target "individual" that can be sanctioned -- without needing to resolve intractable debates about an AI's consciousness or rationality. We explore how individuals fit in to social roles and discuss the use of decentralized digital identity technology, examining both "personhood as a problem", where design choices can create "dark patterns" that exploit human social heuristics, and "personhood as a solution", where conferring a bundle of obligations is necessary to ensure accountability or prevent conflict. By rejecting foundationalist quests for a single, essential definition of personhood, this paper offers a more pragmatic and flexible way to think about integrating AI agents into our society.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Bidding and Coordinated Dispatch of Hybrid Energy Systems in Regulation Markets</title>
<link>https://arxiv.org/abs/2510.26602</link>
<guid>https://arxiv.org/abs/2510.26602</guid>
<content:encoded><![CDATA[
arXiv:2510.26602v1 Announce Type: new 
Abstract: The increasing integration of renewable energy sources and distributed energy resources (DER) into modern power systems introduces significant uncertainty, posing challenges for maintaining grid flexibility and reliability. Hybrid energy systems (HES), composed of controllable generators, flexible loads, and battery storage, offer a decentralized solution to enhance flexibility compared to single centralized resources. This paper presents a two-level framework to enable HES participation in frequency regulation markets. The upper level performs a chance-constrained optimization to choose capacity bids based on historical regulation signals. At the lower level, a real-time control strategy disaggregates the regulation power among the constituent resources. This real-time control strategy is then benchmarked against an offline optimal dispatch to evaluate flexibility performance. Additionally, the framework evaluates the profitability of overbidding strategies and identifies thresholds beyond which performance degradation may lead to market penalties or disqualification. The proposed framework also compare the impact of imbalance of power capacities on performance and battery state of charge (SoC) through asymmetric HES configurations.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph approach for observability analysis in power system dynamic state estimation</title>
<link>https://arxiv.org/abs/2510.26701</link>
<guid>https://arxiv.org/abs/2510.26701</guid>
<content:encoded><![CDATA[
arXiv:2510.26701v1 Announce Type: new 
Abstract: The proposed approach yields a numerical method that provably executes in linear time with respect to the number of nodes and edges in a graph. The graph, constructed from the power system model, requires only knowledge of the dependencies between state-to-state and output-to-state variables within a state-space framework. While graph-based observability analysis methods exist for power system static-state estimation, the approach presented here is the first for dynamic-state estimation (DSE). We examine decentralized and centralized DSE scenarios and compare our findings with a well-established, albeit non-scalable, observability analysis method in the literature. When compared to the latter in a centralized DSE setting, our method reduced computation time by 1440x.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guided Model Merging for Hybrid Data Learning: Leveraging Centralized Data to Refine Decentralized Models</title>
<link>https://arxiv.org/abs/2503.20138</link>
<guid>https://arxiv.org/abs/2503.20138</guid>
<content:encoded><![CDATA[
arXiv:2503.20138v2 Announce Type: replace 
Abstract: Current network training paradigms primarily focus on either centralized or decentralized data regimes. However, in practice, data availability often exhibits a hybrid nature, where both regimes coexist. This hybrid setting presents new opportunities for model training, as the two regimes offer complementary trade-offs: decentralized data is abundant but subject to heterogeneity and communication constraints, while centralized data, though limited in volume and potentially unrepresentative, enables better curation and high-throughput access. Despite its potential, effectively combining these paradigms remains challenging, and few frameworks are tailored to hybrid data regimes. To address this, we propose a novel framework that constructs a model atlas from decentralized models and leverages centralized data to refine a global model within this structured space. The refined model is then used to reinitialize the decentralized models. Our method synergizes federated learning (to exploit decentralized data) and model merging (to utilize centralized data), enabling effective training under hybrid data availability. Theoretically, we show that our approach achieves faster convergence than methods relying solely on decentralized data, due to variance reduction in the merging process. Extensive experiments demonstrate that our framework consistently outperforms purely centralized, purely decentralized, and existing hybrid-adaptable methods. Notably, our method remains robust even when the centralized and decentralized data domains differ or when decentralized data contains noise, significantly broadening its applicability.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiple Proposer Transaction Fee Mechanism Design: Robust Incentives Against Censorship and Bribery</title>
<link>https://arxiv.org/abs/2505.13751</link>
<guid>https://arxiv.org/abs/2505.13751</guid>
<content:encoded><![CDATA[
arXiv:2505.13751v2 Announce Type: replace 
Abstract: Censorship resistance is one of the core value proposition of blockchains. A recurring design pattern aimed at providing censorship resistance is enabling multiple proposers to contribute inputs into block construction. Notably, Fork-Choice Enforced Inclusion Lists (FOCIL) is proposed to be included in Ethereum. However, the current proposal relies on altruistic behavior, without a Transaction Fee Mechanism (TFM). This study aims to address this gap by exploring how multiple proposers should be rewarded to incentivize censorship resistance. The main contribution of this work is the identification of TFMs that ensure censorship resistance under bribery attacks, while also satisfying the incentive compatibility properties of EIP-1559. We provide a concrete payment mechanism for FOCIL, along with generalizable contributions to the literature by analyzing 1) incentive compatibility of TFMs in the presence of a bribing adversary, 2) TFMs in protocols with multiple phases of transaction inclusion, and 3) TFMs of protocols in which parties are uncertain about the behavior and the possible bribe of others.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Formal Verification of a Token Sale Launchpad: A Compositional Approach in Dafny</title>
<link>https://arxiv.org/abs/2510.24798</link>
<guid>https://arxiv.org/abs/2510.24798</guid>
<content:encoded><![CDATA[
arXiv:2510.24798v1 Announce Type: new 
Abstract: The proliferation of decentralized financial (DeFi) systems and smart contracts has underscored the critical need for software correctness. Bugs in such systems can lead to catastrophic financial losses. Formal verification offers a path to achieving mathematical certainty about software behavior. This paper presents the formal verification of the core logic for a token sale launchpad, implemented and proven correct using the Dafny programming language and verification system. We detail a compositional, bottom-up verification strategy, beginning with the proof of fundamental non-linear integer arithmetic properties, and building upon them to verify complex business logic, including asset conversion, time-based discounts, and capped-sale refund mechanics. The principal contributions are the formal proofs of critical safety and lifecycle properties. Most notably, we prove that refunds in a capped sale can never exceed the user's original deposit amount, and that the precision loss in round-trip financial calculations is strictly bounded. Furthermore, we verify the complete lifecycle logic, including user withdrawals under various sale mechanics and the correctness of post-sale token allocation, vesting, and claiming. This work serves as a comprehensive case study in applying rigorous verification techniques to build high-assurance financial software.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fortytwo: Swarm Inference with Peer-Ranked Consensus</title>
<link>https://arxiv.org/abs/2510.24801</link>
<guid>https://arxiv.org/abs/2510.24801</guid>
<content:encoded><![CDATA[
arXiv:2510.24801v1 Announce Type: new 
Abstract: As centralized AI hits compute ceilings and diminishing returns from ever-larger training runs, meeting demand requires an inference layer that scales horizontally in both capacity and capability. We present Fortytwo, a novel protocol that leverages swarm intelligence principles and distributed pairwise ranking consensus to achieve superior performance in AI inference. Our approach reimagines collaboration among AI nodes using swarm inference: a peer-ranked, reputation-weighted consensus across heterogeneous models that surfaces the highest-quality responses. Using pairwise ranking with a custom Bradley-Terry-style aggregation model, we demonstrate that swarm inference substantially outperforms majority voting, achieving 85.90% on GPQA Diamond versus 68.69% for majority voting with the same model set - an improvement of +17.21 percentage points (approximately +25.1% relative). The protocol incorporates on-chain reputation so node influence adapts to demonstrated accuracy over time, yielding a meritocratic consensus that filters low-quality or malicious participants. To resist Sybil attacks, Fortytwo employs proof-of-capability in its consensus: nodes must successfully complete calibration/test requests and stake reputation to enter ranking rounds, making multi-identity attacks economically unattractive while preserving openness. Across six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and AIME, our evaluation indicates higher accuracy and strong resilience to adversarial and noisy free-form prompting (e.g., prompt-injection degradation of only 0.12% versus 6.20% for a monolithic single-model baseline), while retaining practical deployability. Together, these results establish a foundation for decentralized AI systems - democratizing access to high-quality inference through collective intelligence without sacrificing reliability or security.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Merging Control of Connected and Automated Vehicles to Enhance Safety and Energy Efficiency using Control Barrier Functions</title>
<link>https://arxiv.org/abs/2510.24871</link>
<guid>https://arxiv.org/abs/2510.24871</guid>
<content:encoded><![CDATA[
arXiv:2510.24871v1 Announce Type: new 
Abstract: This paper presents a decentralized Control Barrier Function (CBF) based approach for highway merging of Connected and Automated Vehicles (CAVs). In this control algorithm, each "host" vehicle negotiates with other agents in a control zone of the highway network, and enacts its own action, to perform safe and energy-efficient merge maneuvers. It uses predictor-corrector loops within the robust CBF setting for negotiation and to reconcile disagreements that may arise. There is no explicit order of vehicles and no priority. A notable feature is absence of gridlocks due to instability of the inter-agent system. Results from Monte Carlo simulations show significant improvement in the system-wide energy efficiency and traffic flow compared to a first-in-first-out approach, as well as enhanced robustness of the proposed decentralized controller compared to its centralized counterpart.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Timing Games in Responsive Consensus Protocols</title>
<link>https://arxiv.org/abs/2510.25144</link>
<guid>https://arxiv.org/abs/2510.25144</guid>
<content:encoded><![CDATA[
arXiv:2510.25144v1 Announce Type: new 
Abstract: Optimistic responsiveness -- the ability of a consensus protocol to operate at the speed of the network -- is widely used in consensus protocol design to optimize latency and throughput. However, blockchain applications incentivize validators to play timing games by strategically delaying their proposals, since increased block time correlates with greater rewards. Consequently, it may appear that responsiveness (even under optimistic conditions) is impossible in blockchain protocols. In this work, we develop a model of timing games in responsive consensus protocols and find a prisoner's dilemma structure, where cooperation (proposing promptly) is in the validators' best interest, but individual incentives encourage validators to delay proposals selfishly. To attain desirable equilibria, we introduce dynamic block rewards that decrease with round time to explicitly incentivize faster proposals. Delays are measured through a voting mechanism, where other validators vote on the current leader's round time. By carefully setting the protocol parameters, the voting mechanism allows validators to coordinate and reach the cooperative equilibrium, benefiting all through a higher rate-of-reward. Thus, instead of responsiveness being an unattainable property due to timing games, we show that responsiveness itself can promote faster block proposals. One consequence of moving from a static to dynamic block reward is that validator utilities become more sensitive to latency, worsening the gap between the best- and worst-connected validators. Our analysis shows, however, that this effect is minor in both theoretical latency models and simulations based on real-world networks.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study on Privacy-Preserving Scholarship Evaluation Based on Decentralized Identity and Zero-Knowledge Proofs</title>
<link>https://arxiv.org/abs/2510.25477</link>
<guid>https://arxiv.org/abs/2510.25477</guid>
<content:encoded><![CDATA[
arXiv:2510.25477v1 Announce Type: new 
Abstract: Traditional centralized scholarship evaluation processes typically require students to submit detailed academic records and qualification information, which exposes them to risks of data leakage and misuse, making it difficult to simultaneously ensure privacy protection and transparent auditability. To address these challenges, this paper proposes a scholarship evaluation system based on Decentralized Identity (DID) and Zero-Knowledge Proofs (ZKP). The system aggregates multidimensional ZKPs off-chain, and smart contracts verify compliance with evaluation criteria without revealing raw scores or computational details. Experimental results demonstrate that the proposed solution not only automates the evaluation efficiently but also maximally preserves student privacy and data integrity, offering a practical and trustworthy technical paradigm for higher education scholarship programs.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incorporating Social Awareness into Control of Unknown Multi-Agent Systems: A Real-Time Spatiotemporal Tubes Approach</title>
<link>https://arxiv.org/abs/2510.25597</link>
<guid>https://arxiv.org/abs/2510.25597</guid>
<content:encoded><![CDATA[
arXiv:2510.25597v1 Announce Type: new 
Abstract: This paper presents a decentralized control framework that incorporates social awareness into multi-agent systems with unknown dynamics to achieve prescribed-time reach-avoid-stay tasks in dynamic environments. Each agent is assigned a social awareness index that quantifies its level of cooperation or self-interest, allowing heterogeneous social behaviors within the system. Building on the spatiotemporal tube (STT) framework, we propose a real-time STT framework that synthesizes tubes online for each agent while capturing its social interactions with others. A closed-form, approximation-free control law is derived to ensure that each agent remains within its evolving STT, thereby avoiding dynamic obstacles while also preventing inter-agent collisions in a socially aware manner, and reaching the target within a prescribed time. The proposed approach provides formal guarantees on safety and timing, and is computationally lightweight, model-free, and robust to unknown disturbances. The effectiveness and scalability of the framework are validated through simulation and hardware experiments on a 2D omnidirectional
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaLore: Learning to Orchestrate Communication and Computation for Metaverse Synchronization</title>
<link>https://arxiv.org/abs/2510.25705</link>
<guid>https://arxiv.org/abs/2510.25705</guid>
<content:encoded><![CDATA[
arXiv:2510.25705v1 Announce Type: new 
Abstract: As augmented and virtual reality evolve, achieving seamless synchronization between physical and digital realms remains a critical challenge, especially for real-time applications where delays affect the user experience. This paper presents MetaLore, a Deep Reinforcement Learning (DRL) based framework for joint communication and computational resource allocation in Metaverse or digital twin environments. MetaLore dynamically shares the communication bandwidth and computational resources among sensors and mobile devices to optimize synchronization, while offering high throughput performance. Special treatment is given in satisfying end-to-end delay guarantees. A key contribution is the introduction of two novel Age of Information (AoI) metrics: Age of Request Information (AoRI) and Age of Sensor Information (AoSI), integrated into the reward function to enhance synchronization quality. An open source simulator has been extended to incorporate and evaluate the approach. The DRL solution is shown to achieve the performance of full-enumeration brute-force solutions by making use of a small, task-oriented observation space of two queue lengths at the network side. This allows the DRL approach the flexibility to effectively and autonomously adapt to dynamic traffic conditions.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Holon Streaming: Global Aggregations with Windowed CRDTs</title>
<link>https://arxiv.org/abs/2510.25757</link>
<guid>https://arxiv.org/abs/2510.25757</guid>
<content:encoded><![CDATA[
arXiv:2510.25757v1 Announce Type: new 
Abstract: Scaling global aggregations is a challenge for exactly-once stream processing systems. Current systems implement these either by computing the aggregation in a single task instance, or by static aggregation trees, which limits scalability and may become a bottleneck. Moreover, the end-to-end latency is determined by the slowest path in the tree, and failures and reconfiguration cause large latency spikes due to the centralized coordination. Towards these issues, we present Holon Streaming, an exactly-once stream processing system for global aggregations. Its deterministic programming model uses windowed conflict-free replicated data types (Windowed CRDTs), a novel abstraction for shared replicated state. Windowed CRDTs make computing global aggregations scalable. Furthermore, their guarantees such as determinism and convergence enable the design of efficient failure recovery algorithms by decentralized coordination. Our evaluation shows a 5x lower latency and 2x higher throughput than an existing stream processing system on global aggregation workloads, with an 11x latency reduction under failure scenarios. The paper demonstrates the effectiveness of decentralized coordination with determinism, and the utility of Windowed CRDTs for global aggregations.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed learning for automatic modulation recognition in bandwidth-limited networks</title>
<link>https://arxiv.org/abs/2510.24722</link>
<guid>https://arxiv.org/abs/2510.24722</guid>
<content:encoded><![CDATA[
arXiv:2510.24722v1 Announce Type: cross 
Abstract: Automatic Modulation Recognition (AMR) is critical in identifying various modulation types in wireless communication systems. Recent advancements in deep learning have facilitated the integration of algorithms into AMR techniques. However, this integration typically follows a centralized approach that necessitates collecting and processing all training data on high-powered computing devices, which may prove impractical for bandwidth-limited wireless networks. In response to this challenge, this study introduces two methods for distributed learning-based AMR on the collaboration of multiple receivers to perform AMR tasks. The TeMuRAMRD 2023 dataset is employed to support this investigation, uniquely suited for multi-receiver AMR tasks. Within this distributed sensing environment, multiple receivers collaborate in identifying modulation types from the same RF signal, each possessing a partial perspective of the overall environment. Experimental results demonstrate that the centralized-based AMR, with six receivers, attains an impressive accuracy rate of 91%, while individual receivers exhibit a notably lower accuracy, at around 41%. Nonetheless, the two proposed decentralized learning-based AMR methods exhibit noteworthy enhancements. Based on consensus voting among six receivers, the initial method achieves a marginally lower accuracy. It achieves this while substantially reducing the bandwidth demands to a 1/256th of the centralized model. With the second distributed method, each receiver shares its feature map, subsequently aggregated by a central node. This approach also accompanies a substantial bandwidth reduction of 1/8 compared to the centralized approach. These findings highlight the capacity of distributed AMR to significantly enhance accuracy while effectively addressing the constraints of bandwidth-limited wireless networks.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Partially Observable Multi-Agent Reinforcement Learning with Information Sharing</title>
<link>https://arxiv.org/abs/2308.08705</link>
<guid>https://arxiv.org/abs/2308.08705</guid>
<content:encoded><![CDATA[
arXiv:2308.08705v4 Announce Type: replace 
Abstract: We study provable multi-agent reinforcement learning (RL) in the general framework of partially observable stochastic games (POSGs). To circumvent the known hardness results and the use of computationally intractable oracles, we advocate leveraging the potential \emph{information-sharing} among agents, a common practice in empirical multi-agent RL, and a standard model for multi-agent control systems with communication. We first establish several computational complexity results to justify the necessity of information-sharing, as well as the observability assumption that has enabled quasi-polynomial time and sample single-agent RL with partial observations, for tractably solving POSGs. Inspired by the inefficiency of planning in the ground-truth model, we then propose to further \emph{approximate} the shared common information to construct an approximate model of the POSG, in which an approximate \emph{equilibrium} (of the original POSG) can be found in quasi-polynomial-time, under the aforementioned assumptions. Furthermore, we develop a partially observable multi-agent RL algorithm whose time and sample complexities are \emph{both} quasi-polynomial. Finally, beyond equilibrium learning, we extend our algorithmic framework to finding the \emph{team-optimal solution} in cooperative POSGs, i.e., decentralized partially observable Markov decision processes, a more challenging goal. We establish concrete computational and sample complexities under several structural assumptions of the model. We hope our study could open up the possibilities of leveraging and even designing different \emph{information structures}, a well-studied notion in control theory, for developing both sample- and computation-efficient partially observable multi-agent RL.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Deep Reinforcement Learning for Privacy-Preserving Robotic-Assisted Surgery</title>
<link>https://arxiv.org/abs/2505.12153</link>
<guid>https://arxiv.org/abs/2505.12153</guid>
<content:encoded><![CDATA[
arXiv:2505.12153v2 Announce Type: replace 
Abstract: The integration of Reinforcement Learning (RL) into robotic-assisted surgery (RAS) holds significant promise for advancing surgical precision, adaptability, and autonomous decision-making. However, the development of robust RL models in clinical settings is hindered by key challenges, including stringent patient data privacy regulations, limited access to diverse surgical datasets, and high procedural variability. To address these limitations, this paper presents a Federated Deep Reinforcement Learning (FDRL) framework that enables decentralized training of RL models across multiple healthcare institutions without exposing sensitive patient information. A central innovation of the proposed framework is its dynamic policy adaptation mechanism, which allows surgical robots to select and tailor patient-specific policies in real-time, thereby ensuring personalized and Optimised interventions. To uphold rigorous privacy standards while facilitating collaborative learning, the FDRL framework incorporates secure aggregation, differential privacy, and homomorphic encryption techniques. Experimental results demonstrate a 60\% reduction in privacy leakage compared to conventional methods, with surgical precision maintained within a 1.5\% margin of a centralized baseline. This work establishes a foundational approach for adaptive, secure, and patient-centric AI-driven surgical robotics, offering a pathway toward clinical translation and scalable deployment across diverse healthcare environments.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models</title>
<link>https://arxiv.org/abs/2510.23824</link>
<guid>https://arxiv.org/abs/2510.23824</guid>
<content:encoded><![CDATA[
arXiv:2510.23824v1 Announce Type: new 
Abstract: Coordinating multiple autonomous agents in shared environments under decentralized conditions is a long-standing challenge in robotics and artificial intelligence. This work addresses the problem of decentralized goal assignment for multi-agent path planning, where agents independently generate ranked preferences over goals based on structured representations of the environment, including grid visualizations and scenario data. After this reasoning phase, agents exchange their goal rankings, and assignments are determined by a fixed, deterministic conflict-resolution rule (e.g., agent index ordering), without negotiation or iterative coordination. We systematically compare greedy heuristics, optimal assignment, and large language model (LLM)-based agents in fully observable grid-world settings. Our results show that LLM-based agents, when provided with well-designed prompts and relevant quantitative information, can achieve near-optimal makespans and consistently outperform traditional heuristics. These findings underscore the potential of language models for decentralized goal assignment in multi-agent path planning and highlight the importance of information structure in such systems.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EthVault: A Secure and Resource-Conscious FPGA-Based Ethereum Cold Wallet</title>
<link>https://arxiv.org/abs/2510.23847</link>
<guid>https://arxiv.org/abs/2510.23847</guid>
<content:encoded><![CDATA[
arXiv:2510.23847v1 Announce Type: new 
Abstract: Cryptocurrency blockchain networks safeguard digital assets using cryptographic keys, with wallets playing a critical role in generating, storing, and managing these keys. Wallets, typically categorized as hot and cold, offer varying degrees of security and convenience. However, they are generally software-based applications running on microcontrollers. Consequently, they are vulnerable to malware and side-channel attacks, allowing perpetrators to extract private keys by targeting critical algorithms, such as ECC, which processes private keys to generate public keys and authorize transactions. To address these issues, this work presents EthVault, the first hardware architecture for an Ethereum hierarchically deterministic cold wallet, featuring hardware implementations of key algorithms for secure key generation. Also, an ECC architecture resilient to side-channel and timing attacks is proposed. Moreover, an architecture of the child key derivation function, a fundamental component of cryptocurrency wallets, is proposed. The design minimizes resource usage, meeting market demand for small, portable cryptocurrency wallets. FPGA implementation results validate the feasibility of the proposed approach. The ECC architecture exhibits uniform execution behavior across varying inputs, while the complete design utilizes only 27%, 7%, and 6% of LUTs, registers, and RAM blocks, respectively, on a Xilinx Zynq UltraScale+ FPGA.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Causal Discovery using Judo Calculus</title>
<link>https://arxiv.org/abs/2510.23942</link>
<guid>https://arxiv.org/abs/2510.23942</guid>
<content:encoded><![CDATA[
arXiv:2510.23942v1 Announce Type: new 
Abstract: We describe a theory and implementation of an intuitionistic decentralized framework for causal discovery using judo calculus, which is formally defined as j-stable causal inference using j-do-calculus in a topos of sheaves. In real-world applications -- from biology to medicine and social science -- causal effects depend on regime (age, country, dose, genotype, or lab protocol). Our proposed judo calculus formalizes this context dependence formally as local truth: a causal claim is proven true on a cover of regimes, not everywhere at once. The Lawvere-Tierney modal operator j chooses which regimes are relevant; j-stability means the claim holds constructively and consistently across that family. We describe an algorithmic and implementation framework for judo calculus, combining it with standard score-based, constraint-based, and gradient-based causal discovery methods. We describe experimental results on a range of domains, from synthetic to real-world datasets from biology and economics. Our experimental results show the computational efficiency gained by the decentralized nature of sheaf-theoretic causal discovery, as well as improved performance over classical causal discovery methods.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Matchings Under Biased and Correlated Evaluations</title>
<link>https://arxiv.org/abs/2510.23628</link>
<guid>https://arxiv.org/abs/2510.23628</guid>
<content:encoded><![CDATA[
arXiv:2510.23628v1 Announce Type: cross 
Abstract: We study a two-institution stable matching model in which candidates from two distinct groups are evaluated using partially correlated signals that are group-biased. This extends prior work (which assumes institutions evaluate candidates in an identical manner) to a more realistic setting in which institutions rely on overlapping, but independently processed, criteria. These evaluations could consist of a variety of informative tools such as standardized tests, shared recommendation systems, or AI-based assessments with local noise. Two key parameters govern evaluations: the bias parameter $\beta \in (0,1]$, which models systematic disadvantage faced by one group, and the correlation parameter $\gamma \in [0,1]$, which captures the alignment between institutional rankings. We study the representation ratio, i.e., the ratio of disadvantaged to advantaged candidates selected by the matching process in this setting. Focusing on a regime in which all candidates prefer the same institution, we characterize the large-market equilibrium and derive a closed-form expression for the resulting representation ratio. Prior work shows that when $\gamma = 1$, this ratio scales linearly with $\beta$. In contrast, we show that the representation ratio increases nonlinearly with $\gamma$ and even modest losses in correlation can cause sharp drops in the representation ratio. Our analysis identifies critical $\gamma$-thresholds where institutional selection behavior undergoes discrete transitions, and reveals structural conditions under which evaluator alignment or bias mitigation are most effective. Finally, we show how this framework and results enable interventions for fairness-aware design in decentralized selection systems.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Problem-Parameter-Free Decentralized Bilevel Optimization</title>
<link>https://arxiv.org/abs/2510.24288</link>
<guid>https://arxiv.org/abs/2510.24288</guid>
<content:encoded><![CDATA[
arXiv:2510.24288v1 Announce Type: cross 
Abstract: Decentralized bilevel optimization has garnered significant attention due to its critical role in solving large-scale machine learning problems. However, existing methods often rely on prior knowledge of problem parameters-such as smoothness, convexity, or communication network topologies-to determine appropriate stepsizes. In practice, these problem parameters are typically unavailable, leading to substantial manual effort for hyperparameter tuning. In this paper, we propose AdaSDBO, a fully problem-parameter-free algorithm for decentralized bilevel optimization with a single-loop structure. AdaSDBO leverages adaptive stepsizes based on cumulative gradient norms to update all variables simultaneously, dynamically adjusting its progress and eliminating the need for problem-specific hyperparameter tuning. Through rigorous theoretical analysis, we establish that AdaSDBO achieves a convergence rate of $\widetilde{\mathcal{O}}\left(\frac{1}{T}\right)$, matching the performance of well-tuned state-of-the-art methods up to polylogarithmic factors. Extensive numerical experiments demonstrate that AdaSDBO delivers competitive performance compared to existing decentralized bilevel optimization methods while exhibiting remarkable robustness across diverse stepsize configurations.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning with Partially Labeled Data: A Conditional Distillation Approach</title>
<link>https://arxiv.org/abs/2412.18833</link>
<guid>https://arxiv.org/abs/2412.18833</guid>
<content:encoded><![CDATA[
arXiv:2412.18833v2 Announce Type: replace 
Abstract: In medical imaging, developing generalized segmentation models that can handle multiple organs and lesions is crucial. However, the scarcity of fully annotated datasets and strict privacy regulations present significant barriers to data sharing. Federated Learning (FL) allows decentralized model training, but existing FL methods often struggle with partial labeling, leading to model divergence and catastrophic forgetting. We propose ConDistFL, a novel FL framework incorporating conditional distillation to address these challenges. ConDistFL enables effective learning from partially labeled datasets, significantly improving segmentation accuracy across distributed and non-uniform datasets. In addition to its superior segmentation performance, ConDistFL maintains computational and communication efficiency, ensuring its scalability for real-world applications. Furthermore, ConDistFL demonstrates remarkable generalizability, significantly outperforming existing FL methods in out-of-federation tests, even adapting to unseen contrast phases (e.g., non-contrast CT images) in our experiments. Extensive evaluations on 3D CT and 2D chest X-ray datasets show that ConDistFL is an efficient, adaptable solution for collaborative medical image segmentation in privacy-constrained settings.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-augmented empirical game theoretic simulation for social-ecological systems</title>
<link>https://arxiv.org/abs/2510.21965</link>
<guid>https://arxiv.org/abs/2510.21965</guid>
<content:encoded><![CDATA[
arXiv:2510.21965v1 Announce Type: new 
Abstract: Designing institutions for social-ecological systems requires models that capture heterogeneity, uncertainty, and strategic interaction. Multiple modeling approaches have emerged to meet this challenge, including empirical game-theoretic analysis (EGTA), which merges ABM's scale and diversity with game-theoretic models' formal equilibrium analysis. The newly popular class of LLM-driven simulations provides yet another approach, and it is not clear how these approaches can be integrated with one another, nor whether the resulting simulations produce a plausible range of behaviours for real-world social-ecological governance. To address this gap, we compare four LLM-augmented frameworks: procedural ABMs, generative ABMs, LLM-EGTA, and expert guided LLM-EGTA, and evaluate them on a real-world case study of irrigation and fishing in the Amu Darya basin under centralized and decentralized governance. Our results show: first, procedural ABMs, generative ABMs, and LLM-augmented EGTA models produce strikingly different patterns of collective behaviour, highlighting the value of methodological diversity. Second, inducing behaviour through system prompts in LLMs is less effective than shaping behaviour through parameterized payoffs in an expert-guided EGTA-based model.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Power to the Clients: Federated Learning in a Dictatorship Setting</title>
<link>https://arxiv.org/abs/2510.22149</link>
<guid>https://arxiv.org/abs/2510.22149</guid>
<content:encoded><![CDATA[
arXiv:2510.22149v1 Announce Type: new 
Abstract: Federated learning (FL) has emerged as a promising paradigm for decentralized model training, enabling multiple clients to collaboratively learn a shared model without exchanging their local data. However, the decentralized nature of FL also introduces vulnerabilities, as malicious clients can compromise or manipulate the training process. In this work, we introduce dictator clients, a novel, well-defined, and analytically tractable class of malicious participants capable of entirely erasing the contributions of all other clients from the server model, while preserving their own. We propose concrete attack strategies that empower such clients and systematically analyze their effects on the learning process. Furthermore, we explore complex scenarios involving multiple dictator clients, including cases where they collaborate, act independently, or form an alliance in order to ultimately betray one another. For each of these settings, we provide a theoretical analysis of their impact on the global model's convergence. Our theoretical algorithms and findings about the complex scenarios including multiple dictator clients are further supported by empirical evaluations on both computer vision and natural language processing benchmarks.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Cost Allocation in Energy Communities: A DLMP-based Bilevel Optimization with a Shapley Value Approach</title>
<link>https://arxiv.org/abs/2510.22321</link>
<guid>https://arxiv.org/abs/2510.22321</guid>
<content:encoded><![CDATA[
arXiv:2510.22321v1 Announce Type: new 
Abstract: Energy communities (ECs) are emerging as a promising decentralized model for managing cooperative distributed energy resources (DERs). As these communities expand and their operations become increasingly integrated into the grid, ensuring fairness in allocating operating costs among participants becomes a challenge. In distribution networks, DER operations at the community level can influence Distribution Locational Marginal Prices (DLMPs), which in turn affect system's operation cost. This interdependence between local decisions and system-level pricing introduces new challenges for fair and transparent cost allocation. Despite growing interest in fairness-aware methods, most methods do not account for the impact of DLMPs. To fill this gap, we propose a bilevel optimization model in which a Community Energy Aggregator (CEA) schedules DERs across multiple ECs while a Distribution System Operator (DSO) determines DLMPs through network-constrained dispatch. Leveraging the Karush-Kuhn-Tucker (KKT) conditions and strong duality, the bilevel model is reformulated into a tractable single-level problem. We achieve fairness in the cost allocation by applying the Shapley value to quantify each community's marginal contribution to system-wide cost savings. The effectiveness of the proposed method is validated through simulations on several benchmark distribution systems.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ant-inspired Walling Strategies for Scalable Swarm Separation: Reinforcement Learning Approaches Based on Finite State Machines</title>
<link>https://arxiv.org/abs/2510.22524</link>
<guid>https://arxiv.org/abs/2510.22524</guid>
<content:encoded><![CDATA[
arXiv:2510.22524v1 Announce Type: new 
Abstract: In natural systems, emergent structures often arise to balance competing demands. Army ants, for example, form temporary "walls" that prevent interference between foraging trails. Inspired by this behavior, we developed two decentralized controllers for heterogeneous robotic swarms to maintain spatial separation while executing concurrent tasks. The first is a finite-state machine (FSM)-based controller that uses encounter-triggered transitions to create rigid, stable walls. The second integrates FSM states with a Deep Q-Network (DQN), dynamically optimizing separation through emergent "demilitarized zones." In simulation, both controllers reduce mixing between subgroups, with the DQN-enhanced controller improving adaptability and reducing mixing by 40-50% while achieving faster convergence.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZK Coprocessor Bridge: Replay-Safe Private Execution from Solana to Aztec via Wormhole</title>
<link>https://arxiv.org/abs/2510.22536</link>
<guid>https://arxiv.org/abs/2510.22536</guid>
<content:encoded><![CDATA[
arXiv:2510.22536v1 Announce Type: new 
Abstract: We formalize a cross-domain "ZK coprocessor bridge" that lets Solana programs request private execution on Aztec L2 (via Ethereum) using Wormhole Verifiable Action Approvals (VAAs) as authenticated transport. The system comprises: (i) a Solana program that posts messages to Wormhole Core with explicit finality; (ii) an EVM Portal that verifies VAAs, enforces a replay lock, parses a bound payload secretHash||m from the attested VAA, derives a domain-separated field commitment, and enqueues an L1->L2 message into the Aztec Inbox (our reference implementation v0.1.0 currently uses consumeWithSecret(vaa, secretHash); we provide migration guidance to the payload-bound interface); (iii) a minimal Aztec contract that consumes the message privately; and (iv) an off-chain relayer that ferries VAAs and can record receipts on Solana. We present state machines, message formats, and proof sketches for replay-safety, origin authenticity, finality alignment, parameter binding (no relayer front-running of Aztec parameters), privacy, idempotence, and liveness. Finally, we include a concise Reproducibility note with pinned versions and artifacts to replicate a public testnet run.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blockchain Signatures to Ensure Information Integrity and Non-Repudiation in the Digital Era: A comprehensive study</title>
<link>https://arxiv.org/abs/2510.22561</link>
<guid>https://arxiv.org/abs/2510.22561</guid>
<content:encoded><![CDATA[
arXiv:2510.22561v1 Announce Type: new 
Abstract: Blockchain systems rely on decentralized ledgers and strong security guarantees. A key requirement is non-repudiation, which prevents denial of transaction authorship and supports integrity of recorded data. This work surveys digital signature schemes used in blockchain platforms and analyzes how they deliver non-repudiation and contribute to overall system security. We examine representative scheme families and their cryptographic foundations, security assumptions, and properties relevant to deployment, including unforgeability, resistance to malleability, support for aggregation and multisignature or threshold settings, key and signature sizes, and verification cost. Using these criteria, we compare the suitability of different designs for consensus protocols, smart contract constraints, and resource limits. We highlight practical tradeoffs that affect throughput, storage, scalability, and attack surfaces, and summarize benefits and limitations of each scheme in blockchain contexts. The study underscores that carefully chosen digital signatures are central to achieving non-repudiation and preserving information integrity, and it outlines implementation considerations and open directions such as interoperability and post-quantum readiness.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Conditional Diffusion Model with Mean Field Communication as Wireless Resource Allocation Planner</title>
<link>https://arxiv.org/abs/2510.22969</link>
<guid>https://arxiv.org/abs/2510.22969</guid>
<content:encoded><![CDATA[
arXiv:2510.22969v1 Announce Type: new 
Abstract: In wireless communication systems, efficient and adaptive resource allocation plays a crucial role in enhancing overall Quality of Service (QoS). While centralized Multi-Agent Reinforcement Learning (MARL) frameworks rely on a central coordinator for policy training and resource scheduling, they suffer from scalability issues and privacy risks. In contrast, the Distributed Training with Decentralized Execution (DTDE) paradigm enables distributed learning and decision-making, but it struggles with non-stationarity and limited inter-agent cooperation, which can severely degrade system performance. To overcome these challenges, we propose the Multi-Agent Conditional Diffusion Model Planner (MA-CDMP) for decentralized communication resource management. Built upon the Model-Based Reinforcement Learning (MBRL) paradigm, MA-CDMP employs Diffusion Models (DMs) to capture environment dynamics and plan future trajectories, while an inverse dynamics model guides action generation, thereby alleviating the sample inefficiency and slow convergence of conventional DTDE methods. Moreover, to approximate large-scale agent interactions, a Mean-Field (MF) mechanism is introduced as an assistance to the classifier in DMs. This design mitigates inter-agent non-stationarity and enhances cooperation with minimal communication overhead in distributed settings. We further theoretically establish an upper bound on the distributional approximation error introduced by the MF-based diffusion generation, guaranteeing convergence stability and reliable modeling of multi-agent stochastic dynamics. Extensive experiments demonstrate that MA-CDMP consistently outperforms existing MARL baselines in terms of average reward and QoS metrics, showcasing its scalability and practicality for real-world wireless network optimization.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AirFed: Federated Graph-Enhanced Multi-Agent Reinforcement Learning for Multi-UAV Cooperative Mobile Edge Computing</title>
<link>https://arxiv.org/abs/2510.23053</link>
<guid>https://arxiv.org/abs/2510.23053</guid>
<content:encoded><![CDATA[
arXiv:2510.23053v1 Announce Type: new 
Abstract: Multiple Unmanned Aerial Vehicles (UAVs) cooperative Mobile Edge Computing (MEC) systems face critical challenges in coordinating trajectory planning, task offloading, and resource allocation while ensuring Quality of Service (QoS) under dynamic and uncertain environments. Existing approaches suffer from limited scalability, slow convergence, and inefficient knowledge sharing among UAVs, particularly when handling large-scale IoT device deployments with stringent deadline constraints. This paper proposes AirFed, a novel federated graph-enhanced multi-agent reinforcement learning framework that addresses these challenges through three key innovations. First, we design dual-layer dynamic Graph Attention Networks (GATs) that explicitly model spatial-temporal dependencies among UAVs and IoT devices, capturing both service relationships and collaborative interactions within the network topology. Second, we develop a dual-Actor single-Critic architecture that jointly optimizes continuous trajectory control and discrete task offloading decisions. Third, we propose a reputation-based decentralized federated learning mechanism with gradient-sensitive adaptive quantization, enabling efficient and robust knowledge sharing across heterogeneous UAVs. Extensive experiments demonstrate that AirFed achieves 42.9% reduction in weighted cost compared to state-of-the-art baselines, attains over 99% deadline satisfaction and 94.2% IoT device coverage rate, and reduces communication overhead by 54.5%. Scalability analysis confirms robust performance across varying UAV numbers, IoT device densities, and system scales, validating AirFed's practical applicability for large-scale UAV-MEC deployments.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blockchain and Biometrics: Survey, GDPR Analysis, and Future Directions</title>
<link>https://arxiv.org/abs/2302.10883</link>
<guid>https://arxiv.org/abs/2302.10883</guid>
<content:encoded><![CDATA[
arXiv:2302.10883v4 Announce Type: replace 
Abstract: Biometric recognition as an efficient and hard-to-forge way of identification and verification has become an indispensable part of the current digital world. The fast evolution of this technology has been a strong incentive for integration into many applications. Meanwhile, blockchain, the decentralized ledger technology, has been widely received by both research and industry in the past few years, and it is being increasingly deployed today in many different applications, such as money transfer, IoT, healthcare, or logistics. Recently, researchers have started to speculate on the pros and cons and what the best applications would be when these two technologies cross paths. This paper provides a survey of the research literature on the combination of blockchain and biometrics and includes a first legal analysis of this integration based on GDPR to shed light on challenges and potentials. Although the integration of blockchain technology into the biometric sector is still in its infancy, with a growing body of literature discussing specific applications and advanced technological setups, this paper aims to provide a holistic understanding of blockchain applicability in biometrics. Based on published studies, this article discusses, among others, practical examples combining blockchain and biometrics for novel applications in PKI systems, distributed trusted services, and identity management. Challenges and limitations when combining blockchain and biometrics that motivate future work will also be discussed; e.g., blockchain networks at their current stage may not be efficient or economical for some real-time biometric applications. Finally, we also discuss key legal aspects of the EU General Data Protection Regulation (GDPR) related to this combination of technologies (blockchain and biometrics); for example, accountability, immutability, anonymity, and data protection elements.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized EXTRA stochastic gradient Langevin dynamics</title>
<link>https://arxiv.org/abs/2412.01993</link>
<guid>https://arxiv.org/abs/2412.01993</guid>
<content:encoded><![CDATA[
arXiv:2412.01993v2 Announce Type: replace 
Abstract: Langevin algorithms are popular Markov Chain Monte Carlo methods for Bayesian learning, particularly when the aim is to sample from the posterior distribution of a parametric model, given the input data and the prior distribution over the model parameters. Their stochastic versions such as stochastic gradient Langevin dynamics (SGLD) allow iterative learning based on randomly sampled mini-batches of large datasets and are scalable to large datasets. However, when data is decentralized across a network of agents subject to communication and privacy constraints, standard SGLD algorithms cannot be applied. Instead, we employ decentralized SGLD (DE-SGLD) algorithms, where Bayesian learning is performed collaboratively by a network of agents without sharing individual data. Nonetheless, existing DE-SGLD algorithms induce a bias at every agent that can negatively impact performance; this bias persists even when using full batches and is attributable to network effects. Motivated by the EXTRA algorithm and its generalizations for decentralized optimization, we propose the generalized EXTRA stochastic gradient Langevin dynamics, which eliminates this bias in the full-batch setting. Moreover, we show that, in the mini-batch setting, our algorithm provides performance bounds that significantly improve upon those of standard DE-SGLD algorithms in the literature. Our numerical results also demonstrate the efficiency of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Various DeFi Price Manipulations with LLM Reasoning</title>
<link>https://arxiv.org/abs/2502.11521</link>
<guid>https://arxiv.org/abs/2502.11521</guid>
<content:encoded><![CDATA[
arXiv:2502.11521v2 Announce Type: replace 
Abstract: DeFi (Decentralized Finance) is one of the most important applications of today's cryptocurrencies and smart contracts. It manages hundreds of billions in Total Value Locked (TVL) on-chain, yet it remains susceptible to common DeFi price manipulation attacks. Despite state-of-the-art (SOTA) systems like DeFiRanger and DeFort, we found that they are less effective to non-standard price models in custom DeFi protocols, which account for 44.2% of the 95 DeFi price manipulation attacks reported over the past three years. In this paper, we introduce the first LLM-based approach, DeFiScope, for detecting DeFi price manipulation attacks in both standard and custom price models. Our insight is that large language models (LLMs) have certain intelligence to abstract price calculation from smart contract source code and infer the trend of token price changes based on the extracted price models. To further strengthen LLMs in this aspect, we leverage Foundry to synthesize on-chain data and use it to fine-tune a DeFi price-specific LLM. Together with the high-level DeFi operations recovered from low-level transaction data, DeFiScope detects various DeFi price manipulations according to systematically mined patterns. Experimental results show that DeFiScope achieves a high recall of 80% on real-world attacks, a precision of 96% on suspicious transactions, and zero false alarms on benign transactions, significantly outperforming SOTA approaches. Moreover, we evaluate DeFiScope's cost-effectiveness and demonstrate its practicality by helping our industry partner confirm 147 real-world price manipulation attacks, including discovering 81 previously unknown historical incidents.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking</title>
<link>https://arxiv.org/abs/2504.19940</link>
<guid>https://arxiv.org/abs/2504.19940</guid>
<content:encoded><![CDATA[
arXiv:2504.19940v2 Announce Type: replace 
Abstract: The growing spread of online misinformation has created an urgent need for scalable, reliable fact-checking solutions. Crowdsourced fact-checking - where non-experts evaluate claim veracity - offers a cost-effective alternative to expert verification, despite concerns about variability in quality and bias. Encouraged by promising results in certain contexts, major platforms such as X (formerly Twitter), Facebook, and Instagram have begun shifting from centralized moderation to decentralized, crowd-based approaches.
  In parallel, advances in Large Language Models (LLMs) have shown strong performance across core fact-checking tasks, including claim detection and evidence evaluation. However, their potential role in crowdsourced workflows remains unexplored. This paper investigates whether LLM-powered generative agents - autonomous entities that emulate human behavior and decision-making - can meaningfully contribute to fact-checking tasks traditionally reserved for human crowds.
  Using the protocol of La Barbera et al. (2024), we simulate crowds of generative agents with diverse demographic and ideological profiles. Agents retrieve evidence, assess claims along multiple quality dimensions, and issue final veracity judgments. Our results show that agent crowds outperform human crowds in truthfulness classification, exhibit higher internal consistency, and show reduced susceptibility to social and cognitive biases. Compared to humans, agents rely more systematically on informative criteria such as Accuracy, Precision, and Informativeness, suggesting a more structured decision-making process. Overall, our findings highlight the potential of generative agents as scalable, consistent, and less biased contributors to crowd-based fact-checking systems.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniFC: Rethinking Federated Clustering via Lossless and Secure Distance Reconstruction</title>
<link>https://arxiv.org/abs/2505.13071</link>
<guid>https://arxiv.org/abs/2505.13071</guid>
<content:encoded><![CDATA[
arXiv:2505.13071v2 Announce Type: replace 
Abstract: Federated clustering (FC) aims to discover global cluster structures across decentralized clients without sharing raw data, making privacy preservation a fundamental requirement. There are two critical challenges: (1) privacy leakage during collaboration, and (2) robustness degradation due to aggregation of proxy information from non-independent and identically distributed (Non-IID) local data, leading to inaccurate or inconsistent global clustering. Existing solutions typically rely on model-specific local proxies, which are sensitive to data heterogeneity and inherit inductive biases from their centralized counterparts, thus limiting robustness and generality. We propose Omni Federated Clustering (OmniFC), a unified and model-agnostic framework. Leveraging Lagrange coded computing, our method enables clients to share only encoded data, allowing exact reconstruction of the global distance matrix--a fundamental representation of sample relationships--without leaking private information, even under client collusion. This construction is naturally resilient to Non-IID data distributions. This approach decouples FC from model-specific proxies, providing a unified extension mechanism applicable to diverse centralized clustering methods. Theoretical analysis confirms both reconstruction fidelity and privacy guarantees, while comprehensive experiments demonstrate OmniFC's superior robustness, effectiveness, and generality across various benchmarks compared to state-of-the-art methods. Code will be released.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAO-AI: Evaluating Collective Decision-Making through Agentic AI in Decentralized Governance</title>
<link>https://arxiv.org/abs/2510.21117</link>
<guid>https://arxiv.org/abs/2510.21117</guid>
<content:encoded><![CDATA[
arXiv:2510.21117v1 Announce Type: new 
Abstract: This paper presents a first empirical study of agentic AI as autonomous decision-makers in decentralized governance. Using more than 3K proposals from major protocols, we build an agentic AI voter that interprets proposal contexts, retrieves historical deliberation data, and independently determines its voting position. The agent operates within a realistic financial simulation environment grounded in verifiable blockchain data, implemented through a modular composable program (MCP) workflow that defines data flow and tool usage via Agentics framework. We evaluate how closely the agent's decisions align with the human and token-weighted outcomes, uncovering strong alignments measured by carefully designed evaluation metrics. Our findings demonstrate that agentic AI can augment collective decision-making by producing interpretable, auditable, and empirically grounded signals in realistic DAO governance settings. The study contributes to the design of explainable and economically rigorous AI agents for decentralized financial systems.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QAE-BAC: Achieving Quantifiable Anonymity and Efficiency in Blockchain-Based Access Control with Attribute</title>
<link>https://arxiv.org/abs/2510.21124</link>
<guid>https://arxiv.org/abs/2510.21124</guid>
<content:encoded><![CDATA[
arXiv:2510.21124v1 Announce Type: new 
Abstract: Blockchain-based Attribute-Based Access Control (BC-ABAC) offers a decentralized paradigm for secure data governance but faces two inherent challenges: the transparency of blockchain ledgers threatens user privacy by enabling reidentification attacks through attribute analysis, while the computational complexity of policy matching clashes with blockchain's performance constraints. Existing solutions, such as those employing Zero-Knowledge Proofs (ZKPs), often incur high overhead and lack measurable anonymity guarantees, while efficiency optimizations frequently ignore privacy implications. To address these dual challenges, this paper proposes QAEBAC (Quantifiable Anonymity and Efficiency in Blockchain-Based Access Control with Attribute). QAE-BAC introduces a formal (r, t)-anonymity model to dynamically quantify the re-identification risk of users based on their access attributes and history. Furthermore, it features an Entropy-Weighted Path Tree (EWPT) that optimizes policy structure based on realtime anonymity metrics, drastically reducing policy matching complexity. Implemented and evaluated on Hyperledger Fabric, QAE-BAC demonstrates a superior balance between privacy and performance. Experimental results show that it effectively mitigates re-identification risks and outperforms state-of-the-art baselines, achieving up to an 11x improvement in throughput and an 87% reduction in latency, proving its practicality for privacy-sensitive decentralized applications.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Federated Learning for Smart Prediction and Recommendation Applications</title>
<link>https://arxiv.org/abs/2510.21183</link>
<guid>https://arxiv.org/abs/2510.21183</guid>
<content:encoded><![CDATA[
arXiv:2510.21183v1 Announce Type: new 
Abstract: This paper proposes a generative adversarial network and federated learning-based model to address various challenges of the smart prediction and recommendation applications, such as high response time, compromised data privacy, and data scarcity. The integration of the generative adversarial network and federated learning is referred to as Generative Federated Learning (GFL). As a case study of the proposed model, a heart health monitoring application is considered. The realistic synthetic datasets are generated using the generated adversarial network-based proposed algorithm for improving data diversity, data quality, and data augmentation, and remove the data scarcity and class imbalance issues. In this paper, we implement the centralized and decentralized federated learning approaches in an edge computing paradigm. In centralized federated learning, the edge nodes communicate with the central server to build the global and personalized local models in a collaborative manner. In the decentralized federated learning approach, the edge nodes communicate among themselves to exchange model updates for collaborative training. The comparative study shows that the proposed framework outperforms the existing heart health monitoring applications. The results show that using the proposed framework (i) the prediction accuracy is improved by 12% than the conventional framework, and (ii) the response time is reduced by 73% than the conventional cloud-only system.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Powered Detection of Price Manipulation in DeFi</title>
<link>https://arxiv.org/abs/2510.21272</link>
<guid>https://arxiv.org/abs/2510.21272</guid>
<content:encoded><![CDATA[
arXiv:2510.21272v1 Announce Type: new 
Abstract: Decentralized Finance (DeFi) smart contracts manage billions of dollars, making them a prime target for exploits. Price manipulation vulnerabilities, often via flash loans, are a devastating class of attacks causing significant financial losses. Existing detection methods are limited. Reactive approaches analyze attacks only after they occur, while proactive static analysis tools rely on rigid, predefined heuristics, limiting adaptability. Both depend on known attack patterns, failing to identify novel variants or comprehend complex economic logic. We propose PMDetector, a hybrid framework combining static analysis with Large Language Model (LLM)-based reasoning to proactively detect price manipulation vulnerabilities. Our approach uses a formal attack model and a three-stage pipeline. First, static taint analysis identifies potentially vulnerable code paths. Second, a two-stage LLM process filters paths by analyzing defenses and then simulates attacks to evaluate exploitability. Finally, a static analysis checker validates LLM results, retaining only high-risk paths and generating comprehensive vulnerability reports. To evaluate its effectiveness, we built a dataset of 73 real-world vulnerable and 288 benign DeFi protocols. Results show PMDetector achieves 88% precision and 90% recall with Gemini 2.5-flash, significantly outperforming state-of-the-art static analysis and LLM-based approaches. Auditing a vulnerability with PMDetector costs just $0.03 and takes 4.0 seconds with GPT-4.1, offering an efficient and cost-effective alternative to manual audits.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LIDC: A Location Independent Multi-Cluster Computing Framework for Data Intensive Science</title>
<link>https://arxiv.org/abs/2510.21373</link>
<guid>https://arxiv.org/abs/2510.21373</guid>
<content:encoded><![CDATA[
arXiv:2510.21373v1 Announce Type: new 
Abstract: Scientific communities are increasingly using geographically distributed computing platforms. The current methods of compute placement predominantly use logically centralized controllers such as Kubernetes (K8s) to match tasks to available resources. However, this centralized approach is unsuitable in multi-organizational collaborations. Furthermore, workflows often need to use manual configurations tailored for a single platform and cannot adapt to dynamic changes across infrastructure. Our work introduces a decentralized control plane for placing computations on geographically dispersed compute clusters using semantic names. We assign semantic names to computations to match requests with named Kubernetes (K8s) service endpoints. We show that this approach provides multiple benefits. First, it allows placement of computational jobs to be independent of location, enabling any cluster with sufficient resources to execute the computation. Second, it facilitates dynamic compute placement without requiring prior knowledge of cluster locations or predefined configurations.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLAMES: Fine-tuning LLMs to Synthesize Invariants for Smart Contract Security</title>
<link>https://arxiv.org/abs/2510.21401</link>
<guid>https://arxiv.org/abs/2510.21401</guid>
<content:encoded><![CDATA[
arXiv:2510.21401v1 Announce Type: new 
Abstract: Smart contract vulnerabilities cost billions of dollars annually, yet existing automated analysis tools fail to generate deployable defenses. We present FLAMES, a novel automated approach that synthesizes executable runtime guards as Solidity "require" statements to harden smart contracts against exploits. Unlike prior work that relies on vulnerability labels, symbolic analysis, or natural language specifications, FLAMES employs domain-adapted large language models trained through fill-in-the-middle supervised fine-tuning on real-world invariants extracted from 514,506 verified contracts. Our extensive evaluation across three dimensions demonstrates FLAMES's effectiveness: (1) Compilation: FLAMES achieves 96.7% compilability for synthesized invariant (2) Semantic Quality: on a curated test set of 5,000 challenging invariants, FLAMES produces exact or semantically equivalent matches to ground truth in 44.5% of cases; (3) Exploit Mitigation: FLAMES prevents 22 out of 108 real exploits (20.4%) while preserving contract functionality, and (4) FLAMES successfully blocks the real-world APEMAGA incident by synthesizing a pre-condition that mitigates the attack. FLAMES establishes that domain-adapted LLMs can automatically generate production-ready security defenses for smart contracts without requiring vulnerability detection, formal specifications, or human intervention. We release our code, model weights, datasets, and evaluation infrastructure to enable reproducible research in this critical domain.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Catastrophic Forgetting Mitigation Methods in Federated Time Series Forecasting</title>
<link>https://arxiv.org/abs/2510.21491</link>
<guid>https://arxiv.org/abs/2510.21491</guid>
<content:encoded><![CDATA[
arXiv:2510.21491v1 Announce Type: new 
Abstract: Catastrophic forgetting (CF) poses a persistent challenge in continual learning (CL), especially within federated learning (FL) environments characterized by non-i.i.d. time series data. While existing research has largely focused on classification tasks in vision domains, the regression-based forecasting setting prevalent in IoT and edge applications remains underexplored. In this paper, we present the first benchmarking framework tailored to investigate CF in federated continual time series forecasting. Using the Beijing Multi-site Air Quality dataset across 12 decentralized clients, we systematically evaluate several CF mitigation strategies, including Replay, Elastic Weight Consolidation, Learning without Forgetting, and Synaptic Intelligence. Key contributions include: (i) introducing a new benchmark for CF in time series FL, (ii) conducting a comprehensive comparative analysis of state-of-the-art methods, and (iii) releasing a reproducible open-source framework. This work provides essential tools and insights for advancing continual learning in federated time-series forecasting systems.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auction-Based Responsibility Allocation for Scalable Decentralized Safety Filters in Cooperative Multi-Agent Collision Avoidance</title>
<link>https://arxiv.org/abs/2510.21546</link>
<guid>https://arxiv.org/abs/2510.21546</guid>
<content:encoded><![CDATA[
arXiv:2510.21546v1 Announce Type: new 
Abstract: This paper proposes a scalable decentralized safety filter for multi-agent systems based on high-order control barrier functions (HOCBFs) and auction-based responsibility allocation. While decentralized HOCBF formulations ensure pairwise safety under input bounds, they face feasibility and scalability challenges as the number of agents grows. Each agent must evaluate an increasing number of pairwise constraints, raising the risk of infeasibility and making it difficult to meet real-time requirements. To address this, we introduce an auction-based allocation scheme that distributes constraint enforcement asymmetrically among neighbors based on local control effort estimates. The resulting directed responsibility graph guarantees full safety coverage while reducing redundant constraints and per-agent computational load. Simulation results confirm safe and efficient coordination across a range of network sizes and interaction densities.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Genetic Algorithm for Optimal User Order Routing: Multi-Objective Solver Optimization in CoW Protocol Batch Auctions</title>
<link>https://arxiv.org/abs/2510.21647</link>
<guid>https://arxiv.org/abs/2510.21647</guid>
<content:encoded><![CDATA[
arXiv:2510.21647v1 Announce Type: new 
Abstract: CoW Protocol batch auctions aggregate user intents and rely on solvers to find optimal execution paths that maximize user surplus across heterogeneous automated market makers (AMMs) under stringent auction deadlines. Deterministic single-objective heuristics that optimize only expected output frequently fail to exploit split-flow opportunities across multiple parallel paths and to internalize gas, slippage, and execution risk constraints in a unified search. We apply evolutionary multi-objective optimization to this blockchain routing problem, proposing a hybrid genetic algorithm (GA) architecture for real-time solver optimization that combines a production-grade, multi-objective NSGA-II engine with adaptive instance profiling and deterministic baselines. Our core engine encodes variable-length path sets with continuous split ratios and evolves candidate route-and-volume allocations under a Pareto objective vector F = (user surplus, -gas, -slippage, -risk), enabling principled trade-offs and anytime operation within the auction deadline. An adaptive controller selects between GA and a deterministic dual-decomposition optimizer with Bellman-Ford based negative-cycle detection, with a guarantee to never underperform the baseline. The open-source system integrates six protection layers and passes 8/8 tests, validating safety and correctness. In a 14-stratum benchmark (30 seeds each), the hybrid approach yields absolute user-surplus gains of approximately 0.40-9.82 ETH on small-to-medium orders, while large high-fragmentation orders are unprofitable across gas regimes. Convergence occurs in about 0.5 s median (soft capped at 1.0 s) within a 2-second limit. We are not aware of an openly documented multi-objective GA with end-to-end safety for real-time DEX routing.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mean-Field Sampling for Cooperative Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2412.00661</link>
<guid>https://arxiv.org/abs/2412.00661</guid>
<content:encoded><![CDATA[
arXiv:2412.00661v4 Announce Type: replace 
Abstract: Designing efficient algorithms for multi-agent reinforcement learning (MARL) is fundamentally challenging because the size of the joint state and action spaces grows exponentially in the number of agents. These difficulties are exacerbated when balancing sequential global decision-making with local agent interactions. In this work, we propose a new algorithm $\texttt{SUBSAMPLE-MFQ}$ ($\textbf{Subsample}$-$\textbf{M}$ean-$\textbf{F}$ield-$\textbf{Q}$-learning) and a decentralized randomized policy for a system with $n$ agents. For any $k\leq n$, our algorithm learns a policy for the system in time polynomial in $k$. We prove that this learned policy converges to the optimal policy on the order of $\tilde{O}(1/\sqrt{k})$ as the number of subsampled agents $k$ increases. In particular, this bound is independent of the number of agents $n$.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Latent-Space Constraints in Personalized Federated Learning</title>
<link>https://arxiv.org/abs/2505.07525</link>
<guid>https://arxiv.org/abs/2505.07525</guid>
<content:encoded><![CDATA[
arXiv:2505.07525v2 Announce Type: replace 
Abstract: Federated learning (FL) is an effective and widely used approach to training deep learning models on decentralized datasets held by distinct clients. FL also strengthens both security and privacy protections for training data. Common challenges associated with statistical heterogeneity between distributed datasets have spurred significant interest in personalized FL (pFL) methods, where models combine aspects of global learning with local modeling specific to each client's unique characteristics. This work investigates the efficacy of theoretically supported, adaptive MMD measures in pFL, primarily focusing on the Ditto framework, a state-of-the-art technique for distributed data heterogeneity. The use of such measures significantly improves model performance across a variety of tasks, especially those with pronounced feature heterogeneity. Additional experiments demonstrate that such measures are directly applicable to other pFL techniques and yield similar improvements across a number of datasets. Finally, the results motivate the use of constraints tailored to the various kinds of heterogeneity expected in FL systems.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Combinatorial Auction for Blockchain Trade Intents: Being Fair without Knowing What is Fair</title>
<link>https://arxiv.org/abs/2408.12225</link>
<guid>https://arxiv.org/abs/2408.12225</guid>
<content:encoded><![CDATA[
arXiv:2408.12225v3 Announce Type: replace-cross 
Abstract: We study blockchain trade-intent auctions, which currently intermediate about USD 10 billion in trades each month. These auctions are combinatorial because executing multiple trade intents jointly generates additional efficiencies. However, the auctioneer cannot observe what each trader would have received had its order been auctioned individually and hence cannot determine how these efficiencies should be shared. We compare the two dominant mechanisms - batch auctions and simultaneous individual auctions - and introduce a novel definition of fairness applicable to combinatorial auctions. We then propose a fair combinatorial auction that endogenously constructs a fairness benchmark from individual bids and a counterfactual mechanism. Whether fairness guarantees arise in equilibrium depends on the counterfactual: all traders receive more in the equilibrium of the fair combinatorial auction than in the equilibrium of the counterfactual mechanism when the counterfactual is simultaneous first-price auctions, but that may not be the case if the counterfactual is simultaneous second-price auctions.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Context Contracts - MCP-Enabled Framework to Integrate LLMs With Blockchain Smart Contracts</title>
<link>https://arxiv.org/abs/2510.19856</link>
<guid>https://arxiv.org/abs/2510.19856</guid>
<content:encoded><![CDATA[
arXiv:2510.19856v1 Announce Type: new 
Abstract: In recent years, blockchain has experienced widespread adoption across various industries, becoming integral to numerous enterprise applications. Concurrently, the rise of generative AI and LLMs has transformed human-computer interactions, offering advanced capabilities in understanding and generating human-like text. The introduction of the MCP has further enhanced AI integration by standardizing communication between AI systems and external data sources. Despite these advancements, there is still no standardized method for seamlessly integrating LLM applications and blockchain. To address this concern, we propose "MCC: Model Context Contracts" a novel framework that enables LLMs to interact directly with blockchain smart contracts through MCP-like protocol. This integration allows AI agents to invoke blockchain smart contracts, facilitating more dynamic and context-aware interactions between users and blockchain networks. Essentially, it empowers users to interact with blockchain systems and perform transactions using queries in natural language. Within this proposed architecture, blockchain smart contracts can function as intelligent agents capable of recognizing user input in natural language and executing the corresponding transactions. To ensure that the LLM accurately interprets natural language inputs and maps them to the appropriate MCP functions, the LLM was fine-tuned using a custom dataset comprising user inputs paired with their corresponding MCP server functions. This fine-tuning process significantly improved the platform's performance and accuracy. To validate the effectiveness of MCC, we have developed an end-to-end prototype implemented on the Rahasak blockchain with the fine-tuned Llama-4 LLM. To the best of our knowledge, this research represents the first approach to using the concept of Model Context Protocol to integrate LLMs with blockchain.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Privacy-Utility Trade-off in Decentralized Federated Learning via $f$-Differential Privacy</title>
<link>https://arxiv.org/abs/2510.19934</link>
<guid>https://arxiv.org/abs/2510.19934</guid>
<content:encoded><![CDATA[
arXiv:2510.19934v1 Announce Type: new 
Abstract: Differentially private (DP) decentralized Federated Learning (FL) allows local users to collaborate without sharing their data with a central server. However, accurately quantifying the privacy budget of private FL algorithms is challenging due to the co-existence of complex algorithmic components such as decentralized communication and local updates. This paper addresses privacy accounting for two decentralized FL algorithms within the $f$-differential privacy ($f$-DP) framework. We develop two new $f$-DP-based accounting methods tailored to decentralized settings: Pairwise Network $f$-DP (PN-$f$-DP), which quantifies privacy leakage between user pairs under random-walk communication, and Secret-based $f$-Local DP (Sec-$f$-LDP), which supports structured noise injection via shared secrets. By combining tools from $f$-DP theory and Markov chain concentration, our accounting framework captures privacy amplification arising from sparse communication, local iterations, and correlated noise. Experiments on synthetic and real datasets demonstrate that our methods yield consistently tighter $(\epsilon,\delta)$ bounds and improved utility compared to R\'enyi DP-based approaches, illustrating the benefits of $f$-DP in decentralized privacy accounting.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>zk-Agreements: A Privacy-Preserving Way to Establish Deterministic Trust in Confidential Agreements</title>
<link>https://arxiv.org/abs/2510.20007</link>
<guid>https://arxiv.org/abs/2510.20007</guid>
<content:encoded><![CDATA[
arXiv:2510.20007v1 Announce Type: new 
Abstract: Digital transactions currently exceed trillions of dollars annually, yet traditional paper-based agreements remain a bottleneck for automation, enforceability, and dispute resolution. Natural language contracts introduce ambiguity, require manual processing, and lack computational verifiability, all of which hinder efficient digital commerce. Computable legal contracts, expressed in machine-readable formats, offer a potential solution by enabling automated execution and verification. Blockchain-based smart contracts further strengthen enforceability and accelerate dispute resolution; however, current implementations risk exposing sensitive agreement terms on public ledgers, raising serious privacy and competitive intelligence concerns that limit enterprise adoption.
  We introduce zk-agreements, a protocol designed to transition from paper-based trust to cryptographic trust while preserving confidentiality. Our design combines zero-knowledge proofs to protect private agreement terms, secure two-party computation to enable private compliance evaluation, and smart contracts to guarantee automated enforcement. Together, these components achieve both privacy preservation and computational enforceability, resolving the fundamental tension between transparency and confidentiality in blockchain-based agreements.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMAS$^2$: Joint Agent Selection and Information-Theoretic Coordinated Perception In Dec-POMDPs</title>
<link>https://arxiv.org/abs/2510.20009</link>
<guid>https://arxiv.org/abs/2510.20009</guid>
<content:encoded><![CDATA[
arXiv:2510.20009v1 Announce Type: new 
Abstract: We study the problem of jointly selecting sensing agents and synthesizing decentralized active perception policies for the chosen subset of agents within a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) framework. Our approach employs a two-layer optimization structure. In the inner layer, we introduce information-theoretic metrics, defined by the mutual information between the unknown trajectories or some hidden property in the environment and the collective partial observations in the multi-agent system, as a unified objective for active perception problems. We employ various optimization methods to obtain optimal sensor policies that maximize mutual information for distinct active perception tasks. In the outer layer, we prove that under certain conditions, the information-theoretic objectives are monotone and submodular with respect to the subset of observations collected from multiple agents. We then exploit this property to design an IMAS$^2$ (Information-theoretic Multi-Agent Selection and Sensing) algorithm for joint sensing agent selection and sensing policy synthesis. However, since the policy search space is infinite, we adapt the classical Nemhauser-Wolsey argument to prove that the proposed IMAS$^2$ algorithm can provide a tight $(1 - 1/e)$-guarantee on the performance. Finally, we demonstrate the effectiveness of our approach in a multi-agent cooperative perception in a grid-world environment.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADP-VRSGP: Decentralized Learning with Adaptive Differential Privacy via Variance-Reduced Stochastic Gradient Push</title>
<link>https://arxiv.org/abs/2510.20157</link>
<guid>https://arxiv.org/abs/2510.20157</guid>
<content:encoded><![CDATA[
arXiv:2510.20157v1 Announce Type: new 
Abstract: Differential privacy is widely employed in decentralized learning to safeguard sensitive data by introducing noise into model updates. However, existing approaches that use fixed-variance noise often degrade model performance and reduce training efficiency. To address these limitations, we propose a novel approach called decentralized learning with adaptive differential privacy via variance-reduced stochastic gradient push (ADP-VRSGP). This method dynamically adjusts both the noise variance and the learning rate using a stepwise-decaying schedule, which accelerates training and enhances final model performance while providing node-level personalized privacy guarantees. To counteract the slowed convergence caused by large-variance noise in early iterations, we introduce a progressive gradient fusion strategy that leverages historical gradients. Furthermore, ADP-VRSGP incorporates decentralized push-sum and aggregation techniques, making it particularly suitable for time-varying communication topologies. Through rigorous theoretical analysis, we demonstrate that ADP-VRSGP achieves robust convergence with an appropriate learning rate, significantly improving training stability and speed. Experimental results validate that our method outperforms existing baselines across multiple scenarios, highlighting its efficacy in addressing the challenges of privacy-preserving decentralized learning.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRUST: A Decentralized Framework for Auditing Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2510.20188</link>
<guid>https://arxiv.org/abs/2510.20188</guid>
<content:encoded><![CDATA[
arXiv:2510.20188v1 Announce Type: new 
Abstract: Large Language Models generate complex reasoning chains that reveal their decision-making, yet verifying the faithfulness and harmlessness of these intermediate steps remains a critical unsolved problem. Existing auditing methods are centralized, opaque, and hard to scale, creating significant risks for deploying proprietary models in high-stakes domains. We identify four core challenges: (1) Robustness: Centralized auditors are single points of failure, prone to bias or attacks. (2) Scalability: Reasoning traces are too long for manual verification. (3) Opacity: Closed auditing undermines public trust. (4) Privacy: Exposing full reasoning risks model theft or distillation. We propose TRUST, a transparent, decentralized auditing framework that overcomes these limitations via: (1) A consensus mechanism among diverse auditors, guaranteeing correctness under up to $30\%$ malicious participants. (2) A hierarchical DAG decomposition of reasoning traces, enabling scalable, parallel auditing. (3) A blockchain ledger that records all verification decisions for public accountability. (4) Privacy-preserving segmentation, sharing only partial reasoning steps to protect proprietary logic. We provide theoretical guarantees for the security and economic incentives of the TRUST framework. Experiments across multiple LLMs (GPT-OSS, DeepSeek-r1, Qwen) and reasoning tasks (math, medical, science, humanities) show TRUST effectively detects reasoning flaws and remains robust against adversarial auditors. Our work pioneers decentralized AI auditing, offering a practical path toward safe and trustworthy LLM deployment.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NODA-MMH: Certified Learning-Aided Nonlinear Control for Magnetically-Actuated Swarm Experiment Toward On-Orbit Proof</title>
<link>https://arxiv.org/abs/2510.20231</link>
<guid>https://arxiv.org/abs/2510.20231</guid>
<content:encoded><![CDATA[
arXiv:2510.20231v1 Announce Type: new 
Abstract: This study experimentally validates the principle of large-scale satellite swarm control through learning-aided magnetic field interactions generated by satellite-mounted magnetorquers. This actuation presents a promising solution for the long-term formation maintenance of multiple satellites and has primarily been demonstrated in ground-based testbeds for two-satellite position control. However, as the number of satellites increases beyond three, fundamental challenges coupled with the high nonlinearity arise: 1) nonholonomic constraints, 2) underactuation, 3) scalability, and 4) computational cost. Previous studies have shown that time-integrated current control theoretically solves these problems, where the average actuator outputs align with the desired command, and a learning-based technique further enhances their performance. Through multiple experiments, we validate critical aspects of learning-aided time-integrated current control: (1) enhanced controllability of the averaged system dynamics, with a theoretically guaranteed error bound, and (2) decentralized current management. We design two-axis coils and a ground-based experimental setup utilizing an air-bearing platform, enabling a mathematical replication of orbital dynamics. Based on the effectiveness of the learned interaction model, we introduce NODA-MMH (Neural power-Optimal Dipole Allocation for certified learned Model-based Magnetically swarm control Harness) for model-based power-optimal swarm control. This study complements our tutorial paper on magnetically actuated swarms for the long-term formation maintenance problem.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Decentralized Reinforcement Learning for Modular Reconfigurable Lunar Robots</title>
<link>https://arxiv.org/abs/2510.20347</link>
<guid>https://arxiv.org/abs/2510.20347</guid>
<content:encoded><![CDATA[
arXiv:2510.20347v1 Announce Type: new 
Abstract: Modular reconfigurable robots suit task-specific space operations, but the combinatorial growth of morphologies hinders unified control. We propose a decentralized reinforcement learning (Dec-RL) scheme where each module learns its own policy: wheel modules use Soft Actor-Critic (SAC) for locomotion and 7-DoF limbs use Proximal Policy Optimization (PPO) for steering and manipulation, enabling zero-shot generalization to unseen configurations. In simulation, the steering policy achieved a mean absolute error of 3.63{\deg} between desired and induced angles; the manipulation policy plateaued at 84.6 % success on a target-offset criterion; and the wheel policy cut average motor torque by 95.4 % relative to baseline while maintaining 99.6 % success. Lunar-analogue field tests validated zero-shot integration for autonomous locomotion, steering, and preliminary alignment for reconfiguration. The system transitioned smoothly among synchronous, parallel, and sequential modes for Policy Execution, without idle states or control conflicts, indicating a scalable, reusable, and robust approach for modular lunar robots.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SheafAlign: A Sheaf-theoretic Framework for Decentralized Multimodal Alignment</title>
<link>https://arxiv.org/abs/2510.20540</link>
<guid>https://arxiv.org/abs/2510.20540</guid>
<content:encoded><![CDATA[
arXiv:2510.20540v1 Announce Type: new 
Abstract: Conventional multimodal alignment methods assume mutual redundancy across all modalities, an assumption that fails in real-world distributed scenarios. We propose SheafAlign, a sheaf-theoretic framework for decentralized multimodal alignment that replaces single-space alignment with multiple comparison spaces. This approach models pairwise modality relations through sheaf structures and leverages decentralized contrastive learning-based objectives for training. SheafAlign overcomes the limitations of prior methods by not requiring mutual redundancy among all modalities, preserving both shared and unique information. Experiments on multimodal sensing datasets show superior zero-shot generalization, cross-modal alignment, and robustness to missing modalities, with 50\% lower communication cost than state-of-the-art baselines.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Small Gain and Phase Stability Conditions for Grid-Forming Converters: Limitations and Extensions</title>
<link>https://arxiv.org/abs/2510.20544</link>
<guid>https://arxiv.org/abs/2510.20544</guid>
<content:encoded><![CDATA[
arXiv:2510.20544v1 Announce Type: new 
Abstract: The increasing share of converter based resources in power systems calls for scalable methods to analyse stability without relying on exhaustive system wide simulations. Decentralized small gain and small-phase criteria have recently been proposed for this purpose, but their applicability to grid forming converters is severely limited by the sectoriality assumption, which is not typically satisfied at low frequencies. This work revisits and extends mixed gain phase conditions by introducing loop shaping transformations that reformulate converter and network models in alternative coordinate frames. The proposed approach resolves intrinsic non sectoriality at low frequencies and reduces conservativeness, thereby improving the applicability of decentralized stability certification. Analytical results are illustrated using an infinite bus system first and then extended to the IEEE 14 bus network, demonstrating the practicality and scalability of the method. These findings provide a pathway toward less conservative and more widely applicable decentralized stability certificates in power grids.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe Decentralized Density Control of Multi-Robot Systems using PDE-Constrained Optimization with State Constraints</title>
<link>https://arxiv.org/abs/2510.20643</link>
<guid>https://arxiv.org/abs/2510.20643</guid>
<content:encoded><![CDATA[
arXiv:2510.20643v1 Announce Type: new 
Abstract: In this paper, we introduce a decentralized optimization-based density controller designed to enforce set invariance constraints in multi-robot systems. By designing a decentralized control barrier function, we derived sufficient conditions under which local safety constraints guarantee global safety. We account for localization and motion noise explicitly by modeling robots as spatial probability density functions governed by the Fokker-Planck equation. Compared to traditional centralized approaches, our controller requires less computational and communication power, making it more suitable for deployment in situations where perfect communication and localization are impractical. The controller is validated through simulations and experiments with four quadcopters.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Exchange that Mitigate a Bribery Attack</title>
<link>https://arxiv.org/abs/2510.20645</link>
<guid>https://arxiv.org/abs/2510.20645</guid>
<content:encoded><![CDATA[
arXiv:2510.20645v1 Announce Type: new 
Abstract: Despite the popularity of Hashed Time-Locked Contracts (HTLCs) because of their use in wide areas of applications such as payment channels, atomic swaps, etc, their use in exchange is still questionable. This is because of its incentive incompatibility and susceptibility to bribery attacks.
  State-of-the-art solutions such as MAD-HTLC (Oakland'21) and He-HTLC (NDSS'23) address this by leveraging miners' profit-driven behaviour to mitigate such attacks. The former is the mitigation against passive miners; however, the latter works against both active and passive miners. However, they consider only two bribing scenarios where either of the parties involved in the transfer collude with the miner.
  In this paper, we expose vulnerabilities in state-of-the-art solutions by presenting a miner-collusion bribery attack with implementation and game-theoretic analysis. Additionally, we propose a stronger attack on MAD-HTLC than He-HTLC, allowing the attacker to earn profits equivalent to attacking naive HTLC.
  Leveraging our insights, we propose \prot, a game-theoretically secure HTLC protocol resistant to all bribery scenarios. \prot\ employs a two-phase approach, preventing unauthorized token confiscation by third parties, such as miners. In Phase 1, parties commit to the transfer; in Phase 2, the transfer is executed without manipulation. We demonstrate \prot's efficiency in transaction cost and latency via implementations on Bitcoin and Ethereum.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bilevel Analysis of Cost and Emissions Externalities from Data Center Load Shifting</title>
<link>https://arxiv.org/abs/2510.20805</link>
<guid>https://arxiv.org/abs/2510.20805</guid>
<content:encoded><![CDATA[
arXiv:2510.20805v1 Announce Type: new 
Abstract: Data centers are emerging as large, flexible electricity consumers capable of shifting computational workloads across locations in response to economic and environmental signals. While this flexibility has potential for emissions reduction, its impact on power system operations depends critically on how such behavior interacts with network constraints and market signals. We develop a bilevel optimization framework in which a data center minimizes a weighted combination of electricity cost and marginal emissions intensity (LME), while the system operator clears economic dispatch under transmission and generation constraints. Focusing on a stylized three-bus power system, we derive closed-form, piecewise-linear expressions for both the data center and system-wide objectives as functions of the data centers' load shift. These expressions capture threshold-driven regime changes due to congestion and renewable saturation. We identify sufficient conditions under which the data center's decentralized decisions align with or diverge from socially optimal behavior and characterize the resulting externalities. Our results reveal how system topology and generator asymmetry affect incentive alignment and provide insight into when marginal price or emissions signals may fail to guide flexible loads toward socially beneficial outcomes. Our results offer a tractable starting point for analyzing decentralized flexibility under carbon-aware incentives and suggest directions for improving coordination between flexible loads and system operations.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Decentralized Routing Policies via Graph Attention-based Multi-Agent Reinforcement Learning in Lunar Delay-Tolerant Networks</title>
<link>https://arxiv.org/abs/2510.20436</link>
<guid>https://arxiv.org/abs/2510.20436</guid>
<content:encoded><![CDATA[
arXiv:2510.20436v1 Announce Type: cross 
Abstract: We present a fully decentralized routing framework for multi-robot exploration missions operating under the constraints of a Lunar Delay-Tolerant Network (LDTN). In this setting, autonomous rovers must relay collected data to a lander under intermittent connectivity and unknown mobility patterns. We formulate the problem as a Partially Observable Markov Decision Problem (POMDP) and propose a Graph Attention-based Multi-Agent Reinforcement Learning (GAT-MARL) policy that performs Centralized Training, Decentralized Execution (CTDE). Our method relies only on local observations and does not require global topology updates or packet replication, unlike classical approaches such as shortest path and controlled flooding-based algorithms. Through Monte Carlo simulations in randomized exploration environments, GAT-MARL provides higher delivery rates, no duplications, and fewer packet losses, and is able to leverage short-term mobility forecasts; offering a scalable solution for future space robotic systems for planetary exploration, as demonstrated by successful generalization to larger rover teams.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sharp Gaussian approximations for Decentralized Federated Learning</title>
<link>https://arxiv.org/abs/2505.08125</link>
<guid>https://arxiv.org/abs/2505.08125</guid>
<content:encoded><![CDATA[
arXiv:2505.08125v2 Announce Type: replace-cross 
Abstract: Federated Learning has gained traction in privacy-sensitive collaborative environments, with local SGD emerging as a key optimization method in decentralized settings. While its convergence properties are well-studied, asymptotic statistical guarantees beyond convergence remain limited. In this paper, we present two generalized Gaussian approximation results for local SGD and explore their implications. First, we prove a Berry-Esseen theorem for the final local SGD iterates, enabling valid multiplier bootstrap procedures. Second, motivated by robustness considerations, we introduce two distinct time-uniform Gaussian approximations for the entire trajectory of local SGD. The time-uniform approximations support Gaussian bootstrap-based tests for detecting adversarial attacks. Extensive simulations are provided to support our theoretical results.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>POLAR: Policy-based Layerwise Reinforcement Learning Method for Stealthy Backdoor Attacks in Federated Learning</title>
<link>https://arxiv.org/abs/2510.19056</link>
<guid>https://arxiv.org/abs/2510.19056</guid>
<content:encoded><![CDATA[
arXiv:2510.19056v1 Announce Type: new 
Abstract: Federated Learning (FL) enables decentralized model training across multiple clients without exposing local data, but its distributed feature makes it vulnerable to backdoor attacks. Despite early FL backdoor attacks modifying entire models, recent studies have explored the concept of backdoor-critical (BC) layers, which poison the chosen influential layers to maintain stealthiness while achieving high effectiveness. However, existing BC layers approaches rely on rule-based selection without consideration of the interrelations between layers, making them ineffective and prone to detection by advanced defenses. In this paper, we propose POLAR (POlicy-based LAyerwise Reinforcement learning), the first pipeline to creatively adopt RL to solve the BC layer selection problem in layer-wise backdoor attack. Different from other commonly used RL paradigm, POLAR is lightweight with Bernoulli sampling. POLAR dynamically learns an attack strategy, optimizing layer selection using policy gradient updates based on backdoor success rate (BSR) improvements. To ensure stealthiness, we introduce a regularization constraint that limits the number of modified layers by penalizing large attack footprints. Extensive experiments demonstrate that POLAR outperforms the latest attack methods by up to 40% against six state-of-the-art (SOTA) defenses.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Securing IoT Communications via Anomaly Traffic Detection: Synergy of Genetic Algorithm and Ensemble Method</title>
<link>https://arxiv.org/abs/2510.19121</link>
<guid>https://arxiv.org/abs/2510.19121</guid>
<content:encoded><![CDATA[
arXiv:2510.19121v1 Announce Type: new 
Abstract: The rapid growth of the Internet of Things (IoT) has transformed industries by enabling seamless data exchange among connected devices. However, IoT networks remain vulnerable to security threats such as denial of service (DoS) attacks, anomalous traffic, and data manipulation due to decentralized architectures and limited resources. To address these issues, this paper proposes an advanced anomaly detection framework with three main phases. First, data preprocessing is performed using the Median KS Test to remove noise, handle missing values, and balance datasets for cleaner input. Second, a feature selection phase employs a Genetic Algorithm combined with eagle inspired search strategies to identify the most relevant features, reduce dimensionality, and improve efficiency without sacrificing accuracy. Finally, an ensemble classifier integrates Decision Tree, Random Forest, and XGBoost algorithms to achieve accurate and reliable anomaly detection. The proposed model demonstrates high adaptability and scalability across diverse IoT environments. Experimental results show that it outperforms existing methods by achieving 98 percent accuracy, 95 percent detection rate, and reductions in false positive (10 percent) and false negative (5 percent) rates. These results confirm the framework effectiveness and robustness in improving IoT network security against evolving cyber threats.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Communication-Efficient Decentralized Actor-Critic Algorithm</title>
<link>https://arxiv.org/abs/2510.19199</link>
<guid>https://arxiv.org/abs/2510.19199</guid>
<content:encoded><![CDATA[
arXiv:2510.19199v1 Announce Type: new 
Abstract: In this paper, we study the problem of reinforcement learning in multi-agent systems where communication among agents is limited. We develop a decentralized actor-critic learning framework in which each agent performs several local updates of its policy and value function, where the latter is approximated by a multi-layer neural network, before exchanging information with its neighbors. This local training strategy substantially reduces the communication burden while maintaining coordination across the network. We establish finite-time convergence analysis for the algorithm under Markov-sampling. Specifically, to attain the $\varepsilon$-accurate stationary point, the sample complexity is of order $\mathcal{O}(\varepsilon^{-3})$ and the communication complexity is of order $\mathcal{O}(\varepsilon^{-1}\tau^{-1})$, where tau denotes the number of local training steps. We also show how the final error bound depends on the neural network's approximation quality. Numerical experiments in a cooperative control setting illustrate and validate the theoretical findings.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatiotemporal Tubes based Control of Unknown Multi-Agent Systems for Temporal Reach-Avoid-Stay Tasks</title>
<link>https://arxiv.org/abs/2510.19232</link>
<guid>https://arxiv.org/abs/2510.19232</guid>
<content:encoded><![CDATA[
arXiv:2510.19232v1 Announce Type: new 
Abstract: The paper focuses on designing a controller for unknown dynamical multi-agent systems to achieve temporal reach-avoid-stay tasks for each agent while preventing inter-agent collisions. The main objective is to generate a spatiotemporal tube (STT) for each agent and thereby devise a closed-form, approximation-free, and decentralized control strategy that ensures the system trajectory reaches the target within a specific time while avoiding time-varying unsafe sets and collisions with other agents. In order to achieve this, the requirements of STTs are formulated as a robust optimization problem (ROP) and solved using a sampling-based scenario optimization problem (SOP) to address the issue of infeasibility caused by the infinite number of constraints in ROP. The STTs are generated by solving the SOP, and the corresponding closed-form control is designed to fulfill the specified task. Finally, the effectiveness of our approach is demonstrated through two case studies, one involving omnidirectional robots and the other involving multiple drones modelled as Euler-Lagrange systems.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trace: Securing Smart Contract Repository Against Access Control Vulnerability</title>
<link>https://arxiv.org/abs/2510.19254</link>
<guid>https://arxiv.org/abs/2510.19254</guid>
<content:encoded><![CDATA[
arXiv:2510.19254v1 Announce Type: new 
Abstract: Smart contract vulnerabilities, particularly improper Access Control that allows unauthorized execution of restricted functions, have caused billions of dollars in losses. GitHub hosts numerous smart contract repositories containing source code, documentation, and configuration files-these serve as intermediate development artifacts that must be compiled and packaged before deployment. Third-party developers often reference, reuse, or fork code from these repositories during custom development. However, if the referenced code contains vulnerabilities, it can introduce significant security risks. Existing tools for detecting smart contract vulnerabilities are limited in their ability to handle complex repositories, as they typically require the target contract to be compilable to generate an abstract representation for further analysis. This paper presents TRACE, a tool designed to secure non-compilable smart contract repositories against access control vulnerabilities. TRACE employs LLMs to locate sensitive functions involving critical operations (e.g., transfer) within the contract and subsequently completes function snippets into a fully compilable contract. TRACE constructs a function call graph from the abstract syntax tree (AST) of the completed contract. It uses the control flow graph (CFG) of each function as node information. The nodes of the sensitive functions are then analyzed to detect Access Control vulnerabilities. Experimental results demonstrate that TRACE outperforms state-of-the-art tools on an open-sourced CVE dataset, detecting 14 out of 15 CVEs. In addition, it achieves 89.2% precision on 5,000 recent on-chain contracts, far exceeding the best existing tool at 76.9%. On 83 real-world repositories, TRACE achieves 87.0% precision, significantly surpassing DeepSeek-R1's 14.3%.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative penetration testing suite for emerging generative AI algorithms</title>
<link>https://arxiv.org/abs/2510.19303</link>
<guid>https://arxiv.org/abs/2510.19303</guid>
<content:encoded><![CDATA[
arXiv:2510.19303v1 Announce Type: new 
Abstract: Problem Space: AI Vulnerabilities and Quantum Threats Generative AI vulnerabilities: model inversion, data poisoning, adversarial inputs. Quantum threats Shor Algorithm breaking RSA ECC encryption. Challenge Secure generative AI models against classical and quantum cyberattacks. Proposed Solution Collaborative Penetration Testing Suite Five Integrated Components: DAST SAST OWASP ZAP, Burp Suite, SonarQube, Fortify. IAST Contrast Assess integrated with CI CD pipeline. Blockchain Logging Hyperledger Fabric for tamper-proof logs. Quantum Cryptography Lattice based RLWE protocols. AI Red Team Simulations Adversarial ML & Quantum-assisted attacks. Integration Layer: Unified workflow for AI, cybersecurity, and quantum experts. Key Results 300+ vulnerabilities identified across test environments. 70% reduction in high-severity issues within 2 weeks. 90% resolution efficiency for blockchain-logged vulnerabilities. Quantum-resistant cryptography maintained 100% integrity in tests. Outcome: Quantum AI Security Protocol integrating Blockchain Quantum Cryptography AI Red Teaming.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SORA-ATMAS: Adaptive Trust Management and Multi-LLM Aligned Governance for Future Smart Cities</title>
<link>https://arxiv.org/abs/2510.19327</link>
<guid>https://arxiv.org/abs/2510.19327</guid>
<content:encoded><![CDATA[
arXiv:2510.19327v1 Announce Type: new 
Abstract: The rapid evolution of smart cities has increased the reliance on intelligent interconnected services to optimize infrastructure, resources, and citizen well-being. Agentic AI has emerged as a key enabler by supporting autonomous decision-making and adaptive coordination, allowing urban systems to respond in real time to dynamic conditions. Its benefits are evident in areas such as transportation, where the integration of traffic data, weather forecasts, and safety sensors enables dynamic rerouting and a faster response to hazards. However, its deployment across heterogeneous smart city ecosystems raises critical governance, risk, and compliance (GRC) challenges, including accountability, data privacy, and regulatory alignment within decentralized infrastructures. Evaluation of SORA-ATMAS with three domain agents (Weather, Traffic, and Safety) demonstrated that its governance policies, including a fallback mechanism for high-risk scenarios, effectively steer multiple LLMs (GPT, Grok, DeepSeek) towards domain-optimized, policy-aligned outputs, producing an average MAE reduction of 35% across agents. Results showed stable weather monitoring, effective handling of high-risk traffic plateaus 0.85, and adaptive trust regulation in Safety/Fire scenarios 0.65. Runtime profiling of a 3-agent deployment confirmed scalability, with throughput between 13.8-17.2 requests per second, execution times below 72~ms, and governance delays under 100 ms, analytical projections suggest maintained performance at larger scales. Cross-domain rules ensured safe interoperability, with traffic rerouting permitted only under validated weather conditions. These findings validate SORA-ATMAS as a regulation-aligned, context-aware, and verifiable governance framework that consolidates distributed agent outputs into accountable, real-time decisions, offering a resilient foundation for smart-city management.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping and Evolving Interoperability Testing in European Energy Systems: The int:net Perspective</title>
<link>https://arxiv.org/abs/2510.19460</link>
<guid>https://arxiv.org/abs/2510.19460</guid>
<content:encoded><![CDATA[
arXiv:2510.19460v1 Announce Type: new 
Abstract: The ongoing transformation of the European energy landscape, driven by the integration of renewable energy sources, digital technologies, and decentralized systems, requires a high degree of interoperability across diverse components and systems. Ensuring that these elements can exchange information and operate together reliably is essential for achieving a secure, flexible, and efficient energy supply infrastructure. While several initiatives have contributed to the development of smart grid testing infrastructures, they do not provide a dedicated or comprehensive focus on interoperability testing. A structured and harmonized overview of interoperability testing capabilities across Europe is therefore still missing. This work therefore presents a novel contribution by analyzing the European interoperability testing facility landscape through a structured survey of 30 facilities. It provides a categorized inventory of testing infrastructures, applied methodologies, and reference test cases, and introduces a blueprint for the development of future testing environments. The findings contribute to the establishment of a coordinated European ecosystem for interoperability testing, supporting collaboration, innovation, and alignment with the goals of the energy transition.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Chain Sealed-Bid Auctions Using Confidential Compute Blockchains</title>
<link>https://arxiv.org/abs/2510.19491</link>
<guid>https://arxiv.org/abs/2510.19491</guid>
<content:encoded><![CDATA[
arXiv:2510.19491v1 Announce Type: new 
Abstract: Sealed-bid auctions ensure fair competition and efficient allocation but are often deployed on centralized infrastructure, enabling opaque manipulation. Public blockchains eliminate central control, yet their inherent transparency conflicts with the confidentiality required for sealed bidding. Prior attempts struggle to reconcile privacy, verifiability, and scalability without relying on trusted intermediaries, multi-round protocols, or expensive cryptography. We present a sealed-bid auction protocol that executes sensitive bidding logic on a Trusted Execution Environment (TEE)-backed confidential compute blockchain while retaining settlement and enforcement on a public chain. Bidders commit funds to enclave-generated escrow addresses, ensuring confidentiality and binding commitments. After the deadline, any party can trigger resolution: the confidential blockchain determines the winner through verifiable off-chain computation and issues signed settlement transactions for execution on the public chain. Our design provides security, privacy, and scalability without trusted third parties or protocol modifications. We implement it on SUAVE with Ethereum settlement, evaluate its scalability and trust assumptions, and demonstrate deployment with minimal integration on existing infrastructure
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Scaling LT-Coded Blockchains in Heterogeneous Networks and their Vulnerabilities to DoS Threats</title>
<link>https://arxiv.org/abs/2402.05620</link>
<guid>https://arxiv.org/abs/2402.05620</guid>
<content:encoded><![CDATA[
arXiv:2402.05620v3 Announce Type: replace 
Abstract: Coded blockchains have acquired prominence as a promising solution to reduce storage costs and facilitate scalability. Within this class, Luby Transform (LT) coded blockchains are an appealing choice for scalability owing to the availability of a wide range of low-complexity decoders. In the first part of this work, we identify that traditional LT decoders like Belief Propagation and On-the-Fly Gaussian Elimination may not be optimal for heterogeneous networks with nodes that have varying computational and download capabilities. To address this, we introduce a family of hybrid decoders for LT codes and propose optimal operating regimes for them to recover the blockchain at the lowest decoding cost. While LT coded blockchain architecture has been studied from the aspects of storage savings and scalability, not much is known in terms of its security vulnerabilities. Pointing at this research gap, in the second part, we present novel denial-of-service threats on LT coded blockchains that target nodes with specific decoding capabilities, preventing them from joining the network. Our proposed threats are non-oblivious in nature, wherein adversaries gain access to the archived blocks, and choose to execute their attack on a subset of them based on underlying coding scheme. We show that our optimized threats can achieve the same level of damage as that of blind attacks, however, with limited amount of resources. Overall, this is the first work of its kind that opens up new questions on designing coded blockchains to jointly provide storage savings, scalability and also resilience to optimized threats.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn More by Using Less: Distributed Learning with Energy-Constrained Devices</title>
<link>https://arxiv.org/abs/2412.02289</link>
<guid>https://arxiv.org/abs/2412.02289</guid>
<content:encoded><![CDATA[
arXiv:2412.02289v2 Announce Type: replace 
Abstract: Federated Learning (FL) has emerged as a solution for distributed model training across decentralized, privacy-preserving devices, but the different energy capacities of participating devices (system heterogeneity) constrain real-world implementations. These energy limitations not only reduce model accuracy but also increase dropout rates, impacting on convergence in practical FL deployments. In this work, we propose LeanFed, an energy-aware FL framework designed to optimize client selection and training workloads on battery-constrained devices. LeanFed leverages adaptive data usage by dynamically adjusting the fraction of local data each device utilizes during training, thereby maximizing device participation across communication rounds while ensuring they do not run out of battery during the process. We rigorously evaluate LeanFed against traditional FedAvg on CIFAR-10 and CIFAR-100 datasets, simulating various levels of data heterogeneity and device participation rates. Results show that LeanFed consistently enhances model accuracy and stability, particularly in settings with high data heterogeneity and limited battery life, by mitigating client dropout and extending device availability. This approach demonstrates the potential of energy-efficient, privacy-preserving FL in real-world, large-scale applications, setting a foundation for robust and sustainable pervasive AI on resource-constrained networks.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Becoming Immutable: How Ethereum is Made</title>
<link>https://arxiv.org/abs/2506.04940</link>
<guid>https://arxiv.org/abs/2506.04940</guid>
<content:encoded><![CDATA[
arXiv:2506.04940v2 Announce Type: replace-cross 
Abstract: We collect and study 15,097 blocks proposed for inclusion in the Ethereum blockchain during 8 minutes on December 3rd, 2024, corresponding to 39 added blocks. These proposed blocks contain 10,793 unique transactions, and 2,380,014 transaction-block pairings, our primary unit of analysis. We find that 20% of user transactions are delayed: although proposed during a bidding cycle, they are not included in the corresponding winning block. Approximately 30% of such delayed transactions are exclusive to a losing builder. We also identify two arbitrage bots trading between decentralized (DEX) and centralized exchanges (CEX), which are responsible for a significant fraction of the value of proposed blocks. By examining their bidding dynamics, we estimate that the implied price at which these bots trade USDC/WETH and USDT/WETH on CEXes is approximately 2.8 basis points better than the contemporaneous Binance price.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RiskTagger: An LLM-based Agent for Automatic Annotation of Web3 Crypto Money Laundering Behaviors</title>
<link>https://arxiv.org/abs/2510.17848</link>
<guid>https://arxiv.org/abs/2510.17848</guid>
<content:encoded><![CDATA[
arXiv:2510.17848v1 Announce Type: new 
Abstract: While the rapid growth of Web3 has driven the development of decentralized finance, user anonymity and cross-chain asset flows make on-chain laundering behaviors more covert and complex. In this context, constructing high-quality anti-money laundering(AML) datasets has become essential for risk-control systems and on-chain forensic analysis, yet current practices still rely heavily on manual efforts with limited efficiency and coverage. In this paper, we introduce RiskTagger, a large-language-model-based agent for the automatic annotation of crypto laundering behaviors in Web3. RiskTagger is designed to replace or complement human annotators by addressing three key challenges: extracting clues from complex unstructured reports, reasoning over multichain transaction paths, and producing auditor-friendly explanations. RiskTagger implements an end-to-end multi-module agent, integrating a key-clue extractor, a multichain fetcher with a laundering-behavior reasoner, and a data explainer, forming a data annotation pipeline. Experiments on the real case Bybit Hack (with the highest stolen asset value) demonstrate that RiskTagger achieves 100% accuracy in clue extraction, 84.1% consistency with expert judgment, and 90% coverage in explanation generation. Overall, RiskTagger automates laundering behavior annotation while improving transparency and scalability in AML research.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smart Contracts Formal Verification: A Systematic Literature Review</title>
<link>https://arxiv.org/abs/2510.17865</link>
<guid>https://arxiv.org/abs/2510.17865</guid>
<content:encoded><![CDATA[
arXiv:2510.17865v1 Announce Type: new 
Abstract: Formal verification entails testing software to ensure it operates as specified. Smart contracts are self-executing contracts with the terms of the agreement directly written into lines of code. They run on blockchain platforms and automatically enforce and execute the terms of an agreement when meeting predefined conditions. However, Smart Contracts, as software models, often contain notable errors in their operation or specifications. This observation prompts us to conduct a focused study examining related works published across various sources. These publications detail specifications, verification tools, and relevant experiments. Subsequently, this survey proposes an alternative formal verification based on description logic.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Federated Unlearning for Large Language Models</title>
<link>https://arxiv.org/abs/2510.17895</link>
<guid>https://arxiv.org/abs/2510.17895</guid>
<content:encoded><![CDATA[
arXiv:2510.17895v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly integrated into real-world applications, raising concerns about privacy, security and the need to remove undesirable knowledge. Machine Unlearning has emerged as a promising solution, yet faces two key challenges: (1) practical unlearning needs are often continuous and heterogeneous, and (2) they involve decentralized, sensitive data with asymmetric access. These factors result in inter-domain and intra-domain interference, which further amplifies the dilemma of unbalanced forgetting and retaining performance. In response, we propose a federated unlearning approach for LLMs that is scalable and privacy preserving. Our method decouples unlearning and retention via task-specific adapter learning and employs a hierarchical merging strategy to mitigate conflicting objectives and enables robust, adaptable unlearning updates. Comprehensive experiments on benchmarks of WMDP, MUSE, and TOFU showed that our approach effectively handles heterogeneous unlearning requests while maintaining strong LLM utility compared with baseline methods.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Sherpa.ai Blind Vertical Federated Learning Paradigm to Minimize the Number of Communications</title>
<link>https://arxiv.org/abs/2510.17901</link>
<guid>https://arxiv.org/abs/2510.17901</guid>
<content:encoded><![CDATA[
arXiv:2510.17901v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative decentralized training across multiple parties (nodes) while keeping raw data private. There are two main paradigms in FL: Horizontal FL (HFL), where all participant nodes share the same feature space but hold different samples, and Vertical FL (VFL), where participants hold complementary features for the same samples. While HFL is widely adopted, VFL is employed in domains where nodes hold complementary features about the same samples. Still, VFL presents a significant limitation: the vast number of communications required during training. This compromises privacy and security, and can lead to high energy consumption, and in some cases, make model training unfeasible due to the high number of communications.
  In this paper, we introduce Sherpa.ai Blind Vertical Federated Learning (SBVFL), a novel paradigm that leverages a distributed training mechanism enhanced for privacy and security. Decoupling the vast majority of node updates from the server dramatically reduces node-server communication. Experiments show that SBVFL reduces communication by ~99% compared to standard VFL while maintaining accuracy and robustness. Therefore, SBVFL enables practical, privacy-preserving VFL across sensitive domains, including healthcare, finance, manufacturing, aerospace, cybersecurity, and the defense industry.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParaVul: A Parallel Large Language Model and Retrieval-Augmented Framework for Smart Contract Vulnerability Detection</title>
<link>https://arxiv.org/abs/2510.17919</link>
<guid>https://arxiv.org/abs/2510.17919</guid>
<content:encoded><![CDATA[
arXiv:2510.17919v1 Announce Type: new 
Abstract: Smart contracts play a significant role in automating blockchain services. Nevertheless, vulnerabilities in smart contracts pose serious threats to blockchain security. Currently, traditional detection methods primarily rely on static analysis and formal verification, which can result in high false-positive rates and poor scalability. Large Language Models (LLMs) have recently made significant progress in smart contract vulnerability detection. However, they still face challenges such as high inference costs and substantial computational overhead. In this paper, we propose ParaVul, a parallel LLM and retrieval-augmented framework to improve the reliability and accuracy of smart contract vulnerability detection. Specifically, we first develop Sparse Low-Rank Adaptation (SLoRA) for LLM fine-tuning. SLoRA introduces sparsification by incorporating a sparse matrix into quantized LoRA-based LLMs, thereby reducing computational overhead and resource requirements while enhancing their ability to understand vulnerability-related issues. We then construct a vulnerability contract dataset and develop a hybrid Retrieval-Augmented Generation (RAG) system that integrates dense retrieval with Best Matching 25 (BM25), assisting in verifying the results generated by the LLM. Furthermore, we propose a meta-learning model to fuse the outputs of the RAG system and the LLM, thereby generating the final detection results. After completing vulnerability detection, we design chain-of-thought prompts to guide LLMs to generate comprehensive vulnerability detection reports. Simulation results demonstrate the superiority of ParaVul, especially in terms of F1 scores, achieving 0.9398 for single-label detection and 0.9330 for multi-label detection.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPACeR: Self-Play Anchoring with Centralized Reference Models</title>
<link>https://arxiv.org/abs/2510.18060</link>
<guid>https://arxiv.org/abs/2510.18060</guid>
<content:encoded><![CDATA[
arXiv:2510.18060v1 Announce Type: new 
Abstract: Developing autonomous vehicles (AVs) requires not only safety and efficiency, but also realistic, human-like behaviors that are socially aware and predictable. Achieving this requires sim agent policies that are human-like, fast, and scalable in multi-agent settings. Recent progress in imitation learning with large diffusion-based or tokenized models has shown that behaviors can be captured directly from human driving data, producing realistic policies. However, these models are computationally expensive, slow during inference, and struggle to adapt in reactive, closed-loop scenarios. In contrast, self-play reinforcement learning (RL) scales efficiently and naturally captures multi-agent interactions, but it often relies on heuristics and reward shaping, and the resulting policies can diverge from human norms. We propose SPACeR, a framework that leverages a pretrained tokenized autoregressive motion model as a centralized reference policy to guide decentralized self-play. The reference model provides likelihood rewards and KL divergence, anchoring policies to the human driving distribution while preserving RL scalability. Evaluated on the Waymo Sim Agents Challenge, our method achieves competitive performance with imitation-learned policies while being up to 10x faster at inference and 50x smaller in parameter size than large generative models. In addition, we demonstrate in closed-loop ego planning evaluation tasks that our sim agents can effectively measure planner quality with fast and scalable traffic simulation, establishing a new paradigm for testing autonomous driving policies.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PrivaDE: Privacy-preserving Data Evaluation for Blockchain-based Data Marketplaces</title>
<link>https://arxiv.org/abs/2510.18109</link>
<guid>https://arxiv.org/abs/2510.18109</guid>
<content:encoded><![CDATA[
arXiv:2510.18109v1 Announce Type: new 
Abstract: Evaluating the relevance of data is a critical task for model builders seeking to acquire datasets that enhance model performance. Ideally, such evaluation should allow the model builder to assess the utility of candidate data without exposing proprietary details of the model. At the same time, data providers must be assured that no information about their data - beyond the computed utility score - is disclosed to the model builder.
  In this paper, we present PrivaDE, a cryptographic protocol for privacy-preserving utility scoring and selection of data for machine learning. While prior works have proposed data evaluation protocols, our approach advances the state of the art through a practical, blockchain-centric design. Leveraging the trustless nature of blockchains, PrivaDE enforces malicious-security guarantees and ensures strong privacy protection for both models and datasets. To achieve efficiency, we integrate several techniques - including model distillation, model splitting, and cut-and-choose zero-knowledge proofs - bringing the runtime to a practical level. Furthermore, we propose a unified utility scoring function that combines empirical loss, predictive entropy, and feature-space diversity, and that can be seamlessly integrated into active-learning workflows. Evaluation shows that PrivaDE performs data evaluation effectively, achieving online runtimes within 15 minutes even for models with millions of parameters.
  Our work lays the foundation for fair and automated data marketplaces in decentralized machine learning ecosystems.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TaintSentinel: Path-Level Randomness Vulnerability Detection for Ethereum Smart Contracts</title>
<link>https://arxiv.org/abs/2510.18192</link>
<guid>https://arxiv.org/abs/2510.18192</guid>
<content:encoded><![CDATA[
arXiv:2510.18192v1 Announce Type: new 
Abstract: The inherent determinism of blockchain technology poses a significant challenge to generating secure random numbers within smart contracts, leading to exploitable vulnerabilities, particularly in decentralized finance (DeFi) ecosystems and blockchain-based gaming applications. From our observations, the current state-of-the-art detection tools suffer from inadequate precision while dealing with random number vulnerabilities. To address this problem, we propose TaintSentinel, a novel path sensitive vulnerability detection system designed to analyze smart contracts at the execution path level and gradually analyze taint with domain-specific rules. This paper discusses a solution that incorporates a multi-faceted approach, integrating rule-based taint analysis to track data flow, a dual stream neural network to identify complex vulnerability signatures, and evidence-based parameter initialization to minimize false positives. The system's two-phase operation involves semantic graph construction and taint propagation analysis, followed by pattern recognition using PathGNN and global structural analysis via GlobalGCN. Our experiments on 4,844 contracts demonstrate the superior performance of TaintSentinel relative to existing tools, yielding an F1-score of 0.892, an AUC-ROC of 0.94, and a PRA accuracy of 97%.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Optimization of Cooperation Efficiency and Communication Covertness for Target Detection with AUVs</title>
<link>https://arxiv.org/abs/2510.18225</link>
<guid>https://arxiv.org/abs/2510.18225</guid>
<content:encoded><![CDATA[
arXiv:2510.18225v1 Announce Type: new 
Abstract: This paper investigates underwater cooperative target detection using autonomous underwater vehicles (AUVs), with a focus on the critical trade-off between cooperation efficiency and communication covertness. To tackle this challenge, we first formulate a joint trajectory and power control optimization problem, and then present an innovative hierarchical action management framework to solve it. According to the hierarchical formulation, at the macro level, the master AUV models the agent selection process as a Markov decision process and deploys the proximal policy optimization algorithm for strategic task allocation. At the micro level, each selected agent's decentralized decision-making is modeled as a partially observable Markov decision process, and a multi-agent proximal policy optimization algorithm is used to dynamically adjust its trajectory and transmission power based on its local observations. Under the centralized training and decentralized execution paradigm, our target detection framework enables adaptive covert cooperation while satisfying both energy and mobility constraints. By comprehensively modeling the considered system, the involved signals and tasks, as well as energy consumption, theoretical insights and practical solutions for the efficient and secure operation of multiple AUVs are provided, offering significant implications for the execution of underwater covert communication tasks.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Healthcare Data in IoT: A Synergistic Approach with Deep Learning and Blockchain</title>
<link>https://arxiv.org/abs/2510.18568</link>
<guid>https://arxiv.org/abs/2510.18568</guid>
<content:encoded><![CDATA[
arXiv:2510.18568v1 Announce Type: new 
Abstract: The integration of Internet of Things (IoT) devices in healthcare has revolutionized patient care by enabling real-time monitoring, personalized treatments, and efficient data management. However, this technological advancement introduces significant security risks, particularly concerning the confidentiality, integrity, and availability of sensitive medical data. Traditional security measures are often insufficient to address the unique challenges posed by IoT environments, such as heterogeneity, resource constraints, and the need for real-time processing. To tackle these challenges, we propose a comprehensive three-phase security framework designed to enhance the security and reliability of IoT-enabled healthcare systems. In the first phase, the framework assesses the reliability of IoT devices using a reputation-based trust estimation mechanism, which combines device behavior analytics with off-chain data storage to ensure scalability. The second phase integrates blockchain technology with a lightweight proof-of-work mechanism, ensuring data immutability, secure communication, and resistance to unauthorized access. The third phase employs a lightweight Long Short-Term Memory (LSTM) model for anomaly detection and classification, enabling real-time identification of cyber threats. Simulation results demonstrate that the proposed framework outperforms existing methods, achieving a 2% increase in precision, accuracy, and recall, a 5% higher attack detection rate, and a 3% reduction in false alarm rate. These improvements highlight the framework's ability to address critical security concerns while maintaining scalability and real-time performance.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fetch.ai: An Architecture for Modern Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2510.18699</link>
<guid>https://arxiv.org/abs/2510.18699</guid>
<content:encoded><![CDATA[
arXiv:2510.18699v1 Announce Type: new 
Abstract: Recent surges in LLM-driven intelligent systems largely overlook decades of foundational multi-agent systems (MAS) research, resulting in frameworks with critical limitations such as centralization and inadequate trust and communication protocols. This paper introduces the Fetch.ai architecture, an industrial-strength platform designed to bridge this gap by facilitating the integration of classical MAS principles with modern AI capabilities. We present a novel, multi-layered solution built on a decentralized foundation of on-chain blockchain services for verifiable identity, discovery, and transactions. This is complemented by a comprehensive development framework for creating secure, interoperable agents, a cloud-based platform for deployment, and an intelligent orchestration layer where an agent-native LLM translates high-level human goals into complex, multi-agent workflows. We demonstrate the deployed nature of this system through a decentralized logistics use case where autonomous agents dynamically discover, negotiate, and transact with one another securely. Ultimately, the Fetch.ai stack provides a principled architecture for moving beyond current agent implementations towards open, collaborative, and economically sustainable multi-agent ecosystems.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transaction Capacity, Security and Latency in Blockchains</title>
<link>https://arxiv.org/abs/2402.10138</link>
<guid>https://arxiv.org/abs/2402.10138</guid>
<content:encoded><![CDATA[
arXiv:2402.10138v2 Announce Type: replace 
Abstract: We analyze how secure a block is after the block becomes $k$-deep, i.e., security-latency, for Nakamoto consensus under an exponential network delay model. We provide the fault tolerance and extensive bounds on safety violation probabilities given mining rate, delay rate and confirmation rules. Next, modeling the blockchain system as a batch service queue with exponential network delay, we connect the security-latency analysis to sustainable transaction rate of the queue system. As our model assumes exponential network delay, batch service queue models give a meaningful trade-off between transaction capacity, security and latency. Our results indicate that, by simply picking $k=7$-block confirmation rule in Bitcoin instead of the convention of $k=6$, mining rate, latency and throughput can be increased sixfold with the same safety guarantees. We further consider adversarial attacks on the queue service to hamper the service process. In an extreme scenario, we consider the selfish-mining attack for this purpose and provide the maximum adversarial block ratio in the longest chain under the exponential delay model. The ratio in turn reflects the maximum rate of decrease in the sustainable transaction rate of the queue.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BlockScan: Detecting Anomalies in Blockchain Transactions</title>
<link>https://arxiv.org/abs/2410.04039</link>
<guid>https://arxiv.org/abs/2410.04039</guid>
<content:encoded><![CDATA[
arXiv:2410.04039v5 Announce Type: replace 
Abstract: We propose BlockScan, a customized Transformer for anomaly detection in blockchain transactions. Unlike existing methods that rely on rule-based systems or directly apply off-the-shelf large language models (LLMs), BlockScan introduces a series of customized designs to effectively model the unique data structure of blockchain transactions. First, a blockchain transaction is multi-modal, containing blockchain-specific tokens, texts, and numbers. We design a novel modularized tokenizer to handle these multi-modal inputs, balancing the information across different modalities. Second, we design a customized masked language modeling mechanism for pretraining the Transformer architecture, incorporating RoPE embedding and FlashAttention for handling longer sequences. Finally, we design a novel anomaly detection method based on the model outputs. We further provide theoretical analysis for the detection method of our system. Extensive evaluations on Ethereum and Solana transactions demonstrate BlockScan's exceptional capability in anomaly detection while maintaining a low false positive rate. Remarkably, BlockScan is the only method that successfully detects anomalous transactions on Solana with high accuracy, whereas all other approaches achieved very low or zero detection recall scores. This work sets a new benchmark for applying Transformer-based approaches in blockchain data analysis.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asynchronous Federated Learning: A Scalable Approach for Decentralized Machine Learning</title>
<link>https://arxiv.org/abs/2412.17723</link>
<guid>https://arxiv.org/abs/2412.17723</guid>
<content:encoded><![CDATA[
arXiv:2412.17723v4 Announce Type: replace 
Abstract: Federated Learning (FL) has emerged as a powerful paradigm for decentralized machine learning, enabling collaborative model training across diverse clients without sharing raw data. However, traditional FL approaches often face limitations in scalability and efficiency due to their reliance on synchronous client updates, which can result in significant delays and increased communication overhead, particularly in heterogeneous and dynamic environments. To address these challenges in this paper, we propose an Asynchronous Federated Learning (AFL) algorithm, which allows clients to update the global model independently and asynchronously. Our key contributions include a comprehensive convergence analysis of AFL in the presence of client delays and model staleness. By leveraging martingale difference sequence theory and variance bounds, we ensure robust convergence despite asynchronous updates. Assuming strongly convex local objective functions, we establish bounds on gradient variance under random client sampling and derive a recursion formula quantifying the impact of client delays on convergence. Furthermore, we demonstrate the practical applicability of the AFL algorithm by training decentralized linear regression and Support Vector Machine (SVM) based classifiers and compare its results with synchronous FL algorithm to effectively handling non-IID data distributed among clients. The proposed AFL algorithm addresses key limitations of traditional FL methods, such as inefficiency due to global synchronization and susceptibility to client drift. It enhances scalability, robustness, and efficiency in real-world settings with heterogeneous client populations and dynamic network conditions. Our results underscore the potential of AFL to drive advancements indistributed learning systems, particularly for large-scale, privacy-preserving applications in resource-constrained environments.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wonder Wins Ways: Curiosity-Driven Exploration through Multi-Agent Contextual Calibration</title>
<link>https://arxiv.org/abs/2509.20648</link>
<guid>https://arxiv.org/abs/2509.20648</guid>
<content:encoded><![CDATA[
arXiv:2509.20648v2 Announce Type: replace 
Abstract: Autonomous exploration in complex multi-agent reinforcement learning (MARL) with sparse rewards critically depends on providing agents with effective intrinsic motivation. While artificial curiosity offers a powerful self-supervised signal, it often confuses environmental stochasticity with meaningful novelty. Moreover, existing curiosity mechanisms exhibit a uniform novelty bias, treating all unexpected observations equally. However, peer behavior novelty, which encode latent task dynamics, are often overlooked, resulting in suboptimal exploration in decentralized, communication-free MARL settings. To this end, inspired by how human children adaptively calibrate their own exploratory behaviors via observing peers, we propose a novel approach to enhance multi-agent exploration. We introduce CERMIC, a principled framework that empowers agents to robustly filter noisy surprise signals and guide exploration by dynamically calibrating their intrinsic curiosity with inferred multi-agent context. Additionally, CERMIC generates theoretically-grounded intrinsic rewards, encouraging agents to explore state transitions with high information gain. We evaluate CERMIC on benchmark suites including VMAS, Meltingpot, and SMACv2. Empirical results demonstrate that exploration with CERMIC significantly outperforms SoTA algorithms in sparse-reward environments.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Prompt Refinement with Non-Interfering Attention Masking for One-Shot Federated Learning</title>
<link>https://arxiv.org/abs/2509.22700</link>
<guid>https://arxiv.org/abs/2509.22700</guid>
<content:encoded><![CDATA[
arXiv:2509.22700v2 Announce Type: replace 
Abstract: Federated Prompt Learning (FPL) enables communication-efficient adaptation by tuning lightweight prompts on top of frozen pre-trained models. Existing FPL methods typically rely on global information, which is only available after the second training round, to facilitate collaboration among client models. Therefore, they are inherently dependent on multi-round communication to fully exhibit their strengths. Moreover, existing one-shot federated learning methods typically focus on fitting seen tasks, but lack cross-task generalization. To bridge this gap, we propose the Global Prompt Refinement with Non-Interfering Attention Masking (GPR-NIAM) method for one-shot FPL. The core idea is to design a masking mechanism that restricts excessive interaction between the original text embeddings and the learnable prompt embeddings. GPR-NIAM achieves this through the collaboration of two key modules. Firstly, the attention isolation module suppresses attention from the learnable prompt tokens to the original text tokens, and reweights the reverse attention which preserves generalization across tasks. Secondly, the cross-silo collaborative refinement module integrates decentralized visual knowledge into a unified base and calibrates the global prompt through multi-source cross-modal knowledge alignment, further mitigating the inconsistency caused by data heterogeneity. Extensive experiments conducted on ten benchmark datasets under two tasks show that GPR-NIAM outperforms eight state-of-the-art methods in both class-level and domain-level generalization.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoCks - Simplifying Firmware and Software Integration for Heterogeneous SoCs</title>
<link>https://arxiv.org/abs/2510.15910</link>
<guid>https://arxiv.org/abs/2510.15910</guid>
<content:encoded><![CDATA[
arXiv:2510.15910v1 Announce Type: new 
Abstract: Modern heterogeneous System-on-Chip (SoC) devices integrate advanced components into a single package, offering powerful capabilities while also introducing significant complexity. To manage these sophisticated devices, firmware and software developers need powerful development tools. However, as these tools become increasingly complex, they often lack adequate support, resulting in a steep learning curve and challenging troubleshooting. To address this, this work introduces System-on-Chip blocks (SoCks), a flexible and expandable build framework that reduces complexity by partitioning the SoC image into high-level units called blocks. SoCks builds each firmware and software block in an encapsulated way, independently from other components of the image, thereby reducing dependencies to a minimum. While some information exchange between the blocks is unavoidable to ensure seamless runtime integration, this interaction is standardized via interfaces. A small number of dependencies and well-defined interfaces simplify the reuse of existing block implementations and facilitate seamless substitution between versions-for instance, when choosing root file systems for the embedded Linux operating system. Additionally, this approach facilitates the establishment of a decentralized and partially automated development flow through Continuous Integration and Continuous Delivery (CI/CD). Measurement results demonstrate that SoCks can build a complete SoC image up to three times faster than established tools.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On-Chain Decentralized Learning and Cost-Effective Inference for DeFi Attack Mitigation</title>
<link>https://arxiv.org/abs/2510.16024</link>
<guid>https://arxiv.org/abs/2510.16024</guid>
<content:encoded><![CDATA[
arXiv:2510.16024v1 Announce Type: new 
Abstract: Billions of dollars are lost every year in DeFi platforms by transactions exploiting business logic or accounting vulnerabilities. Existing defenses focus on static code analysis, public mempool screening, attacker contract detection, or trusted off-chain monitors, none of which prevents exploits submitted through private relays or malicious contracts that execute within the same block. We present the first decentralized, fully on-chain learning framework that: (i) performs gas-prohibitive computation on Layer-2 to reduce cost, (ii) propagates verified model updates to Layer-1, and (iii) enables gas-bounded, low-latency inference inside smart contracts. A novel Proof-of-Improvement (PoIm) protocol governs the training process and verifies each decentralized micro update as a self-verifying training transaction. Updates are accepted by \textit{PoIm} only if they demonstrably improve at least one core metric (e.g., accuracy, F1-score, precision, or recall) on a public benchmark without degrading any of the other core metrics, while adversarial proposals get financially penalized through an adaptable test set for evolving threats. We develop quantization and loop-unrolling techniques that enable inference for logistic regression, SVM, MLPs, CNNs, and gated RNNs (with support for formally verified decision tree inference) within the Ethereum block gas limit, while remaining bit-exact to their off-chain counterparts, formally proven in Z3. We curate 298 unique real-world exploits (2020 - 2025) with 402 exploit transactions across eight EVM chains, collectively responsible for \$3.74 B in losses.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nondeterminism-Aware Optimistic Verification for Floating-Point Neural Networks</title>
<link>https://arxiv.org/abs/2510.16028</link>
<guid>https://arxiv.org/abs/2510.16028</guid>
<content:encoded><![CDATA[
arXiv:2510.16028v1 Announce Type: new 
Abstract: Neural networks increasingly run on hardware outside the user's control (cloud GPUs, inference marketplaces). Yet ML-as-a-Service reveals little about what actually ran or whether returned outputs faithfully reflect the intended inputs. Users lack recourse against service downgrades (model swaps, quantization, graph rewrites, or discrepancies like altered ad embeddings). Verifying outputs is hard because floating-point(FP) execution on heterogeneous accelerators is inherently nondeterministic. Existing approaches are either impractical for real FP neural networks or reintroduce vendor trust. We present NAO: a Nondeterministic tolerance Aware Optimistic verification protocol that accepts outputs within principled operator-level acceptance regions rather than requiring bitwise equality. NAO combines two error models: (i) sound per-operator IEEE-754 worst-case bounds and (ii) tight empirical percentile profiles calibrated across hardware. Discrepancies trigger a Merkle-anchored, threshold-guided dispute game that recursively partitions the computation graph until one operator remains, where adjudication reduces to a lightweight theoretical-bound check or a small honest-majority vote against empirical thresholds. Unchallenged results finalize after a challenge window, without requiring trusted hardware or deterministic kernels. We implement NAO as a PyTorch-compatible runtime and a contract layer currently deployed on Ethereum Holesky testnet. The runtime instruments graphs, computes per-operator bounds, and runs unmodified vendor kernels in FP32 with negligible overhead (0.3% on Qwen3-8B). Across CNNs, Transformers and diffusion models on A100, H100, RTX6000, RTX4090, empirical thresholds are $10^2-10^3$ times tighter than theoretical bounds, and bound-aware adversarial attacks achieve 0% success. NAO reconciles scalability with verifiability for real-world heterogeneous ML compute.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework For Decentralized Micro-credential Verification Towards Higher Qualifications</title>
<link>https://arxiv.org/abs/2510.16050</link>
<guid>https://arxiv.org/abs/2510.16050</guid>
<content:encoded><![CDATA[
arXiv:2510.16050v1 Announce Type: new 
Abstract: Student retention is one of the rising problems seen in educational institutions. With the rising cost of education and issues in the education sector, such as curriculum relevance, student engagement, and rapidly changing technological advancements, ensuring the relevance of academic programs in a fast-evolving job market has created a significant concern for educational institutions. With the intent to adapt to such challenges, educational institutions are dealing with alternative solutions for education, in which micro-credentials are at the very center of this, which are short-term academic programs or standalone courses. However, one of the challenges of micro-credentials is a lack of credit transfer among institutions. With the lack of standardization of assessments among educational institutions, it is difficult to transfer micro-credentials to larger qualifications. Regarding such challenges, micro-credentials with blockchain technology can bring significant benefits. Blockchain technology offers a decentralized and immutable platform for securely storing and verifying credentials. This paper presents a prototype model for micro-credential verification. With the policies decided by the educational institution, the learner provides a micro-credential certificate to the system. Upon validation of the certificate by the verifying body, the educational institution will review the assessment criteria and provide exemptions based on the provided criteria. The prototype uses the Hyper-ledger Fabric platform and utilizes off-chain technology, which acts as a middle-man storage platform. With the combination of off-chain and on-chain technologies, congestion on the blockchain is reduced, and transaction speed is improved. In summary, this research proposes a prototype for secure micro-credential verification and a more efficient course exemption process.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Blockchain-Based CI/CD Framework to Enhance Security in Cloud Environments</title>
<link>https://arxiv.org/abs/2510.16087</link>
<guid>https://arxiv.org/abs/2510.16087</guid>
<content:encoded><![CDATA[
arXiv:2510.16087v1 Announce Type: new 
Abstract: Security is becoming a pivotal point in cloud platforms. Several divisions, such as business organisations, health care, government, etc., have experienced cyber-attacks on their infrastructures. This research focuses on security issues within Continuous Integration and Deployment (CI/CD) pipelines in a cloud platform as a reaction to recent cyber breaches. This research proposes a blockchain-based solution to enhance CI/CD pipeline security. This research aims to develop a framework that leverages blockchain's distributed ledger technology and tamper-resistant features to improve CI/CD pipeline security. The goal is to emphasise secure software deployment by integrating threat modelling frameworks and adherence to coding standards. It also aims to employ tools to automate security testing to detect publicly disclosed vulnerabilities and flaws, such as an outdated version of Java Spring Framework, a JavaScript library from an unverified source, or a database library that allows SQL injection attacks in the deployed software through the framework.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SentinelNet: Safeguarding Multi-Agent Collaboration Through Credit-Based Dynamic Threat Detection</title>
<link>https://arxiv.org/abs/2510.16219</link>
<guid>https://arxiv.org/abs/2510.16219</guid>
<content:encoded><![CDATA[
arXiv:2510.16219v1 Announce Type: new 
Abstract: Malicious agents pose significant threats to the reliability and decision-making capabilities of Multi-Agent Systems (MAS) powered by Large Language Models (LLMs). Existing defenses often fall short due to reactive designs or centralized architectures which may introduce single points of failure. To address these challenges, we propose SentinelNet, the first decentralized framework for proactively detecting and mitigating malicious behaviors in multi-agent collaboration. SentinelNet equips each agent with a credit-based detector trained via contrastive learning on augmented adversarial debate trajectories, enabling autonomous evaluation of message credibility and dynamic neighbor ranking via bottom-k elimination to suppress malicious communications. To overcome the scarcity of attack data, it generates adversarial trajectories simulating diverse threats, ensuring robust training. Experiments on MAS benchmarks show SentinelNet achieves near-perfect detection of malicious agents, close to 100% within two debate rounds, and recovers 95% of system accuracy from compromised baselines. By exhibiting strong generalizability across domains and attack patterns, SentinelNet establishes a novel paradigm for safeguarding collaborative MAS.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DESTinE Block: Private Blockchain Based Data Storage Framework for Power System</title>
<link>https://arxiv.org/abs/2510.16593</link>
<guid>https://arxiv.org/abs/2510.16593</guid>
<content:encoded><![CDATA[
arXiv:2510.16593v1 Announce Type: new 
Abstract: This paper presents DESTinE Block, a blockchain-based data storage framework designed for power systems and optimized for resource-constrained environments, including grid-edge devices such as single-board computers. The proposed architecture leverages the InterPlanetary File System (IPFS) for storing large files while maintaining secure and traceable metadata on a custom blockchain named DESTinE Block. The metadata, comprising the IPFS Content Identifier (CID), uploader identity, administrator verification, and timestamp; is immutably recorded on-chain to ensure authenticity and integrity. DESTinE Block adopts a dual-blockchain abstraction, where the blockchain remains unaware of the IPFS storage layer to enhance security and limit the exposure of sensitive file data. The consensus mechanism is based on Proof of Authority (PoA), where both an administrator and an uploader with distinct cryptographic key pairs are required to create a block collaboratively. Each block contains verified signatures of both parties and is designed to be computationally efficient, enabling deployment on devices like the Raspberry Pi 5. The framework was tested on both an x86-based device and an ARM64-based Raspberry Pi, demonstrating its potential for secure, decentralized logging and measurement storage in smart grid applications. Moreover, DESTinE Block is compared with a similar framework based on Multichain. The results indicate that DESTinE Block provides a promising solution for tamper-evident data retention in distributed power system infrastructure while maintaining minimal hardware requirements.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Verifiable Fine-Tuning for LLMs: Zero-Knowledge Training Proofs Bound to Data Provenance and Policy</title>
<link>https://arxiv.org/abs/2510.16830</link>
<guid>https://arxiv.org/abs/2510.16830</guid>
<content:encoded><![CDATA[
arXiv:2510.16830v1 Announce Type: new 
Abstract: Large language models are often adapted through parameter efficient fine tuning, but current release practices provide weak assurances about what data were used and how updates were computed. We present Verifiable Fine Tuning, a protocol and system that produces succinct zero knowledge proofs that a released model was obtained from a public initialization under a declared training program and an auditable dataset commitment. The approach combines five elements. First, commitments that bind data sources, preprocessing, licenses, and per epoch quota counters to a manifest. Second, a verifiable sampler that supports public replayable and private index hiding batch selection. Third, update circuits restricted to parameter efficient fine tuning that enforce AdamW style optimizer semantics and proof friendly approximations with explicit error budgets. Fourth, recursive aggregation that folds per step proofs into per epoch and end to end certificates with millisecond verification. Fifth, provenance binding and optional trusted execution property cards that attest code identity and constants. On English and bilingual instruction mixtures, the method maintains utility within tight budgets while achieving practical proof performance. Policy quotas are enforced with zero violations, and private sampling windows show no measurable index leakage. Federated experiments demonstrate that the system composes with probabilistic audits and bandwidth constraints. These results indicate that end to end verifiable fine tuning is feasible today for real parameter efficient pipelines, closing a critical trust gap for regulated and decentralized deployments.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiRAC - Distributed Robot Awareness and Consensus</title>
<link>https://arxiv.org/abs/2510.16850</link>
<guid>https://arxiv.org/abs/2510.16850</guid>
<content:encoded><![CDATA[
arXiv:2510.16850v1 Announce Type: new 
Abstract: DiRAC is a scalable, distributed framework designed to enable efficient task assignment and path planning in very large robotic swarms. It introduces a novel zone-partitioned architecture with dynamically elected leaders and a tick-synchronized consensus protocol that yields strong consistency and deterministic outcomes. For path planning, DiRAC uses a novel algorithm, a force-based decentralized planner for real-time collision resolution. Validated within ROS 2 middleware through preliminary simulation, DiRAC demonstrates architectural scalability and modular efficiency in simulated warehouse environments, laying the groundwork for real-world deployment in large-scale industrial and logistics domains.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Real-Time Planning for Multi-UAV Cooperative Manipulation via Imitation Learning</title>
<link>https://arxiv.org/abs/2510.17143</link>
<guid>https://arxiv.org/abs/2510.17143</guid>
<content:encoded><![CDATA[
arXiv:2510.17143v1 Announce Type: new 
Abstract: Existing approaches for transporting and manipulating cable-suspended loads using multiple UAVs along reference trajectories typically rely on either centralized control architectures or reliable inter-agent communication. In this work, we propose a novel machine learning based method for decentralized kinodynamic planning that operates effectively under partial observability and without inter-agent communication. Our method leverages imitation learning to train a decentralized student policy for each UAV by imitating a centralized kinodynamic motion planner with access to privileged global observations. The student policy generates smooth trajectories using physics-informed neural networks that respect the derivative relationships in motion. During training, the student policies utilize the full trajectory generated by the teacher policy, leading to improved sample efficiency. Moreover, each student policy can be trained in under two hours on a standard laptop. We validate our method in both simulation and real-world environments to follow an agile reference trajectory, demonstrating performance comparable to that of centralized approaches.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of Input-Output Mappings in Coinjoin Transactions with Arbitrary Values</title>
<link>https://arxiv.org/abs/2510.17284</link>
<guid>https://arxiv.org/abs/2510.17284</guid>
<content:encoded><![CDATA[
arXiv:2510.17284v1 Announce Type: new 
Abstract: A coinjoin protocol aims to increase transactional privacy for Bitcoin and Bitcoin-like blockchains via collaborative transactions, by violating assumptions behind common analysis heuristics. Estimating the resulting privacy gain is a crucial yet unsolved problem due to a range of influencing factors and large computational complexity.
  We adapt the BlockSci on-chain analysis software to coinjoin transactions, demonstrating a significant (10-50%) average post-mix anonymity set size decrease for all three major designs with a central coordinator: Whirlpool, Wasabi 1.x, and Wasabi 2.x. The decrease is highest during the first day and negligible after one year from a coinjoin creation.
  Moreover, we design a precise, parallelizable privacy estimation method, which takes into account coinjoin fees, implementation-specific limitations and users' post-mix behavior. We evaluate our method in detail on a set of emulated and real-world Wasabi 2.x coinjoins and extrapolate to its largest real-world coinjoins with hundreds of inputs and outputs. We conclude that despite the users' undesirable post-mix behavior, correctly attributing the coins to their owners is still very difficult, even with our improved analysis algorithm.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Local Combining with Decentralized Decoding for Distributed Massive MIMO</title>
<link>https://arxiv.org/abs/2510.17445</link>
<guid>https://arxiv.org/abs/2510.17445</guid>
<content:encoded><![CDATA[
arXiv:2510.17445v1 Announce Type: new 
Abstract: A major bottleneck in uplink distributed massive multiple-input multiple-output networks is the sub-optimal performance of local combining schemes, coupled with high fronthaul load and computational cost inherent in centralized large scale fading decoding (LSFD) architectures. This paper introduces a decentralized decoding architecture that fundamentally breaks from the conventional LSFD, by allowing each AP calculates interference-suppressing local weights independently and applies them to its data estimates before transmission. Furthermore, two generalized local zero-forcing (ZF) framework, generalized partial full-pilot ZF (G-PFZF) and generalized protected weak PFZF (G-PWPFZF), are introduced, where each access point (AP) adaptively and independently determines its combining strategy through a local sum spectral efficiency optimization that classifies user equipments (UEs) as strong or weak using only local information, eliminating the fixed thresholds used in PFZF and PWPFZF. To further enhance scalability, pilot-dependent combining vectors instead of user-dependent ones are introduced and are shared among users with the same pilot. The corresponding closed-form spectral efficiency expressions are derived. Numerical results show that the proposed generalized schemes consistently outperform fixed-threshold counterparts, while the introduction of local weights yields lower overhead and computation costs with minimal performance penalty compared to them.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-driven Communication and Control Design for Distributed Frequency Regulation with Black-box Inverters</title>
<link>https://arxiv.org/abs/2510.17769</link>
<guid>https://arxiv.org/abs/2510.17769</guid>
<content:encoded><![CDATA[
arXiv:2510.17769v1 Announce Type: new 
Abstract: The increasing penetration of inverter-based resources into the power grid, with often only black-box models available, challenges long-standing frequency control methods. Most recent works take a decentralized approach without online device coordination via communication. This paper considers both dynamic behavior and communication within secondary frequency control on an intermediate timescale. We develop a distributed data-driven approach that utilizes peer-to-peer communication between inverters to avoid the need for a central control center. To enable a trade off between communication network requirements and control performance, we present a framework to guide communication topology design for secondary frequency regulation. Following design of the inter-agent information exchange scheme, we design a controller that is structured according to the communication topology with a closed-loop stability guarantee. Case studies on the IEEE 39-bus system validate the framework and illustrate the trade-off between communication requirements and control performance that is enabled by our approach.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Sensing Performance Design for Enhancing Secure Communication in Networked ISAC Systems</title>
<link>https://arxiv.org/abs/2510.16397</link>
<guid>https://arxiv.org/abs/2510.16397</guid>
<content:encoded><![CDATA[
arXiv:2510.16397v1 Announce Type: cross 
Abstract: The channel state information (CSI) of an eavesdropper is crucial for physical layer security (PLS) design, but it is difficult to obtain due to the passive and non-cooperative nature of the eavesdropper. To this end, integrated sensing and communication (ISAC) offers a novel solution by estimating the CSI of the eavesdropper based on sensing information. However, existing studies normally impose explicit and fixed sensing performance requirement without considering the varying communication conditions, which hinders the system from fully exploiting the synergy between sensing and communication. To address this issue, this paper proposes sensing-enhanced secure communication with adaptive sensing performance. Specifically, we formulate the sensing performance implicitly in the information leakage rate and adaptively optimize it for the minimization of the power consumption, offering enhanced flexibility and adaptability in sensing performance. We consider both centralized and decentralized designs to thoroughly investigate the impact of network structure on system performance and complexity. Specifically, we devise a block coordinate descent (BCD)-based method for centralized design. For decentralized design, we develop an optimization framework based on consensus alternating direction method of multipliers (ADMM) to reduce complexity and information exchange overhead. Experimental results demonstrate the advantage of the proposed implicit sensing performance requirement design due to its capability to adaptively adjust the sensing performance to enhance the system performance for varying system configurations.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Federated Learning: Architectural Elements and Future Directions</title>
<link>https://arxiv.org/abs/2510.17642</link>
<guid>https://arxiv.org/abs/2510.17642</guid>
<content:encoded><![CDATA[
arXiv:2510.17642v1 Announce Type: cross 
Abstract: Federated learning (FL) focuses on collaborative model training without the need to move the private data silos to a central server. Despite its several benefits, the classical FL is plagued with several limitations, such as high computational power required for model training(which is critical for low-resource clients), privacy risks, large update traffic, and non-IID heterogeneity. This chapter surveys a hybrid paradigm - Quantum Federated Learning (QFL), which introduces quantum computation, that addresses multiple challenges of classical FL and offers rapid computing capability while keeping the classical orchestration intact. Firstly, we motivate QFL with a concrete presentation on pain points of classical FL, followed by a discussion on a general architecture of QFL frameworks specifying the roles of client and server, communication primitives and the quantum model placement. We classify the existing QFL systems based on four criteria - quantum architecture (pure QFL, hybrid QFL), data processing method (quantum data encoding, quantum feature mapping, and quantum feature selection & dimensionality reduction), network topology (centralized, hierarchial, decentralized), and quantum security mechanisms (quantum key distribution, quantum homomorphic encryption, quantum differential privacy, blind quantum computing). We then describe applications of QFL in healthcare, vehicular networks, wireless networks, and network security, clearly highlighting where QFL improves communication efficiency, security, and performance compared to classical FL. We close with multiple challenges and future works in QFL, including extension of QFL beyond classification tasks, adversarial attacks, realistic hardware deployment, quantum communication protocols deployment, aggregation of different quantum models, and quantum split learning as an alternative to QFL.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proto: A Guided Journey through Modern OS Construction</title>
<link>https://arxiv.org/abs/2504.17984</link>
<guid>https://arxiv.org/abs/2504.17984</guid>
<content:encoded><![CDATA[
arXiv:2504.17984v2 Announce Type: replace 
Abstract: Proto is a new instructional OS that runs on commodity, portable hardware. It showcases modern features, including per-app address spaces, threading, commodity filesystems, USB, DMA, multicore support, self-hosted debugging, and a window manager. It supports rich applications such as 2D/3D games, music and video players, and a blockchain miner. Unlike traditional instructional systems, Proto emphasizes engaging, media-rich apps that go beyond basic terminal programs. Our method breaks down a full-featured OS into a set of incremental, self-contained prototypes. Each prototype introduces a minimal set of OS mechanisms, driven by the needs of specific apps. The construction process then progressively enables these apps by bringing up one mechanism at a time. Proto enables a wider audience to experience building a self-contained software system used in daily life
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smart Traffic Signals: Comparing MARL and Fixed-Time Strategies</title>
<link>https://arxiv.org/abs/2505.14544</link>
<guid>https://arxiv.org/abs/2505.14544</guid>
<content:encoded><![CDATA[
arXiv:2505.14544v3 Announce Type: replace 
Abstract: Urban traffic congestion, particularly at intersections, significantly impacts travel time, fuel consumption, and emissions. Traditional fixed-time signal control systems often lack the adaptability to manage dynamic traffic patterns effectively. This study explores the application of multi-agent reinforcement learning (MARL) to optimize traffic signal coordination across multiple intersections within a simulated environment. Utilizing Pygame, a simulation was developed to model a network of interconnected intersections with randomly generated vehicle flows to reflect realistic traffic variability. A decentralized MARL controller was implemented, in which each traffic signal operates as an autonomous agent, making decisions based on local observations and information from neighboring agents. Performance was evaluated against a baseline fixed-time controller using metrics such as average vehicle wait time and overall throughput. The MARL approach demonstrated statistically significant improvements, including reduced average waiting times and improved throughput. These findings suggest that MARL-based dynamic control strategies hold substantial promise for improving urban traffic management efficiency. More research is recommended to address scalability and real-world implementation challenges.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal online model aggregation</title>
<link>https://arxiv.org/abs/2403.15527</link>
<guid>https://arxiv.org/abs/2403.15527</guid>
<content:encoded><![CDATA[
arXiv:2403.15527v3 Announce Type: replace-cross 
Abstract: Conformal prediction equips machine learning models with a reasonable notion of uncertainty quantification without making strong distributional assumptions. It wraps around any prediction model and converts point predictions into set predictions with a predefined marginal coverage guarantee. However, conformal prediction only works if we fix the underlying machine learning model in advance. A relatively unaddressed issue in conformal prediction is that of model selection and/or aggregation: given a set of prediction models, which one should we conformalize? This paper suggests that instead of performing model selection, it can be prudent and practical to perform conformal set aggregation in an online, adaptive fashion. We propose a wrapper that takes in several conformal prediction sets (themselves wrapped around black-box prediction models), and outputs a single adaptively-combined prediction set. Our method, called conformal online model aggregation (COMA), is based on combining the prediction sets from several algorithms by weighted voting, and can be thought of as a sort of online stacking of the underlying conformal sets. As long as the input sets have (distribution-free) coverage guarantees, COMA retains coverage guarantees, under a negative correlation assumption between errors and weights. We verify that the assumption holds empirically in all settings considered. COMA is well-suited for decentralized or distributed settings, where different users may have different models, and are only willing to share their prediction sets for a new test point in a black-box fashion. As we demonstrate, it is also well-suited to settings with distribution drift and shift, where model selection can be imprudent.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Role of Federated Learning in Improving Financial Security: A Survey</title>
<link>https://arxiv.org/abs/2510.14991</link>
<guid>https://arxiv.org/abs/2510.14991</guid>
<content:encoded><![CDATA[
arXiv:2510.14991v1 Announce Type: new 
Abstract: With the growth of digital financial systems, robust security and privacy have become a concern for financial institutions. Even though traditional machine learning models have shown to be effective in fraud detections, they often compromise user data by requiring centralized access to sensitive information. In IoT-enabled financial endpoints such as ATMs and POS Systems that regularly produce sensitive data that is sent over the network. Federated Learning (FL) offers a privacy-preserving, decentralized model training across institutions without sharing raw data. FL enables cross-silo collaboration among banks while also using cross-device learning on IoT endpoints. This survey explores the role of FL in enhancing financial security and introduces a novel classification of its applications based on regulatory and compliance exposure levels ranging from low-exposure tasks such as collaborative portfolio optimization to high-exposure tasks like real-time fraud detection. Unlike prior surveys, this work reviews FL's practical use within financial systems, discussing its regulatory compliance and recent successes in fraud prevention and blockchain-integrated frameworks. However, FL deployment in finance is not without challenges. Data heterogeneity, adversarial attacks, and regulatory compliance make implementation far from easy. This survey reviews current defense mechanisms and discusses future directions, including blockchain integration, differential privacy, secure multi-party computation, and quantum-secure frameworks. Ultimately, this work aims to be a resource for researchers exploring FL's potential to advance secure, privacy-compliant financial systems.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Q-EnergyDEX: A Zero-Trust Distributed Energy Trading Framework Driven by Quantum Key Distribution and Blockchain</title>
<link>https://arxiv.org/abs/2510.15045</link>
<guid>https://arxiv.org/abs/2510.15045</guid>
<content:encoded><![CDATA[
arXiv:2510.15045v1 Announce Type: new 
Abstract: The rapid decentralization and digitalization of local electricity markets have introduced new cyber-physical vulnerabilities, including key leakage, data tampering, and identity spoofing. Existing blockchain-based solutions provide transparency and traceability but still depend on classical cryptographic primitives that are vulnerable to quantum attacks. To address these challenges, this paper proposes Q-EnergyDEX, a zero-trust distributed energy trading framework driven by quantum key distribution and blockchain. The framework integrates physical-layer quantum randomness with market-level operations, providing an end-to-end quantum-secured infrastructure. A cloud-based Quantum Key Management Service continuously generates verifiable entropy and regulates key generation through a rate-adaptive algorithm to sustain high-quality randomness. A symmetric authentication protocol (Q-SAH) establishes secure and low-latency sessions, while the quantum-aided consensus mechanism (PoR-Lite) achieves probabilistic ledger finality within a few seconds. Furthermore, a Stackelberg-constrained bilateral auction couples market clearing with entropy availability, ensuring both economic efficiency and cryptographic security. Simulation results show that Q-EnergyDEX maintains robust key stability and near-optimal social welfare, demonstrating its feasibility for large-scale decentralized energy markets.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combinatorial Contract Design: Recent Progress and Emerging Frontiers</title>
<link>https://arxiv.org/abs/2510.15065</link>
<guid>https://arxiv.org/abs/2510.15065</guid>
<content:encoded><![CDATA[
arXiv:2510.15065v1 Announce Type: new 
Abstract: Contract theory studies how a principal can incentivize agents to exert costly, unobservable effort through performance-based payments. While classical economic models provide elegant characterizations of optimal solutions, modern applications, ranging from online labor markets and healthcare to AI delegation and blockchain protocols, call for an algorithmic perspective. The challenge is no longer only which contracts induce desired behavior, but whether such contracts can be computed efficiently. This viewpoint has given rise to \emph{algorithmic contract design}, paralleling the rise of algorithmic mechanism design two decades ago.
  This article focuses on \emph{combinatorial contracts}, an emerging frontier within algorithmic contract design, where agents may choose among exponentially many combinations of actions, or where multiple agents must work together as a team, and the challenge lies in selecting the right composition. These models capture a wide variety of real-world contracting environments, from hospitals coordinating physicians across treatment protocols to firms hiring teams of engineers for interdependent tasks. We review three combinatorial settings: (i) a single agent choosing multiple actions, (ii) multiple agents with binary actions, and (iii) multiple agents each selecting multiple actions. For each, we highlight structural insights, algorithmic techniques, and complexity barriers. Results include tractable cases such as gross substitutes reward functions, hardness results, and approximation guarantees under value- and demand-oracle access. By charting these advances, the article maps the emerging landscape of combinatorial contract design, and highlights fundamental open questions and promising directions for future work.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NEMO: Faster Parallel Execution for Highly Contended Blockchain Workloads (Full version)</title>
<link>https://arxiv.org/abs/2510.15122</link>
<guid>https://arxiv.org/abs/2510.15122</guid>
<content:encoded><![CDATA[
arXiv:2510.15122v1 Announce Type: new 
Abstract: Following the design of more efficient blockchain consensus algorithms, the execution layer has emerged as the new performance bottleneck of blockchains, especially under high contention. Current parallel execution frameworks either rely on optimistic concurrency control (OCC) or on pessimistic concurrency control (PCC), both of which see their performance decrease when workloads are highly contended, albeit for different reasons. In this work, we present NEMO, a new blockchain execution engine that combines OCC with the object data model to address this challenge. NEMO introduces four core innovations: (i) a greedy commit rule for transactions using only owned objects; (ii) refined handling of dependencies to reduce re-executions; (iii) the use of incomplete but statically derivable read/write hints to guide execution; and (iv) a priority-based scheduler that favors transactions that unblock others. Through simulated execution experiments, we demonstrate that NEMO significantly reduces redundant computation and achieves higher throughput than representative approaches. For example, with 16 workers NEMO's throughput is up to 42% higher than the one of Block-STM, the state-of-the-art OCC approach, and 61% higher than the pessimistic concurrency control baseline used.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DFCA: Decentralized Federated Clustering Algorithm</title>
<link>https://arxiv.org/abs/2510.15300</link>
<guid>https://arxiv.org/abs/2510.15300</guid>
<content:encoded><![CDATA[
arXiv:2510.15300v1 Announce Type: new 
Abstract: Clustered Federated Learning has emerged as an effective approach for handling heterogeneous data across clients by partitioning them into clusters with similar or identical data distributions. However, most existing methods, including the Iterative Federated Clustering Algorithm (IFCA), rely on a central server to coordinate model updates, which creates a bottleneck and a single point of failure, limiting their applicability in more realistic decentralized learning settings. In this work, we introduce DFCA, a fully decentralized clustered FL algorithm that enables clients to collaboratively train cluster-specific models without central coordination. DFCA uses a sequential running average to aggregate models from neighbors as updates arrive, providing a communication-efficient alternative to batch aggregation while maintaining clustering performance. Our experiments on various datasets demonstrate that DFCA outperforms other decentralized algorithms and performs comparably to centralized IFCA, even under sparse connectivity, highlighting its robustness and practicality for dynamic real-world decentralized networks.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Parameter-Free Online Learning</title>
<link>https://arxiv.org/abs/2510.15644</link>
<guid>https://arxiv.org/abs/2510.15644</guid>
<content:encoded><![CDATA[
arXiv:2510.15644v1 Announce Type: new 
Abstract: We propose the first parameter-free decentralized online learning algorithms with network regret guarantees, which achieve sublinear regret without requiring hyperparameter tuning. This family of algorithms connects multi-agent coin-betting and decentralized online learning via gossip steps. To enable our decentralized analysis, we introduce a novel "betting function" formulation for coin-betting that simplifies the multi-agent regret analysis. Our analysis shows sublinear network regret bounds and is validated through experiments on synthetic and real datasets. This family of algorithms is applicable to distributed sensing, decentralized optimization, and collaborative ML applications.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grassroots Logic Programs: A Secure, Multiagent, Concurrent, Logic Programming Language</title>
<link>https://arxiv.org/abs/2510.15747</link>
<guid>https://arxiv.org/abs/2510.15747</guid>
<content:encoded><![CDATA[
arXiv:2510.15747v1 Announce Type: new 
Abstract: Grassroots platforms are distributed applications run by\linebreak cryptographically-identified people on their networked personal devices, where multiple disjoint platform instances emerge independently and coalesce when they interoperate. Their foundation is the grassroots social graph, upon which grassroots social networks, grassroots cryptocurrencies, and grassroots democratic federations can be built.
  Grassroots platforms have yet to be implemented, the key challenge being faulty and malicious participants: without secure programming support, correct participants cannot reliably identify each other, establish secure communication, or verify each other's code integrity.
  We present Grassroots Logic Programs (GLP), a secure, multiagent, concurrent, logic programming language for implementing grassroots platforms. GLP extends logic programs with paired single-reader/single-writer (SRSW) logic variables, providing secure communication channels among cryptographically-identified people through encrypted, signed and attested messages, which enable identity and code integrity verification. We present GLP progressively: logic programs, concurrent GLP, multiagent GLP, augmenting it with cryptographic security, and providing smartphone implementation-ready specifications. We prove safety properties including that GLP computations are deductions, SRSW preservation, acyclicity, and monotonicity. We prove multiagent GLP is grassroots and that GLP streams achieve blockchain security properties. We present a grassroots social graph protocol establishing authenticated peer-to-peer connections and demonstrate secure grassroots social networking applications.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Ego-graph inference for Networked Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.16606</link>
<guid>https://arxiv.org/abs/2509.16606</guid>
<content:encoded><![CDATA[
arXiv:2509.16606v2 Announce Type: replace 
Abstract: In networked multi-agent reinforcement learning (Networked-MARL), decentralized agents must act under local observability and constrained communication over fixed physical graphs. Existing methods often assume static neighborhoods, limiting adaptability to dynamic or heterogeneous environments. While centralized frameworks can learn dynamic graphs, their reliance on global state access and centralized infrastructure is impractical in real-world decentralized systems. We propose a stochastic graph-based policy for Networked-MARL, where each agent conditions its decision on a sampled subgraph over its local physical neighborhood. Building on this formulation, we introduce BayesG, a decentralized actor-framework that learns sparse, context-aware interaction structures via Bayesian variational inference. Each agent operates over an ego-graph and samples a latent communication mask to guide message passing and policy computation. The variational distribution is trained end-to-end alongside the policy using an evidence lower bound (ELBO) objective, enabling agents to jointly learn both interaction topology and decision-making strategies. BayesG outperforms strong MARL baselines on large-scale traffic control tasks with up to 167 agents, demonstrating superior scalability, efficiency, and performance.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized non-convex optimization via bi-level SQP and ADMM</title>
<link>https://arxiv.org/abs/2204.08786</link>
<guid>https://arxiv.org/abs/2204.08786</guid>
<content:encoded><![CDATA[
arXiv:2204.08786v4 Announce Type: replace-cross 
Abstract: Decentralized non-convex optimization is important in many problems of practical relevance. Existing decentralized methods, however, typically either lack convergence guarantees for general non-convex problems, or they suffer from a high subproblem complexity. We present a novel bi-level SQP method, where the inner quadratic problems are solved via ADMM. A decentralized stopping criterion from inexact Newton methods allows the early termination of ADMM as an inner algorithm to improve computational efficiency. The method has local convergence guarantees for non-convex problems. Moreover, it only solves sequences of Quadratic Programs, whereas many existing algorithms solve sequences of Nonlinear Programs. The method shows competitive numerical performance for an optimal power flow problem.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Real-Time Iterations for Distributed NMPC</title>
<link>https://arxiv.org/abs/2401.14898</link>
<guid>https://arxiv.org/abs/2401.14898</guid>
<content:encoded><![CDATA[
arXiv:2401.14898v3 Announce Type: replace-cross 
Abstract: This article presents a Real-Time Iteration (RTI) scheme for distributed Nonlinear Model Predictive Control (NMPC). The scheme transfers the well-known RTI approach, a key enabler for many industrial real-time NMPC implementations, to the setting of cooperative distributed control. At each sampling instant, one outer iteration of a bi-level decentralized Sequential Quadratic Programming (dSQP) method is applied to a centralized optimal control problem. This ensures that real-time requirements are met and it facilitates cooperation between subsystems. Combining novel dSQP convergence results with RTI stability guarantees, we prove local exponential stability under standard assumptions on the MPC design with and without terminal constraints. The proposed scheme only requires neighbor-to-neighbor communication and avoids a central coordinator. A numerical example with coupled inverted pendulums demonstrates the efficacy of the approach.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving and Incentive-Driven Relay-Based Framework for Cross-Domain Blockchain Interoperability</title>
<link>https://arxiv.org/abs/2510.14151</link>
<guid>https://arxiv.org/abs/2510.14151</guid>
<content:encoded><![CDATA[
arXiv:2510.14151v1 Announce Type: new 
Abstract: Interoperability is essential for transforming blockchains from isolated networks into collaborative ecosystems, unlocking their full potential. While significant progress has been made in public blockchain interoperability, bridging permissioned and permissionless blockchains poses unique challenges due to differences in access control, architectures, and security requirements. This paper introduces a blockchain-agnostic framework to enable interoperability between permissioned and permissionless networks. Leveraging cryptographic techniques, the framework ensures secure data exchanges. Its lightweight architectural design simplifies implementation and maintenance, while the integration of Clover and Dandelion++ protocols enhances transaction anonymity. Performance evaluations demonstrate the framework's effectiveness in achieving secure and efficient interoperability by measuring the forwarding time, the throughput, the availability, and their collusion impact of the system across heterogeneous blockchain ecosystems.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proof-Carrying Fair Ordering: Asymmetric Verification for BFT via Incremental Graphs</title>
<link>https://arxiv.org/abs/2510.14186</link>
<guid>https://arxiv.org/abs/2510.14186</guid>
<content:encoded><![CDATA[
arXiv:2510.14186v1 Announce Type: new 
Abstract: Byzantine Fault-Tolerant (BFT) consensus protocols ensure agreement on transaction ordering despite malicious actors, but unconstrained ordering power enables sophisticated value extraction attacks like front running and sandwich attacks - a critical threat to blockchain systems. Order-fair consensus curbs adversarial value extraction by constraining how leaders may order transactions. While state-of-the-art protocols such as Themis attain strong guarantees through graph-based ordering, they ask every replica to re-run the leader's expensive ordering computation for validation - an inherently symmetric and redundant paradigm. We present AUTIG, a high-performance, pluggable order-fairness service that breaks this symmetry. Our key insight is that verifying a fair order does not require re-computing it. Instead, verification can be reduced to a stateless audit of succinct, verifiable assertions about the ordering graph's properties. AUTIG realizes this via an asymmetric architecture: the leader maintains a persistent Unconfirmed-Transaction Incremental Graph (UTIG) to amortize graph construction across rounds and emits a structured proof of fairness with each proposal; followers validate the proof without maintaining historical state. AUTIG introduces three critical innovations: (i) incremental graph maintenance driven by threshold-crossing events and state changes; (ii) a decoupled pipeline that overlaps leader-side collection/update/extraction with follower-side stateless verification; and (iii) a proof design covering all internal pairs in the finalized prefix plus a frontier completeness check to rule out hidden external dependencies. We implement AUTIG and evaluate it against symmetric graph-based baselines under partial synchrony. Experiments show higher throughput and lower end-to-end latency while preserving gamma-batch-order-fairness.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentive-Based Federated Learning</title>
<link>https://arxiv.org/abs/2510.14208</link>
<guid>https://arxiv.org/abs/2510.14208</guid>
<content:encoded><![CDATA[
arXiv:2510.14208v1 Announce Type: new 
Abstract: Federated learning promises to revolutionize machine learning by enabling collaborative model training without compromising data privacy. However, practical adaptability can be limited by critical factors, such as the participation dilemma. Participating entities are often unwilling to contribute to a learning system unless they receive some benefits, or they may pretend to participate and free-ride on others. This chapter identifies the fundamental challenges in designing incentive mechanisms for federated learning systems. It examines how foundational concepts from economics and game theory can be applied to federated learning, alongside technology-driven solutions such as blockchain and deep reinforcement learning. This work presents a comprehensive taxonomy that thoroughly covers both centralized and decentralized architectures based on the aforementioned theoretical concepts. Furthermore, the concepts described are presented from an application perspective, covering emerging industrial applications, including healthcare, smart infrastructure, vehicular networks, and blockchain-based decentralized systems. Through this exploration, this chapter demonstrates that well-designed incentive mechanisms are not merely optional features but essential components for the practical success of federated learning. This analysis reveals both the promising solutions that have emerged and the significant challenges that remain in building truly sustainable, fair, and robust federated learning ecosystems.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Certifying optimal MEV strategies with Lean</title>
<link>https://arxiv.org/abs/2510.14480</link>
<guid>https://arxiv.org/abs/2510.14480</guid>
<content:encoded><![CDATA[
arXiv:2510.14480v1 Announce Type: new 
Abstract: Maximal Extractable Value (MEV) refers to a class of attacks to decentralized applications where the adversary profits by manipulating the ordering, inclusion, or exclusion of transactions in a blockchain. Decentralized Finance (DeFi) protocols are a primary target of these attacks, as their logic depends critically on transaction sequencing. To date, MEV attacks have already extracted billions of dollars in value, underscoring their systemic impact on blockchain security. Verifying the absence of MEV attacks requires determining suitable upper bounds, i.e. proving that no adversarial strategy can extract more value (if any) than expected by protocol designers. This problem is notoriously difficult: the space of adversarial strategies is extremely vast, making empirical studies and pen-and-paper reasoning insufficiently rigorous. In this paper, we present the first mechanized formalization of MEV in the Lean theorem prover. We introduce a methodology to construct machine-checked proofs of MEV bounds, providing correctness guarantees beyond what is possible with existing techniques. To demonstrate the generality of our approach, we model and analyse the MEV of two paradigmatic DeFi protocols. Notably, we develop the first machine-checked proof of the optimality of sandwich attacks in Automated Market Makers, a fundamental DeFi primitive.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Helmsman: Autonomous Synthesis of Federated Learning Systems via Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2510.14512</link>
<guid>https://arxiv.org/abs/2510.14512</guid>
<content:encoded><![CDATA[
arXiv:2510.14512v1 Announce Type: new 
Abstract: Federated Learning (FL) offers a powerful paradigm for training models on decentralized data, but its promise is often undermined by the immense complexity of designing and deploying robust systems. The need to select, combine, and tune strategies for multifaceted challenges like data heterogeneity and system constraints has become a critical bottleneck, resulting in brittle, bespoke solutions. To address this, we introduce Helmsman, a novel multi-agent system that automates the end-to-end synthesis of federated learning systems from high-level user specifications. It emulates a principled research and development workflow through three collaborative phases: (1) interactive human-in-the-loop planning to formulate a sound research plan, (2) modular code generation by supervised agent teams, and (3) a closed-loop of autonomous evaluation and refinement in a sandboxed simulation environment. To facilitate rigorous evaluation, we also introduce AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess the system-level generation capabilities of agentic systems in FL. Extensive experiments demonstrate that our approach generates solutions competitive with, and often superior to, established hand-crafted baselines. Our work represents a significant step towards the automated engineering of complex decentralized AI systems.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JASDA: Introducing Job-Aware Scheduling in Scheduler-Driven Job Atomization</title>
<link>https://arxiv.org/abs/2510.14599</link>
<guid>https://arxiv.org/abs/2510.14599</guid>
<content:encoded><![CDATA[
arXiv:2510.14599v1 Announce Type: new 
Abstract: The increasing complexity and temporal variability of workloads on MIG-enabled GPUs challenge the scalability of traditional centralized scheduling. Building upon the SJA concept, this paper introduces JASDA-a novel paradigm that extends SJA from a largely centralized scheduling model toward a fully decentralized negotiation process. In JASDA, jobs actively generate and score feasible subjobs in response to scheduler-announced execution windows, while the scheduler performs policy-driven clearing that balances utilization, fairness, and temporal responsiveness. This bidirectional, iterative interaction embeds feedback, calibration, and probabilistic safety directly into the scheduling loop, enabling adaptive and transparent decision-making. By coupling principles from auction theory and online optimization with the temporal granularity of GPU workloads, JASDA provides a scalable foundation for market-aware and fairness-driven resource management-bridging theoretical scheduling models with practical deployment in modern MIG-enabled environments relevant to Artificial Intelligence and Agriculture 4.0.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Bidding Games: Reinforcement Learning for MEV Extraction on Polygon Blockchain</title>
<link>https://arxiv.org/abs/2510.14642</link>
<guid>https://arxiv.org/abs/2510.14642</guid>
<content:encoded><![CDATA[
arXiv:2510.14642v1 Announce Type: new 
Abstract: In blockchain networks, the strategic ordering of transactions within blocks has emerged as a significant source of profit extraction, known as Maximal Extractable Value (MEV). The transition from spam-based Priority Gas Auctions to structured auction mechanisms like Polygon Atlas has transformed MEV extraction from public bidding wars into sealed-bid competitions under extreme time constraints. While this shift reduces network congestion, it introduces complex strategic challenges where searchers must make optimal bidding decisions within a sub-second window without knowledge of competitor behavior or presence. Traditional game-theoretic approaches struggle in this high-frequency, partially observable environment due to their reliance on complete information and static equilibrium assumptions. We present a reinforcement learning framework for MEV extraction on Polygon Atlas and make three contributions: (1) A novel simulation environment that accurately models the stochastic arrival of arbitrage opportunities and probabilistic competition in Atlas auctions; (2) A PPO-based bidding agent optimized for real-time constraints, capable of adaptive strategy formulation in continuous action spaces while maintaining production-ready inference speeds; (3) Empirical validation demonstrating our history-conditioned agent captures 49\% of available profits when deployed alongside existing searchers and 81\% when replacing the market leader, significantly outperforming static bidding strategies. Our work establishes that reinforcement learning provides a critical advantage in high-frequency MEV environments where traditional optimization methods fail, offering immediate value for industrial participants and protocol designers alike.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FibRace: a large-scale benchmark of client-side proving on mobile devices</title>
<link>https://arxiv.org/abs/2510.14693</link>
<guid>https://arxiv.org/abs/2510.14693</guid>
<content:encoded><![CDATA[
arXiv:2510.14693v1 Announce Type: new 
Abstract: FibRace, jointly developed by KKRT Labs and Hyli, was the first large-scale experiment to test client-side proof generation on smartphones using Cairo M. Presented as a mobile game in which players proved Fibonacci numbers and climbed a leaderboard, FibRace served a dual purpose: to engage the public and to provide empirical benchmarking. Over a three-week campaign (September 11-30, 2025), 6,047 players across 99 countries generated 2,195,488 proofs on 1,420 unique device models. The results show that most modern smartphones can complete a proof in under 5 seconds, confirming that *mobile devices are now capable of producing zero-knowledge proofs reliably*, without the need for remote provers or specialized hardware. Performance was correlated primarily with RAM capacity and SoC (System on Chip) performance: devices with at least 3 GB of RAM proved stably, when Apple's A19 Pro and M-series chips achieved the fastest proving times. Hyli's blockchain natively verified every proof onchain without congestion. FibRace provides the most comprehensive dataset to date on mobile proving performance, establishing a practical baseline for future research in lightweight provers, proof-powered infrastructure, and privacy-preserving mobile applications.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedPPA: Progressive Parameter Alignment for Personalized Federated Learning</title>
<link>https://arxiv.org/abs/2510.14698</link>
<guid>https://arxiv.org/abs/2510.14698</guid>
<content:encoded><![CDATA[
arXiv:2510.14698v1 Announce Type: new 
Abstract: Federated Learning (FL) is designed as a decentralized, privacy-preserving machine learning paradigm that enables multiple clients to collaboratively train a model without sharing their data. In real-world scenarios, however, clients often have heterogeneous computational resources and hold non-independent and identically distributed data (non-IID), which poses significant challenges during training. Personalized Federated Learning (PFL) has emerged to address these issues by customizing models for each client based on their unique data distribution. Despite its potential, existing PFL approaches typically overlook the coexistence of model and data heterogeneity arising from clients with diverse computational capabilities. To overcome this limitation, we propose a novel method, called Progressive Parameter Alignment (FedPPA), which progressively aligns the weights of common layers across clients with the global model's weights. Our approach not only mitigates inconsistencies between global and local models during client updates, but also preserves client's local knowledge, thereby enhancing personalization robustness in non-IID settings. To further enhance the global model performance while retaining strong personalization, we also integrate entropy-based weighted averaging into the FedPPA framework. Experiments on three image classification datasets, including MNIST, FMNIST, and CIFAR-10, demonstrate that FedPPA consistently outperforms existing FL algorithms, achieving superior performance in personalized adaptation.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Augmenting Smart Contract Decompiler Output through Fine-grained Dependency Analysis and LLM-facilitated Semantic Recovery</title>
<link>https://arxiv.org/abs/2501.08670</link>
<guid>https://arxiv.org/abs/2501.08670</guid>
<content:encoded><![CDATA[
arXiv:2501.08670v2 Announce Type: replace 
Abstract: Decompiler is a specialized type of reverse engineering tool extensively employed in program analysis tasks, particularly in program comprehension and vulnerability detection. However, current Solidity smart contract decompilers face significant limitations in reconstructing the original source code. In particular, the bottleneck of SOTA decompilers lies in inaccurate method identification, incorrect variable type recovery, and missing contract attributes. These deficiencies hinder downstream tasks and understanding of the program logic. To address these challenges, we propose SmartHalo, a new framework that enhances decompiler output by combining static analysis (SA) and large language models (LLM). SmartHalo leverages the complementary strengths of SA's accuracy in control and data flow analysis and LLM's capability in semantic prediction. More specifically, \system{} constructs a new data structure - Dependency Graph (DG), to extract semantic dependencies via static analysis. Then, it takes DG to create prompts for LLM optimization. Finally, the correctness of LLM outputs is validated through symbolic execution and formal verification. Evaluation on a dataset consisting of 465 randomly selected smart contract methods shows that SmartHalo significantly improves the quality of the decompiled code, compared to SOTA decompilers (e.g., Gigahorse). Notably, integrating GPT-4o with SmartHalo further enhances its performance, achieving precision rates of 87.39% for method boundaries, 90.39% for variable types, and 80.65% for contract attributes.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Far Have LLMs Come Toward Automated SATD Taxonomy Construction?</title>
<link>https://arxiv.org/abs/2506.09601</link>
<guid>https://arxiv.org/abs/2506.09601</guid>
<content:encoded><![CDATA[
arXiv:2506.09601v2 Announce Type: replace 
Abstract: Technical debt refers to suboptimal code that degrades software quality. When developers intentionally introduce such debt, it is called self-admitted technical debt (SATD). Since SATD hinders maintenance, identifying its categories is key to uncovering quality issues. Traditionally, constructing such taxonomies requires manually inspecting SATD comments and surrounding code, which is time-consuming, labor-intensive, and often inconsistent due to annotator subjectivity. In this study, we investigate to what extent large language models (LLMs) can generate SATD taxonomies. We designed a structured, LLM-driven pipeline that mirrors the taxonomy construction steps researchers typically follow. We evaluated it on SATD datasets from three domains: quantum software, smart contracts, and machine learning. It successfully recovered domain-specific categories reported in prior work, such as Layer Configuration in machine learning. It also completed taxonomy generation in under two hours and for less than $1, even on the largest dataset. These results suggest that, while full automation remains challenging, LLMs can support semi-automated SATD taxonomy construction. Furthermore, our work opens up avenues for future work, such as automated taxonomy generation in other areas.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Safety, Alignment, and Ethics (AI SAE)</title>
<link>https://arxiv.org/abs/2509.24065</link>
<guid>https://arxiv.org/abs/2509.24065</guid>
<content:encoded><![CDATA[
arXiv:2509.24065v2 Announce Type: replace 
Abstract: This paper grounds ethics in evolutionary biology, viewing moral norms as adaptive mechanisms that render cooperation fitness-viable under selection pressure. Current alignment approaches add ethics post hoc, treating it as an external constraint rather than embedding it as an evolutionary strategy for cooperation. The central question is whether normative architectures can be embedded directly into AI systems to sustain human--AI cooperation (symbiosis) as capabilities scale. To address this, I propose a governance--embedding--representation pipeline linking moral representation learning to system-level design and institutional governance, treating alignment as a multi-level problem spanning cognition, optimization, and oversight. I formalize moral norm representation through the moral problem space, a learnable subspace in neural representations where cooperative norms can be encoded and causally manipulated. Using sparse autoencoders, activation steering, and causal interventions, I outline a research program for engineering moral representations and embedding them into the full semantic space -- treating competing theories of morality as empirical hypotheses about representation geometry rather than philosophical positions. Governance principles leverage these learned moral representations to regulate how cooperative behaviors evolve within the AI ecosystem. Through replicator dynamics and multi-agent game theory, I model how internal representational features can shape population-level incentives by motivating the design of sanctions and subsidies structured to yield decentralized normative institutions.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARTeX: Anonymity Real-world-assets Token eXchange</title>
<link>https://arxiv.org/abs/2510.12821</link>
<guid>https://arxiv.org/abs/2510.12821</guid>
<content:encoded><![CDATA[
arXiv:2510.12821v1 Announce Type: new 
Abstract: This paper addresses one of the most noteworthy issues in the recent virtual asset market, the privacy concerns related to token transactions of Real-World Assets tokens, known as RWA tokens. Following the advent of Bitcoin, the virtual asset market has experienced explosive growth, spawning movements to link real-world assets with virtual assets. However, due to the transparency principle of blockchain technology, the anonymity of traders cannot be guaranteed. In the existing blockchain environment, there have been instances of protecting the privacy of fungible tokens (FTs) using mixer services. Moreover, numerous studies have been conducted to secure the privacy of non-fungible tokens (NFTs). However, due to the unique characteristics of RWA tokens and the limitations of each study, it has been challenging to achieve the goal of anonymity protection effectively. This paper proposes a new token trading platform, the ARTeX, designed to resolve these issues. This platform not only addresses the shortcomings of existing methods but also ensures the anonymity of traders while enhancing safeguards against illegal activities.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching for Heterogeneous Tasks and Clusters</title>
<link>https://arxiv.org/abs/2510.12889</link>
<guid>https://arxiv.org/abs/2510.12889</guid>
<content:encoded><![CDATA[
arXiv:2510.12889v1 Announce Type: new 
Abstract: This paper introduces Dodoor, an efficient randomized decentralized scheduler designed for task scheduling in modern data centers. Dodoor leverages advanced research on the weighted balls-into-bins model with b-batched setting. Unlike other decentralized schedulers that rely on real-time probing of remote servers, Dodoor makes scheduling decisions based on cached server information, which is updated in batches, to reduce communication overheads. To schedule tasks with dynamic, multidimensional resource requirements in heterogeneous cluster, Dodoor uses a novel load score to measure servers' loads for each scheduled task. This score captures the anti-affinity between servers and tasks in contrast to the commonly used heuristic of counting pending tasks to balance load. On a 101-node heterogeneous cluster, Dodoor is evaluated using two workloads: (i) simulated Azure virtual machines placements and (ii) real serverless Python functions executions in Docker. The evaluation shows that Dodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can also increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency by 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two workloads.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Altruistic Ride Sharing: A Community-Driven Approach to Short-Distance Mobility</title>
<link>https://arxiv.org/abs/2510.13227</link>
<guid>https://arxiv.org/abs/2510.13227</guid>
<content:encoded><![CDATA[
arXiv:2510.13227v1 Announce Type: new 
Abstract: Urban mobility faces persistent challenges of congestion and fuel consumption, specifically when people choose a private, point-to-point commute option. Profit-driven ride-sharing platforms prioritize revenue over fairness and sustainability. This paper introduces Altruistic Ride-Sharing (ARS), a decentralized, peer-to-peer mobility framework where participants alternate between driver and rider roles based on altruism points rather than monetary incentives. The system integrates multi-agent reinforcement learning (MADDPG) for dynamic ride-matching, game-theoretic equilibrium guarantees for fairness, and a population model to sustain long-term balance. Using real-world New York City taxi data, we demonstrate that ARS reduces travel distance and emissions, increases vehicle utilization, and promotes equitable participation compared to both no-sharing and optimization-based baselines. These results establish ARS as a scalable, community-driven alternative to conventional ride-sharing, aligning individual behavior with collective urban sustainability goals.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BlendFL: Blended Federated Learning for Handling Multimodal Data Heterogeneity</title>
<link>https://arxiv.org/abs/2510.13266</link>
<guid>https://arxiv.org/abs/2510.13266</guid>
<content:encoded><![CDATA[
arXiv:2510.13266v1 Announce Type: new 
Abstract: One of the key challenges of collaborative machine learning, without data sharing, is multimodal data heterogeneity in real-world settings. While Federated Learning (FL) enables model training across multiple clients, existing frameworks, such as horizontal and vertical FL, are only effective in `ideal' settings that meet specific assumptions. Hence, they struggle to address scenarios where neither all modalities nor all samples are represented across the participating clients. To address this gap, we propose BlendFL, a novel FL framework that seamlessly blends the principles of horizontal and vertical FL in a synchronized and non-restrictive fashion despite the asymmetry across clients. Specifically, any client within BlendFL can benefit from either of the approaches, or both simultaneously, according to its available dataset. In addition, BlendFL features a decentralized inference mechanism, empowering clients to run collaboratively trained local models using available local data, thereby reducing latency and reliance on central servers for inference. We also introduce BlendAvg, an adaptive global model aggregation strategy that prioritizes collaborative model updates based on each client's performance. We trained and evaluated BlendFL and other state-of-the-art baselines on three classification tasks using a large-scale real-world multimodal medical dataset and a popular multimodal benchmark. Our results highlight BlendFL's superior performance for both multimodal and unimodal classification. Ablation studies demonstrate BlendFL's faster convergence compared to traditional approaches, accelerating collaborative learning. Overall, in our study we highlight the potential of BlendFL for handling multimodal data heterogeneity for collaborative learning in real-world settings where data privacy is crucial, such as in healthcare and finance.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Authenticated and Interoperable Multimedia Healthcare Data over Hybrid-Storage Blockchains</title>
<link>https://arxiv.org/abs/2510.13318</link>
<guid>https://arxiv.org/abs/2510.13318</guid>
<content:encoded><![CDATA[
arXiv:2510.13318v1 Announce Type: new 
Abstract: The integration of blockchain technology into healthcare presents a paradigm shift for secure data management, enabling decentralized and tamper-proof storage and sharing of sensitive Electronic Health Records (EHRs). However, existing blockchain-based healthcare systems, while providing robust access control, commonly overlook the high latency in user-side re-computation of hashes for integrity verification of large multimedia data, impairing their practicality, especially in time-sensitive clinical scenarios. In this paper, we propose FAITH, an innovative scheme for \underline{F}ast \underline{A}uthenticated and \underline{I}nteroperable mul\underline{T}imedia \underline{H}ealthcare data storage and sharing over hybrid-storage blockchains. Rather than user-side hash re-computations, FAITH lets an off-chain storage provider generate verifiable proofs using recursive Zero-Knowledge Proofs (ZKPs), while the user only needs to perform lightweight verification. For flexible access authorization, we leverage Proxy Re-Encryption (PRE) and enable the provider to conduct ciphertext re-encryption, in which the re-encryption correctness can be verified via ZKPs against the malicious provider. All metadata and proofs are recorded on-chain for public verification. We provide a comprehensive analysis of FAITH's security regarding data privacy and integrity. We implemented a prototype of FAITH, and extensive experiments demonstrated its practicality for time-critical healthcare applications, dramatically reducing user-side verification latency by up to $98\%$, bringing it from $4$ s down to around $70$ ms for a $5$ GB encrypted file.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multifractality and its sources in the digital currency market</title>
<link>https://arxiv.org/abs/2510.13785</link>
<guid>https://arxiv.org/abs/2510.13785</guid>
<content:encoded><![CDATA[
arXiv:2510.13785v1 Announce Type: cross 
Abstract: Multifractality in time series analysis characterizes the presence of multiple scaling exponents, indicating heterogeneous temporal structures and complex dynamical behaviors beyond simple monofractal models. In the context of digital currency markets, multifractal properties arise due to the interplay of long-range temporal correlations and heavy-tailed distributions of returns, reflecting intricate market microstructure and trader interactions. Incorporating multifractal analysis into the modeling of cryptocurrency price dynamics enhances the understanding of market inefficiencies, may improve volatility forecasting and facilitate the detection of critical transitions or regime shifts. Based on the multifractal cross-correlation analysis (MFCCA) whose spacial case is the multifractal detrended fluctuation analysis (MFDFA), as the most commonly used practical tools for quantifying multifractality, in the present contribution a recently proposed method of disentangling sources of multifractality in time series was applied to the most representative instruments from the digital market. They include Bitcoin (BTC), Ethereum (ETH), decentralized exchanges (DEX) and non-fungible tokens (NFT). The results indicate the significant role of heavy tails in generating a broad multifractal spectrum. However, they also clearly demonstrate that the primary source of multifractality are temporal correlations in the series, and without them, multifractality fades out. It appears characteristic that these temporal correlations, to a large extent, do not depend on the thickness of the tails of the fluctuation distribution. These observations, made here in the context of the digital currency market, provide a further strong argument for the validity of the proposed methodology of disentangling sources of multifractality in time series.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Ensemble-Based Semi-Supervised Learning for Illicit Account Detection in Ethereum DeFi Transactions</title>
<link>https://arxiv.org/abs/2412.02408</link>
<guid>https://arxiv.org/abs/2412.02408</guid>
<content:encoded><![CDATA[
arXiv:2412.02408v2 Announce Type: replace 
Abstract: The advent of smart contracts has enabled the rapid rise of Decentralized Finance (DeFi) on the Ethereum blockchain, offering substantial rewards in financial innovation and inclusivity. This growth, however, is accompanied by significant security risks such as illicit accounts engaged in fraud. Effective detection is further limited by the scarcity of labeled data and the evolving tactics of malicious accounts. To address these challenges with a robust solution for safeguarding the DeFi ecosystem, we propose $\textbf{SLEID}$, a $\textbf{S}$elf-$\textbf{L}$earning $\textbf{E}$nsemble-based $\textbf{I}$llicit account $\textbf{D}$etection framework. SLEID uses an Isolation Forest model for initial outlier detection and a self-training mechanism to iteratively generate pseudo-labels for unlabeled accounts, enhancing detection accuracy. Experiments on 6,903,860 Ethereum transactions with extensive DeFi interaction coverage demonstrate that SLEID significantly outperforms supervised and semi-supervised baselines with $\textbf{+2.56}$ percentage-point precision, comparable recall, and $\textbf{+0.90}$ percentage-point F1 -- particularly for the minority illicit class -- alongside $\textbf{+3.74}$ percentage-points higher accuracy and improvements in PR-AUC, while substantially reducing reliance on labeled data.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking LLMs' Swarm intelligence</title>
<link>https://arxiv.org/abs/2505.04364</link>
<guid>https://arxiv.org/abs/2505.04364</guid>
<content:encoded><![CDATA[
arXiv:2505.04364v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) show potential for complex reasoning, yet their capacity for emergent coordination in Multi-Agent Systems (MAS) when operating under strict swarm-like constraints-limited local perception and communication-remains largely unexplored. Existing benchmarks often do not fully capture the unique challenges of decentralized coordination when agents operate with incomplete spatio-temporal information. To bridge this gap, we introduce SwarmBench, a novel benchmark designed to systematically evaluate the swarm intelligence capabilities of LLMs acting as decentralized agents. SwarmBench features five foundational MAS coordination tasks (Pursuit, Synchronization, Foraging, Flocking, Transport) within a configurable 2D grid environment, forcing agents to rely solely on local sensory input ($k\times k$ view) and local communication. We propose metrics for coordination effectiveness and analyze emergent group dynamics. Zero-shot evaluations of leading LLMs (e.g., deepseek-v3, o4-mini) reveal significant task-dependent performance variations. While some rudimentary coordination is observed, our results indicate that current LLMs significantly struggle with robust long-range planning and adaptive strategy formation under the uncertainty inherent in these decentralized scenarios. Assessing LLMs under such swarm-like constraints is crucial for understanding their utility in future decentralized intelligent systems. We release SwarmBench as an open, extensible toolkit-built on a customizable physical system-providing environments, prompts, evaluation scripts, and comprehensive datasets. This aims to foster reproducible research into LLM-based MAS coordination and the theoretical underpinnings of emergent collective behavior under severe informational decentralization. Our code repository is available at https://github.com/x66ccff/swarmbench.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralizing Multi-Agent Reinforcement Learning with Temporal Causal Information</title>
<link>https://arxiv.org/abs/2506.07829</link>
<guid>https://arxiv.org/abs/2506.07829</guid>
<content:encoded><![CDATA[
arXiv:2506.07829v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) algorithms can find an optimal policy for a single agent to accomplish a particular task. However, many real-world problems require multiple agents to collaborate in order to achieve a common goal. For example, a robot executing a task in a warehouse may require the assistance of a drone to retrieve items from high shelves. In Decentralized Multi-Agent RL (DMARL), agents learn independently and then combine their policies at execution time, but often must satisfy constraints on compatibility of local policies to ensure that they can achieve the global task when combined. In this paper, we study how providing high-level symbolic knowledge to agents can help address unique challenges of this setting, such as privacy constraints, communication limitations, and performance concerns. In particular, we extend the formal tools used to check the compatibility of local policies with the team task, making decentralized training with theoretical guarantees usable in more scenarios. Furthermore, we empirically demonstrate that symbolic knowledge about the temporal evolution of events in the environment can significantly expedite the learning process in DMARL.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rationally Analyzing Shelby: Proving Incentive Compatibility in a Decentralized Storage Network</title>
<link>https://arxiv.org/abs/2510.11866</link>
<guid>https://arxiv.org/abs/2510.11866</guid>
<content:encoded><![CDATA[
arXiv:2510.11866v1 Announce Type: new 
Abstract: Decentralized storage is one of the most natural applications built on blockchains and a central component of the Web3 ecosystem. Yet despite a decade of active development -- from IPFS and Filecoin to more recent entrants -- most of these storage protocols have received limited formal analysis of their incentive properties. Claims of incentive compatibility are sometimes made, but rarely proven. This gap matters: without well-designed incentives, a system may distribute storage but fail to truly decentralize it.
  We analyze Shelby -- a storage network protocol recently proposed by Aptos Labs and Jump Crypto -- and provide the first formal proof of its incentive properties. Our game-theoretic model shows that while off-chain audits alone collapse to universal shirking, Shelby's combination of peer audits with occasional on-chain verification yields incentive compatibility under natural parameter settings. We also examine coalition behavior and outline a simple modification that strengthens the protocol's collusion-resilience.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stable and Fault-Tolerant Decentralized Traffic Engineering</title>
<link>https://arxiv.org/abs/2510.11937</link>
<guid>https://arxiv.org/abs/2510.11937</guid>
<content:encoded><![CDATA[
arXiv:2510.11937v1 Announce Type: new 
Abstract: Cloud providers have recently decentralized their wide-area network traffic engineering (TE) systems to contain the impact of TE controller failures. In the decentralized design, a controller fault only impacts its slice of the network, limiting the blast radius to a fraction of the network. However, we find that autonomous slice controllers can arrive at divergent traffic allocations that overload links by 30% beyond their capacity. We present Symphony, a decentralized TE system that addresses the challenge of divergence-induced congestion while preserving the fault-isolation benefits of decentralization. By augmenting TE objectives with quadratic regularization, Symphony makes traffic allocations robust to demand perturbations, ensuring TE controllers naturally converge to compatible allocations without coordination. In parallel, Symphony's randomized slicing algorithm partitions the network to minimize blast radius by distributing critical traffic sources across slices, preventing any single failure from becoming catastrophic. These innovations work in tandem: regularization ensures algorithmic stability to traffic allocations while intelligent slicing provides architectural resilience in the network. Through extensive evaluation on cloud provider WANs, we show Symphony reduces divergence-induced congestion by 14x and blast radius by 79% compared to current practice.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adding All Flavors: A Hybrid Random Number Generator for dApps and Web3</title>
<link>https://arxiv.org/abs/2510.12062</link>
<guid>https://arxiv.org/abs/2510.12062</guid>
<content:encoded><![CDATA[
arXiv:2510.12062v1 Announce Type: new 
Abstract: Random numbers play a vital role in many decentralized applications (dApps), such as gaming and decentralized finance (DeFi) applications.
  Existing random number provision mechanisms can be roughly divided into two categories, on-chain, and off-chain.
  On-chain approaches usually rely on the blockchain as the major input and all computations are done by blockchain nodes.
  The major risk for this type of method is that the input itself is susceptible to the adversary's influence.
  Off-chain approaches, as the name suggested, complete the generation without the involvement of blockchain nodes and share the result directly with a dApp.
  These mechanisms usually have a strong security assumption and high complexity.
  To mitigate these limitations and provide a framework that allows a dApp to balance different factors involved in random number generation, we propose a hybrid random number generation solution that leverages IoT devices equipped with trusted execution environment (TEE) as the randomness sources, and then utilizes a set of cryptographic tools to aggregate the multiple sources and obtain the final random number that can be consumed by the dApp.
  The new approach only needs one honest random source to guarantee the unbiasedness of the final random number and a user can configure the system to tolerate malicious participants who can refuse to respond to avoid unfavored results.
  We also provide a concrete construction that can further reduce the on-chain computation complexity to lower the cost of the solution in practice.
  We evaluate the computation and gas costs to demonstrate the effectiveness of the improvement.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VeilAudit: Breaking the Deadlock Between Privacy and Accountability Across Blockchains</title>
<link>https://arxiv.org/abs/2510.12153</link>
<guid>https://arxiv.org/abs/2510.12153</guid>
<content:encoded><![CDATA[
arXiv:2510.12153v1 Announce Type: new 
Abstract: Cross chain interoperability in blockchain systems exposes a fundamental tension between user privacy and regulatory accountability. Existing solutions enforce an all or nothing choice between full anonymity and mandatory identity disclosure, which limits adoption in regulated financial settings. We present VeilAudit, a cross chain auditing framework that introduces Auditor Only Linkability, which allows auditors to link transaction behaviors that originate from the same anonymous entity without learning its identity. VeilAudit achieves this with a user generated Linkable Audit Tag that embeds a zero knowledge proof to attest to its validity without exposing the user master wallet address, and with a special ciphertext that only designated auditors can test for linkage. To balance privacy and compliance, VeilAudit also supports threshold gated identity revelation under due process. VeilAudit further provides a mechanism for building reputation in pseudonymous environments, which enables applications such as cross chain credit scoring based on verifiable behavioral history. We formalize the security guarantees and develop a prototype that spans multiple EVM chains. Our evaluation shows that the framework is practical for today multichain environments.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedMMKT:Co-Enhancing a Server Text-to-Image Model and Client Task Models in Multi-Modal Federated Learning</title>
<link>https://arxiv.org/abs/2510.12254</link>
<guid>https://arxiv.org/abs/2510.12254</guid>
<content:encoded><![CDATA[
arXiv:2510.12254v1 Announce Type: new 
Abstract: Text-to-Image (T2I) models have demonstrated their versatility in a wide range of applications. However, adaptation of T2I models to specialized tasks is often limited by the availability of task-specific data due to privacy concerns. On the other hand, harnessing the power of rich multimodal data from modern mobile systems and IoT infrastructures presents a great opportunity. This paper introduces Federated Multi-modal Knowledge Transfer (FedMMKT), a novel framework that enables co-enhancement of a server T2I model and client task-specific models using decentralized multimodal data without compromising data privacy.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Runtime Composition in Dynamic System of Systems: A Systematic Review of Challenges, Solutions, Tools, and Evaluation Methods</title>
<link>https://arxiv.org/abs/2510.12616</link>
<guid>https://arxiv.org/abs/2510.12616</guid>
<content:encoded><![CDATA[
arXiv:2510.12616v1 Announce Type: new 
Abstract: Context: Modern Systems of Systems (SoSs) increasingly operate in dynamic environments (e.g., smart cities, autonomous vehicles) where runtime composition -- the on-the-fly discovery, integration, and coordination of constituent systems (CSs)--is crucial for adaptability. Despite growing interest, the literature lacks a cohesive synthesis of runtime composition in dynamic SoSs. Objective: This study synthesizes research on runtime composition in dynamic SoSs and identifies core challenges, solution strategies, supporting tools, and evaluation methods. Methods: We conducted a Systematic Literature Review (SLR), screening 1,774 studies published between 2019 and 2024 and selecting 80 primary studies for thematic analysis (TA). Results: Challenges fall into four categories: modeling and analysis, resilient operations, system orchestration, and heterogeneity of CSs. Solutions span seven areas: co-simulation and digital twins, semantic ontologies, integration frameworks, adaptive architectures, middleware, formal methods, and AI-driven resilience. Service-oriented frameworks for composition and integration dominate tooling, while simulation platforms support evaluation. Interoperability across tools, limited cross-toolchain workflows, and the absence of standardized benchmarks remain key gaps. Evaluation approaches include simulation-based, implementation-driven, and human-centered studies, which have been applied in domains such as smart cities, healthcare, defense, and industrial automation. Conclusions: The synthesis reveals tensions, including autonomy versus coordination, the modeling-reality gap, and socio-technical integration. It calls for standardized evaluation metrics, scalable decentralized architectures, and cross-domain frameworks. The analysis aims to guide researchers and practitioners in developing and implementing dynamically composable SoSs.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chronicles of Jockeying in Queuing Systems</title>
<link>https://arxiv.org/abs/2402.11061</link>
<guid>https://arxiv.org/abs/2402.11061</guid>
<content:encoded><![CDATA[
arXiv:2402.11061v5 Announce Type: replace 
Abstract: Emerging trends in communication systems, such as network softwarization, functional disaggregation, and multi-access edge computing (MEC), are reshaping both the infrastructural landscape and the application ecosystem. These transformations introduce new challenges for packet transmission, task offloading, and resource allocation under stringent service-level requirements. A key factor in this context is queue impatience, where waiting entities alter their behavior in response to delay. While balking and reneging have been widely studied, this survey focuses on the less explored but operationally significant phenomenon of jockeying, i.e. the switching of jobs or users between queues. Although a substantial body of literature models jockeying behavior, the diversity of approaches raises questions about their practical applicability in dynamic, distributed environments such as 5G and Beyond. This chronicle reviews and classifies these studies with respect to their methodologies, modeling assumptions, and use cases, with particular emphasis on communication systems and MEC scenarios. We argue that forthcoming architectural transformations in next-generation networks will render many existing jockeying models inapplicable. By highlighting emerging paradigms such as MEC, network slicing, and network function virtualization, we identify open challenges, including state dissemination, migration cost, and stability, that undermine classical assumptions. We further outline design principles and research directions, emphasizing hybrid architectures and decentralized decision making as foundations for re-conceptualizing impatience in next-generation communication systems.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChipmunkRing: A Practical Post-Quantum Ring Signature Scheme for Blockchain Applications</title>
<link>https://arxiv.org/abs/2510.09617</link>
<guid>https://arxiv.org/abs/2510.09617</guid>
<content:encoded><![CDATA[
arXiv:2510.09617v1 Announce Type: new 
Abstract: ChipmunkRing, a practical post-quantum ring signature construction tailored for blockchain environments. Building on our Chipmunk lattice-based cryptographic framework, this implementation delivers compact digital signatures ranging from 20.5 to 279.7KB, with rapid signing operations completing in 1.1-15.1ms and efficient validation processes requiring only 0.4-4.5ms for participant groups of 2-64 members. The cornerstone of our approach is Acorn Verification-a streamlined zero-knowledge protocol that supersedes the classical Fiat-Shamir methodology. This innovation enables linear O(n) authentication complexity using concise 96-byte cryptographic proofs per participant, yielding a remarkable 17.7x performance enhancement for 32-member rings when compared to conventional techniques. Our work includes rigorous mathematical security demonstrations confirming 112-bit post-quantum protection (NIST Level 1), extensive computational benchmarking, and comprehensive support for both standard anonymity sets and collaborative threshold constructions with flexible participation requirements.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Literature Review on Fundamental Technologies and Security Challenges in the Metaverse Platforms</title>
<link>https://arxiv.org/abs/2510.09621</link>
<guid>https://arxiv.org/abs/2510.09621</guid>
<content:encoded><![CDATA[
arXiv:2510.09621v1 Announce Type: new 
Abstract: The Metaverse utilizes emerging technologies such as Extended Reality (XR), Artificial Intelligence (AI), blockchain, and digital twins to provide an immersive and interactive virtual experience. As Metaverse continues to evolve, it bring a range of security and privacy threats, such as identity management, data governance, and user interactions. This survey aims to provide a comprehensive review of the enabling technologies for the Metaverse. It also aims to provide a thorough analysis of key vulnerabilities and threats that may compromise its sustainability and user safety. We perform a systematic literature review (SLR) to identify key vulnerabilities and their countermeasures in Metaverse platforms. Metaverse offers a much larger attack surface compared to conventional digital platforms. Immersive, decentralized, and permanent characteristics of the Metaverse generate new vulnerabilities. Although there are many countermeasures to these vulnerabilities, most of them are theoretical or have not been tested in real-world environments. Our review highlights current advancements, identifies research gaps, and outlines future directions to ensure a secure, resilient, and ethically governed Metaverse.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Transaction Tracing Techniques for Blockchain Systems</title>
<link>https://arxiv.org/abs/2510.09624</link>
<guid>https://arxiv.org/abs/2510.09624</guid>
<content:encoded><![CDATA[
arXiv:2510.09624v1 Announce Type: new 
Abstract: With the proliferation of new blockchain-based cryptocurrencies/assets and platforms that make it possible to transact across them, it becomes important to consider not just whether the transfer of coins/assets can be tracked within their respective transaction ledger, but also if they can be tracked as they move across ledgers. This is especially important given that there are documented cases of criminals attempting to use these cross-ledger trades to obscure the flow of their coins/assets. In this paper, we perform a systematic review of the various tracing techniques for blockchain transactions proposed in literature, categorize them using multiple criteria (such as tracing approach and targeted objective) and compare them. Based on the above categorization, we provide insights on the state of blockchain transaction tracing literature and identify the limitations of existing approaches. Finally, we suggest directions for future research in this area based on our analysis.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scalable, Privacy-Preserving Decentralized Identity and Verifiable Data Sharing Framework based on Zero-Knowledge Proofs</title>
<link>https://arxiv.org/abs/2510.09715</link>
<guid>https://arxiv.org/abs/2510.09715</guid>
<content:encoded><![CDATA[
arXiv:2510.09715v1 Announce Type: new 
Abstract: With the proliferation of decentralized applications (DApps), the conflict between the transparency of blockchain technology and user data privacy has become increasingly prominent. While Decentralized Identity (DID) and Verifiable Credentials (VCs) provide a standardized framework for user data sovereignty, achieving trusted identity verification and data sharing without compromising privacy remains a significant challenge. This paper proposes a novel, comprehensive framework that integrates DIDs and VCs with efficient Zero-Knowledge Proof (ZKP) schemes to address this core issue. The key contributions of this framework are threefold: first, it constructs a set of strong privacy-preserving protocols based on zk-STARKs, allowing users to prove that their credentials satisfy specific conditions (e.g., "age is over 18") without revealing any underlying sensitive data. Second, it designs a scalable, privacy-preserving credential revocation mechanism based on cryptographic accumulators, effectively solving credential management challenges in large-scale scenarios. Finally, it integrates a practical social key recovery scheme, significantly enhancing system usability and security. Through a prototype implementation and performance evaluation, this paper quantitatively analyzes the framework's performance in terms of proof generation time, verification overhead, and on-chain costs. Compared to existing state-of-the-art systems based on zk-SNARKs, our framework, at the cost of a larger proof size, significantly improves prover efficiency for complex computations and provides stronger security guarantees, including no trusted setup and post-quantum security. Finally, a case study in the decentralized finance (DeFi) credit scoring scenario demonstrates the framework's immense potential for unlocking capital efficiency and fostering a trusted data economy.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zk-SNARK Marketplace with Proof of Useful Work</title>
<link>https://arxiv.org/abs/2510.09729</link>
<guid>https://arxiv.org/abs/2510.09729</guid>
<content:encoded><![CDATA[
arXiv:2510.09729v1 Announce Type: new 
Abstract: Proof of Work (PoW) is widely regarded as the most secure permissionless blockchain consensus protocol. However, its reliance on computationally intensive yet externally useless puzzles results in excessive electric energy wasting. To alleviate this, Proof of Useful Work (PoUW) has been explored as an alternative to secure blockchain platforms while also producing real-world value. Despite this promise, existing PoUW proposals often fail to embed the integrity of the chain and identity of the miner into the puzzle solutions, not meeting necessary requirements for PoW and thus rendering them vulnerable. In this work, we propose a PoUW consensus protocol that computes client-outsourced zk-SNARKs proofs as a byproduct, which are at the same time used to secure the consensus protocol. We further leverage this mechanism to design a decentralized marketplace for outsourcing zk-SNARK proof generation, which is, to the best of our knowledge, the first such marketplace operating at the consensus layer, while meeting all necessary properties of PoW.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Farewell to Westphalia: Crypto Sovereignty and Post-Nation-State Governaance</title>
<link>https://arxiv.org/abs/2510.09840</link>
<guid>https://arxiv.org/abs/2510.09840</guid>
<content:encoded><![CDATA[
arXiv:2510.09840v1 Announce Type: new 
Abstract: We argue that the principal application for blockchain technology will not be in the financial sector, but rather in maintaining decentralized human governance, from archives to transparent policies encoded in the blockchain in the form of smart contracts.. Such decentralized, blockchain-grounded governance comes not a moment too soon, as nation states are dissolving before our eyes. Will blockchain-based communities replace the nation state? What are the prospects and dangers of this development?
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Cooperative Multi-Agent Reinforcement Learning: a Bayesian Network Perspective</title>
<link>https://arxiv.org/abs/2510.09937</link>
<guid>https://arxiv.org/abs/2510.09937</guid>
<content:encoded><![CDATA[
arXiv:2510.09937v1 Announce Type: new 
Abstract: The empirical success of multi-agent reinforcement learning (MARL) has motivated the search for more efficient and scalable algorithms for large scale multi-agent systems. However, existing state-of-the-art algorithms do not fully exploit inter-agent coupling information to develop MARL algorithms. In this paper, we propose a systematic approach to leverage structures in the inter-agent couplings for efficient model-free reinforcement learning. We model the cooperative MARL problem via a Bayesian network and characterize the subset of agents, termed as the value dependency set, whose information is required by each agent to estimate its local action value function exactly. Moreover, we propose a partially decentralized training decentralized execution (P-DTDE) paradigm based on the value dependency set. We theoretically establish that the total variance of our P-DTDE policy gradient estimator is less than the centralized training decentralized execution (CTDE) policy gradient estimator. We derive a multi-agent policy gradient theorem based on the P-DTDE scheme and develop a scalable actor-critic algorithm. We demonstrate the efficiency and scalability of the proposed algorithm on multi-warehouse resource allocation and multi-zone temperature control examples. For dense value dependency sets, we propose an approximation scheme based on truncation of the Bayesian network and empirically show that it achieves a faster convergence than the exact value dependence set for applications with a large number of agents.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-Quantum Cryptography and Quantum-Safe Security: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2510.10436</link>
<guid>https://arxiv.org/abs/2510.10436</guid>
<content:encoded><![CDATA[
arXiv:2510.10436v1 Announce Type: new 
Abstract: Post-quantum cryptography (PQC) is moving from evaluation to deployment as NIST finalizes standards for ML-KEM, ML-DSA, and SLH-DSA. This survey maps the space from foundations to practice. We first develop a taxonomy across lattice-, code-, hash-, multivariate-, isogeny-, and MPC-in-the-Head families, summarizing security assumptions, cryptanalysis, and standardization status. We then compare performance and communication costs using representative, implementation-grounded measurements, and review hardware acceleration (AVX2, FPGA/ASIC) and implementation security with a focus on side-channel resistance. Building upward, we examine protocol integration (TLS, DNSSEC), PKI and certificate hygiene, and deployment in constrained and high-assurance environments (IoT, cloud, finance, blockchain). We also discuss complementarity with quantum technologies (QKD, QRNGs) and the limits of near-term quantum computing. Throughout, we emphasize crypto-agility, hybrid migration, and evidence-based guidance for operators. We conclude with open problems spanning parameter agility, leakage-resilient implementations, and domain-specific rollout playbooks. This survey aims to be a practical reference for researchers and practitioners planning quantum-safe systems, bridging standards, engineering, and operations.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multitask Learning with Learned Task Relationships</title>
<link>https://arxiv.org/abs/2510.10570</link>
<guid>https://arxiv.org/abs/2510.10570</guid>
<content:encoded><![CDATA[
arXiv:2510.10570v1 Announce Type: new 
Abstract: Classical consensus-based strategies for federated and decentralized learning are statistically suboptimal in the presence of heterogeneous local data or task distributions. As a result, in recent years, there has been growing interest in multitask or personalized strategies, which allow individual agents to benefit from one another in pursuing locally optimal models without enforcing consensus. Existing strategies require either precise prior knowledge of the underlying task relationships or are fully non-parametric and instead rely on meta-learning or proximal constructions. In this work, we introduce an algorithmic framework that strikes a balance between these extremes. By modeling task relationships through a Gaussian Markov Random Field with an unknown precision matrix, we develop a strategy that jointly learns both the task relationships and the local models, allowing agents to self-organize in a way consistent with their individual data distributions. Our theoretical analysis quantifies the quality of the learned relationship, and our numerical experiments demonstrate its practical effectiveness.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toxic Ink on Immutable Paper: Content Moderation for Ethereum Input Data Messages (IDMs)</title>
<link>https://arxiv.org/abs/2510.10761</link>
<guid>https://arxiv.org/abs/2510.10761</guid>
<content:encoded><![CDATA[
arXiv:2510.10761v1 Announce Type: new 
Abstract: Decentralized communication is becoming an important use case within Web3. On Ethereum, users can repurpose the transaction input data field to embed natural-language messages, commonly known as Input Data Messages (IDMs). However, as IDMs gain wider adoption, there has been a growing volume of toxic content on-chain. This trend is concerning, as Ethereum provides no protocol-level support for content moderation.
  We propose two moderation frameworks for Ethereum IDMs: (i) BUILDERMOD, where builders perform semantic checks during block construction; and (ii) USERMOD, where users proactively obtain moderation proofs from external classifiers and embed them in transactions. Our evaluation reveals that BUILDERMOD incurs high block-time overhead, which limits its practicality. In contrast, USERMOD enables lower-latency validation and scales more effectively, making it a more practical approach in moderation-aware Ethereum environments.
  Our study lays the groundwork for protocol-level content governance in decentralized systems, and we hope it contributes to the development of a decentralized communication environment that is safe, trustworthy, and socially responsible.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stabilizing the Staking Rate, Dynamically Distributed Inflation and Delay Induced Oscillations</title>
<link>https://arxiv.org/abs/2510.11065</link>
<guid>https://arxiv.org/abs/2510.11065</guid>
<content:encoded><![CDATA[
arXiv:2510.11065v1 Announce Type: new 
Abstract: Dynamically distributed inflation is a common mechanism used to guide a blockchain's staking rate towards a desired equilibrium between network security and token liquidity.
  However, the high sensitivity of the annual percentage yield to changes in the staking rate, coupled with the inherent feedback delays in staker responses, can induce undesirable oscillations around this equilibrium.
  This paper investigates this instability phenomenon. We analyze the dynamics of inflation-based reward systems and propose a novel distribution model designed to stabilize the staking rate. Our solution effectively dampens oscillations, stabilizing the yield within a target staking range.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Decentralized Microservice Scheduling Approach Using Service Mesh in Cloud-Edge Systems</title>
<link>https://arxiv.org/abs/2510.11189</link>
<guid>https://arxiv.org/abs/2510.11189</guid>
<content:encoded><![CDATA[
arXiv:2510.11189v1 Announce Type: new 
Abstract: As microservice-based systems scale across the cloud-edge continuum, traditional centralized scheduling mechanisms increasingly struggle with latency, coordination overhead, and fault tolerance. This paper presents a new architectural direction: leveraging service mesh sidecar proxies as decentralized, in-situ schedulers to enable scalable, low-latency coordination in large-scale, cloud-native environments. We propose embedding lightweight, autonomous scheduling logic into each sidecar, allowing scheduling decisions to be made locally without centralized control. This approach leverages the growing maturity of service mesh infrastructures, which support programmable distributed traffic management. We describe the design of such an architecture and present initial results demonstrating its scalability potential in terms of response time and latency under varying request rates. Rather than delivering a finalized scheduling algorithm, this paper presents a system-level architectural direction and preliminary evidence to support its scalability potential.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fast-Converging Decentralized Approach to the Weighted Minimum Vertex Cover Problem</title>
<link>https://arxiv.org/abs/2510.11697</link>
<guid>https://arxiv.org/abs/2510.11697</guid>
<content:encoded><![CDATA[
arXiv:2510.11697v1 Announce Type: new 
Abstract: We address the problem of computing a Minimum Weighted Vertex Cover (MWVC) in a decentralized network. MWVC, a classical NP-hard problem, is foundational in applications such as network monitoring and resource placement. We propose a fully decentralized protocol where each node makes decisions using only local knowledge and communicates with its neighbors. The method is adaptive, communication-efficient, and avoids centralized coordination. We evaluate the protocol on real-world and synthetic graphs, comparing it to both centralized and decentralized baselines. Our results demonstrate competitive solution quality with reduced communication overhead, highlighting the feasibility of MWVC computation in decentralized environments.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exposing the Vulnerability of Decentralized Learning to Membership Inference Attacks Through the Lens of Graph Mixing</title>
<link>https://arxiv.org/abs/2412.12837</link>
<guid>https://arxiv.org/abs/2412.12837</guid>
<content:encoded><![CDATA[
arXiv:2412.12837v3 Announce Type: replace 
Abstract: The primary promise of decentralized learning is to allow users to engage in the training of machine learning models in a collaborative manner while keeping their data on their premises and without relying on any central entity. However, this paradigm necessitates the exchange of model parameters or gradients between peers. Such exchanges can be exploited to infer sensitive information about training data, which is achieved through privacy attacks (e.g., Membership Inference Attacks -- MIA). In order to devise effective defense mechanisms, it is important to understand the factors that increase/reduce the vulnerability of a given decentralized learning architecture to MIA. In this study, we extensively explore the vulnerability to MIA of various decentralized learning architectures by varying the graph structure (e.g., number of neighbors), the graph dynamics, and the aggregation strategy, across diverse datasets and data distributions. Our key finding, which to the best of our knowledge we are the first to report, is that the vulnerability to MIA is heavily correlated to (i) the local model mixing strategy performed by each node upon reception of models from neighboring nodes and (ii) the global mixing properties of the communication graph. We illustrate these results experimentally using four datasets and by theoretically analyzing the mixing properties of various decentralized architectures. We also empirically show that enhancing mixing properties is highly beneficial when combined with other privacy-preserving techniques such as Differential Privacy. Our paper draws a set of lessons learned for devising decentralized learning systems that reduce by design the vulnerability to MIA.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clustering Deposit and Withdrawal Activity in Tornado Cash: A Cross-Chain Analysis</title>
<link>https://arxiv.org/abs/2510.09433</link>
<guid>https://arxiv.org/abs/2510.09433</guid>
<content:encoded><![CDATA[
arXiv:2510.09433v2 Announce Type: replace 
Abstract: Tornado Cash is a decentralised mixer that uses cryptographic techniques to sever the on-chain trail between depositors and withdrawers. In practice, however, its anonymity can be undermined by user behaviour and operational quirks. We conduct the first cross-chain empirical study of Tornado Cash activity on Ethereum, BNB Smart Chain, and Polygon, introducing three clustering heuristics-(i) address-reuse, (ii) transactional-linkage, and (iii) a novel first-in-first-out (FIFO) temporal-matching rule. Together, these heuristics reconnect deposits to withdrawals and deanonymise a substantial share of recipients. Our analysis shows that 5.1 - 12.6% of withdrawals can already be traced to their originating deposits through address reuse and transactional linkage heuristics. Adding our novel First-In-First-Out (FIFO) temporal-matching heuristic lifts the linkage rate by a further 15 - 22 percentage points. Statistical tests confirm that these FIFO matches are highly unlikely to occur by chance. Comparable leakage across Ethereum, BNB Smart Chain, and Polygon indicates chain-agnostic user misbehaviour, rather than chain-specific protocol flaws. These results expose how quickly cryptographic guarantees can unravel in everyday use, underscoring the need for both disciplined user behaviour and privacy-aware protocol design. In total, our heuristics link over $2.3 billion in Tornado Cash withdrawals to identifiable deposits, exposing significant cracks in practical anonymity.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Impact of Sanctions on decentralised Privacy Tools: A Case Study of Tornado Cash</title>
<link>https://arxiv.org/abs/2510.09443</link>
<guid>https://arxiv.org/abs/2510.09443</guid>
<content:encoded><![CDATA[
arXiv:2510.09443v2 Announce Type: replace 
Abstract: This paper investigates the impact of sanctions on Tornado Cash, a smart contract protocol designed to enhance transaction privacy. Following the U.S. Department of the Treasury's sanctions against Tornado Cash in August 2022, platform activity declined sharply. We document a significant and sustained reduction in transaction volume, user diversity, and overall protocol utilization after the sanctions were imposed. Our analysis draws on transaction data from three major blockchains: Ethereum, BNB Smart Chain, and Polygon. We further examine developments following the partial lifting and eventual removal of sanctions by the U.S. Office of Foreign Assets Control (OFAC) in March 2025. Although activity partially recovered, the rebound remained limited. The Tornado Cash case illustrates how regulatory interventions can affect decentralized protocols, while also highlighting the challenges of fully enforcing such measures in decentralized environments.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible Swarm Learning May Outpace Foundation Models in Essential Tasks</title>
<link>https://arxiv.org/abs/2510.06349</link>
<guid>https://arxiv.org/abs/2510.06349</guid>
<content:encoded><![CDATA[
arXiv:2510.06349v1 Announce Type: new 
Abstract: Foundation models have rapidly advanced AI, raising the question of whether their decisions will ultimately surpass human strategies in real-world domains. The exponential, and possibly super-exponential, pace of AI development makes such analysis elusive. Nevertheless, many application areas that matter for daily life and society show only modest gains so far; a prominent case is diagnosing and treating dynamically evolving disease in intensive care.
  The common challenge is adapting complex systems to dynamic environments. Effective strategies must optimize outcomes in systems composed of strongly interacting functions while avoiding shared side effects; this requires reliable, self-adaptive modeling. These tasks align with building digital twins of highly complex systems whose mechanisms are not fully or quantitatively understood. It is therefore essential to develop methods for self-adapting AI models with minimal data and limited mechanistic knowledge. As this challenge extends beyond medicine, AI should demonstrate clear superiority in these settings before assuming broader decision-making roles.
  We identify the curse of dimensionality as a fundamental barrier to efficient self-adaptation and argue that monolithic foundation models face conceptual limits in overcoming it. As an alternative, we propose a decentralized architecture of interacting small agent networks (SANs). We focus on agents representing the specialized substructure of the system, where each agent covers only a subset of the full system functions. Drawing on mathematical results on the learning behavior of SANs and evidence from existing applications, we argue that swarm-learning in diverse swarms can enable self-adaptive SANs to deliver superior decision-making in dynamic environments compared with monolithic foundation models, though at the cost of reduced reproducibility in detail.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R3R: Decentralized Multi-Agent Collision Avoidance with Infinite-Horizon Safety</title>
<link>https://arxiv.org/abs/2510.06436</link>
<guid>https://arxiv.org/abs/2510.06436</guid>
<content:encoded><![CDATA[
arXiv:2510.06436v1 Announce Type: new 
Abstract: Existing decentralized methods for multi-agent motion planning lack formal, infinite-horizon safety guarantees, especially for communication-constrained systems. We present R3R, to our knowledge the first decentralized and asynchronous framework for multi-agent motion planning under distance-based communication constraints with infinite-horizon safety guarantees for systems of nonlinear agents. R3R's novelty lies in combining our gatekeeper safety framework with a geometric constraint called R-Boundedness, which together establish a formal link between an agent's communication radius and its ability to plan safely. We constrain trajectories to within a fixed planning radius that is a function of the agent's communication radius, which enables trajectories to be shown provably safe for all time, using only local information. Our algorithm is fully asynchronous, and ensures the forward invariance of these guarantees even in time-varying networks where agents asynchronously join, leave, and replan. We validate our approach in simulations of up to 128 Dubins vehicles, demonstrating 100% safety in dense, obstacle rich scenarios. Our results demonstrate that R3R's performance scales with agent density rather than problem size, providing a practical solution for scalable and provably safe multi-agent systems.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Aware Inference via Performance Forecasting in Decentralized Learning Networks</title>
<link>https://arxiv.org/abs/2510.06444</link>
<guid>https://arxiv.org/abs/2510.06444</guid>
<content:encoded><![CDATA[
arXiv:2510.06444v1 Announce Type: new 
Abstract: In decentralized learning networks, predictions from many participants are combined to generate a network inference. While many studies have demonstrated performance benefits of combining multiple model predictions, existing strategies using linear pooling methods (ranging from simple averaging to dynamic weight updates) face a key limitation. Dynamic prediction combinations that rely on historical performance to update weights are necessarily reactive. Due to the need to average over a reasonable number of epochs (with moving averages or exponential weighting), they tend to be slow to adjust to changing circumstances (phase or regime changes). In this work, we develop a model that uses machine learning to forecast the performance of predictions by models at each epoch in a time series. This enables `context-awareness' by assigning higher weight to models that are likely to be more accurate at a given time. We show that adding a performance forecasting worker in a decentralized learning network, following a design similar to the Allora network, can improve the accuracy of network inferences. Specifically, we find forecasting models that predict regret (performance relative to the network inference) or regret z-score (performance relative to other workers) show greater improvement than models predicting losses, which often do not outperform the naive network inference (historically weighted average of all inferences). Through a series of optimization tests, we show that the performance of the forecasting model can be sensitive to choices in the feature set and number of training epochs. These properties may depend on the exact problem and should be tailored to each domain. Although initially designed for a decentralized learning network, using performance forecasting for prediction combination may be useful in any situation where predictive rather than reactive model weighting is needed.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Fronthaul Quantization for Flexible User Load in Cell-Free Massive MIMO</title>
<link>https://arxiv.org/abs/2510.06734</link>
<guid>https://arxiv.org/abs/2510.06734</guid>
<content:encoded><![CDATA[
arXiv:2510.06734v1 Announce Type: new 
Abstract: We investigate the physical layer (PHY) spectral efficiency and fronthaul network load of a scalable user-centric cell-free massive MIMO system. Each user-centric cluster processor responsible for cluster-level signal processing is located at one of multiple decentralized units (DUs). Thus, the radio units in the cluster must exchange data with the corresponding DU over the fronthaul. Because the fronthaul links have limited capacity, this data must be quantized before it is sent over the fronthaul. We consider a routed fronthaul network, where the cluster processor placement and fronthaul traffic routing are jointly optimized with a mixed-integer linear program. For different numbers of users in the network, we investigate the effect of fronthaul quantization rates, a system parameter computed based on rate-distortion theory. Our results show that with optimized quantization rates, the fronthaul load is quite stable for a wide range of user loads without significant PHY performance loss. This demonstrates that the cell-free massive MIMO PHY and fronthaul network are resilient to varying user densities.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bionetta: Efficient Client-Side Zero-Knowledge Machine Learning Proving</title>
<link>https://arxiv.org/abs/2510.06784</link>
<guid>https://arxiv.org/abs/2510.06784</guid>
<content:encoded><![CDATA[
arXiv:2510.06784v1 Announce Type: new 
Abstract: In this report, we compare the performance of our UltraGroth-based zero-knowledge machine learning framework Bionetta to other tools of similar purpose such as EZKL, Lagrange's deep-prove, or zkml. The results show a significant boost in the proving time for custom-crafted neural networks: they can be proven even on mobile devices, enabling numerous client-side proving applications. While our scheme increases the cost of one-time preprocessing steps, such as circuit compilation and generating trusted setup, our approach is, to the best of our knowledge, the only one that is deployable on the native EVM smart contracts without overwhelming proof size and verification overheads.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized CBF-based Safety Filters for Collision Avoidance of Cooperative Missile Systems with Input Constraints</title>
<link>https://arxiv.org/abs/2510.06846</link>
<guid>https://arxiv.org/abs/2510.06846</guid>
<content:encoded><![CDATA[
arXiv:2510.06846v1 Announce Type: new 
Abstract: This paper presents a decentralized safety filter for collision avoidance in multi-agent aerospace interception scenarios. The approach leverages robust control barrier functions (RCBFs) to guarantee forward invariance of safety sets under bounded inputs and high-relative-degree dynamics. Each effector executes its nominal cooperative guidance command, while a local quadratic program (QP) modifies the input only when necessary. Event-triggered activation based on range and zero-effort miss (ZEM) criteria ensures scalability by restricting active constraints to relevant neighbors. To resolve feasibility issues from simultaneous constraints, a slack-variable relaxation scheme is introduced that prioritizes critical agents in a Pareto-optimal manner. Simulation results in many-on-many interception scenarios demonstrate that the proposed framework maintains collision-free operation with minimal deviation from nominal guidance, providing a computationally efficient and scalable solution for safety-critical multi-agent aerospace systems.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Unlearning in the Wild: Rethinking Fairness and Data Discrepancy</title>
<link>https://arxiv.org/abs/2510.07022</link>
<guid>https://arxiv.org/abs/2510.07022</guid>
<content:encoded><![CDATA[
arXiv:2510.07022v1 Announce Type: new 
Abstract: Machine unlearning is critical for enforcing data deletion rights like the "right to be forgotten." As a decentralized paradigm, Federated Learning (FL) also requires unlearning, but realistic implementations face two major challenges. First, fairness in Federated Unlearning (FU) is often overlooked. Exact unlearning methods typically force all clients into costly retraining, even those uninvolved. Approximate approaches, using gradient ascent or distillation, make coarse interventions that can unfairly degrade performance for clients with only retained data. Second, most FU evaluations rely on synthetic data assumptions (IID/non-IID) that ignore real-world heterogeneity. These unrealistic benchmarks obscure the true impact of unlearning and limit the applicability of current methods. We first conduct a comprehensive benchmark of existing FU methods under realistic data heterogeneity and fairness conditions. We then propose a novel, fairness-aware FU approach, Federated Cross-Client-Constrains Unlearning (FedCCCU), to explicitly address both challenges. FedCCCU offers a practical and scalable solution for real-world FU. Experimental results show that existing methods perform poorly in realistic settings, while our approach consistently outperforms them.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pseudo-MDPs: A Novel Framework for Efficiently Optimizing Last Revealer Seed Manipulations in Blockchains</title>
<link>https://arxiv.org/abs/2510.07080</link>
<guid>https://arxiv.org/abs/2510.07080</guid>
<content:encoded><![CDATA[
arXiv:2510.07080v1 Announce Type: new 
Abstract: This study tackles the computational challenges of solving Markov Decision Processes (MDPs) for a restricted class of problems. It is motivated by the Last Revealer Attack (LRA), which undermines fairness in some Proof-of-Stake (PoS) blockchains such as Ethereum (\$400B market capitalization). We introduce pseudo-MDPs (pMDPs) a framework that naturally models such problems and propose two distinct problem reductions to standard MDPs. One problem reduction provides a novel, counter-intuitive perspective, and combining the two problem reductions enables significant improvements in dynamic programming algorithms such as value iteration. In the case of the LRA which size is parameterized by $\kappa$ (in Ethereum's case $\kappa$ = 32), we reduce the computational complexity from O(2^$\kappa$ $\kappa$^2^($\kappa$+2)) to O($\kappa$^4) (per iteration). This solution also provide the usual benefits from Dynamic Programming solutions: exponentially fast convergence toward the optimal solution is guaranteed. The dual perspective also simplifies policy extraction, making the approach well-suited for resource-constrained agents who can operate with very limited memory and computation once the problem has been solved. Furthermore, we generalize those results to a broader class of MDPs, enhancing their applicability. The framework is validated through two case studies: a fictional card game and the LRA on the Ethereum random seed consensus protocol. These applications demonstrate the framework's ability to solve large-scale problems effectively while offering actionable insights into optimal strategies. This work advances the study of MDPs and contributes to understanding security vulnerabilities in blockchain systems.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RevealNet: Distributed Traffic Correlation for Attack Attribution on Programmable Networks</title>
<link>https://arxiv.org/abs/2505.00618</link>
<guid>https://arxiv.org/abs/2505.00618</guid>
<content:encoded><![CDATA[
arXiv:2505.00618v2 Announce Type: replace 
Abstract: Network attackers have increasingly resorted to proxy chains, VPNs, and anonymity networks to conceal their activities. To tackle this issue, past research has explored the applicability of traffic correlation techniques to perform attack attribution, i.e., to identify an attacker's true network location. However, current traffic correlation approaches rely on well-provisioned and centralized systems that ingest flows from multiple network probes to compute correlation scores. Unfortunately, this makes correlation efforts scale poorly for large high-speed networks.
  In this paper, we propose RevealNet, a decentralized framework for attack attribution that orchestrates a fleet of P4-programmable switches to perform traffic correlation. RevealNet builds on a set of correlation primitives inspired by prior work on computing and comparing flow sketches -- compact summaries of flows' key characteristics -- to enable efficient, distributed, in-network traffic correlation. Our evaluation suggests that RevealNet achieves comparable accuracy to centralized attack attribution systems while significantly reducing both the computational complexity and bandwidth overheads imposed by correlation tasks.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentZero++: Modeling Fear-Based Behavior</title>
<link>https://arxiv.org/abs/2510.05185</link>
<guid>https://arxiv.org/abs/2510.05185</guid>
<content:encoded><![CDATA[
arXiv:2510.05185v1 Announce Type: new 
Abstract: We present AgentZero++, an agent-based model that integrates cognitive, emotional, and social mechanisms to simulate decentralized collective violence in spatially distributed systems. Building on Epstein's Agent\_Zero framework, we extend the original model with eight behavioral enhancements: age-based impulse control; memory-based risk estimation; affect-cognition coupling; endogenous destructive radius; fight-or-flight dynamics; affective homophily; retaliatory damage; and multi-agent coordination. These additions allow agents to adapt based on internal states, previous experiences, and social feedback, producing emergent dynamics such as protest asymmetries, escalation cycles, and localized retaliation. Implemented in Python using the Mesa ABM framework, AgentZero++ enables modular experimentation and visualization of how micro-level cognitive heterogeneity shapes macro-level conflict patterns. Our results highlight how small variations in memory, reactivity, and affective alignment can amplify or dampen unrest through feedback loops. By explicitly modeling emotional thresholds, identity-driven behavior, and adaptive networks, this work contributes a flexible and extensible platform for analyzing affective contagion and psychologically grounded collective action.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constraint-Level Design of zkEVMs: Architectures, Trade-offs, and Evolution</title>
<link>https://arxiv.org/abs/2510.05376</link>
<guid>https://arxiv.org/abs/2510.05376</guid>
<content:encoded><![CDATA[
arXiv:2510.05376v1 Announce Type: new 
Abstract: Zero-knowledge Ethereum Virtual Machines (zkEVMs) must reconcile a fundamental contradiction: the Ethereum Virtual Machine was designed for transparent sequential execution, while zero-knowledge proofs require algebraic circuit representations. This survey provides the first systematic analysis of how existing major production zkEVM implementations resolve this tension through distinct constraint engineering strategies. We develop a comparative framework that maps the design space across three architectural dimensions. First, arithmetization schemes reveal stark trade-offs: R1CS requires compositional gadget libraries, PLONKish achieves elegance through custom gates that capture complex EVM opcodes in single constraints, while the homogeneous structure of AIR fundamentally mismatches the irregular instruction set of EVM. Second, dispatch mechanisms determine constraint activation patterns: selector-based systems waste trace width on inactive constraints, while ROM-based approaches trade memory lookups for execution flexibility. Third, the Type 1-4 spectrum quantifies an inescapable trade-off: the bit-level EVM compatibility of Type 1 demands significantly higher constraint complexity than the custom instruction sets of Type 4. Beyond cataloging implementations, we identify critical open problems across multiple domains: performance barriers preventing sub-second proving, absence of formal verification for constraint-to-EVM semantic equivalence, lack of standardized benchmarking frameworks, and architectural gaps in hybrid zkEVM/zkVM designs, decentralized prover coordination, privacy preservation, and interoperability.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanism design and equilibrium analysis of smart contract mediated resource allocation</title>
<link>https://arxiv.org/abs/2510.05504</link>
<guid>https://arxiv.org/abs/2510.05504</guid>
<content:encoded><![CDATA[
arXiv:2510.05504v1 Announce Type: new 
Abstract: Decentralized coordination and digital contracting are becoming critical in complex industrial ecosystems, yet existing approaches often rely on ad hoc heuristics or purely technical blockchain implementations without a rigorous economic foundation. This study develops a mechanism design framework for smart contract-based resource allocation that explicitly embeds efficiency and fairness in decentralized coordination. We establish the existence and uniqueness of contract equilibria, extending classical results in mechanism design, and introduce a decentralized price adjustment algorithm with provable convergence guarantees that can be implemented in real time. To evaluate performance, we combine extensive synthetic benchmarks with a proof-of-concept real-world dataset (MovieLens). The synthetic tests probe robustness under fee volatility, participation shocks, and dynamic demand, while the MovieLens case study illustrates how the mechanism can balance efficiency and fairness in realistic allocation environments. Results demonstrate that the proposed mechanism achieves substantial improvements in both efficiency and equity while remaining resilient to abrupt perturbations, confirming its stability beyond steady state analysis. The findings highlight broad managerial and policy relevance for supply chains, logistics, energy markets, healthcare resource allocation, and public infrastructure, where transparent and auditable coordination is increasingly critical. By combining theoretical rigor with empirical validation, the study shows how digital contracts can serve not only as technical artifacts but also as institutional instruments for transparency, accountability, and resilience in high-stakes resource allocation.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Correlation-Aware Dual-View Pose and Velocity Estimation for Dynamic Robotic Manipulation</title>
<link>https://arxiv.org/abs/2510.05536</link>
<guid>https://arxiv.org/abs/2510.05536</guid>
<content:encoded><![CDATA[
arXiv:2510.05536v1 Announce Type: new 
Abstract: Accurate pose and velocity estimation is essential for effective spatial task planning in robotic manipulators. While centralized sensor fusion has traditionally been used to improve pose estimation accuracy, this paper presents a novel decentralized fusion approach to estimate both pose and velocity. We use dual-view measurements from an eye-in-hand and an eye-to-hand vision sensor configuration mounted on a manipulator to track a target object whose motion is modeled as random walk (stochastic acceleration model). The robot runs two independent adaptive extended Kalman filters formulated on a matrix Lie group, developed as part of this work. These filters predict poses and velocities on the manifold $\mathbb{SE}(3) \times \mathbb{R}^3 \times \mathbb{R}^3$ and update the state on the manifold $\mathbb{SE}(3)$. The final fused state comprising the fused pose and velocities of the target is obtained using a correlation-aware fusion rule on Lie groups. The proposed method is evaluated on a UFactory xArm 850 equipped with Intel RealSense cameras, tracking a moving target. Experimental results validate the effectiveness and robustness of the proposed decentralized dual-view estimation framework, showing consistent improvements over state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving On-chain Permissioning for KYC-Compliant Decentralized Applications</title>
<link>https://arxiv.org/abs/2510.05807</link>
<guid>https://arxiv.org/abs/2510.05807</guid>
<content:encoded><![CDATA[
arXiv:2510.05807v1 Announce Type: new 
Abstract: Decentralized applications (dApps) in Decentralized Finance (DeFi) face a fundamental tension between regulatory compliance requirements like Know Your Customer (KYC) and maintaining decentralization and privacy. Existing permissioned DeFi solutions often fail to adequately protect private attributes of dApp users and introduce implicit trust assumptions, undermining the blockchain's decentralization. Addressing these limitations, this paper presents a novel synthesis of Self-Sovereign Identity (SSI), Zero-Knowledge Proofs (ZKPs), and Attribute-Based Access Control to enable privacy-preserving on-chain permissioning based on decentralized policy decisions. We provide a comprehensive framework for permissioned dApps that aligns decentralized trust, privacy, and transparency, harmonizing blockchain principles with regulatory compliance. Our framework supports multiple proof types (equality, range, membership, and time-dependent) with efficient proof generation through a commit-and-prove scheme that moves credential authenticity verification outside the ZKP circuit. Experimental evaluation of our KYC-compliant DeFi implementation shows considerable performance improvement for different proof types compared to baseline approaches. We advance the state-of-the-art through a holistic approach, flexible proof mechanisms addressing diverse real-world requirements, and optimized proof generation enabling practical deployment.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EARL: Efficient Agentic Reinforcement Learning Systems for Large Language Models</title>
<link>https://arxiv.org/abs/2510.05943</link>
<guid>https://arxiv.org/abs/2510.05943</guid>
<content:encoded><![CDATA[
arXiv:2510.05943v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has become a pivotal component of large language model (LLM) post-training, and agentic RL extends this paradigm to operate as agents through multi-turn interaction and tool use. Scaling such systems exposes two practical bottlenecks: (1) context length grows rapidly during training, inflating memory usage and latency, and triggering out-of-memory (OOM) failures; and (2) intermediate tensors accumulate with context length, making cross-device data movement a major system bottleneck.
  We present EARL, a scalable system for efficient agentic RL. EARL designs a parallelism selector that dynamically adapts model and training parallelism across RL stages based on sequence length and system load, and a data dispatcher that performs layout-aware, decentralized exchange of intermediate data batches. Together, these components increase throughput, reduce long-context failures, and enable stable large-scale training of agentic LLMs without relying on hard limits or penalties of context length.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Robot Distributed Optimization for Exploration and Mapping of Unknown Environments using Bioinspired Tactile-Sensor</title>
<link>https://arxiv.org/abs/2510.06085</link>
<guid>https://arxiv.org/abs/2510.06085</guid>
<content:encoded><![CDATA[
arXiv:2510.06085v1 Announce Type: new 
Abstract: This project proposes a bioinspired multi-robot system using Distributed Optimization for efficient exploration and mapping of unknown environments. Each robot explores its environment and creates a map, which is afterwards put together to form a global 2D map of the environment. Inspired by wall-following behaviors, each robot autonomously explores its neighborhood based on a tactile sensor, similar to the antenna of a cockroach, mounted on the surface of the robot. Instead of avoiding obstacles, robots log collision points when they touch obstacles. This decentralized control strategy ensures effective task allocation and efficient exploration of unknown terrains, with applications in search and rescue, industrial inspection, and environmental monitoring. The approach was validated through experiments using e-puck robots in a simulated 1.5 x 1.5 m environment with three obstacles. The results demonstrated the system's effectiveness in achieving high coverage, minimizing collisions, and constructing accurate 2D maps.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved High-probability Convergence Guarantees of Decentralized SGD</title>
<link>https://arxiv.org/abs/2510.06141</link>
<guid>https://arxiv.org/abs/2510.06141</guid>
<content:encoded><![CDATA[
arXiv:2510.06141v1 Announce Type: new 
Abstract: Convergence in high-probability (HP) has been receiving increasing interest, due to its attractive properties, such as exponentially decaying tail bounds and strong guarantees for each individual run of an algorithm. While HP guarantees are extensively studied in centralized settings, much less is understood in the decentralized, networked setup. Existing HP studies in decentralized settings impose strong assumptions, like uniformly bounded gradients, or asymptotically vanishing noise, resulting in a significant gap between assumptions used to establish convergence in the HP and the mean-squared error (MSE) sense, even for vanilla Decentralized Stochastic Gradient Descent ($\mathtt{DSGD}$) algorithm. This is contrary to centralized settings, where it is known that $\mathtt{SGD}$ converges in HP under the same conditions on the cost function as needed to guarantee MSE convergence. Motivated by this observation, we revisit HP guarantees for $\mathtt{DSGD}$ in the presence of light-tailed noise. We show that $\mathtt{DSGD}$ converges in HP under the same conditions on the cost as in the MSE sense, removing uniformly bounded gradients and other restrictive assumptions, while simultaneously achieving order-optimal rates for both non-convex and strongly convex costs. Moreover, our improved analysis yields linear speed-up in the number of users, demonstrating that $\mathtt{DSGD}$ maintains strong performance in the HP sense and matches existing MSE guarantees. Our improved results stem from a careful analysis of the MGF of quantities of interest (norm-squared of gradient or optimality gap) and the MGF of the consensus gap between users' models. To achieve linear speed-up, we provide a novel result on the variance-reduction effect of decentralized methods in the HP sense and more fine-grained bounds on the MGF for strongly convex costs, which are both of independent interest.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Coordination for Distributed Decision-Making</title>
<link>https://arxiv.org/abs/2401.16926</link>
<guid>https://arxiv.org/abs/2401.16926</guid>
<content:encoded><![CDATA[
arXiv:2401.16926v2 Announce Type: replace 
Abstract: In decentralized network control, communication plays a critical role by transforming local observations into shared knowledge, enabling agents to coordinate their actions. This paper investigates how communication facilitates cooperation behavior and therefore improves the overall performance in the vector-valued Witsenhausen counterexample, a canonical toy example in distributed decision-making. We consider setups where the encoder, i.e., the first decision-maker (DM) acts causally and the decoder, i.e., the second DM, operates noncausally, 1) without and 2) with access to channel feedback. Using a coordination coding framework, we characterize the achievable power-estimation cost regions in single-letter expressions for both scenarios. The first result is that, when restricted to Gaussian random variables, the cost is identical across all setups featuring at least one causal DM - regardless of the presence of feedback information. Next, building on the characterization of the power-estimation cost region, we propose a hybrid scheme that combines discrete quantization with a continuous Gaussian codebook - the Zero Estimation Cost (ZEC) scheme - which achieves an arbitrarily small estimation cost. This scheme uses coding tools that allow perfect reconstruction of the target symbols, leading to an asymptotic estimation cost equal to zero, while significantly reducing the asymptotic power consumption. Furthermore, when channel feedback is available at the first DM, we propose an analogous scheme that simultaneously achieves zero power and zero estimation cost in the low-noise regime.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Collective World Model for Emergent Communication and Coordination</title>
<link>https://arxiv.org/abs/2504.03353</link>
<guid>https://arxiv.org/abs/2504.03353</guid>
<content:encoded><![CDATA[
arXiv:2504.03353v3 Announce Type: replace 
Abstract: We propose a fully decentralized multi-agent world model that enables both symbol emergence for communication and coordinated behavior through temporal extension of collective predictive coding. Unlike previous research that focuses on either communication or coordination separately, our approach achieves both simultaneously. Our method integrates world models with communication channels, enabling agents to predict environmental dynamics, estimate states from partial observations, and share critical information through bidirectional message exchange with contrastive learning for message alignment. Using a two-agent trajectory drawing task, we demonstrate that our communication-based approach outperforms non-communicative models when agents have divergent perceptual capabilities, achieving the second-best coordination after centralized models. Importantly, our decentralized approach with constraints preventing direct access to other agents' internal states facilitates the emergence of more meaningful symbol systems that accurately reflect environmental states. These findings demonstrate the effectiveness of decentralized communication for supporting coordination while developing shared representations of the environment.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QLLM: Do We Really Need a Mixing Network for Credit Assignment in Multi-Agent Reinforcement Learning?</title>
<link>https://arxiv.org/abs/2504.12961</link>
<guid>https://arxiv.org/abs/2504.12961</guid>
<content:encoded><![CDATA[
arXiv:2504.12961v3 Announce Type: replace 
Abstract: Credit assignment has remained a fundamental challenge in multi-agent reinforcement learning (MARL). Previous studies have primarily addressed this issue through value decomposition methods under the centralized training with decentralized execution paradigm, where neural networks are utilized to approximate the nonlinear relationship between individual Q-values and the global Q-value. Although these approaches have achieved considerable success in various benchmark tasks, they still suffer from several limitations, including imprecise attribution of contributions, limited interpretability, and poor scalability in high-dimensional state spaces. To address these challenges, we propose a novel algorithm, \textbf{QLLM}, which facilitates the automatic construction of credit assignment functions using large language models (LLMs). Specifically, the concept of \textbf{TFCAF} is introduced, wherein the credit allocation process is represented as a direct and expressive nonlinear functional formulation. A custom-designed \textit{coder-evaluator} framework is further employed to guide the generation, verification, and refinement of executable code by LLMs, significantly mitigating issues such as hallucination and shallow reasoning during inference. Extensive experiments conducted on several standard MARL benchmarks demonstrate that the proposed method consistently outperforms existing state-of-the-art baselines. Moreover, QLLM exhibits strong generalization capability and maintains compatibility with a wide range of MARL algorithms that utilize mixing networks, positioning it as a promising and versatile solution for complex multi-agent scenarios.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Concept to Measurement: A Survey of How the Blockchain Trilemma Is Analyzed</title>
<link>https://arxiv.org/abs/2505.03768</link>
<guid>https://arxiv.org/abs/2505.03768</guid>
<content:encoded><![CDATA[
arXiv:2505.03768v4 Announce Type: replace 
Abstract: The blockchain trilemma highlights the difficulty of simultaneously achieving a high degree of decentralization (DoD), scalability, and security in blockchain systems. While numerous constructs and metrics have been proposed to analyze these subconcepts, existing guidance is fragmented and inconsistent, limiting comparability across studies. This lack of clarity hinders practitioners in identifying Pareto-optimal blockchain system designs that meet common non-functional requirements. We systematically reviewed literature on the blockchain trilemma and blockchain benchmarks to synthesize constructs and their operationalizations through metrics to analyze the trilemma's subconcepts. We identified 12 constructs, operationalized through 15 metrics, that capture DoD, scalability, and security. We explain how these constructs apply across different blockchain systems and provide a structured overview that supports benchmarking and blockchain system design. Beyond blockchain, the findings offer insights for distributed database systems that rely on consensus and state machine replication. This work contributes a harmonized foundation for quantitative analyses of the blockchain trilemma, guiding both researchers in developing analysis approaches and practitioners in evaluating real-world systems.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge-FIT: Federated Instruction Tuning of Quantized LLMs for Privacy-Preserving Smart Home Environments</title>
<link>https://arxiv.org/abs/2510.03284</link>
<guid>https://arxiv.org/abs/2510.03284</guid>
<content:encoded><![CDATA[
arXiv:2510.03284v1 Announce Type: new 
Abstract: This paper proposes Edge-FIT (Federated Instruction Tuning on the Edge), a scalable framework for Federated Instruction Tuning (FIT) of Large Language Models (LLMs). Traditional Federated Learning (TFL) methods, like FedAvg, fail when confronted with the massive parameter size of LLMs [3], [6]. Our Edge-FIT framework combines federated learning with 4-bit Quantized Low-Rank Adaptation (QLORA), mitigating the core issues of communication and computational overhead. We demonstrate this by filtering the general-purpose Databricks Dolly 15k dataset for the IoT domain. Experimental results show the Edge-FIT tuned Llama 2(7B) achieves an F1-Score of 0.89. We also demonstrate a viable trade-off using the 3.8B Phi-3-mini model, validating Edge-FIT as a scalable framework for decentralized LLM deployment on home compute gateways.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Adaptive Responsible AI Governance Framework for Decentralized Organizations</title>
<link>https://arxiv.org/abs/2510.03368</link>
<guid>https://arxiv.org/abs/2510.03368</guid>
<content:encoded><![CDATA[
arXiv:2510.03368v1 Announce Type: new 
Abstract: This paper examines the assessment challenges of Responsible AI (RAI) governance efforts in globally decentralized organizations through a case study collaboration between a leading research university and a multinational enterprise. While there are many proposed frameworks for RAI, their application in complex organizational settings with distributed decision-making authority remains underexplored. Our RAI assessment, conducted across multiple business units and AI use cases, reveals four key patterns that shape RAI implementation: (1) complex interplay between group-level guidance and local interpretation, (2) challenges translating abstract principles into operational practices, (3) regional and functional variation in implementation approaches, and (4) inconsistent accountability in risk oversight. Based on these findings, we propose an Adaptive RAI Governance (ARGO) Framework that balances central coordination with local autonomy through three interdependent layers: shared foundation standards, central advisory resources, and contextual local implementation. We contribute insights from academic-industry collaboration for RAI assessments, highlighting the importance of modular governance approaches that accommodate organizational complexity while maintaining alignment with responsible AI principles. These lessons offer practical guidance for organizations navigating the transition from RAI principles to operational practice within decentralized structures.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Robust Clustered Federated Learning Approach for Non-IID Data with Quantity Skew</title>
<link>https://arxiv.org/abs/2510.03380</link>
<guid>https://arxiv.org/abs/2510.03380</guid>
<content:encoded><![CDATA[
arXiv:2510.03380v1 Announce Type: new 
Abstract: Federated Learning (FL) is a decentralized paradigm that enables a client-server architecture to collaboratively train a global Artificial Intelligence model without sharing raw data, thereby preserving privacy. A key challenge in FL is Non-IID data. Quantity Skew (QS) is a particular problem of Non-IID, where clients hold highly heterogeneous data volumes. Clustered Federated Learning (CFL) is an emergent variant of FL that presents a promising solution to Non-IID problem. It improves models' performance by grouping clients with similar data distributions into clusters. CFL methods generally fall into two operating strategies. In the first strategy, clients select the cluster that minimizes the local training loss. In the second strategy, the server groups clients based on local model similarities. However, most CFL methods lack systematic evaluation under QS but present significant challenges because of it.  In this paper, we present two main contributions. The first one is an evaluation of state-of-the-art CFL algorithms under various Non-IID settings, applying multiple QS scenarios to assess their robustness. Our second contribution is a novel iterative CFL algorithm, named CORNFLQS, which proposes an optimal coordination between both operating strategies of CFL. Our approach is robust against the different variations of QS settings. We conducted intensive experiments on six image classification datasets, resulting in 270 Non-IID configurations. The results show that CORNFLQS achieves the highest average ranking in both accuracy and clustering quality, as well as strong robustness to QS perturbations. Overall, our approach outperforms actual CFL algorithms.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paris: A Decentralized Trained Open-Weight Diffusion Model</title>
<link>https://arxiv.org/abs/2510.03434</link>
<guid>https://arxiv.org/abs/2510.03434</guid>
<content:encoded><![CDATA[
arXiv:2510.03434v1 Announce Type: new 
Abstract: We present Paris, the first publicly released diffusion model pre-trained entirely through decentralized computation. Paris demonstrates that high-quality text-to-image generation can be achieved without centrally coordinated infrastructure. Paris is open for research and commercial use. Paris required implementing our Distributed Diffusion Training framework from scratch. The model consists of 8 expert diffusion models (129M-605M parameters each) trained in complete isolation with no gradient, parameter, or intermediate activation synchronization. Rather than requiring synchronized gradient updates across thousands of GPUs, we partition data into semantically coherent clusters where each expert independently optimizes its subset while collectively approximating the full distribution. A lightweight transformer router dynamically selects appropriate experts at inference, achieving generation quality comparable to centrally coordinated baselines. Eliminating synchronization enables training on heterogeneous hardware without specialized interconnects. Empirical validation confirms that Paris's decentralized training maintains generation quality while removing the dedicated GPU cluster requirement for large-scale diffusion models. Paris achieves this using 14$\times$ less training data and 16$\times$ less compute than the prior decentralized baseline.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A distributed routing protocol for sending data from things to the cloud leveraging fog technology in the large-scale IoT ecosystem</title>
<link>https://arxiv.org/abs/2510.03524</link>
<guid>https://arxiv.org/abs/2510.03524</guid>
<content:encoded><![CDATA[
arXiv:2510.03524v1 Announce Type: new 
Abstract: Fog computing integrates cloud and edge resources. According to an intelligent and decentralized method, this technology processes data generated by IoT sensors to seamlessly integrate physical and cyber environments. Internet of Things uses wireless and smart objects. They communicate with each other, monitor the environment, collect information, and respond to user requests. These objects have limited energy resources since they use batteries to supply energy. Also, they cannot replace their batteries. As a result, the network lifetime is limited and short. Thus, reducing energy consumption and accelerating the data transmission process are very important challenges in IoT networks to reduce the response time. In the data transmission process, selecting an appropriate cluster head node is very important because it can reduce the delay when sending data to the fog. In this paper, cluster head nodes are selected based on several important criteria such as distance, residual energy, received signal strength, and link expiration time. Then, objects send the processed data to the server hierarchically through a balanced tree. The simulation results show that the proposed method outperforms the energy-efficient centroid-based routing protocol (EECRP) and the Emergency Response IoT based on Global Information Decision (ERGID) in terms of packet delivery rate, delay, response time, and network lifetime.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning for Multi-Agent Coordination</title>
<link>https://arxiv.org/abs/2510.03592</link>
<guid>https://arxiv.org/abs/2510.03592</guid>
<content:encoded><![CDATA[
arXiv:2510.03592v1 Announce Type: new 
Abstract: We address the challenge of coordinating multiple robots in narrow and confined environments, where congestion and interference often hinder collective task performance. Drawing inspiration from insect colonies, which achieve robust coordination through stigmergy -- modifying and interpreting environmental traces -- we propose a Stigmergic Multi-Agent Deep Reinforcement Learning (S-MADRL) framework that leverages virtual pheromones to model local and social interactions, enabling decentralized emergent coordination without explicit communication. To overcome the convergence and scalability limitations of existing algorithms such as MADQN, MADDPG, and MAPPO, we leverage curriculum learning, which decomposes complex tasks into progressively harder sub-problems. Simulation results show that our framework achieves the most effective coordination of up to eight agents, where robots self-organize into asymmetric workload distributions that reduce congestion and modulate group performance. This emergent behavior, analogous to strategies observed in nature, demonstrates a scalable solution for decentralized multi-agent coordination in crowded environments with communication constraints.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Limits of Consensus under Dynamic Availability and Reconfiguration</title>
<link>https://arxiv.org/abs/2510.03625</link>
<guid>https://arxiv.org/abs/2510.03625</guid>
<content:encoded><![CDATA[
arXiv:2510.03625v1 Announce Type: new 
Abstract: Proof-of-stake blockchains require consensus protocols that support Dynamic Availability and Reconfiguration (so-called DAR setting), where the former means that the consensus protocol should remain live even if a large number of nodes temporarily crash, and the latter means it should be possible to change the set of operating nodes over time. State-of-the-art protocols for the DAR setting, such as Ethereum, Cardano's Ouroboros, or Snow White, require unrealistic additional assumptions, such as social consensus, or that key evolution is performed even while nodes are not participating. In this paper, we identify the necessary and sufficient adversarial condition under which consensus can be achieved in the DAR setting without additional assumptions. We then introduce a new and realistic additional assumption: honest nodes dispose of their cryptographic keys the moment they express intent to exit from the set of operating nodes. To add reconfiguration to any dynamically available consensus protocol, we provide a bootstrapping gadget that is particularly simple and efficient in the common optimistic case of few reconfigurations and no double-spending attempts.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Time-Bound Signature Scheme for Blockchains</title>
<link>https://arxiv.org/abs/2510.03697</link>
<guid>https://arxiv.org/abs/2510.03697</guid>
<content:encoded><![CDATA[
arXiv:2510.03697v1 Announce Type: new 
Abstract: We introduce a modified Schnorr signature scheme to allow for time-bound signatures for transaction fee auction bidding and smart contract purposes in a blockchain context, ensuring an honest producer can only validate a signature before a given block height. The immutable blockchain is used as a source of universal time for the signature scheme. We show the use of such a signature scheme leads to lower MEV revenue for builders. We then apply our time-bound signatures to Ethereum's EIP-1559 and show how it can be used to mitigate the effect of MEV on predicted equilibrium strategies.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Security Analysis of Ponzi Schemes in Ethereum Smart Contracts</title>
<link>https://arxiv.org/abs/2510.03819</link>
<guid>https://arxiv.org/abs/2510.03819</guid>
<content:encoded><![CDATA[
arXiv:2510.03819v1 Announce Type: new 
Abstract: The rapid advancement of blockchain technology has precipitated the widespread adoption of Ethereum and smart contracts across a variety of sectors. However, this has also given rise to numerous fraudulent activities, with many speculators embedding Ponzi schemes within smart contracts, resulting in significant financial losses for investors. Currently, there is a lack of effective methods for identifying and analyzing such new types of fraudulent activities. This paper categorizes these scams into four structural types and explores the intrinsic characteristics of Ponzi scheme contract source code from a program analysis perspective. The Mythril tool is employed to conduct static and dynamic analyses of representative cases, thereby revealing their vulnerabilities and operational mechanisms. Furthermore, this paper employs shell scripts and command patterns to conduct batch detection of open-source smart contract code, thereby unveiling the common characteristics of Ponzi scheme smart contracts.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Data Center Low-Voltage Ride-Through</title>
<link>https://arxiv.org/abs/2510.03867</link>
<guid>https://arxiv.org/abs/2510.03867</guid>
<content:encoded><![CDATA[
arXiv:2510.03867v1 Announce Type: new 
Abstract: Data center loads have expanded significantly in recent years. Compared to traditional loads, data centers are highly sensitive to voltage deviations and thus their protection mechanisms trip more proactively during voltage fluctuations. During a grid fault, simultaneous tripping of large-scale data centers can further destabilize the transmission system and even lead to cascading failures. In response, transmission system operators are imposing voltage ride-through (VRT) requirements for data centers. In this work, we enhance the VRT capability of data centers by designing voltage controllers for their internal power distribution network. We first systematically analyze VRT standards and the controllable resources related to data centers. These resources enable the design of voltage control strategies to regulate voltages internal to the data center, thereby allowing loads to remain online during voltage disturbances from the external transmission grid. We study and contrast both centralized and decentralized controllers that unify the control of heterogeneous flexible resources. Additionally, we construct an integrated test system that simulates both the transient fault response of the transmission system and the data center distribution network. Case studies demonstrate that the proposed voltage control mechanisms provide effective yet simple solutions to enhance data center low-voltage ride-through capability.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Does the Engineering Manager Still Exist in Agile Software Development?</title>
<link>https://arxiv.org/abs/2510.03920</link>
<guid>https://arxiv.org/abs/2510.03920</guid>
<content:encoded><![CDATA[
arXiv:2510.03920v1 Announce Type: new 
Abstract: Although Agile methodologies emphasize decentralized decision-making and team autonomy, engineering managers continue to be employed in Agile software organizations. This apparent paradox suggests that traditional managerial functions persist despite the theoretical displacement of managerial hierarchy in Agile. This paper explores the persistence of engineering managers through a multidimensional framework encompassing historical context, theoretical tensions, organizational realities, empirical evidence, evolving managerial roles, and practical implications. A systematic literature review underpins our multifaceted analysis, supplemented by illustrative case studies. We conclude by proposing a conceptual model that reconciles Agile principles with managerial necessity, offering guidance for practitioners, researchers, and tool designers. Implications for leadership development, tool integration, and future research are discussed.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cooperative Flexibility Exchange: Fair and Comfort-Aware Decentralized Resource Allocation</title>
<link>https://arxiv.org/abs/2510.04192</link>
<guid>https://arxiv.org/abs/2510.04192</guid>
<content:encoded><![CDATA[
arXiv:2510.04192v1 Announce Type: new 
Abstract: The growing electricity demand and increased use of smart appliances are placing new pressures on power grids, making efficient energy management more important than ever. The existing energy management systems often prioritize system efficiency (balanced energy demand and supply) at the expense of user comfort. This paper addresses this gap by proposing a novel decentralized multi-agent coordination-based demand-side management system. The proposed system enables individual agents to coordinate for demand-side energy optimization while improving the user comfort and maintaining the system efficiency. A key innovation of this work is the introduction of a slot exchange mechanism, where agents first receive optimized appliance-level energy consumption schedules and then coordinate with each other to adjust these schedules through slot exchanges. This approach improves user comfort even when agents show non-altruistic behaviour, and it scales well with large populations. The system also promotes fairness by balancing satisfaction levels across users. For performance evaluation, a real-world dataset is used, and the results demonstrate that the proposed slot exchange mechanism increases user comfort and fairness without raising system inefficiency cost, making it a practical and scalable solution for future smart grids.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedSRD: Sparsify-Reconstruct-Decompose for Communication-Efficient Federated Large Language Models Fine-Tuning</title>
<link>https://arxiv.org/abs/2510.04601</link>
<guid>https://arxiv.org/abs/2510.04601</guid>
<content:encoded><![CDATA[
arXiv:2510.04601v1 Announce Type: new 
Abstract: The current paradigm of training large language models (LLMs) on publicly available Web data is becoming unsustainable, with high-quality data sources in specialized domains nearing exhaustion. Federated Learning (FL) emerges as a practical solution for the next generation of AI on a decentralized Web, enabling privacy-preserving collaborative fine-tuning by leveraging private data distributed across a global client base. While Low-Rank Adaptation (LoRA) is the standard for efficient fine-tuning, its application in federated settings presents a critical challenge: communication overhead remains a significant bottleneck across the Web's heterogeneous network conditions. The structural redundancy within LoRA parameters not only incurs a heavy communication burden but also introduces conflicts when aggregating client updates. To address this, we propose FedSRD, a Sparsify-Reconstruct-Decompose framework designed for communication-efficient FL. We first introduce an importance-aware sparsification method that preserves the structural integrity of LoRA updates to reduce the uploaded parameter count. The server then reconstructs and aggregates these updates in a full-rank space to mitigate conflicts. Finally, it decomposes the global update into a sparse low-rank format for broadcast, ensuring a symmetrically efficient cycle. We also propose an efficient variant, FedSRD-e, to reduce computational overhead. Experimental results on 10 benchmarks demonstrate that our framework significantly reduces communication costs by up to 90\% while even improving model performance on heterogeneous client data.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PoS-CoPOR: Proof-of-Stake Consensus Protocol with Native Onion Routing Providing Scalability and DoS-Resistance</title>
<link>https://arxiv.org/abs/2510.04619</link>
<guid>https://arxiv.org/abs/2510.04619</guid>
<content:encoded><![CDATA[
arXiv:2510.04619v1 Announce Type: new 
Abstract: Proof-of-Stake (PoS) consensus protocols often face a trade-off between performance and security. Protocols that pre-elect leaders for subsequent rounds are vulnerable to Denial-of-Service (DoS) attacks, which can disrupt the network and compromise liveness. In this work, we present PoS-CoPOR, a single-chain PoS consensus protocol that mitigates this vulnerability by integrating a native onion routing mechanism into the consensus protocol itself. PoS-CoPOR combines stake-weighted probabilistic leader election with an anonymization layer that conceals the network identity of the next block proposer. This approach prevents targeted DoS attacks on leaders before they produce a block, thus enhancing network resilience. We implemented and evaluated PoS-CoPOR, demonstrating its ability to achieve a throughput of up to 110 tx/s with 6 nodes, even with the overhead of the anonymization layer. The results show that native anonymization can provide robust DoS resistance with only a modest impact on performance, offering a solution to build secure and scalable PoS blockchains.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impossible Cloud Network: A Decentralized Internet Infrastructure Layer</title>
<link>https://arxiv.org/abs/2510.04620</link>
<guid>https://arxiv.org/abs/2510.04620</guid>
<content:encoded><![CDATA[
arXiv:2510.04620v1 Announce Type: new 
Abstract: The internet faces a sovereignty crisis due to power concentration and data growth among a few hyperscalers, leading to centralization and loss of user control. This consolidation risks censorship and creates single points of failure. While Web3 offers decentralized solutions, they often sacrifice either scalability, decentralization, or security, which are key elements in the blockchain trilemma. These solutions also struggle with limited access to enterprise-grade hardware and frequently rely on centralized infrastructure. The Impossible Cloud Network (ICN) addresses these issues by creating a multi-tiered, decentralized infrastructure layer. ICN offers a composable service layer, an enterprise-grade hardware resource layer, and a transparent, permissionless HyperNode network for performance enforcement. By strategically decoupling and decentralizing each layer, ICN aims to provide an open, extensively scalable infrastructure that ensures digital sovereignty, eliminates single points of trust, enables service programmability, and offers a decoupled architecture for limitless possibilities in the future internet.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LMM-Incentive: Large Multimodal Model-based Incentive Design for User-Generated Content in Web 3.0</title>
<link>https://arxiv.org/abs/2510.04765</link>
<guid>https://arxiv.org/abs/2510.04765</guid>
<content:encoded><![CDATA[
arXiv:2510.04765v1 Announce Type: new 
Abstract: Web 3.0 represents the next generation of the Internet, which is widely recognized as a decentralized ecosystem that focuses on value expression and data ownership. By leveraging blockchain and artificial intelligence technologies, Web 3.0 offers unprecedented opportunities for users to create, own, and monetize their content, thereby enabling User-Generated Content (UGC) to an entirely new level. However, some self-interested users may exploit the limitations of content curation mechanisms and generate low-quality content with less effort, obtaining platform rewards under information asymmetry. Such behavior can undermine Web 3.0 performance. To this end, we propose \textit{LMM-Incentive}, a novel Large Multimodal Model (LMM)-based incentive mechanism for UGC in Web 3.0. Specifically, we propose an LMM-based contract-theoretic model to motivate users to generate high-quality UGC, thereby mitigating the adverse selection problem from information asymmetry. To alleviate potential moral hazards after contract selection, we leverage LMM agents to evaluate UGC quality, which is the primary component of the contract, utilizing prompt engineering techniques to improve the evaluation performance of LMM agents. Recognizing that traditional contract design methods cannot effectively adapt to the dynamic environment of Web 3.0, we develop an improved Mixture of Experts (MoE)-based Proximal Policy Optimization (PPO) algorithm for optimal contract design. Simulation results demonstrate the superiority of the proposed MoE-based PPO algorithm over representative benchmarks in the context of contract design. Finally, we deploy the designed contract within an Ethereum smart contract framework, further validating the effectiveness of the proposed scheme.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge</title>
<link>https://arxiv.org/abs/2510.04772</link>
<guid>https://arxiv.org/abs/2510.04772</guid>
<content:encoded><![CDATA[
arXiv:2510.04772v1 Announce Type: new 
Abstract: Purpose: The FedSurg challenge was designed to benchmark the state of the art in federated learning for surgical video classification. Its goal was to assess how well current methods generalize to unseen clinical centers and adapt through local fine-tuning while enabling collaborative model development without sharing patient data. Methods: Participants developed strategies to classify inflammation stages in appendicitis using a preliminary version of the multi-center Appendix300 video dataset. The challenge evaluated two tasks: generalization to an unseen center and center-specific adaptation after fine-tuning. Submitted approaches included foundation models with linear probing, metric learning with triplet loss, and various FL aggregation schemes (FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and Expected Cost, with ranking robustness evaluated via bootstrapping and statistical testing. Results: In the generalization task, performance across centers was limited. In the adaptation task, all teams improved after fine-tuning, though ranking stability was low. The ViViT-based submission achieved the strongest overall performance. The challenge highlighted limitations in generalization, sensitivity to class imbalance, and difficulties in hyperparameter tuning in decentralized training, while spatiotemporal modeling and context-aware preprocessing emerged as promising strategies. Conclusion: The FedSurg Challenge establishes the first benchmark for evaluating FL strategies in surgical video classification. Findings highlight the trade-off between local personalization and global robustness, and underscore the importance of architecture choice, preprocessing, and loss design. This benchmarking offers a reference point for future development of imbalance-aware, adaptive, and robust FL methods in clinical surgical AI.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Distributed Optimization With Feasible Set Privacy</title>
<link>https://arxiv.org/abs/2510.05068</link>
<guid>https://arxiv.org/abs/2510.05068</guid>
<content:encoded><![CDATA[
arXiv:2510.05068v1 Announce Type: new 
Abstract: We consider the problem of decentralized constrained optimization with multiple agents $E_1,\ldots,E_N$ who jointly wish to learn the optimal solution set while keeping their feasible sets $\mathcal{P}_1,\ldots,\mathcal{P}_N$ private from each other. We assume that the objective function $f$ is known to all agents and each feasible set is a collection of points from a universal alphabet $\mathcal{P}_{alph}$. A designated agent (leader) starts the communication with the remaining (non-leader) agents, and is the first to retrieve the solution set. The leader searches for the solution by sending queries to and receiving answers from the non-leaders, such that the information on the individual feasible sets revealed to the leader should be no more than nominal, i.e., what is revealed from learning the solution set alone. We develop achievable schemes for obtaining the solution set at nominal information leakage, and characterize their communication costs under two communication setups between agents. In this work, we focus on two kinds of network setups: i) ring, where each agent communicates with two adjacent agents, and ii) star, where only the leader communicates with the remaining agents. We show that, if the leader first learns the joint feasible set through an existing private set intersection (PSI) protocol and then deduces the solution set, the information leaked to the leader is greater than nominal. Moreover, we draw connection of our schemes to threshold PSI (ThPSI), which is a PSI-variant where the intersection is revealed only when its cardinality is larger than a threshold value. Finally, for various realizations of $f$ mapped uniformly at random to a fixed range of values, our schemes are more communication-efficient with a high probability compared to retrieving the entire feasible set through PSI.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position Paper: Assessing Robustness, Privacy, and Fairness in Federated Learning Integrated with Foundation Models</title>
<link>https://arxiv.org/abs/2402.01857</link>
<guid>https://arxiv.org/abs/2402.01857</guid>
<content:encoded><![CDATA[
arXiv:2402.01857v2 Announce Type: replace 
Abstract: Federated Learning (FL), while a breakthrough in decentralized machine learning, contends with significant challenges such as limited data availability and the variability of computational resources, which can stifle the performance and scalability of the models. The integration of Foundation Models (FMs) into FL presents a compelling solution to these issues, with the potential to enhance data richness and reduce computational demands through pre-training and data augmentation. However, this incorporation introduces novel issues in terms of robustness, privacy, and fairness, which have not been sufficiently addressed in the existing research. We make a preliminary investigation into this field by systematically evaluating the implications of FM-FL integration across these dimensions. We analyze the trade-offs involved, uncover the threats and issues introduced by this integration, and propose a set of criteria and strategies for navigating these challenges. Furthermore, we identify potential research directions for advancing this field, laying a foundation for future development in creating reliable, secure, and equitable FL systems.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proof-of-Data: A Consensus Protocol for Collaborative Intelligence</title>
<link>https://arxiv.org/abs/2501.02971</link>
<guid>https://arxiv.org/abs/2501.02971</guid>
<content:encoded><![CDATA[
arXiv:2501.02971v2 Announce Type: replace 
Abstract: Existing research on federated learning has been focused on the setting where learning is coordinated by a centralized entity. Yet the greatest potential of future collaborative intelligence would be unleashed in a more open and democratized setting with no central entity in a dominant role, referred to as "decentralized federated learning". New challenges arise accordingly in achieving both correct model training and fair reward allocation with collective effort among all participating nodes, especially with the threat of the Byzantine node jeopardising both tasks.
  In this paper, we propose a blockchain-based decentralized Byzantine fault-tolerant federated learning framework based on a novel Proof-of-Data (PoD) consensus protocol to resolve both the "trust" and "incentive" components. By decoupling model training and contribution accounting, PoD is able to enjoy not only the benefit of learning efficiency and system liveliness from asynchronous societal-scale PoW-style learning but also the finality of consensus and reward allocation from epoch-based BFT-style voting. To mitigate false reward claims by data forgery from Byzantine attacks, a privacy-aware data verification and contribution-based reward allocation mechanism is designed to complete the framework. Our evaluation results show that PoD demonstrates performance in model training close to that of the centralized counterpart while achieving trust in consensus and fairness for reward allocation with a fault tolerance ratio of 1/3.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exact and Linear Convergence for Federated Learning under Arbitrary Client Participation is Attainable</title>
<link>https://arxiv.org/abs/2503.20117</link>
<guid>https://arxiv.org/abs/2503.20117</guid>
<content:encoded><![CDATA[
arXiv:2503.20117v3 Announce Type: replace 
Abstract: This work tackles the fundamental challenges in Federated Learning (FL) posed by arbitrary client participation and data heterogeneity, prevalent characteristics in practical FL settings. It is well-established that popular FedAvg-style algorithms struggle with exact convergence and can suffer from slow convergence rates since a decaying learning rate is required to mitigate these scenarios. To address these issues, we introduce the concept of stochastic matrix and the corresponding time-varying graphs as a novel modeling tool to accurately capture the dynamics of arbitrary client participation and the local update procedure. Leveraging this approach, we offer a fresh decentralized perspective on designing FL algorithms and present FOCUS, Federated Optimization with Exact Convergence via Push-pull Strategy, a provably convergent algorithm designed to effectively overcome the previously mentioned two challenges. More specifically, we provide a rigorous proof demonstrating that FOCUS achieves exact convergence with a linear rate regardless of the arbitrary client participation, establishing it as the first work to demonstrate this significant result.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Spatiotemporal Graph Learning for Passive Attack Detection in Smart Grids</title>
<link>https://arxiv.org/abs/2510.02371</link>
<guid>https://arxiv.org/abs/2510.02371</guid>
<content:encoded><![CDATA[
arXiv:2510.02371v1 Announce Type: new 
Abstract: Smart grids are exposed to passive eavesdropping, where attackers listen silently to communication links. Although no data is actively altered, such reconnaissance can reveal grid topology, consumption patterns, and operational behavior, creating a gateway to more severe targeted attacks. Detecting this threat is difficult because the signals it produces are faint, short-lived, and often disappear when traffic is examined by a single node or along a single timeline. This paper introduces a graph-centric, multimodal detector that fuses physical-layer and behavioral indicators over ego-centric star subgraphs and short temporal windows to detect passive attacks. To capture stealthy perturbations, a two-stage encoder is introduced: graph convolution aggregates spatial context across ego-centric star subgraphs, while a bidirectional GRU models short-term temporal dependencies. The encoder transforms heterogeneous features into a unified spatio-temporal representation suitable for classification. Training occurs in a federated learning setup under FedProx, improving robustness to heterogeneous local raw data and contributing to the trustworthiness of decentralized training; raw measurements remain on client devices. A synthetic, standards-informed dataset is generated to emulate heterogeneous HAN/NAN/WAN communications with wireless-only passive perturbations, event co-occurrence, and leak-safe splits. The model achieves a testing accuracy of 98.32% per-timestep (F1_{attack}=0.972) and 93.35% per-sequence at 0.15% FPR using a simple decision rule with run-length m=2 and threshold $\tau=0.55$. The results demonstrate that combining spatial and temporal context enables reliable detection of stealthy reconnaissance while maintaining low false-positive rates, making the approach suitable for non-IID federated smart-grid deployments.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Modeling and Control System Analysis for Continuous-Disc Filters in Pulp Mill Operations</title>
<link>https://arxiv.org/abs/2510.02385</link>
<guid>https://arxiv.org/abs/2510.02385</guid>
<content:encoded><![CDATA[
arXiv:2510.02385v1 Announce Type: new 
Abstract: Vacuum disc filtration is critical in pulp mills for white liquor clarification and pulp washing, involving tightly coupled dynamics between rotational speed, vacuum pressure, slurry concentration, filtrate flow, and cake thickness. These nonlinear interactions are often regulated using empirical methods, lacking formal modeling and control. This article develops a dynamic, multivariable model of a continuous-disc filter (CD-filter) system based on first principles, simplified to a single representative disc for tractability. A linearized state-space model supports the design of two control strategies: a decentralized PI-based scheme and a centralized model predictive control (MPC). MATLAB-Simulink simulations reveal that MPC outperforms PI in tracking accuracy, overshoot reduction, and disturbance rejection. A 3D efficiency surface illustrates the importance of coordinating inlet flow and solids concentration. Results highlight the need for advanced multivariable control in optimizing CD-filter performance.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PolyLink: A Blockchain Based Decentralized Edge AI Platform for LLM Inference</title>
<link>https://arxiv.org/abs/2510.02395</link>
<guid>https://arxiv.org/abs/2510.02395</guid>
<content:encoded><![CDATA[
arXiv:2510.02395v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) in recent years has revolutionized the AI landscape. However, the deployment model and usage of LLM services remain highly centralized, creating significant trust issues and costs for end users and developers. To address these issues, we propose PolyLink, a blockchain-based decentralized AI platform that decentralizes LLM development and inference. Specifically, PolyLink introduces a decentralized crowdsourcing architecture that supports single-device and cross-device model deployment and inference across heterogeneous devices at the edge. Moreover, to ensure the inference integrity, we design the TIQE protocol, which combines a lightweight cross-encoder model and an LLM-as-a-Judge for a high-accuracy inference evaluation. Lastly, we integrate a comprehensive token-based incentive model with dynamic pricing and reward mechanisms for all participants. We have deployed PolyLink and conducted an extensive real-world evaluation through geo-distributed deployment across heterogeneous devices. Results indicate that the inference and verification latency is practical. Our security analysis demonstrates that the system is resistant to model degradation attacks and validator corruptions. PolyLink is now available at https://github.com/IMCL-PolyLink/PolyLink.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoK: Preconfirmations</title>
<link>https://arxiv.org/abs/2510.02947</link>
<guid>https://arxiv.org/abs/2510.02947</guid>
<content:encoded><![CDATA[
arXiv:2510.02947v1 Announce Type: new 
Abstract: In recent years, significant research efforts have focused on improving blockchain throughput and confirmation speeds without compromising security. While decreasing the time it takes for a transaction to be included in the blockchain ledger enhances user experience, a fundamental delay still remains between when a transaction is issued by a user and when its inclusion is confirmed in the blockchain ledger. This delay limits user experience gains through the confirmation uncertainty it brings for users. This inherent delay in conventional blockchain protocols has led to the emergence of preconfirmation protocols -- protocols that provide users with early guarantees of eventual transaction confirmation.
  This article presents a Systematization of Knowledge (SoK) on preconfirmations. We present the core terms and definitions needed to understand preconfirmations, outline a general framework for preconfirmation protocols, and explore the economics and risks of preconfirmations. Finally, we survey and apply our framework to several implementations of real-world preconfirmation protocols, bridging the gap between theory and practice.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pioneering Scalable Prototyping for Mid-Band XL-MIMO Systems: Design and Implementation</title>
<link>https://arxiv.org/abs/2510.02793</link>
<guid>https://arxiv.org/abs/2510.02793</guid>
<content:encoded><![CDATA[
arXiv:2510.02793v1 Announce Type: cross 
Abstract: The mid-band frequency range, combined with extra large-scale multiple-input multiple-output (XL-MIMO), is emerging as a key enabler for future communication systems. Thanks to the advent of new spectrum resources and degrees of freedom brought by the near-field propagation, the mid-band XL-MIMO system is expected to significantly enhance throughput and inherently support advanced functionalities such as integrated sensing and communication. Although theoretical studies have highlighted the benefits of mid-band XL-MIMO systems, the promised performance gains have yet to be validated in practical systems, posing a major challenge to the standardization. In this paper, preliminaries are first discussed, followed by an analysis of key challenges in constructing a real-time prototype system. Subsequently, the design and implementation of a real-time mid-band XL-MIMO prototype system are presented. Benefiting from the novel architecture, the proposed prototype system supports metrics aligned with standardization, including a bandwidth of 200 MHz, up to 1024 antenna elements, and up to 256 transceiver chains. Operating in time-division duplexing (TDD) mode, the prototype enables multiuser communication with support for up to 12 users, while retaining standard communication procedures. Built on software-defined radio (SDR) platforms, the system is programmable and allows for flexible deployment of advanced algorithms. Moreover, the modular architecture ensures high scalability, making the system adaptable to various configurations, including distributed deployments and decentralized signal processing. Experimental results with the proposed prototype system demonstrate real-time digital sample processing at 1167.85 Gbps, a peak data throughput of 15.81 Gbps for 12 users, and a maximal spectral efficiency approaching 80 bit/s/Hz.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proxima. A DAG based cooperative distributed ledger</title>
<link>https://arxiv.org/abs/2411.16456</link>
<guid>https://arxiv.org/abs/2411.16456</guid>
<content:encoded><![CDATA[
arXiv:2411.16456v5 Announce Type: replace 
Abstract: This paper introduces a novel architecture for a distributed ledger, commonly referred to as a "blockchain", which is organized in the form of directed acyclic graph (DAG) with UTXO transactions as vertices, rather than as a chain of blocks. Consensus on the state of ledger assets is achieved through the cooperative consensus: a profit-driven behavior of token holders themselves, which is viable only when they cooperate by following the "biggest ledger coverage rule", akin the "longest chain rule" of Bitcoin. The cooperative behavior is facilitated by enforcing purposefully designed UTXO transaction validity constraints. Token holders are the sole category of participants authorized to make amendments to the ledger, making participation completely permissionless - without miners, validators, committees or staking - and without any need of knowledge about the composition of the set of all participants in the consensus. The setup allows to achieve high throughput and scalability alongside with low transaction costs, while preserving key aspects of high decentralization, open participation, and asynchronicity found in Bitcoin and other proof-of-work blockchains, but without huge energy consumption. Sybil protection is achieved similarly to proof-of-stake blockchains, using tokens native to the ledger, yet the architecture operates in a leaderless manner without block proposers and committee selection.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ColNet: Collaborative Optimization in Decentralized Federated Multi-task Learning Systems</title>
<link>https://arxiv.org/abs/2501.10347</link>
<guid>https://arxiv.org/abs/2501.10347</guid>
<content:encoded><![CDATA[
arXiv:2501.10347v2 Announce Type: replace 
Abstract: The integration of Federated Learning (FL) and Multi-Task Learning (MTL) has been explored to address client heterogeneity, with Federated Multi-Task Learning (FMTL) treating each client as a distinct task. However, most existing research focuses on data heterogeneity (e.g., addressing non-IID data) rather than task heterogeneity, where clients solve fundamentally different tasks. Additionally, much of the work relies on centralized settings with a server managing the federation, leaving the more challenging domain of decentralized FMTL largely unexplored. Thus, this work bridges this gap by proposing ColNet, a framework designed for heterogeneous tasks in decentralized federated environments.
  ColNet partitions models into a backbone and task-specific heads, and uses adaptive clustering based on model and data sensitivity to form task-coherent client groups. Backbones are averaged within groups, and group leaders perform hyper-conflict-averse cross-group aggregation. Across datasets and federations, ColNet outperforms competing schemes under label and task heterogeneity and shows robustness to poisoning attacks.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinP: Fairness-in-Privacy in Federated Learning by Addressing Disparities in Privacy Risk</title>
<link>https://arxiv.org/abs/2502.17748</link>
<guid>https://arxiv.org/abs/2502.17748</guid>
<content:encoded><![CDATA[
arXiv:2502.17748v2 Announce Type: replace 
Abstract: Ensuring fairness in machine learning extends to the critical dimension of privacy, particularly in human-centric federated learning (FL) settings where decentralized data necessitates an equitable distribution of privacy risk across clients. This paper introduces FinP, a novel framework specifically designed to address disparities in privacy risk by mitigating disproportionate vulnerability to source inference attacks (SIA). FinP employs a two-pronged strategy: (1) server-side adaptive aggregation, which dynamically adjusts client contributions to the global model to foster fairness, and (2) client-side regularization, which enhances the privacy robustness of individual clients. This comprehensive approach directly tackles both the symptoms and underlying causes of privacy unfairness in FL. Extensive evaluations on the Human Activity Recognition (HAR) and CIFAR-10 datasets demonstrate FinP's effectiveness, achieving improvement in fairness-in-privacy on HAR and CIFAR-10 with minimal impact on utility. FinP improved group fairness with respect to disparity in privacy risk using equal opportunity in CIFAR-10 by 57.14% compared to the state-of-the-art. Furthermore, FinP significantly mitigates SIA risks on CIFAR-10, underscoring its potential to establish fairness in privacy within FL systems without compromising utility.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Odontoceti: Ultra-Fast DAG Consensus with Two Round Commitment</title>
<link>https://arxiv.org/abs/2510.01216</link>
<guid>https://arxiv.org/abs/2510.01216</guid>
<content:encoded><![CDATA[
arXiv:2510.01216v1 Announce Type: new 
Abstract: Users of blockchains value scalability, expecting fast confirmations and immediate transaction processing. Odontoceti, the latest in DAG-based consensus, addresses these concerns by prioritizing low latency and high throughput, making a strategic trade-off in security by operating with a 20% fault tolerance instead of the established 33% level. It is the first DAG-based protocol to achieve commitment in just two communication rounds, delivering median latency of 300 milliseconds while processing 10,000 transactions per second under realistic network conditions. Odontoceti operates with n = 5f + 1 validators and creates an uncertified DAG with a novel decision rule for committing blocks. The protocol includes an optimization that advances progress when participants are slow, benefiting crash fault scenarios which are more common in practice than Byzantine faults. Evaluation results demonstrate 20-25% latency improvements compared to an existing production protocol, validating that reducing wave length from three rounds to two rounds yields meaningful performance benefits. This paper establishes the practical viability of lower fault tolerance consensus protocols for blockchains.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Securing IoT Devices in Smart Cities: A Review of Proposed Solutions</title>
<link>https://arxiv.org/abs/2510.01445</link>
<guid>https://arxiv.org/abs/2510.01445</guid>
<content:encoded><![CDATA[
arXiv:2510.01445v1 Announce Type: new 
Abstract: Privacy and security in Smart Cities remain at constant risk due to the vulnerabilities introduced by Internet of Things (IoT) devices. The limited computational resources of these devices make them especially susceptible to attacks, while their widespread adoption increases the potential impact of security breaches. This article presents a review of security proposals aimed at protecting IoT devices in Smart City environments. The review was conducted by analyzing recent literature on device-level security, with particular emphasis on lightweight cryptography, physically unclonable functions (PUFs), and blockchain-based solutions. Findings highlight both the strengths and limitations of current approaches, as well as the need for more practical, scalable, and resource-efficient mechanisms to ensure user privacy and data protection in IoT ecosystems.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FOSS-chain: using blockchain for Open Source Software license compliance</title>
<link>https://arxiv.org/abs/2510.01740</link>
<guid>https://arxiv.org/abs/2510.01740</guid>
<content:encoded><![CDATA[
arXiv:2510.01740v1 Announce Type: new 
Abstract: Open Source Software (OSS) is widely used and carries licenses that indicate the terms under which the software is provided for use, also specifying modification and distribution rules. Ensuring that users are respecting OSS license terms when creating derivative works is a complex process. Compliance issues arising from incompatibilities among licenses may lead to legal disputes. At the same time, the blockchain technology with immutable entries offers a mechanism to provide transparency when it comes to licensing and ensure software changes are recorded. In this work, we are introducing an integration of blockchain and license management when creating derivative works, in order to tackle the issue of OSS license compatibility. We have designed, implemented and performed a preliminary evaluation of FOSS-chain, a web platform that uses blockchain and automates the license compliance process, covering 14 OSS licenses. We have evaluated the initial prototype version of the FOSS-chain platform via a small scale user study. Our preliminary results are promising, demonstrating the potential of the platform for adaptation on realistic software systems.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeMuon: A Decentralized Muon for Matrix Optimization over Graphs</title>
<link>https://arxiv.org/abs/2510.01377</link>
<guid>https://arxiv.org/abs/2510.01377</guid>
<content:encoded><![CDATA[
arXiv:2510.01377v1 Announce Type: cross 
Abstract: In this paper, we propose DeMuon, a method for decentralized matrix optimization over a given communication topology. DeMuon incorporates matrix orthogonalization via Newton-Schulz iterations-a technique inherited from its centralized predecessor, Muon-and employs gradient tracking to mitigate heterogeneity among local functions. Under heavy-tailed noise conditions and additional mild assumptions, we establish the iteration complexity of DeMuon for reaching an approximate stochastic stationary point. This complexity result matches the best-known complexity bounds of centralized algorithms in terms of dependence on the target tolerance. To the best of our knowledge, DeMuon is the first direct extension of Muon to decentralized optimization over graphs with provable complexity guarantees. We conduct preliminary numerical experiments on decentralized transformer pretraining over graphs with varying degrees of connectivity. Our numerical results demonstrate a clear margin of improvement of DeMuon over other popular decentralized algorithms across different network topologies.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LPAC: Learnable Perception-Action-Communication Loops with Applications to Coverage Control</title>
<link>https://arxiv.org/abs/2401.04855</link>
<guid>https://arxiv.org/abs/2401.04855</guid>
<content:encoded><![CDATA[
arXiv:2401.04855v4 Announce Type: replace 
Abstract: Coverage control is the problem of navigating a robot swarm to collaboratively monitor features or a phenomenon of interest not known a priori. The problem is challenging in decentralized settings with robots that have limited communication and sensing capabilities. We propose a learnable Perception-Action-Communication (LPAC) architecture for the problem, wherein a convolutional neural network (CNN) processes localized perception; a graph neural network (GNN) facilitates robot communications; finally, a shallow multi-layer perceptron (MLP) computes robot actions. The GNN enables collaboration in the robot swarm by computing what information to communicate with nearby robots and how to incorporate received information. Evaluations show that the LPAC models -- trained using imitation learning -- outperform standard decentralized and centralized coverage control algorithms. The learned policy generalizes to environments different from the training dataset, transfers to larger environments with more robots, and is robust to noisy position estimates. The results indicate the suitability of LPAC architectures for decentralized navigation in robot swarms to achieve collaborative behavior.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DFPL: Decentralized Federated Prototype Learning Across Heterogeneous Data Distributions</title>
<link>https://arxiv.org/abs/2505.04947</link>
<guid>https://arxiv.org/abs/2505.04947</guid>
<content:encoded><![CDATA[
arXiv:2505.04947v4 Announce Type: replace 
Abstract: Federated learning is a distributed machine learning paradigm through centralized model aggregation. However, standard federated learning relies on a centralized server, making it vulnerable to server failures. While existing solutions utilize blockchain technology to implement Decentralized Federated Learning (DFL), the statistical heterogeneity of data distributions among clients severely degrades the performance of DFL. Driven by this issue, this paper proposes a decentralized federated prototype learning framework, named DFPL, which significantly improves the performance of DFL under heterogeneous data distributions. Specifically, DFPL introduces prototype learning into DFL to mitigate the impact of statistical heterogeneity and reduces the amount of parameters exchanged between clients. Additionally, blockchain is embedded into our framework, enabling the training and mining processes to be executed locally on each client. From a theoretical perspective, we analyze the convergence of DFPL by modeling the required computational resources during both training and mining. The experiment results highlight the superiority of DFPL in both model performance and communication efficiency across four benchmark datasets with heterogeneous data distributions.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Cybersecurity Architecture for Digital Product Ecosystems Using Agentic AI</title>
<link>https://arxiv.org/abs/2509.20640</link>
<guid>https://arxiv.org/abs/2509.20640</guid>
<content:encoded><![CDATA[
arXiv:2509.20640v2 Announce Type: replace 
Abstract: Traditional static cybersecurity models often struggle with scalability, real-time detection, and contextual responsiveness in the current digital product ecosystems which include cloud services, application programming interfaces (APIs), mobile platforms, and edge devices. This study introduces autonomous goal driven agents capable of dynamic learning and context-aware decision making as part of an adaptive cybersecurity architecture driven by agentic artificial intelligence (AI). To facilitate autonomous threat mitigation, proactive policy enforcement, and real-time anomaly detection, this framework integrates agentic AI across the key ecosystem layers. Behavioral baselining, decentralized risk scoring, and federated threat intelligence sharing are important features. The capacity of the system to identify zero-day attacks and dynamically modify access policies was demonstrated through native cloud simulations. The evaluation results show increased adaptability, decreased response latency, and improved detection accuracy. The architecture provides an intelligent and scalable blueprint for safeguarding complex digital infrastructure and is compatible with zero-trust models, thereby supporting the adherence to international cybersecurity regulations.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdGT: Decentralized Gradient Tracking with Tuning-free Per-Agent Stepsize</title>
<link>https://arxiv.org/abs/2504.15196</link>
<guid>https://arxiv.org/abs/2504.15196</guid>
<content:encoded><![CDATA[
arXiv:2504.15196v2 Announce Type: replace-cross 
Abstract: In decentralized optimization, the choice of stepsize plays a critical role in algorithm performance. A common approach is to use a shared stepsize across all agents to ensure convergence. However, selecting an optimal stepsize often requires careful tuning, which can be time-consuming and may lead to slow convergence, especially when there is significant variation in the smoothness (L-smoothness) of local objective functions across agents. Individually tuning stepsizes per agent is also impractical, particularly in large-scale networks. To address these limitations, we propose AdGT, an adaptive gradient tracking method that enables each agent to adjust its stepsize based on the smoothness of its local objective. We prove that AdGT achieves linear convergence to the global optimal solution. Through numerical experiments, we compare AdGT with fixed-stepsize gradient tracking methods and demonstrate its superior performance. Additionally, we compare AdGT with adaptive gradient descent (AdGD) in a centralized setting and observe that fully adaptive stepsizes offer greater benefits in decentralized networks than in centralized ones.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Lead Themselves: Agentic AI in MAS using MARL</title>
<link>https://arxiv.org/abs/2510.00022</link>
<guid>https://arxiv.org/abs/2510.00022</guid>
<content:encoded><![CDATA[
arXiv:2510.00022v1 Announce Type: new 
Abstract: As autonomous systems move from prototypes to real deployments, the ability of multiple agents to make decentralized, cooperative decisions becomes a core requirement. This paper examines how agentic artificial intelligence, agents that act independently, adaptively and proactively can improve task allocation and coordination in multi-agent systems, with primary emphasis on drone delivery and secondary relevance to warehouse automation. We formulate the problem in a cooperative multi-agent reinforcement learning setting and implement a lightweight multi-agent Proximal Policy Optimization, called IPPO, approach in PyTorch under a centralized-training, decentralized-execution paradigm. Experiments are conducted in PettingZoo environment, where multiple homogeneous drones or agents must self-organize to cover distinct targets without explicit communication.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calyx: Privacy-Preserving Multi-Token Optimistic-Rollup Protocol</title>
<link>https://arxiv.org/abs/2510.00164</link>
<guid>https://arxiv.org/abs/2510.00164</guid>
<content:encoded><![CDATA[
arXiv:2510.00164v1 Announce Type: new 
Abstract: Rollup protocols have recently received significant attention as a promising class of Layer 2 (L2) scalability solutions. By utilizing the Layer 1 (L1) blockchain solely as a bulletin board for a summary of the executed transactions and state changes, rollups enable secure off-chain execution while avoiding the complexity of other L2 mechanisms. However, to ensure data availability, current rollup protocols require the plaintext of executed transactions to be published on-chain, resulting in inherent privacy limitations.
  In this paper, we address this problem by introducing Calyx, the first privacy-preserving multi-token optimistic-Rollup protocol. Calyx guarantees full payment privacy for all L2 transactions, revealing no information about the sender, recipient, transferred amount, or token type. The protocol further supports atomic execution of multiple multi-token transactions and introduces a transaction fee scheme to enable broader application scenarios while ensuring the sustainable operation of the protocol. To enforce correctness, Calyx adopts an efficient one-step fraud-proof mechanism. We analyze the security and privacy guarantees of the protocol and provide an implementation and evaluation. Our results show that executing a single transaction costs approximately $0.06 (0.00002 ETH) and incurs only constant-size on-chain cost in asymptotic terms.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lattica: A Decentralized Cross-NAT Communication Framework for Scalable AI Inference and Training</title>
<link>https://arxiv.org/abs/2510.00183</link>
<guid>https://arxiv.org/abs/2510.00183</guid>
<content:encoded><![CDATA[
arXiv:2510.00183v1 Announce Type: new 
Abstract: The rapid expansion of distributed Artificial Intelligence (AI) workloads beyond centralized data centers creates a demand for new communication substrates. These substrates must operate reliably in heterogeneous and permissionless environments, where Network Address Translators (NATs) and firewalls impose significant constraints. Existing solutions, however, are either designed for controlled data center deployments or implemented as monolithic systems that tightly couple machine learning logic with networking code. To address these limitations, we present Lattica, a decentralized cross-NAT communication framework designed to support distributed AI systems. Lattica integrates three core components. First, it employs a robust suite of NAT traversal mechanisms to establish a globally addressable peer-to-peer mesh. Second, it provides a decentralized data store based on Conflict-free Replicated Data Types (CRDTs), ensuring verifiable and eventually consistent state replication. Third, it incorporates a content discovery layer that leverages distributed hash tables (DHTs) together with an optimized RPC protocol for efficient model synchronization. By integrating these components, Lattica delivers a complete protocol stack for sovereign, resilient, and scalable AI systems that operate independently of centralized intermediaries. It is directly applicable to edge intelligence, collaborative reinforcement learning, and other large-scale distributed machine learning scenarios.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BlockSDN-VC: A SDN-Based Virtual Coordinate-Enhanced Transaction Broadcast Framework for High-Performance Blockchains</title>
<link>https://arxiv.org/abs/2510.00306</link>
<guid>https://arxiv.org/abs/2510.00306</guid>
<content:encoded><![CDATA[
arXiv:2510.00306v1 Announce Type: new 
Abstract: Modern blockchains need fast, reliable propagation to balance security and throughput. Virtual-coordinate methods speed dissemination but rely on slow iterative updates, leaving nodes out of sync. We present BlockSDN-VC, a transaction-broadcast protocol that centralises coordinate computation and forwarding control in an SDN controller, delivering global consistency, minimal path stretch and rapid response to churn or congestion. In geo-distributed simulations, BlockSDN-VC cuts median latency by up to 62% and accelerates convergence fourfold over state-of-the-art schemes with under 3% control-plane overhead. In a real blockchain environment, BlockSDN-VC boosts confirmed-transaction throughput by 17% under adversarial workloads, requiring no modifications to existing clients.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow of Knowledge: Federated Fine-Tuning of LLMs in Healthcare under Non-IID Conditions</title>
<link>https://arxiv.org/abs/2510.00543</link>
<guid>https://arxiv.org/abs/2510.00543</guid>
<content:encoded><![CDATA[
arXiv:2510.00543v1 Announce Type: new 
Abstract: Large language models (LLMs) show great promise in healthcare, but their applications are hindered by data privacy restrictions and the challenges of cross-institution collaboration. Sensitive medical data cannot be centralized, while non-independent and identically distributed (non-IID) characteristics across institutions further complicate convergence and fairness. To address these issues, we present a federated fine-tuning approach based on Low-Rank Adaptation (LoRA), enabling privacy-preserving knowledge flow across institutions. The method iteratively combines local LoRA adaptation with global parameter aggregation, allowing efficient knowledge sharing without exposing raw data. A blockchain identity scheme is used for identifying individual LLM in such a distributed network. We evaluate this approach on heterogeneous and highly non-IID medical text datasets, where experiments demonstrate that federated LoRA not only enhances cross-client generalization but also improves the performance of the weakest client, achieving stable convergence and fairer outcomes. These findings highlight federated LoRA fine-tuning as a practical and effective paradigm for adapting LLMs in healthcare, offering a new path for multi-center medical AI collaboration.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized and Self-adaptive Core Maintenance on Temporal Graphs</title>
<link>https://arxiv.org/abs/2510.00758</link>
<guid>https://arxiv.org/abs/2510.00758</guid>
<content:encoded><![CDATA[
arXiv:2510.00758v1 Announce Type: new 
Abstract: Key graph-based problems play a central role in understanding network topology and uncovering patterns of similarity in homogeneous and temporal data. Such patterns can be revealed by analyzing communities formed by nodes, which in turn can be effectively modeled through temporal $k$-cores. This paper introduces a novel decentralized and incremental algorithm for computing the core decomposition of temporal networks. Decentralized solutions leverage the ability of network nodes to communicate and coordinate locally, addressing complex problems in a scalable, adaptive, and timely manner. By leveraging previously computed coreness values, our approach significantly reduces the activation of nodes and the volume of message exchanges when the network changes over time. This enables scalability with only a minimal trade-off in precision. Experimental evaluations on large real-world networks under varying levels of dynamism demonstrate the efficiency of our solution compared to a state-of-the-art approach, particularly in terms of active nodes, communication overhead, and convergence speed.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Verifiable Federated Unlearning: Framework, Challenges, and The Road Ahead</title>
<link>https://arxiv.org/abs/2510.00833</link>
<guid>https://arxiv.org/abs/2510.00833</guid>
<content:encoded><![CDATA[
arXiv:2510.00833v1 Announce Type: new 
Abstract: Federated unlearning (FUL) enables removing the data influence from the model trained across distributed clients, upholding the right to be forgotten as mandated by privacy regulations. FUL facilitates a value exchange where clients gain privacy-preserving control over their data contributions, while service providers leverage decentralized computing and data freshness. However, this entire proposition is undermined because clients have no reliable way to verify that their data influence has been provably removed, as current metrics and simple notifications offer insufficient assurance. We envision unlearning verification becoming a pivotal and trust-by-design part of the FUL life-cycle development, essential for highly regulated and data-sensitive services and applications like healthcare. This article introduces veriFUL, a reference framework for verifiable FUL that formalizes verification entities, goals, approaches, and metrics. Specifically, we consolidate existing efforts and contribute new insights, concepts, and metrics to this domain. Finally, we highlight research challenges and identify potential applications and developments for verifiable FUL and veriFUL.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structuring Automotive Data for Systems Engineering: A Taxonomy-Based Approach</title>
<link>https://arxiv.org/abs/2510.00963</link>
<guid>https://arxiv.org/abs/2510.00963</guid>
<content:encoded><![CDATA[
arXiv:2510.00963v1 Announce Type: new 
Abstract: Vehicle data is essential for advancing data-driven development throughout the automotive lifecycle, including requirements engineering, design, verification, and validation, and post-deployment optimization. Developers currently collect data in a decentralized and fragmented manner across simulations, test benches, and real-world driving, resulting in data silos, inconsistent formats, and limited interoperability. This leads to redundant efforts, inefficient integration, and suboptimal use of data. This fragmentation results in data silos, inconsistent storage structures, and limited interoperability, leading to redundant data collection, inefficient integration, and suboptimal application. To address these challenges, this article presents a structured literature review and develops an inductive taxonomy for automotive data. This taxonomy categorizes data according to its sources and applications, improving data accessibility and utilization. The analysis reveals a growing emphasis on real-world driving and machine learning applications while highlighting a critical gap in data availability for requirements engineering. By providing a systematic framework for structuring automotive data, this research contributes to more efficient data management and improved decision-making in the automotive industry.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universally Composable Termination Analysis of Tendermint</title>
<link>https://arxiv.org/abs/2510.01097</link>
<guid>https://arxiv.org/abs/2510.01097</guid>
<content:encoded><![CDATA[
arXiv:2510.01097v1 Announce Type: new 
Abstract: Modern blockchain systems operating in adversarial environments require robust consensus protocols that guarantee both safety and termination under network delay attacks. Tendermint, a widely adopted consensus protocol in consortium blockchains, achieves high throughput and finality. However, previous analysis of the safety and termination has been done in a standalone fashion, with no consideration of the composition with other protocols interacting with it in a concurrent manner. Moreover, the termination properties under adaptive network delays caused by Byzantine adversaries have not been formally analyzed. This paper presents the first universally composable (UC) security analysis of Tendermint, demonstrating its resilience against strategic message-delay attacks. By constructing a UC ideal model of Tendermint, we formalize its core mechanisms: phase-base consensus procedure, dynamic timeouts, proposal locking, leader rotation, and others, under a network adversary that selectively delays protocol messages. Our main result proves that the Tendermint protocol UC-realizes the ideal Tendermint model, which ensures bounded termination latency, i.e., guaranteed termination, even when up to $f<n/3$ nodes are Byzantine (where $n$ is the number of nodes participating in the consensus), provided that network delays remain within a protocol-defined threshold under the partially synchronous net assumption. Specifically, through formal proofs within the UC framework, we show that Tendermint maintains safety and termination. By the composition theorem of UC, this guarantees that these properties are maintained when Tendermint is composed with various blockchain components.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy Preserved Federated Learning with Attention-Based Aggregation for Biometric Recognition</title>
<link>https://arxiv.org/abs/2510.01113</link>
<guid>https://arxiv.org/abs/2510.01113</guid>
<content:encoded><![CDATA[
arXiv:2510.01113v1 Announce Type: new 
Abstract: Because biometric data is sensitive, centralized training poses a privacy risk, even though biometric recognition is essential for contemporary applications. Federated learning (FL), which permits decentralized training, provides a privacy-preserving substitute. Conventional FL, however, has trouble with interpretability and heterogeneous data (non-IID). In order to handle non-IID biometric data, this framework adds an attention mechanism at the central server that weights local model updates according to their significance. Differential privacy and secure update protocols safeguard data while preserving accuracy. The A3-FL framework is evaluated in this study using FVC2004 fingerprint data, with each client's features extracted using a Siamese Convolutional Neural Network (Siamese-CNN). By dynamically modifying client contributions, the attention mechanism increases the accuracy of the global model.The accuracy, convergence speed, and robustness of the A3-FL framework are superior to those of standard FL (FedAvg) and static baselines, according to experimental evaluations using fingerprint data (FVC2004). The accuracy of the attention-based approach was 0.8413, while FedAvg, Local-only, and Centralized approaches were 0.8164, 0.7664, and 0.7997, respectively. Accuracy stayed high at 0.8330 even with differential privacy. A scalable and privacy-sensitive biometric system for secure and effective recognition in dispersed environments is presented in this work.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Networked Control and Mean Field Problems Under Diagonal Dominance: Decentralized and Social Optimality</title>
<link>https://arxiv.org/abs/2510.01067</link>
<guid>https://arxiv.org/abs/2510.01067</guid>
<content:encoded><![CDATA[
arXiv:2510.01067v1 Announce Type: cross 
Abstract: In this article, we employ an input-output approach to expand the study of cooperative multi-agent control and optimization problems characterized by mean-field interactions that admit decentralized and selfish solutions. The setting involves $n$ independent agents that interact solely through a shared cost function, which penalizes deviations of each agent from the group's average collective behavior. Building on our earlier results established for homogeneous agents, we extend the framework to nonidentical agents and show that, under a diagonal dominant interaction of the collective dynamics, with bounded local open-loop dynamics, the optimal controller for $H_\infty$ and $H_2$ norm minimization remains decentralized and selfish in the limit as the number of agents $n$ grows to infinity.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XRZoo: A Large-Scale and Versatile Dataset of Extended Reality (XR) Applications</title>
<link>https://arxiv.org/abs/2412.06759</link>
<guid>https://arxiv.org/abs/2412.06759</guid>
<content:encoded><![CDATA[
arXiv:2412.06759v3 Announce Type: replace 
Abstract: The rapid advancement of Extended Reality (XR, encompassing AR, MR, and VR) and spatial computing technologies forms a foundational layer for the emerging Metaverse, enabling innovative applications across healthcare, education, manufacturing, and entertainment. However, research in this area is often limited by the lack of large, representative, and highquality application datasets that can support empirical studies and the development of new approaches benefiting XR software processes. In this paper, we introduce XRZoo, a comprehensive and curated dataset of XR applications designed to bridge this gap. XRZoo contains 12,528 free XR applications, spanning nine app stores, across all XR techniques (i.e., AR, MR, and VR) and use cases, with detailed metadata on key aspects such as application descriptions, application categories, release dates, user review numbers, and hardware specifications, etc. By making XRZoo publicly available, we aim to foster reproducible XR software engineering and security research, enable cross-disciplinary investigations, and also support the development of advanced XR systems by providing examples to developers. Our dataset serves as a valuable resource for researchers and practitioners interested in improving the scalability, usability, and effectiveness of XR applications. XRZoo will be released and actively maintained.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Domain Shift in Federated Learning via Intra- and Inter-Domain Prototypes</title>
<link>https://arxiv.org/abs/2501.08521</link>
<guid>https://arxiv.org/abs/2501.08521</guid>
<content:encoded><![CDATA[
arXiv:2501.08521v3 Announce Type: replace 
Abstract: Federated Learning (FL) has emerged as a decentralized machine learning technique, allowing clients to train a global model collaboratively without sharing private data. However, most FL studies ignore the crucial challenge of heterogeneous domains where each client has a distinct feature distribution, which is popular in real-world scenarios. Prototype learning, which leverages the mean feature vectors within the same classes, has become a prominent solution for federated learning under domain shift. However, existing federated prototype learning methods focus soley on inter-domain prototypes and neglect intra-domain perspectives. In this work, we introduce a novel federated prototype learning method, namely I$^2$PFL, which incorporates $\textbf{I}$ntra-domain and $\textbf{I}$nter-domain $\textbf{P}$rototypes, to mitigate domain shift from both perspectives and learn a generalized global model across multiple domains in federated learning. To construct intra-domain prototypes, we propose feature alignment with MixUp-based augmented prototypes to capture the diversity within local domains and enhance the generalization of local features. Additionally, we introduce a reweighting mechanism for inter-domain prototypes to generate generalized prototypes that reduce domain shift while providing inter-domain knowledge across multiple clients. Extensive experiments on the Digits, Office-10, and PACS datasets illustrate the superior performance of our method compared to other baselines.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CYCle: Choosing Your Collaborators Wisely to Enhance Collaborative Fairness in Decentralized Learning</title>
<link>https://arxiv.org/abs/2501.12344</link>
<guid>https://arxiv.org/abs/2501.12344</guid>
<content:encoded><![CDATA[
arXiv:2501.12344v2 Announce Type: replace 
Abstract: Collaborative learning (CL) enables multiple participants to jointly train machine learning (ML) models on decentralized data sources without raw data sharing. While the primary goal of CL is to maximize the expected accuracy gain for each participant, it is also important to ensure that the gains are fairly distributed: no client should be negatively impacted, and gains should reflect contributions. Most existing CL methods require central coordination and focus only on gain maximization, overlooking fairness. In this work, we first show that the existing measure of collaborative fairness based on the correlation between accuracy values without and with collaboration has drawbacks because it does not account for negative collaboration gain. We argue that maximizing mean collaboration gain (MCG) while simultaneously minimizing the collaboration gain spread (CGS) is a fairer alternative. Next, we propose the CYCle protocol that enables individual participants in a private decentralized learning (PDL) framework to achieve this objective through a novel reputation scoring method based on gradient alignment between the local cross-entropy and distillation losses. We further extend the CYCle protocol to operate on top of gossip-based decentralized algorithms such as Gossip-SGD. We also theoretically show that CYCle performs better than standard FedAvg in a two-client mean estimation setting under high heterogeneity. Empirical experiments demonstrate the effectiveness of the CYCle protocol to ensure positive and fair collaboration gain for all participants, even in cases where the data distributions of participants are highly skewed.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noninterference Analysis of Irreversible or Reversible Systems with Nondeterminism and Probabilities</title>
<link>https://arxiv.org/abs/2501.19290</link>
<guid>https://arxiv.org/abs/2501.19290</guid>
<content:encoded><![CDATA[
arXiv:2501.19290v2 Announce Type: replace 
Abstract: Noninterference theory supports the analysis of secure computations in multi-level security systems. Classical equivalence-based approaches to noninterference mainly rely on bisimilarity. In a nondeterministic setting, assessing noninterference through weak bisimilarity is adequate for irreversible systems, whereas for reversible ones branching bisimilarity has been recently proven to be more appropriate. In this paper we address the same two families of systems, with the difference that probabilities come into play in addition to nondeterminism according to the alternating model of Hansson and Jonsson. For irreversible systems we extend the results of Aldini, Bravetti, and Gorrieri developed in a generative-reactive probabilistic setting, while for reversible systems we extend the results of Esposito, Aldini, Bernardo, and Rossi developed in a purely nondeterministic setting. We recast noninterference properties by adopting probabilistic variants of weak and branching bisimilarities for irreversible and reversible systems respectively. Then we investigate a taxonomy of those properties as well as their preservation and compositionality aspects, along with a comparison with the nondeterministic taxonomy. The adequacy of the extended noninterference theory is illustrated via a probabilistic smart contract lottery.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The challenge of hidden gifts in multi-agent reinforcement learning</title>
<link>https://arxiv.org/abs/2505.20579</link>
<guid>https://arxiv.org/abs/2505.20579</guid>
<content:encoded><![CDATA[
arXiv:2505.20579v5 Announce Type: replace 
Abstract: Sometimes we benefit from actions that others have taken even when we are unaware that they took those actions. For example, if your neighbor chooses not to take a parking spot in front of your house when you are not there, you can benefit, even without being aware that they took this action. These ``hidden gifts'' represent an interesting challenge for multi-agent reinforcement learning (MARL), since assigning credit when the beneficial actions of others are hidden is non-trivial. Here, we study the impact of hidden gifts with a very simple MARL task. In this task, agents in a grid-world environment have individual doors to unlock in order to obtain individual rewards. As well, if all the agents unlock their door the group receives a larger collective reward. However, there is only one key for all of the doors, such that the collective reward can only be obtained when the agents drop the key for others after they use it. Notably, there is nothing to indicate to an agent that the other agents have dropped the key, thus this act for others is a ``hidden gift''. We show that several different state-of-the-art MARL algorithms, including MARL specific architectures, fail to learn how to obtain the collective reward in this simple task. Interestingly, we find that decentralized actor-critic policy gradient agents can succeed when we provide them with information about their own action history, but MARL agents still cannot solve the task with action history. Finally, we derive a correction term for policy gradient agents, inspired by learning aware approaches, which reduces the variance in learning and helps them to converge to collective success more reliably. These results show that credit assignment in multi-agent settings can be particularly challenging in the presence of ``hidden gifts'', and demonstrate that self learning-awareness in decentralized agents can benefit these settings.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hitchhiker's Guide to Privacy-Preserving Digital Payment Systems: A Survey on Anonymity, Confidentiality, and Auditability</title>
<link>https://arxiv.org/abs/2505.21008</link>
<guid>https://arxiv.org/abs/2505.21008</guid>
<content:encoded><![CDATA[
arXiv:2505.21008v2 Announce Type: replace 
Abstract: Crypto-assets and central bank digital currencies (CBDCs) are reshaping how value is exchanged in distributed computing environments. These systems combine cryptographic primitives, protocol design, and system architectures to provide transparency and efficiency while raising critical challenges around privacy and regulatory compliance. This survey offers a comprehensive overview of privacy-preserving digital payment systems, covering both decentralized ledgers and CBDCs. We present a taxonomy of privacy goals -- including anonymity, confidentiality, unlinkability, and auditability -- and map them to underlying cryptographic primitives, protocols, and system architectures. Our work adopts a design-oriented perspective, linking high-level privacy objectives to concrete implementations. We also trace the evolution of privacy-preserving digital payment systems through three generations, highlighting shifts from basic anonymity guarantees toward more nuanced privacy-accountability trade-offs. Finally, we identify open challenges, motivating further research into architectures and solutions that balance strong privacy with real-world auditability needs.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blockchain-Based Secure Online Voting Platform Ensuring Voter Anonymity, Integrity, and End-to-End Verifiability</title>
<link>https://arxiv.org/abs/2509.22965</link>
<guid>https://arxiv.org/abs/2509.22965</guid>
<content:encoded><![CDATA[
arXiv:2509.22965v2 Announce Type: replace 
Abstract: Casting a ballot from a phone or laptop sounds appealing, but only if voters can be confident their choice remains secret and results cannot be altered in the dark. This paper proposes a hybrid blockchain-based voting model that stores encrypted votes on a private blockchain maintained by election organizers and neutral observers, while periodically anchoring hashes of these votes onto a public blockchain as a tamper-evident seal. The system issues voters one-time blind-signed tokens to protect anonymity, and provides receipts so they can confirm their vote was counted. We implemented a live prototype using common web technologies (Next.js, React, Firebase) to demonstrate end-to-end functionality, accessibility, and cost efficiency. Our contributions include developing a working demo, a complete election workflow, a hybrid blockchain design, and a user-friendly interface that balances privacy, security, transparency, and practicality. This research highlights the feasibility of secure, verifiable, and scalable online voting for organizations ranging from small groups to larger institutions.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Causal Inference from Multi-Site Observational Data via Propensity Score Aggregation</title>
<link>https://arxiv.org/abs/2505.17961</link>
<guid>https://arxiv.org/abs/2505.17961</guid>
<content:encoded><![CDATA[
arXiv:2505.17961v2 Announce Type: replace-cross 
Abstract: Causal inference typically assumes centralized access to individual-level data. Yet, in practice, data are often decentralized across multiple sites, making centralization infeasible due to privacy, logistical, or legal constraints. We address this problem by estimating the Average Treatment Effect (ATE) from decentralized observational data via a Federated Learning (FL) approach, allowing inference through the exchange of aggregate statistics rather than individual-level data. We propose a novel method to estimate propensity scores by computing a federated weighted average of local scores with Membership Weights (MW)--probabilities of site membership conditional on covariates--which can be flexibly estimated using parametric or non-parametric classification models. Unlike density ratio weights (DW) from the transportability and generalization literature, which either rely on strong modeling assumptions or cannot be implemented in FL, MW can be estimated using standard FL algorithms and are more robust, as they support flexible, non-parametric models--making them the preferred choice in multi-site settings with strict data-sharing constraints. The resulting propensity scores are used to construct Federated Inverse Propensity Weighting (Fed-IPW) and Augmented IPW (Fed-AIPW) estimators. Unlike meta-analysis methods, which fail when any site violates positivity, our approach leverages heterogeneity in treatment assignment across sites to improve overlap. We show that Fed-IPW and Fed-AIPW perform well under site-level heterogeneity in sample sizes, treatment mechanisms, and covariate distributions. Both theoretical analysis and experiments on simulated and real-world data highlight their advantages over meta-analysis and related methods.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Permuting Transactions in Ethereum Blocks: An Empirical Study</title>
<link>https://arxiv.org/abs/2509.25415</link>
<guid>https://arxiv.org/abs/2509.25415</guid>
<content:encoded><![CDATA[
arXiv:2509.25415v1 Announce Type: new 
Abstract: Several recent proposals implicitly or explicitly suggest making use of randomized transaction ordering within a block to mitigate centralization effects and to improve fairness in the Ethereum ecosystem. However, transactions and blocks are subject to gas limits and protocol rules. In a randomized transaction order, the behavior of transactions may change depending on other transactions in the same block, leading to invalid blocks and varying gas consumptions. In this paper, we quantify and characterize protocol violations, execution errors and deviations in gas consumption of blocks and transactions to examine technical deployability. For that, we permute and execute the transactions of over 335,000 Ethereum Mainnet blocks multiple times. About 22% of block permutations are invalid due to protocol violations caused by privately mined transactions or blocks close to their gas limit. Also, almost all transactions which show execution errors under permutation but not in the original order are privately mined transactions. Only 6% of transactions show deviations in gas consumption and 98% of block permutations deviate at most 10% from their original gas consumption. From a technical perspective, these results suggest that randomized transaction ordering may be feasible if transaction selection is handled carefully.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Interact in World Latent for Team Coordination</title>
<link>https://arxiv.org/abs/2509.25550</link>
<guid>https://arxiv.org/abs/2509.25550</guid>
<content:encoded><![CDATA[
arXiv:2509.25550v1 Announce Type: new 
Abstract: This work presents a novel representation learning framework, interactive world latent (IWoL), to facilitate team coordination in multi-agent reinforcement learning (MARL). Building effective representation for team coordination is a challenging problem, due to the intricate dynamics emerging from multi-agent interaction and incomplete information induced by local observations. Our key insight is to construct a learnable representation space that jointly captures inter-agent relations and task-specific world information by directly modeling communication protocols. This representation, we maintain fully decentralized execution with implicit coordination, all while avoiding the inherent drawbacks of explicit message passing, e.g., slower decision-making, vulnerability to malicious attackers, and sensitivity to bandwidth constraints. In practice, our representation can be used not only as an implicit latent for each agent, but also as an explicit message for communication. Across four challenging MARL benchmarks, we evaluate both variants and show that IWoL provides a simple yet powerful key for team coordination. Moreover, we demonstrate that our representation can be combined with existing MARL algorithms to further enhance their performance.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Split Learning with Sharded and Blockchain-Enabled SplitFed Approaches</title>
<link>https://arxiv.org/abs/2509.25555</link>
<guid>https://arxiv.org/abs/2509.25555</guid>
<content:encoded><![CDATA[
arXiv:2509.25555v1 Announce Type: new 
Abstract: Collaborative and distributed learning techniques, such as Federated Learning (FL) and Split Learning (SL), hold significant promise for leveraging sensitive data in privacy-critical domains. However, FL and SL suffer from key limitations -- FL imposes substantial computational demands on clients, while SL leads to prolonged training times. To overcome these challenges, SplitFed Learning (SFL) was introduced as a hybrid approach that combines the strengths of FL and SL. Despite its advantages, SFL inherits scalability, performance, and security issues from SL. In this paper, we propose two novel frameworks: Sharded SplitFed Learning (SSFL) and Blockchain-enabled SplitFed Learning (BSFL). SSFL addresses the scalability and performance constraints of SFL by distributing the workload and communication overhead of the SL server across multiple parallel shards. Building upon SSFL, BSFL replaces the centralized server with a blockchain-based architecture that employs a committee-driven consensus mechanism to enhance fairness and security. BSFL incorporates an evaluation mechanism to exclude poisoned or tampered model updates, thereby mitigating data poisoning and model integrity attacks. Experimental evaluations against baseline SL and SFL approaches show that SSFL improves performance and scalability by 31.2% and 85.2%, respectively. Furthermore, BSFL increases resilience to data poisoning attacks by 62.7% while maintaining superior performance under normal operating conditions. To the best of our knowledge, BSFL is the first blockchain-enabled framework to implement an end-to-end decentralized SplitFed Learning system.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight and Robust Federated Data Valuation</title>
<link>https://arxiv.org/abs/2509.25560</link>
<guid>https://arxiv.org/abs/2509.25560</guid>
<content:encoded><![CDATA[
arXiv:2509.25560v1 Announce Type: new 
Abstract: Federated learning (FL) faces persistent robustness challenges due to non-IID data distributions and adversarial client behavior. A promising mitigation strategy is contribution evaluation, which enables adaptive aggregation by quantifying each client's utility to the global model. However, state-of-the-art Shapley-value-based approaches incur high computational overhead due to repeated model reweighting and inference, which limits their scalability. We propose FedIF, a novel FL aggregation framework that leverages trajectory-based influence estimation to efficiently compute client contributions. FedIF adapts decentralized FL by introducing normalized and smoothed influence scores computed from lightweight gradient operations on client updates and a public validation set. Theoretical analysis demonstrates that FedIF yields a tighter bound on one-step global loss change under noisy conditions. Extensive experiments on CIFAR-10 and Fashion-MNIST show that FedIF achieves robustness comparable to or exceeding SV-based methods in the presence of label noise, gradient noise, and adversarial samples, while reducing aggregation overhead by up to 450x. Ablation studies confirm the effectiveness of FedIF's design choices, including local weight normalization and influence smoothing. Our results establish FedIF as a practical, theoretically grounded, and scalable alternative to Shapley-value-based approaches for efficient and robust FL in real-world deployments.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero Trust-based Decentralized Identity Management System for Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2509.25566</link>
<guid>https://arxiv.org/abs/2509.25566</guid>
<content:encoded><![CDATA[
arXiv:2509.25566v1 Announce Type: new 
Abstract: The rise of autonomous vehicles (AVs) promises to significantly enhance transportation safety and efficiency by mitigating human error, which is responsible for over 90\% of road accidents. However, the increasing connectivity of AVs introduces new cybersecurity challenges, as traditional perimeter-based security models are inadequate for dynamic and untrusted environments. This paper presents a novel Zero Trust-based Decentralized Identity Management (D-IM) protocol for AVs. By integrating the core principles of Zero Trust Architecture, "never trust, always verify", with the tamper resistant and decentralized nature of a blockchain network, our framework eliminates reliance on centralized authorities and provides continuous verification for every entity. We detail the system's design, which leverages Hyperledger Iroha to enable lightweight and secure authentication without a central trusted entity. A comprehensive experimental evaluation, conducted across both urban and highway scenarios, validates the protocol's practicality. Our results demonstrate that the D-IM framework introduces minimal overhead, with less than 7.5\% reduction in Packet Reception Rate (PRR) in urban settings and an increase of under 11\% in Channel Busy Ratio (CBR) for LTE-V2X. These findings prove the protocol's efficiency and robustness, providing a resilient foundation for securing real-time V2X communication against impersonation and replay attacks.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Asynchronous Multi-player Bandits</title>
<link>https://arxiv.org/abs/2509.25824</link>
<guid>https://arxiv.org/abs/2509.25824</guid>
<content:encoded><![CDATA[
arXiv:2509.25824v1 Announce Type: new 
Abstract: In recent years, multi-player multi-armed bandits (MP-MAB) have been extensively studied due to their wide applications in cognitive radio networks and Internet of Things systems. While most existing research on MP-MAB focuses on synchronized settings, real-world systems are often decentralized and asynchronous, where players may enter or leave the system at arbitrary times, and do not have a global clock. This decentralized asynchronous setting introduces two major challenges. First, without a global time, players cannot implicitly coordinate their actions through time, making it difficult to avoid collisions. Second, it is important to detect how many players are in the system, but doing so may cost a lot. In this paper, we address the challenges posed by such a fully asynchronous setting in a decentralized environment. We develop a novel algorithm in which players adaptively change between exploration and exploitation. During exploration, players uniformly pull their arms, reducing the probability of collisions and effectively mitigating the first challenge. Meanwhile, players continue pulling arms currently exploited by others with a small probability, enabling them to detect when a player has left, thereby addressing the second challenge. We prove that our algorithm achieves a regret of $\mathcal{O}(\sqrt{T \log T} + {\log T}/{\Delta^2})$, where $\Delta$ is the minimum expected reward gap between any two arms. To the best of our knowledge, this is the first efficient MP-MAB algorithm in the asynchronous and decentralized environment. Extensive experiments further validate the effectiveness and robustness of our algorithm, demonstrating its applicability to real-world scenarios.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Achieving Pareto Optimality in Games via Single-bit Feedback</title>
<link>https://arxiv.org/abs/2509.25921</link>
<guid>https://arxiv.org/abs/2509.25921</guid>
<content:encoded><![CDATA[
arXiv:2509.25921v1 Announce Type: new 
Abstract: Efficient coordination in multi-agent systems often incurs high communication overhead or slow convergence rates, making scalable welfare optimization difficult. We propose Single-Bit Coordination Dynamics for Pareto-Efficient Outcomes (SBC-PE), a decentralized learning algorithm requiring only a single-bit satisfaction signal per agent each round. Despite this extreme efficiency, SBC-PE guarantees convergence to the exact optimal solution in arbitrary finite games. We establish explicit regret bounds, showing expected regret grows only logarithmically with the horizon, i.e., O(log T). Compared with prior payoff-based or bandit-style rules, SBC-PE uniquely combines minimal signaling, general applicability, and finite-time guarantees. These results show scalable welfare optimization is achievable under minimal communication constraints.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parallax: Efficient LLM Inference Service over Decentralized Environment</title>
<link>https://arxiv.org/abs/2509.26182</link>
<guid>https://arxiv.org/abs/2509.26182</guid>
<content:encoded><![CDATA[
arXiv:2509.26182v1 Announce Type: new 
Abstract: Deploying a large language model (LLM) inference service remains costly because centralized serving depends on specialized GPU clusters and high-bandwidth interconnects in datacenters. An appealing alternative is to leverage collaborative decentralized GPU pools. However, heterogeneity in GPU and limited interconnected network bandwidth, along with potentially dynamic availability, make efficient scheduling the central challenge in this scenario. In this paper, we present Parallax, a decentralized LLM serving system that turns a pool of heterogeneous GPUs into an efficient inference platform via a two-phase scheduler. Parallax decomposes planning into (i) model allocation, which places layers of each replica across diverse GPUs to jointly optimize latency and throughput under memory and link-bandwidth constraints, and (ii) request-time GPU pipeline selection, which stitches layers from different replicas into end-to-end execution chains that balance load and adapt to current conditions. We implement Parallax and evaluate it on open-source LLMs deployed over real volunteer nodes. Parallax consistently reduces latency and increases throughput relative to decentralized baselines, demonstrating that principled scheduling can make volunteer compute a practical, affordable substrate for LLM inference.
  Github Repo at: https://github.com/GradientHQ/parallax.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Decentralized Federated Learning</title>
<link>https://arxiv.org/abs/2509.26462</link>
<guid>https://arxiv.org/abs/2509.26462</guid>
<content:encoded><![CDATA[
arXiv:2509.26462v1 Announce Type: new 
Abstract: CLIP has revolutionized zero-shot learning by enabling task generalization without fine-tuning. While prompting techniques like CoOp and CoCoOp enhance CLIP's adaptability, their effectiveness in Federated Learning (FL) remains an open challenge. Existing federated prompt learning approaches, such as FedCoOp and FedTPG, improve performance but face generalization issues, high communication costs, and reliance on a central server, limiting scalability and privacy. We propose Zero-shot Decentralized Federated Learning (ZeroDFL), a fully decentralized framework that enables zero-shot adaptation across distributed clients without a central coordinator. ZeroDFL employs an iterative prompt-sharing mechanism, allowing clients to optimize and exchange textual prompts to enhance generalization while drastically reducing communication overhead. We validate ZeroDFL on nine diverse image classification datasets, demonstrating that it consistently outperforms--or remains on par with--state-of-the-art federated prompt learning methods. More importantly, ZeroDFL achieves this performance in a fully decentralized setting while reducing communication overhead by 118x compared to FedTPG. These results highlight that our approach not only enhances generalization in federated zero-shot learning but also improves scalability, efficiency, and privacy preservation--paving the way for decentralized adaptation of large vision-language models in real-world applications.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAP: Two-Stage Adaptive Personalization of Multi-task and Multi-Modal Foundation Models in Federated Learning</title>
<link>https://arxiv.org/abs/2509.26524</link>
<guid>https://arxiv.org/abs/2509.26524</guid>
<content:encoded><![CDATA[
arXiv:2509.26524v1 Announce Type: new 
Abstract: Federated Learning (FL), despite demonstrating impressive capabilities in the training of multiple models in a decentralized manner, has been shown to produce a final model not necessarily well-suited to the needs of each client. While extensive work has been conducted on how to create tailored personalized models, called Personalized Federated Learning (PFL), less attention has been given to personalization via fine-tuning of foundation models with multi-task and multi-modal properties. Moreover, there exists a lack of understanding in the literature on how to fine-tune and personalize such models in a setting that is heterogeneous across clients not only in data, but also in tasks and modalities. To address this gap in the literature, we propose TAP (Two-Stage Adaptive Personalization), which (i) leverages mismatched model architectures between the clients and server to selectively conduct replacement operations when it benefits a client's local tasks and (ii) engages in post-FL knowledge distillation for capturing beneficial general knowledge without compromising personalization. We also introduce the first convergence analysis of the server model under its modality-task pair architecture, and demonstrate that as the number of modality-task pairs increases, its ability to cater to all tasks suffers. Through extensive experiments, we demonstrate the effectiveness of our proposed algorithm across a variety of datasets and tasks in comparison to a multitude of baselines. Implementation code is publicly available at https://github.com/lee3296/TAP.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Searcher Competition in Block Building</title>
<link>https://arxiv.org/abs/2407.07474</link>
<guid>https://arxiv.org/abs/2407.07474</guid>
<content:encoded><![CDATA[
arXiv:2407.07474v2 Announce Type: replace 
Abstract: We study the amount of maximal extractable value (MEV) captured by validators, as a function of searcher competition, in blockchains with competitive block building markets such as Ethereum. We argue that the core is a suitable solution concept in this context that makes robust predictions that are independent of implementation details or specific mechanisms chosen. We characterize how much value validators extract in the core and quantify the surplus share of validators as a function of searcher competition. Searchers can obtain at most the marginal value increase of the winning block relative to the best block that can be built without their bundles. Dually this gives a lower bound on the value extracted by the validator. If arbitrages are easy to find and many searchers find similar bundles, the validator gets paid all value almost surely, while searchers can capture most value if there is little searcher competition per arbitrage. For the case of passive block-proposers we study, moreover, mechanisms that implement core allocations in dominant strategies and find that for submodular value, there is a unique dominant-strategy incentive compatible core-selecting mechanism that gives each searcher exactly their marginal value contribution to the winning block. We validate our theoretical prediction empirically with aggregate bundle data and find a significant positive relation between the number of submitted backruns for the same opportunity and the median value captured by the proposer from the opportunity.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Competition to Centralization: The Oligopoly in Ethereum Block Building Auctions</title>
<link>https://arxiv.org/abs/2412.18074</link>
<guid>https://arxiv.org/abs/2412.18074</guid>
<content:encoded><![CDATA[
arXiv:2412.18074v2 Announce Type: replace 
Abstract: Block production on the Ethereum blockchain has adopted an auction-based mechanism known as Proposer--Builder Separation (PBS), where validators outsource block creation to builders competing in MEV--Boost auctions for Maximal Extractable Value (MEV) rewards. We employ empirical game-theoretic analysis based on simulations to examine how advantages in latency and MEV access shape builder strategic bidding and auction outcomes. We find that a small set of dominant builders leverage these advantages, consolidating power, reducing auction efficiency, and heightening centralization. Our results underscore the need for fair MEV distribution and sustained efforts to promote decentralization in Ethereum's block building market.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thunderdome: Timelock-Free Rationally-Secure Virtual Channels</title>
<link>https://arxiv.org/abs/2501.14418</link>
<guid>https://arxiv.org/abs/2501.14418</guid>
<content:encoded><![CDATA[
arXiv:2501.14418v4 Announce Type: replace 
Abstract: Payment channel networks (PCNs) offer a promising solution to address the limited transaction throughput of deployed blockchains. However, several attacks have recently been proposed that stress the vulnerability of PCNs to timelock and censoring attacks. To address such attacks, we introduce Thunderdome, the first timelock-free PCN. Instead, Thunderdome leverages the design rationale of virtual channels to extend a timelock-free payment channel primitive, thereby enabling multi-hop transactions without timelocks. Previous works either utilize timelocks or do not accommodate transactions between parties that do not share a channel.
  At its core, Thunderdome relies on a committee of non-trusted watchtowers, known as wardens, who ensure that no honest party loses funds, even when offline, during the channel closure process. We introduce tailored incentive mechanisms to ensure that all participants follow the protocol's correct execution. Besides a traditional security proof that assumes an honest majority of the committee, we conduct a formal game-theoretic analysis to demonstrate the security of Thunderdome when all participants, including wardens, act rationally. We implement a proof of concept of Thunderdome on Ethereum to validate its feasibility and evaluate its costs. Our evaluation shows that deploying Thunderdome, including opening the underlying payment channel, costs approximately \$15 (0.0089 ETH), while the worst-case cost for closing a channel is about \$7 (0.004 ETH).
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AntiFLipper: A Secure and Efficient Defense Against Label-Flipping Attacks in Federated Learning</title>
<link>https://arxiv.org/abs/2509.22873</link>
<guid>https://arxiv.org/abs/2509.22873</guid>
<content:encoded><![CDATA[
arXiv:2509.22873v2 Announce Type: replace 
Abstract: Federated learning (FL) enables privacy-preserving model training by keeping data decentralized. However, it remains vulnerable to label-flipping attacks, where malicious clients manipulate labels to poison the global model. Despite their simplicity, these attacks can severely degrade model performance, and defending against them remains challenging. We introduce AntiFLipper, a novel and computationally efficient defense against multi-class label-flipping attacks in FL. Unlike existing methods that ensure security at the cost of high computational overhead, AntiFLipper employs a novel client-side detection strategy, significantly reducing the central server's burden during aggregation. Comprehensive empirical evaluations across multiple datasets under different distributions demonstrate that AntiFLipper achieves accuracy comparable to state-of-the-art defenses while requiring substantially fewer computational resources in server side. By balancing security and efficiency, AntiFLipper addresses a critical gap in existing defenses, making it particularly suitable for resource-constrained FL deployments where both model integrity and operational efficiency are essential.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Satellite: Detecting and Analyzing Smart Contract Vulnerabilities caused by Subcontract Misuse</title>
<link>https://arxiv.org/abs/2509.23679</link>
<guid>https://arxiv.org/abs/2509.23679</guid>
<content:encoded><![CDATA[
arXiv:2509.23679v1 Announce Type: new 
Abstract: Developers of smart contracts pervasively reuse subcontracts to improve development efficiency. Like any program language, such subcontract reuse may unexpectedly include, or introduce vulnerabilities to the end-point smart contract. Unfortunately, automatically detecting such issues poses several unique challenges. Particularly, in most cases, smart contracts are compiled as bytecode, whose class-level information (e.g., inheritance, virtual function table), and even semantics (e.g., control flow and data flow) are fully obscured as a single smart contract after compilation.
  In this paper, we propose Satellite, a new bytecode-level static analysis framework for subcontract misuse vulnerability (SMV) detection in smart contracts. Satellite incorporates a series of novel designs to enhance its overall effectiveness.. Particularly, Satellite utilizes a transfer learning method to recover the inherited methods, which are critical for identifying subcontract reuse in smart contracts. Further, Satellite extracts a set of fine-grained method-level features and performs a method-level comparison, for identifying the reuse part of subcontract in smart contracts. Finally, Satellite summarizes a set of SMV indicators according to their types, and hence effectively identifies SMVs. To evaluate Satellite, we construct a dataset consisting of 58 SMVs derived from real-world attacks and collect additional 56 SMV patterns from SOTA studies. Experiment results indicate that Satellite exhibits good performance in identifying SMV, with a precision rate of 84.68% and a recall rate of 92.11%. In addition, Satellite successfully identifies 14 new/unknown SMV over 10,011 real-world smart contracts, affecting a total amount of digital assets worth 201,358 USD.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Dynamic Cooperation of Personalized Models for Federated Continual Learning</title>
<link>https://arxiv.org/abs/2509.23683</link>
<guid>https://arxiv.org/abs/2509.23683</guid>
<content:encoded><![CDATA[
arXiv:2509.23683v1 Announce Type: new 
Abstract: Federated continual learning (FCL) has garnered increasing attention for its ability to support distributed computation in environments with evolving data distributions. However, the emergence of new tasks introduces both temporal and cross-client shifts, making catastrophic forgetting a critical challenge. Most existing works aggregate knowledge from clients into a global model, which may not enhance client performance since irrelevant knowledge could introduce interference, especially in heterogeneous scenarios. Additionally, directly applying decentralized approaches to FCL suffers from ineffective group formation caused by task changes. To address these challenges, we propose a decentralized dynamic cooperation framework for FCL, where clients establish dynamic cooperative learning coalitions to balance the acquisition of new knowledge and the retention of prior learning, thereby obtaining personalized models. To maximize model performance, each client engages in selective cooperation, dynamically allying with others who offer meaningful performance gains. This results in non-overlapping, variable coalitions at each stage of the task. Moreover, we use coalitional affinity game to simulate coalition relationships between clients. By assessing both client gradient coherence and model similarity, we quantify the client benefits derived from cooperation. We also propose a merge-blocking algorithm and a dynamic cooperative evolution algorithm to achieve cooperative and dynamic equilibrium. Comprehensive experiments demonstrate the superiority of our method compared to various baselines. Code is available at: https://github.com/ydn3229/DCFCL.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Short-Term Guidance Algorithm on a Drone Road System</title>
<link>https://arxiv.org/abs/2509.23794</link>
<guid>https://arxiv.org/abs/2509.23794</guid>
<content:encoded><![CDATA[
arXiv:2509.23794v1 Announce Type: new 
Abstract: Unmanned Aerial Vehicles (UAVs), commonly known as drones, have experienced expanding use in urban environments in recent years. However, the growing density of drones raises significant challenges, such as avoiding collisions and managing air traffic efficiently, especially in congested areas. To address these issues, a structured road system and an effective guidance algorithm are essential. In this paper, we introduce a markup language allowing to describe drone road systems (DRS), in which a road system is given by a set of individual roads, each of which can have a varying number of lanes. Roads can be linked through connecting lanes. Furthermore, we propose a novel short-term decentralized greedy (STDG) guidance algorithm that uses only the position and speed information of nearby drones -- communicated via periodically transmitted beacons -- to make real-time decisions such as stopping, changing lanes, or adjusting speed for the next few seconds. Unlike existing methods that rely on centralized coordination, our algorithm enables drones to operate independently while ensuring safety and efficiency. We present simulation results showing the impact of key wireless and algorithm parameters on performance metrics like the drone collision rate, average speed and throughput of the drone road system.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SolContractEval: A Benchmark for Evaluating Contract-Level Solidity Code Generation</title>
<link>https://arxiv.org/abs/2509.23824</link>
<guid>https://arxiv.org/abs/2509.23824</guid>
<content:encoded><![CDATA[
arXiv:2509.23824v1 Announce Type: new 
Abstract: The rise of blockchain has brought smart contracts into mainstream use, creating a demand for smart contract generation tools. While large language models (LLMs) excel at generating code in general-purpose languages, their effectiveness on Solidity, the primary language for smart contracts, remains underexplored. Solidity constitutes only a small portion of typical LLM training data and differs from general-purpose languages in its version-sensitive syntax and limited flexibility. These factors raise concerns about the reliability of existing LLMs for Solidity code generation. Critically, existing evaluations, focused on isolated functions and synthetic inputs, fall short of assessing models' capabilities in real-world contract development.
  To bridge this gap, we introduce SolContractEval, the first contract-level benchmark for Solidity code generation. It comprises 124 tasks drawn from real on-chain contracts across nine major domains. Each task input, consisting of complete context dependencies, a structured contract framework, and a concise task prompt, is independently annotated and cross-validated by experienced developers. To enable precise and automated evaluation of functional correctness, we also develop a dynamic evaluation framework based on historical transaction replay. Building on SolContractEval, we perform a systematic evaluation of six mainstream LLMs. We find that Claude-3.7-Sonnet achieves the highest overall performance, though evaluated models underperform relative to their capabilities on class-level generation tasks in general-purpose programming languages. Second, current models perform better on tasks that follow standard patterns but struggle with complex logic and inter-contract dependencies. Finally, they exhibit limited understanding of Solidity-specific features and contextual dependencies.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAD-PINN: A Decentralized Physics-Informed Machine Learning Framework for Safe and Optimal Multi-Agent Control</title>
<link>https://arxiv.org/abs/2509.23960</link>
<guid>https://arxiv.org/abs/2509.23960</guid>
<content:encoded><![CDATA[
arXiv:2509.23960v1 Announce Type: new 
Abstract: Co-optimizing safety and performance in large-scale multi-agent systems remains a fundamental challenge. Existing approaches based on multi-agent reinforcement learning (MARL), safety filtering, or Model Predictive Control (MPC) either lack strict safety guarantees, suffer from conservatism, or fail to scale effectively. We propose MAD-PINN, a decentralized physics-informed machine learning framework for solving the multi-agent state-constrained optimal control problem (MASC-OCP). Our method leverages an epigraph-based reformulation of SC-OCP to simultaneously capture performance and safety, and approximates its solution via a physics-informed neural network. Scalability is achieved by training the SC-OCP value function on reduced-agent systems and deploying them in a decentralized fashion, where each agent relies only on local observations of its neighbours for decision-making. To further enhance safety and efficiency, we introduce an Hamilton-Jacobi (HJ) reachability-based neighbour selection strategy to prioritize safety-critical interactions, and a receding-horizon policy execution scheme that adapts to dynamic interactions while reducing computational burden. Experiments on multi-agent navigation tasks demonstrate that MAD-PINN achieves superior safety-performance trade-offs, maintains scalability as the number of agents grows, and consistently outperforms state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curriculum-Guided Reinforcement Learning for Synthesizing Gas-Efficient Financial Derivatives Contracts</title>
<link>https://arxiv.org/abs/2509.23976</link>
<guid>https://arxiv.org/abs/2509.23976</guid>
<content:encoded><![CDATA[
arXiv:2509.23976v1 Announce Type: new 
Abstract: Smart contract-based automation of financial derivatives offers substantial efficiency gains, but its real-world adoption is constrained by the complexity of translating financial specifications into gas-efficient executable code. In particular, generating code that is both functionally correct and economically viable from high-level specifications, such as the Common Domain Model (CDM), remains a significant challenge. This paper introduces a Reinforcement Learning (RL) framework to generate functional and gas-optimized Solidity smart contracts directly from CDM specifications. We employ a Proximal Policy Optimization (PPO) agent that learns to select optimal code snippets from a pre-defined library. To manage the complex search space, a two-phase curriculum first trains the agent for functional correctness before shifting its focus to gas optimization. Our empirical results show the RL agent learns to generate contracts with significant gas savings, achieving cost reductions of up to 35.59% on unseen test data compared to unoptimized baselines. This work presents a viable methodology for the automated synthesis of reliable and economically sustainable smart contracts, bridging the gap between high-level financial agreements and efficient on-chain execution.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiple Concurrent Proposers: Why and How</title>
<link>https://arxiv.org/abs/2509.23984</link>
<guid>https://arxiv.org/abs/2509.23984</guid>
<content:encoded><![CDATA[
arXiv:2509.23984v1 Announce Type: new 
Abstract: Traditional single-proposer blockchains suffer from miner extractable value (MEV), where validators exploit their serial monopoly on transaction inclusion and ordering to extract rents from users. While there have been many developments at the application layer to reduce the impact of MEV, these approaches largely require auctions as a subcomponent. Running auctions efficiently on chain requires two key properties of the underlying consensus protocol: selective-censorship resistance and hiding. These properties guarantee that an adversary can neither selectively delay transactions nor see their contents before they are confirmed. We propose a multiple concurrent proposer (MCP) protocol offering exactly these properties.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimism as Risk-Seeking in Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.24047</link>
<guid>https://arxiv.org/abs/2509.24047</guid>
<content:encoded><![CDATA[
arXiv:2509.24047v1 Announce Type: new 
Abstract: Risk sensitivity has become a central theme in reinforcement learning (RL), where convex risk measures and robust formulations provide principled ways to model preferences beyond expected return. Recent extensions to multi-agent RL (MARL) have largely emphasized the risk-averse setting, prioritizing robustness to uncertainty. In cooperative MARL, however, such conservatism often leads to suboptimal equilibria, and a parallel line of work has shown that optimism can promote cooperation. Existing optimistic methods, though effective in practice, are typically heuristic and lack theoretical grounding. Building on the dual representation for convex risk measures, we propose a principled framework that interprets risk-seeking objectives as optimism. We introduce optimistic value functions, which formalize optimism as divergence-penalized risk-seeking evaluations. Building on this foundation, we derive a policy-gradient theorem for optimistic value functions, including explicit formulas for the entropic risk/KL-penalty setting, and develop decentralized optimistic actor-critic algorithms that implement these updates. Empirical results on cooperative benchmarks demonstrate that risk-seeking optimism consistently improves coordination over both risk-neutral baselines and heuristic optimistic methods. Our framework thus unifies risk-sensitive learning and optimism, offering a theoretically grounded and practically effective approach to cooperation in MARL.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VeriLLM: A Lightweight Framework for Publicly Verifiable Decentralized Inference</title>
<link>https://arxiv.org/abs/2509.24257</link>
<guid>https://arxiv.org/abs/2509.24257</guid>
<content:encoded><![CDATA[
arXiv:2509.24257v1 Announce Type: new 
Abstract: Decentralized inference is an appealing paradigm for serving large language models (LLMs), offering strong security, high efficiency, and lower operating costs. Yet the permissionless setting admits no a priori trust in participating nodes, making output verifiability a prerequisite for secure deployment. We present VeriLLM, a publicly verifiable protocol for decentralized LLM inference that (i) achieves security under a one-honest-verifier assumption, (ii) attains near-negligible verification cost (about 1% of the underlying inference) via a lightweight verification algorithm designed explicitly for LLMs, and (iii) enforces honest checking through a peer-prediction mechanism that mitigates lazy verification in naive voting. We further introduce an isomorphic inference-verification network that multiplexes both roles on the same set of GPU workers. This architecture (i) increases GPU utilization and thereby improves end-to-end throughput for both inference and verification, (ii) expands the effective pool of available validators, strengthening robustness and security, and (iii) enforces task indistinguishability at the worker boundary to prevent job-type-conditioned behavior. Finally, we provide a formal game-theoretic analysis and prove that, under our incentives, honest inference and verification constitute a Nash equilibrium, ensuring incentive compatibility against rational adversaries. To our knowledge, this is the first decentralized inference verification protocol with an end-to-end game-theoretic security proof.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H+: An Efficient Similarity-Aware Aggregation for Byzantine Resilient Federated Learning</title>
<link>https://arxiv.org/abs/2509.24330</link>
<guid>https://arxiv.org/abs/2509.24330</guid>
<content:encoded><![CDATA[
arXiv:2509.24330v1 Announce Type: new 
Abstract: Federated Learning (FL) enables decentralized model training without sharing raw data. However, it remains vulnerable to Byzantine attacks, which can compromise the aggregation of locally updated parameters at the central server. Similarity-aware aggregation has emerged as an effective strategy to mitigate such attacks by identifying and filtering out malicious clients based on similarity between client model parameters and those derived from clean data, i.e., data that is uncorrupted and trustworthy. However, existing methods adopt this strategy only in FL systems with clean data, making them inapplicable to settings where such data is unavailable. In this paper, we propose H+, a novel similarity-aware aggregation approach that not only outperforms existing methods in scenarios with clean data, but also extends applicability to FL systems without any clean data. Specifically, H+ randomly selects $r$-dimensional segments from the $p$-dimensional parameter vectors uploaded to the server and applies a similarity check function $H$ to compare each segment against a reference vector, preserving the most similar client vectors for aggregation. The reference vector is derived either from existing robust algorithms when clean data is unavailable or directly from clean data. Repeating this process $K$ times enables effective identification of honest clients. Moreover, H+ maintains low computational complexity, with an analytical time complexity of $\mathcal{O}(KMr)$, where $M$ is the number of clients and $Kr \ll p$. Comprehensive experiments validate H+ as a state-of-the-art (SOTA) method, demonstrating substantial robustness improvements over existing approaches under varying Byzantine attack ratios and multiple types of traditional Byzantine attacks, across all evaluated scenarios and benchmark datasets.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BugMagnifier: TON Transaction Simulator for Revealing Smart Contract Vulnerabilities</title>
<link>https://arxiv.org/abs/2509.24444</link>
<guid>https://arxiv.org/abs/2509.24444</guid>
<content:encoded><![CDATA[
arXiv:2509.24444v1 Announce Type: new 
Abstract: The Open Network (TON) blockchain employs an asynchronous execution model that introduces unique security challenges for smart contracts, particularly race conditions arising from unpredictable message processing order. While previous work established vulnerability patterns through static analysis of audit reports, dynamic detection of temporal dependencies through systematic testing remains an open problem. We present BugMagnifier, a transaction simulation framework that systematically reveals vulnerabilities in TON smart contracts through controlled message orchestration. Built atop TON Sandbox and integrated with the TON Virtual Machine (TVM), our tool combines precise message queue manipulation with differential state analysis and probabilistic permutation testing to detect asynchronous execution flaws. Experimental evaluation demonstrates BugMagnifier's effectiveness through extensive parametric studies on purpose-built vulnerable contracts, revealing message ratio-dependent detection complexity that aligns with theoretical predictions. This quantitative model enables predictive vulnerability assessment while shifting discovery from manual expert analysis to automated evidence generation. By providing reproducible test scenarios for temporal vulnerabilities, BugMagnifier addresses a critical gap in the TON security tooling, offering practical support for safer smart contract development in asynchronous blockchain environments.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributionally Robust Federated Learning with Outlier Resilience</title>
<link>https://arxiv.org/abs/2509.24462</link>
<guid>https://arxiv.org/abs/2509.24462</guid>
<content:encoded><![CDATA[
arXiv:2509.24462v1 Announce Type: new 
Abstract: Federated learning (FL) enables collaborative model training without direct data sharing, but its performance can degrade significantly in the presence of data distribution perturbations. Distributionally robust optimization (DRO) provides a principled framework for handling this by optimizing performance against the worst-case distributions within a prescribed ambiguity set. However, existing DRO-based FL methods often overlook the detrimental impact of outliers in local datasets, which can disproportionately bias the learned models. In this work, we study distributionally robust federated learning with explicit outlier resilience. We introduce a novel ambiguity set based on the unbalanced Wasserstein distance, which jointly captures geometric distributional shifts and incorporates a non-geometric Kullback--Leibler penalization to mitigate the influence of outliers. This formulation naturally leads to a challenging min--max--max optimization problem. To enable decentralized training, we reformulate the problem as a tractable Lagrangian penalty optimization, which admits robustness certificates. Building on this reformulation, we propose the distributionally outlier-robust federated learning algorithm and establish its convergence guarantees. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Specification Generator for Move Programs</title>
<link>https://arxiv.org/abs/2509.24515</link>
<guid>https://arxiv.org/abs/2509.24515</guid>
<content:encoded><![CDATA[
arXiv:2509.24515v1 Announce Type: new 
Abstract: While LLM-based specification generation is gaining traction, existing tools primarily focus on mainstream programming languages like C, Java, and even Solidity, leaving emerging and yet verification-oriented languages like Move underexplored. In this paper, we introduce MSG, an automated specification generation tool designed for Move smart contracts. MSG aims to highlight key insights that uniquely present when applying LLM-based specification generation to a new ecosystem. Specifically, MSG demonstrates that LLMs exhibit robust code comprehension and generation capabilities even for non-mainstream languages. MSG successfully generates verifiable specifications for 84% of tested Move functions and even identifies clauses previously overlooked by experts. Additionally, MSG shows that explicitly leveraging specification language features through an agentic, modular design improves specification quality substantially (generating 57% more verifiable clauses than conventional designs). Incorporating feedback from the verification toolchain further enhances the effectiveness of MSG, leading to a 30% increase in generated verifiable specifications.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompting Robot Teams with Natural Language</title>
<link>https://arxiv.org/abs/2509.24575</link>
<guid>https://arxiv.org/abs/2509.24575</guid>
<content:encoded><![CDATA[
arXiv:2509.24575v1 Announce Type: new 
Abstract: This paper presents a framework towards prompting multi-robot teams with high-level tasks using natural language expressions. Our objective is to use the reasoning capabilities demonstrated by recent language models in understanding and decomposing human expressions of intent, and repurpose these for multi-robot collaboration and decision-making. The key challenge is that an individual's behavior in a collective can be hard to specify and interpret, and must continuously adapt to actions from others. This necessitates a framework that possesses the representational capacity required by the logic and semantics of a task, and yet supports decentralized and interactive real-time operation. We solve this dilemma by recognizing that a task can be represented as a deterministic finite automaton (DFA), and that recurrent neural networks (RNNs) can encode numerous automata. This allows us to distill the logic and sequential decompositions of sub-tasks obtained from a language model into an RNN, and align its internal states with the semantics of a given task. By training a graph neural network (GNN) control policy that is conditioned on the hidden states of the RNN and the language embeddings, our method enables robots to execute task-relevant actions in a decentralized manner. We present evaluations of this single light-weight interpretable model on various simulated and real-world multi-robot tasks that require sequential and collaborative behavior by the team -- sites.google.com/view/prompting-teams.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LISA Technical Report: An Agentic Framework for Smart Contract Auditing</title>
<link>https://arxiv.org/abs/2509.24698</link>
<guid>https://arxiv.org/abs/2509.24698</guid>
<content:encoded><![CDATA[
arXiv:2509.24698v1 Announce Type: new 
Abstract: We present LISA, an agentic smart contract vulnerability detection framework that combines rule-based and logic-based methods to address a broad spectrum of vulnerabilities in smart contracts. LISA leverages data from historical audit reports to learn the detection experience (without model fine-tuning), enabling it to generalize learned patterns to unseen projects and evolving threat profiles. In our evaluation, LISA significantly outperforms both LLM-based approaches and traditional static analysis tools, achieving superior coverage of vulnerability types and higher detection accuracy. Our results suggest that LISA offers a compelling solution for industry: delivering more reliable and comprehensive vulnerability detection while reducing the dependence on manual effort.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blockchain-Driven Federation for Distributed Edge Systems: Design and Experimental Validation</title>
<link>https://arxiv.org/abs/2509.24846</link>
<guid>https://arxiv.org/abs/2509.24846</guid>
<content:encoded><![CDATA[
arXiv:2509.24846v1 Announce Type: new 
Abstract: Edge computing brings computation near end users, enabling the provisioning of novel use cases. To satisfy end-user requirements, the concept of edge federation has recently emerged as a key mechanism for dynamic resources and services sharing across edge systems managed by different administrative domains. However, existing federation solutions often rely on pre-established agreements and face significant limitations, including operational complexity, delays caused by manual operations, high overhead costs, and dependence on trusted third parties. In this context, blockchain can create dynamic federation agreements that enable service providers to securely interact and share services without prior trust.
  This article first describes the problem of edge federation, using the standardized ETSI multi-access edge computing framework as a reference architecture, and how it is being addressed. Then, it proposes a novel solution using blockchain and smart contracts to enable distributed MEC systems to dynamically negotiate and execute federation in a secure, automated, and scalable manner. We validate our framework's feasibility through a performance evaluation using a private Ethereum blockchain, built on the open-source Hyperledger Besu platform. The testbed includes a large number of MEC systems and compares two blockchain consensus algorithms. Experimental results demonstrate that our solution automates the entire federation lifecycle-from negotiation to deployment-with a quantifiable overhead, achieving federation in approximately 18 seconds in a baseline scenario. The framework scales efficiently in concurrent request scenarios, where multiple MEC systems initiate federation requests simultaneously. This approach provides a promising direction for addressing the complexities of dynamic, multi-domain federations across the edge-to-cloud continuum.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Free Option Problem of ePBS</title>
<link>https://arxiv.org/abs/2509.24849</link>
<guid>https://arxiv.org/abs/2509.24849</guid>
<content:encoded><![CDATA[
arXiv:2509.24849v1 Announce Type: new 
Abstract: Ethereum's upcoming Glamsterdam upgrade introduces EIP-7732 enshrined Proposer--Builder Separation (ePBS), which improves the block production pipeline by addressing trust and scalability challenges. Yet it also creates a new liveness risk: builders gain a short-dated ``free'' option to prevent the execution payload they committed to from becoming canonical, without incurring an additional penalty. Exercising this option renders an empty block for the slot in question, thereby degrading network liveness.
  We present the first systematic study of the free option problem. Our theoretical results predict that option value and exercise probability grow with market volatility, the length of the option window, and the share of block value derived from external signals such as external market prices. The availability of a free option will lead to mispricing and LP losses. The problem would be exacerbated if Ethereum further scales and attracts more liquidity. Empirical estimates of values and exercise probabilities on historical blocks largely confirm our theoretical predictions. While the option is rarely profitable to exercise on average (0.82\% of blocks assuming an 8-second option time window), it becomes significant in volatile periods, reaching up to 6\% of blocks on high-volatility days -- precisely when users most require timely execution.
  Moreover, builders whose block value relies heavily on CEX-DEX arbitrage are more likely to exercise the option. We demonstrate that mitigation strategies -- shortening the option window or penalizing exercised options -- effectively reduce liveness risk.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Secret Leader Election in Ethereum PoS: An Empirical Security Analysis of Whisk and Homomorphic Sortition under DoS on the Leader and Censorship Attacks</title>
<link>https://arxiv.org/abs/2509.24955</link>
<guid>https://arxiv.org/abs/2509.24955</guid>
<content:encoded><![CDATA[
arXiv:2509.24955v1 Announce Type: new 
Abstract: Proposer anonymity in Proof-of-Stake (PoS) blockchains is a critical concern due to the risk of targeted attacks such as malicious denial-of-service (DoS) and censorship attacks. While several Secret Single Leader Election (SSLE) mechanisms have been proposed to address these threats, their practical impact and trade-offs remain insufficiently explored. In this work, we present a unified experimental framework for evaluating SSLE mechanisms under adversarial conditions, grounded in a simplified yet representative model of Ethereum's PoS consensus layer. The framework includes configurable adversaries capable of launching targeted DoS and censorship attacks, including coordinated strategies that simultaneously compromise groups of validators. We simulate and compare key protection mechanisms - Whisk, and homomorphic sortition. To the best of our knowledge, this is the first comparative study to examine adversarial DoS scenarios involving multiple attackers under diverse protection mechanisms. Our results show that while both designs offer strong protection against targeted DoS attacks on the leader, neither defends effectively against coordinated attacks on validator groups. Moreover, Whisk simplifies a DoS attack by narrowing the target set from all validators to a smaller list of known candidates. Homomorphic sortition, despite its theoretical strength, remains impractical due to the complexity of cryptographic operations over large validator sets.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MARLIN: Multi-Agent Reinforcement Learning with Murmuration Intelligence and LLM Guidance for Reservoir Management</title>
<link>https://arxiv.org/abs/2509.25034</link>
<guid>https://arxiv.org/abs/2509.25034</guid>
<content:encoded><![CDATA[
arXiv:2509.25034v1 Announce Type: new 
Abstract: As climate change intensifies extreme weather events, water disasters pose growing threats to global communities, making adaptive reservoir management critical for protecting vulnerable populations and ensuring water security. Modern water resource management faces unprecedented challenges from cascading uncertainties propagating through interconnected reservoir networks. These uncertainties, rooted in physical water transfer losses and environmental variability, make precise control difficult. For example, sending 10 tons downstream may yield only 8-12 tons due to evaporation and seepage. Traditional centralized optimization approaches suffer from exponential computational complexity and cannot effectively handle such real-world uncertainties, while existing multi-agent reinforcement learning (MARL) methods fail to achieve effective coordination under uncertainty. To address these challenges, we present MARLIN, a decentralized reservoir management framework inspired by starling murmurations intelligence. Integrating bio-inspired alignment, separation, and cohesion rules with MARL, MARLIN enables individual reservoirs to make local decisions while achieving emergent global coordination. In addition, a LLM provides real-time reward shaping signals, guiding agents to adapt to environmental changes and human-defined preferences. Experiments on real-world USGS data show that MARLIN improves uncertainty handling by 23\%, cuts computation by 35\%, and accelerates flood response by 68\%, exhibiting super-linear coordination, with complexity scaling 5.4x from 400 to 10,000 nodes. These results demonstrate MARLIN's potential for disaster prevention and protecting communities through intelligent, scalable water resource management.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding How Network Geometry Influences Diffusion Processes in Complex Networks: A Focus on Cryptocurrency Blockchains and Critical Infrastructure Networks</title>
<link>https://arxiv.org/abs/2509.23450</link>
<guid>https://arxiv.org/abs/2509.23450</guid>
<content:encoded><![CDATA[
arXiv:2509.23450v1 Announce Type: cross 
Abstract: This study provides essential insights into how diffusion processes unfold in complex networks, with a focus on cryptocurrency blockchains and infrastructure networks. The structural properties of these networks, such as hub-dominated, heavy-tailed topology, network motifs, and node centrality, significantly influence diffusion speed and reach. Using epidemic diffusion models, specifically the Kertesz threshold model and the Susceptible-Infected (SI) model, we analyze key factors affecting diffusion dynamics. To assess the uncertainty in the fraction of infected nodes over time, we employ bootstrap confidence intervals, while Bayesian credible intervals are constructed to quantify parameter uncertainties in the SI models. Our findings reveal substantial variations across different network types, including Erd\H{o}s--R\'enyi networks, Geometric Random Graphs, and Delaunay Triangulation networks, emphasizing the role of network architecture in failure propagation. We identify that network motifs are crucial in diffusion. We highlight that hub-dominated networks, which dominate blockchain ecosystems, provide resilience against random failures but remain vulnerable to targeted attacks, posing significant risks to network stability. Furthermore, centrality measures such as degree, betweenness, and clustering coefficient strongly influence the transmissibility of diffusion in both blockchain and critical infrastructure networks.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TOB-SVD: Total-Order Broadcast with Single-Vote Decisions in the Sleepy Model</title>
<link>https://arxiv.org/abs/2310.11331</link>
<guid>https://arxiv.org/abs/2310.11331</guid>
<content:encoded><![CDATA[
arXiv:2310.11331v3 Announce Type: replace 
Abstract: Over the past years, distributed consensus research has expanded its focus to address challenges in large-scale, permissionless systems, such as blockchains. This shift reflects the need to accommodate dynamic participation, in contrast to the traditional model of a static set of continuously online validators. Works like Bitcoin and the sleepy model have laid the groundwork for this evolving framework.
  Notable contributions by Momose and Ren (CCS 2022), along with subsequent research, have introduced Total-Order Broadcast protocols that leverage Graded Agreement primitives and support dynamic participation. However, these approaches often require multiple phases of voting per decision, which can create a bottleneck for real-world, large-scale systems.
  To address this, our paper introduces TOB-SVD, a novel Total-Order Broadcast protocol in the sleepy model that is resilient to up to 1/2 Byzantine validators. TOB-SVD requires only a single phase of voting per decision in the best case and achieves lower expected latency compared to existing approaches offering the same optimal adversarial resilience. This work paves the way for more practical Total-Order Broadcast protocols that can be implemented in real-world systems involving a large number of validators with fluctuating participation over time.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Analysis of the Nostr Social Network: Decentralization, Availability, and Replication Overhead</title>
<link>https://arxiv.org/abs/2402.05709</link>
<guid>https://arxiv.org/abs/2402.05709</guid>
<content:encoded><![CDATA[
arXiv:2402.05709v2 Announce Type: replace 
Abstract: Nostr is a decentralized social network launched in 2022, emphasizing high availability and censorship resistance. Since launching, it has gained substantial attention, boasting over 100 million posts. Nostr resembles a micro-blogging service like Twitter but with distinct underlying infrastructure. Nostr introduces the concept of relays, which act as open storage servers that receive, store, and distribute user posts. Each user is uniquely identified by a public key, ensuring authenticity of posts through digital signatures. Users are able to securely replicate and retrieve posts through multiple relays, which frees them from single-server reliance and enhances post availability, thereby attempting to make Nostr censorship resistant. However, this aggressive design also presents challenges, such as the overhead required for extensive post replication and the difficulty in obtaining a global view of post replication locations, which remain unexplored or unaddressed. This necessitates a thorough understanding of the Nostr ecosystem; therefore, we conduct the first large-scale study on this topic. Our study focuses on two key aspects: Nostr relays and post replication strategies. We find that Nostr achieves superior decentralization compared to traditional Fediverse applications. However, relay availability remains a challenge, where financial sustainability (particularly for free-to-use relays) emerges as a contributing factor. We also find that the replication of posts across relays enhances censorship-resistance but introduces significant overhead. To address this, we propose two improvements: one to control the number of post replications, and another to reduce the overhead during post retrieval. Via a data-driven evaluation, we demonstrate their ability to reduce overhead without negatively impacting post availability under the simulated scenarios.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BlockFUL: Enabling Unlearning in Blockchained Federated Learning</title>
<link>https://arxiv.org/abs/2402.16294</link>
<guid>https://arxiv.org/abs/2402.16294</guid>
<content:encoded><![CDATA[
arXiv:2402.16294v3 Announce Type: replace 
Abstract: Unlearning in Federated Learning (FL) presents significant challenges, as models grow and evolve with complex inheritance relationships. This complexity is amplified when blockchain is employed to ensure the integrity and traceability of FL, where the need to edit multiple interlinked blockchain records and update all inherited models complicates the process.In this paper, we introduce Blockchained Federated Unlearning (BlockFUL), a novel framework with a dual-chain structure comprising a live chain and an archive chain for enabling unlearning capabilities within Blockchained FL. BlockFUL introduces two new unlearning paradigms, i.e., parallel and sequential paradigms, which can be effectively implemented through gradient-ascent-based and re-training-based unlearning methods. These methods enhance the unlearning process across multiple inherited models by enabling efficient consensus operations and reducing computational costs. Our extensive experiments validate that these methods effectively reduce data dependency and operational overhead, thereby boosting the overall performance of unlearning inherited models within BlockFUL on CIFAR-10 and Fashion-MNIST datasets using AlexNet, ResNet18, and MobileNetV2 models.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fast Confirmation Rule (aka Fast Synchronous Finality) for the Ethereum Consensus Protocol</title>
<link>https://arxiv.org/abs/2405.00549</link>
<guid>https://arxiv.org/abs/2405.00549</guid>
<content:encoded><![CDATA[
arXiv:2405.00549v3 Announce Type: replace 
Abstract: A Confirmation Rule, within blockchain networks, refers to an algorithm implemented by network nodes that determines (either probabilistically or deterministically) the permanence of certain blocks on the blockchain. An example of Confirmation Ruble is the Bitcoin's longest chain Confirmation Rule where a block $b$ is confirmed (with high probability) when it has a sufficiently long chain of successors, its siblings have notably shorter successor chains, the majority of the network's total computation power (hashing) is controlled by honest nodes, and network synchrony holds.
  The only Confirmation Rule currently available in the Ethereum protocol, Gasper, is the FFG Finalization Rule. While this Confirmation Rule works under asynchronous network conditions, it is quite slow for many use cases. Specifically, best-case scenario, it takes around 13 to 19 min to confirm a transaction, where the actual figure depends on when the transaction is submitted to the network.
  In this work, we devise a Fast Confirmation Rule for Ethereum's consensus protocol. Our Confirmation Rule relies on synchrony conditions, but provides a best-case confirmation time of 12 seconds only, greatly improving on the latency of the FFG Finalization Rule.
  Users can then rely on the Confirmation Rule that best suits their needs depending on their belief about the network conditions and the need for a quick response.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PROMFUZZ: Leveraging LLM-Driven and Bug-Oriented Composite Analysis for Detecting Functional Bugs in Smart Contracts</title>
<link>https://arxiv.org/abs/2503.23718</link>
<guid>https://arxiv.org/abs/2503.23718</guid>
<content:encoded><![CDATA[
arXiv:2503.23718v2 Announce Type: replace 
Abstract: Smart contracts are fundamental pillars of the blockchain, playing a crucial role in facilitating various business transactions. However, these smart contracts are vulnerable to exploitable bugs that can lead to substantial monetary losses. A recent study reveals that over 80% of these exploitable bugs, which are primarily functional bugs, can evade the detection of current tools. The primary issue is the significant gap between understanding the high-level logic of the business model and checking the low-level implementations in smart contracts. Furthermore, identifying deeply rooted functional bugs in smart contracts requires the automated generation of effective detection oracles based on various bug features. To address these challenges, we design and implement PROMFUZZ, an automated and scalable system to detect functional bugs, in smart contracts. In PROMFUZZ, we first propose a novel Large Language Model (LLM)-driven analysis framework, which leverages a dual-agent prompt engineering strategy to pinpoint potentially vulnerable functions for further scrutiny. We then implement a dual-stage coupling approach, which focuses on generating invariant checkers that leverage logic information extracted from potentially vulnerable functions. Finally, we design a bug-oriented fuzzing engine, which maps the logical information from the high-level business model to the low-level smart contract implementations, and performs the bug-oriented fuzzing on targeted functions. We compare PROMFUZZ with multiple state-of-the-art methods. The results show that PROMFUZZ achieves 86.96% recall and 93.02% F1-score in detecting functional bugs, marking at least a 50% improvement in both metrics over state-of-the-art methods. Moreover, we perform an in-depth analysis on real-world DeFi projects and detect 30 zero-day bugs. Up to now, 24 zero-day bugs have been assigned CVE IDs.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering the Herd: A Framework for LLM-based Control of Social Learning</title>
<link>https://arxiv.org/abs/2504.02648</link>
<guid>https://arxiv.org/abs/2504.02648</guid>
<content:encoded><![CDATA[
arXiv:2504.02648v3 Announce Type: replace 
Abstract: Algorithms increasingly serve as information mediators--from social media feeds and targeted advertising to the increasing ubiquity of LLMs. This engenders a joint process where agents combine private, algorithmically-mediated signals with learning from peers to arrive at decisions. To study such settings, we introduce a model of controlled sequential social learning in which an information-mediating planner (e.g. an LLM) controls the information structure of agents while they also learn from the decisions of earlier agents. The planner may seek to improve social welfare (altruistic planner) or to induce a specific action the planner prefers (biased planner). Our framework presents a new optimization problem for social learning that combines dynamic programming with decentralized action choices and Bayesian belief updates.
  We prove the convexity of the value function and characterize the optimal policies of altruistic and biased planners, which attain desired tradeoffs between the costs they incur and the payoffs they earn from induced agent choices. Notably, in some regimes the biased planner intentionally obfuscates the agents' signals. Even under stringent transparency constraints--information parity with individuals, no lying or cherry-picking, and full observability--we show that information mediation can substantially shift social welfare in either direction. We complement our theory with simulations in which LLMs act as both planner and agents. Notably, the LLM planner in our simulations exhibits emergent strategic behavior in steering public opinion that broadly mirrors the trends predicted, though key deviations suggest the influence of non-Bayesian reasoning consistent with the cognitive patterns of both humans and LLMs trained on human-like data. Together, we establish our framework as a tractable basis for studying the impact and regulation of LLM information mediators.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Federated Learning Meets Quantum Computing: Survey and Research Opportunities</title>
<link>https://arxiv.org/abs/2504.08814</link>
<guid>https://arxiv.org/abs/2504.08814</guid>
<content:encoded><![CDATA[
arXiv:2504.08814v3 Announce Type: replace 
Abstract: Quantum Federated Learning (QFL) is an emerging field that harnesses advances in Quantum Computing (QC) to improve the scalability and efficiency of decentralized Federated Learning (FL) models. This paper provides a systematic and comprehensive survey of the emerging problems and solutions when FL meets QC, from research protocol to a novel taxonomy, particularly focusing on both quantum and federated limitations, such as their architectures, Noisy Intermediate Scale Quantum (NISQ) devices, and privacy preservation, so on. This work explores key developments and integration strategies, along with the impact of quantum computing on FL, keeping a sharp focus on hybrid quantum-classical approaches. The paper offers an in-depth understanding of how the strengths of QC, such as gradient hiding, state entanglement, quantum key distribution, quantum security, and quantum-enhanced differential privacy, have been integrated into FL to ensure the privacy of participants in an enhanced, fast, and secure framework. Finally, this study proposes potential future directions to address the identified research gaps and challenges, aiming to inspire faster and more secure QFL models for practical use.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Proximal Gradient Method With Probabilistic Multi-Gossip Communications for Decentralized Composite Optimization</title>
<link>https://arxiv.org/abs/2312.11861</link>
<guid>https://arxiv.org/abs/2312.11861</guid>
<content:encoded><![CDATA[
arXiv:2312.11861v3 Announce Type: replace-cross 
Abstract: Decentralized optimization methods with local updates have recently gained attention for their provable ability to communication acceleration. In these methods, nodes perform several iterations of local computations between the communication rounds. Nevertheless, this capability is effective only when the network is sufficiently well-connected and the loss function is smooth. In this paper, we propose a communication-efficient method MG-Skip with probabilistic local updates and multi-gossip communications for decentralized composite (smooth + nonsmooth) optimization, whose stepsize is independent of the number of local updates and the network topology. For any undirected and connected networks, MG-Skip allows for the multi-gossip communications to be skipped in most iterations in the strongly convex setting, while its computation complexity is $\mathcal{O}\left(\kappa \log \frac{1}{\epsilon}\right)$ and communication complexity is only $\mathcal{O}\left(\sqrt{\frac{\kappa}{(1-\rho)}} \log \frac{1}{\epsilon}\right)$, where $\kappa$ is the condition number of the loss function, $\rho$ reflects the connectivity of the network topology, and $\epsilon$ is the target accuracy. The theoretical results indicate that MG-Skip achieves provable communication acceleration, thereby validating the advantages of local updates in the nonsmooth setting.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Learning for Grant-Free Activity Detection in Cell-Free Massive MIMO Networks</title>
<link>https://arxiv.org/abs/2503.11305</link>
<guid>https://arxiv.org/abs/2503.11305</guid>
<content:encoded><![CDATA[
arXiv:2503.11305v3 Announce Type: replace-cross 
Abstract: Grant-free random access (GF-RA) is a promising access technique for massive machine-type communications (mMTC) in future wireless networks, particularly in the context of 5G and beyond (6G) systems. Within the context of GF-RA, this study investigates the efficiency of employing supervised machine learning techniques to tackle the challenges on the device activity detection (AD). GF-RA addresses scalability by employing non-orthogonal pilot sequences, which provides an efficient alternative comparing to conventional grant-based random access (GB-RA) technique that are constrained by the scarcity of orthogonal preamble resources. In this paper, we propose a novel lightweight data-driven algorithmic framework specifically designed for activity detection in GF-RA for mMTC in cell-free massive multiple-input multiple-output (CF-mMIMO) networks. We propose two distinct framework deployment strategies, centralized and decentralized, both tailored to streamline the proposed approach implementation across network infrastructures. Moreover, we introduce optimized post-detection methodologies complemented by a clustering stage to enhance overall detection performances. Our 3GPP-compliant simulations have validated that the proposed algorithm achieves state-of-the-art model-based activity detection accuracy while significantly reducing complexity. Achieving 99% accuracy, it demonstrates real-world viability and effectiveness.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing Ethereum's Geographical (De)Centralization Beyond the Atlantic</title>
<link>https://arxiv.org/abs/2509.21475</link>
<guid>https://arxiv.org/abs/2509.21475</guid>
<content:encoded><![CDATA[
arXiv:2509.21475v1 Announce Type: new 
Abstract: Decentralization has a geographic dimension that conventional metrics such as stake distribution overlook. Where validators run affects resilience to regional shocks (outages, disasters, government intervention) and fairness in reward access. Yet in permissionless systems, locations cannot be mandated, but they emerge from incentives. Today, Ethereum's validators cluster along the Atlantic (EU and U.S. East Coast), where latency is structurally favorable. This raises a key question: when some regions already enjoy latency advantages, how does protocol design shape validator incentives and the geography of (de)centralization? We develop a latency-calibrated agent-based model and compare two Ethereum block-building paradigms: a Single-Source Paradigm (SSP), akin to MEV-Boost, where proposers fetch full blocks from a relay that also propagates them; and a Multi-Source Paradigm (MSP), where proposers aggregate value from multiple sources and broadcast the block themselves. Simulations show that SSP concentrates around relay placement but more slowly, since proximity mainly affects propagation, and the marginal value of time is relatively uniform across regions. MSP centralizes faster: aggregating across sources makes marginal value location-dependent, amplifying payoff dispersion and migration toward latency minima. Source placement and consensus settings can dampen or intensify these effects, though once validators are already clustered, the impact of source placement on decentralization is marginal. In most cases, North America consistently emerges as the focal hub. These findings show that protocol design materially shapes validator geography and offer levers for promoting geographical decentralization.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Indexing to Coding: A New Paradigm for Data Availability Sampling</title>
<link>https://arxiv.org/abs/2509.21586</link>
<guid>https://arxiv.org/abs/2509.21586</guid>
<content:encoded><![CDATA[
arXiv:2509.21586v1 Announce Type: new 
Abstract: The data availability problem is a central challenge in blockchain systems and lies at the core of the accessibility and scalability issues faced by platforms such as Ethereum. Modern solutions employ several approaches, with data availability sampling (DAS) being the most self-sufficient and minimalistic in its security assumptions. Existing DAS methods typically form cryptographic commitments on codewords of fixed-rate erasure codes, which restrict light nodes to sampling from a predetermined set of coded symbols.
  In this paper, we introduce a new approach to DAS that modularizes the coding and commitment process by committing to the uncoded data while performing sampling through on-the-fly coding. The resulting samples are significantly more expressive, enabling light nodes to obtain, in concrete implementations, up to multiple orders of magnitude stronger assurances of data availability than from sampling pre-committed symbols from a fixed-rate redundancy code as done in established DAS schemes using Reed Solomon or low density parity check codes. We present a concrete protocol that realizes this paradigm using random linear network coding (RLNC).
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Agnostic Federated Continual Learning via Replay-Free Gradient Projection</title>
<link>https://arxiv.org/abs/2509.21606</link>
<guid>https://arxiv.org/abs/2509.21606</guid>
<content:encoded><![CDATA[
arXiv:2509.21606v1 Announce Type: new 
Abstract: Federated continual learning (FCL) enables distributed client devices to learn from streaming data across diverse and evolving tasks. A major challenge to continual learning, catastrophic forgetting, is exacerbated in decentralized settings by the data heterogeneity, constrained communication and privacy concerns. We propose Federated gradient Projection-based Continual Learning with Task Identity Prediction (FedProTIP), a novel FCL framework that mitigates forgetting by projecting client updates onto the orthogonal complement of the subspace spanned by previously learned representations of the global model. This projection reduces interference with earlier tasks and preserves performance across the task sequence. To further address the challenge of task-agnostic inference, we incorporate a lightweight mechanism that leverages core bases from prior tasks to predict task identity and dynamically adjust the global model's outputs. Extensive experiments across standard FCL benchmarks demonstrate that FedProTIP significantly outperforms state-of-the-art methods in average accuracy, particularly in settings where task identities are a priori unknown.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NEO-Grid: A Neural Approximation Framework for Optimization and Control in Distribution Grids</title>
<link>https://arxiv.org/abs/2509.21668</link>
<guid>https://arxiv.org/abs/2509.21668</guid>
<content:encoded><![CDATA[
arXiv:2509.21668v1 Announce Type: new 
Abstract: The rise of distributed energy resources (DERs) is reshaping modern distribution grids, introducing new challenges in attaining voltage stability under dynamic and decentralized operating conditions. This paper presents NEO-Grid, a unified learning-based framework for volt-var optimization (VVO) and volt-var control (VVC) that leverages neural network surrogates for power flow and deep equilibrium models (DEQs) for closed-loop control. Our method replaces traditional linear approximations with piecewise-linear ReLU networks trained to capture the nonlinear relationship between power injections and voltage magnitudes. For control, we model the recursive interaction between voltage and inverter response using DEQs, allowing direct fixed-point computation and efficient training via implicit differentiation. We evaluated NEO-Grid on the IEEE 33-bus system, demonstrating that it significantly improves voltage regulation performance compared to standard linear and heuristic baselines in both optimization and control settings. Our results establish NEO-Grid as a scalable, accurate, and interpretable solution for learning-based voltage regulation in distribution grids.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentially-Private Decentralized Learning in Heterogeneous Multicast Networks</title>
<link>https://arxiv.org/abs/2509.21688</link>
<guid>https://arxiv.org/abs/2509.21688</guid>
<content:encoded><![CDATA[
arXiv:2509.21688v1 Announce Type: new 
Abstract: We propose a power-controlled differentially private decentralized learning algorithm designed for a set of clients aiming to collaboratively train a common learning model. The network is characterized by a row-stochastic adjacency matrix, which reflects different channel gains between the clients. In our privacy-preserving approach, both the transmit power for model updates and the level of injected Gaussian noise are jointly controlled to satisfy a given privacy and energy budget. We show that our proposed algorithm achieves a convergence rate of O(log T), where T is the horizon bound in the regret function. Furthermore, our numerical results confirm that our proposed algorithm outperforms existing works.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preference-Guided Learning for Sparse-Reward Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.21828</link>
<guid>https://arxiv.org/abs/2509.21828</guid>
<content:encoded><![CDATA[
arXiv:2509.21828v1 Announce Type: new 
Abstract: We study the problem of online multi-agent reinforcement learning (MARL) in environments with sparse rewards, where reward feedback is not provided at each interaction but only revealed at the end of a trajectory. This setting, though realistic, presents a fundamental challenge: the lack of intermediate rewards hinders standard MARL algorithms from effectively guiding policy learning. To address this issue, we propose a novel framework that integrates online inverse preference learning with multi-agent on-policy optimization into a unified architecture. At its core, our approach introduces an implicit multi-agent reward learning model, built upon a preference-based value-decomposition network, which produces both global and local reward signals. These signals are further used to construct dual advantage streams, enabling differentiated learning targets for the centralized critic and decentralized actors. In addition, we demonstrate how large language models (LLMs) can be leveraged to provide preference labels that enhance the quality of the learned reward model. Empirical evaluations on state-of-the-art benchmarks, including MAMuJoCo and SMACv2, show that our method achieves superior performance compared to existing baselines, highlighting its effectiveness in addressing sparse-reward challenges in online MARL.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Dark Art of Financial Disguise in Web3: Money Laundering Schemes and Countermeasures</title>
<link>https://arxiv.org/abs/2509.21831</link>
<guid>https://arxiv.org/abs/2509.21831</guid>
<content:encoded><![CDATA[
arXiv:2509.21831v1 Announce Type: new 
Abstract: The rise of Web3 and Decentralized Finance (DeFi) has enabled borderless access to financial services empowered by smart contracts and blockchain technology. However, the ecosystem's trustless, permissionless, and borderless nature presents substantial regulatory challenges. The absence of centralized oversight and the technical complexity create fertile ground for financial crimes. Among these, money laundering is particularly concerning, as in the event of successful scams, code exploits, and market manipulations, it facilitates covert movement of illicit gains. Beyond this, there is a growing concern that cryptocurrencies can be leveraged to launder proceeds from drug trafficking, or to transfer funds linked to terrorism financing.
  This survey aims to outline a taxonomy of high-level strategies and underlying mechanisms exploited to facilitate money laundering in Web3. We examine how criminals leverage the pseudonymous nature of Web3, alongside weak regulatory frameworks, to obscure illicit financial activities. Our study seeks to bridge existing knowledge gaps on laundering schemes, identify open challenges in the detection and prevention of such activities, and propose future research directions to foster a more transparent Web3 financial ecosystem -- offering valuable insights for researchers, policymakers, and industry practitioners.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Path Finding via Offline RL and LLM Collaboration</title>
<link>https://arxiv.org/abs/2509.22130</link>
<guid>https://arxiv.org/abs/2509.22130</guid>
<content:encoded><![CDATA[
arXiv:2509.22130v1 Announce Type: new 
Abstract: Multi-Agent Path Finding (MAPF) poses a significant and challenging problem critical for applications in robotics and logistics, particularly due to its combinatorial complexity and the partial observability inherent in realistic environments. Decentralized reinforcement learning methods commonly encounter two substantial difficulties: first, they often yield self-centered behaviors among agents, resulting in frequent collisions, and second, their reliance on complex communication modules leads to prolonged training times, sometimes spanning weeks. To address these challenges, we propose an efficient decentralized planning framework based on the Decision Transformer (DT), uniquely leveraging offline reinforcement learning to substantially reduce training durations from weeks to mere hours. Crucially, our approach effectively handles long-horizon credit assignment and significantly improves performance in scenarios with sparse and delayed rewards. Furthermore, to overcome adaptability limitations inherent in standard RL methods under dynamic environmental changes, we integrate a large language model (GPT-4o) to dynamically guide agent policies. Extensive experiments in both static and dynamically changing environments demonstrate that our DT-based approach, augmented briefly by GPT-4o, significantly enhances adaptability and performance.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiency Boost in Decentralized Optimization: Reimagining Neighborhood Aggregation with Minimal Overhead</title>
<link>https://arxiv.org/abs/2509.22174</link>
<guid>https://arxiv.org/abs/2509.22174</guid>
<content:encoded><![CDATA[
arXiv:2509.22174v1 Announce Type: new 
Abstract: In today's data-sensitive landscape, distributed learning emerges as a vital tool, not only fortifying privacy measures but also streamlining computational operations. This becomes especially crucial within fully decentralized infrastructures where local processing is imperative due to the absence of centralized aggregation. Here, we introduce DYNAWEIGHT, a novel framework to information aggregation in multi-agent networks. DYNAWEIGHT offers substantial acceleration in decentralized learning with minimal additional communication and memory overhead. Unlike traditional static weight assignments, such as Metropolis weights, DYNAWEIGHT dynamically allocates weights to neighboring servers based on their relative losses on local datasets. Consequently, it favors servers possessing diverse information, particularly in scenarios of substantial data heterogeneity. Our experiments on various datasets MNIST, CIFAR10, and CIFAR100 incorporating various server counts and graph topologies, demonstrate notable enhancements in training speeds. Notably, DYNAWEIGHT functions as an aggregation scheme compatible with any underlying server-level optimization algorithm, underscoring its versatility and potential for widespread integration.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>I-ETL: an interoperability-aware health (meta) data pipeline to enable federated analyses</title>
<link>https://arxiv.org/abs/2509.22351</link>
<guid>https://arxiv.org/abs/2509.22351</guid>
<content:encoded><![CDATA[
arXiv:2509.22351v1 Announce Type: new 
Abstract: Clinicians are interested in better understanding complex diseases, such as cancer or rare diseases, so they need to produce and exchange data to mutualize sources and join forces. To do so and ensure privacy, a natural way consists in using a decentralized architecture and Federated Learning algorithms. This ensures that data stays in the organization in which it has been collected, but requires data to be collected in similar settings and similar models. In practice, this is often not the case because healthcare institutions work individually with different representations and raw data; they do not have means to normalize their data, and even less to do so across centers. For instance, clinicians have at hand phenotypic, clinical, imaging and genomic data (each individually collected) and want to better understand some diseases by analyzing them together. This example highlights the needs and challenges for a cooperative use of this wealth of information. We designed and implemented a framework, named I-ETL, for integrating highly heterogeneous healthcare datasets of hospitals in interoperable databases. Our proposal is twofold: (i) we devise two general and extensible conceptual models for modeling both data and metadata and (ii) we propose an Extract-Transform-Load (ETL) pipeline ensuring and assessing interoperability from the start. By conducting experiments on open-source datasets, we show that I-ETL succeeds in representing various health datasets in a unified way thanks to our two general conceptual models. Next, we demonstrate the importance of blending interoperability as a first-class citizen in integration pipelines, ensuring possible collaboration between different centers.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Aware Multi-Robot Task Allocation With Strongly Coupled Inter-Robot Rewards</title>
<link>https://arxiv.org/abs/2509.22469</link>
<guid>https://arxiv.org/abs/2509.22469</guid>
<content:encoded><![CDATA[
arXiv:2509.22469v1 Announce Type: new 
Abstract: This paper proposes a task allocation algorithm for teams of heterogeneous robots in environments with uncertain task requirements. We model these requirements as probability distributions over capabilities and use this model to allocate tasks such that robots with complementary skills naturally position near uncertain tasks, proactively mitigating task failures without wasting resources. We introduce a market-based approach that optimizes the joint team objective while explicitly capturing coupled rewards between robots, offering a polynomial-time solution in decentralized settings with strict communication assumptions. Comparative experiments against benchmark algorithms demonstrate the effectiveness of our approach and highlight the challenges of incorporating coupled rewards in a decentralized formulation.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Dual-Mode Distillation with Incentive Schemes for Scalable, Heterogeneous Federated Learning on Non-IID Data</title>
<link>https://arxiv.org/abs/2509.22507</link>
<guid>https://arxiv.org/abs/2509.22507</guid>
<content:encoded><![CDATA[
arXiv:2509.22507v1 Announce Type: new 
Abstract: Federated Learning (FL) has emerged as a promising decentralized learning (DL) approach that enables the use of distributed data without compromising user privacy. However, FL poses several key challenges. First, it is frequently assumed that every client can train the same machine learning models, however, not all clients are able to meet this assumption because of differences in their business needs and computational resources. Second, statistical heterogeneity (a.k.a. non-IID data) poses a major challenge in FL, which can lead to lower global model performance. Third, while addressing these challenges, there is a need for a cost-effective incentive mechanism to encourage clients to participate in FL training. In response to these challenges, we propose several methodologies: DL-SH, which facilitates efficient, privacy-preserving, and communication-efficient learning in the context of statistical heterogeneity; DL-MH, designed to manage fully heterogeneous models while tackling statistical disparities; and I-DL-MH, an incentive-based extension of DL-MH that promotes client engagement in federated learning training by providing incentives within this complex federated learning framework. Comprehensive experiments were carried out to assess the performance and scalability of the proposed approaches across a range of complex experimental settings. This involved utilizing various model architectures, in diverse data distributions, including IID and several non-IID scenarios, as well as multiple datasets. Experimental results demonstrate that the proposed approaches significantly enhance accuracy and decrease communication costs while effectively addressing statistical heterogeneity and model heterogeneity in comparison to existing state-of-the-art approaches and baselines, with DL-SH improving global model accuracy by 153%, and I-DL-MH achieving a 225% improvement under non-IID conditions.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Technical Capability and User Accessibility: Off-grid Civilian Emergency Communication</title>
<link>https://arxiv.org/abs/2509.22568</link>
<guid>https://arxiv.org/abs/2509.22568</guid>
<content:encoded><![CDATA[
arXiv:2509.22568v1 Announce Type: new 
Abstract: During large-scale crises disrupting cellular and Internet infrastructure, civilians lack reliable methods for communication, aid coordination, and access to trustworthy information. This paper presents a unified emergency communication system integrating a low-power, long-range network with a crisis-oriented smartphone application, enabling decentralized and off-grid civilian communication. Unlike previous solutions separating physical layer resilience from user layer usability, our design merges these aspects into a cohesive crisis-tailored framework.
  The system is evaluated in two dimensions: communication performance and application functionality. Field experiments in urban Z\"urich demonstrate that the 868 MHz band, using the LongFast configuration, achieves a communication range of up to 1.2 km with 92% Packet Delivery Ratio, validating network robustness under real-world infrastructure degraded conditions. In parallel, a purpose-built mobile application featuring peer-to-peer messaging, identity verification, and community moderation was evaluated through a requirements-based analysis.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voting-Bloc Entropy: A New Metric for DAO Decentralization</title>
<link>https://arxiv.org/abs/2509.22620</link>
<guid>https://arxiv.org/abs/2509.22620</guid>
<content:encoded><![CDATA[
arXiv:2509.22620v1 Announce Type: new 
Abstract: Decentralized Autonomous Organizations (DAOs) use smart contracts to foster communities working toward common goals. Existing definitions of decentralization, however -- the 'D' in DAO -- fall short of capturing the key properties characteristic of diverse and equitable participation. This work proposes a new framework for measuring DAO decentralization called Voting-Bloc Entropy (VBE, pronounced ''vibe''). VBE is based on the idea that voters with closely aligned interests act as a centralizing force and should be modeled as such. VBE formalizes this notion by measuring the similarity of participants' utility functions across a set of voting rounds. Unlike prior, ad hoc definitions of decentralization, VBE derives from first principles: We introduce a simple (yet powerful) reinforcement learning-based conceptual model for voting, that in turn implies VBE. We first show VBE's utility as a theoretical tool. We prove a number of results about the (de)centralizing effects of vote delegation, proposal bundling, bribery, etc. that are overlooked in previous notions of DAO decentralization. Our results lead to practical suggestions for enhancing DAO decentralization. We also show how VBE can be used empirically by presenting measurement studies and VBE-based governance experiments. We make the tools we developed for these results available to the community in the form of open-source artifacts in order to facilitate future study of DAO decentralization.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linear Risk Sharing on Networks</title>
<link>https://arxiv.org/abs/2509.21411</link>
<guid>https://arxiv.org/abs/2509.21411</guid>
<content:encoded><![CDATA[
arXiv:2509.21411v1 Announce Type: cross 
Abstract: Over the past decade alternatives to traditional insurance and banking have grown in popularity. The desire to encourage local participation has lead products such as peer-to-peer insurance, reciprocal contracts, and decentralized finance platforms to increasingly rely on network structures to redistribute risk among participants. In this paper, we develop a comprehensive framework for linear risk sharing (LRS), where random losses are reallocated through nonnegative linear operators which can accommodate a wide range of networks. Building on the theory of stochastic and doubly stochastic matrices, we establish conditions under which constraints such as budget balance, fairness, and diversification are guaranteed. The convex order framework allows us to compare different allocations rigorously, highlighting variance reduction and majorization as natural consequences of doubly stochastic mixing. We then extend the analysis to network-based sharing, showing how their topology shapes risk outcomes in complete, star, ring, random, and scale-free graphs. A second layer of randomness, where the sharing matrix itself is random, is introduced via Erd\H{o}s--R\'enyi and preferential-attachment networks, connecting risk-sharing properties to degree distributions. Finally, we study convex combinations of identity and network-induced operators, capturing the trade-off between self-retention and diversification. Our results provide design principles for fair and efficient peer-to-peer insurance and network-based risk pooling, combining mathematical soundness with economic interpretability.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proving and Rewarding Client Diversity to Strengthen Resilience of Blockchain Networks</title>
<link>https://arxiv.org/abs/2411.18401</link>
<guid>https://arxiv.org/abs/2411.18401</guid>
<content:encoded><![CDATA[
arXiv:2411.18401v2 Announce Type: replace 
Abstract: Client diversity is a cornerstone of blockchain resilience, yet most networks suffer from a dangerously skewed distribution of client implementations. This monoculture exposes the network to very risky scenarios, such as massive financial losses in the event of a majority client failure. In this paper, we present a novel framework that combines verifiable execution and economic incentives to provably identify and reward the use of minority clients, thereby promoting a healthier, more robust ecosystem. Our approach leverages state-of-the-art verifiable computation (zkVMs and TEEs) to generate cryptographic proofs of client execution, which are then verified on-chain. We design and implement an end-to-end prototype of verifiable client diversity in the context of Ethereum, by modifying the popular Lighthouse client and by deploying our novel diversity-aware reward protocol. Through comprehensive experiments, we quantify the practicality of our approach, from overheads of proof production and verification to the effectiveness of the incentive mechanism. This work demonstrates, for the first time, a practical and economically viable path to encourage and ensure provable client diversity in blockchain networks. Our findings inform the design of future protocols that seek to maximize the resilience of decentralized systems
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond A Single AI Cluster: A Survey of Decentralized LLM Training</title>
<link>https://arxiv.org/abs/2503.11023</link>
<guid>https://arxiv.org/abs/2503.11023</guid>
<content:encoded><![CDATA[
arXiv:2503.11023v3 Announce Type: replace 
Abstract: The emergence of large language models (LLMs) has revolutionized AI development, yet the resource demands beyond a single cluster or even datacenter, limiting accessibility to well-resourced organizations. Decentralized training has emerged as a promising paradigm to leverage dispersed resources across clusters, datacenters and regions, offering the potential to democratize LLM development for broader communities. As the first comprehensive exploration of this emerging field, we present decentralized LLM training as a resource-driven paradigm and categorize existing efforts into community-driven and organizational approaches. We further clarify this through: (1) a comparison with related paradigms, (2) a characterization of decentralized resources, and (3) a taxonomy of recent advancements. We also provide up-to-date case studies and outline future directions to advance research in decentralized LLM training.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedOC: Multi-Server FL with Overlapping Client Relays in Wireless Edge Networks</title>
<link>https://arxiv.org/abs/2509.19398</link>
<guid>https://arxiv.org/abs/2509.19398</guid>
<content:encoded><![CDATA[
arXiv:2509.19398v1 Announce Type: new 
Abstract: Multi-server Federated Learning (FL) has emerged as a promising solution to mitigate communication bottlenecks of single-server FL. We focus on a typical multi-server FL architecture, where the regions covered by different edge servers (ESs) may overlap. A key observation of this architecture is that clients located in the overlapping areas can access edge models from multiple ESs. Building on this insight, we propose FedOC (Federated learning with Overlapping Clients), a novel framework designed to fully exploit the potential of these overlapping clients. In FedOC, overlapping clients could serve dual roles: (1) as Relay Overlapping Clients (ROCs), they forward edge models between neighboring ESs in real time to facilitate model sharing among different ESs; and (2) as Normal Overlapping Clients (NOCs), they dynamically select their initial model for local training based on the edge model delivery time, which enables indirect data fusion among different regions of ESs. The overall FedOC workflow proceeds as follows: in every round, each client trains local model based on the earliest received edge model and transmits to the respective ESs for model aggregation. Then each ES transmits the aggregated edge model to neighboring ESs through ROC relaying. Upon receiving the relayed models, each ES performs a second aggregation and subsequently broadcasts the updated model to covered clients. The existence of ROCs enables the model of each ES to be disseminated to the other ESs in a decentralized manner, which indirectly achieves intercell model and speeding up the training process, making it well-suited for latency-sensitive edge environments. Extensive experimental results show remarkable performance gains of our scheme compared to existing methods.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Sharding Advancements, Methodologies, and Adoption Potential in Hedera</title>
<link>https://arxiv.org/abs/2509.19478</link>
<guid>https://arxiv.org/abs/2509.19478</guid>
<content:encoded><![CDATA[
arXiv:2509.19478v1 Announce Type: new 
Abstract: Sharding has emerged as a critical solution to address the scalability challenges faced by blockchain networks, enabling them to achieve higher transaction throughput, reduced latency, and optimized resource usage. This paper investigates the advancements, methodologies, and adoption potential of sharding in the context of Hedera, a distributed ledger technology known for its unique Gossip about Gossip protocol and asynchronous Byzantine Fault Tolerance (ABFT). We explore various academic and industrial sharding techniques, emphasizing their benefits and trade-offs. Building on these insights, we propose a hybrid sharding solution for Hedera that partitions the network into local and global committees, facilitating efficient cross-shard transactions and ensuring robust security through dynamic reconfiguration. Our analysis highlights significant reductions in storage and communication overhead, improved scalability, and enhanced fault tolerance, demonstrating the feasibility and advantages of integrating sharding into Hedera's architecture.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Aggregation of Demand Flexibility</title>
<link>https://arxiv.org/abs/2509.19612</link>
<guid>https://arxiv.org/abs/2509.19612</guid>
<content:encoded><![CDATA[
arXiv:2509.19612v1 Announce Type: new 
Abstract: This paper proposes a federated framework for demand flexibility aggregation to support grid operations. Unlike existing geometric methods that rely on a static, pre-defined base set as the geometric template for aggregation, our framework establishes a true federated process by enabling the collaborative optimization of this base set without requiring the participants sharing sensitive data with the aggregator. Specifically, we first formulate the base set optimization problem as a bilevel program. Using optimal solution functions, we then reformulate the bilevel program into a single-level, unconstrained learning task. By exploiting the decomposable structure of the overall gradient, we further design a decentralized gradient-based algorithm to solve this learning task. The entire framework, encompassing base set optimization, aggregation, and disaggregation, operates by design without exchanging raw user data. Numerical results demonstrate that our proposed framework unlocks substantially more flexibility than the approaches with static base sets, thus providing a promising framework for efficient and privacy-enhanced approaches to coordinate demand flexibility at scale.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Governing Together: Toward Infrastructure for Community-Run Social Media</title>
<link>https://arxiv.org/abs/2509.19653</link>
<guid>https://arxiv.org/abs/2509.19653</guid>
<content:encoded><![CDATA[
arXiv:2509.19653v1 Announce Type: new 
Abstract: Decentralizing the governance of social computing systems to communities promises to empower them to make independent decisions, with nuance and in accordance with their values. Yet, communities do not govern in isolation. Many problems communities face are common, or move across their boundaries. We therefore propose designing for "inter-community governance:" mechanisms that support relationships and interactions between communities to coordinate on governance issues. Drawing from workshops with 24 individuals on decentralized, community-run social media, we present six challenges in designing for inter-community governance surfaced through ideas proposed in workshops. Together, these ideas come together as an ecosystem of resources, infrastructures, and tools that highlight three key principles for designing for inter-community governance: modularity, forkability, and polycentricity. We end with a discussion of how the ideas proposed in workshops might be implemented in future work aiming to support community governance in social computing systems broadly.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Orbital Stabilization and Time Synchronization of Unstable Periodic Motions in Underactuated Robots</title>
<link>https://arxiv.org/abs/2509.20082</link>
<guid>https://arxiv.org/abs/2509.20082</guid>
<content:encoded><![CDATA[
arXiv:2509.20082v1 Announce Type: new 
Abstract: This paper presents a control methodology for achieving orbital stabilization with simultaneous time synchronization of periodic trajectories in underactuated robotic systems. The proposed approach extends the classical transverse linearization framework to explicitly incorporate time-desynchronization dynamics. To stabilize the resulting extended transverse dynamics, we employ a combination of time-varying LQR and sliding-mode control. The theoretical results are validated experimentally through the implementation of both centralized and decentralized control strategies on a group of six Butterfly robots.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>chainScale: Secure Functionality-oriented Scalability for Decentralized Resource Markets</title>
<link>https://arxiv.org/abs/2509.20356</link>
<guid>https://arxiv.org/abs/2509.20356</guid>
<content:encoded><![CDATA[
arXiv:2509.20356v1 Announce Type: new 
Abstract: Decentralized resource markets are Web 3.0 applications that build open-access platforms for trading digital resources among users without any central management. They promise cost reduction, transparency, and flexible service provision. However, these markets usually have large workload that must be processed in a timely manner, leading to serious scalability problems. Despite the large amount of work on blockchain scalability, existing solutions are ineffective as they do not account for these markets' work models and traffic patterns.
  We introduce chainScale, a secure hybrid sidechain-sharding solution that aims to boost throughput of decentralized resource markets and reduce their latency and storage footprint. At its core, chainScale leverages dependent sidechains and functionality-oriented workload splitting to parallelize traffic processing by having each market module assigned to a sidechain. Different from sharding, chainScale does not incur any cross-sidechain transactions that tend to be costly. chainScale introduces several techniques, including hierarchical workload sharing that further sub-divides overloaded modules, and weighted miner assignment that assigns miners with vested interest in the system to critical modules' sidechains. Furthermore, chainScale employs sidechain syncing to maintain the mainchain as the single truth of system state, and pruning to discard stale records. Beside analyzing security, we build a proof-of-concept implementation for a distributed file storage market as a use case. Our experiments show that, compared to a single sidechain-based prior solution, chainScale boosts throughput by 4x and reduces confirmation latency by 5x. Also, they show that chainScale outperforms sharding by 2.5x in throughput and 3.5x in latency.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proof-of-Social-Capital: A Consensus Protocol Replacing Stake for Social Capital</title>
<link>https://arxiv.org/abs/2505.12144</link>
<guid>https://arxiv.org/abs/2505.12144</guid>
<content:encoded><![CDATA[
arXiv:2505.12144v2 Announce Type: replace 
Abstract: Consensus protocols used today in blockchains mostly rely on scarce resources such as computational power or financial stake, favoring wealthy individuals due to a high entry barrier. We propose Proof-of-Social-Capital (PoSC), a new consensus protocol fueled by social capital as a staking resource to ensure fairness and decentralization. Consensus nodes in our system do not require financial or computational resources that are expensive to acquire; instead, they require preexisting social media influence, distributing consensus power not according to wealth but social capital. Our approach integrates zkSNARK proofs, verifiable credentials with a uniqueness-enforcing mechanism to prevent Sybil attacks, and the incentive scheme that rewards engagement with social media content by followers. This work offers a new concept aligned with modern social media lifestyle applied in finance, providing a practical insight for the evolution of decentralized consensus protocols.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Learning Strategies for Estimation Error Minimization with Graph Neural Networks</title>
<link>https://arxiv.org/abs/2404.03227</link>
<guid>https://arxiv.org/abs/2404.03227</guid>
<content:encoded><![CDATA[
arXiv:2404.03227v3 Announce Type: replace-cross 
Abstract: We address the challenge of sampling and remote estimation for autoregressive Markovian processes in a multi-hop wireless network with statistically-identical agents. Agents cache the most recent samples from others and communicate over wireless collision channels governed by an underlying graph topology. Our goal is to minimize time-average estimation error and/or age of information with decentralized scalable sampling and transmission policies, considering both oblivious (where decision-making is independent of the physical processes) and non-oblivious policies (where decision-making depends on physical processes). We prove that in oblivious policies, minimizing estimation error is equivalent to minimizing the age of information. The complexity of the problem, especially the multi-dimensional action spaces and arbitrary network topologies, makes theoretical methods for finding optimal transmission policies intractable. We optimize the policies using a graphical multi-agent reinforcement learning framework, where each agent employs a permutation-equivariant graph neural network architecture. Theoretically, we prove that our proposed framework exhibits desirable transferability properties, allowing transmission policies trained on small- or moderate-size networks to be executed effectively on large-scale topologies. Numerical experiments demonstrate that (i) Our proposed framework outperforms state-of-the-art baselines; (ii) The trained policies are transferable to larger networks, and their performance gains increase with the number of agents; (iii) The training procedure withstands non-stationarity even if we utilize independent learning techniques; and, (iv) Recurrence is pivotal in both independent learning and centralized training and decentralized execution, and improves the resilience to non-stationarity in independent learning.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentor-V: Lightweight ML Training on Low-Power RISC-V Edge Devices</title>
<link>https://arxiv.org/abs/2509.18118</link>
<guid>https://arxiv.org/abs/2509.18118</guid>
<content:encoded><![CDATA[
arXiv:2509.18118v1 Announce Type: new 
Abstract: Modern IoT devices increasingly rely on machine learning solutions to process data locally. However, the lack of graphics processing units (GPUs) or dedicated accelerators on most platforms makes on-device training largely infeasible, often requiring cloud-based services to perform this task. This procedure often raises privacy-related concerns, and creates dependency on reliable and always-on connectivity. Federated Learning (FL) is a new trend that addresses these issues by enabling decentralized and collaborative training directly on devices, but it requires highly efficient optimization algorithms. L-SGD, a lightweight variant of stochastic gradient descent, has enabled neural network training on Arm Cortex-M Microcontroller Units (MCUs). This work extends L-SGD to RISC-V-based MCUs, an open and emerging architecture that still lacks robust support for on-device training. L-SGD was evaluated on both Arm and RISC-V platforms using 32-bit floating-point arithmetic, highlighting the performance impact of the absence of Floating-Point Units (FPUs) in RISC-V MCUs. To mitigate these limitations, we introduce an 8-bit quantized version of L-SGD for RISC-V, which achieves nearly 4x reduction in memory usage and a 2.2x speedup in training time, with negligible accuracy degradation.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anomaly Detection in Electric Vehicle Charging Stations Using Federated Learning</title>
<link>https://arxiv.org/abs/2509.18126</link>
<guid>https://arxiv.org/abs/2509.18126</guid>
<content:encoded><![CDATA[
arXiv:2509.18126v1 Announce Type: new 
Abstract: Federated Learning (FL) is a decentralized training framework widely used in IoT ecosystems that preserves privacy by keeping raw data local, making it ideal for IoT-enabled cyber-physical systems with sensing and communication like Smart Grids (SGs), Connected and Automated Vehicles (CAV), and Electric Vehicle Charging Stations (EVCS). With the rapid expansion of electric vehicle infrastructure, securing these IoT-based charging stations against cyber threats has become critical. Centralized Intrusion Detection Systems (IDS) raise privacy concerns due to sensitive network and user data, making FL a promising alternative. However, current FL-based IDS evaluations overlook practical challenges such as system heterogeneity and non-IID data. To address these challenges, we conducted experiments to evaluate the performance of federated learning for anomaly detection in EV charging stations under system and data heterogeneity. We used FedAvg and FedAvgM, widely studied optimization approaches, to analyze their effectiveness in anomaly detection. Under IID settings, FedAvg achieves superior performance to centralized models using the same neural network. However, performance degrades with non-IID data and system heterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous settings, showing better convergence and higher anomaly detection accuracy. Our results demonstrate that FL can handle heterogeneity in IoT-based EVCS without significant performance loss, with FedAvgM as a promising solution for robust, privacy-preserving EVCS security.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Visual Deepfake Detection: Can AI Predict and Prevent Fake Content Before It's Created?</title>
<link>https://arxiv.org/abs/2509.18461</link>
<guid>https://arxiv.org/abs/2509.18461</guid>
<content:encoded><![CDATA[
arXiv:2509.18461v1 Announce Type: new 
Abstract: Generative adversarial networks (GANs) and diffusion models have dramatically advanced deepfake technology, and its threats to digital security, media integrity, and public trust have increased rapidly. This research explored zero-shot deepfake detection, an emerging method even when the models have never seen a particular deepfake variation. In this work, we studied self-supervised learning, transformer-based zero-shot classifier, generative model fingerprinting, and meta-learning techniques that better adapt to the ever-evolving deepfake threat. In addition, we suggested AI-driven prevention strategies that mitigated the underlying generation pipeline of the deepfakes before they occurred. They consisted of adversarial perturbations for creating deepfake generators, digital watermarking for content authenticity verification, real-time AI monitoring for content creation pipelines, and blockchain-based content verification frameworks. Despite these advancements, zero-shot detection and prevention faced critical challenges such as adversarial attacks, scalability constraints, ethical dilemmas, and the absence of standardized evaluation benchmarks. These limitations were addressed by discussing future research directions on explainable AI for deepfake detection, multimodal fusion based on image, audio, and text analysis, quantum AI for enhanced security, and federated learning for privacy-preserving deepfake detection. This further highlighted the need for an integrated defense framework for digital authenticity that utilized zero-shot learning in combination with preventive deepfake mechanisms. Finally, we highlighted the important role of interdisciplinary collaboration between AI researchers, cybersecurity experts, and policymakers to create resilient defenses against the rising tide of deepfake attacks.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Agent Access (A\^3) Network: An Embodied, Communication-Aware Multi-Agent Framework for 6G Coverage</title>
<link>https://arxiv.org/abs/2509.18526</link>
<guid>https://arxiv.org/abs/2509.18526</guid>
<content:encoded><![CDATA[
arXiv:2509.18526v1 Announce Type: new 
Abstract: The vision of 6G communication demands autonomous and resilient networking in environments without fixed infrastructure. Yet most multi-agent reinforcement learning (MARL) approaches focus on isolated stages - exploration, relay formation, or access - under static deployments and centralized control, limiting adaptability. We propose the AI Agent Access (A\^3) Network, a unified, embodied intelligence-driven framework that transforms multi-agent networking into a dynamic, decentralized, and end-to-end system. Unlike prior schemes, the A\^3 Network integrates exploration, target user access, and backhaul maintenance within a single learning process, while supporting on-demand agent addition during runtime. Its decentralized policies ensure that even a single agent can operate independently with limited observations, while coordinated agents achieve scalable, communication-optimized coverage. By embedding link-level communication metrics into actor-critic learning, the A\^3 Network couples topology formation with robust decision-making. Numerical simulations demonstrate that the A\^3 Network not only balances exploration and communication efficiency but also delivers system-level adaptability absent in existing MARL frameworks, offering a new paradigm for 6G multi-agent networks.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining I2P Resilience: Effect of Centrality-based Attack</title>
<link>https://arxiv.org/abs/2509.18572</link>
<guid>https://arxiv.org/abs/2509.18572</guid>
<content:encoded><![CDATA[
arXiv:2509.18572v1 Announce Type: new 
Abstract: This study examines the robustness of I2P, a well-regarded anonymous and decentralized peer-to-peer network designed to ensure anonymity, confidentiality, and circumvention of censorship. Unlike its more widely researched counterpart, TOR, I2P's resilience has received less scholarly attention. Employing network analysis, this research evaluates I2P's susceptibility to adversarial percolation. By utilizing the degree centrality as a measure of nodes' influence in the network, the finding suggests the network is vulnerable to targeted disruptions. Before percolation, the network exhibited a density of 0.01065443 and an average path length of 6.842194. At the end of the percolation process, the density decreased by approximately 10%, and the average path length increased by 33%, indicating a decline in efficiency and connectivity. These results highlight that even decentralized networks, such as I2P, exhibit structural fragility under targeted attacks, emphasizing the need for improved design strategies to enhance resilience against adversarial disruptions.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smart Cellular Bricks for Decentralized Shape Classification and Damage Recovery</title>
<link>https://arxiv.org/abs/2509.18659</link>
<guid>https://arxiv.org/abs/2509.18659</guid>
<content:encoded><![CDATA[
arXiv:2509.18659v1 Announce Type: new 
Abstract: Biological systems possess remarkable capabilities for self-recognition and morphological regeneration, often relying solely on local interactions. Inspired by these decentralized processes, we present a novel system of physical 3D bricks--simple cubic units equipped with local communication, processing, and sensing--that are capable of inferring their global shape class and detecting structural damage. Leveraging Neural Cellular Automata (NCA), a learned, fully-distributed algorithm, our system enables each module to independently execute the same neural network without access to any global state or positioning information. We demonstrate the ability of collections of hundreds of these cellular bricks to accurately classify a variety of 3D shapes through purely local interactions. The approach shows strong robustness to out-of-distribution shape variations and high tolerance to communication faults and failed modules. In addition to shape inference, the same decentralized framework is extended to detect missing or damaged components, allowing the collective to localize structural disruptions and to guide a recovery process. This work provides a physical realization of large-scale, decentralized self-recognition and damage detection, advancing the potential of robust, adaptive, and bio-inspired modular systems.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generic Adversarial Smart Contract Detection with Semantics and Uncertainty-Aware LLM</title>
<link>https://arxiv.org/abs/2509.18934</link>
<guid>https://arxiv.org/abs/2509.18934</guid>
<content:encoded><![CDATA[
arXiv:2509.18934v1 Announce Type: new 
Abstract: Adversarial smart contracts, mostly on EVM-compatible chains like Ethereum and BSC, are deployed as EVM bytecode to exploit vulnerable smart contracts typically for financial gains. Detecting such malicious contracts at the time of deployment is an important proactive strategy preventing loss from victim contracts. It offers a better cost-benefit than detecting vulnerabilities on diverse potential victims. However, existing works are not generic with limited detection types and effectiveness due to imbalanced samples, while the emerging LLM technologies, which show its potentials in generalization, have two key problems impeding its application in this task: hard digestion of compiled-code inputs, especially those with task-specific logic, and hard assessment of LLMs' certainty in their binary answers, i.e., yes-or-no answers. Therefore, we propose a generic adversarial smart contracts detection framework FinDet, which leverages LLMs with two enhancements addressing above two problems. FinDet takes as input only the EVM-bytecode contracts and identifies adversarial ones among them with high balanced accuracy. The first enhancement extracts concise semantic intentions and high-level behavioral logic from the low-level bytecode inputs, unleashing the LLM reasoning capability restricted by the task input. The second enhancement probes and measures the LLM uncertainty to its multi-round answering to the same query, improving the LLM answering robustness for binary classifications required by the task output. Our comprehensive evaluation shows that FinDet achieves a BAC of 0.9223 and a TPR of 0.8950, significantly outperforming existing baselines. It remains robust under challenging conditions including unseen attack patterns, low-data settings, and feature obfuscation. FinDet detects all 5 public and 20+ unreported adversarial contracts in a 10-day real-world test, confirmed manually.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAPPO for Edge Server Monitoring</title>
<link>https://arxiv.org/abs/2509.19079</link>
<guid>https://arxiv.org/abs/2509.19079</guid>
<content:encoded><![CDATA[
arXiv:2509.19079v1 Announce Type: new 
Abstract: In this paper, we consider a goal-oriented communication problem for edge server monitoring, where jobs arrive intermittently at multiple dispatchers and must be assigned to shared edge servers with finite queues and time-varying availability. Accurate knowledge of server status is critical for sustaining high throughput, yet remains challenging under dynamic workloads and partial observability. To address this challenge, each dispatcher maintains server knowledge through two complementary mechanisms: (i) active status queries that provide instantaneous updates at a communication cost, and (ii) job execution feedback that reveals server conditions opportunistically. We formulate a cooperative multi-agent distributed decision-making problem in which dispatchers jointly optimize query scheduling to balance throughput against communication overhead. To solve this problem, we propose a Multi-Agent Proximal Policy Optimization (MAPPO)-based algorithm that leverages centralized training with decentralized execution (CTDE) to learn distributed query-and-dispatch policies under partial and stale observations. Numerical evaluations show that MAPPO achieves superior throughput-cost tradeoffs and significantly outperforms baseline strategies, achieving on average a 30% improvement over the closest baseline.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs as verification oracles for Solidity</title>
<link>https://arxiv.org/abs/2509.19153</link>
<guid>https://arxiv.org/abs/2509.19153</guid>
<content:encoded><![CDATA[
arXiv:2509.19153v1 Announce Type: new 
Abstract: Ensuring the correctness of smart contracts is critical, as even subtle flaws can lead to severe financial losses. While bug detection tools able to spot common vulnerability patterns can serve as a first line of defense, most real-world exploits and losses stem from errors in the contract business logic. Formal verification tools such as SolCMC and the Certora Prover address this challenge, but their impact remains limited by steep learning curves and restricted specification languages. Recent works have begun to explore the use of large language models (LLMs) for security-related tasks such as vulnerability detection and test generation. Yet, a fundamental question remains open: can LLMs serve as verification oracles, capable of reasoning about arbitrary contract-specific properties? In this paper, we provide the first systematic evaluation of GPT-5, a state-of-the-art reasoning LLM, in this role. We benchmark its performance on a large dataset of verification tasks, compare its outputs against those of established formal verification tools, and assess its practical effectiveness in real-world auditing scenarios. Our study combines quantitative metrics with qualitative analysis, and shows that recent reasoning-oriented LLMs can be surprisingly effective as verification oracles, suggesting a new frontier in the convergence of AI and formal methods for secure smart contract development and auditing.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A decentralized future for the open-science databases</title>
<link>https://arxiv.org/abs/2509.19206</link>
<guid>https://arxiv.org/abs/2509.19206</guid>
<content:encoded><![CDATA[
arXiv:2509.19206v1 Announce Type: new 
Abstract: Continuous and reliable access to curated biological data repositories is indispensable for accelerating rigorous scientific inquiry and fostering reproducible research. Centralized repositories, though widely used, are vulnerable to single points of failure arising from cyberattacks, technical faults, natural disasters, or funding and political uncertainties. This can lead to widespread data unavailability, data loss, integrity compromises, and substantial delays in critical research, ultimately impeding scientific progress. Centralizing essential scientific resources in a single geopolitical or institutional hub is inherently dangerous, as any disruption can paralyze diverse ongoing research. The rapid acceleration of data generation, combined with an increasingly volatile global landscape, necessitates a critical re-evaluation of the sustainability of centralized models. Implementing federated and decentralized architectures presents a compelling and future-oriented pathway to substantially strengthen the resilience of scientific data infrastructures, thereby mitigating vulnerabilities and ensuring the long-term integrity of data. Here, we examine the structural limitations of centralized repositories, evaluate federated and decentralized models, and propose a hybrid framework for resilient, FAIR, and sustainable scientific data stewardship. Such an approach offers a significant reduction in exposure to governance instability, infrastructural fragility, and funding volatility, and also fosters fairness and global accessibility. The future of open science depends on integrating these complementary approaches to establish a globally distributed, economically sustainable, and institutionally robust infrastructure that safeguards scientific data as a public good, further ensuring continued accessibility, interoperability, and preservation for generations to come.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stability and Generalization of Adversarial Diffusion Training</title>
<link>https://arxiv.org/abs/2509.19234</link>
<guid>https://arxiv.org/abs/2509.19234</guid>
<content:encoded><![CDATA[
arXiv:2509.19234v1 Announce Type: new 
Abstract: Algorithmic stability is an established tool for analyzing generalization. While adversarial training enhances model robustness, it often suffers from robust overfitting and an enlarged generalization gap. Although recent work has established the convergence of adversarial training in decentralized networks, its generalization properties remain unexplored. This work presents a stability-based generalization analysis of adversarial training under the diffusion strategy for convex losses. We derive a bound showing that the generalization error grows with both the adversarial perturbation strength and the number of training steps, a finding consistent with single-agent case but novel for decentralized settings. Numerical experiments on logistic regression validate these theoretical predictions.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Filtering amplitude dependence of correlation dynamics in complex systems: application to the cryptocurrency market</title>
<link>https://arxiv.org/abs/2509.18820</link>
<guid>https://arxiv.org/abs/2509.18820</guid>
<content:encoded><![CDATA[
arXiv:2509.18820v1 Announce Type: cross 
Abstract: Based on the cryptocurrency market dynamics, this study presents a general methodology for analyzing evolving correlation structures in complex systems using the $q$-dependent detrended cross-correlation coefficient \rho(q,s). By extending traditional metrics, this approach captures correlations at varying fluctuation amplitudes and time scales. The method employs $q$-dependent minimum spanning trees ($q$MSTs) to visualize evolving network structures. Using minute-by-minute exchange rate data for 140 cryptocurrencies on Binance (Jan 2021-Oct 2024), a rolling window analysis reveals significant shifts in $q$MSTs, notably around April 2022 during the Terra/Luna crash. Initially centralized around Bitcoin (BTC), the network later decentralized, with Ethereum (ETH) and others gaining prominence. Spectral analysis confirms BTC's declining dominance and increased diversification among assets. A key finding is that medium-scale fluctuations exhibit stronger correlations than large-scale ones, with $q$MSTs based on the latter being more decentralized. Properly exploiting such facts may offer the possibility of a more flexible optimal portfolio construction. Distance metrics highlight that major disruptions amplify correlation differences, leading to fully decentralized structures during crashes. These results demonstrate $q$MSTs' effectiveness in uncovering fluctuation-dependent correlations, with potential applications beyond finance, including biology, social and other complex systems.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Atomic Smart Contract Interoperability with High Efficiency via Cross-Chain Integrated Execution</title>
<link>https://arxiv.org/abs/2502.12820</link>
<guid>https://arxiv.org/abs/2502.12820</guid>
<content:encoded><![CDATA[
arXiv:2502.12820v2 Announce Type: replace 
Abstract: With the development of Ethereum, numerous blockchains compatible with Ethereum's execution environment (i.e., Ethereum Virtual Machine, EVM) have emerged. Developers can leverage smart contracts to run various complex decentralized applications on top of blockchains. However, the increasing number of EVM-compatible blockchains has introduced significant challenges in cross-chain interoperability, particularly in ensuring efficiency and atomicity for the whole cross-chain application. Existing solutions are either limited in guaranteeing overall atomicity for the cross-chain application, or inefficient due to the need for multiple rounds of cross-chain smart contract execution. To address this gap, we propose IntegrateX, an efficient cross-chain interoperability system that ensures the overall atomicity of cross-chain smart contract invocations. The core idea is to deploy the logic required for cross-chain execution onto a single blockchain, where it can be executed in an integrated manner. This allows cross-chain applications to perform all cross-chain logic efficiently within the same blockchain. IntegrateX consists of a cross-chain smart contract deployment protocol and a cross-chain smart contract integrated execution protocol. The former achieves efficient and secure cross-chain deployment by decoupling smart contract logic from state, and employing an off-chain cross-chain deployment mechanism combined with on-chain cross-chain verification. The latter ensures atomicity of cross-chain invocations through a 2PC-based mechanism, and enhances performance through transaction aggregation and fine-grained state lock. We implement a prototype of IntegrateX. Extensive experiments demonstrate that it reduces up to 61.2% latency compared to the state-of-the-art baseline while maintaining low gas consumption.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fully Decentralized Cooperative Multi-Agent Reinforcement Learning is A Context Modeling Problem</title>
<link>https://arxiv.org/abs/2509.15519</link>
<guid>https://arxiv.org/abs/2509.15519</guid>
<content:encoded><![CDATA[
arXiv:2509.15519v1 Announce Type: new 
Abstract: This paper studies fully decentralized cooperative multi-agent reinforcement learning, where each agent solely observes the states, its local actions, and the shared rewards. The inability to access other agents' actions often leads to non-stationarity during value function updates and relative overgeneralization during value function estimation, hindering effective cooperative policy learning. However, existing works fail to address both issues simultaneously, due to their inability to model the joint policy of other agents in a fully decentralized setting. To overcome this limitation, we propose a novel method named Dynamics-Aware Context (DAC), which formalizes the task, as locally perceived by each agent, as an Contextual Markov Decision Process, and further addresses both non-stationarity and relative overgeneralization through dynamics-aware context modeling. Specifically, DAC attributes the non-stationary local task dynamics of each agent to switches between unobserved contexts, each corresponding to a distinct joint policy. Then, DAC models the step-wise dynamics distribution using latent variables and refers to them as contexts. For each agent, DAC introduces a context-based value function to address the non-stationarity issue during value function update. For value function estimation, an optimistic marginal value is derived to promote the selection of cooperative actions, thereby addressing the relative overgeneralization issue. Experimentally, we evaluate DAC on various cooperative tasks (including matrix game, predator and prey, and SMAC), and its superior performance against multiple baselines validates its effectiveness.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonconvex Decentralized Stochastic Bilevel Optimization under Heavy-Tailed Noises</title>
<link>https://arxiv.org/abs/2509.15543</link>
<guid>https://arxiv.org/abs/2509.15543</guid>
<content:encoded><![CDATA[
arXiv:2509.15543v1 Announce Type: new 
Abstract: Existing decentralized stochastic optimization methods assume the lower-level loss function is strongly convex and the stochastic gradient noise has finite variance. These strong assumptions typically are not satisfied in real-world machine learning models. To address these limitations, we develop a novel decentralized stochastic bilevel optimization algorithm for the nonconvex bilevel optimization problem under heavy-tailed noises. Specifically, we develop a normalized stochastic variance-reduced bilevel gradient descent algorithm, which does not rely on any clipping operation. Moreover, we establish its convergence rate by innovatively bounding interdependent gradient sequences under heavy-tailed noises for nonconvex decentralized bilevel optimization problems. As far as we know, this is the first decentralized bilevel optimization algorithm with rigorous theoretical guarantees under heavy-tailed noises. The extensive experimental results confirm the effectiveness of our algorithm in handling heavy-tailed noises.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Angelfish: Consensus with Optimal Throughput and Latency Across the Leader-DAG Spectrum</title>
<link>https://arxiv.org/abs/2509.15847</link>
<guid>https://arxiv.org/abs/2509.15847</guid>
<content:encoded><![CDATA[
arXiv:2509.15847v1 Announce Type: new 
Abstract: To maximize performance, many modern blockchain systems rely on eventually-synchronous, Byzantine fault-tolerant (BFT) consensus protocols. Two protocol designs have emerged in this space: protocols that minimize latency using a leader that drives both data dissemination and consensus, and protocols that maximize throughput using a separate, asynchronous data dissemination layer. Recent protocols such as Partially-Synchronous Bullshark and Sailfish combine elements of both approaches by using a DAG to enable parallel data dissemination and a leader that paces DAG formation. This improves latency while achieving state-of-the-art throughput. Yet the latency of leader-based protocols is still better under moderate loads.
  We present Angelfish, a hybrid protocol that adapts smoothly across this design space, from leader-based to Sailfish-like DAG-based consensus. Angelfish lets a dynamically-adjusted subset of parties use best-effort broadcast to issue lightweight votes instead of reliably broadcasting costlier DAG vertices. This reduces communication, helps lagging nodes catch up, and lowers latency in practice compared to prior DAG-based protocols. Our empirical evaluation shows that Angelfish attains state-of-the-art peak throughput while matching the latency of leader-based protocols under moderate throughput, delivering the best of both worlds.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Swarm Oracle: Trustless Blockchain Agreements through Robot Swarms</title>
<link>https://arxiv.org/abs/2509.15956</link>
<guid>https://arxiv.org/abs/2509.15956</guid>
<content:encoded><![CDATA[
arXiv:2509.15956v1 Announce Type: new 
Abstract: Blockchain consensus, rooted in the principle ``don't trust, verify'', limits access to real-world data, which may be ambiguous or inaccessible to some participants. Oracles address this limitation by supplying data to blockchains, but existing solutions may reduce autonomy, transparency, or reintroduce the need for trust. We propose Swarm Oracle: a decentralized network of autonomous robots -- that is, a robot swarm -- that use onboard sensors and peer-to-peer communication to collectively verify real-world data and provide it to smart contracts on public blockchains. Swarm Oracle leverages the built-in decentralization, fault tolerance and mobility of robot swarms, which can flexibly adapt to meet information requests on-demand, even in remote locations. Unlike typical cooperative robot swarms, Swarm Oracle integrates robots from multiple stakeholders, protecting the system from single-party biases but also introducing potential adversarial behavior. To ensure the secure, trustless and global consensus required by blockchains, we employ a Byzantine fault-tolerant protocol that enables robots from different stakeholders to operate together, reaching social agreements of higher quality than the estimates of individual robots. Through extensive experiments using both real and simulated robots, we showcase how consensus on uncertain environmental information can be achieved, despite several types of attacks orchestrated by large proportions of the robots, and how a reputation system based on blockchain tokens lets Swarm Oracle autonomously recover from faults and attacks, a requirement for long-term operation.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Exclusive are Ethereum Transactions? Evidence from non-winning blocks</title>
<link>https://arxiv.org/abs/2509.16052</link>
<guid>https://arxiv.org/abs/2509.16052</guid>
<content:encoded><![CDATA[
arXiv:2509.16052v1 Announce Type: new 
Abstract: We analyze 15,097 blocks proposed for inclusion in Ethereum's blockchain over an 8-minute window on December 3, 2024, during which 38 blocks were added to the chain. We classify transactions as exclusive -- present only in blocks from a single builder -- or private -- absent from the public mempool but included in blocks from multiple builders. We find that exclusive transactions account for 84% of the total fees paid by transactions in winning blocks. Furthermore, we show that exclusivity cannot be fully explained by exclusive relationships between senders and builders: about 7% of all exclusive transactions included on-chain, by value, come from senders who route exclusively to a single builder. Analyzing transaction logs shows that some exclusive transactions are duplicates or variations of the same strategy, but even accounting for that, the share of the total fees paid by transactions in winning blocks is at least 77.2%. Taken together, our findings highlight that exclusive transactions are the dominant source of builder revenues.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Quadruped Cooperative Object Transport: Learning Decentralized Pinch-Lift-Move</title>
<link>https://arxiv.org/abs/2509.14342</link>
<guid>https://arxiv.org/abs/2509.14342</guid>
<content:encoded><![CDATA[
arXiv:2509.14342v1 Announce Type: new 
Abstract: We study decentralized cooperative transport using teams of N-quadruped robots with arm that must pinch, lift, and move ungraspable objects through physical contact alone. Unlike prior work that relies on rigid mechanical coupling between robots and objects, we address the more challenging setting where mechanically independent robots must coordinate through contact forces alone without any communication or centralized control. To this end, we employ a hierarchical policy architecture that separates base locomotion from arm control, and propose a constellation reward formulation that unifies position and orientation tracking to enforce rigid contact behavior. The key insight is encouraging robots to behave as if rigidly connected to the object through careful reward design and training curriculum rather than explicit mechanical constraints. Our approach enables coordination through shared policy parameters and implicit synchronization cues - scaling to arbitrary team sizes without retraining. We show extensive simulation experiments to demonstrate robust transport across 2-10 robots on diverse object geometries and masses, along with sim2real transfer results on lightweight objects.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks</title>
<link>https://arxiv.org/abs/2509.14380</link>
<guid>https://arxiv.org/abs/2509.14380</guid>
<content:encoded><![CDATA[
arXiv:2509.14380v1 Announce Type: new 
Abstract: Multi-Agent Reinforcement Learning (MARL) provides a powerful framework for learning coordination in multi-agent systems. However, applying MARL to robotics still remains challenging due to high-dimensional continuous joint action spaces, complex reward design, and non-stationary transitions inherent to decentralized settings. On the other hand, humans learn complex coordination through staged curricula, where long-horizon behaviors are progressively built upon simpler skills. Motivated by this, we propose CRAFT: Coaching Reinforcement learning Autonomously using Foundation models for multi-robot coordination Tasks, a framework that leverages the reasoning capabilities of foundation models to act as a "coach" for multi-robot coordination. CRAFT automatically decomposes long-horizon coordination tasks into sequences of subtasks using the planning capability of Large Language Models (LLMs). In what follows, CRAFT trains each subtask using reward functions generated by LLM, and refines them through a Vision Language Model (VLM)-guided reward-refinement loop. We evaluate CRAFT on multi-quadruped navigation and bimanual manipulation tasks, demonstrating its capability to learn complex coordination behaviors. In addition, we validate the multi-quadruped navigation policy in real hardware experiments.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry-Aware Decentralized Sinkhorn for Wasserstein Barycenters</title>
<link>https://arxiv.org/abs/2509.14521</link>
<guid>https://arxiv.org/abs/2509.14521</guid>
<content:encoded><![CDATA[
arXiv:2509.14521v1 Announce Type: new 
Abstract: Distributed systems require fusing heterogeneous local probability distributions into a global summary over sparse and unreliable communication networks. Traditional consensus algorithms, which average distributions in Euclidean space, ignore their inherent geometric structure, leading to misleading results. Wasserstein barycenters offer a geometry-aware alternative by minimizing optimal transport costs, but their entropic approximations via the Sinkhorn algorithm typically require centralized coordination. This paper proposes a fully decentralized Sinkhorn algorithm that reformulates the centralized geometric mean as an arithmetic average in the log-domain, enabling approximation through local gossip protocols. Agents exchange log-messages with neighbors, interleaving consensus phases with local updates to mimic centralized iterations without a coordinator. To optimize bandwidth, we integrate event-triggered transmissions and b-bit quantization, providing tunable trade-offs between accuracy and communication while accommodating asynchrony and packet loss. Under mild assumptions, we prove convergence to a neighborhood of the centralized entropic barycenter, with bias linearly dependent on consensus tolerance, trigger threshold, and quantization error. Complexity scales near-linearly with network size. Simulations confirm near-centralized accuracy with significantly fewer messages, across various topologies and conditions.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Software-Defined Radio Testbed for Distributed LiDAR Point Cloud Sharing with IEEE 802.11p in V2V Networks</title>
<link>https://arxiv.org/abs/2509.14523</link>
<guid>https://arxiv.org/abs/2509.14523</guid>
<content:encoded><![CDATA[
arXiv:2509.14523v1 Announce Type: new 
Abstract: We present a Software Defined Radio (SDR)-based IEEE 802.11p testbed for distributed Vehicle-to-Vehicle (V2V) communication. The platform bridges the gap between network simulation and deployment by providing a modular codebase configured for cost-effective ADALM-Pluto SDRs. Any device capable of running a Docker with ROS, executing Matlab and interface with a Pluto via USB can act as a communication node. To demonstrate collaborative sensing, we share LiDAR point clouds between nodes and fuse them into a collective perception environment. We evaluated a theoretical model for leveraging decentralized storage systems (IPFS and Filecoin), analyzing constraints such as node storage convergence, latency, and scalability. In addition, we provide a channel quality study.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEED: A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.14680</link>
<guid>https://arxiv.org/abs/2509.14680</guid>
<content:encoded><![CDATA[
arXiv:2509.14680v1 Announce Type: new 
Abstract: Multi-agent reinforcement learning (MARL) holds substantial promise for intelligent decision-making in complex environments. However, it suffers from a coordination and scalability bottleneck as the number of agents increases. To address these issues, we propose the LLM-empowered expert demonstrations framework for multi-agent reinforcement learning (LEED). LEED consists of two components: a demonstration generation (DG) module and a policy optimization (PO) module. Specifically, the DG module leverages large language models to generate instructions for interacting with the environment, thereby producing high-quality demonstrations. The PO module adopts a decentralized training paradigm, where each agent utilizes the generated demonstrations to construct an expert policy loss, which is then integrated with its own policy loss. This enables each agent to effectively personalize and optimize its local policy based on both expert knowledge and individual experience. Experimental results show that LEED achieves superior sample efficiency, time efficiency, and robust scalability compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Federated Learning for Social Network with Mobility</title>
<link>https://arxiv.org/abs/2509.14938</link>
<guid>https://arxiv.org/abs/2509.14938</guid>
<content:encoded><![CDATA[
arXiv:2509.14938v1 Announce Type: new 
Abstract: Federated Learning (FL) offers a decentralized solution that allows collaborative local model training and global aggregation, thereby protecting data privacy. In conventional FL frameworks, data privacy is typically preserved under the assumption that local data remains absolutely private, whereas the mobility of clients is frequently neglected in explicit modeling. In this paper, we propose a hierarchical federated learning framework based on the social network with mobility namely HFL-SNM that considers both data sharing among clients and their mobility patterns. Under the constraints of limited resources, we formulate a joint optimization problem of resource allocation and client scheduling, which objective is to minimize the energy consumption of clients during the FL process. In social network, we introduce the concepts of Effective Data Coverage Rate and Redundant Data Coverage Rate. We analyze the impact of effective data and redundant data on the model performance through preliminary experiments. We decouple the optimization problem into multiple sub-problems, analyze them based on preliminary experimental results, and propose Dynamic Optimization in Social Network with Mobility (DO-SNM) algorithm. Experimental results demonstrate that our algorithm achieves superior model performance while significantly reducing energy consumption, compared to traditional baseline algorithms.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blockchain-Enabled Explainable AI for Trusted Healthcare Systems</title>
<link>https://arxiv.org/abs/2509.14987</link>
<guid>https://arxiv.org/abs/2509.14987</guid>
<content:encoded><![CDATA[
arXiv:2509.14987v1 Announce Type: new 
Abstract: This paper introduces a Blockchain-Integrated Explainable AI Framework (BXHF) for healthcare systems to tackle two essential challenges confronting health information networks: safe data exchange and comprehensible AI-driven clinical decision-making. Our architecture incorporates blockchain, ensuring patient records are immutable, auditable, and tamper-proof, alongside Explainable AI (XAI) methodologies that yield transparent and clinically relevant model predictions. By incorporating security assurances and interpretability requirements into a unified optimization pipeline, BXHF ensures both data-level trust (by verified and encrypted record sharing) and decision-level trust (with auditable and clinically aligned explanations). Its hybrid edge-cloud architecture allows for federated computation across different institutions, enabling collaborative analytics while protecting patient privacy. We demonstrate the framework's applicability through use cases such as cross-border clinical research networks, uncommon illness detection and high-risk intervention decision support. By ensuring transparency, auditability, and regulatory compliance, BXHF improves the credibility, uptake, and effectiveness of AI in healthcare, laying the groundwork for safer and more reliable clinical decision-making.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Hypergraph Learning with Local Differential Privacy: Toward Privacy-Aware Hypergraph Structure Completion</title>
<link>https://arxiv.org/abs/2408.05160</link>
<guid>https://arxiv.org/abs/2408.05160</guid>
<content:encoded><![CDATA[
arXiv:2408.05160v3 Announce Type: replace 
Abstract: The rapid growth of graph-structured data necessitates partitioning and distributed storage across decentralized systems, driving the emergence of federated graph learning to collaboratively train Graph Neural Networks (GNNs) without compromising privacy. However, current methods exhibit limited performance when handling hypergraphs, which inherently represent complex high-order relationships beyond pairwise connections. Partitioning hypergraph structures across federated subsystems amplifies structural complexity, hindering high-order information mining and compromising local information integrity. To bridge the gap between hypergraph learning and federated systems, we develop FedHGL, a first-of-its-kind framework for federated hypergraph learning on disjoint and privacy-constrained hypergraph partitions. Beyond collaboratively training a comprehensive hypergraph neural network across multiple clients, FedHGL introduces a pre-propagation hyperedge completion mechanism to preserve high-order structural integrity within each client. This procedure leverages the federated central server to perform cross-client hypergraph convolution without exposing internal topological information, effectively mitigating the high-order information loss induced by subgraph partitioning. Furthermore, by incorporating two kinds of local differential privacy (LDP) mechanisms, we provide formal privacy guarantees for this process, ensuring that sensitive node features remain protected against inference attacks from potentially malicious servers or clients. Experimental results on seven real-world datasets confirm the effectiveness of our approach and demonstrate its performance advantages over traditional federated graph learning methods.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication-Efficient and Privacy-Adaptable Mechanism for Federated Learning</title>
<link>https://arxiv.org/abs/2501.12046</link>
<guid>https://arxiv.org/abs/2501.12046</guid>
<content:encoded><![CDATA[
arXiv:2501.12046v2 Announce Type: replace 
Abstract: Training machine learning models on decentralized private data via federated learning (FL) poses two key challenges: communication efficiency and privacy protection. In this work, we address these challenges within the trusted aggregator model by introducing a novel approach called the Communication-Efficient and Privacy-Adaptable Mechanism (CEPAM), achieving both objectives simultaneously. In particular, CEPAM leverages the rejection-sampled universal quantizer (RSUQ), a construction of randomized vector quantizer whose resulting distortion is equivalent to a prescribed noise, such as Gaussian or Laplace noise, enabling joint differential privacy and compression. Our CEPAM provides the additional benefit of privacy adaptability, allowing clients and the server to customize privacy protection based on required accuracy and protection. We theoretically analyze the privacy guarantee of CEPAM and investigate the trade-offs among user privacy and accuracy of CEPAM through experimental evaluations. Moreover, we assess CEPAM's utility performance using MNIST dataset, demonstrating that CEPAM surpasses baseline models in terms of learning accuracy.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.04317</link>
<guid>https://arxiv.org/abs/2505.04317</guid>
<content:encoded><![CDATA[
arXiv:2505.04317v4 Announce Type: replace 
Abstract: In this paper, we tackle the problem of learning to play 3v3 multi-drone volleyball, a new embodied competitive task that requires both high-level strategic coordination and low-level agile control. The task is turn-based, multi-agent, and physically grounded, posing significant challenges due to its long-horizon dependencies, tight inter-agent coupling, and the underactuated dynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play (HCSP), a hierarchical reinforcement learning framework that separates centralized high-level strategic decision-making from decentralized low-level motion control. We design a three-stage population-based training pipeline to enable both strategy and skill to emerge from scratch without expert demonstrations: (I) training diverse low-level skills, (II) learning high-level strategy via self-play with fixed low-level skills, and (III) joint fine-tuning through co-self-play. Experiments show that HCSP achieves superior performance, outperforming non-hierarchical self-play and rule-based hierarchical baselines with an average 82.9% win rate and a 71.5% win rate against the two-stage variant. Moreover, co-self-play leads to emergent team behaviors such as role switching and coordinated formations, demonstrating the effectiveness of our hierarchical design and training scheme. The project page is at https://sites.google.com/view/hi-co-self-play.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Timestamp Manipulation: Timestamp-based Nakamoto-style Blockchains are Vulnerable</title>
<link>https://arxiv.org/abs/2505.05328</link>
<guid>https://arxiv.org/abs/2505.05328</guid>
<content:encoded><![CDATA[
arXiv:2505.05328v4 Announce Type: replace 
Abstract: Nakamoto consensus are the most widely adopted decentralized consensus mechanism in cryptocurrency systems. Since it was proposed in 2008, many studies have focused on analyzing its security. Most of them focus on maximizing the profit of the adversary. Examples include the selfish mining attack [FC '14] and the recent riskless uncle maker (RUM) attack [CCS '23]. In this work, we introduce the Staircase-Unrestricted Uncle Maker (SUUM), the first block withholding attack targeting the timestamp-based Nakamoto-style blockchain. Through block withholding, timestamp manipulation, and difficulty risk control, SUUM adversaries are capable of launching persistent attacks with zero cost and minimal difficulty risk characteristics, indefinitely exploiting rewards from honest participants. This creates a self-reinforcing cycle that threatens the security of blockchains. We conduct a comprehensive and systematic evaluation of SUUM, including the attack conditions, its impact on blockchains, and the difficulty risks. Finally, we further discuss four feasible mitigation measures against SUUM.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SwarmSearch: Decentralized Search Engine with Self-Funding Economy</title>
<link>https://arxiv.org/abs/2505.07452</link>
<guid>https://arxiv.org/abs/2505.07452</guid>
<content:encoded><![CDATA[
arXiv:2505.07452v2 Announce Type: replace 
Abstract: Centralized search engines control what we see, read, believe, and vote. Consequently, they raise concerns over information control, censorship, and bias. Decentralized search engines offer a remedy to this problem, but their adoption has been hindered by their inferior quality and lack of a self-sustaining economic framework. We present SwarmSearch, a fully decentralized, AI-powered search engine with a self-funding architecture. Our system is designed for deployment within the decentralized file-sharing software Tribler. SwarmSearch integrates volunteer-based with profit-driven mechanisms to foster an implicit marketplace for resources. Employing the state-of-the-art of AI-based retrieval and relevance ranking, we also aim to close the quality gap between decentralized search and centralized alternatives. Our system demonstrates high retrieval accuracy while showing robustness in the presence of 50% adversarial nodes.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trustless Autonomy: Understanding Motivations, Benefits, and Governance Dilemmas in Self-Sovereign Decentralized AI Agents</title>
<link>https://arxiv.org/abs/2505.09757</link>
<guid>https://arxiv.org/abs/2505.09757</guid>
<content:encoded><![CDATA[
arXiv:2505.09757v2 Announce Type: replace 
Abstract: The recent trend of self-sovereign Decentralized AI Agents (DeAgents) combines Large Language Model (LLM)-based AI agents with decentralization technologies such as blockchain smart contracts and trusted execution environments (TEEs). These tamper-resistant trustless substrates allow agents to achieve self-sovereignty through ownership of cryptowallet private keys and control of digital assets and social media accounts. DeAgents eliminate centralized control and reduce human intervention, addressing key trust concerns inherent in centralized AI systems. This contributes to social computing by enabling new human cooperative paradigm "intelligence as commons." However, given ongoing challenges in LLM reliability such as hallucinations, this creates paradoxical tension between trustlessness and unreliable autonomy. This study addresses this empirical research gap through interviews with DeAgents stakeholders-experts, founders, and developers-to examine their motivations, benefits, and governance dilemmas. The findings will guide future DeAgents system and protocol design and inform discussions about governance in sociotechnical AI systems in the future agentic web.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vehicle-to-Grid Integration: Ensuring Grid Stability, Strengthening Cybersecurity, and Advancing Energy Market Dynamics</title>
<link>https://arxiv.org/abs/2509.13393</link>
<guid>https://arxiv.org/abs/2509.13393</guid>
<content:encoded><![CDATA[
arXiv:2509.13393v1 Announce Type: new 
Abstract: The increasing adoption of electric vehicles has spurred significant interest in Vehicle-to-Grid technology as a transformative approach to modern energy systems. This paper presents a systematic review of V2G systems, focusing on their integration challenges and potential solutions. First, the current state of V2G development is examined, highlighting its growing importance in mitigating peak demand, enhancing voltage and frequency regulation, and reinforcing grid resilience. The study underscores the pivotal role of artificial intelligence and machine learning in optimizing energy management, load forecasting, and real-time grid control. A critical analysis of cybersecurity risks reveals heightened vulnerabilities stemming from V2G's dependence on interconnected networks and real-time data exchange, prompting an exploration of advanced mitigation strategies, including federated learning, blockchain, and quantum-resistant cryptography. Furthermore, the paper reviews economic and market aspects, including business models (V2G as an aggregator or due to self-consumption), regulation (as flexibility service provider) and factors influencing user acceptance shaping V2G adoption. Data from global case studies and pilot programs offer a snapshot of how V2G has been implemented at different paces across regions. Finally, the study suggests a multi-layered framework that incorporates grid stability resilience, cybersecurity resiliency, and energy market dynamics and provides strategic recommendations to enable scalable, secure, and economically viable V2G deployment.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Secure UAV-assisted Federated Learning: A Digital Twin-Driven Approach with Zero-Knowledge Proofs</title>
<link>https://arxiv.org/abs/2509.13634</link>
<guid>https://arxiv.org/abs/2509.13634</guid>
<content:encoded><![CDATA[
arXiv:2509.13634v1 Announce Type: new 
Abstract: Federated learning (FL) has gained popularity as a privacy-preserving method of training machine learning models on decentralized networks. However to ensure reliable operation of UAV-assisted FL systems, issues like as excessive energy consumption, communication inefficiencies, and security vulnerabilities must be solved. This paper proposes an innovative framework that integrates Digital Twin (DT) technology and Zero-Knowledge Federated Learning (zkFed) to tackle these challenges. UAVs act as mobile base stations, allowing scattered devices to train FL models locally and upload model updates for aggregation. By incorporating DT technology, our approach enables real-time system monitoring and predictive maintenance, improving UAV network efficiency. Additionally, Zero-Knowledge Proofs (ZKPs) strengthen security by allowing model verification without exposing sensitive data. To optimize energy efficiency and resource management, we introduce a dynamic allocation strategy that adjusts UAV flight paths, transmission power, and processing rates based on network conditions. Using block coordinate descent and convex optimization techniques, our method significantly reduces system energy consumption by up to 29.6% compared to conventional FL approaches. Simulation results demonstrate improved learning performance, security, and scalability, positioning this framework as a promising solution for next-generation UAV-based intelligent networks.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dissipativity-Based Data-Driven Decentralized Control of Interconnected Systems</title>
<link>https://arxiv.org/abs/2509.14047</link>
<guid>https://arxiv.org/abs/2509.14047</guid>
<content:encoded><![CDATA[
arXiv:2509.14047v1 Announce Type: new 
Abstract: We propose data-driven decentralized control algorithms for stabilizing interconnected systems. We first derive a data-driven condition to synthesize a local controller that ensures the dissipativity of the local subsystems. Then, we propose data-driven decentralized stability conditions for the global system based on the dissipativity of each local system. Since both conditions take the form of linear matrix inequalities and are based on dissipativity theory, this yields a unified pipeline, resulting in a data-driven decentralized control algorithm. As a special case, we also consider stabilizing systems interconnected through diffusive coupling and propose a control algorithm. We validate the effectiveness and the scalability of the proposed control algorithms in numerical examples in the context of microgrids.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrazyMARL: Decentralized Direct Motor Control Policies for Cooperative Aerial Transport of Cable-Suspended Payloads</title>
<link>https://arxiv.org/abs/2509.14126</link>
<guid>https://arxiv.org/abs/2509.14126</guid>
<content:encoded><![CDATA[
arXiv:2509.14126v1 Announce Type: new 
Abstract: Collaborative transportation of cable-suspended payloads by teams of Unmanned Aerial Vehicles (UAVs) has the potential to enhance payload capacity, adapt to different payload shapes, and provide built-in compliance, making it attractive for applications ranging from disaster relief to precision logistics. However, multi-UAV coordination under disturbances, nonlinear payload dynamics, and slack--taut cable modes remains a challenging control problem. To our knowledge, no prior work has addressed these cable mode transitions in the multi-UAV context, instead relying on simplifying rigid-link assumptions. We propose CrazyMARL, a decentralized Reinforcement Learning (RL) framework for multi-UAV cable-suspended payload transport. Simulation results demonstrate that the learned policies can outperform classical decentralized controllers in terms of disturbance rejection and tracking precision, achieving an 80% recovery rate from harsh conditions compared to 44% for the baseline method. We also achieve successful zero-shot sim-to-real transfer and demonstrate that our policies are highly robust under harsh conditions, including wind, random external disturbances, and transitions between slack and taut cable dynamics. This work paves the way for autonomous, resilient UAV teams capable of executing complex payload missions in unstructured environments.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Complete Decentralization of Linear Quadratic Gaussian Control for the Discrete Wave Equation</title>
<link>https://arxiv.org/abs/2509.13446</link>
<guid>https://arxiv.org/abs/2509.13446</guid>
<content:encoded><![CDATA[
arXiv:2509.13446v1 Announce Type: cross 
Abstract: The linear quadratic Gaussian (LQG) control problem for the linear wave equation on the unit circle with fully distributed actuation and partial state measurements is considered. An analytical solution to a spatial discretization of the problem is obtained. The main result of this work illustrates that for specific parameter values, the optimal LQG policy is completely decentralized, meaning only a measurement at spatial location $i$ is needed to compute an optimal control signal to actuate at this location. The relationship between performance and decentralization as a function of parameters is explored. Conditions for complete decentralization are related to metrics of kinetic and potential energy quantities and control effort.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Networks for Next-Generation-IoT: Recent Advances and Open Challenges</title>
<link>https://arxiv.org/abs/2412.20634</link>
<guid>https://arxiv.org/abs/2412.20634</guid>
<content:encoded><![CDATA[
arXiv:2412.20634v3 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) have emerged as a powerful framework for modeling complex interconnected systems, hence making them particularly well-suited to address the growing challenges of next-generation Internet of Things (NG-IoT) networks. Existing studies remain fragmented, and there is a lack of comprehensive guidance on how GNNs can be systematically applied to NG-IoT systems. As NG-IoT systems evolve toward 6G, they incorporate diverse technologies. These advances promise unprecedented connectivity, sensing, and automation but also introduce significant complexity, requiring new approaches for scalable learning, dynamic optimization, and secure, decentralized decision-making. This survey provides a comprehensive and forward-looking exploration of how GNNs can empower NG-IoT environments. We commence by exploring the fundamental paradigms of GNNs and articulating the motivation for their use in NG-IoT networks. Besides, we intrinsically connect GNNs with the family of low-density parity-check codes, modeling the NG-IoT as dynamic constrained graphs. We highlight the distinct roles of node-, edge-, and graph-level tasks in tackling key challenges and demonstrate the GNNs' ability to overcome the limitations of traditional optimization. We examine the application of GNNs across core NG-enabling technologies and their integration with distributed frameworks to support privacy-preservation and distributed intelligence. We then delve into the challenges posed by adversarial attacks, offering insights into defense mechanisms. Lastly, we examine how GNNs can be integrated with emerging technologies. Our findings highlight the transformative potential of GNNs in improving efficiency, scalability, and security. Finally, we summarize the key lessons learned and outline promising future research directions, along with a set of design guidelines tailored for NG-IoT applications.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Commit-Reveal$^2$: Securing Randomness Beacons with Randomized Reveal Order in Smart Contracts</title>
<link>https://arxiv.org/abs/2504.03936</link>
<guid>https://arxiv.org/abs/2504.03936</guid>
<content:encoded><![CDATA[
arXiv:2504.03936v2 Announce Type: replace 
Abstract: Simple commit-reveal beacons are vulnerable to last-revealer strategies, and existing descriptions often leave accountability and recovery mechanisms unspecified for practical deployments. We present Commit-Reveal$^2$, a layered design for blockchain deployments that cryptographically randomizes the final reveal order, together with a concrete accountability and fallback mechanism that we implement as smart-contract logic. The protocol is architected as a hybrid system, where routine coordination runs off chain for efficiency and the blockchain acts as the trust anchor for commitments and the final arbiter for disputes. Our implementation covers leader coordination, on-chain verification, slashing for non-cooperation, and an explicit on-chain recovery path that maintains progress when off-chain coordination fails. We formally define two security goals for distributed randomness beacons, unpredictability and bit-wise bias resistance, and we show that Commit-Reveal$^2$ meets these notions under standard hash assumptions in the random-oracle model. In measurements with small to moderate operator sets, the hybrid design reduces on-chain gas by more than 80% compared to a fully on-chain baseline. We release a publicly verifiable prototype and evaluation artifacts to support replication and adoption in blockchain applications.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedDiverse: Tackling Data Heterogeneity in Federated Learning with Diversity-Driven Client Selection</title>
<link>https://arxiv.org/abs/2504.11216</link>
<guid>https://arxiv.org/abs/2504.11216</guid>
<content:encoded><![CDATA[
arXiv:2504.11216v2 Announce Type: replace 
Abstract: Federated Learning (FL) enables decentralized training of machine learning models on distributed data while preserving privacy. However, in real-world FL settings, client data is often non-identically distributed and imbalanced, resulting in statistical data heterogeneity which impacts the generalization capabilities of the server's model across clients, slows convergence and reduces performance. In this paper, we address this challenge by proposing first a characterization of statistical data heterogeneity by means of 6 metrics of global and client attribute imbalance, class imbalance, and spurious correlations. Next, we create and share 7 computer vision datasets for binary and multiclass image classification tasks in Federated Learning that cover a broad range of statistical data heterogeneity and hence simulate real-world situations. Finally, we propose FEDDIVERSE, a novel client selection algorithm in FL which is designed to manage and leverage data heterogeneity across clients by promoting collaboration between clients with complementary data distributions. Experiments on the seven proposed FL datasets demonstrate FEDDIVERSE's effectiveness in enhancing the performance and robustness of a variety of FL methods while having low communication and computational overhead.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Benchmark of Federated Learning with Kolmogorov-Arnold Networks for Medical Imaging</title>
<link>https://arxiv.org/abs/2504.19639</link>
<guid>https://arxiv.org/abs/2504.19639</guid>
<content:encoded><![CDATA[
arXiv:2504.19639v2 Announce Type: replace 
Abstract: Federated Learning (FL) enables model training across decentralized devices without sharing raw data, thereby preserving privacy in sensitive domains like healthcare. In this paper, we evaluate Kolmogorov-Arnold Networks (KAN) architectures against traditional MLP across six state-of-the-art FL algorithms on a blood cell classification dataset. Notably, our experiments demonstrate that KAN can effectively replace MLP in federated environments, achieving superior performance with simpler architectures. Furthermore, we analyze the impact of key hyperparameters-grid size and network architecture-on KAN performance under varying degrees of Non-IID data distribution. In addition, our ablation studies reveal that optimizing KAN width while maintaining minimal depth yields the best performance in federated settings. As a result, these findings establish KAN as a promising alternative for privacy-preserving medical imaging applications in distributed healthcare. To the best of our knowledge, this is the first comprehensive benchmark of KAN in FL settings for medical imaging task.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Up Data Parallelism in Decentralized Deep Learning</title>
<link>https://arxiv.org/abs/2509.12213</link>
<guid>https://arxiv.org/abs/2509.12213</guid>
<content:encoded><![CDATA[
arXiv:2509.12213v1 Announce Type: new 
Abstract: Although it has been extensively explored in theory, decentralized learning is not yet green-lighted for production use, largely due to a lack of stability, scalability, and generality in large scale DNN training. To shed light on the production use of decentralized learning, this work studies decentralized data parallel training at scale. To this end, we introduce a benchmarking framework, namely DBench, to host both centralized and decentralized DNN training. Building upon DBench, we introduce a benchmarking methodology to uncover the correlations between model accuracy and the variances of parameter tensors by varying communication graphs and training scales. Based on the benchmarking results, we observe that, (1) Similar to centralized learning, decentralized data parallel training also presents the issues of scalability and generality when the training scales up; (2) The model accuracy of decentralized learning is correlated to the number of connections in a communication graph; (3) The model accuracy of decentralized learning is surprisingly sensitive to the variance of parameter tensors across model replicas. Built upon the observations, we propose Ada, a decentralized adaptive approach that performs large scale DNN training following a decentralized SGD method and adapting the communication graph in use dynamically throughout training iterations. We apply Ada on large scale training and observe that Ada can obtain the best convergence rates consistently in decentralized DNN training, and delivers equally or comparably good model accuracy for all sample applications as centralized learning does, even when training ResNet50 for ImageNet-1K on the scale of 1008 GPUs.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ratio1 -- AI meta-OS</title>
<link>https://arxiv.org/abs/2509.12223</link>
<guid>https://arxiv.org/abs/2509.12223</guid>
<content:encoded><![CDATA[
arXiv:2509.12223v1 Announce Type: new 
Abstract: We propose the Ratio1 AI meta-operating system (meta-OS), a decentralized MLOps protocol that unifies AI model development, deployment, and inference across heterogeneous edge devices. Its key innovation is an integrated blockchain-based framework that transforms idle computing resources (laptops, smartphones, cloud VMs) into a trustless global supercomputer. The architecture includes novel components: a decentralized authentication layer (dAuth), an in-memory state database (CSTORE), a distributed storage system (R1FS), homomorphic encrypted federated learning (EDIL), decentralized container orchestration (Deeploy) and an oracle network (OracleSync), which collectively ensure secure, resilient execution of AI pipelines and other container based apps at scale. The protocol enforces a formal circular token-economic model combining Proof-of-Availability (PoA) and Proof-of-AI (PoAI) consensus. Compared to centralized heterogeneous cloud MLOps and existing decentralized compute platforms, which often lack integrated AI toolchains or trusted Ratio1 node operators (R1OP) mechanics, Ratio1's holistic design lowers barriers for AI deployment and improves cost-efficiency. We provide mathematical formulations of its secure licensing and reward protocols, and include descriptive information for the system architecture and protocol flow. We argue that our proposed fully functional ecosystem proposes and demonstrates significant improvements in accessibility, scalability, and security over existing alternatives.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Private Markovian Equilibrium in Stackelberg Markov Games for Smart Grid Demand Response</title>
<link>https://arxiv.org/abs/2509.12225</link>
<guid>https://arxiv.org/abs/2509.12225</guid>
<content:encoded><![CDATA[
arXiv:2509.12225v1 Announce Type: new 
Abstract: The increasing integration of renewable energy introduces a great challenge to the supply and demand balance of the power grid. To address this challenge, this paper formulates a Stackelberg Markov game (SMG) between an aggregator and multiple users, where the aggregator sets electricity prices and users make demand and storage decisions. Considering that users' storage levels are private information, we introduce private states and propose the new concepts of private Markovian strategies (PMS) and private Markovian equilibrium (PME). We establish the existence of a pure PME in the lower-level Markov game and prove that it can be computed in polynomial time. Notably, computing equilibrium in general Markov games is hard, and polynomial-time algorithms are rarely available. Based on these theoretical results, we develop a scalable solution framework combining centralized and decentralized algorithms for the lower-level PME computation with upper-level pricing optimization. Numerical simulations with up to 50 users based on real data validate the effectiveness and scalability of the proposed methods, whereas prior studies typically consider no more than 5 users.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Storage places in diplomatic texts (7th-13th centuries). Lexical, semantic, and digital investigation</title>
<link>https://arxiv.org/abs/2509.12230</link>
<guid>https://arxiv.org/abs/2509.12230</guid>
<content:encoded><![CDATA[
arXiv:2509.12230v1 Announce Type: new 
Abstract: This study examines the evolution of references to grain storage structures in medieval European charters, based on a quantitative and semantic analysis of the digitized CEMA (Cartae Europae Medii Aevi) corpus comprising more than 225,000 documents. The author applies text mining and distributional analysis methods to a lexicon of some forty terms designating storage locations (grangia, horreum, granarium, granica, etc.), cross-referencing these data with references to grain and analyzing their semantic contexts over the long term. The analysis reveals a paradigm shift between the early Middle Ages (decentralized, loosely regulated storage) and the 12th-13th centuries (centralization of storage by the ruling classes). Granaries became instruments of spatial polarization and social control, contributing to the accentuation of social domination in medieval Europe. This evolution was accompanied by a new conceptualization of storage, both material and spiritual.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Collapse-Inspired Multi-Label Federated Learning under Label-Distribution Skew</title>
<link>https://arxiv.org/abs/2509.12544</link>
<guid>https://arxiv.org/abs/2509.12544</guid>
<content:encoded><![CDATA[
arXiv:2509.12544v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative model training across distributed clients while preserving data privacy. However, the performance of deep learning often deteriorates in FL due to decentralized and heterogeneous data. This challenge is further amplified in multi-label scenarios, where data exhibit complex characteristics such as label co-occurrence, inter-label dependency, and discrepancies between local and global label relationships. While most existing FL research primarily focuses on single-label classification, many real-world applications, particularly in domains such as medical imaging, often involve multi-label settings. In this paper, we address this important yet underexplored scenario in FL, where clients hold multi-label data with skewed label distributions. Neural Collapse (NC) describes a geometric structure in the latent feature space where features of each class collapse to their class mean with vanishing intra-class variance, and the class means form a maximally separated configuration. Motivated by this theory, we propose a method to align feature distributions across clients and to learn high-quality, well-clustered representations. To make the NC-structure applicable to multi-label settings, where image-level features may contain multiple semantic concepts, we introduce a feature disentanglement module that extracts semantically specific features. The clustering of these disentangled class-wise features is guided by a predefined shared NC structure, which mitigates potential conflicts between client models due to diverse local data distributions. In addition, we design regularisation losses to encourage compact clustering in the latent feature space. Experiments conducted on four benchmark datasets across eight diverse settings demonstrate that our approach outperforms existing methods, validating its effectiveness in this challenging FL scenario.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asymmetric Grid Quorum Systems for Heterogeneous Processes</title>
<link>https://arxiv.org/abs/2509.12942</link>
<guid>https://arxiv.org/abs/2509.12942</guid>
<content:encoded><![CDATA[
arXiv:2509.12942v1 Announce Type: new 
Abstract: Quorum systems are a common way to formalize failure assumptions in distributed systems. Traditionally, these assumptions are shared by all involved processes. More recently, systems have emerged which allow processes some freedom in choosing their own, subjective or asymmetric, failure assumptions. For such a system to work, individual processes' assumptions must be compatible. However, this leads to a Catch-22-style scenario: How can processes collaborate to agree on compatible failure assumptions when they have no compatible failure assumptions to start with?
  We introduce asymmetric grid quorum systems that allow a group of processes to specify heterogeneous trust assumptions independently of each other and without coordination. They are based on qualitative attributes describing how the processes differ. Each process may select a quorum system from this class that aligns best with its subjective view. The available choices are designed to be compatible by definition, thereby breaking the cycling dependency. Asymmetric grid quorum systems have many applications that range from cloud platforms to blockchain networks.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>xRWA: A Cross-Chain Framework for Interoperability of Real-World Assets</title>
<link>https://arxiv.org/abs/2509.12957</link>
<guid>https://arxiv.org/abs/2509.12957</guid>
<content:encoded><![CDATA[
arXiv:2509.12957v1 Announce Type: new 
Abstract: Real-World Assets (RWAs) have recently attracted increasing attention as a means of bridging traditional financial instruments with decentralized infrastructures. By representing assets such as bonds, commodities, and real estate on blockchains, RWAs can enhance liquidity, broaden accessibility, and extend the scope of decentralized finance. Industry forecasts further suggest rapid growth of tokenized RWAs in the coming years, underscoring their potential role in the evolution of digital financial markets. However, when deployed across multiple blockchains, RWAs face challenges such as repeated authentication on different chains and inefficiency caused by multi-step settlement protocols. To address these issues, we present a cross-chain framework for RWAs that emphasizes identity management, authentication, and interaction. The framework integrates Decentralized Identifiers and Verifiable Credentials with customized attributes to support decentralized identification, and incorporates an authentication protocol based on Simplified Payment Verification to avoid redundant verification across chains. Furthermore, we design a cross-chain channel that enables the settlement of RWAs without requiring channel closure, thereby improving operational efficiency. We implement the framework and evaluate it through simulations, which confirm its feasibility and demonstrate improvements in efficiency for RWAs in cross-chain settings.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Validating Solidity Code Defects using Symbolic and Concrete Execution powered by Large Language Models</title>
<link>https://arxiv.org/abs/2509.13023</link>
<guid>https://arxiv.org/abs/2509.13023</guid>
<content:encoded><![CDATA[
arXiv:2509.13023v1 Announce Type: new 
Abstract: The high rate of false alarms from static analysis tools and Large Language Models (LLMs) complicates vulnerability detection in Solidity Smart Contracts, demanding methods that can formally or empirically prove the presence of defects. This paper introduces a novel detection pipeline that integrates custom Slither-based detectors, LLMs, Kontrol, and Forge. Our approach is designed to reliably detect defects and generate proofs.  We currently perform experiments with promising results for seven types of critical defects. We demonstrate the pipeline's efficacy by presenting our findings for three vulnerabilities -- Reentrancy, Complex Fallback, and Faulty Access Control Policies -- that are challenging for current verification solutions, which often generate false alarms or fail to detect them entirely. We highlight the potential of either symbolic or concrete execution in correctly classifying such code faults. By chaining these instruments, our method effectively validates true positives, significantly reducing the manual verification burden. Although we identify potential limitations, such as the inconsistency and the cost of LLMs, our findings establish a robust framework for combining heuristic analysis with formal verification to achieve more reliable and automated smart contract auditing.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Internet of Intelligent Things Framework for Decentralized Heterogeneous Platforms</title>
<link>https://arxiv.org/abs/2509.10507</link>
<guid>https://arxiv.org/abs/2509.10507</guid>
<content:encoded><![CDATA[
arXiv:2509.10507v1 Announce Type: new 
Abstract: Internet of Intelligent Things (IoIT), an emerging field, combines the utility of Internet of Things (IoT) devices with the innovation of embedded AI algorithms. However, it does not come without challenges, and struggles regarding available computing resources, energy supply, and storage limitations. In particular, many impediments to IoIT are linked to the energy-efficient deployment of machine learning (ML)/deep learning (DL) models in embedded devices. Research has been conducted to design energy-efficient IoIT platforms, but these papers often focus on centralized systems, in which some central entity processes all the data and coordinates actions. This can be problematic, e.g., serve as bottleneck or lead to security concerns. In a decentralized system, nodes/devices would self-organize and make their own decisions. Therefore, to address such issues, we propose a heterogeneous, decentralized sensing and monitoring IoIT peer-to-peer mesh network system model. Nodes in the network will coordinate towards several optimization goals: reliability, energy efficiency, and latency. The system employs federated learning to train nodes in a distributed manner, metaheuristics to optimize task allocation and routing paths, and multi-objective optimization to balance conflicting performance goals.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Identity Management on Ripple: A Conceptual Framework for High-Speed, Low-Cost Identity Transactions in Attestation-Based Attribute-Based Identity</title>
<link>https://arxiv.org/abs/2509.10545</link>
<guid>https://arxiv.org/abs/2509.10545</guid>
<content:encoded><![CDATA[
arXiv:2509.10545v1 Announce Type: new 
Abstract: Recent years have seen many industrial implementations and much scholastic research, i.e., prototypes and theoretical frameworks, in Decentralized Identity Management Systems (DIDMS). It is safe to say that Attestation-Based Attribute-Based Decentralized IDM (ABABDIDM) has not received anywhere near the same level of attention in the literature as general Attribute-Based DIDMs (ABDIDM), i.e, decentralized Attribute-Based Access Control (ABAC). The use of decentralization, i.e., DIDM, is to improve upon the security and privacy-related issues of centralized Identity Management Systems (IDM) and Attribute-Based IDMs (ABIDM). And blockchain is the framework used for decentralization in all these schemes. Many DIDMs - even ABDIDMs - have been defined on popular blockchains such as Hyperledger, Ethereum, and Bitcoin. However, despite the characteristics of Ripple that makes it appealing for an ABIDM, there is a lack of research to develop an Identity Management System (IDMS) on Ripple in literature. We have attempted to conceptualize an ABABDIDM on Ripple.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZapGPT: Free-form Language Prompting for Simulated Cellular Control</title>
<link>https://arxiv.org/abs/2509.10660</link>
<guid>https://arxiv.org/abs/2509.10660</guid>
<content:encoded><![CDATA[
arXiv:2509.10660v1 Announce Type: new 
Abstract: Human language is one of the most expressive tools for conveying intent, yet most artificial or biological systems lack mechanisms to interpret or respond meaningfully to it. Bridging this gap could enable more natural forms of control over complex, decentralized systems. In AI and artificial life, recent work explores how language can specify high-level goals, but most systems still depend on engineered rewards, task-specific supervision, or rigid command sets, limiting generalization to novel instructions. Similar constraints apply in synthetic biology and bioengineering, where the locus of control is often genomic rather than environmental perturbation.
  A key open question is whether artificial or biological collectives can be guided by free-form natural language alone, without task-specific tuning or carefully designed evaluation metrics. We provide one possible answer here by showing, for the first time, that simple agents' collective behavior can be guided by free-form language prompts: one AI model transforms an imperative prompt into an intervention that is applied to simulated cells; a second AI model scores how well the prompt describes the resulting cellular dynamics; and the former AI model is evolved to improve the scores generated by the latter.
  Unlike previous work, our method does not require engineered fitness functions or domain-specific prompt design. We show that the evolved system generalizes to unseen prompts without retraining. By treating natural language as a control layer, the system suggests a future in which spoken or written prompts could direct computational, robotic, or biological systems to desired behaviors. This work provides a concrete step toward this vision of AI-biology partnerships, in which language replaces mathematical objective functions, fixed rules, and domain-specific programming.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Experimental Validation of Decentralized Affine Transformation</title>
<link>https://arxiv.org/abs/2509.10791</link>
<guid>https://arxiv.org/abs/2509.10791</guid>
<content:encoded><![CDATA[
arXiv:2509.10791v1 Announce Type: new 
Abstract: This paper presents an experimental validation of decentralized affine transformation (AT) in multi-agent systems using teams of mini-quadcopters. The AT framework enables an agent team to safely navigate constrained, obstacle-rich environments while allowing aggressive changes in inter-agent distances, which are formally characterized through the decomposition of the AT transformation matrix. Without loss of generality, we focus on two-dimensional AT, formulated as a decentralized leader-follower problem. In this formulation, three leader quadcopters are positioned at the vertices of a triangle, while all follower quadcopters remain within the triangle. The leaders know the desired trajectories prescribed by the AT, whereas the followers do not. Instead, the followers infer their trajectories through local communication governed by fixed communication weights determined by the initial spatial configuration of the team. Experimental results validate the asymptotic convergence of decentralized AT and demonstrate its capability to safely guide multi-agent teams through obstacle-laden environments.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arguzz: Testing zkVMs for Soundness and Completeness Bugs</title>
<link>https://arxiv.org/abs/2509.10819</link>
<guid>https://arxiv.org/abs/2509.10819</guid>
<content:encoded><![CDATA[
arXiv:2509.10819v1 Announce Type: new 
Abstract: Zero-knowledge virtual machines (zkVMs) are increasingly deployed in decentralized applications and blockchain rollups since they enable verifiable off-chain computation. These VMs execute general-purpose programs, frequently written in Rust, and produce succinct cryptographic proofs. However, zkVMs are complex, and bugs in their constraint systems or execution logic can cause critical soundness (accepting invalid executions) or completeness (rejecting valid ones) issues.
  We present Arguzz, the first automated tool for testing zkVMs for soundness and completeness bugs. To detect such bugs, Arguzz combines a novel variant of metamorphic testing with fault injection. In particular, it generates semantically equivalent program pairs, merges them into a single Rust program with a known output, and runs it inside a zkVM. By injecting faults into the VM, Arguzz mimics malicious or buggy provers to uncover overly weak constraints.
  We used Arguzz to test six real-world zkVMs (RISC Zero, Nexus, Jolt, SP1, OpenVM, and Pico) and found eleven bugs in three of them. One RISC Zero bug resulted in a $50,000 bounty, despite prior audits, demonstrating the critical need for systematic testing of zkVMs.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Paradigm Shift to Audit Rift: Exploring Vulnerabilities and Audit Tips for TON Smart Contracts</title>
<link>https://arxiv.org/abs/2509.10823</link>
<guid>https://arxiv.org/abs/2509.10823</guid>
<content:encoded><![CDATA[
arXiv:2509.10823v1 Announce Type: new 
Abstract: The Open Network (TON) is a high-performance blockchain platform designed for scalability and efficiency, leveraging an asynchronous execution model and a multi-layered architecture. While TON's design offers significant advantages, it also introduces unique challenges for smart contract development and security. This paper introduces a comprehensive audit checklist for TON smart contracts, based on an analysis of 34 professional audit reports containing 233 real-world vulnerabilities. The checklist addresses TON-specific challenges, such as asynchronous message handling, and provides actionable insights for developers and auditors. We also present detailed case studies of vulnerabilities in TON smart contracts, highlighting their implications and offering lessons learned. By adopting this checklist, developers and auditors can systematically identify and mitigate vulnerabilities, enhancing the security and reliability of TON-based projects. Our work bridges the gap between Ethereum's mature audit methodologies and the emerging needs of the TON ecosystem, fostering a more secure and robust blockchain environment.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design of scalable orthogonal digital encoding architecture for large-area flexible tactile sensing in robotics</title>
<link>https://arxiv.org/abs/2509.10888</link>
<guid>https://arxiv.org/abs/2509.10888</guid>
<content:encoded><![CDATA[
arXiv:2509.10888v1 Announce Type: new 
Abstract: Human-like embodied tactile perception is crucial for the next-generation intelligent robotics. Achieving large-area, full-body soft coverage with high sensitivity and rapid response, akin to human skin, remains a formidable challenge due to critical bottlenecks in encoding efficiency and wiring complexity in existing flexible tactile sensors, thus significantly hinder the scalability and real-time performance required for human skin-level tactile perception. Herein, we present a new architecture employing code division multiple access-inspired orthogonal digital encoding to overcome these challenges. Our decentralized encoding strategy transforms conventional serial signal transmission by enabling parallel superposition of energy-orthogonal base codes from distributed sensing nodes, drastically reducing wiring requirements and increasing data throughput. We implemented and validated this strategy with off-the-shelf 16-node sensing array to reconstruct the pressure distribution, achieving a temporal resolution of 12.8 ms using only a single transmission wire. Crucially, the architecture can maintain sub-20ms latency across orders-of-magnitude variations in node number (to thousands of nodes). By fundamentally redefining signal encoding paradigms in soft electronics, this work opens new frontiers in developing scalable embodied intelligent systems with human-like sensory capabilities.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Framework for Formal Modelling of Metaverse Applications Using Hierarchical Colored Petri Nets</title>
<link>https://arxiv.org/abs/2509.10936</link>
<guid>https://arxiv.org/abs/2509.10936</guid>
<content:encoded><![CDATA[
arXiv:2509.10936v1 Announce Type: new 
Abstract: The Metaverse emerges by integrating highly-distributed, complex, and interconnecting technologies. These technologies need to be formally verified and evaluated through formal modelling before executing them in real-world applications, in order to avoid negative impacts on the real world due to failure of the Metaverse technologies. However, the formal modelling of Metaverse technologies is challenging due to its highly complex nature. Therefore, a comprehensive formal verification of the Metaverse technologies is needed for its realization in multiple potential areas. In this study, a framework is proposed for the formal modelling of Metaverse technologies, which allows holistic insights for all applications of Metaverse technologies. By utilizing the proposed framework, Metaverse applications of any complexity can be modeled. The working of the proposed framework is illustrated by modelling a case study of an Air Traffic Control system. In the proposed framework, we utilize hierarchical colored Petri nets for formal modelling of behavior of the air traffic control system. The correctness of air traffic control system properties, such as liveness, reachability, and boundedness, is verified in the proposed framework. The results of the case study reveal that the proposed framework can be used as a template for mathematical verification of challenging and complex Metaverse applications. The results also show that formal modelling provides an effective tool for identifying flaws in the early phases of the design of Metaverse applications. The implication of using formal verification is that it can increase confidence about the correctness of the Metaverse applications.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Range-Based Sharding (RBS) Protocol for Scalable Enterprise Blockchain</title>
<link>https://arxiv.org/abs/2509.11006</link>
<guid>https://arxiv.org/abs/2509.11006</guid>
<content:encoded><![CDATA[
arXiv:2509.11006v1 Announce Type: new 
Abstract: Blockchain technology offers decentralization and security but struggles with scalability, particularly in enterprise settings where efficiency and controlled access are paramount. Sharding is a promising solution for private blockchains, yet existing approaches face challenges in coordinating shards, ensuring fault tolerance with limited nodes, and minimizing the high overhead of consensus mechanisms like PBFT. This paper proposes the Range-Based Sharding (RBS) Protocol, a novel sharding mechanism tailored for enterprise blockchains, implemented on Quorum. Unlike traditional sharding models such as OmniLedger and non-sharding Corda framework, RBS employs a commit-reveal scheme for secure and unbiased shard allocation, ensuring fair validator distribution while reducing cross-shard transaction delays. Our approach enhances scalability by balancing computational loads across shards, reducing consensus overhead, and improving parallel transaction execution. Experimental evaluations demonstrate that RBS achieves significantly higher throughput and lower latency compared to existing enterprise sharding frameworks, making it a viable and efficient solution for largescale blockchain deployments.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General Decentralized Stochastic Optimal Control via Change of Measure: Applications to the Witsenhausen Counterexample</title>
<link>https://arxiv.org/abs/2509.11013</link>
<guid>https://arxiv.org/abs/2509.11013</guid>
<content:encoded><![CDATA[
arXiv:2509.11013v1 Announce Type: new 
Abstract: In this paper we present global and person-by-person (PbP) optimality conditions for general decentralized stochastic dynamic optimal control problems, using a discrete-time version of Girsanov's change of measure. The PbP optimality conditions are applied to the Witsenhausen counterexample to show that the two strategies satisfy two coupled nonlinear integral equations. Further, we prove a fixed point theorem in a function space, establishing existence and uniqueness of solutions to the integral equations. We also provide numerical solutions of the two integral equations using the Gauss Hermite Quadrature scheme, and include a detail comparison to other numerical methods of the literature. The numerical solutions confirm Witsehausen's observation that, for certain choices of parameters, linear or affine strategies are optimal, while for other choices of parameters nonlinear strategies outperformed affine strategies.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Username Suggestion and Multimodal Gender Detection in Online Platforms: Introducing the PNGT-26K Dataset</title>
<link>https://arxiv.org/abs/2509.11136</link>
<guid>https://arxiv.org/abs/2509.11136</guid>
<content:encoded><![CDATA[
arXiv:2509.11136v1 Announce Type: new 
Abstract: Persian names present unique challenges for natural language processing applications, particularly in gender detection and digital identity creation, due to transliteration inconsistencies and cultural-specific naming patterns. Existing tools exhibit significant performance degradation on Persian names, while the scarcity of comprehensive datasets further compounds these limitations. To address these challenges, the present research introduces PNGT-26K, a comprehensive dataset of Persian names, their commonly associated gender, and their English transliteration, consisting of approximately 26,000 tuples. As a demonstration of how this resource can be utilized, we also introduce two frameworks, namely Open Gender Detection and Nominalist. Open Gender Detection is a production-grade, ready-to-use framework for using existing data from a user, such as profile photo and name, to give a probabilistic guess about the person's gender. Nominalist, the second framework introduced by this paper, utilizes agentic AI to help users choose a username for their social media accounts on any platform. It can be easily integrated into any website to provide a better user experience. The PNGT-26K dataset, Nominalist and Open Gender Detection frameworks are publicly available on Github.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Recommender System with Data Valuation for E-commerce Platform</title>
<link>https://arxiv.org/abs/2509.11196</link>
<guid>https://arxiv.org/abs/2509.11196</guid>
<content:encoded><![CDATA[
arXiv:2509.11196v1 Announce Type: new 
Abstract: Federated Learning (FL) is gaining prominence in machine learning as privacy concerns grow. This paradigm allows each client (e.g., an individual online store) to train a recommendation model locally while sharing only model updates, without exposing the raw interaction logs to a central server, thereby preserving privacy in a decentralized environment. Nonetheless, most existing FL-based recommender systems still rely solely on each client's private data, despite the abundance of publicly available datasets that could be leveraged to enrich local training; this potential remains largely underexplored. To this end, we consider a realistic scenario wherein a large shopping platform collaborates with multiple small online stores to build a global recommender system. The platform possesses global data, such as shareable user and item lists, while each store holds a portion of interaction data privately (or locally). Although integrating global data can help mitigate the limitations of sparse and biased clients' local data, it also introduces additional challenges: simply combining all global interactions can amplify noise and irrelevant patterns, worsening personalization and increasing computational costs. To address these challenges, we propose FedGDVE, which selectively augments each client's local graph with semantically aligned samples from the global dataset. FedGDVE employs: (i) a pre-trained graph encoder to extract global structural features, (ii) a local valid predictor to assess client-specific relevance, (iii) a reinforcement-learning-based probability estimator to filter and sample only the most pertinent global interactions. FedGDVE improves performance by up to 34.86% on recognized benchmarks in FL environments.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Web-Based Environment for the Specification and Generation of Smart Legal Contracts</title>
<link>https://arxiv.org/abs/2509.11258</link>
<guid>https://arxiv.org/abs/2509.11258</guid>
<content:encoded><![CDATA[
arXiv:2509.11258v1 Announce Type: new 
Abstract: Monitoring the compliance of contract performance against legal obligations is important in order to detect violations, ideally, as soon as they occur. Such monitoring can nowadays be achieved through the use of smart contracts, which provide protection against tampering as well as some level of automation in handling violations. However, there exists a large gap between natural language contracts and smart contract implementations. This paper introduces a Web-based environment that partly fills that gap by supporting the user-assisted refinement of Symboleo specifications corresponding to legal contract templates, followed by the automated generation of monitoring smart contracts deployable on the Hyperledger Fabric platform. This environment, illustrated using a sample contract from the transactive energy domain, shows much potential in accelerating the development of smart contracts in a legal compliance context.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Incentive-Compatible Reward Sharing Mechanism for Mitigating Mirroring Attacks in Decentralized Data-Feed Systems</title>
<link>https://arxiv.org/abs/2509.11294</link>
<guid>https://arxiv.org/abs/2509.11294</guid>
<content:encoded><![CDATA[
arXiv:2509.11294v1 Announce Type: new 
Abstract: Decentralized data-feed systems enable blockchain-based smart contracts to access off-chain information by aggregating values from multiple oracles. To improve accuracy, these systems typically use an aggregation function, such as majority voting, to consolidate the inputs they receive from oracles and make a decision. Depending on the final decision and the values reported by the oracles, the participating oracles are compensated through shared rewards. However, such incentive mechanisms are vulnerable to mirroring attacks, where a single user controls multiple oracles to bias the decision of the aggregation function and maximize rewards. This paper analyzes the impact of mirroring attacks on the reliability and dependability of majority voting-based data-feed systems. We demonstrate how existing incentive mechanisms can unintentionally encourage rational users to implement such attacks. To address this, we propose a new incentive mechanism that discourages Sybil behavior. We prove that the proposed mechanism leads to a Nash Equilibrium in which each user operates only one oracle. Finally, we discuss the practical implementation of the proposed incentive mechanism and provide numerical examples to demonstrate its effectiveness.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Escaping Efficiency of Distributed Adversarial Training Algorithms</title>
<link>https://arxiv.org/abs/2509.11337</link>
<guid>https://arxiv.org/abs/2509.11337</guid>
<content:encoded><![CDATA[
arXiv:2509.11337v1 Announce Type: new 
Abstract: Adversarial training has been widely studied in recent years due to its role in improving model robustness against adversarial attacks. This paper focuses on comparing different distributed adversarial training algorithms--including centralized and decentralized strategies--within multi-agent learning environments. Previous studies have highlighted the importance of model flatness in determining robustness. To this end, we develop a general theoretical framework to study the escaping efficiency of these algorithms from local minima, which is closely related to the flatness of the resulting models. We show that when the perturbation bound is sufficiently small (i.e., when the attack strength is relatively mild) and a large batch size is used, decentralized adversarial training algorithms--including consensus and diffusion--are guaranteed to escape faster from local minima than the centralized strategy, thereby favoring flatter minima. However, as the perturbation bound increases, this trend may no longer hold. In the simulation results, we illustrate our theoretical findings and systematically compare the performance of models obtained through decentralized and centralized adversarial training algorithms. The results highlight the potential of decentralized strategies to enhance the robustness of models in distributed settings.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Edge Learning for Predictive Maintenance in 6G Small Cell Networks</title>
<link>https://arxiv.org/abs/2509.11421</link>
<guid>https://arxiv.org/abs/2509.11421</guid>
<content:encoded><![CDATA[
arXiv:2509.11421v1 Announce Type: new 
Abstract: The rollout of 6G networks introduces unprecedented demands for autonomy, reliability, and scalability. However, the transmission of sensitive telemetry data to central servers raises concerns about privacy and bandwidth. To address this, we propose a federated edge learning framework for predictive maintenance in 6G small cell networks. The system adopts a Knowledge Defined Networking (KDN) architecture in Data, Knowledge, and Control Planes to support decentralized intelligence, telemetry-driven training, and coordinated policy enforcement. In the proposed model, each base station independently trains a failure prediction model using local telemetry metrics, including SINR, jitter, delay, and transport block size, without sharing raw data. A threshold-based multi-label encoding scheme enables the detection of concurrent fault conditions. We then conduct a comparative analysis of centralized and federated training strategies to evaluate their performance in this context. A realistic simulation environment is implemented using the ns-3 mmWave module, incorporating hybrid user placement and base station fault injection across various deployment scenarios. The learning pipeline is orchestrated via the Flower framework, and model aggregation is performed using the Federated Averaging (FedAvg) algorithm. Experimental results demonstrate that the federated model achieves performance comparable to centralized training in terms of accuracy and per-label precision, while preserving privacy and reducing communication overhead.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media</title>
<link>https://arxiv.org/abs/2509.11444</link>
<guid>https://arxiv.org/abs/2509.11444</guid>
<content:encoded><![CDATA[
arXiv:2509.11444v1 Announce Type: new 
Abstract: The emergence of decentralized social media platforms presents new opportunities and challenges for real-time analysis of public discourse. This study introduces CognitiveSky, an open-source and scalable framework designed for sentiment, emotion, and narrative analysis on Bluesky, a federated Twitter or X.com alternative. By ingesting data through Bluesky's Application Programming Interface (API), CognitiveSky applies transformer-based models to annotate large-scale user-generated content and produces structured and analyzable outputs. These summaries drive a dynamic dashboard that visualizes evolving patterns in emotion, activity, and conversation topics. Built entirely on free-tier infrastructure, CognitiveSky achieves both low operational cost and high accessibility. While demonstrated here for monitoring mental health discourse, its modular design enables applications across domains such as disinformation detection, crisis response, and civic sentiment analysis. By bridging large language models with decentralized networks, CognitiveSky offers a transparent, extensible tool for computational social science in an era of shifting digital ecosystems.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Partitioning techniques for non-centralized predictive control: A systematic review and novel theoretical insights</title>
<link>https://arxiv.org/abs/2509.11470</link>
<guid>https://arxiv.org/abs/2509.11470</guid>
<content:encoded><![CDATA[
arXiv:2509.11470v1 Announce Type: new 
Abstract: The partitioning problem is of central relevance for designing and implementing non-centralized Model Predictive Control (MPC) strategies for large-scale systems. These control approaches include decentralized MPC, distributed MPC, hierarchical MPC, and coalitional MPC. Partitioning a system for the application of non-centralized MPC consists of finding the best definition of the subsystems, and their allocation into groups for the definition of local controllers, to maximize the relevant performance indicators. The present survey proposes a novel systematization of the partitioning approaches in the literature in five main classes: optimization-based, algorithmic, community-detection-based, game-theoretic-oriented, and heuristic approaches. A unified graph-theoretical formalism, a mathematical re-formulation of the problem in terms of mixed-integer programming, the novel concepts of predictive partitioning and multi-topological representations, and a methodological formulation of quality metrics are developed to support the classification and further developments of the field. We analyze the different classes of partitioning techniques, and we present an overview of their strengths and limitations, which include a technical discussion about the different approaches. Representative case studies are discussed to illustrate the application of partitioning techniques for non-centralized MPC in various sectors, including power systems, water networks, wind farms, chemical processes, transportation systems, communication networks, industrial automation, smart buildings, and cyber-physical systems. An outlook of future challenges completes the survey.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dstack: A Zero Trust Framework for Confidential Containers</title>
<link>https://arxiv.org/abs/2509.11555</link>
<guid>https://arxiv.org/abs/2509.11555</guid>
<content:encoded><![CDATA[
arXiv:2509.11555v1 Announce Type: new 
Abstract: Web3 applications require execution platforms that maintain confidentiality and integrity without relying on centralized trust authorities. While Trusted Execution Environments (TEEs) offer promising capabilities for confidential computing, current implementations face significant limitations when applied to Web3 contexts, particularly in security reliability, censorship resistance, and vendor independence.
  This paper presents dstack, a comprehensive framework that transforms raw TEE technology into a true Zero Trust platform. We introduce three key innovations: (1) Portable Confidential Containers that enable seamless workload migration across heterogeneous TEE environments while maintaining security guarantees, (2) Decentralized Code Management that leverages smart contracts for transparent governance of TEE applications, and (3) Verifiable Domain Management that ensures secure and verifiable application identity without centralized authorities.
  These innovations are implemented through three core components: dstack-OS, dstack-KMS, and dstack-Gateway. Together, they demonstrate how to achieve both the performance advantages of VM-level TEE solutions and the trustless guarantees required by Web3 applications. Our evaluation shows that dstack provides comprehensive security guarantees while maintaining practical usability for real-world applications.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Evaluation to Enhancement: Large Language Models for Zero-Knowledge Proof Code Generation</title>
<link>https://arxiv.org/abs/2509.11708</link>
<guid>https://arxiv.org/abs/2509.11708</guid>
<content:encoded><![CDATA[
arXiv:2509.11708v1 Announce Type: new 
Abstract: Zero-knowledge proofs (ZKPs) are increasingly deployed in domains such as privacy-preserving authentication, blockchain scalability, and secure finance. However, authoring ZK programs remains challenging: unlike mainstream programming, ZK development requires reasoning about finite field arithmetic, constraint systems, and gadgets, making it knowledge-intensive and error-prone. While large language models (LLMs) have demonstrated strong code generation capabilities in general-purpose languages, their effectiveness for ZK programming, where correctness hinges on both language mastery and gadget-level reasoning, remains unexplored. To address this gap, we propose \textsc{ZK-Eval}, a domain-specific evaluation pipeline that probes LLM capabilities at three levels: language knowledge, gadget competence, and end-to-end program generation. Our evaluation of four state-of-the-art LLMs reveals that models excel at surface-level syntax but struggle with gadget usage and semantic correctness, often yielding incorrect programs. Based on these insights, we introduce \textsc{ZK-Coder}, an agentic framework that augments LLMs with constraint sketching, guided retrieval, and interactive repair. Experiments on Circom and Noir show substantial gains, with success rates improving from 17.35\% to 83.38\% and from 32.21\% to 90.05\%, respectively. With \textsc{ZK-Eval} and \textsc{ZK-Coder}, we establish a foundation for systematically measuring and augmenting LLMs in ZK code generation to lower barriers for practitioners and advance trustworthy computation.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-Constrained Intelligent Adversaries for Automation Vulnerability Testing: A Multi-Robot Patrol Case Study</title>
<link>https://arxiv.org/abs/2509.11971</link>
<guid>https://arxiv.org/abs/2509.11971</guid>
<content:encoded><![CDATA[
arXiv:2509.11971v1 Announce Type: new 
Abstract: Simulating hostile attacks of physical autonomous systems can be a useful tool to examine their robustness to attack and inform vulnerability-aware design. In this work, we examine this through the lens of multi-robot patrol, by presenting a machine learning-based adversary model that observes robot patrol behavior in order to attempt to gain undetected access to a secure environment within a limited time duration. Such a model allows for evaluation of a patrol system against a realistic potential adversary, offering insight into future patrol strategy design. We show that our new model outperforms existing baselines, thus providing a more stringent test, and examine its performance against multiple leading decentralized multi-robot patrol strategies.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MusicSwarm: Biologically Inspired Intelligence for Music Composition</title>
<link>https://arxiv.org/abs/2509.11973</link>
<guid>https://arxiv.org/abs/2509.11973</guid>
<content:encoded><![CDATA[
arXiv:2509.11973v1 Announce Type: new 
Abstract: We show that coherent, long-form musical composition can emerge from a decentralized swarm of identical, frozen foundation models that coordinate via stigmergic, peer-to-peer signals, without any weight updates. We compare a centralized multi-agent system with a global critic to a fully decentralized swarm in which bar-wise agents sense and deposit harmonic, rhythmic, and structural cues, adapt short-term memory, and reach consensus. Across symbolic, audio, and graph-theoretic analyses, the swarm yields superior quality while delivering greater diversity and structural variety and leads across creativity metrics. The dynamics contract toward a stable configuration of complementary roles, and self-similarity networks reveal a small-world architecture with efficient long-range connectivity and specialized bridging motifs, clarifying how local novelties consolidate into global musical form. By shifting specialization from parameter updates to interaction rules, shared memory, and dynamic consensus, MusicSwarm provides a compute- and data-efficient route to long-horizon creative structure that is immediately transferable beyond music to collaborative writing, design, and scientific discovery.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When MoE Meets Blockchain: A Trustworthy Distributed Framework of Large Models</title>
<link>https://arxiv.org/abs/2509.12141</link>
<guid>https://arxiv.org/abs/2509.12141</guid>
<content:encoded><![CDATA[
arXiv:2509.12141v2 Announce Type: new 
Abstract: As an enabling architecture of Large Models (LMs), Mixture of Experts (MoE) has become prevalent thanks to its sparsely-gated mechanism, which lowers computational overhead while maintaining learning performance comparable to dense LMs. The essence of MoE lies in utilizing a group of neural networks (called experts) with each specializing in different types of tasks, along with a trainable gating network that selectively activates a subset of these experts to handle specific tasks. Traditional cloud-based MoE encounters challenges such as prolonged response latency, high bandwidth consumption, and data privacy leakage. To address these issues, researchers have proposed to deploy MoE over distributed edge networks. However, a key concern of distributed MoE frameworks is the lack of trust in data interactions among distributed experts without the surveillance of any trusted authority, and thereby prone to potential attacks such as data manipulation. In response to the security issues of traditional distributed MoE, we propose a blockchain-aided trustworthy MoE (B-MoE) framework that consists of three layers: the edge layer, the blockchain layer, and the storage layer. In this framework, the edge layer employs the activated experts downloaded from the storage layer to process the learning tasks, while the blockchain layer functions as a decentralized trustworthy network to trace, verify, and record the computational results of the experts from the edge layer. The experimental results demonstrate that B-MoE is more robust to data manipulation attacks than traditional distributed MoE during both the training and inference processes.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V-ZOR: Enabling Verifiable Cross-Blockchain Communication via Quantum-Driven ZKP Oracle Relays</title>
<link>https://arxiv.org/abs/2509.10996</link>
<guid>https://arxiv.org/abs/2509.10996</guid>
<content:encoded><![CDATA[
arXiv:2509.10996v1 Announce Type: cross 
Abstract: Cross-chain bridges and oracle DAOs represent some of the most vulnerable components of decentralized systems, with more than $2.8 billion lost due to trust failures, opaque validation behavior, and weak incentives. Current oracle designs are based on multisigs, optimistic assumptions, or centralized aggregation, exposing them to attacks and delays. Moreover, predictable committee selection enables manipulation, which threatens data integrity across chains. We propose V-ZOR, a verifiable oracle relay that integrates zero-knowledge proofs, quantum-grade randomness, and cross-chain restaking to mitigate these risks. Each oracle packet includes a Halo 2 proof verifying that the reported data was correctly aggregated using a deterministic median. To prevent committee manipulation, VZOR reseeds its VRF using auditable quantum entropy, ensuring unpredictable and secure selection of reporters. Reporters stake once on a shared restaking hub; any connected chain can submit a fraud proof to trigger slashing, removing the need for multisigs or optimistic assumptions. A prototype in Sepolia and Scroll achieves sub-300k gas verification, one-block latency, and a 10x increase in collusion cost. V-ZOR demonstrates that combining ZK attestation with quantum-randomized restaking enables a trust-minimized, high-performance oracle layer for cross-chain DeFi.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Model-based Control Strategies for a Quadruple Tank System: Decentralized PID, LMPC, and NMPC</title>
<link>https://arxiv.org/abs/2509.11235</link>
<guid>https://arxiv.org/abs/2509.11235</guid>
<content:encoded><![CDATA[
arXiv:2509.11235v1 Announce Type: cross 
Abstract: This paper compares the performance of a decentralized proportional-integral-derivative (PID) controller, a linear model predictive controller (LMPC), and a nonlinear model predictive controller (NMPC) applied to a quadruple tank system (QTS). We present experimental data from a physical setup of the QTS as well as simulation results. The QTS is modeled as a stochastic nonlinear continuous-discrete-time system, with parameters estimated using a maximum-likelihood prediction-error-method (ML-PEM). The NMPC applies the stochastic nonlinear continuous-discrete-time model, while the LMPC uses a linearized version of the same model. We tune the decentralized PID controller using the simple internal model control (SIMC) rules. The SIMC rules require transfer functions of the process, and we obtain these from the linearized model. We compare the controller performances based on systematic tests using both the physical setup and the simulated QTS. We measure the performance in terms of tracking errors and rate of movement in the manipulated variables. The LMPC and the NMPC perform better than the decentralized PID control system for tracking pre-announced time-varying setpoints. For disturbance rejection, the MPCs perform only slightly better than the decentralized PID controller. The primary advantage of the MPCs is their ability to use the information of future setpoints. We demonstrate this by providing simulation results of the MPCs with and without such information. Finally, the NMPC achieves slightly improved tracking errors compared to the LMPC but at the expense of having a higher input rate of movement.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collusion-Resilience in Transaction Fee Mechanism Design</title>
<link>https://arxiv.org/abs/2402.09321</link>
<guid>https://arxiv.org/abs/2402.09321</guid>
<content:encoded><![CDATA[
arXiv:2402.09321v3 Announce Type: replace 
Abstract: Users bid in a transaction fee mechanism (TFM) to get their transactions included and confirmed by a blockchain protocol. Roughgarden (EC'21) initiated the formal treatment of TFMs and proposed three requirements: user incentive compatibility (UIC), miner incentive compatibility (MIC), and a form of collusion-resilience called OCA-proofness. Ethereum's EIP-1559 mechanism satisfies all three properties simultaneously when there is no contention between transactions, but loses the UIC property when there are too many eligible transactions to fit in a single block. Chung and Shi (SODA'23) considered an alternative notion of collusion-resilience, called $c$-side-contract-proofness ($c$-SCP), and showed that, when there is contention between transactions, no TFM can satisfy UIC, MIC, and $c$-SCP for any $c\geq 1$. OCA-proofness asserts that the users and a miner should not be able to "steal from the protocol." On the other hand, the $c$-SCP condition requires that a coalition of a miner and a subset of users should not be able to profit through strategic deviations (whether at the expense of the protocol or of the users outside the coalition).
  Our main result is the first proof that, when there is contention between transactions, no (possibly randomized) TFM in which users are expected to bid truthfully satisfies UIC, MIC, and OCA-proofness.This result resolves the main open question in Roughgarden (EC'21). We also suggest several relaxations of the basic model that allow our impossibility result to be circumvented.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanism Design for Automated Market Makers</title>
<link>https://arxiv.org/abs/2402.09357</link>
<guid>https://arxiv.org/abs/2402.09357</guid>
<content:encoded><![CDATA[
arXiv:2402.09357v3 Announce Type: replace 
Abstract: Blockchains have popularized automated market makers (AMMs). An AMM exchange is an application running on a blockchain which maintains a pool of crypto-assets and automatically trades assets with users governed by some pricing function that prices the assets based on their relative demand/supply. AMMs have created an important challenge commonly known as the Miner Extractable Value (MEV). In particular, the miners who control the contents and ordering of transactions in a block can extract value by front-running and back-running users' transactions, leading to arbitrage opportunities that guarantee them risk-free returns.
  In this paper, we consider how to design AMM mechanisms that eliminate MEV opportunities. Specifically, we propose a new AMM mechanism that processes all transactions contained within a block in a batch. We show that our new mechanism satisfies two tiers of guarantees. First, for legacy blockchains where each block is proposed by a single (possibly rotating) miner, we prove that our mechanism satisfies arbitrage resilience, i.e., a miner cannot gain risk-free profit. Moreover, we also guarantee fair treatment among all transactions within the same block, such that the miner is unable to sell off favorable positions in the block to users or arbitragers. Second, for blockchains where the block proposal process is decentralized and offers sequencing-fairness, we prove a stronger notion called incentive compatibility -- roughly speaking, we guarantee that any individual user's best response is to follow the honest strategy.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Formal Foundation for Blockchain Rollups</title>
<link>https://arxiv.org/abs/2406.16219</link>
<guid>https://arxiv.org/abs/2406.16219</guid>
<content:encoded><![CDATA[
arXiv:2406.16219v3 Announce Type: replace 
Abstract: Blockchains like Bitcoin and Ethereum have revolutionized digital transactions, yet scalability issues persist. Layer 2 solutions, such as validity proof Rollups (ZK-Rollups), aim to address these challenges by processing transactions off-chain and validating them on the main chain. However, concerns remain about security and censorship resistance, particularly regarding centralized control in Layer 2 and inadequate mechanisms for enforcing these properties through Layer 1 smart contracts. In their current form, L2s are susceptible to multisig attacks that can lead to total user funds loss. This work presents a formal analysis using the Alloy specification language to examine and design key Layer 2 functionalities, including forced transaction queues, safe blacklisting, and upgradeability. Through this analysis, we identify pitfalls in existing designs and introduce an enhanced model that has been model-checked to be correct. Finally, we propose a complete end-to-end methodology to analyze rollups' security and censorship resistance based on manually translating Alloy properties to property-based testing invariants, setting new standards.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Mean Field Social Control: A Unified Reinforcement Learning Framework</title>
<link>https://arxiv.org/abs/2502.20029</link>
<guid>https://arxiv.org/abs/2502.20029</guid>
<content:encoded><![CDATA[
arXiv:2502.20029v2 Announce Type: replace 
Abstract: This paper studies linear quadratic Gaussian robust mean field social control problems in the presence of multiplicative noise. We aim to compute asymptotic decentralized strategies without requiring full prior knowledge of agents' dynamics. The primary challenges lie in solving an indefinite stochastic algebraic Riccati equation for feedback gains, and an indefinite algebraic Riccati equation for feedforward gains. To overcome these challenges, we first propose a unified dual-loop iterative framework that handles both indefinite Riccati-type equations, and provide rigorous convergence proofs for both the outer-loop and inner-loop iterations. Secondly, considering the potential biases arising in the iterative processes due to estimation and modeling errors, we verify the robustness of the proposed algorithm using the small-disturbance input-to-state stability technique. Convergence to a neighborhood of the optimal solution is thus ensured, even in the existence of disturbances. Finally, to relax the limitation of requiring precise knowledge of agents' dynamics, we employ the integral reinforcement learning technique to develop a data-driven method within the dual-loop iterative framework. A numerical example is provided to demonstrate the effectiveness of the proposed algorithm.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Implicit Barrier from Utility Maximization: Lightweight Interior-Point Methods for Market Equilibrium</title>
<link>https://arxiv.org/abs/2508.04822</link>
<guid>https://arxiv.org/abs/2508.04822</guid>
<content:encoded><![CDATA[
arXiv:2508.04822v2 Announce Type: replace-cross 
Abstract: We study the computation of the market equilibrium in Fisher exchange markets with divisible goods and players endowed with heterogeneous utilities. In particular, we consider the decentralized polynomial-time interior-point strategies that update \emph{only} the prices, mirroring the t\^atonnement process. The key ingredient is the \emph{implicit barrier} inherent from utility maximization, which induces unbounded demand when the goods are almost free of charge. Focusing on a ubiquitous class of utilities, we formalize this observation. A companion result suggests that no additional effort is required for computing high-order derivatives; all the necessary information is readily available when collecting the best responses. To tackle the Newton systems in the interior-point methods, we present an explicitly invertible approximation of the Hessian operator with high probability guarantees, and a scaling matrix that minimizes the condition number of the linear system. Building on these tools, we design two inexact lightweight interior-point methods. One such method has $\cO(\log(\tfrac{1}{\epsilon}))$ complexity rate. Under mild conditions, the other method achieves a non-asymptotic superlinear convergence rate. Preliminary experiments are presented to justify the capability of the proposed methods for large-scale problems. Extensions of our approach are also discussed.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Setchain Algorithms for Blockchain Scalability</title>
<link>https://arxiv.org/abs/2509.09795</link>
<guid>https://arxiv.org/abs/2509.09795</guid>
<content:encoded><![CDATA[
arXiv:2509.09795v1 Announce Type: new 
Abstract: Setchain has been proposed to increase blockchain scalability by relaxing the strict total order requirement among transactions. Setchain organizes elements into a sequence of sets, referred to as epochs, so that elements within each epoch are unordered. In this paper, we propose and evaluate three distinct Setchain algorithms, that leverage an underlying block-based ledger. Vanilla is a basic implementation that serves as a reference point. Compresschain aggregates elements into batches, and compresses these batches before appending them as epochs in the ledger. Hashchain converts batches into fixed-length hashes which are appended as epochs in the ledger. This requires Hashchain to use a distributed service to obtain the batch contents from its hash. To allow light clients to safely interact with only one server, the proposed algorithms maintain, as part of the Setchain, proofs for the epochs. An epoch-proof is the hash of the epoch, cryptographically signed by a server. A client can verify the correctness of an epoch with $f+1$ epoch-proofs (where $f$ is the maximum number of Byzantine servers assumed). All three Setchain algorithms are implemented on top of the CometBFT blockchain application platform. We conducted performance evaluations across various configurations, using clusters of four, seven, and ten servers. Our results show that the Setchain algorithms reach orders of magnitude higher throughput than the underlying blockchain, and achieve finality with latency below 4 seconds.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ordered Consensus with Equal Opportunity</title>
<link>https://arxiv.org/abs/2509.09868</link>
<guid>https://arxiv.org/abs/2509.09868</guid>
<content:encoded><![CDATA[
arXiv:2509.09868v1 Announce Type: new 
Abstract: The specification of state machine replication (SMR) has no requirement on the final total order of commands. In blockchains based on SMR, however, order matters, since different orders could provide their clients with different financial rewards. Ordered consensus augments the specification of SMR to include specific guarantees on such order, with a focus on limiting the influence of Byzantine nodes. Real-world ordering manipulations, however, can and do happen even without Byzantine replicas, typically because of factors, such as faster networks or closer proximity to the blockchain infrastructure, that give some clients an unfair advantage. To address this challenge, this paper proceeds to extend ordered consensus by requiring it to also support equal opportunity, a concrete notion of fairness, widely adopted in social sciences. Informally, equal opportunity requires that two candidates who, according to a set of criteria deemed to be relevant, are equally qualified for a position (in our case, a specific slot in the SMR total order), should have an equal chance of landing it. We show how randomness can be leveraged to keep bias in check, and, to this end, introduce the secret random oracle (SRO), a system component that generates randomness in a fault-tolerant manner. We describe two SRO designs based, respectively, on trusted hardware and threshold verifiable random functions, and instantiate them in Bercow, a new ordered consensus protocol that, by approximating equal opportunity up to within a configurable factor, can effectively mitigate well-known ordering attacks in SMR-based blockchains.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Predictions in Power System Voltage Control: An Adaptive Approach</title>
<link>https://arxiv.org/abs/2509.09937</link>
<guid>https://arxiv.org/abs/2509.09937</guid>
<content:encoded><![CDATA[
arXiv:2509.09937v1 Announce Type: new 
Abstract: High variability of solar PV and sudden changes in load (e.g., electric vehicles and storage) can lead to large voltage fluctuations in the distribution system. In recent years, a number of controllers have been designed to optimize voltage control. These controllers, however, almost always assume that the net load in the system remains constant over a sufficiently long time, such that the control actions converge before the load changes again. Given the intermittent and uncertain nature of renewable resources, it is becoming important to explicitly consider net load that is time-varying.
  This paper proposes an adaptive approach to voltage control in power systems with significant time-varying net load. We leverage advances in short-term load forecasting, where the net load in the system can be partially predicted using local measurements. We integrate these predictions into the design of adaptive controllers, and prove that the overall control architecture achieves input-to-state stability in a decentralized manner. We optimize the control policy through reinforcement learning. Case studies are conducted using time-varying load data from a real-world distribution system.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SmartCoder-R1: Towards Secure and Explainable Smart Contract Generation with Security-Aware Group Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2509.09942</link>
<guid>https://arxiv.org/abs/2509.09942</guid>
<content:encoded><![CDATA[
arXiv:2509.09942v1 Announce Type: new 
Abstract: Smart contracts automate the management of high-value assets, where vulnerabilities can lead to catastrophic financial losses. This challenge is amplified in Large Language Models (LLMs) by two interconnected failures: they operate as unauditable "black boxes" lacking a transparent reasoning process, and consequently, generate code riddled with critical security vulnerabilities. To address both issues, we propose SmartCoder-R1 (based on Qwen2.5-Coder-7B), a novel framework for secure and explainable smart contract generation. It begins with Continual Pre-training (CPT) to specialize the model. We then apply Long Chain-of-Thought Supervised Fine-Tuning (L-CoT SFT) on 7,998 expert-validated reasoning-and-code samples to train the model to emulate human security analysis. Finally, to directly mitigate vulnerabilities, we employ Security-Aware Group Relative Policy Optimization (S-GRPO), a reinforcement learning phase that refines the generation policy by optimizing a weighted reward signal for compilation success, security compliance, and format correctness. Evaluated against 17 baselines on a benchmark of 756 real-world functions, SmartCoder-R1 establishes a new state of the art, achieving top performance across five key metrics: a ComPass of 87.70%, a VulRate of 8.60%, a SafeAval of 80.16%, a FuncRate of 53.84%, and a FullRate of 50.53%. This FullRate marks a 45.79% relative improvement over the strongest baseline, DeepSeek-R1. Crucially, its generated reasoning also excels in human evaluations, achieving high-quality ratings for Functionality (82.7%), Security (85.3%), and Clarity (90.7%).
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedRP: A Communication-Efficient Approach for Differentially Private Federated Learning Using Random Projection</title>
<link>https://arxiv.org/abs/2509.10041</link>
<guid>https://arxiv.org/abs/2509.10041</guid>
<content:encoded><![CDATA[
arXiv:2509.10041v1 Announce Type: new 
Abstract: Federated learning (FL) offers an innovative paradigm for collaborative model training across decentralized devices, such as smartphones, balancing enhanced predictive performance with the protection of user privacy in sensitive areas like Internet of Things (IoT) and medical data analysis. Despite its advantages, FL encounters significant challenges related to user privacy protection against potential attacks and the management of communication costs. This paper introduces a novel federated learning algorithm called FedRP, which integrates random projection techniques with the Alternating Direction Method of Multipliers (ADMM) optimization framework. This approach enhances privacy by employing random projection to reduce the dimensionality of model parameters prior to their transmission to a central server, reducing the communication cost. The proposed algorithm offers a strong $(\epsilon, \delta)$-differential privacy guarantee, demonstrating resilience against data reconstruction attacks. Experimental results reveal that FedRP not only maintains high model accuracy but also outperforms existing methods, including conventional differential privacy approaches and FedADMM, in terms of both privacy preservation and communication efficiency.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cost-Free Personalization via Information-Geometric Projection in Bayesian Federated Learning</title>
<link>https://arxiv.org/abs/2509.10132</link>
<guid>https://arxiv.org/abs/2509.10132</guid>
<content:encoded><![CDATA[
arXiv:2509.10132v1 Announce Type: new 
Abstract: Bayesian Federated Learning (BFL) combines uncertainty modeling with decentralized training, enabling the development of personalized and reliable models under data heterogeneity and privacy constraints. Existing approaches typically rely on Markov Chain Monte Carlo (MCMC) sampling or variational inference, often incorporating personalization mechanisms to better adapt to local data distributions. In this work, we propose an information-geometric projection framework for personalization in parametric BFL. By projecting the global model onto a neighborhood of the user's local model, our method enables a tunable trade-off between global generalization and local specialization. Under mild assumptions, we show that this projection step is equivalent to computing a barycenter on the statistical manifold, allowing us to derive closed-form solutions and achieve cost-free personalization. We apply the proposed approach to a variational learning setup using the Improved Variational Online Newton (IVON) optimizer and extend its application to general aggregation schemes in BFL. Empirical evaluations under heterogeneous data distributions confirm that our method effectively balances global and local performance with minimal computational overhead.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks</title>
<link>https://arxiv.org/abs/2509.10163</link>
<guid>https://arxiv.org/abs/2509.10163</guid>
<content:encoded><![CDATA[
arXiv:2509.10163v1 Announce Type: new 
Abstract: As sixth-generation (6G) networks move toward ultra-dense, intelligent edge environments, efficient resource management under stringent privacy, mobility, and energy constraints becomes critical. This paper introduces a novel Federated Multi-Agent Reinforcement Learning (Fed-MARL) framework that incorporates cross-layer orchestration of both the MAC layer and application layer for energy-efficient, privacy-preserving, and real-time resource management across heterogeneous edge devices. Each agent uses a Deep Recurrent Q-Network (DRQN) to learn decentralized policies for task offloading, spectrum access, and CPU energy adaptation based on local observations (e.g., queue length, energy, CPU usage, and mobility). To protect privacy, we introduce a secure aggregation protocol based on elliptic curve Diffie Hellman key exchange, which ensures accurate model updates without exposing raw data to semi-honest adversaries. We formulate the resource management problem as a partially observable multi-agent Markov decision process (POMMDP) with a multi-objective reward function that jointly optimizes latency, energy efficiency, spectral efficiency, fairness, and reliability under 6G-specific service requirements such as URLLC, eMBB, and mMTC. Simulation results demonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines in task success rate, latency, energy efficiency, and fairness, while ensuring robust privacy protection and scalability in dynamic, resource-constrained 6G edge networks.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>