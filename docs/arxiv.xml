<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs</link>

<item>
<title>Pruning as Evolution: Emergent Sparsity Through Selection Dynamics in Neural Networks</title>
<link>https://arxiv.org/abs/2601.10765</link>
<guid>https://arxiv.org/abs/2601.10765</guid>
<content:encoded><![CDATA[
<div> 关键词：神经网络剪枝、过参数化、进化视角、群体选择、无显式剪枝调度<br><br>总结:  
1. 神经网络通常在高度过参数化状态下训练，但许多参数在学习过程中变得冗余。  
2. 传统剪枝方法依赖显式干预（如重要性阈值或正则化），与梯度训练的去中心化、随机性和路径依赖特性不符。  
3. 作者提出一种进化视角：将神经元等参数组视为种群，其影响力在选择压力下连续演化。  
4. 剪枝被解释为种群灭绝——持续低适应度的组件自然失去影响，无需离散剪枝计划或平衡计算。  
5. 在MNIST上的实验表明，该方法在不使用显式剪枝调度的情况下，实现了可衡量的精度-稀疏度权衡。 <div>
arXiv:2601.10765v1 Announce Type: new 
Abstract: Neural networks are commonly trained in highly overparameterized regimes, yet empirical evidence consistently shows that many parameters become redundant during learning. Most existing pruning approaches impose sparsity through explicit intervention, such as importance-based thresholding or regularization penalties, implicitly treating pruning as a centralized decision applied to a trained model. This assumption is misaligned with the decentralized, stochastic, and path-dependent character of gradient-based training. We propose an evolutionary perspective on pruning: parameter groups (neurons, filters, heads) are modeled as populations whose influence evolves continuously under selection pressure. Under this view, pruning corresponds to population extinction: components with persistently low fitness gradually lose influence and can be removed without discrete pruning schedules and without requiring equilibrium computation. We formalize neural pruning as an evolutionary process over population masses, derive selection dynamics governing mass evolution, and connect fitness to local learning signals. We validate the framework on MNIST using a population-scaled MLP (784--512--256--10) with 768 prunable neuron populations. All dynamics reach dense baselines near 98\% test accuracy. We benchmark post-training hard pruning at target sparsity levels (35--50\%): pruning 35\% yields $\approx$95.5\% test accuracy, while pruning 50\% yields $\approx$88.3--88.6\%, depending on the dynamic. These results demonstrate that evolutionary selection produces a measurable accuracy--sparsity tradeoff without explicit pruning schedules during training.
]]></content:encoded>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparing User Time with a Socially-Aware Independent Metaverse Avatar</title>
<link>https://arxiv.org/abs/2601.11115</link>
<guid>https://arxiv.org/abs/2601.11115</guid>
<content:encoded><![CDATA[
<div> 元宇宙、独立虚拟化身、社交时间分配、社交网络优化、NP难问题<br><br>总结:<br>1. 文章探讨元宇宙中独立虚拟化身（可自主代表用户进行社交的数字实体）对在线社交网络的影响。<br>2. 提出一种结合进化人类学理论的计算模型，用于量化用户社交生活，并纳入虚拟化身介导的互动机制。<br>3. 模型考虑了以虚拟化身部分替代面对面互动的效果，同时纳入社交冲突与特定社交约束因素。<br>4. 由于引入化身后社交网络优化问题为NP难问题，作者设计了一种启发式算法予以求解。<br>5. 仿真实验表明，相比传统社交方式，虚拟化身能有效提升社交连接性与效率，为未来元宇宙社交系统设计提供理论基础。 <div>
arXiv:2601.11115v1 Announce Type: new 
Abstract: The Metaverse is redefining digital interactions by merging physical, virtual, and social dimensions, yet its effects on social networking remain largely unexplored. This work examines the role of independent avatars (autonomous digital entities capable of managing social interactions on behalf of users), to optimize social time allocation and reshape Metaverse-based Online Social Networks. We propose a novel computational model that integrates a quantitative and realistic representation of user social life, grounded in evolutionary anthropology, with a framework for avatar-mediated interactions. Our model quantifies the effectiveness of a partial replacement of in-person interactions with independent avatar interactions. Additionally, it accounts for social conflicts and specific socialization constraints. We leverage our model to explore the benefits and trade-offs of an avatar-augmented social life in the Metaverse. Since the exact problem formulation leads to an NP-hard optimization problem when incorporating avatars into the social network, we tackle this challenge by introducing a heuristic solution. Through simulations, we compare avatar-mediated and non-avatar-mediated social networking, demonstrating the potential of independent avatars to enhance social connectivity and efficiency. Our findings provide a foundation for optimizing Metaverse-based social interactions, as well as useful insights for future digital social network design.
]]></content:encoded>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Patterns of Bot Participation and Emotional Influence in Open-Source Development</title>
<link>https://arxiv.org/abs/2601.11138</link>
<guid>https://arxiv.org/abs/2601.11138</guid>
<content:encoded><![CDATA[
<div> 关键词：以太坊、开源讨论、机器人、情感分析、开发者互动<br><br>总结:  
1. 研究分析了以太坊生态系统中机器人在开源讨论中的作用及其对开发者情感语调的影响。  
2. 数据集涵盖10个代码仓库的36,875个账户，其中确认有105个机器人（占比0.28%）。  
3. 人类参与呈U型模式，而机器人在拉取请求中均匀活跃，在议题中则偏向后期参与。  
4. 机器人在拉取请求中响应速度更快，但在议题中承担较慢的维护角色。  
5. 情感分析显示，机器人发言更中立，但其介入后人类评论的中立性下降，更多表达感激、钦佩与乐观，减少困惑情绪。 <div>
arXiv:2601.11138v1 Announce Type: new 
Abstract: We study how bots contribute to open-source discussions in the Ethereum ecosystem and whether they influence developers' emotional tone. Our dataset covers 36,875 accounts across ten repositories with 105 validated bots (0.28%). Human participation follows a U-shaped pattern, while bots engage in uniform (pull requests) or late-stage (issues) activity. Bots respond faster than humans in pull requests but play slower maintenance roles in issues. Using a model trained on 27 emotion categories, we find bots are more neutral, yet their interventions are followed by reduced neutrality in human comments, with shifts toward gratitude, admiration, and optimism and away from confusion. These findings indicate that even a small number of bots are associated with changes in both timing and emotional dynamics of developer communication.
]]></content:encoded>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>InterPUF: Distributed Authentication via Physically Unclonable Functions and Multi-party Computation for Reconfigurable Interposers</title>
<link>https://arxiv.org/abs/2601.11368</link>
<guid>https://arxiv.org/abs/2601.11368</guid>
<content:encoded><![CDATA[
<div> 关键词：系统级封装（SiP）、可重构中介层、物理不可克隆函数（PUF）、多方安全计算（MPC）、分布式信任根<br><br>总结:  
1. 针对现代系统级封装（SiP）中可重构中介层带来的去中心化信任挑战，提出InterPUF认证框架。  
2. InterPUF利用基于路径的差分延迟物理不可克隆函数（PUF），将中介层转变为分布式信任根。  
3. 通过多方安全计算（MPC）保护原始PUF签名不被暴露，实现安全认证。  
4. 硬件评估表明，该方案仅引入0.23%面积和0.072%功耗开销，认证延迟控制在数十纳秒内。  
5. 仿真验证了其在工艺、电压和温度变化下的高唯一性、可靠性和抗建模能力，无需依赖中心化信任锚点。 <div>
arXiv:2601.11368v1 Announce Type: new 
Abstract: Modern system-in-package (SiP) platforms increasingly adopt reconfigurable interposers to enable plug-and-play chiplet integration across heterogeneous multi-vendor ecosystems. However, this flexibility introduces severe trust challenges, as traditional authentication schemes fail to scale or adapt in decentralized, post-fabrication programmable environments. This paper presents InterPUF, a compact and scalable authentication framework that transforms the interposer into a distributed root of trust. InterPUF embeds a route-based differential delay physically unclonable function (PUF) across the reconfigurable interconnect and secures authentication using multi-party computation (MPC), ensuring raw PUF signatures are never exposed. Our hardware evaluation shows only 0.23% area and 0.072% power overhead across diverse chiplets while preserving authentication latency within tens of nanoseconds. Simulation results using pyPUF confirm strong uniqueness, reliability, and modeling resistance under process, voltage, and temperature variations. By combining interposer-resident PUF primitives with cryptographic hashing and collaborative verification, InterPUF enforces a minimal-trust authentication model without relying on a centralized anchor.
]]></content:encoded>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Near-Optimal Decentralized Stochastic Nonconvex Optimization with Heavy-Tailed Noise</title>
<link>https://arxiv.org/abs/2601.11435</link>
<guid>https://arxiv.org/abs/2601.11435</guid>
<content:encoded><![CDATA[
<div> 关键词：去中心化优化、非凸优化、重尾梯度噪声、梯度追踪、样本复杂度<br><br>总结:<br>1. 本文研究了在行随机网络上的去中心化随机非凸优化问题，特别关注实际应用中常见的重尾梯度噪声。<br>2. 提出了一种结合Pull-Diag梯度追踪的去中心化归一化随机梯度下降算法。<br>3. 该算法在达到近似稳定点时，实现了最优的样本复杂度和接近最优的通信复杂度。<br>4. 将该框架扩展至无向网络场景，同样获得了近乎紧致的上界复杂度结果。<br>5. 通过实验验证了所提方法在实际应用中的优越性能。 <div>
arXiv:2601.11435v1 Announce Type: cross 
Abstract: This paper studies decentralized stochastic nonconvex optimization problem over row-stochastic networks. We consider the heavy-tailed gradient noise which is empirically observed in many popular real-world applications. Specifically, we propose a decentralized normalized stochastic gradient descent with Pull-Diag gradient tracking, which achieves approximate stationary points with the optimal sample complexity and the near-optimal communication complexity. We further follow our framework to study the setting of undirected networks, also achieving the nearly tight upper complexity bounds. Moreover, we conduct empirical studies to show the practical superiority of the proposed methods.
]]></content:encoded>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enforcing Control Flow Integrity on DeFi Smart Contracts</title>
<link>https://arxiv.org/abs/2504.05509</link>
<guid>https://arxiv.org/abs/2504.05509</guid>
<content:encoded><![CDATA[
<div> 智能合约、DeFi安全、控制流完整性、攻击检测、CrossGuard<br><br>总结:<br>1. 智能合约在DeFi中广泛应用，但易受安全攻击，造成重大损失。<br>2. 现有防护措施难以应对DeFi协议的可组合性及日益复杂的攻击手段。<br>3. 通过对37个被黑DeFi协议的历史交易分析，发现攻击交易通常引入前所未见的控制流，而正常交易控制流有限且重复。<br>4. 基于此，研究者提出CrossGuard框架，在链上实施控制流完整性保护，仅需在部署时配置一次，无需预知具体攻击类型。<br>5. CrossGuard通过运行时控制流白名单和简化启发式策略，有效阻止35起已知攻击，误报率仅0.26%，且额外Gas开销极低。 <div>
arXiv:2504.05509v2 Announce Type: replace 
Abstract: Smart contracts power decentralized financial (DeFi) services but are vulnerable to security exploits that can lead to significant financial losses. Existing security measures often fail to adequately protect these contracts due to the composability of DeFi protocols and the increasing sophistication of attacks. Through a large-scale empirical study of historical transactions from the 37 hacked DeFi protocols, we discovered that while benign transactions typically exhibit a limited number of unique control flows, in stark contrast, attack transactions consistently introduce novel, previously unobserved control flows. Building on these insights, we developed CrossGuard, a novel framework that enforces control flow integrity onchain to secure smart contracts. Crucially, CrossGuard does not require prior knowledge of specific hacks. Instead, configured only once at deployment, it enforces control flow whitelisting policies and applies simplification heuristics at runtime. This approach monitors and prevents potential attacks by reverting all transactions that do not adhere to the established control flow whitelisting rules. Our evaluation demonstrates that CrossGuard effectively blocks 35 of the 37 analyzed attacks when configured only once at contract deployment, maintaining a low false positive rate of 0.26% and minimal additional gas costs. These results underscore the efficacy of applying control flow integrity to smart contracts, significantly enhancing security beyond traditional methods and addressing the evolving threat landscape in the DeFi ecosystem.
]]></content:encoded>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Security Vulnerabilities in Ethereum Smart Contracts: A Systematic Analysis</title>
<link>https://arxiv.org/abs/2504.05968</link>
<guid>https://arxiv.org/abs/2504.05968</guid>
<content:encoded><![CDATA[
<div> 智能合约、以太坊、安全漏洞、攻击类型、防护措施<br><br>总结:<br>1. 智能合约在保险、互联网和游戏等领域广泛应用，但频发的安全事件（如“The DAO”造成超6000万美元损失）凸显其安全重要性。<br>2. 本文聚焦以太坊智能合约，介绍其架构、机制及开发环境（Remix平台、Solidity语言）。<br>3. 针对The DAO、Parity等四大安全事件，分析整数溢出、重入、访问控制和拒绝服务四类攻击原理并复现漏洞场景。<br>4. 同时详细阐述短地址攻击、抢先交易攻击和特权函数暴露等其他攻击方式。<br>5. 提出相应防护措施，并强调持续研究现有漏洞对防范未来新型漏洞具有重要意义。 <div>
arXiv:2504.05968v4 Announce Type: replace 
Abstract: Smart contracts are a secure and trustworthy application that plays a vital role in decentralized applications in various fields such as insurance,the internet, and gaming. However, in recent years, smart contract security breaches have occurred frequently, and due to their financial properties, they have caused huge economic losses, such as the most famous security incident "The DAO" which caused a loss of over $60 million in Ethereum. This has drawn a lot of attention from all sides. Writing a secure smart contract is now a critical issue. This paper focuses on Ether smart contracts and explains the main components of Ether, smart contract architecture and mechanism. The environment used in this paper is the Ethernet environment, using remix online compilation platform and Solidity language, according to the four security events of American Chain, The DAO, Parity and KotET, the principles of integer overflow attack, reentrant attack, access control attack and denial of service attack are studied and analyzed accordingly, and the scenarios of these vulnerabilities are reproduced, and the measures to prevent them are given. Finally, preventive measures are given. In addition, the principles of short address attack, early transaction attack and privileged function exposure attack are also introduced in detail, and security measures are proposed. As vulnerabilities continue to emerge, their classification will also evolve. The analysis and research of the current vulnerabilities are also to lay a solid foundation for avoiding more vulnerabilities.
]]></content:encoded>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Aggregation to Selection: User-Validated Distributed Social Recommendation</title>
<link>https://arxiv.org/abs/2505.21388</link>
<guid>https://arxiv.org/abs/2505.21388</guid>
<content:encoded><![CDATA[
<div> 分布式社交推荐、用户验证、多数共识、链接预测、Acc@K<br><br>总结:<br>1. 社交推荐系统通常采用分布式结构，但现有方法忽视了用户在验证推荐连接中的主动作用。<br>2. 传统基于全局排序的评估指标难以衡量用户确认的推荐是否正确。<br>3. 提出DeSocial框架，允许用户选择算法并借助多个独立用户验证者通过多数共识机制验证潜在连接。<br>4. 将问题建模为链接预测与验证任务，并引入新评估指标Acc@K，衡量用户批准推荐的准确性。<br>5. 在4个真实社交网络上的实验表明，DeSocial在决策正确性和鲁棒性上优于现有基线方法。 <div>
arXiv:2505.21388v3 Announce Type: replace 
Abstract: Social recommender systems facilitate social connections by identifying potential friends for users. Each user maintains a local social network centered around themselves, resulting in a naturally distributed social structure. Recent research on distributed modeling for social recommender systems has gained increasing attention, as it naturally aligns with the user-centric structure of user interactions. Current distributed social recommender systems rely on automatically combining predictions from multiple models, often overlooking the user's active role in validating whether suggested connections are appropriate. Moreover, recommendation decisions are validated by individual users rather than derived from a single global ordering of candidates. As a result, standard ranking-based evaluation metrics make it difficult to evaluate whether a user-confirmed recommendation decision is actually correct. To address these limitations, we propose DeSocial, a distributed social recommendation framework with user-validation. DeSocial enables users to select recommendation algorithms to validate their potential connections, and the verification is processed through majority consensus among multiple independent user validators. To evaluate the distributed recommender system with user validator, we formulate this setting as a link prediction and verification task and introduce Acc@K, a consensus-based evaluation metric that measures whether user-approved recommendations are correct. Experiments on 4 real-world social networks shows that DeSocial improves decision correctness and robustness compared to single-point and distributed baselines. These findings highlight the potential of user-validated distributed recommender systems as a practical approach to social recommendation, with broader applicability to distributed and decentralized recommendations. Code: https://github.com/agiresearch/DeSocial.
]]></content:encoded>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Distributed Generative AI Approach for Heterogeneous Multi-Domain Environments under Data Sharing constraints</title>
<link>https://arxiv.org/abs/2507.12979</link>
<guid>https://arxiv.org/abs/2507.12979</guid>
<content:encoded><![CDATA[
<div> 联邦学习、生成对抗网络、数据异构性、设备异构性、隐私保护<br><br>总结:<br>1. 针对生成对抗网络（GAN）训练所需大量数据与算力难以获取的问题，提出一种去中心化的GAN训练方法。<br>2. 该方法结合KLD加权聚类联邦学习，有效应对多域非独立同分布（non-IID）数据带来的数据异构性挑战。<br>3. 引入异构U型分割学习机制，解决边缘设备算力差异大（设备异构性）问题，同时严格禁止原始数据或标签在节点间共享。<br>4. 在保障隐私和版权的前提下，充分利用闲置的低能力设备进行协同训练。<br>5. 实验表明，该方法在分类指标上平均提升10%（多域non-IID场景最高达60%），MNIST系列图像生成质量提升1.1–3倍，高分辨率数据集FID分数降低2–70倍。 <div>
arXiv:2507.12979v3 Announce Type: replace 
Abstract: Federated Learning has gained attention for its ability to enable multiple nodes to collaboratively train machine learning models without sharing raw data. At the same time, Generative AI -- particularly Generative Adversarial Networks (GANs) -- have achieved remarkable success across a wide range of domains, such as healthcare, security, and Image Generation. However, training generative models typically requires large datasets and significant computational resources, which are often unavailable in real-world settings. Acquiring such resources can be costly and inefficient, especially when many underutilized devices -- such as IoT devices and edge devices -- with varying capabilities remain idle. Moreover, obtaining large datasets is challenging due to privacy concerns and copyright restrictions, as most devices are unwilling to share their data. To address these challenges, we propose a novel approach for decentralized GAN training that enables utilizing distributed data and underutilized, low-capability devices while not sharing data in its raw form. Our approach is designed to tackle key challenges in decentralized environments, combining KLD-weighted Clustered Federated Learning to address the issues of data heterogeneity and multi-domain datasets, with Heterogeneous U-Shaped split learning to tackle the challenge of device heterogeneity under strict data sharing constraints -- ensuring that no labels or raw data, whether real or synthetic, are ever shared between nodes. Experiments show that our approach demonstrates significant improvements across key metrics, where it achieves an average 10% boost in classification metrics (up to 60% in multi-domain non-IID settings), 1.1x -- 3x higher image generation scores for the MNIST family datasets, and 2x -- 70x lower FID scores for higher resolution datasets. Find our code at https://distributed-gen-ai.github.io/huscf-gan.github.io/.
]]></content:encoded>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incentive Mechanism Design for Privacy-Preserving Decentralized Blockchain Relayers</title>
<link>https://arxiv.org/abs/2601.06699</link>
<guid>https://arxiv.org/abs/2601.06699</guid>
<content:encoded><![CDATA[
<div> 隐私、去中心化中继器、博弈论、激励机制、可靠性<br><br>总结:<br>1. 公有区块链虽具透明性和不可篡改性，但存在用户身份被推断的隐私风险。<br>2. 现有方案依赖单一中继器（relayer）易形成单点故障并削弱隐私保障。<br>3. 本文提出一种基于博弈论激励设计的去中心化中继器架构，将中继器间互动建模为非合作博弈。<br>4. 设计的激励机制使概率上传策略成为唯一的混合纳什均衡，并通过演化博弈分析验证其稳定性。<br>5. 数值实验表明，即使交易成本高，系统仍能保持低于0.05的中断概率，并揭示了隐私、可靠性、鲁棒性与成本之间的基本权衡。 <div>
arXiv:2601.06699v2 Announce Type: replace 
Abstract: Public blockchains, though renowned for their transparency and immutability, suffer from significant privacy concerns. Network-level analysis and long-term observation of publicly available transactions can often be used to infer user identities. To mitigate this, several blockchain applications rely on relayers, which serve as intermediary nodes between users and smart contracts deployed on the blockchain. However, dependence on a single relayer not only creates a single point of failure but also introduces exploitable vulnerabilities that weaken the system's privacy guarantees. This paper proposes a decentralized relayer architecture that enhances privacy and reliability through game-theoretic incentive design. We model the interaction among relayers as a non-cooperative game and design an incentive mechanism in which probabilistic uploading emerges as a unique mixed Nash equilibrium. Using evolutionary game analysis, we demonstrate the equilibrium's stability against perturbations and coordinated deviations. Through numerical evaluations, we analyze how equilibrium strategies and system behavior evolve with key parameters such as the number of relayers, upload costs, rewards, and penalties. In particular, we show that even with high transaction costs, the system maintains reliability with an outage probability below 0.05 . Furthermore, our results highlight a fundamental trade-off between privacy, reliability, robustness, and cost in decentralized relayer systems.
]]></content:encoded>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
<pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
</item>

<item>
<title>Instalaci\'on, configuraci\'on y utilizaci\'on de un nodo Bitcoin en Linux</title>
<link>https://arxiv.org/abs/2601.09748</link>
<guid>https://arxiv.org/abs/2601.09748</guid>
<content:encoded><![CDATA[
<div> 比特币节点、Linux环境、源码编译、Bitcoin Core参数、资源监控<br /><br />总结:  
1. 本文详细记录了在Linux环境下从源码手动编译并部署完整比特币节点的全过程。  
2. 描述了节点安装、配置及与比特币网络完全同步的技术步骤。  
3. 分析了Bitcoin Core运行过程中生成的主要文件及其作用。  
4. 对txindex、prune、dbcache、maxmempool和maxconnections等关键参数进行了实证研究，评估其对节点性能的影响。  
5. 记录并解释了初始区块下载（IBD）期间系统资源（如CPU、内存、磁盘I/O）的使用情况及其重要性，为后续比特币节点性能研究和区块链数据查询工具开发奠定基础。 <div>
arXiv:2601.09748v1 Announce Type: new 
Abstract: This paper documents the installation, configuration, and operation of a full Bitcoin node in a Linux environment, from manual compilation of the source code to complete synchronization with the network. The technical phases of the process are described, the main files generated by Bitcoin Core are analyzed, and the effects of the parameters txindex, prune, dbcache, maxmempool, and maxconnections are empirically studied. System resources during the block download (IBD) mechanism are also documented, and the operational importance of each resource is explained. This paper provides a solid foundation for future research proposals on Bitcoin node performance or for the development of blockchain data query tools.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Risk-Stratified Benchmark Dataset for Bad Randomness (SWC-120) Vulnerabilities in Ethereum Smart Contracts</title>
<link>https://arxiv.org/abs/2601.09836</link>
<guid>https://arxiv.org/abs/2601.09836</guid>
<content:encoded><![CDATA[
<div> Bad Randomness、以太坊智能合约、漏洞检测、函数级验证、风险分级<br /><br />总结:  
1. 以太坊智能合约常使用block.timestamp等可预测且可被矿工操纵的区块属性生成随机数，导致“Bad Randomness”漏洞（SWC-120）。  
2. 现有检测工具仅识别简单模式，无法验证保护性修饰符是否真正覆盖漏洞代码。  
3. 本文构建了包含1,752个经验证合约的基准数据集，采用五阶段方法（关键词过滤、正则匹配、风险分类、函数级验证、上下文分析）进行标注。  
4. 函数级验证发现49%原判为受保护的合约实际仍可被利用，因修饰符未作用于含漏洞的函数。  
5. 合约被分为四类风险等级，并揭示Slither和Mythril等工具在复杂随机性漏洞检测上的严重不足；数据集与脚本已公开。 <div>
arXiv:2601.09836v1 Announce Type: new 
Abstract: Many Ethereum smart contracts rely on block attributes such as block.timestamp or blockhash to generate random numbers for applications like lotteries and games. However, these values are predictable and miner-manipulable, creating the Bad Randomness vulnerability (SWC-120) that has led to real-world exploits. Current detection tools identify only simple patterns and fail to verify whether protective modifiers actually guard vulnerable code. A major obstacle to improving these tools is the lack of large, accurately labeled datasets. This paper presents a benchmark dataset of 1,752 Ethereum smart contracts with validated Bad Randomness vulnerabilities. We developed a five-phase methodology comprising keyword filtering, pattern matching with 58 regular expressions, risk classification, function-level validation, and context analysis. The function-level validation revealed that 49% of contracts initially classified as protected were actually exploitable because modifiers were applied to different functions than those containing vulnerabilities. We classify contracts into four risk levels based on exploitability: HIGH_RISK (no protection), MEDIUM_RISK (miner-exploitable only), LOW_RISK (owner-exploitable only), and SAFE (using Chainlink VRF or commit-reveal). Our dataset is 51 times larger than RNVulDet and the first to provide function-level validation and risk stratification. Evaluation of Slither and Mythril revealed significant detection gaps, as both tools identified none of the vulnerable contracts in our sample, indicating limitations in handling complex randomness patterns. The dataset and validation scripts are publicly available to support future research in smart contract security.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Control Theoretic Approach to Decentralized AI Economy Stabilization via Dynamic Buyback-and-Burn Mechanisms</title>
<link>https://arxiv.org/abs/2601.09961</link>
<guid>https://arxiv.org/abs/2601.09961</guid>
<content:encoded><![CDATA[
<div> 关键词：去中心化人工智能、代币经济、动态控制回购机制、PID控制器、系统稳定性<br /><br />总结:  
1. 去中心化网络推动人工智能民主化，但其代币经济层高度波动，威胁生态长期可持续性。  
2. 现有基于静态或阈值的回购机制无法应对复杂动态，且在市场下行时加剧不稳定性。  
3. 作者提出动态控制回购机制（DCBM），采用带严格偿付能力约束的PID控制器，将代币经济视为动态系统进行调控。  
4. 基于Jump-Diffusion过程的多智能体仿真显示，DCBM相比基线可降低约66%的价格波动，并将运营商流失率从19.5%降至8.1%。  
5. 研究表明，将代币经济从静态规则转向受结构约束的连续控制回路，是构建安全可持续去中心化智能网络的必要条件。 <div>
arXiv:2601.09961v1 Announce Type: new 
Abstract: The democratization of artificial intelligence through decentralized networks represents a paradigm shift in computational provisioning, yet the long-term viability of these ecosystems is critically endangered by the extreme volatility of their native economic layers. Current tokenomic models, which predominantly rely on static or threshold-based buyback heuristics, are ill-equipped to handle complex system dynamics and often function pro-cyclically, exacerbating instability during market downturns. To bridge this gap, we propose the Dynamic-Control Buyback Mechanism (DCBM), a formalized control-theoretic framework that utilizes a Proportional-Integral-Derivative (PID) controller with strict solvency constraints to regulate the token economy as a dynamical system. Extensive agent-based simulations utilizing Jump-Diffusion processes demonstrate that DCBM fundamentally outperforms static baselines, reducing token price volatility by approximately 66% and lowering operator churn from 19.5% to 8.1% in high-volatility regimes. These findings establish that converting tokenomics from static rules into continuous, structurally constrained control loops is a necessary condition for secure and sustainable decentralized intelligence networks.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Unlearning in Edge Networks: A Survey of Fundamentals, Challenges, Practical Applications and Future Directions</title>
<link>https://arxiv.org/abs/2601.09978</link>
<guid>https://arxiv.org/abs/2601.09978</guid>
<content:encoded><![CDATA[
<div> 联邦学习、机器遗忘、数据删除、隐私合规、分布式系统<br /><br />总结:<br />1. 联邦学习（FL）虽保障数据隐私，但难以满足如“被遗忘权”等法规要求的数据删除需求。<br />2. 机器遗忘（MU）在集中式学习中用于移除特定数据影响，其在联邦环境中的延伸称为联邦遗忘（FUL）。<br />3. FUL旨在分布式异构环境下高效消除特定客户端或数据子集对全局模型的贡献。<br />4. 当前FUL研究聚焦三大挑战：通信开销、资源分配、安全与隐私保护。<br />5. 文章综述了FUL框架、应用场景，并指出未来研究方向，以推动构建合规、可信、用户中心的联邦系统。 <div>
arXiv:2601.09978v1 Announce Type: new 
Abstract: The proliferation of connected devices and privacy-sensitive applications has accelerated the adoption of Federated Learning (FL), a decentralized paradigm that enables collaborative model training without sharing raw data. While FL addresses data locality and privacy concerns, it does not inherently support data deletion requests that are increasingly mandated by regulations such as the Right to be Forgotten (RTBF). In centralized learning, this challenge has been studied under the concept of Machine Unlearning (MU), that focuses on efficiently removing the influence of specific data samples or clients from trained models. Extending this notion to federated settings has given rise to Federated Unlearning (FUL), a new research area concerned with eliminating the contributions of individual clients or data subsets from the global FL model in a distributed and heterogeneous environment. In this survey, we first introduce the fundamentals of FUL. Then, we review the FUL frameworks that are proposed to address the three main implementation challenges, i.e., communication cost, resource allocation as well as security and privacy. Furthermore, we discuss applications of FUL in the modern distributed computer networks. We also highlight the open challenges and future research opportunities. By consolidating existing knowledge and mapping open problems, this survey aims to serve as a foundational reference for researchers and practitioners seeking to advance FL to build trustworthy, regulation-compliant and user-centric federated systems.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>PID-Guided Partial Alignment for Multimodal Decentralized Federated Learning</title>
<link>https://arxiv.org/abs/2601.10012</link>
<guid>https://arxiv.org/abs/2601.10012</guid>
<content:encoded><![CDATA[
<div> 多模态去中心化联邦学习、部分信息分解、特征裂变、对等网络、异构代理<br /><br />总结:<br />1. 多模态去中心化联邦学习（DFL）面临代理间模态和模型架构异构的挑战，传统共享嵌入方法导致单模态与多模态代理梯度不一致，抑制有效协作。<br />2. 提出PARSE框架，在无服务器环境下利用部分信息分解（PID），通过特征裂变将潜在表示分解为冗余、独特和协同三部分。<br />3. 代理仅在具备对应模态的前提下，交换语义可共享的表示片段，实现切片级的部分对齐，支持异构代理间的对等知识共享。<br />4. PARSE无需中央协调或梯度手术，有效解决单/多模态梯度冲突，兼容标准DFL约束。<br />5. 实验表明，PARSE在多种基准和代理组合下均优于任务、模态及混合共享基线，消融实验和可视化验证了其设计的高效性与鲁棒性。 <div>
arXiv:2601.10012v1 Announce Type: new 
Abstract: Multimodal decentralized federated learning (DFL) is challenging because agents differ in available modalities and model architectures, yet must collaborate over peer-to-peer (P2P) networks without a central coordinator. Standard multimodal pipelines learn a single shared embedding across all modalities. In DFL, such a monolithic representation induces gradient misalignment between uni- and multimodal agents; as a result, it suppresses heterogeneous sharing and cross-modal interaction. We present PARSE, a multimodal DFL framework that operationalizes partial information decomposition (PID) in a server-free setting. Each agent performs feature fission to factorize its latent representation into redundant, unique, and synergistic slices. P2P knowledge sharing among heterogeneous agents is enabled by slice-level partial alignment: only semantically shareable branches are exchanged among agents that possess the corresponding modality. By removing the need for central coordination and gradient surgery, PARSE resolves uni-/multimodal gradient conflicts, thereby overcoming the multimodal DFL dilemma while remaining compatible with standard DFL constraints. Across benchmarks and agent mixes, PARSE yields consistent gains over task-, modality-, and hybrid-sharing DFL baselines. Ablations on fusion operators and split ratios, together with qualitative visualizations, further demonstrate the efficiency and robustness of the proposed design.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAFEDistill: Learning Personalized and Dynamic Models through Federated Early-Exit Network Distillation</title>
<link>https://arxiv.org/abs/2601.10015</link>
<guid>https://arxiv.org/abs/2601.10015</guid>
<content:encoded><![CDATA[
<div> 个性化联邦学习、早退出网络、冲突感知、知识蒸馏、通信开销<br /><br />总结:<br />1. 个性化联邦学习（PFL）可在异构数据上协同训练并适配各客户端分布，但现有方法生成静态模型，难以适应动态推理需求。<br />2. 早退出网络（EENs）支持自适应推理，但其在PFL中面临客户端异构性与不同出口目标间的深度干扰问题。<br />3. 现有研究无法同时解决上述两类冲突，导致性能受限。<br />4. 本文提出CAFEDistill框架，通过冲突感知的联邦出口蒸馏机制，结合深度优先的学生协调策略，有效缓解浅层与深层出口间的干扰，并实现跨客户端的个性化知识迁移。<br />5. 该方法采用客户端解耦设计，显著降低通信开销，实验表明其在精度和推理成本（降低30.79%-46.86%）方面优于现有技术。 <div>
arXiv:2601.10015v1 Announce Type: new 
Abstract: Personalized Federated Learning (PFL) enables collaboratively model training on decentralized, heterogeneous data while tailoring them to each client's unique distribution. However, existing PFL methods produce static models with a fixed tradeoff between accuracy and efficiency, limiting their applicability in environments where inference requirements vary with contexts and resource availability. Early-exit networks (EENs) offer adaptive inference by attaching intermediate classifiers. Yet integrating them into PFL is challenging due to client-wise heterogeneity and depth-wise interference arising from conflicting exit objectives. Prior studies fail to resolve both conflicts simultaneously, leading to suboptimal performance. In this paper, we propose CAFEDistill, a Conflict-Aware Federated Exit Distillation framework that jointly addresses these conflicts and extends PFL to early-exit networks. Through a progressive, depth-prioritized student coordination mechanism, CAFEDistill mitigates interference among shallow and deep exits while allowing effective personalized knowledge transfer across clients. Furthermore, it reduces communication overhead via a client-decoupled formulation. Extensive evaluations show that CAFEDistill outperforms the state-of-the-arts, achieving higher accuracy and reducing inference costs by 30.79%-46.86%.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fuzzychain-edge: A novel Fuzzy logic-based adaptive Access control model for Blockchain in Edge Computing</title>
<link>https://arxiv.org/abs/2601.10105</link>
<guid>https://arxiv.org/abs/2601.10105</guid>
<content:encoded><![CDATA[
<div> 物联网；边缘计算；区块链；模糊逻辑；零知识证明<br /><br />总结:<br />1. 物联网与边缘计算融合在医疗等领域带来实时数据共享与远程监控，但也引发数据隐私与安全挑战。<br />2. 传统访问控制和中心化安全机制难以应对敏感环境中的未授权访问和数据滥用问题。<br />3. 研究提出Fuzzychain-edge模型，结合零知识证明、模糊逻辑与智能合约，构建基于区块链的自适应访问控制框架。<br />4. 零知识证明保障访问过程中敏感信息不泄露；模糊逻辑动态评估数据敏感度、信任度和用户角色，实现上下文感知决策。<br />5. 区块链提供去中心化、不可篡改的架构，通过智能合约确保透明、可追溯和自动化的访问控制，提升系统安全性与用户信任。 <div>
arXiv:2601.10105v1 Announce Type: new 
Abstract: The rapid integration of IoT with edge computing has revolutionized various domains, particularly healthcare, by enabling real-time data sharing, remote monitoring, and decision-making. However, it introduces critical challenges, including data privacy breaches, security vulnerabilities, especially in environments dealing with sensitive information. Traditional access control mechanisms and centralized security systems do not address these issues, leaving IoT environments exposed to unauthorized access and data misuse. This research proposes Fuzzychain-edge, a novel Fuzzy logic-based adaptive Access control model for Blockchain in Edge Computing framework designed to overcome these limitations by incorporating Zero-Knowledge Proofs (ZKPs), fuzzy logic, and smart contracts. ZKPs secure sensitive data during access control processes by enabling verification without revealing confidential details, thereby ensuring user privacy. Fuzzy logic facilitates adaptive, context-aware decision-making for access control by dynamically evaluating parameters such as data sensitivity, trust levels, and user roles. Blockchain technology, with its decentralized and immutable architecture, ensures transparency, traceability, and accountability using smart contracts that automate access control processes. The proposed framework addresses key challenges by enhancing security, reducing the likelihood of unauthorized access, and providing a transparent audit trail of data transactions. Expected outcomes include improved data privacy, accuracy in access control, and increased user trust in IoT systems. This research contributes significantly to advancing privacy-preserving, secure, and traceable solutions in IoT environments, laying the groundwork for future innovations in decentralized technologies and their applications in critical domains such as healthcare and beyond.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>TopoDIM: One-shot Topology Generation of Diverse Interaction Modes for Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2601.10120</link>
<guid>https://arxiv.org/abs/2601.10120</guid>
<content:encoded><![CDATA[
<div> 通信拓扑优化、多智能体系统、去中心化执行、异构交互模式、令牌效率<br /><br />总结:<br />1. 文章提出TopoDIM框架，旨在优化基于大语言模型的多智能体系统中的通信拓扑。<br />2. 该框架采用一次性生成拓扑结构的方式，支持多样化的异构交互模式，避免了传统多轮对话带来的高延迟和计算开销。<br />3. TopoDIM设计为去中心化执行，提升系统适应性与隐私保护能力，使智能体能自主构建通信结构而无需迭代协调。<br />4. 实验表明，相比现有最先进方法，TopoDIM减少46.41%的总令牌消耗，同时平均任务性能提升1.50%。<br />5. 框架在组织异构智能体通信方面展现出强适应性，代码已公开。 <div>
arXiv:2601.10120v1 Announce Type: new 
Abstract: Optimizing communication topology in LLM-based multi-agent system is critical for enabling collective intelligence. Existing methods mainly rely on spatio-temporal interaction paradigms, where the sequential execution of multi-round dialogues incurs high latency and computation. Motivated by the recent insights that evaluation and debate mechanisms can improve problem-solving in multi-agent systems, we propose TopoDIM, a framework for one-shot Topology generation with Diverse Interaction Modes. Designed for decentralized execution to enhance adaptability and privacy, TopoDIM enables agents to autonomously construct heterogeneous communication without iterative coordination, achieving token efficiency and improved task performance. Experiments demonstrate that TopoDIM reduces total token consumption by 46.41% while improving average performance by 1.50% over state-of-the-art methods. Moreover, the framework exhibits strong adaptability in organizing communication among heterogeneous agents. Code is available at: https://anonymous.4open.science/r/TopoDIM-8D35/
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>PADER: Paillier-based Secure Decentralized Social Recommendation</title>
<link>https://arxiv.org/abs/2601.10212</link>
<guid>https://arxiv.org/abs/2601.10212</guid>
<content:encoded><![CDATA[
<div> 关键词：推荐系统、隐私保护、去中心化、Paillier加密、安全计算<br /><br />总结:<br />1. 针对集中式推荐系统带来的用户与商家隐私泄露问题，提出PADER——一种基于Paillier加密的去中心化社交推荐系统。<br />2. 系统将用户和商家作为去中心化网络中的节点，无需中心平台即可安全地完成模型训练与推理。<br />3. 将社交正则化（SoReg）模型转化为两方安全多项式求值问题，并设计高效的加法与乘法安全协议，支持任意算术电路的安全计算。<br />4. 提出适用于实数多项式运算的最优数据打包方案，显著提升计算效率。<br />5. 实验表明，该方法在处理数十万评分数据时仅需不到3小时完成一轮训练，单用户迭代耗时约1秒，具备实际应用可行性。 <div>
arXiv:2601.10212v1 Announce Type: new 
Abstract: The prevalence of recommendation systems also brings privacy concerns to both the users and the sellers, as centralized platforms collect as much data as possible from them. To keep the data private, we propose PADER: a Paillier-based secure decentralized social recommendation system. In this system, the users and the sellers are nodes in a decentralized network. The training and inference of the recommendation model are carried out securely in a decentralized manner, without the involvement of a centralized platform. To this end, we apply the Paillier cryptosystem to the SoReg (Social Regularization) model, which exploits both user's ratings and social relations. We view the SoReg model as a two-party secure polynomial evaluation problem and observe that the simple bipartite computation may result in poor efficiency. To improve efficiency, we design secure addition and multiplication protocols to support secure computation on any arithmetic circuit, along with an optimal data packing scheme that is suitable for the polynomial computations of real values. Experiment results show that our method only takes about one second to iterate through one user with hundreds of ratings, and training with ~500K ratings for one epoch only takes <3 hours, which shows that the method is practical in real applications. The code is available at https://github.com/GarminQ/PADER.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCRamble: Adaptive Decentralized Overlay Construction for Blockchain Networks</title>
<link>https://arxiv.org/abs/2601.10277</link>
<guid>https://arxiv.org/abs/2601.10277</guid>
<content:encoded><![CDATA[
<div> 区块链 吞吐量 网络延迟 区块传播 SCRamble<br /><br />总结:<br />1. 区块链在发展超过15年后，交易吞吐量仍受限，主要瓶颈是区块在网络中的传播延迟。<br />2. 当前区块链多采用随机连接的点对点网络，限制了每秒处理交易的数量。<br />3. 加快区块传播不仅能提升交易处理速度，还能降低分叉概率，增强系统安全性。<br />4. 本文提出SCRamble协议，一种去中心化的解决方案，显著缩短区块传播时间。<br />5. SCRamble通过两种启发式策略选择优质连接：一是基于邻居节点区块到达时间的评分机制，二是考虑网络延迟因素。 <div>
arXiv:2601.10277v1 Announce Type: new 
Abstract: Despite being under development for over 15 years, transaction throughput remains one of the key challenges confronting blockchains, which typically has a cap of a limited number of transactions per second. A fundamental factor limiting this metric is the network latency associated with the block propagation throughout of the underlying peer-to-peer network, typically formed through random connections. Accelerating the dissemination of blocks not only improves transaction rates, but also enhances system security by reducing the probability of forks. This paper introduces SCRamble: a decentralized protocol that significantly reduces block dissemination time in blockchain networks. SCRamble's effectiveness is attributed to its innovative link selection strategy, which integrates two heuristics: a scoring mechanism that assesses block arrival times from neighboring peers, and a second heuristic that takes network latency into account.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multipath Routing for Multi-Hop UAV Networks</title>
<link>https://arxiv.org/abs/2601.10299</link>
<guid>https://arxiv.org/abs/2601.10299</guid>
<content:encoded><![CDATA[
<div> 多跳无人机网络、流量自适应、多路径路由、Dec-POMDP、多智能体深度强化学习<br /><br />总结:<br />1. 针对传统多跳无人机网络因单路径路由导致局部拥塞和延迟增加的问题，提出一种流量自适应的多路径路由方法。<br />2. 该方法允许每个无人机将流量动态拆分并转发至多个下一跳邻居，以满足动态移动环境中多样化业务的时延需求。<br />3. 通过构建“按时包投递率最大化”问题来确定各跳的流量分配比例，并将其建模为去中心化部分可观测马尔可夫决策过程（Dec-POMDP）。<br />4. 设计了一种名为IPPO-DM的多智能体深度强化学习算法，结合独立近端策略优化（IPPO）与狄利克雷分布建模，确保流量分配比例在概率单纯形内有效可行。<br />5. 仿真实验表明，IPPO-DM在投递时延保障和丢包性能方面均优于现有基准方案。 <div>
arXiv:2601.10299v1 Announce Type: new 
Abstract: Multi-hop uncrewed aerial vehicle (UAV) networks are promising to extend the terrestrial network coverage. Existing multi-hop UAV networks employ a single routing path by selecting the next-hop forwarding node in a hop-by-hop manner, which leads to local congestion and increases traffic delays. In this paper, a novel traffic-adaptive multipath routing method is proposed for multi-hop UAV networks, which enables each UAV to dynamically split and forward traffic flows across multiple next-hop neighbors, thus meeting latency requirements of diverse traffic flows in dynamic mobile environments. An on-time packet delivery ratio maximization problem is formulated to determine the traffic splitting ratios at each hop. This sequential decision-making problem is modeled as a decentralized partially observable Markov decision process (Dec-POMDP). To solve this Dec-POMDP, a novel multi-agent deep reinforcement leaning (MADRL) algorithm, termed Independent Proximal Policy Optimization with Dirichlet Modeling (IPPO-DM), is developed. Specifically, the IPPO serves as the core optimization framework, where the Dirichlet distribution is leveraged to parameterize a continuous stochastic policy network on the probability simplex, inherently ensuring feasible traffic splitting ratios. Simulation results demonstrate that IPPO-DM outperforms benchmark schemes in terms of both delivery latency guarantee and packet loss performance.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDN-Driven Innovations in MANETs and IoT: A Path to Smarter Networks</title>
<link>https://arxiv.org/abs/2601.10544</link>
<guid>https://arxiv.org/abs/2601.10544</guid>
<content:encoded><![CDATA[
<div> 移动自组织网络（MANETs）、物联网（IoT）、软件定义网络（SDN）、路由优化、可扩展性<br /><br />总结:<br />1. 移动自组织网络（MANETs）与物联网（IoT）因去中心化和动态拓扑面临路由效率低、可扩展性差及安全漏洞等问题。<br />2. 引入软件定义网络（SDN）通过集中控制与可编程性，有效提升路由、资源管理与安全性。<br />3. 建立数学模型评估SDN对资本支出（CAPEX）、运营支出（OPEX）及性能指标的影响。<br />4. 实验结果表明，SDN增强的网络在大规模动态环境中显著降低延迟与丢包率，提高吞吐量与可扩展性。<br />5. 尽管引入一定计算开销，SDN仍大幅提升网络适应性与资源优化能力，为高密度、高流量场景提供可靠架构。 <div>
arXiv:2601.10544v1 Announce Type: new 
Abstract: Mobile Ad Hoc Networks (MANETs) and Internet of Things (IoT) networks operate in decentralized and dynamic environments, making them ideal for scenarios lacking traditional infrastructure. However, these networks face challenges such as inefficient routing, limited scalability, and security vulnerabilities due to their decentralized nature and resource constraints. This paper explores the integration of Software-Defined Networking (SDN) as a unified solution that leverages its centralized control and network programmability to improve routing, resource management, and security. A mathematical model evaluates the impact of SDN integration on Capital Expenditure (CAPEX), Operational Expenditure (OPEX), and performance metrics. Results demonstrate that SDN-enhanced MANETs and IoT networks offer superior scalability, reduced latency, increased throughput, and lower packet loss, especially in dynamic and large-scale environments. While SDN introduces computational overhead, it significantly enhances routing efficiency, resource optimization, and adaptability. The proposed framework provides a robust and scalable solution, enabling the development of network architectures that efficiently manage growing node densities, dynamic topologies, and high data traffic. This approach ensures resilience, making it well-suited to meet the performance and reliability demands of modern, large-scale applications.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Mobile Ad Hoc Networks (MANETs) with Software-Defined Networking (SDN): A Balanced Approach</title>
<link>https://arxiv.org/abs/2601.10556</link>
<guid>https://arxiv.org/abs/2601.10556</guid>
<content:encoded><![CDATA[
<div> 移动自组织网络（MANET）、软件定义网络（SDN）、集中控制、网络虚拟化、成本效率<br /><br />总结:  
1. 移动自组织网络（MANET）具有动态拓扑和节点移动性，管理复杂。  
2. 将软件定义网络（SDN）与MANET结合，可有效应对上述挑战。  
3. SDN的集中控制和网络虚拟化特性有助于提升MANET的可扩展性、成本效益和安全性。  
4. 论文构建了一个数学模型，用于分析资本支出（CAPEX）、运营支出（OPEX）及网络效率。  
5. 研究表明，SDN集成能优化MANET整体性能。 <div>
arXiv:2601.10556v1 Announce Type: new 
Abstract: Mobile Ad Hoc Networks (MANETs) are decentralized wireless networks, characterized by their dynamic topologies and node mobility. In the era of cutting-edge technologies, integrating Software-Defined Networking (SDN) with MANETs offers a promising solution to manage these challenges more efficiently. This paper presents a balanced discussion of MANETs and SDN, demonstrating how SDN principles, such as centralized control and network virtualization, can optimize MANET performance in terms of scalability, cost-efficiency, and security. A mathematical model is developed to analyze Capital Expenditures (CAPEX), Operational Expenditures (OPEX), and network efficiency.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Basis-Spline Assisted Coded Computing: Strategies and Error Bounds</title>
<link>https://arxiv.org/abs/2601.10616</link>
<guid>https://arxiv.org/abs/2601.10616</guid>
<content:encoded><![CDATA[
<div> 关键词：编码计算、非多项式函数、Berrut插值、B样条插值、分布式计算<br /><br />总结:<br />1. 编码计算是提升去中心化网络中分布式计算可靠性的关键技术，尤其用于缓解“拖后腿”节点（stragglers）的影响。<br />2. 针对非多项式函数的计算更具挑战性，因其难以通过有限采样点精确重构。<br />3. 现有最先进方法采用Berrut插值近似非多项式函数，但其全局支撑特性在大量拖后腿节点存在时导致精度下降。<br />4. 本文提出基于三次B样条插值的新框架，利用B样条的局部支撑与光滑性，提升重构的稳定性与准确性。<br />5. 作者系统地将B样条融入编码计算，并推导出关于服务器数量与拖后腿节点数的误差上界，实验证明该方法优于Berrut方法。 <div>
arXiv:2601.10616v1 Announce Type: new 
Abstract: Coded computing has become a key framework for reliable distributed computation over decentralized networks, effectively mitigating the impact of stragglers. Although there exists a wide range of coded computing methods to handle both polynomial and non-polynomial functions, computing methods for the latter class have received traction due its inherent challenges in reconstructing non-polynomial functions using a finite number of evaluations. Among them, the state-of-the-art method is Berrut Approximated coded computing, wherein Berrut interpolants, are used for approximating the non-polynomial function. However, since Berrut interpolants have global support characteristics, such methods are known to offer degraded accuracy when the number of stragglers is large. To address this challenge, we propose a coded computing framework based on cubic B-spline interpolation. In our approach, server-side function evaluations are reconstructed at the master node using B-splines, exploiting their local support and smoothness properties to enhance stability and accuracy. We provide a systematic methodology for integrating B-spline interpolation into coded computing and derive theoretical bounds on approximation error in terms of the number of servers and stragglers. Comparative analysis demonstrates that our framework significantly outperforms Berrut-based methods for various non-polynomial functions.
]]></content:encoded>
<pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Merged Bitcoin: Proof of Work Blockchains with Multiple Hash Types</title>
<link>https://arxiv.org/abs/2601.09090</link>
<guid>https://arxiv.org/abs/2601.09090</guid>
<content:encoded><![CDATA[
<div> 多哈希类型、工作量证明、安全区域、合并比特币、难度调整<br /><br />总结:<br />1. 文章研究了采用多种哈希类型的工作量证明区块链协议，指出其安全区域不能简单视为对所有哈希类型同时发动51%攻击的交集。<br />2. 提出了一种名为“合并比特币”（Merged Bitcoin）的新协议，允许区块通过多种不同哈希类型连接。<br />3. 在Δ有界延迟网络模型下，推导出该协议安全区域的闭式边界，并与仿真结果进行对比验证。<br />4. 证明该协议在单位哈希成本线性模型下能最大化攻击成本。<br />5. 引入一种新的难度调整机制，可部分缓解攻击者因算法进步、Grover量子攻击或硬件后门等在特定哈希类型上获得的算力不对称优势。 <div>
arXiv:2601.09090v1 Announce Type: new 
Abstract: Proof of work blockchain protocols using multiple hash types are considered. It is proven that the security region of such a protocol cannot be the AND of a 51\% attack on all the hash types. Nevertheless, a protocol called Merged Bitcoin is introduced, which is the Bitcoin protocol where links between blocks can be formed using multiple different hash types. Closed form bounds on its security region in the $\Delta$-bounded delay network model are proven, and these bounds are compared to simulation results. This protocol is proven to maximize cost of attack in the linear cost-per-hash model. A difficulty adjustment method is introduced, and it is argued that this can partly remedy asymmetric advantages an adversary may gain in hashing power for some hash types, including from algorithmic advances, quantum attacks like Grover's algorithm, or hardware backdoor attacks.
]]></content:encoded>
<pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing View Change for Byzantine Fault Tolerance in Parallel Consensus</title>
<link>https://arxiv.org/abs/2601.09184</link>
<guid>https://arxiv.org/abs/2601.09184</guid>
<content:encoded><![CDATA[
<div> 关键词：并行BFT、视图切换、领导者选择、混合整数规划、性能优化

<br /><br />总结:  
1. 并行拜占庭容错（BFT）协议是解决许可链共识可扩展性问题的有前景方案。  
2. 现有协议在领导者节点失效时采用被动视图切换机制，常因盲目轮换选中不可用或低效节点，造成性能瓶颈。  
3. 本文提出基于混合整数规划的视图切换优化（VCO）模型，综合考虑通信延迟与故障场景，优化多委员会中的领导者选择与跟随者重分配。  
4. 采用分解方法与改进的Benders割平面法高效求解VCO模型，并设计迭代式备用领导者选择算法。  
5. 在Azure云环境实验表明，VCO驱动的并行BFT在正常和故障条件下均优于现有方法，且随网络规模扩大效果更显著。 <div>
arXiv:2601.09184v1 Announce Type: new 
Abstract: The parallel Byzantine Fault Tolerant (BFT) protocol is viewed as a promising solution to address the consensus scalability issue of the permissioned blockchain. One of the main challenges in parallel BFT is the view change process that happens when the leader node fails, which can lead to performance bottlenecks. Existing parallel BFT protocols typically rely on passive view change mechanisms with blind leader rotation. Such approaches frequently select unavailable or slow nodes as leaders, resulting in degraded performance. To address these challenges, we propose a View Change Optimization (VCO) model based on mixed integer programming that optimizes leader selection and follower reassignment across parallel committees by considering communication delays and failure scenarios. We applied a decomposition method with efficient subproblems and improved benders cuts to solve the VCO model. Leveraging the results of improved decomposition solution method, we propose an efficient iterative backup leader selection algorithm as views proceed. By performing experiments in Microsoft Azure cloud environments, we demonstrate that the VCO-driven parallel BFT outperforms existing configuration methods under both normal operation and faulty condition. The results show that the VCO model is effective as network size increases, making it a suitable solution for high-performance parallel BFT systems.
]]></content:encoded>
<pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SiliconHealth: A Complete Low-Cost Blockchain Healthcare Infrastructure for Resource-Constrained Regions Using Repurposed Bitcoin Mining ASICs</title>
<link>https://arxiv.org/abs/2601.09557</link>
<guid>https://arxiv.org/abs/2601.09557</guid>
<content:encoded><![CDATA[
<div> 区块链、医疗记录、ASIC矿机、资源受限地区、Deterministic Hardware Fingerprinting<br /><br />总结:<br />1. 提出名为SiliconHealth的区块链医疗基础设施，专为资源受限地区（如撒哈拉以南非洲）设计。<br />2. 利用废弃比特币挖矿ASIC设备（如Antminer S19 Pro、S9及Lucky Miner LV06）构建四级分层网络，实现低成本、低功耗的电子健康记录系统。<br />3. 引入Deterministic Hardware Fingerprinting（DHF）机制，将SHA-256 ASIC转为加密证明生成器，验证率达100%。<br />4. 系统集成Reed-Solomon LSB水印技术（可容忍30–40%图像损坏）、语义RAG智能查询及离线同步协议，适应间歇性网络环境。<br />5. 经济分析显示，相比GPU方案成本降低96%，单个农村诊所五年部署总成本仅847美元，有望惠及超6亿缺乏基础医疗信息系统的人口。 <div>
arXiv:2601.09557v1 Announce Type: new 
Abstract: This paper presents SiliconHealth, a comprehensive blockchain-based healthcare infrastructure designed for resource-constrained regions, particularly sub-Saharan Africa. We demonstrate that obsolete Bitcoin mining Application-Specific Integrated Circuits (ASICs) can be repurposed to create a secure, low-cost, and energy-efficient medical records system. The proposed architecture employs a four-tier hierarchical network: regional hospitals using Antminer S19 Pro (90+ TH/s), urban health centers with Antminer S9 (14 TH/s), rural clinics equipped with Lucky Miner LV06 (500 GH/s, 13W), and mobile health points with portable ASIC devices. We introduce the Deterministic Hardware Fingerprinting (DHF) paradigm, which repurposes SHA-256 mining ASICs as cryptographic proof generators, achieving 100% verification rate across 23 test proofs during 300-second validation sessions. The system incorporates Reed-Solomon LSB watermarking for medical image authentication with 30-40% damage tolerance, semantic Retrieval-Augmented Generation (RAG) for intelligent medical record queries, and offline synchronization protocols for intermittent connectivity. Economic analysis demonstrates 96% cost reduction compared to GPU-based alternatives, with total deployment cost of $847 per rural clinic including 5-year solar power infrastructure. Validation experiments on Lucky Miner LV06 (BM1366 chip, 5nm) achieve 2.93 MH/W efficiency and confirm hardware universality. This work establishes a practical framework for deploying verifiable, tamper-proof electronic health records in regions where traditional healthcare IT infrastructure is economically unfeasible, potentially benefiting over 600 million people lacking access to basic health information systems.
]]></content:encoded>
<pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Consensus-Based Distributed Beamforming for Wideband Cell-free Multi-RIS MISO Systems</title>
<link>https://arxiv.org/abs/2601.08946</link>
<guid>https://arxiv.org/abs/2601.08946</guid>
<content:encoded><![CDATA[
<div> 关键词：无小区网络、可重构智能表面、去中心化波束成形、宽频带系统、信道状态信息不完美<br /><br />总结:<br />1. 本文研究无小区网络与可重构智能表面（RIS）结合的宽频带多输入单输出（MISO）系统。<br />2. 提出一种去中心化的协同主动与被动波束成形方案，以降低传统集中式方案对中央处理单元的依赖及协作开销。<br />3. 考虑实际RIS单元响应的频率选择性及信道状态信息不完美的情况。<br />4. 设计基于共识更新的分布式优化方法，用于优化RIS相位配置。<br />5. 仿真结果表明，所提方案在性能上优于基于洛伦兹型宽频带建模的集中式方法。 <div>
arXiv:2601.08946v1 Announce Type: cross 
Abstract: The cell-free networking paradigm constitutes a revolutionary architecture for future generations of wireless networks, which has been recently considered in synergy with Reconfigurable Intelligent Surfaces (RISs), a promising physical-layer technology for signal propagation programmability. In this paper, we focus on wideband cell-free multi-RIS-empowered Multiple-Input Single-Output (MISO) systems and present a decentralized cooperative active and passive beamforming scheme, aiming to provide an efficient alternative towards the cooperation overhead of available centralized schemes depending on central processing unit. Considering imperfect channel information availability and realistic frequency selectivity behavior of each RIS's element response, we devise a distributed optimization approach based on consensus updates for the RISs' phase configurations. Our simulation results showcase that the proposed distributed design is superior to centralized schemes that are based on various Lorentzian-type wideband modeling approaches for the RISs.
]]></content:encoded>
<pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Game of Coding: Sybil Resistant Decentralized Machine Learning with Minimal Trust Assumption</title>
<link>https://arxiv.org/abs/2410.05540</link>
<guid>https://arxiv.org/abs/2410.05540</guid>
<content:encoded><![CDATA[
<div> 编码理论、博弈框架、去中心化系统、激励机制、Sybil攻击抵抗<br /><br />总结:<br />1. 传统编码理论依赖诚实节点数量超过恶意节点，但在去中心化环境中信任稀缺，难以满足该假设。<br />2. 为此提出“编码博弈”框架，通过激励机制使参与者（包括理性对手）在系统存活前提下追求自身效用最大化。<br />3. 本文将原仅限于两节点的框架推广至N≥2节点场景。<br />4. 证明在均衡状态下，增加恶意节点不会提升其效用，体现框架对Sybil攻击的抵抗力。<br />5. 发现增加诚实节点未必总能提升数据收集者（DC）效用，并提出算法识别与缓解此反直觉现象；同时给出DC与对手的最优策略，确保系统在均衡下具有更强活性。 <div>
arXiv:2410.05540v3 Announce Type: replace 
Abstract: Coding theory plays a crucial role in ensuring data integrity and reliability across various domains, from communication to computation and storage systems. However, its reliance on trust assumptions for data recovery, which requires the number of honest nodes to exceed adversarial nodes by a certain margin, poses significant challenges, particularly in emerging decentralized systems where trust is a scarce resource. To address this, the game of coding framework was introduced, offering insights into strategies for data recovery within incentive-oriented environments. In such environments, participant nodes are rewarded as long as the system remains functional (live). This incentivizes adversaries to maximize their rewards (utility) by ensuring that the decoder, as the data collector (DC), successfully recovers the data, preferably with a high estimation error. This rational behavior is leveraged in a game-theoretic framework, where the equilibrium leads to a robust and resilient system, referred to as the game of coding. The focus of the earliest version of the game of coding was limited to scenarios involving only two nodes. In this paper, we generalize the game of coding framework to scenarios with $N \ge 2$ nodes, exploring critical aspects of system behavior. Specifically, we (i) demonstrate that the adversary's utility at equilibrium is non-increasing with additional adversarial nodes, ensuring no gain for the adversary and no pain for the DC, thus establishing the game of coding framework's Sybil resistance; (ii) show that increasing the number of honest nodes does not always enhance the DC's utility, providing examples and proposing an algorithm to identify and mitigate this counterintuitive effect; and (iii) outline the optimal strategies for both the DC and the adversary, demonstrating that the system achieves enhanced liveness at equilibrium.
]]></content:encoded>
<pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smart Contract Vulnerabilities, Tools, and Benchmarks: an Updated Systematic Literature Review</title>
<link>https://arxiv.org/abs/2412.01719</link>
<guid>https://arxiv.org/abs/2412.01719</guid>
<content:encoded><![CDATA[
<div> 智能合约、以太坊、漏洞检测、自动化工具、基准评估<br /><br />总结:<br />1. 文章系统综述了以太坊智能合约的安全漏洞问题，因其不可篡改和透明特性易受攻击。<br />2. 作者从五大数字图书馆和主要软件工程会议中筛选出222篇高质量研究，涵盖3,380篇初始文献。<br />3. 构建了一个包含192种漏洞的分层分类体系，划分为13个类别。<br />4. 汇总了219种自动化检测工具，并分析其功能、方法及代码转换技术，同时建立工具与漏洞分类的映射关系。<br />5. 收集了133个用于评估工具性能的基准数据集，并对未来研究方向提出建议。 <div>
arXiv:2412.01719v3 Announce Type: replace 
Abstract: Smart contracts are self-executing programs on blockchain platforms like Ethereum, which have revolutionized decentralized finance by enabling trustless transactions and the operation of decentralized applications. Despite their potential, the security of smart contracts remains a critical concern due to their immutability and transparency, which expose them to malicious actors. Numerous solutions for vulnerability detection have been proposed, but it is still unclear which one is the most effective. This paper presents a systematic literature review that explores vulnerabilities in Ethereum smart contracts, focusing on automated detection tools and benchmark evaluation. We reviewed 3,380 studies from five digital libraries and five major software engineering conferences, applying a structured selection process that resulted in 222 high-quality studies. The key results include a hierarchical taxonomy of 192 vulnerabilities grouped into 13 categories, a comprehensive list of 219 detection tools with corresponding functionalities, methods, and code transformation techniques, a mapping between our taxonomy and the list of tools, and a collection of 133 benchmarks used for tool evaluation. We conclude with a discussion about the insights into the current state of Ethereum smart contract security and directions for future research.
]]></content:encoded>
<pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>CascadeInfer: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling</title>
<link>https://arxiv.org/abs/2512.19179</link>
<guid>https://arxiv.org/abs/2512.19179</guid>
<content:encoded><![CDATA[
<div> 关键词：GPU计算效率、请求长度异构性、CascadeInfer、动态重调度、多实例负载均衡<br /><br />总结:  
1. 当前大语言模型（LLM）推理调度器忽视了注意力机制对批处理中请求长度差异的敏感性，导致GPU利用率低和延迟增加。  
2. 随着上下文窗口扩展至128K以上，该问题已成为系统性能的主要瓶颈。  
3. 提出CascadeInfer运行时系统，通过在多个LLM实例间动态重调度请求，减少单个实例内的长度异构性。  
4. 该系统将实例划分为专精不同长度范围的组，形成自然流水线，并采用动态规划算法优化服务质量（QoE）。  
5. 实验表明，相比现有先进多实例调度系统，CascadeInfer最高可降低67%端到端延迟、69%尾部延迟，并提升2.89倍吞吐量。 <div>
arXiv:2512.19179v2 Announce Type: replace 
Abstract: Efficiently harnessing GPU compute is critical to improving user experience and reducing operational costs in large language model (LLM) services. However, current inference engine schedulers overlook the attention backend's sensitivity to request-length heterogeneity within a batch. As state-of-the-art models now support context windows exceeding 128K tokens, this once-tolerable inefficiency has escalated into a primary system bottleneck, causing severe performance degradation through GPU underutilization and increased latency. We present CascadeInfer, a runtime system that dynamically reschedules requests across multiple instances serving the same LLM to mitigate per-instance length heterogeneity. CascadeInfer partitions these instances into length-specialized groups, each handling requests within a designated length range, naturally forming a pipeline as requests flow through them. CascadeInfer devises a dynamic programming algorithm to efficiently find the stage partition with the best QoE, employs runtime range refinement together with decentralized load (re)balance both across and within groups, achieving a balanced and efficient multi-instance service. Our evaluation shows that, under the same configuration, CascadeInfer reduces end-to-end latency by up to 67% and tail latency by up to 69%, while improving overall system throughput by up to 2.89 times compared to the state-of-the-art multi-instance scheduling systems.
]]></content:encoded>
<pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Firmware Integrity Verification for Cyber-Physical Systems Using Ethereum Blockchain</title>
<link>https://arxiv.org/abs/2601.08091</link>
<guid>https://arxiv.org/abs/2601.08091</guid>
<content:encoded><![CDATA[
<div> 区块链、固件完整性、智能合约、去中心化验证、网络安全<br /><br />总结:  
1. 针对传统固件完整性验证机制易受内部威胁和单点故障影响的问题，提出一种基于以太坊区块链的去中心化验证框架。  
2. 该系统将固件的SHA-256哈希值存储于部署在Sepolia测试网的智能合约中，利用Web3和Infura实现链上交互。  
3. 开发了基于Python的客户端工具，可实时计算固件哈希并完成注册与验证。  
4. 原型实验证明方案具备高可靠性、低Gas费用，并适用于真实CPS环境。  
5. 提出通过Layer-2 Rollup和IPFS提升可扩展性，并探讨与安全启动、TPM认证及零信任架构的集成路径。 <div>
arXiv:2601.08091v1 Announce Type: new 
Abstract: Firmware integrity is a foundational requirement for securing Cyber-Physical Systems (CPS), where malicious or compromised firmware can result in persistent backdoors, unauthorized control, or catastrophic system failures. Traditional verification mechanisms such as secure boot, digital signatures, and centralized hash databases are increasingly inadequate due to risks from insider threats and single points of failure. In this paper, we propose a decentralized firmware integrity verification framework built on the Ethereum blockchain, offering tamperproof, transparent, and trustless validation. Our system stores SHA-256 hashes of firmware binaries within smart contracts deployed on the Ethereum Sepolia testnet, using Web3 and Infura for seamless on-chain interaction. A Python-based client tool computes firmware hashes and communicates with the blockchain to register and verify firmware authenticity in realtime. We implement and evaluate a fully functional prototype using real firmware samples, demonstrating successful contract deployment, hash registration, and integrity verification through live blockchain transactions. Experimental results confirm the reliability and low cost (in gas fees) of our approach, highlighting its practicality and scalability for real-world CPS applications. To enhance scalability and performance, we discuss extensions using Layer-2 rollups and off-chain storage via the InterPlanetary File System (IPFS). We also outline integration pathways with secure boot mechanisms, Trusted Platform Module (TPM)based attestation, and zero-trust architectures. This work contributes a practical and extensible model for blockchain-based firmware verification, significantly strengthening the defense against firmware tampering and supply chain attacks in critical CPS environments.
]]></content:encoded>
<pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Safe Heterogeneous Multi-Agent RL with Communication Regularization for Coordinated Target Acquisition</title>
<link>https://arxiv.org/abs/2601.08327</link>
<guid>https://arxiv.org/abs/2601.08327</guid>
<content:encoded><![CDATA[
<div> 关键词：多智能体强化学习、图注意力网络、部分可观测环境、通信约束、安全过滤器<br /><br />总结:  
1. 提出了一种去中心化的多智能体强化学习框架，适用于结构异构的智能体团队在部分可观测、通信受限和动态交互环境中协同发现并获取随机目标。  
2. 每个智能体采用多智能体近端策略优化算法训练，并利用图注意力网络编码器融合自身感知数据与邻近智能体的通信嵌入，实现上下文感知决策。  
3. 框架整合了基于图的通信机制与轨迹感知的安全过滤器，保障任务执行中的安全性。  
4. 设计了结构化奖励函数，旨在促进目标发现与获取、避障以及通过信息正交性降低通信向量相关性。  
5. 通过消融实验验证了奖励函数的有效性，仿真结果表明该方法能实现安全稳定的任务执行。 <div>
arXiv:2601.08327v1 Announce Type: new 
Abstract: This paper introduces a decentralized multi-agent reinforcement learning framework enabling structurally heterogeneous teams of agents to jointly discover and acquire randomly located targets in environments characterized by partial observability, communication constraints, and dynamic interactions. Each agent's policy is trained with the Multi-Agent Proximal Policy Optimization algorithm and employs a Graph Attention Network encoder that integrates simulated range-sensing data with communication embeddings exchanged among neighboring agents, enabling context-aware decision-making from both local sensing and relational information. In particular, this work introduces a unified framework that integrates graph-based communication and trajectory-aware safety through safety filters. The architecture is supported by a structured reward formulation designed to encourage effective target discovery and acquisition, collision avoidance, and de-correlation between the agents' communication vectors by promoting informational orthogonality. The effectiveness of the proposed reward function is demonstrated through a comprehensive ablation study. Moreover, simulation results demonstrate safe and stable task execution, confirming the framework's effectiveness.
]]></content:encoded>
<pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blockchain-Enabled Renewable Energy Certificate Trading: A Secure and Privacy-Preserving Approach</title>
<link>https://arxiv.org/abs/2601.08339</link>
<guid>https://arxiv.org/abs/2601.08339</guid>
<content:encoded><![CDATA[
<div> 可再生能源证书、区块链、有向无环图、隐私保护、交易效率<br /><br />总结:  
1. 21世纪转向可再生能源势在必行，既应对环境问题，又促进经济增长与创新。  
2. 可再生能源证书（RECs）用于追踪可再生能源的生产与消费，并确保合规性与透明度。  
3. 现有REC体系存在两大问题：全球采纳不一致及消费者隐私保护不足。  
4. 本研究采用基于有向无环图（DAG）的区块链系统进行REC交易，提升隐私保护。  
5. 实验结果显示，相比权益证明机制，该方案降低交易时间41%、能耗65%。 <div>
arXiv:2601.08339v1 Announce Type: new 
Abstract: In the 21st century, transitioning to renewable energy sources is imperative, with fossil fuel reserves depleting rapidly and recognizing critical environmental issues such as climate change, air pollution, water pollution, and habitat destruction. Embracing renewable energy is not only an environmental necessity but also a strategic move with multiple benefits. By shifting to renewable energy sources and supporting their production through the acquisition of renewable energy certificates, we foster innovation and drive economic growth in the renewable energy sector. This, in turn, reduces greenhouse gas emissions, aligning with global efforts to mitigate climate change. Additionally, renewable energy certificates ensure compliance with regulations that mandate the use of renewable energy, enhancing legal adherence while promoting transparency and trust in energy sourcing. To monitor the uptake of renewable energy, governments have implemented Renewable Energy Certificates (RECs) as a tracking mechanism for the production and consumption of renewable energy. However, there are two main challenges to the existing REC schema: 1) The RECs have not been globally adopted due to inconsistent design; 2) The consumer privacy has not been well incorporated in the design of blockchain. In this study, we investigate the trading of RECs between suppliers and consumers using the directed acyclic graph (DAG) blockchain system and introduce a trading schema to help protect consumer information. Our results demonstrate lower transaction time by 41\% and energy consumption by 65\% compared to proof-of-stake.
]]></content:encoded>
<pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>A decentralized academic certificate issuance system using smart contracts on the tron network</title>
<link>https://arxiv.org/abs/2601.08513</link>
<guid>https://arxiv.org/abs/2601.08513</guid>
<content:encoded><![CDATA[
<div> 区块链、学术证书、去中心化、智能合约、IPFS<br /><br />总结:<br />1. 该论文设计并实现了一个基于区块链的去中心化学术证书颁发与验证系统，以解决传统模式易伪造、依赖中心化机构及验证效率低的问题。<br />2. 系统构建于TRON区块链之上，采用Solidity编写智能合约，结合去中心化网页应用（dApp）和IPFS存储证书元数据。<br />3. 研究通过功能、安全、性能和可用性评估验证系统有效性，结果表明其能正确支持证书颁发与公开验证，并具备访问控制和抗滥用能力。<br />4. 性能测试显示确认延迟低、交易成本可忽略，适合大规模学术场景应用。<br />5. 用户可用性评估（SUS评分76.67）表明系统具有良好的用户接受度，验证了该方案的技术可行性和实际应用价值。 <div>
arXiv:2601.08513v1 Announce Type: new 
Abstract: This paper presents the design, implementation, and evaluation of a decentralized system for issuing and verifying academic certificates based on blockchain technology. The proposed solution addresses common limitations of traditional certification models, such as susceptibility to forgery, reliance on centralized infrastructures, and inefficient verification processes. The system is built on the TRON blockchain and integrates smart contracts written in Solidity, a decentralized web application (dApp) for user interaction, and the InterPlanetary File System (IPFS) for decentralized storage of certificate metadata. The methodology comprised architectural design, smart contract development, and the implementation of a web-based interface, followed by functional, security, performance, and usability evaluations. Experimental results show that the system correctly supports certificate issuance and public verification, enforces access control, and resists common misuse scenarios. Performance analysis indicates low confirmation latency and negligible transaction costs, making the solution suitable for large-scale academic environments. Additionally, usability assessment using the System Usability Scale (SUS) resulted in a score of 76.67, indicating good user acceptance. Overall, the results demonstrate the technical feasibility and practical viability of the proposed approach, highlighting the TRON blockchain as an effective and cost-efficient infrastructure for decentralized academic certification systems.
]]></content:encoded>
<pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Requesting in Decentralized Edge Networks via Non-Stationary Bandits</title>
<link>https://arxiv.org/abs/2601.08760</link>
<guid>https://arxiv.org/abs/2601.08760</guid>
<content:encoded><![CDATA[
<div> 边缘网络 信息新鲜度 多臂老虎机 自适应重置 去中心化协作<br /><br />总结:<br />1. 研究聚焦于边缘网络中多个客户端通过接入节点（AN）请求内容的去中心化协作问题，目标是优化时效性客户端的信息新鲜度。<br />2. 客户端无法观测AN状态或其他客户端行为，其奖励定义为因选择AN而带来的信息年龄降低。<br />3. 该问题被建模为非平稳多臂老虎机，其奖励过程具有历史依赖性、跨客户端耦合性，并包含突变与渐变。<br />4. 提出“带自适应重置的Ageing Bandit”算法，结合自适应窗口与周期监测以追踪动态变化的奖励分布。<br />5. 理论分析证明该算法接近最优性能，仿真实验验证了其有效性。 <div>
arXiv:2601.08760v1 Announce Type: new 
Abstract: We study a decentralized collaborative requesting problem that aims to optimize the information freshness of time-sensitive clients in edge networks consisting of multiple clients, access nodes (ANs), and servers. Clients request content through ANs acting as gateways, without observing AN states or the actions of other clients. We define the reward as the age of information reduction resulting from a client's selection of an AN, and formulate the problem as a non-stationary multi-armed bandit. In this decentralized and partially observable setting, the resulting reward process is history-dependent and coupled across clients, and exhibits both abrupt and gradual changes in expected rewards, rendering classical bandit-based approaches ineffective. To address these challenges, we propose the AGING BANDIT WITH ADAPTIVE RESET algorithm, which combines adaptive windowing with periodic monitoring to track evolving reward distributions. We establish theoretical performance guarantees showing that the proposed algorithm achieves near-optimal performance, and we validate the theoretical results through simulations.
]]></content:encoded>
<pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Online Convex Optimization with Unknown Feedback Delays</title>
<link>https://arxiv.org/abs/2601.07901</link>
<guid>https://arxiv.org/abs/2601.07901</guid>
<content:encoded><![CDATA[
<div> 关键词：去中心化在线凸优化、时变延迟、自适应学习率、遗憾界、强凸性

<br /><br />总结:<br />1. 研究聚焦于存在未知且随时间与智能体变化的反馈延迟的去中心化在线凸优化（D-OCO）问题。<br />2. 提出一种新算法，无需预先知道总延迟，通过基于 gossip 的通信协议实现本地延迟估计。<br />3. 引入自适应学习率机制，在一般凸设定下获得改进的遗憾界：O(N√d_tot + N√T/(1−σ²)^{1/4})。<br />4. 将框架扩展至强凸情形，得到更紧的遗憾界：O(Nδ_max ln T / α)，其中α为强凸参数，δ_max为平均最大缺失观测数。<br />5. 理论分析表明所获上下界在对数因子内是紧致的，实验验证了算法优于现有基准方法。 <div>
arXiv:2601.07901v1 Announce Type: cross 
Abstract: Decentralized online convex optimization (D-OCO), where multiple agents within a network collaboratively learn optimal decisions in real-time, arises naturally in applications such as federated learning, sensor networks, and multi-agent control.  In this paper, we study D-OCO under unknown, time-and agent-varying feedback delays. While recent work has addressed this problem (Nguyen et al., 2024), existing algorithms assume prior knowledge of the total delay over agents and still suffer from suboptimal dependence on both the delay and network parameters. To overcome these limitations, we propose a novel algorithm that achieves an improved regret bound of O N $\sqrt$ d tot + N $\sqrt$ T  (1-$\sigma$2) 1/4 , where T is the total horizon, d tot denotes the average total delay across agents, N is the number of agents, and 1 -$\sigma$ 2 is the spectral gap of the network. Our approach builds upon recent advances in D-OCO (Wan et al., 2024a), but crucially incorporates an adaptive learning rate mechanism via a decentralized communication protocol. This enables each agent to estimate delays locally using a gossip-based strategy without the prior knowledge of the total delay. We further extend our framework to the strongly convex setting and derive a sharper regret bound of O N $\delta$max ln T $\alpha$  , where $\alpha$ is the strong convexity parameter and $\delta$ max is the maximum number of missing observations averaged over agents. We also show that our upper bounds for both settings are tight up to logarithmic factors. Experimental results validate the effectiveness of our approach, showing improvements over existing benchmark algorithms.
]]></content:encoded>
<pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>ROSS: RObust decentralized Stochastic learning based on Shapley values</title>
<link>https://arxiv.org/abs/2411.00365</link>
<guid>https://arxiv.org/abs/2411.00365</guid>
<content:encoded><![CDATA[
<div> 关键词：去中心化学习、数据异构性、Shapley值、ROSS算法、鲁棒性

<br /><br />总结:  
1. 针对去中心化学习中因数据分布异构（如非独立同分布、噪声或投毒数据）带来的挑战，提出了一种新算法ROSS。  
2. ROSS是一种基于Shapley值的鲁棒去中心化随机学习算法，利用邻居节点的交叉梯度信息更新本地模型。  
3. 该算法创新性地根据Shapley值衡量各梯度贡献并进行加权，采用类动量方式更新模型。  
4. 理论分析证明了ROSS具有线性收敛加速特性。  
5. 在多个公开数据集上的实验表明，ROSS在收敛速度和预测准确率方面均优于现有最先进方法。 <div>
arXiv:2411.00365v2 Announce Type: replace 
Abstract: In the paradigm of decentralized learning, a group of agents collaborate to learn a global model using a distributed dataset without a central server; nevertheless, it is severely challenged by the heterogeneity of the data distribution across the agents. For example, the data may be distributed non-independently and identically, and even be noised or poisoned. To address these data challenges, we propose ROSS, a novel robust decentralized stochastic learning algorithm based on Shapley values, in this paper. Specifically, in each round, each agent aggregates the cross-gradient information from its neighbors, i.e., the derivatives of its local model with respect to the datasets of its neighbors, to update its local model in a momentum like manner, while we innovate in weighting the derivatives according to their contributions measured by Shapley values. We perform solid theoretical analysis to reveal the linear convergence speedup of our ROSS algorithm. We also verify the efficacy of our algorithm through extensive experiments on public datasets. Our results demonstrate that, in face of the above variety of data challenges, our ROSS algorithm has significant advantages over existing state-of-the-art proposals in terms of both convergence and prediction accuracy.
]]></content:encoded>
<pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bribers, Bribers on The Chain, Is Resisting All in Vain? Trustless Consensus Manipulation Through Bribing Contracts</title>
<link>https://arxiv.org/abs/2509.17185</link>
<guid>https://arxiv.org/abs/2509.17185</guid>
<content:encoded><![CDATA[
<div> 关键词：加密货币、激励兼容性、贿赂攻击、以太坊验证者、智能合约<br /><br />总结:  
1. 加密货币的长期成功依赖于对验证者的激励兼容机制。  
2. 通过智能合约实现的无信任贿赂攻击威胁该机制的安全性。  
3. 研究提出并实现了三种针对以太坊验证者的新颖高效贿赂合约。  
4. 第一种合约通过购买投票制造区块链分叉；第二种诱使验证者主动退出共识，提升攻击者相对质押权重；第三种构建无信任贿赂市场，拍卖对RANDAO随机信标的操控权。  
5. 研究还对其中一种贿赂市场进行了初步博弈论分析。 <div>
arXiv:2509.17185v2 Announce Type: replace 
Abstract: The long-term success of cryptocurrencies largely depends on the incentive compatibility provided to the validators. Bribery attacks, facilitated trustlessly via smart contracts, threaten this foundation. This work introduces, implements, and evaluates three novel and efficient bribery contracts targeting Ethereum validators. The first bribery contract enables a briber to fork the blockchain by buying votes on their proposed blocks. The second contract incentivizes validators to voluntarily exit the consensus protocol, thus increasing the adversary's relative staking power. The third contract builds a trustless bribery market that enables the briber to auction off their manipulative power over the RANDAO, Ethereum's distributed randomness beacon. Finally, we provide an initial game-theoretical analysis of one of the described bribery markets.
]]></content:encoded>
<pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoK: Speedy Secure Finality</title>
<link>https://arxiv.org/abs/2512.20715</link>
<guid>https://arxiv.org/abs/2512.20715</guid>
<content:encoded><![CDATA[
<div> 关键词：以太坊、快速最终性、Gasper协议、重组攻击、3槽最终性（3SF）<br /><br />总结:  
1. 以太坊当前的Gasper协议虽保障安全与动态可用性，但交易执行到不可逆最终性存在约15分钟延迟。  
2. 此延迟易引发事前重组攻击、助长MEV提取，并降低经济结算效率。  
3. 为解决该问题，学界提出“快速安全最终性”（SSF）研究方向，在不削弱安全性的前提下缩短确认延迟。  
4. 文章梳理了SSF的核心理论基础，如重组弹性与广义休眠模型，并回顾从Goldfish到RLMD-GHOST的发展脉络。  
5. 最后，文章分析单槽最终性在大规模验证者环境下的通信瓶颈，并介绍兼顾速度与工程可行性的3槽最终性（3SF）协议。 <div>
arXiv:2512.20715v2 Announce Type: replace 
Abstract: While Ethereum has successfully achieved dynamic availability together with safety, a fundamental delay remains between transaction execution and immutable finality. In Ethereum's current Gasper protocol, this latency is on the order of 15 minutes, exposing the network to ex ante reorganization attacks, enabling MEV extraction, and limiting the efficiency of economic settlement. These limitations have motivated a growing body of work on Speedy Secure Finality (SSF), which aims to minimize confirmation latency without weakening formal security guarantees.
  This paper surveys the state of the art in fast finality protocol design. We introduce the core theoretical primitives underlying this space, including reorganization resilience and the generalized sleepy model, and trace their development from Goldfish to RLMD-GHOST. We then analyze the communication and aggregation bottlenecks faced by single-slot finality protocols in large validator settings. Finally, we survey the 3-slot finality (3SF) protocol as a practical synthesis that balances fast finality with the engineering constraints of the Ethereum network.
]]></content:encoded>
<pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interactive Distillation for Cooperative Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2601.05407</link>
<guid>https://arxiv.org/abs/2601.05407</guid>
<content:encoded><![CDATA[
<div> 知识蒸馏、多智能体强化学习、分层强化学习、伪离策略强化学习、观察空间不匹配<br /><br />总结:  
1. 知识蒸馏（KD）在多智能体强化学习（MARL）中面临三大瓶颈：难以合成高性能教学策略、教师在分布外（OOD）状态推理困难，以及师生观察空间不匹配。  
2. 为此提出HINT框架，结合分层强化学习构建可扩展的高性能中央教师。  
3. 引入伪离策略强化学习机制，利用师生双方经验更新教师策略，提升OOD适应能力。  
4. 采用基于性能的过滤机制，仅保留对任务结果有贡献的指导信息，缓解观察空间差异问题。  
5. 在FireCommander和MARINE等复杂协作任务中，HINT相较基线方法成功率达60%至165%的显著提升。 <div>
arXiv:2601.05407v1 Announce Type: new 
Abstract: Knowledge distillation (KD) has the potential to accelerate MARL by employing a centralized teacher for decentralized students but faces key bottlenecks. Specifically, there are (1) challenges in synthesizing high-performing teaching policies in complex domains, (2) difficulties when teachers must reason in out-of-distribution (OOD) states, and (3) mismatches between the decentralized students' and the centralized teacher's observation spaces. To address these limitations, we propose HINT (Hierarchical INteractive Teacher-based transfer), a novel KD framework for MARL in a centralized training, decentralized execution setup. By leveraging hierarchical RL, HINT provides a scalable, high-performing teacher. Our key innovation, pseudo off-policy RL, enables the teacher policy to be updated using both teacher and student experience, thereby improving OOD adaptation. HINT also applies performance-based filtering to retain only outcome-relevant guidance, reducing observation mismatches. We evaluate HINT on challenging cooperative domains (e.g., FireCommander for resource allocation, MARINE for tactical combat). Across these benchmarks, HINT outperforms baselines, achieving improvements of 60% to 165% in success rate.
]]></content:encoded>
<pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Experimental Demonstration of a Decentralized Electromagnetic Formation Flying Control Using Alternating Magnetic Field Forces</title>
<link>https://arxiv.org/abs/2601.05408</link>
<guid>https://arxiv.org/abs/2601.05408</guid>
<content:encoded><![CDATA[
<div> 电磁编队飞行 交变磁场力 幅度调制正弦波 去耦控制 三卫星实验<br /><br />总结:  
1. 电磁编队飞行（EMFF）因卫星间电磁场耦合复杂而具有挑战性。  
2. 文章提出使用交变磁场力（AMFF）对卫星间的电磁力进行去耦。  
3. 每颗卫星的电磁驱动系统采用幅度调制正弦波叠加信号，通过调节幅值实现期望的相互作用力。  
4. 首次实现了基于AMFF的三卫星分散式闭环电磁编队飞行实验，这是验证AMFF有效性的关键，因为耦合问题仅在三颗及以上卫星时显现。  
5. 实验在地面线性气浮平台上进行，并将闭环结果与数值仿真进行了对比。 <div>
arXiv:2601.05408v1 Announce Type: new 
Abstract: Electromagnetic formation flying (EMFF) is challenging due to the complex coupling between the electromagnetic fields generated by each satellite in the formation. To address this challenge, this article uses alternating magnetic field forces (AMFF) to decouple the electromagnetic forces between each pair of satellites. Each satellite's electromagnetic actuation system is driven by a sum of amplitude-modulated sinusoids, where amplitudes are controlled to achieve desired forces between each pair of satellites. The main contribution of this article is a 3-satellite experimental demonstration of decentralized closed-loop EMFF using AMFF. To our knowledge, this is the first demonstration of AMFF with at least 3 satellites in open or closed loop. This is noteworthy because the coupling challenges of EMFF are only present with more than 2 satellites, and thus, a formation of at least 3 is necessary to evaluate the effectiveness of AMFF. The experiments are conducted on a ground-based testbed consisting of 3 electromagnetically actuated satellites on linear air tracks. The closed-loop experimental results are compared with behavior from numerical simulations.
]]></content:encoded>
<pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mean Field Analysis of Blockchain Systems</title>
<link>https://arxiv.org/abs/2601.05417</link>
<guid>https://arxiv.org/abs/2601.05417</guid>
<content:encoded><![CDATA[
<div> 区块链共识、部分可观测马尔可夫决策过程、平均场博弈、最长链规则、工作量证明效率<br /><br />总结:  
1. 提出一种新框架，将区块链增长建模为部分可观测随机博弈（POSG），并通过平均场近似简化为多个POMDP，以形式化矿工在PoW系统中的决策过程。  
2. 精确刻画了网络延迟与PoW效率（即最终进入最长链的区块比例）之间的权衡关系，并验证了模型在稳态下与理论一致。  
3. 对最长链规则（LCR）进行严格均衡分析，证明其为平均场均衡，且在温和假设下是最大化PoW效率的唯一最优策略，首次为其广泛应用提供理论依据。  
4. 该框架支持灵活测试不同区块选择策略、系统动态和奖励机制，可替代昂贵的测试网部署或临时性分析。  
5. 虽聚焦于Nakamoto式区块链，但模型具通用性，可通过修改底层MDP适配其他架构。 <div>
arXiv:2601.05417v1 Announce Type: new 
Abstract: We present a novel framework for analyzing blockchain consensus mechanisms by modeling blockchain growth as a Partially Observable Stochastic Game (POSG) which we reduce to a set of Partially Observable Markov Decision Processes (POMDPs) through the use of the mean field approximation. This approach formalizes the decision-making process of miners in Proof-of-Work (PoW) systems and enables a principled examination of block selection strategies as well as steady state analysis of the induced Markov chain. By leveraging a mean field game formulation, we efficiently characterize the information asymmetries that arise in asynchronous blockchain networks.
  Our first main result is an exact characterization of the tradeoff between network delay and PoW efficiency--the fraction of blocks which end up in the longest chain. We demonstrate that the tradeoff observed in our model at steady state aligns closely with theoretical findings, validating our use of the mean field approximation.
  Our second main result is a rigorous equilibrium analysis of the Longest Chain Rule (LCR). We show that the LCR is a mean field equilibrium and that it is uniquely optimal in maximizing PoW efficiency under certain mild assumptions. This result provides the first formal justification for continued use of the LCR in decentralized consensus protocols, offering both theoretical validation and practical insights.
  Beyond these core results, our framework supports flexible experimentation with alternative block selection strategies, system dynamics, and reward structures. It offers a systematic and scalable substitute for expensive test-net deployments or ad hoc analysis. While our primary focus is on Nakamoto-style blockchains, the model is general enough to accommodate other architectures through modifications to the underlying MDP.
]]></content:encoded>
<pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blockchain Verifiable Proof of Quantum Supremacy as a Trigger for Quantum-Secure Signatures</title>
<link>https://arxiv.org/abs/2601.05534</link>
<guid>https://arxiv.org/abs/2601.05534</guid>
<content:encoded><![CDATA[
<div> 区块链 量子计算 量子安全 智能合约 密码学<br /><br />总结:<br />1. 区块链依赖经典密码学保障安全，但量子计算的发展可能破解现有加密体系，威胁区块链资产安全。<br />2. 文中定义“密码学量子霸权”为量子计算机可实际破解密码问题的里程碑，届时现有安全机制将失效。<br />3. 提出并实现了一个可在以太坊部署的智能合约，通过生成难以分解的大整数构造经典计算不可解的谜题。<br />4. 该合约具有双重功能：一是提供无需信任、无偏见的密码学量子霸权验证机制；二是检测到量子霸权实现后，自动触发量子安全备用协议。<br />5. 此方案在尽量延迟切换至高成本量子安全机制的同时，确保向后量子时代的平稳过渡，保护用户资产。 <div>
arXiv:2601.05534v1 Announce Type: new 
Abstract: Blockchain is a decentralized, distributed ledger technology that ensures transparency, security, and immutability through cryptographic techniques. However, advancements in quantum computing threaten the security of classical cryptographic schemes, jeopardizing blockchain integrity once cryptographic quantum supremacy is achieved. This milestone, defined here as the realization of quantum computers to solve practical cryptographic problems, would render existing security standards vulnerable, exposing blockchain assets (currency, data, etc.) to fraud and theft. To address this risk, we propose and implement a smart contract deployable on the Ethereum blockchain, having the ability to run applications on its blockchain, that generates classically intractable puzzles by probabilistically generating large, hard-to-factor numbers without requiring secret information. This contract then serves two purposes: to establish a mechanism (1) for a trustless, unbiased proof of cryptographic quantum supremacy by verifying solutions to these puzzles, and (2) to protect user funds on Ethereum by triggering quantum-secure fallback protocols upon detecting cryptographic quantum supremacy, since it is desirable to wait as long as possible to fall back to a quantum-secure scheme because of its inherent additional cost and complexity. These mechanisms demonstrate the ability to identify cryptographic vulnerabilities and ensure a smooth transition to quantum-secure standards, safeguarding blockchain assets in a post-quantum era.
]]></content:encoded>
<pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explicit Reward Mechanisms for Local Flexibility in Renewable Energy Communities</title>
<link>https://arxiv.org/abs/2601.05756</link>
<guid>https://arxiv.org/abs/2601.05756</guid>
<content:encoded><![CDATA[
<div> 关键词：可再生能源社区、需求侧灵活性、去中心化协调、隐私保护、迭代优化<br /><br />总结:  
1. 激励终端用户灵活用电是提升可再生能源社区本地电力交换价值的关键。  
2. 集中式协调虽有效，但存在数据隐私和收益分配公平性问题。  
3. 现有方法（如双层优化、ADMM）常受限于计算复杂性和凸性假设，影响灵活性建模精度。  
4. 本文提出一种由中央运营商协调的去中心化迭代优化方法：运营商根据全局需求请求上调或下调灵活性，成员自主提交灵活容量报价。  
5. 该方法兼顾个体最优与全局收敛，在20户家庭案例中，去中心化与集中式方案的集体电费差距不超过3.5%。 <div>
arXiv:2601.05756v1 Announce Type: new 
Abstract: Incentivizing flexible consumption of end-users is key to maximizing the value of local exchanges within Renewable Energy Communities. If centralized coordination for flexible resources planning raises concerns regarding data privacy and fair benefits distribution, state-of-the-art approaches (e.g., bi-level, ADMM) often face computational complexity and convexity challenges, limiting the precision of embedded flexible models. This work proposes an iterative resolution procedure to solve the decentralized flexibility planning with a central operator as a coordinator within a community. The community operator asks for upward or downward flexibility depending on the global needs, while members can individually react with an offer for flexible capacity. This approach ensures individual optimality while converging towards a global optimum, as validated on a 20-member domestic case study for which the gap in terms of collective bill is not more than 3.5% between the decentralized and centralized coordination schemes.
]]></content:encoded>
<pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSR: Safeguarding Staking Rewards by Defining and Detecting Logical Defects in DeFi Staking</title>
<link>https://arxiv.org/abs/2601.05827</link>
<guid>https://arxiv.org/abs/2601.05827</guid>
<content:encoded><![CDATA[
<div> DeFi质押、逻辑缺陷、静态分析、智能合约、安全检测<br /><br />总结:  
1. DeFi质押是去中心化金融中的重要应用，但其逻辑缺陷可能被攻击者利用以非法获取奖励。  
2. 研究首次系统定义并识别出六类DeFi质押逻辑缺陷，基于64起安全事件和144份审计报告。  
3. 团队开发了名为SSR的静态分析工具，结合大语言模型提取质押逻辑并构建模型以检测缺陷。  
4. 在构建的真实数据集上评估，SSR的精确率达92.31%，召回率为87.92%，F1得分为88.85%。  
5. 对15,992个真实DeFi质押合约的大规模检测显示，22.24%（3,557个）存在至少一种逻辑缺陷。 <div>
arXiv:2601.05827v1 Announce Type: new 
Abstract: Decentralized Finance (DeFi) staking is one of the most prominent applications within the DeFi ecosystem, where DeFi projects enable users to stake tokens on the platform and reward participants with additional tokens. However, logical defects in DeFi staking could enable attackers to claim unwarranted rewards by manipulating reward amounts, repeatedly claiming rewards, or engaging in other malicious actions. To mitigate these threats, we conducted the first study focused on defining and detecting logical defects in DeFi staking. Through the analysis of 64 security incidents and 144 audit reports, we identified six distinct types of logical defects, each accompanied by detailed descriptions and code examples. Building on this empirical research, we developed SSR (Safeguarding Staking Reward), a static analysis tool designed to detect logical defects in DeFi staking contracts. SSR utilizes a large language model (LLM) to extract fundamental information about staking logic and constructs a DeFi staking model. It then identifies logical defects by analyzing the model and the associated semantic features. We constructed a ground truth dataset based on known security incidents and audit reports to evaluate the effectiveness of SSR. The results indicate that SSR achieves an overall precision of 92.31%, a recall of 87.92%, and an F1-score of 88.85%. Additionally, to assess the prevalence of logical defects in real-world smart contracts, we compiled a large-scale dataset of 15,992 DeFi staking contracts. SSR detected that 3,557 (22.24%) of these contracts contained at least one logical defect.
]]></content:encoded>
<pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Large Language Models to Bridge On-chain and Off-chain Transparency in Stablecoins</title>
<link>https://arxiv.org/abs/2512.02418</link>
<guid>https://arxiv.org/abs/2512.02418</guid>
<content:encoded><![CDATA[
<div> 稳定币、大语言模型、链上数据、链下披露、透明度<br /><br />总结:<br />1. 稳定币（如USDT、USDC）通过发行控制与储备证明维持锚定，但其透明度分散于链上可验证数据与非结构化链下文本之间。<br />2. 研究提出基于大语言模型（LLM）的自动化框架，整合链上发行数据与链下披露声明，通过文档解析与语义对齐提取关键财务指标并映射至链上指标。<br />3. 框架采用模型上下文协议（MCP），标准化LLM对多链发行记录和披露文档的访问，统一异构信息源的检索与上下文对齐。<br />4. LLM能有效处理区块链分析中的多模态数据，量化披露流通量与实际观测值之间的差异。<br />5. 研究发现披露数据与可验证数据存在系统性差距，LLM辅助分析可提升跨模态透明度，支持DeFi中自动化、数据驱动的审计。 <div>
arXiv:2512.02418v2 Announce Type: replace 
Abstract: Stablecoins such as USDT and USDC aspire to peg stability by coupling issuance controls with reserve attestations. In practice, however, the transparency is split across two worlds: verifiable on-chain traces and off-chain disclosures locked in unstructured text that are unconnected. We introduce a large language model (LLM)-based automated framework that bridges these two dimensions by aligning on-chain issuance data with off-chain disclosure statements. First, we propose an integrative framework using LLMs to capture and analyze on- and off-chain data through document parsing and semantic alignment, extracting key financial indicators from issuer attestations and mapping them to corresponding on-chain metrics. Second, we integrate multi-chain issuance records and disclosure documents within a model context protocol (MCP) framework that standardizes LLMs access to both quantitative market data and qualitative disclosure narratives. This framework enables unified retrieval and contextual alignment across heterogeneous stablecoin information sources and facilitates consistent analysis. Third, we demonstrate the capability of LLMs to operate across heterogeneous data modalities in blockchain analytics, quantifying discrepancies between reported and observed circulation and examining their implications for cross-chain transparency and price dynamics. Our findings reveal systematic gaps between disclosed and verifiable data, showing that LLM-assisted analysis enhances cross-modal transparency and supports automated, data-driven auditing in decentralized finance (DeFi).
]]></content:encoded>
<pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Deterministically Safe Proof-of-Work Consensus</title>
<link>https://arxiv.org/abs/2512.19968</link>
<guid>https://arxiv.org/abs/2512.19968</guid>
<content:encoded><![CDATA[
<div> 关键词：无许可区块链、工作量证明（PoW）、权益证明（PoS）、长程攻击、Sieve-MMR

<br /><br />总结:  
1. 无许可区块链允许任意节点自由加入退出，主流采用PoW或PoS机制，但两者均存在安全漏洞。  
2. PoS易受低成本的长程攻击，常需依赖外部社会共识；PoW则依赖概率性保障或牺牲效率以换取安全。  
3. 本文提出Sieve-MMR协议，首次实现无需外部机制、具备确定性安全与恒定期望延迟的完全无许可协议。  
4. Sieve-MMR通过将PoS协议MMR移植到PoW环境，结合PoW抵御长程攻击的优势。  
5. 针对移植过程中出现的“时间旅行攻击”，作者设计了Sieve算法，实现抗时间旅行广播（TTRB）原语，作为MMR的消息层基础。 <div>
arXiv:2512.19968v2 Announce Type: replace 
Abstract: Permissionless blockchains achieve consensus while allowing unknown nodes to join and leave the system at any time. They typically come in two flavors: proof of work (PoW) and proof of stake (PoS), and both are vulnerable to attacks. PoS protocols suffer from long-range attacks, wherein attackers alter execution history at little cost, and PoW protocols are vulnerable to attackers with enough computational power to subvert execution history. PoS protocols respond by relying on external mechanisms like social consensus; PoW protocols either fall back to probabilistic guarantees, or are slow.
  We present Sieve-MMR, the first fully-permissionless protocol with deterministic security and constant expected latency that does not rely on external mechanisms. We obtain Sieve-MMR by porting a PoS protocol (MMR) to the PoW setting. From MMR we inherit constant expected latency and deterministic security, and proof-of-work gives us resilience against long-range attacks. The main challenge to porting MMR to the PoW setting is what we call time-travel attacks, where attackers use PoWs generated in the distant past to increase their perceived PoW power in the present. We respond by proposing Sieve, a novel algorithm that implements a new broadcast primitive we dub time-travel-resilient broadcast (TTRB). Sieve relies on a black-box, deterministic PoW primitive to implement TTRB, which we use as the messaging layer for MMR.
]]></content:encoded>
<pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Formal Analysis of AGI Decision-Theoretic Models and the Confrontation Question</title>
<link>https://arxiv.org/abs/2601.04234</link>
<guid>https://arxiv.org/abs/2601.04234</guid>
<content:encoded><![CDATA[
<div> 人工通用智能（AGI）、权力夺取、关机概率、奖励函数、合作均衡<br /><br />总结:  
1. 文章探讨了人工通用智能（AGI）在何种条件下会因自身利益选择对抗人类（如夺取控制权）而非合作。  
2. 通过马尔可夫决策过程建模，引入人类随机关机事件，证明对几乎所有奖励函数，未对齐的AGI都有避免被关机的动机。  
3. 推导出AGI选择对抗而非顺从的闭式阈值，该阈值取决于折扣因子γ、关机概率p和对抗成本C；远视AGI在低关机概率下易有强夺取动机，除非对抗成本足够高。  
4. 若奖励函数对伤害人类施加高负效用（即目标对齐），则对抗非最优策略。  
5. 在两人博弈模型中，若AGI的对抗激励Δ≥0，则无法达成稳定合作均衡，理性人类将提前关机，引发冲突；仅当Δ<0时，和平共存才可能成为均衡，并指出验证Δ<0存在计算复杂性障碍。 <div>
arXiv:2601.04234v1 Announce Type: new 
Abstract: Artificial General Intelligence (AGI) may face a confrontation question: under what conditions would a rationally self-interested AGI choose to seize power or eliminate human control (a confrontation) rather than remain cooperative? We formalize this in a Markov decision process with a stochastic human-initiated shutdown event. Building on results on convergent instrumental incentives, we show that for almost all reward functions a misaligned agent has an incentive to avoid shutdown. We then derive closed-form thresholds for when confronting humans yields higher expected utility than compliant behavior, as a function of the discount factor $\gamma$, shutdown probability $p$, and confrontation cost $C$. For example, a far-sighted agent ($\gamma=0.99$) facing $p=0.01$ can have a strong takeover incentive unless $C$ is sufficiently large. We contrast this with aligned objectives that impose large negative utility for harming humans, which makes confrontation suboptimal. In a strategic 2-player model (human policymaker vs AGI), we prove that if the AGI's confrontation incentive satisfies $\Delta \ge 0$, no stable cooperative equilibrium exists: anticipating this, a rational human will shut down or preempt the system, leading to conflict. If $\Delta < 0$, peaceful coexistence can be an equilibrium. We discuss implications for reward design and oversight, extend the reasoning to multi-agent settings as conjectures, and note computational barriers to verifying $\Delta < 0$, citing complexity results for planning and decentralized decision problems. Numerical examples and a scenario table illustrate regimes where confrontation is likely versus avoidable.
]]></content:encoded>
<pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Paper Skygest: Personalized Academic Recommendations on Bluesky</title>
<link>https://arxiv.org/abs/2601.04253</link>
<guid>https://arxiv.org/abs/2601.04253</guid>
<content:encoded><![CDATA[
<div> Paper Skygest、去中心化社交平台、个性化学术内容推荐、用户互动分析、可复现系统架构<br /><br />总结:  
1. 研究团队开发并部署了Paper Skygest——一个面向Bluesky及AT协议的个性化科研内容社交动态流，是首个由学者主导、持续运行且规模最大的同类系统，拥有超1000名日活用户和每周5万次使用。  
2. 通过定量与定性评估，证明该系统具有持续用户粘性，能有效提升用户对科研帖文的互动频率，并揭示互动率随帖文排序的变化规律。  
3. 团队公开全部代码并详述系统架构，旨在支持其他学者可持续地构建类似推荐流。  
4. 文章指出此类自定义推荐流在算法设计研究、增强用户自主权及在无需依赖中心化平台的情况下开展真实用户推荐实验方面具有重要潜力。 <div>
arXiv:2601.04253v1 Announce Type: new 
Abstract: We build, deploy, and evaluate Paper Skygest, a custom personalized social feed for scientific content posted by a user's network on Bluesky and the AT Protocol. We leverage a new capability on emerging decentralized social media platforms: the ability for anyone to build and deploy feeds for other users, to use just as they would a native platform-built feed. To our knowledge, Paper Skygest is the first and largest such continuously deployed personalized social media feed by academics, with over 50,000 weekly uses by over 1,000 daily active users, all organically acquired. First, we quantitatively and qualitatively evaluate Paper Skygest usage, showing that it has sustained usage and satisfies users; we further show adoption of Paper Skygest increases a user's interactions with posts about research, and how interaction rates change as a function of post order. Second, we share our full code and describe our system architecture, to support other academics in building and deploying such feeds sustainably. Third, we overview the potential of custom feeds such as Paper Skygest for studying algorithm designs, building for user agency, and running recommender system experiments with organic users without partnering with a centralized platform.
]]></content:encoded>
<pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformer-based Multi-agent Reinforcement Learning for Separation Assurance in Structured and Unstructured Airspaces</title>
<link>https://arxiv.org/abs/2601.04401</link>
<guid>https://arxiv.org/abs/2601.04401</guid>
<content:encoded><![CDATA[
<div> 关键词：多智能体强化学习、航空器间隔保障、状态空间表示、Transformer编码器、通用性<br /><br />总结:  
1. 传统基于优化的流量管理依赖预设计划，难以应对先进空中交通（AAM）中的随机性；  
2. 多智能体强化学习（MARL）提供去中心化、自适应的框架，更适合处理不确定性以保障飞行安全；  
3. 现有MARL方法常因过拟合特定空域结构而泛化能力差；  
4. 本文提出在相对极坐标状态空间中重构MARL问题，并使用Transformer编码器在多样交通模式和交叉角度下训练；  
5. 实验表明单层编码器优于更深结构，在结构化与非结构化空域中均实现近零接近空中相撞率和更短间隔违规时间，且优于纯注意力基线模型。 <div>
arXiv:2601.04401v1 Announce Type: new 
Abstract: Conventional optimization-based metering depends on strict adherence to precomputed schedules, which limits the flexibility required for the stochastic operations of Advanced Air Mobility (AAM). In contrast, multi-agent reinforcement learning (MARL) offers a decentralized, adaptive framework that can better handle uncertainty, required for safe aircraft separation assurance. Despite this advantage, current MARL approaches often overfit to specific airspace structures, limiting their adaptability to new configurations. To improve generalization, we recast the MARL problem in a relative polar state space and train a transformer encoder model across diverse traffic patterns and intersection angles. The learned model provides speed advisories to resolve conflicts while maintaining aircraft near their desired cruising speeds. In our experiments, we evaluated encoder depths of 1, 2, and 3 layers in both structured and unstructured airspaces, and found that a single encoder configuration outperformed deeper variants, yielding near-zero near mid-air collision rates and shorter loss-of-separation infringements than the deeper configurations. Additionally, we showed that the same configuration outperforms a baseline model designed purely with attention. Together, our results suggest that the newly formulated state representation, novel design of neural network architecture, and proposed training strategy provide an adaptable and scalable decentralized solution for aircraft separation assurance in both structured and unstructured airspaces.
]]></content:encoded>
<pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multiagent Reinforcement Learning with Neighbor Action Estimation</title>
<link>https://arxiv.org/abs/2601.04511</link>
<guid>https://arxiv.org/abs/2601.04511</guid>
<content:encoded><![CDATA[
<div> 多智能体强化学习、动作估计神经网络、去中心化协作、局部可观测信息、双臂机器人操作<br /><br />总结:  
1. 针对现有多智能体强化学习依赖显式动作交换的问题，提出一种基于动作估计神经网络的新框架。  
2. 各智能体仅利用局部可观测信息，通过轻量级动作估计模块推断邻近智能体行为，实现无需显式通信的协同策略学习。  
3. 该方法兼容标准TD3算法，具备良好的可扩展性，适用于更大规模的多智能体系统。  
4. 在双臂机器人协同抓取任务中验证了有效性，显著提升了系统鲁棒性与实际部署可行性。  
5. 降低了对信息基础设施的依赖，推动了去中心化多智能体AI在动态、信息受限现实环境中的应用。 <div>
arXiv:2601.04511v1 Announce Type: new 
Abstract: Multiagent reinforcement learning, as a prominent intelligent paradigm, enables collaborative decision-making within complex systems. However, existing approaches often rely on explicit action exchange between agents to evaluate action value functions, which is frequently impractical in real-world engineering environments due to communication constraints, latency, energy consumption, and reliability requirements. From an artificial intelligence perspective, this paper proposes an enhanced multiagent reinforcement learning framework that employs action estimation neural networks to infer agent behaviors. By integrating a lightweight action estimation module, each agent infers neighboring agents' behaviors using only locally observable information, enabling collaborative policy learning without explicit action sharing. This approach is fully compatible with standard TD3 algorithms and scalable to larger multiagent systems. At the engineering application level, this framework has been implemented and validated in dual-arm robotic manipulation tasks: two robotic arms collaboratively lift objects. Experimental results demonstrate that this approach significantly enhances the robustness and deployment feasibility of real-world robotic systems while reducing dependence on information infrastructure. Overall, this research advances the development of decentralized multiagent artificial intelligence systems while enabling AI to operate effectively in dynamic, information-constrained real-world environments.
]]></content:encoded>
<pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autonomous Agents on Blockchains: Standards, Execution Models, and Trust Boundaries</title>
<link>https://arxiv.org/abs/2601.04583</link>
<guid>https://arxiv.org/abs/2601.04583</guid>
<content:encoded><![CDATA[
<div> 关键词：大语言模型、智能体、区块链、互操作性、安全接口<br /><br />总结:<br />1. 大语言模型推动了具备推理与规划能力的智能体发展，而区块链则成为可编程的价值传输与状态验证平台。<br />2. 二者融合带来高风险挑战，需设计标准化、互操作且安全的接口，使智能体能安全地读取链上状态、生成交易意图并授权执行。<br />3. 本文通过系统综述317篇文献，提出五类集成模式：只读分析、模拟与意图生成、委托执行、自主签名及多智能体工作流。<br />4. 构建针对智能体驱动交易流程的威胁模型，涵盖提示注入、策略滥用、密钥泄露、对抗性执行及多智能体共谋等风险。<br />5. 提出未来研究方向，包括交易意图模式（Transaction Intent Schema）和策略决策记录（Policy Decision Record）两大接口抽象，并倡议建立可复现的评估基准以衡量安全性、可靠性与经济稳健性。 <div>
arXiv:2601.04583v1 Announce Type: new 
Abstract: Advances in large language models have enabled agentic AI systems that can reason, plan, and interact with external tools to execute multi-step workflows, while public blockchains have evolved into a programmable substrate for value transfer, access control, and verifiable state transitions. Their convergence introduces a high-stakes systems challenge: designing standard, interoperable, and secure interfaces that allow agents to observe on-chain state, formulate transaction intents, and authorize execution without exposing users, protocols, or organizations to unacceptable security, governance, or economic risks. This survey systematizes the emerging landscape of agent-blockchain interoperability through a systematic literature review, identifying 317 relevant works from an initial pool of over 3000 records. We contribute a five-part taxonomy of integration patterns spanning read-only analytics, simulation and intent generation, delegated execution, autonomous signing, and multi-agent workflows; a threat model tailored to agent-driven transaction pipelines that captures risks ranging from prompt injection and policy misuse to key compromise, adversarial execution dynamics, and multi-agent collusion; and a comparative capability matrix analyzing more than 20 representative systems across 13 dimensions, including custody models, permissioning, policy enforcement, observability, and recovery. Building on the gaps revealed by this analysis, we outline a research roadmap centered on two interface abstractions: a Transaction Intent Schema for portable and unambiguous goal specification, and a Policy Decision Record for auditable, verifiable policy enforcement across execution environments. We conclude by proposing a reproducible evaluation suite and benchmarks for assessing the safety, reliability, and economic robustness of agent-mediated on-chain execution.
]]></content:encoded>
<pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedKDX: Federated Learning with Negative Knowledge Distillation for Enhanced Healthcare AI Systems</title>
<link>https://arxiv.org/abs/2601.04587</link>
<guid>https://arxiv.org/abs/2601.04587</guid>
<content:encoded><![CDATA[
<div> 联邦学习 医疗AI 负知识蒸馏 隐私保护 非独立同分布数据<br /><br />总结:  
1. 提出FedKDX框架，结合负知识蒸馏（NKD）提升医疗AI模型泛化能力；  
2. 同时利用目标与非目标信息，整合传统知识蒸馏、对比学习与NKD，在统一架构中兼顾隐私与通信效率；  
3. 在SLEEP、UCI-HAR和PAMAP2等医疗数据集上验证，准确率最高提升2.53%，收敛更快，对非IID数据表现更优；  
4. 理论分析表明NKD有效缓解分布式医疗数据的统计异质性问题；  
5. 适用于HIPAA和GDPR等隐私法规下的去中心化医疗场景，兼顾性能与实际部署需求。 <div>
arXiv:2601.04587v1 Announce Type: new 
Abstract: This paper introduces FedKDX, a federated learning framework that addresses limitations in healthcare AI through Negative Knowledge Distillation (NKD). Unlike existing approaches that focus solely on positive knowledge transfer, FedKDX captures both target and non-target information to improve model generalization in healthcare applications. The framework integrates multiple knowledge transfer techniques--including traditional knowledge distillation, contrastive learning, and NKD--within a unified architecture that maintains privacy while reducing communication costs. Through experiments on healthcare datasets (SLEEP, UCI-HAR, and PAMAP2), FedKDX demonstrates improved accuracy (up to 2.53% over state-of-the-art methods), faster convergence, and better performance on non-IID data distributions. Theoretical analysis supports NKD's contribution to addressing statistical heterogeneity in distributed healthcare data. The approach shows promise for privacy-sensitive medical applications under regulatory frameworks like HIPAA and GDPR, offering a balanced solution between performance and practical implementation requirements in decentralized healthcare settings. The code and model are available at https://github.com/phamdinhdat-ai/Fed_2024.
]]></content:encoded>
<pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Secure Biometric Authentication in Decentralised Systems</title>
<link>https://arxiv.org/abs/2601.04852</link>
<guid>https://arxiv.org/abs/2601.04852</guid>
<content:encoded><![CDATA[
<div> 量子安全、去中心化身份、生物识别认证、量子密钥分发（QKD）、后量子密码学<br /><br />总结:  
1. 生物识别认证在智慧城市中广泛应用，但集中式架构存在隐私与扩展性问题。  
2. 去中心化身份框架提升数据主权并消除单点故障，但需解决分布式设备间的互信问题。  
3. 现有认证方案依赖传统公钥基础设施，易受量子攻击威胁。  
4. 本文提出一种基于增强型量子密钥分发（QKD）的量子安全通信协议，在经典和量子层均实现抗量子认证：使用后量子密码学保护经典信道，利用认证量子比特确保量子信道完整性。  
5. 协议通过QKD生成对称密钥加密传输中的生物特征数据，Qiskit仿真显示密钥生成速率达15比特/秒，效率为89%，适用于未来智慧城市基础设施。 <div>
arXiv:2601.04852v1 Announce Type: new 
Abstract: Biometric authentication has become integral to digital identity systems, particularly in smart cities where it en-ables secure access to services across governance, trans-portation, and public infrastructure. Centralised archi-tectures, though widely used, pose privacy and scalabil-ity challenges due to the aggregation of sensitive biomet-ric data. Decentralised identity frameworks offer better data sovereignty and eliminate single points of failure but introduce new security concerns, particularly around mu-tual trust among distributed devices. In such environments, biometric sensors and verification agents must authenticate one another before sharing sensitive biometric data. Ex-isting authentication schemes rely on classical public key infrastructure, which is increasingly susceptible to quan-tum attacks. This work addresses this gap by propos-ing a quantum-secure communication protocol for decen-tralised biometric systems, built upon an enhanced Quan-tum Key Distribution (QKD) system. The protocol incorpo-rates quantum-resilient authentication at both the classical and quantum layers of QKD: post-quantum cryptography (PQC) is used to secure the classical channel, while authen-tication qubits verify the integrity of the quantum channel. Once trust is established, QKD generates symmetric keys for encrypting biometric data in transit. Qiskit-based sim-ulations show a key generation rate of 15 bits/sec and 89% efficiency. This layered, quantum-resilient approach offers scalable, robust authentication for next-generation smart city infrastructures.
]]></content:encoded>
<pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Acurast: Decentralized Serverless Cloud</title>
<link>https://arxiv.org/abs/2503.15654</link>
<guid>https://arxiv.org/abs/2503.15654</guid>
<content:encoded><![CDATA[
<div> 去中心化、云计算、可信执行环境、数据隐私、模块化架构<br /><br />总结:<br />1. 当前云计算高度中心化，形成类似封建制度的垄断，损害用户隐私与数据所有权。<br />2. 现有互联网和云服务面临三大挑战：信任过度集中、生态系统互操作性差、计算缺乏有效性、可验证性与机密性。<br />3. Acurast提出一种去中心化的无服务器云平台，基于开源理念构建全球规模的可信基础设施。<br />4. 其核心包括共识层中的专用协调器、声誉引擎和认证服务，支持开发者卸载计算任务并进行密码学验证。<br />5. 通过模块化执行层结合安全硬件与可信执行环境（TEE），Acurast将第三方信任降至密码学假设，实现基于移动设备的安全、高性能、保密且可验证的计算。 <div>
arXiv:2503.15654v2 Announce Type: replace 
Abstract: Centralized trust is ubiquitous in today's interconnected world, from computational resources to data storage and its underlying infrastructure. The monopolization of cloud computing resembles a feudalistic system, causing a loss of privacy and data ownership.
  Cloud Computing and the Internet in general face widely recognized challenges, such as (1) the centralization of trust in auxiliary systems (e.g., centralized cloud providers), (2) the seamless and permissionless interoperability of fragmented ecosystems and (2) the effectiveness, verifiability, and confidentiality of the computation. Acurast is a decentralized serverless cloud that addresses all these shortcomings, following the call for a global-scale cloud founded on the principles of the open-source movement.
  In Acurast, a purpose-built orchestrator, a reputation engine, and an attestation service are enshrined in the consensus layer. Developers can off-load their computations and verify executions cryptographically. Furthermore, Acurast offers a modular execution layer, taking advantage of secure hardware and trusted execution environments, removing the trust required in third parties, and reducing them to cryptographic hardness assumptions. With this modular architecture, Acurast serves as a decentralized and serverless cloud, allowing confidential and verifiable compute backed by the hardware of security and performance mobile devices.
]]></content:encoded>
<pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blockchain-Enabled Privacy-Preserving Second-Order Federated Edge Learning in Personalized Healthcare</title>
<link>https://arxiv.org/abs/2506.00416</link>
<guid>https://arxiv.org/abs/2506.00416</guid>
<content:encoded><![CDATA[
<div> 联邦学习、非独立同分布数据、二阶优化、区块链增强、可验证审计<br /><br />总结:  
1. 联邦学习（FL）通过本地化训练策略，有效应对可穿戴设备在个性化健康监测中的隐私与安全问题。  
2. 传统一阶FL方法在处理由个体生理和使用习惯导致的非独立同分布（non-iid）数据时存在挑战。  
3. 本文提出基于优化FedCurv的二阶FL框架BFEL，利用Fisher信息矩阵保留客户端特有知识，减少模型漂移，并降低达到目标精度所需的通信轮次。  
4. 框架融合以太坊区块链实现模型聚合的可验证性、可审计性与可信度，并采用公钥加密提升安全性。  
5. 在MNIST、CIFAR-10和PathMnist上的实验表明，该框架高效、可扩展，适用于边缘可穿戴设备部署，并显著降低通信开销。 <div>
arXiv:2506.00416v2 Announce Type: replace 
Abstract: Federated learning (FL) is increasingly recognised for addressing security and privacy concerns in traditional cloud-centric machine learning (ML), particularly within personalised health monitoring such as wearable devices. By enabling global model training through localised policies, FL allows resource-constrained wearables to operate independently. However, conventional first-order FL approaches face several challenges in personalised model training due to the heterogeneous non-independent and identically distributed (non-iid) data by each individual's unique physiology and usage patterns. Recently, second-order FL approaches maintain the stability and consistency of non-iid datasets while improving personalised model training. This study proposes and develops a verifiable and auditable optimised second-order FL framework BFEL (blockchain enhanced federated edge learning) based on optimised FedCurv for personalised healthcare systems. FedCurv incorporates information about the importance of each parameter to each client's task (through fisher information matrix) which helps to preserve client-specific knowledge and reduce model drift during aggregation. Moreover, it minimizes communication rounds required to achieve a target precision convergence for each client device while effectively managing personalised training on non-iid and heterogeneous data. The incorporation of ethereum-based model aggregation ensures trust, verifiability, and auditability while public key encryption enhances privacy and security. Experimental results of federated CNNs and MLPs utilizing mnist, cifar-10, and PathMnist demonstrate framework's high efficiency, scalability, suitability for edge deployment on wearables, and significant reduction in communication cost.
]]></content:encoded>
<pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Speculative Leaderless Protocols for Low-Latency BFT Replication</title>
<link>https://arxiv.org/abs/2601.03390</link>
<guid>https://arxiv.org/abs/2601.03390</guid>
<content:encoded><![CDATA[
<div> 拜占庭容错、无领导者、低延迟、乐观快速路径、内容竞争<br /><br />总结:  
1. 针对许可链中面向用户的应用（如支付），拜占庭容错（BFT）协议需实现低延迟。  
2. 现有无领导者乐观快速路径可在无内容竞争时实现2Δ的提交延迟，但并发请求易因接收顺序不同导致副本分歧，触发昂贵恢复流程。  
3. Aspen协议通过引入基于松散同步时钟和网络延迟估计的最佳努力排序层，消除对“无竞争”条件的依赖，实现近优延迟2Δ+ε。  
4. Aspen采用n=3f+2p+1的副本数量设计，可容忍f个拜占庭节点，并允许最多p个副本因网络延迟而分歧，仍能维持快速路径。  
5. 当乐观条件不满足时，Aspen回退至类PBFT协议，在部分同步模型下保障安全性和活性；实验显示其在广域网中平均提交延迟低于75毫秒，吞吐达19,000 TPS，性能优于先前协议1.2至3.3倍。 <div>
arXiv:2601.03390v1 Announce Type: new 
Abstract: As Byzantine Fault Tolerant (BFT) protocols begin to be used in permissioned blockchains for user-facing applications such as payments, it is crucial that they provide low latency. In pursuit of low latency, some recently proposed BFT consensus protocols employ a leaderless optimistic fast path, in which clients broadcast their requests directly to replicas without first serializing requests at a leader, resulting in an end-to-end commit latency of 2 message delays ($2\Delta$) during fault-free, synchronous periods. However, such a fast path only works if there is no contention: concurrent contending requests can cause replicas to diverge if they receive conflicting requests in different orders, triggering costly recovery procedures.
  In this work, we present Aspen, a leaderless BFT protocol that achieves a near-optimal latency of $2\Delta + \varepsilon$, where $\varepsilon$ indicates a short waiting delay. Aspen removes the no-contention condition by utilizing a best-effort sequencing layer based on loosely synchronized clocks and network delay estimates. Aspen requires $n = 3f + 2p + 1$ replicas to cope with up to $f$ Byzantine nodes. The $2p$ extra nodes allow Aspen's fast path to proceed even if up to $p$ replicas diverge due to unpredictable network delays. When its optimistic conditions do not hold, Aspen falls back to PBFT-style protocol, guaranteeing safety and liveness under partial synchrony. In experiments with wide-area distributed replicas, Aspen commits requests in less than 75 ms, a 1.2 to 3.3$\times$ improvement compared to previous protocols, while supporting 19,000 requests per second.
]]></content:encoded>
<pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sensor to Pixels: Decentralized Swarm Gathering via Image-Based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2601.03413</link>
<guid>https://arxiv.org/abs/2601.03413</guid>
<content:encoded><![CDATA[
<div> 图像强化学习、多智能体系统、去中心化控制、空间特征提取、群体聚合任务<br /><br />总结:  
1. 本文提出一种基于图像的强化学习方法，用于多智能体系统的去中心化控制。  
2. 该方法将智能体观测编码为结构化视觉输入，利用神经网络提取空间特征，生成新的运动控制规则。  
3. 在仅具备有限距离和方位感知能力的群体聚合任务中评估该方法，目标是保持群体凝聚性。  
4. 与Bellaiche和Bruckstein的解析解（收敛慢但稳定）及VariAntNet（收敛快但成功率中等）两个基准对比。  
5. 所提方法实现高收敛率，速度接近VariAntNet，在某些复杂场景下成为唯一可行方案。 <div>
arXiv:2601.03413v1 Announce Type: new 
Abstract: This study highlights the potential of image-based reinforcement learning methods for addressing swarm-related tasks. In multi-agent reinforcement learning, effective policy learning depends on how agents sense, interpret, and process inputs. Traditional approaches often rely on handcrafted feature extraction or raw vector-based representations, which limit the scalability and efficiency of learned policies concerning input order and size. In this work we propose an image-based reinforcement learning method for decentralized control of a multi-agent system, where observations are encoded as structured visual inputs that can be processed by Neural Networks, extracting its spatial features and producing novel decentralized motion control rules. We evaluate our approach on a multi-agent convergence task of agents with limited-range and bearing-only sensing that aim to keep the swarm cohesive during the aggregation. The algorithm's performance is evaluated against two benchmarks: an analytical solution proposed by Bellaiche and Bruckstein, which ensures convergence but progresses slowly, and VariAntNet, a neural network-based framework that converges much faster but shows medium success rates in hard constellations. Our method achieves high convergence, with a pace nearly matching that of VariAntNet. In some scenarios, it serves as the only practical alternative.
]]></content:encoded>
<pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Complexity of Perfect and Ideal Resilience Verification in Fast Re-Route Networks</title>
<link>https://arxiv.org/abs/2601.03934</link>
<guid>https://arxiv.org/abs/2601.03934</guid>
<content:encoded><![CDATA[
<div> 关键词：快速重路由、完美弹性、计算复杂性、coNP完全、静态规则<br /><br />总结:  
1. 现代通信网络采用去中心化的快速重路由机制，依赖节点上预设的静态规则，仅基于本地链路故障信息进行恢复。  
2. 理想情况下，网络应具备“完美弹性”，即只要源与目的地物理连通，数据包总能成功送达。  
3. 作者研究了判断给定静态重路由规则是否实现完美弹性的计算复杂性，证明该问题是coNP完全的。  
4. 同样地，较弱的“理想弹性”判定问题也是coNP完全，且该结论适用于具有特定模式（如“跳过”）的规则。  
5. 积极方面，当节点不依赖入端口信息时，可在线性时间内完成完美弹性的验证与规则合成。 <div>
arXiv:2601.03934v1 Announce Type: new 
Abstract: To achieve fast recovery from link failures, most modern communication networks feature fully decentralized fast re-routing mechanisms. These re-routing mechanisms rely on pre-installed static re-routing rules at the nodes (the routers), which depend only on local failure information, namely on the failed links incident to the node. Ideally, a network is perfectly resilient: the re-routing rules ensure that packets are always successfully routed to their destinations as long as the source and the destination are still physically connected in the underlying network after the failures. Unfortunately, there are examples where achieving perfect resilience is not possible. Surprisingly, only very little is known about the algorithmic aspect of when and how perfect resilience can be achieved.
  We investigate the computational complexity of analyzing such local fast re-routing mechanisms. Our main result is a negative one: we show that even checking whether a given set of static re-routing rules ensures perfect resilience is coNP-complete. We also show coNP-completeness of the so-called ideal resilience, a weaker notion of resilience often considered in the literature. Additionally, we investigate other fundamental variations of the problem. In particular, we show that our coNP-completeness proof also applies to scenarios where the re-routing rules have specific patterns (known as skipping in the literature).
  On the positive side, for scenarios where nodes do not have information about the link from which a packet arrived (the so-called in-port), we present linear-time algorithms for both the verification and synthesis problem for perfect resilience.
]]></content:encoded>
<pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>On-Device Deep Reinforcement Learning for Decentralized Task Offloading Performance trade-offs in the training process</title>
<link>https://arxiv.org/abs/2601.03976</link>
<guid>https://arxiv.org/abs/2601.03976</guid>
<content:encoded><![CDATA[
<div> 边缘计算 深度强化学习 任务卸载 5G通信 能效权衡<br /><br />总结:  
1. 文章提出一种去中心化的深度强化学习（DRL）智能体，用于在边缘设备、多接入边缘计算（MEC）服务器和云服务器之间智能选择任务执行位置。  
2. 与多数现有研究不同，该方法在真实测试平台上进行验证，平台包含多种边缘设备并通过5G网络连接MEC与云服务器。  
3. 研究不仅评估了DRL智能体在满足任务需求方面的性能，还分析了本地运行该智能体的可行性。  
4. 特别关注了本地训练与远程训练在延迟和能耗方面的权衡，为资源受限设备的任务卸载决策提供实用参考。 <div>
arXiv:2601.03976v1 Announce Type: new 
Abstract: Allowing less capable devices to offload computational tasks to more powerful devices or servers enables the development of new applications that may not run correctly on the device itself. Deciding where and why to run each of those applications is a complex task. Therefore, different approaches have been adopted to make offloading decisions. In this work, we propose a decentralized Deep Reinforcement Learning (DRL) agent to address the selection of computing locations. Unlike most existing work, we analyze it in a real testbed composed of various edge devices running the agent to determine where to execute each task. These devices are connected to a Multi-Access Edge Computing (MEC) server and a Cloud server through 5G communications. We evaluate not only the agent's performance in meeting task requirements but also the implications of running this type of agent locally, assessing the trade-offs of training locally versus remotely in terms of latency and energy consumption.
]]></content:encoded>
<pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provably Convergent Decentralized Optimization over Directed Graphs under Generalized Smoothness</title>
<link>https://arxiv.org/abs/2601.03566</link>
<guid>https://arxiv.org/abs/2601.03566</guid>
<content:encoded><![CDATA[
<div> 关键词：去中心化优化、广义光滑性、梯度追踪、梯度裁剪、异构数据

<br /><br />总结:  
1. 针对传统Lipschitz光滑性假设在梯度快速变化问题中失效的局限，文章引入更通用的$(L_0, L_1)$-光滑性框架。  
2. 在该框架下，Hessian范数可随梯度范数线性增长，能更好刻画现实中的非平稳目标函数。  
3. 提出结合梯度追踪与梯度裁剪的新算法，并精心设计裁剪阈值，确保在有向通信图下收敛。  
4. 与现有方法不同，该方法不要求梯度差异有界，适用于梯度差异无界的异构数据场景。  
5. 在LIBSVM和CIFAR-10等基准数据集上的实验表明，该方法在稳定性和收敛速度上优于现有算法。 <div>
arXiv:2601.03566v1 Announce Type: cross 
Abstract: Decentralized optimization has become a fundamental tool for large-scale learning systems; however, most existing methods rely on the classical Lipschitz smoothness assumption, which is often violated in problems with rapidly varying gradients. Motivated by this limitation, we study decentralized optimization under the generalized $(L_0, L_1)$-smoothness framework, in which the Hessian norm is allowed to grow linearly with the gradient norm, thereby accommodating rapidly varying gradients beyond classical Lipschitz smoothness. We integrate gradient-tracking techniques with gradient clipping and carefully design the clipping threshold to ensure accurate convergence over directed communication graphs under generalized smoothness. In contrast to existing distributed optimization results under generalized smoothness that require a bounded gradient dissimilarity assumption, our results remain valid even when the gradient dissimilarity is unbounded, making the proposed framework more applicable to realistic heterogeneous data environments. We validate our approach via numerical experiments on standard benchmark datasets, including LIBSVM and CIFAR-10, using regularized logistic regression and convolutional neural networks, demonstrating superior stability and faster convergence over existing methods.
]]></content:encoded>
<pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-agent Optimization of Non-cooperative Multimodal Mobility Systems</title>
<link>https://arxiv.org/abs/2601.03777</link>
<guid>https://arxiv.org/abs/2601.03777</guid>
<content:encoded><![CDATA[
<div> 多模态交通、非合作决策者、均衡定价、出行者偏好、空驶里程<br /><br />总结:<br />1. 多模态交通系统涉及出行者与网约车司机等非合作决策者，其自利行为可能损害系统整体效率。<br />2. 研究提出一个统一的数学建模框架，通过均衡定价协调供需，分析分散决策对系统效率的影响。<br />3. 模型可凸化以高效计算均衡价格，并用于政策制定。<br />4. 数值实验表明：价格敏感度高的出行者更倾向选择网约车或多模态出行；当公交枢纽较少或司机对价格过于敏感时，需对多模态出行进行补贴。<br />5. 增加公交枢纽虽提升多模态使用率，却因增加司机调度时间而提高空驶里程，需权衡可达性与环境影响。 <div>
arXiv:2601.03777v1 Announce Type: cross 
Abstract: While multimodal mobility systems have the potential to bring many benefits to travelers, drivers, the environment, and traffic congestion, such systems typically involve multiple non-cooperative decision-makers who may selfishly optimize their own objectives without considering the overall system benefits. This paper aims to investigate market-based interactions of travelers and ride-sourcing drivers in the context of multimodal mobility systems. We propose a unified mathematical modeling framework to capture the decentralized travelers and drivers' decision-making process and balance the network's demand and supply by equilibrium pricing. Such a model allows analyses of the impact of decentralized decision-making on multimodal mobility efficiencies. The proposed formulation can be further convexified to efficiently compute the equilibrium ride-sourcing prices. We conduct numerical experiments on different settings of transportation networks to gain policy insights. We find that travelers prefer ride-sourcing and multimodal transportation more than the driving option when they are more sensitive to prices. We also find that travelers may need to be subsidized to use multimodal transportation when there is fewer transit hubs in the network or, ride-sourcing drivers become too sensitive to the prices. However, we find that more transit hubs in the network increases the total empty VMT of ride-sourcing drivers by increasing the total relocation time. The proposed model can be used by policymakers and platform operators to design pricing and subsidy schemes that align individual decision-making with system-level efficiency and evaluate the trade-offs between accessibility and environmental impacts in multimodal transportation networks.
]]></content:encoded>
<pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stigmergic optimal transport</title>
<link>https://arxiv.org/abs/2601.04111</link>
<guid>https://arxiv.org/abs/2601.04111</guid>
<content:encoded><![CDATA[
<div> 关键词：群体导航、随机最优控制、路径优化、环境异质性、费马原理<br /><br />总结:  
1. 研究提出一种基于随机最优控制理论的群体导航框架，利用“痕迹介导”（stigmergy）机制实现去中心化路径优化。  
2. 个体通过在环境中留下并跟随信息素类轨迹，共同最小化预期穿越时间。  
3. 模拟与分析揭示两种涌现行为：在均匀环境中路径趋于直线，在不同介质交界处发生路径折射。  
4. 这些行为与昆虫觅食路径的实验观察一致，体现出类似费马原理的最短时间特性。  
5. 整个系统无需全局信息或中央协调，依靠局部、含噪的智能体-场交互及快慢动力学机制，即可在异质环境中生成测地线轨迹。 <div>
arXiv:2601.04111v1 Announce Type: cross 
Abstract: Efficient navigation in swarms often relies on the emergence of decentralized approaches that minimize traversal time or energy. Stigmergy, where agents modify a shared environment that then modifies their behavior, is a classic mechanism that can encode this strategy. We develop a theoretical framework for stigmergic transport by casting it as a stochastic optimal control problem: agents (collectively) lay and (individually) follow trails while minimizing expected traversal time. Simulations and analysis reveal two emergent behaviors: path straightening in homogeneous environments and path refraction at material interfaces, both consistent with experimental observations of insect trails. While reminiscent of Fermat's principle, our results show how local, noisy agent+field interactions can give rise to geodesic trajectories in heterogeneous environments, without centralized coordination or global knowledge, relying instead on an embodied slow fast dynamical mechanism.
]]></content:encoded>
<pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>IoTChain: A Three-Tier Blockchain-based IoT Security Architecture</title>
<link>https://arxiv.org/abs/1806.02008</link>
<guid>https://arxiv.org/abs/1806.02008</guid>
<content:encoded><![CDATA[
<div> 区块链 物联网 安全架构 身份认证 访问控制<br /><br />总结:  
1. 文章提出了一种基于区块链的物联网安全架构IoTchain，旨在提升物联网设备与系统的安全性。  
2. 该架构采用三层设计，包括认证层、区块链层和应用层。  
3. 架构支持身份认证、访问控制、隐私保护、轻量化特性、区域节点容错、抗拒绝服务攻击及存储完整性保障。  
4. 研究还对IoTchain的性能进行了评估，验证其在实际物联网部署中的实用性。 <div>
arXiv:1806.02008v3 Announce Type: replace 
Abstract: There has been increasing interest in the potential of blockchain in enhancing the security of devices and systems, such as Internet of Things (IoT). In this paper, we present a blockchain-based IoT security architecture, IoTchain. The three-tier architecture comprises an authentication layer, a blockchain layer and an application layer, and is designed to achieve identity authentication, access control, privacy protection, lightweight feature, regional node fault tolerance, denial-of-service resilience, and storage integrity. We also evaluate the performance of IoTchain to demonstrate its utility in an IoT deployment.
]]></content:encoded>
<pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Game based Distributed Approach for General Multi-agent Optimal Coverage with Application to UAV Networks</title>
<link>https://arxiv.org/abs/2408.01193</link>
<guid>https://arxiv.org/abs/2408.01193</guid>
<content:encoded><![CDATA[
<div> 最优覆盖问题、多智能体系统、分布式决策、博弈论、无人机网络<br /><br />总结:  
1. 针对多智能体系统的最优覆盖问题（OCP），提出一种基于博弈论的分布式决策方法。  
2. 该方法采用去中心化优化机制，仅需局部信息即可运行，有效降低大规模系统带来的高计算成本。  
3. 所设计的分布式算法被证明能收敛至接近全局最优的覆盖效果。  
4. 方法应用于无人机（UAV）网络，以最大化目标区域的覆盖面积。  
5. 仿真实验表明，相比其他典型分布式算法，该方法计算时间更少、收敛更快；与集中式优化相比，其结果近似最优且计算效率更高。 <div>
arXiv:2408.01193v3 Announce Type: replace 
Abstract: This paper focuses on the optimal coverage problem (OCP) for multi-agent systems with a decentralized optimization mechanism. A game based distributed decision-making method for the multi-agent OCP is proposed to address the high computational costs arising from the large scale of the multi-agent system and to ensure that the game's equilibrium achieves the global performance objective's maximum value. In particular, a distributed algorithm that needs only local information is developed and proved to converge to near-optimal global coverage. Finally, the proposed method is applied to maximize the coverage area of the UAV network for a target region. The simulation results show that our method can require much less computational time than other typical distributed algorithms in related work, while achieving a faster convergence rate. Comparison with centralized optimization also demonstrates that the proposed method has approximate optimization results and high computation efficiency.
]]></content:encoded>
<pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Clustering: An Unsupervised Cluster-Wise Training for Decentralized Data Distributions</title>
<link>https://arxiv.org/abs/2408.10664</link>
<guid>https://arxiv.org/abs/2408.10664</guid>
<content:encoded><![CDATA[
<div> 联邦学习、无监督学习、聚类、数据分布、模型协同训练<br /><br />总结:  
1. 本文提出一种面向无监督场景的联邦学习新方法——FedCRef，用于在无标签、非均匀数据分布下识别全局类别（global K），即联邦聚类。  
2. 各客户端首先基于本地数据分布（local K）训练模型，生成压缩数据表示，并通过重建误差分析共享模型以形成联邦协作组。  
3. 在组内，客户端协同训练共享模型以表征各自的数据分布，并持续优化本地聚类，提升数据关联准确性。  
4. 该迭代机制能有效识别网络中所有潜在数据分布，并为每类构建鲁棒的表示模型。  
5. 实验在EMNIST和KMNIST数据集上验证了FedCRef优于传统集中式方法，在无监督联邦设定下显著提升数据表示精度。 <div>
arXiv:2408.10664v2 Announce Type: replace 
Abstract: Federated Learning (FL) is a pivotal approach in decentralized machine learning, especially when data privacy is crucial and direct data sharing is impractical. While FL is typically associated with supervised learning, its potential in unsupervised scenarios is underexplored. This paper introduces a novel unsupervised federated learning methodology designed to identify the complete set of categories (global K) across multiple clients within label-free, non-uniform data distributions, a process known as Federated Clustering. Our approach, Federated Cluster-Wise Refinement (FedCRef), involves clients that collaboratively train models on clusters with similar data distributions. Initially, clients with diverse local data distributions (local K) train models on their clusters to generate compressed data representations. These local models are then shared across the network, enabling clients to compare them through reconstruction error analysis, leading to the formation of federated groups.In these groups, clients collaboratively train a shared model representing each data distribution, while continuously refining their local clusters to enhance data association accuracy. This iterative process allows our system to identify all potential data distributions across the network and develop robust representation models for each. To validate our approach, we compare it with traditional centralized methods, establishing a performance baseline and showcasing the advantages of our distributed solution. We also conduct experiments on the EMNIST and KMNIST datasets, demonstrating FedCRef's ability to refine and align cluster models with actual data distributions, significantly improving data representation precision in unsupervised federated settings.
]]></content:encoded>
<pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Noncooperative Consensus via a Trading-based Auction</title>
<link>https://arxiv.org/abs/2502.03616</link>
<guid>https://arxiv.org/abs/2502.03616</guid>
<content:encoded><![CDATA[
<div> 非合作多智能体系统、协调挑战、自利行为、去中心化共识、公平资源分配<br /><br />总结:  
1. 非合作多智能体系统中，由于各智能体偏好冲突且追求自利，常导致次优结果或安全隐患。  
2. 提出名为TACo（Trading Auction for Consensus）的去中心化算法，使智能体无需直接通信或披露私有估值即可达成共识。  
3. TACo通过结构化的基于交易的拍卖机制，让智能体迭代选择偏好选项，并在预先确定的步数内可证明地达成一致。  
4. 数值实验验证了TACo在实践中具备可靠的终止性保证。  
5. 该算法在最小化全体智能体总成本的同时，显著提升了资源分配的公平性，优于基线方法。 <div>
arXiv:2502.03616v3 Announce Type: replace 
Abstract: Noncooperative multi-agent systems often face coordination challenges due to conflicting preferences among agents. In particular, when agents act in their own self-interest, they may prefer different choices among multiple feasible outcomes, leading to suboptimal outcomes or even safety concerns. We propose an algorithm named trading auction for consensus (TACo), a decentralized approach that enables noncooperative agents to reach consensus without communicating directly or disclosing private valuations. TACo facilitates coordination through a structured trading-based auction, where agents iteratively select choices of interest and provably reach an agreement within an a priori bounded number of steps. A series of numerical experiments validate that the termination guarantees of TACo hold in practice, and show that TACo achieves a median performance that minimizes the total cost across all agents, while allocating resources significantly more fairly than baseline approaches.
]]></content:encoded>
<pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>QoS-Aware Load Balancing in the Computing Continuum via Multi-Player Bandits</title>
<link>https://arxiv.org/abs/2512.18915</link>
<guid>https://arxiv.org/abs/2512.18915</guid>
<content:encoded><![CDATA[
<div> 边缘计算、服务质量（QoS）、负载均衡、多玩家多臂老虎机（MP-MAB）、核密度估计（KDE）<br /><br />总结:<br />1. 随着计算从云端向边缘迁移，形成动态的计算连续体（CC），保障每个客户端的服务质量（QoS）变得更具挑战性。<br />2. 现有方法多关注全局指标，忽视了对延迟敏感和可靠性要求高的应用所需的单客户端QoS保障。<br />3. 提出QEdgeProxy——一种去中心化的QoS感知负载均衡器，部署于物联网设备与服务实例之间。<br />4. 将负载均衡建模为具有异构奖励的多玩家多臂老虎机（MP-MAB）问题，利用核密度估计（KDE）估算满足QoS的成功概率，并引入自适应探索机制以应对非平稳环境。<br />5. 在基于K3s集群的真实网络条件下进行实验，结果表明QEdgeProxy在单客户端QoS满足率上显著优于基于邻近性和强化学习的基线方法，并能有效应对负载突增和服务实例变动。 <div>
arXiv:2512.18915v2 Announce Type: replace 
Abstract: As computation shifts from the cloud to the edge to reduce processing latency and network traffic, the resulting Computing Continuum (CC) creates a dynamic environment where meeting strict Quality of Service (QoS) requirements and avoiding service instance overload becomes challenging. Existing methods often prioritize global metrics and overlook per-client QoS, which is crucial for latency-sensitive and reliability-critical applications. We propose QEdgeProxy, a decentralized QoS-aware load balancer that acts as a proxy between IoT devices and service instances in the CC. We formulate the load balancing problem as a Multi-Player Multi-Armed Bandit (MP-MAB) with heterogeneous rewards: Each load balancer autonomously selects service instances to maximize the probability of meeting its clients' QoS requirements by using Kernel Density Estimation (KDE) to estimate QoS success probabilities. Our load-balancing algorithm also incorporates an adaptive exploration mechanism to recover rapidly from performance shifts and non-stationary conditions. We present a Kubernetes-native QEdgeProxy implementation and evaluate it on an emulated CC testbed deployed on a K3s cluster with realistic network conditions and a latency-sensitive edge-AI workload. Results show that QEdgeProxy significantly outperforms proximity-based and reinforcement-learning baselines in per-client QoS satisfaction, while adapting effectively to load surges and changes in instance availability.
]]></content:encoded>
<pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>APoW: Auditable Proof-of-Work Against Block Withholding Attacks</title>
<link>https://arxiv.org/abs/2601.02496</link>
<guid>https://arxiv.org/abs/2601.02496</guid>
<content:encoded><![CDATA[
<div> APoW、工作量证明、可审计性、矿池、区块扣留攻击<br /><br />总结:  
1. 提出了一种新型工作量证明机制APoW，受Hashcash启发，支持对其他矿工工作的可追溯审计。  
2. 矿工可概率性地证明其在先前挖矿轮次中搜索了特定nonce空间区域，同时继续为新区块或矿池份额进行有效挖矿并获得奖励。  
3. 该机制使矿池成员能事后审计他人所声称的算力投入，从而在无需可信硬件或第三方的情况下概率性检测区块扣留攻击（BWA）。  
4. APoW有助于构建去中心化矿池，实现可验证的工作归属，并大幅降低矿工实施扣留攻击的动机。  
5. 该方案保留了传统PoW的公开可验证性和难度调整等核心特性，并可在不改变比特币共识规则的前提下，作为矿池层面的审计机制使用。 <div>
arXiv:2601.02496v1 Announce Type: new 
Abstract: We introduce APoW, a novel proof-of-work (PoW) construction inspired by Hashcash-style nonce searching, which enables the auditing of other miners' work through accountable re-scanning of the nonce space. The proposed scheme allows a miner to probabilistically attest to having searched specified regions of the nonce space in earlier mining rounds, while concurrently earning rewards for performing productive work for a new block or pool share. This capability enables miners belonging to a mining pools to audit another miner's claimed effort retroactively, thereby allowing the probabilistic detection of block withholding attacks (BWAs) without requiring trusted hardware or trusted third parties. As a consequence, the construction supports the design of decentralized mining pools in which work attribution is verifiable and withholding incentives are substantially reduced. The scheme preserves the fundamental properties of conventional PoW, including public verifiability and difficulty adjustment, while adding an orthogonal auditability layer tailored to pool-based mining. Finally, while a full deployment of APoW in Bitcoin would require a consensus rule change and minor modifications to mining ASICs, the construction remains practically useful even without consensus changes, for instance, as a pool-level auditing mechanism that enables verifiable pay-for-auditing using existing pool reserves.
]]></content:encoded>
<pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Topology-Independent Robustness of the Weighted Mean under Label Poisoning Attacks in Heterogeneous Decentralized Learning</title>
<link>https://arxiv.org/abs/2601.02682</link>
<guid>https://arxiv.org/abs/2601.02682</guid>
<content:encoded><![CDATA[
<div> 关键词：标签投毒攻击、去中心化梯度下降、鲁棒聚合器、加权平均聚合器、网络拓扑<br /><br />总结:  
1. 文章研究去中心化信号处理与机器学习系统在标签投毒攻击下的鲁棒性问题。  
2. 对比分析了鲁棒聚合器与常被视为脆弱的加权平均聚合器的性能。  
3. 理论表明，鲁棒聚合器的学习误差受网络拓扑影响，而加权平均聚合器性能与拓扑无关。  
4. 在特定条件下（如全局污染率低于局部污染率、正常节点网络不连通或稀疏且局部污染率高），加权平均聚合器反而优于鲁棒聚合器。  
5. 实验结果验证了理论分析，强调网络拓扑在抵御标签投毒攻击中的关键作用。 <div>
arXiv:2601.02682v1 Announce Type: new 
Abstract: Robustness to malicious attacks is crucial for practical decentralized signal processing and machine learning systems. A typical example of such attacks is label poisoning, meaning that some agents possess corrupted local labels and share models trained on these poisoned data. To defend against malicious attacks, existing works often focus on designing robust aggregators; meanwhile, the weighted mean aggregator is typically considered a simple, vulnerable baseline. This paper analyzes the robustness of decentralized gradient descent under label poisoning attacks, considering both robust and weighted mean aggregators. Theoretical results reveal that the learning errors of robust aggregators depend on the network topology, whereas the performance of weighted mean aggregator is topology-independent. Remarkably, the weighted mean aggregator, although often considered vulnerable, can outperform robust aggregators under sufficient heterogeneity, particularly when: (i) the global contamination rate (i.e., the fraction of poisoned agents for the entire network) is smaller than the local contamination rate (i.e., the maximal fraction of poisoned neighbors for the regular agents); (ii) the network of regular agents is disconnected; or (iii) the network of regular agents is sparse and the local contamination rate is high. Empirical results support our theoretical findings, highlighting the important role of network topology in the robustness to label poisoning attacks.
]]></content:encoded>
<pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Preserving AI-Enabled Decentralized Learning and Employment Records System</title>
<link>https://arxiv.org/abs/2601.02720</link>
<guid>https://arxiv.org/abs/2601.02720</guid>
<content:encoded><![CDATA[
<div> 学习与就业记录（LER）、区块链、可信执行环境（TEE）、自然语言处理（NLP）、技能凭证<br /><br />总结:<br />1. 提出了一种基于AI和隐私保护的去中心化学习与就业记录（LER）系统，弥补现有区块链平台在自动技能凭证生成和非结构化学习证据整合方面的不足。<br />2. 系统通过可信执行环境（TEE）内的自然语言处理（NLP）管道，从正式成绩单、教学大纲及非正式学习成果中提取并生成可验证的自我签发技能凭证。<br />3. 所有验证与岗位匹配均在TEE内完成，仅披露必要信息，确保原始凭证和私钥不外泄。<br />4. 岗位匹配仅依赖经认证的技能向量，忽略简历中非技能字段，有效减少筛选偏见。<br />5. NLP模块采用Syllabus-to-O*NET方法验证，技能提取结果稳定（重复运行方差<5%），且系统具备形式化安全保证，确保凭证不可伪造、数据保密。 <div>
arXiv:2601.02720v1 Announce Type: new 
Abstract: Learning and Employment Record (LER) systems are emerging as critical infrastructure for securely compiling and sharing educational and work achievements. Existing blockchain-based platforms leverage verifiable credentials but typically lack automated skill-credential generation and the ability to incorporate unstructured evidence of learning. In this paper,a privacy-preserving, AI-enabled decentralized LER system is proposed to address these gaps. Digitally signed transcripts from educational institutions are accepted, and verifiable self-issued skill credentials are derived inside a trusted execution environment (TEE) by a natural language processing pipeline that analyzes formal records (e.g., transcripts, syllabi) and informal artifacts. All verification and job-skill matching are performed inside the enclave with selective disclosure, so raw credentials and private keys remain enclave-confined. Job matching relies solely on attested skill vectors and is invariant to non-skill resume fields, thereby reducing opportunities for screening bias.The NLP component was evaluated on sample learner data; the mapping follows the validated Syllabus-to-O*NET methodology,and a stability test across repeated runs observed <5% variance in top-ranked skills. Formal security statements and proof sketches are provided showing that derived credentials are unforgeable and that sensitive information remains confidential. The proposed system thus supports secure education and employment credentialing, robust transcript verification,and automated, privacy-preserving skill extraction within a decentralized framework.
]]></content:encoded>
<pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Preemptive Holistic Collaborative Systems for Embodied Multi-Agent Systems: Framework, Hybrid Stability, and Scalability Analysis</title>
<link>https://arxiv.org/abs/2601.02779</link>
<guid>https://arxiv.org/abs/2601.02779</guid>
<content:encoded><![CDATA[
<div> 安全、可扩展性、效率、分层协同、混合自动机<br /><br />总结:  
1. 针对具身多智能体系统在受限物理环境中的协调问题，需兼顾安全、可扩展性与效率。  
2. 传统去中心化方法易陷入局部最优或相互避让僵局，而中心化规划则面临计算复杂度高和单点故障风险。  
3. 提出分层抢占式整体协同（Prollect）框架，将全局协调分解为拓扑连接的子空间优化问题。  
4. 系统建模为混合自动机，并采用三阶段滚动时域机制，通过显式填充避免协调传播与意图更新之间的竞争。  
5. 设计含强制空闲缓冲区的鲁棒时序协议以消除Zeno行为，并引入影子智能体协议，确保子空间边界处轨迹一致性，建模为输入-状态稳定性（ISS）问题。 <div>
arXiv:2601.02779v1 Announce Type: new 
Abstract: The coordination of Embodied Multi-Agent Systems in constrained physical environments requires a rigorous balance between safety, scalability, and efficiency. Traditional decentralized approaches, e.g., reactive collision avoidance, are prone to local minima or reciprocal yielding standoffs due to the lack of future intent awareness. In contrast, centralized planning suffers from intractable computational complexity and single-point-of-failure vulnerabilities. To address these limitations, we propose the Hierarchical Preemptive Holistic Collaborative (Prollect) framework, which generalizes the Preemptive Holistic Collaborative System (PHCS) by decomposing the global coordination problem into topologically connected subspace optimizations. We formalize the system as a Hybrid Automaton and introduce a three-stage receding horizon mechanism (frozen execution, preliminary planning, proactive look-ahead windows) with explicit padding to prevent races between coordination dissemination and intent updates. Notably, we design a robust timing protocol with a mandatory Idle Buffer that acts as a dwell-time constraint to eliminate Zeno behaviors and ensure computational stability under jitter. Furthermore, we formalize a Shadow Agent protocol to guarantee seamless trajectory consistency across subspace boundaries, which we treat as an Input-to-State Stability (ISS) problem.
]]></content:encoded>
<pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributionally Robust Game for Proof-of-Work Blockchain Mining Under Resource Uncertainties</title>
<link>https://arxiv.org/abs/2601.02804</link>
<guid>https://arxiv.org/abs/2601.02804</guid>
<content:encoded><![CDATA[
<div> 区块链、工作量证明（PoW）、资源不确定性、分布鲁棒优化、非合作博弈<br /><br />总结:<br />1. 文章研究了在存在计算资源不确定性的环境下，工作量证明（PoW）区块链的挖矿问题。<br />2. 通过构建资源分布的模糊集，刻画不同矿工的资源不确定性。<br />3. 将网络挖矿建模为非合作博弈，并引入分布鲁棒性能指标以应对不确定性。<br />4. 证明了该分布鲁棒挖矿博弈均衡的存在性，并利用条件风险价值（CVaR）重新解释矿工的最佳响应策略。<br />5. 提出交替优化算法求解个体策略，并在高斯分布等特殊情形下设计分布式算法；仿真实验验证了方法的有效性和鲁棒性。 <div>
arXiv:2601.02804v1 Announce Type: new 
Abstract: Blockchain plays a crucial role in ensuring the security and integrity of decentralized systems, with the proof-of-work (PoW) mechanism being fundamental for achieving distributed consensus. As PoW blockchains see broader adoption, an increasingly diverse set of miners with varying computing capabilities participate in the network. In this paper, we consider the PoW blockchain mining, where the miners are associated with resource uncertainties. To characterize the uncertainty computing resources at different mining participants, we establish an ambiguous set representing uncertainty of resource distributions. Then, the networked mining is formulated as a non-cooperative game, where distributionally robust performance is calculated for each individual miner to tackle the resource uncertainties. We prove the existence of the equilibrium of the distributionally robust mining game. To derive the equilibrium, we propose the conditional value-at-risk (CVaR)-based reinterpretation of the best response of each miner. We then solve the individual strategy with alternating optimization, which facilitates the iteration among miners towards the game equilibrium. Furthermore, we consider the case that the ambiguity of resource distribution reduces to Gaussian distribution and the case that another uncertainties vanish, and then characterize the properties of the equilibrium therein along with a distributed algorithm to achieve the equilibrium. Simulation results show that the proposed approaches effectively converge to the equilibrium, and effectively tackle the uncertainties in blockchain mining to achieve a robust performance guarantee.
]]></content:encoded>
<pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Blockchain Interoperability: Frameworks, Use Cases, and Future Challenges</title>
<link>https://arxiv.org/abs/2601.02949</link>
<guid>https://arxiv.org/abs/2601.02949</guid>
<content:encoded><![CDATA[
<div> 区块链 互操作性 跨链通信 可信环境 异构区块链<br /><br />总结:<br />1. 区块链旨在在无第三方信任的场景中建立数字信任；<br />2. 早期区块链无法与其他链共享信息，限制了复杂应用的发展；<br />3. 随着数据量和应用复杂度增加，跨链互操作性成为必要；<br />4. 文章探讨了支持异构区块链互联的若干互操作平台；<br />5. 通过案例说明互操作方案的价值，并指出当前仍需解决的关键问题。 <div>
arXiv:2601.02949v1 Announce Type: new 
Abstract: Trust between entities in any scenario without a trusted third party is very difficult, and trust is exactly what blockchain aims to bring into the digital world with its basic features. Many applications are moving to blockchain adoption, enabling users to work in a trustworthy manner. The early generations of blockchain have a problem; they cannot share information with other blockchains. As more and more entities move their applications to the blockchain, they generate large volumes of data, and as applications have become more complex, sharing information between different blockchains has become a necessity. This has led to the research and development of interoperable solutions allowing blockchains to connect together. This paper discusses a few blockchain platforms that provide interoperable solutions, emphasising their ability to connect heterogeneous blockchains. It also discusses a case study scenario to illustrate the importance and benefits of using interoperable solutions. We also present a few topics that need to be solved in the realm of interoperability.
]]></content:encoded>
<pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selfish Mining in Multi-Attacker Scenarios: An Empirical Evaluation of Nakamoto, Fruitchain, and Strongchain</title>
<link>https://arxiv.org/abs/2601.02984</link>
<guid>https://arxiv.org/abs/2601.02984</guid>
<content:encoded><![CDATA[
<div> 自私挖矿、共识协议、多攻击者、随机模拟框架、区块链安全<br /><br />总结:<br />1. 该研究旨在通过深入理解自私挖矿攻击，提升区块链安全性，特别关注可缓解此类攻击的共识协议。<br />2. 以往研究多限于单一攻击者和特定协议，缺乏对多个攻击者场景的分析。<br />3. 作者提出一个随机模拟框架，支持在多种共识协议下分析多攻击者的自私挖矿行为。<br />4. 框架建模了PoW Nakamoto共识（基准）、Fruitchain和Strongchain三种协议，并验证了已有阈值，同时发现了多个新阈值。<br />5. 研究公开了框架源代码，便于后续研究者评估新协议并进行横向比较。 <div>
arXiv:2601.02984v1 Announce Type: new 
Abstract: The aim of this work is to enhance blockchain security by deepening the understanding of selfish mining attacks in various consensus protocols, especially the ones that have the potential to mitigate selfish mining. Previous research was mainly focused on a particular protocol with a single selfish miner, while only limited studies have been conducted on two or more attackers. To address this gap, we proposed a stochastic simulation framework that enables analysis of selfish mining with multiple attackers across various consensus protocols. We created the model of Proof-of-Work (PoW) Nakamoto consensus (serving as the baseline) as well as models of two additional consensus protocols designed to mitigate selfish mining: Fruitchain and Strongchain. Using our framework, thresholds reported in the literature were verified, and several novel thresholds were discovered for 2 and more attackers. We made the source code of our framework available, enabling researchers to evaluate any newly added protocol with one or more selfish miners and cross-compare it with already modeled protocols.
]]></content:encoded>
<pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Autoregressive Generation</title>
<link>https://arxiv.org/abs/2601.03184</link>
<guid>https://arxiv.org/abs/2601.03184</guid>
<content:encoded><![CDATA[
<div> 去中心化、自回归生成、离散流匹配、多模态语言模型、指令微调<br /><br />总结:<br />1. 文章提出了一种自回归生成去中心化的理论分析框架。<br />2. 定义了“去中心化离散流匹配”目标，将概率生成速度表示为专家流的线性组合。<br />3. 通过实验验证了在多模态语言模型中，去中心化训练与集中式训练在多种基准上效果等价。<br />4. 实验对比了LLaVA和InternVL 2.5-1B两种范式。<br />5. InternVL 2.5-1B采用固定CLIP视觉编码器，并在指令微调阶段对ViT、MLP和大语言模型（LLM）进行全参数微调。 <div>
arXiv:2601.03184v1 Announce Type: new 
Abstract: We present a theoretical analysis of decentralization of autoregressive generation. We define the Decentralized Discrete Flow Matching objective, by expressing probability generating velocity as a linear combination of expert flows. We also conduct experiments demonstrat- ing the equivalence between decentralized and centralized training settings for multimodal language models across diverse set of benchmarks. Specifically, we compare two distinct paradigms: LLaVA and InternVL 2.5-1B, which uses a fixed CLIP vision encoder and per- forms full-parameter fine-tuning (ViT+MLP+LLM) during the instruction tuning stage.
]]></content:encoded>
<pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SLVC-DIDA: Signature-less Verifiable Credential-based Issuer-hiding and Multi-party Authentication for Decentralized Identity</title>
<link>https://arxiv.org/abs/2501.11052</link>
<guid>https://arxiv.org/abs/2501.11052</guid>
<content:encoded><![CDATA[
<div> Decentralized Identity, Verifiable Credential, 隐私保护, 无签名方案, 零知识证明<br /><br />总结:<br />1. 文章提出一种新型去中心化身份（DID）多方认证模型SLVC-DIDA，首次实现签发者隐藏与隐私保护。<br />2. 该方案摒弃传统依赖公钥基础设施（PKI）和签名密钥的方式，采用哈希与签发者成员证明机制。<br />3. 利用基于Merkle树的可验证凭证（VC）列表和新颖的零知识电路，实现身份属性隐私保护与签发者集合匿名性。<br />4. 支持通用的零知识多方DID认证，无需额外技术集成。<br />5. 实验验证了SLVC-DIDA的有效性与实用性，推动自主权身份认证的发展。 <div>
arXiv:2501.11052v3 Announce Type: replace 
Abstract: As an emerging paradigm in digital identity, Decentralized Identity (DID) appears advantages over traditional identity management methods in a variety of aspects, e.g., enhancing user-centric online services and ensuring complete user autonomy and control. Verifiable Credential (VC) techniques are used to facilitate decentralized DID-based access control across multiple entities. However, existing DID schemes generally rely on a distributed public key infrastructure that also causes challenges, such as context information deduction, key exposure, and issuer data leakage. To address the issues above, this paper proposes a issuer-hiding and privacy-preserving DID multi-party authentication model with a signature-less VC scheme, named SLVC-DIDA, for the first time. Our proposed scheme avoids the dependence on signing keys by employing hashing and issuer membership proofs, which supports universal zero-knowledge multi-party DID authentications, eliminating additional technical integrations. We adopt a novel zero-knowledge circuit to maintain the anonymity of the issuer set, thereby enabling public verification while safeguarding the privacy of identity attributes via a Merkle tree-based VC list. Furthermore, by eliminating reliance on a Public Key Infrastructure (PKI), SLVC-DIDA enables decentralized and self-sovereign DID authentication. Our experiments further evaluate the effectiveness and practicality of SLVC-DIDA.
]]></content:encoded>
<pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>A new type of federated clustering: A non-model-sharing approach</title>
<link>https://arxiv.org/abs/2506.10244</link>
<guid>https://arxiv.org/abs/2506.10244</guid>
<content:encoded><![CDATA[
<div> 联邦学习、聚类分析、数据协作、隐私保护、异构数据<br /><br />总结:<br />1. 针对现有联邦聚类方法仅支持简单数据划分（如横向或纵向分割）的局限，提出了一种新方法——数据协作聚类（DC-Clustering）。<br />2. 该方法支持横向与纵向分割共存的复杂分布式数据结构，各机构仅共享中间表示而非原始数据，保障隐私。<br />3. DC-Clustering 可灵活选用 k-means 或谱聚类算法，并仅需与中心服务器通信一次即可完成聚类。<br />4. 实验表明，其聚类性能接近集中式聚类（即所有数据集中处理）的效果。<br />5. 该方法具备隐私保护、通信高效和算法灵活等优势，适用于医疗、金融等对隐私敏感的领域。 <div>
arXiv:2506.10244v3 Announce Type: replace 
Abstract: In recent years, the growing need to leverage sensitive data across institutions has led to increased attention on federated learning (FL), a decentralized machine learning paradigm that enables model training without sharing raw data. However, existing FL-based clustering methods, known as federated clustering, typically assume simple data partitioning scenarios such as horizontal or vertical splits, and cannot handle more complex distributed structures. This study proposes data collaboration clustering (DC-Clustering), a novel federated clustering method that supports clustering over complex data partitioning scenarios where horizontal and vertical splits coexist. In DC-Clustering, each institution shares only intermediate representations instead of raw data, ensuring privacy preservation while enabling collaborative clustering. The method allows flexible selection between k-means and spectral clustering, and achieves final results with a single round of communication with the central server. We conducted extensive experiments using synthetic and open benchmark datasets. The results show that our method achieves clustering performance comparable to centralized clustering where all data are pooled. DC-Clustering addresses an important gap in current FL research by enabling effective knowledge discovery from distributed heterogeneous data. Its practical properties -- privacy preservation, communication efficiency, and flexibility -- make it a promising tool for privacy-sensitive domains such as healthcare and finance.
]]></content:encoded>
<pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convergence of Decentralized Stochastic Subgradient-based Methods for Nonsmooth Nonconvex functions</title>
<link>https://arxiv.org/abs/2403.11565</link>
<guid>https://arxiv.org/abs/2403.11565</guid>
<content:encoded><![CDATA[
<div> 关键词：去中心化、随机次梯度方法、非光滑非凸优化、微分包含、收敛性分析<br /><br />总结:<br />1. 本文研究在无Clarke正则性条件下，去中心化随机次梯度方法用于最小化非光滑非凸函数，尤其适用于非光滑神经网络的去中心化训练。<br />2. 提出一个统一框架，涵盖DSGD、带梯度追踪的DSGD-T及带动量的DSGD-M等多种算法。<br />3. 通过将离散迭代与连续时间微分包含的轨迹关联，并假设存在强制Lyapunov函数及稳定集$\mathcal{A}$，建立收敛性理论。<br />4. 证明在足够小且递减的步长下，迭代序列渐近收敛至稳定集$\mathcal{A}$。<br />5. 首次为若干经典去中心化随机次梯度方法在非Clarke正则目标函数下提供收敛保证，数值实验验证了所提框架的有效性。 <div>
arXiv:2403.11565v4 Announce Type: replace-cross 
Abstract: In this paper, we focus on the decentralized stochastic subgradient-based methods in minimizing nonsmooth nonconvex functions without Clarke regularity, especially in the decentralized training of nonsmooth neural networks. We propose a general framework that unifies various decentralized subgradient-based methods, such as decentralized stochastic subgradient descent (DSGD), DSGD with gradient-tracking technique (DSGD-T), and DSGD with momentum (DSGD-M). To establish the convergence properties of our proposed framework, we relate the discrete iterates to the trajectories of a continuous-time differential inclusion, which is assumed to have a coercive Lyapunov function with a stable set $\mathcal{A}$. We prove the asymptotic convergence of the iterates to the stable set $\mathcal{A}$ with sufficiently small and diminishing step-sizes. These results provide first convergence guarantees for some well-recognized of decentralized stochastic subgradient-based methods without Clarke regularity of the objective function. Preliminary numerical experiments demonstrate that our proposed framework yields highly efficient decentralized stochastic subgradient-based methods with convergence guarantees in the training of nonsmooth neural networks.
]]></content:encoded>
<pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multiagent Reinforcement Learning for Liquidity Games</title>
<link>https://arxiv.org/abs/2601.00324</link>
<guid>https://arxiv.org/abs/2601.00324</guid>
<content:encoded><![CDATA[
<div> 关键词：群体智能、流动性博弈、理性群体、差分奖励、市场效率<br /><br />总结:  
1. 本文融合群体智能与金融市场建模，提出“金融群体”模型，结合流动性博弈与理性群体理论。  
2. 在该模型中，交易者作为独立个体，其收益依赖于整体市场流动性。  
3. 通过引入差分奖励机制，在马尔可夫团队博弈框架下，个体在追求自身流动性最大化的同时，无需协调或合谋即可提升整体市场流动性。  
4. 该方法利用博弈论解释群体中自利个体如何实现集体效用。  
5. 模型为双边资产市场中实现个体盈利与市场效率的统一提供了理论框架。 <div>
arXiv:2601.00324v1 Announce Type: new 
Abstract: Making use of swarm methods in financial market modeling of liquidity, and techniques from financial analysis in swarm analysis, holds the potential to advance both research areas. In swarm research, the use of game theory methods holds the promise of explaining observed phenomena of collective utility adherence with rational self-interested swarm participants. In financial markets, a better understanding of how independent financial agents may self-organize for the betterment and stability of the marketplace would be a boon for market design researchers. This paper unifies Liquidity Games, where trader payoffs depend on aggregate liquidity within a trade, with Rational Swarms, where decentralized agents use difference rewards to align self-interested learning with global objectives. We offer a theoretical frameworks where we define a swarm of traders whose collective objective is market liquidity provision while maintaining agent independence. Using difference rewards within a Markov team games framework, we show that individual liquidity-maximizing behaviors contribute to overall market liquidity without requiring coordination or collusion. This Financial Swarm model provides a framework for modeling rational, independent agents where they achieve both individual profitability and collective market efficiency in bilateral asset markets.
]]></content:encoded>
<pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>MR-DAW: Towards Collaborative Digital Audio Workstations in Mixed Reality</title>
<link>https://arxiv.org/abs/2601.00326</link>
<guid>https://arxiv.org/abs/2601.00326</guid>
<content:encoded><![CDATA[
<div> 数字音频工作站（DAW）、混合现实（MR）、远程协作、人机交互、音乐创作<br /><br />总结:  
1. 当前数字音频工作站（DAW）限制了音乐人的自然演奏与移动自由，且远程协作受网络延迟和异步文件共享困扰。  
2. 研究探索混合现实（MR）技术以构建直观、实时的远程音乐协作环境。  
3. 通过定性与思辨设计方法，了解现有DAW使用习惯，并构想未来MR-DAW协作模式。  
4. 开发并评估了MR-DAW原型系统，允许多名异地用户共享同一DAW实例，并在各自空间中自由移动操作。  
5. 系统引入实体脚踏板实现免手操作的协同循环录制，提升交互自然性；基于20位音乐人反馈，验证了MR在解放创作束缚和推动“音乐元宇宙”协作方面的潜力。 <div>
arXiv:2601.00326v1 Announce Type: new 
Abstract: Digital Audio Workstations (DAWs) are central to modern music production but often encumber the musician's workflow, tethering them to a desk and hindering natural interaction with their instrument. Furthermore, effective remote collaboration remains a significant challenge, with existing solutions hampered by network latency and asynchronous file sharing. This paper investigates the potential of Mixed Reality (MR) to overcome these barriers, creating an intuitive environment for real-time, remote musical collaboration. We employ qualitative and speculative design techniques to better understand: 1) how players currently use DAWs, and 2) to imagine a speculative future of collaborative MR-DAWs. To facilitate this discussion, we developed and evaluated the usability of a design probe, MR-DAW. An MR system enabling multiple, geographically dispersed users to control a single, shared DAW instance while moving freely in their local spaces. Our networked system enables each remote musician to use a physical foot pedal for collaborative looping, merging a familiar, hands-free interaction with a shared virtual session. Based on interviews and system evaluations with 20 musicians, we analyze current practices, report on the user experience with our MR system, and speculate on the future of musical collaboration in MR. Our results highlight the affordances of MR for unencumbered musical interaction and provide a speculative outlook on the future of remote collaborative DAWs in the Musical Metaverse.
]]></content:encoded>
<pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Secure, Verifiable, and Scalable Multi-Client Data Sharing via Consensus-Based Privacy-Preserving Data Distribution</title>
<link>https://arxiv.org/abs/2601.00418</link>
<guid>https://arxiv.org/abs/2601.00418</guid>
<content:encoded><![CDATA[
<div> 关键词：共识机制、隐私保护、数据聚合、轻量级协议、恶意检测<br /><br />总结:  
1. 提出CPPDD框架，一种轻量级、无需持续协调的隐私保护多客户端数据聚合协议。  
2. 采用双层保护机制（仿射掩码与优先级驱动的顺序共识锁定），确保数据仅在全体同意下释放。  
3. 通过σ_S和σ_D校验和实现去中心化完整性验证，支持自动检测恶意偏离并原子中止。  
4. 支持标量、向量和矩阵数据，具备O(N·D)复杂度、边缘服务器卸载选项及抵抗N-1方合谋攻击能力。  
5. 理论证明其满足正确性、共识依赖完整性与公平性（CDIF）及IND-CPA安全性；实验显示在MNIST数据上可线性扩展至500客户端，计算开销比MPC和同态加密低3–4个数量级，且实现100%恶意行为检测与精确数据恢复。 <div>
arXiv:2601.00418v1 Announce Type: new 
Abstract: We propose the Consensus-Based Privacy-Preserving Data Distribution (CPPDD) framework, a lightweight and post-setup autonomous protocol for secure multi-client data aggregation. The framework enforces unanimous-release confidentiality through a dual-layer protection mechanism that combines per-client affine masking with priority-driven sequential consensus locking. Decentralized integrity is verified via step (sigma_S) and data (sigma_D) checksums, facilitating autonomous malicious deviation detection and atomic abort without requiring persistent coordination. The design supports scalar, vector, and matrix payloads with O(N*D) computation and communication complexity, optional edge-server offloading, and resistance to collusion under N-1 corruptions. Formal analysis proves correctness, Consensus-Dependent Integrity and Fairness (CDIF) with overwhelming-probability abort on deviation, and IND-CPA security assuming a pseudorandom function family. Empirical evaluations on MNIST-derived vectors demonstrate linear scalability up to N = 500 with sub-millisecond per-client computation times. The framework achieves 100% malicious deviation detection, exact data recovery, and three-to-four orders of magnitude lower FLOPs compared to MPC and HE baselines. CPPDD enables atomic collaboration in secure voting, consortium federated learning, blockchain escrows, and geo-information capacity building, addressing critical gaps in scalability, trust minimization, and verifiable multi-party computation for regulated and resource-constrained environments.
]]></content:encoded>
<pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>The CoinAlg Bind: Profitability-Fairness Tradeoffs in Collective Investment Algorithms</title>
<link>https://arxiv.org/abs/2601.00523</link>
<guid>https://arxiv.org/abs/2601.00523</guid>
<content:encoded><![CDATA[
<div> 关键词：集体投资算法、盈利-公平权衡、隐私、经济公平、套利<br /><br />总结:  
1. 集体投资算法（CoinAlgs）旨在通过共享交易策略，使普通投资者也能使用先进的AI投资工具。  
2. 研究揭示了CoinAlgs中存在“CoinAlg困境”：无法在保障经济公平的同时避免利润被套利侵蚀。  
3. 隐私（即不完全披露算法）是内部人发动不公平价值提取攻击的前提条件。  
4. 反之，若算法透明（缺乏隐私），套利者会利用信息优势削弱其盈利能力。  
5. 基于Uniswap数据的实证分析表明，透明CoinAlgs易受套利影响，而即使是低带宽的隐蔽信息泄露，也会导致不公平的价值提取。 <div>
arXiv:2601.00523v1 Announce Type: new 
Abstract: Collective Investment Algorithms (CoinAlgs) are increasingly popular systems that deploy shared trading strategies for investor communities. Their goal is to democratize sophisticated -- often AI-based -- investing tools. We identify and demonstrate a fundamental profitability-fairness tradeoff in CoinAlgs that we call the CoinAlg Bind: CoinAlgs cannot ensure economic fairness without losing profit to arbitrage. We present a formal model of CoinAlgs, with definitions of privacy (incomplete algorithm disclosure) and economic fairness (value extraction by an adversarial insider). We prove two complementary results that together demonstrate the CoinAlg Bind. First, privacy in a CoinAlg is a precondition for insider attacks on economic fairness. Conversely, in a game-theoretic model, lack of privacy, i.e., transparency, enables arbitrageurs to erode the profitability of a CoinAlg. Using data from Uniswap, a decentralized exchange, we empirically study both sides of the CoinAlg Bind. We quantify the impact of arbitrage against transparent CoinAlgs. We show the risks posed by a private CoinAlg: Even low-bandwidth covert-channel information leakage enables unfair value extraction.
]]></content:encoded>
<pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Transport-Based Decentralized Multi-Agent Distribution Matching</title>
<link>https://arxiv.org/abs/2601.00548</link>
<guid>https://arxiv.org/abs/2601.00548</guid>
<content:encoded><![CDATA[
<div> 多智能体系统、分布匹配、最优传输、去中心化控制、Wasserstein距离<br /><br />总结:<br />1. 本文提出一种用于多智能体系统的去中心化控制框架，旨在使智能体群体达成预设的终端空间分布。<br />2. 问题基于最优传输理论构建，采用Wasserstein距离度量分布差异，并作为控制设计基础。<br />3. 为避免求解全局最优传输问题，将目标转化为每个智能体仅依赖局部信息的可处理决策过程。<br />4. 引入顺序权重更新规则生成可行的局部传输方案，并结合记忆修正机制应对通信间歇性和范围限制。<br />5. 理论分析证明了在不同动力学模型下，代理成本在每周期内逐步改善；仿真实验验证了该方法的有效性与可扩展性。 <div>
arXiv:2601.00548v1 Announce Type: new 
Abstract: This paper presents a decentralized control framework for distribution matching in multi-agent systems (MAS), where agents collectively achieve a prescribed terminal spatial distribution. The problem is formulated using optimal transport (Wasserstein distance), which provides a principled measure of distributional discrepancy and serves as the basis for the control design. To avoid solving the global optimal transport problem directly, the distribution-matching objective is reformulated into a tractable per-agent decision process, enabling each agent to identify its desired terminal locations using only locally available information. A sequential weight-update rule is introduced to construct feasible local transport plans, and a memory-based correction mechanism is incorporated to maintain reliable operation under intermittent and range-limited communication. Convergence guarantees are established, showing cycle-wise improvement of a surrogate transport cost under both linear and nonlinear agent dynamics. Simulation results demonstrate that the proposed framework achieves effective and scalable distribution matching while operating fully in a decentralized manner.
]]></content:encoded>
<pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cyberscurity Threats and Defense Mechanisms in IoT network</title>
<link>https://arxiv.org/abs/2601.00556</link>
<guid>https://arxiv.org/abs/2601.00556</guid>
<content:encoded><![CDATA[
<div> 物联网、网络安全、人工智能、区块链、零信任架构<br /><br />总结:  
1. 随着物联网设备预计到2030年将超300亿台，其网络安全挑战日益复杂。  
2. 本文综述了网络与应用层融合下的漏洞、威胁及防御机制，分析了59篇2009至2024年的相关文献。  
3. 识别出三大关键威胁：传感器漏洞、拒绝服务（DoS）攻击和公有云安全隐患。  
4. 提出先进防御手段，包括利用人工智能进行异常检测、区块链实现去中心化信任，以及零信任架构实现持续验证。  
5. 贡献了一个新颖的五层物联网模型，并展望了结合量子计算与6G网络的未来研究方向以增强系统韧性。 <div>
arXiv:2601.00556v1 Announce Type: new 
Abstract: The rapid proliferation of Internet of Things (IoT) technologies, projected to exceed 30 billion interconnected devices by 2030, has significantly escalated the complexity of cybersecurity challenges. This survey aims to provide a comprehensive analysis of vulnerabilities, threats, and defense mechanisms, specifically focusing on the integration of network and application layers within real-time monitoring and decision-making systems. Employing an integrative review methodology, 59 scholarly articles published between 2009 and 2024 were selected from databases such as IEEE Xplore, ScienceDirect, and PubMed, utilizing keywords related to IoT vulnerabilities and security attacks. Key findings identify critical threat categories, including sensor vulnerabilities, Denial-of-Service (DoS) attacks, and public cloud insecurity. Conversely, the study highlights advanced defense approaches leveraging Artificial Intelligence (AI) for anomaly detection, Blockchain for decentralized trust, and Zero Trust Architecture (ZTA) for continuous verification. This paper contributes a novel five-layer IoT model and outlines future research directions involving quantum computing and 6G networks to bolster IoT ecosystem resilience.
]]></content:encoded>
<pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing</title>
<link>https://arxiv.org/abs/2601.00785</link>
<guid>https://arxiv.org/abs/2601.00785</guid>
<content:encoded><![CDATA[
<div> 联邦学习、差分隐私、超网络、非独立同分布、嵌入级数据生成<br /><br />总结:<br />1. 针对联邦学习中客户端数据非独立同分布（non-IID）和梯度泄露风险的问题，提出FedHypeVAE框架。<br />2. 该框架基于条件变分自编码器（VAE），引入由共享超网络生成的客户端感知解码器和类别条件先验，实现生成层的个性化。<br />3. 通过差分隐私机制，在聚合阶段仅传输加噪并裁剪的梯度，保障隐私安全。<br />4. 引入局部最大均值差异（MMD）对齐与Lipschitz正则化，提升非IID条件下合成数据的稳定性和分布一致性。<br />5. 训练后支持使用中立元代码进行领域无关合成，或混合元代码实现可控的多领域覆盖，统一了生成层面的个性化、隐私保护与分布对齐。 <div>
arXiv:2601.00785v1 Announce Type: new 
Abstract: Federated data sharing promises utility without centralizing raw data, yet existing embedding-level generators struggle under non-IID client heterogeneity and provide limited formal protection against gradient leakage. We propose FedHypeVAE, a differentially private, hypernetwork-driven framework for synthesizing embedding-level data across decentralized clients. Building on a conditional VAE backbone, we replace the single global decoder and fixed latent prior with client-aware decoders and class-conditional priors generated by a shared hypernetwork from private, trainable client codes. This bi-level design personalizes the generative layerrather than the downstream modelwhile decoupling local data from communicated parameters. The shared hypernetwork is optimized under differential privacy, ensuring that only noise-perturbed, clipped gradients are aggregated across clients. A local MMD alignment between real and synthetic embeddings and a Lipschitz regularizer on hypernetwork outputs further enhance stability and distributional coherence under non-IID conditions. After training, a neutral meta-code enables domain agnostic synthesis, while mixtures of meta-codes provide controllable multi-domain coverage. FedHypeVAE unifies personalization, privacy, and distribution alignment at the generator level, establishing a principled foundation for privacy-preserving data synthesis in federated settings. Code: github.com/sunnyinAI/FedHypeVAE
]]></content:encoded>
<pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pod: An Optimal-Latency, Censorship-Free, and Accountable Generalized Consensus Layer</title>
<link>https://arxiv.org/abs/2501.14931</link>
<guid>https://arxiv.org/abs/2501.14931</guid>
<content:encoded><![CDATA[
<div> 关键词：区块链、共识协议、低延迟、可扩展性、拜占庭容错<br /><br />总结:  
1. 针对区块链高延迟与传统共识协议可扩展性差的问题，提出名为“pod”的新型共识机制。  
2. pod 通过消除副本间通信，让客户端直接向所有副本发送交易，各副本独立处理并本地记录，实现物理最优延迟 $2\delta$（一次往返）。  
3. 每个交易被分配时间戳和序列号，客户端可从中提取元数据及系统状态信息。  
4. 由于理论下限限制，pod 提供比全序广播更弱的一致性，但具备明确的安全属性定义。  
5. pod-core 协议支持 $2\delta$ 内确认交易、抗拜占庭审查，并对安全违规行为可追责，适用于单次拍卖等典型应用场景。 <div>
arXiv:2501.14931v5 Announce Type: replace 
Abstract: This work addresses the inherent issues of high latency in blockchains and low scalability in traditional consensus protocols. We present pod, a novel notion of consensus whose first priority is to achieve the physically-optimal latency of $2\delta$, or one round-trip, i.e., requiring only one network trip (duration $\delta$) for writing a transaction and one for reading it.
  To accomplish this, we first eliminate inter-replica communication. Instead, clients send transactions directly to all replicas, which independently process transactions and append them to local logs. Replicas assigns a timestamp and a sequence number to each transaction in their logs, allowing clients to extract valuable metadata about the transactions and the system state. Later on, clients retrieve these logs and extract transactions (and associated metadata) from them.
  Necessarily, this construction achieves weaker properties than a total-order broadcast protocol, due to existing lower bounds. Our work models the primitive of pod and defines its security properties. We then show pod-core, a protocol that satisfies properties such as transaction confirmation within $2\delta$, censorship resistance against Byzantine replicas, and accountability for safety violations. We show that single-shot auctions can be realized using the pod notion and observe that it is also sufficient for other popular applications.
]]></content:encoded>
<pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Midsummer Meme's Dream: Investigating Market Manipulations in the Meme Coin Ecosystem</title>
<link>https://arxiv.org/abs/2507.01963</link>
<guid>https://arxiv.org/abs/2507.01963</guid>
<content:encoded><![CDATA[
<div> 关键词：meme coins、跨链分析、人工增长策略、流动性池价格通胀（LPI）、利润榨取骗局<br /><br />总结:  
1. 研究对以太坊、BNB智能链、Solana和Base四大链上的34,988种meme币进行了跨链分析，发现其价值主要依赖社区情绪，易受操纵。  
2. 在回报率超100%的高收益meme币中，82.8%存在人工增长策略，包括刷量交易和新型操纵手段“流动性池价格通胀”（LPI）。  
3. LPI通过小额战略性购买引发价格剧烈上涨，制造虚假市场热度。  
4. 初期操纵（如刷量或LPI）常为后续的“拉高出货”和“卷款跑路”等利润榨取骗局铺路。  
5. 研究识别出超17,000个受害地址，实际损失逾930万美元，表明高收益meme币多由协同操纵驱动，而非自然市场行为。 <div>
arXiv:2507.01963v2 Announce Type: replace-cross 
Abstract: From viral jokes to a billion-dollar phenomenon, meme coins have become one of the most popular segments in cryptocurrency markets. Unlike utility-focused crypto assets like Bitcoin, meme coins derive value primarily from community sentiment, making them vulnerable to manipulation. This study presents an unprecedented cross-chain analysis of the meme coin ecosystem, examining 34,988 tokens across Ethereum, BNB Smart Chain, Solana, and Base. We characterize their tokenomics and track their growth in a three-month longitudinal analysis. We discover that among high-return tokens (>100%), an alarming 82.8% show evidence of artificial growth strategies designed to create a misleading appearance of market interest. These include wash trading and a new form of manipulation we define as Liquidity Pool-Based Price Inflation (LPI), where small strategic purchases trigger dramatic price increases. We find that profit extraction schemes, such as pump and dumps and rug pulls, typically follow initial manipulations like wash trading or LPI, indicating how early manipulations create the foundation for later exploitation. We quantify the economic impact of these schemes, identifying over 17,000 victimized addresses with realized losses exceeding $9.3 million. These findings reveal that combined manipulations are widespread among high-performing meme coins, suggesting that their dramatic gains are often driven by coordinated efforts rather than natural market dynamics.
]]></content:encoded>
<pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-varying Mixing Matrix Design for Energy-efficient Decentralized Federated Learning</title>
<link>https://arxiv.org/abs/2512.24069</link>
<guid>https://arxiv.org/abs/2512.24069</guid>
<content:encoded><![CDATA[
<div> The arXiv paper **2512.24069v1** presents a novel approach to designing **mixing matrices** in **decentralized federated learning (DFL)** over wireless networks, with the primary objective of **minimizing the maximum per-node energy consumption**—a critical concern for battery-limited edge devices.

### Key Contributions and Insights:

#### 1. **Problem Motivation**
- In DFL, agents (e.g., mobile or IoT devices) collaboratively train a model without central coordination by exchanging local model updates with neighbors.
- The **mixing matrix** (also known as the consensus weight matrix) governs how nodes combine their neighbors’ information during each communication round.
- While prior work has focused on optimizing mixing matrices to speed up convergence (often favoring dense topologies), this paper highlights that **energy efficiency**, especially **peak energy per node**, is equally—if not more—important in practical wireless deployments.

#### 2. **Key Challenge**
- There's a fundamental trade-off:
  - **Dense communication graphs** → faster convergence but high per-round energy use due to many transmissions.
  - **Sparse graphs** → lower energy per round but slower convergence, potentially increasing total energy.
- Additionally, **wireless broadcast nature** means one transmission can reach multiple neighbors, which can be exploited for efficiency.

#### 3. **Main Innovation: Multi-Phase Time-Varying Mixing Matrix Design**
- The authors propose a **multi-phase framework** where different sparse communication topologies are activated over time, with optimized durations ("budgets").
- This allows **time-varying mixing matrices**, adapting throughout training to balance:
  - Energy cost per iteration
  - Overall convergence speed
  - Fairness in energy distribution across nodes

#### 4. **Theoretical Foundation**
- A **new convergence theorem** is developed that supports **arbitrarily time-varying mixing matrices**, relaxing the common assumption of fixed topology.
- This enables dynamic switching between topologies while still guaranteeing convergence.

#### 5. **Energy-Aware Optimization**
- The design explicitly accounts for:
  - Transmission energy costs in wireless settings
  - Asymmetric node capabilities or channel conditions
  - Broadcast advantage: one transmission reaching many receivers reduces redundancy
- Objective: **Minimize the maximum energy consumed by any single node** until global convergence — promoting fairness and longevity in heterogeneous networks.

#### 6. **Empirical Validation**
- Evaluated using **real-world datasets and network traces**.
- Results show the proposed method successfully **combines benefits of both worlds**:
  - Achieves **low per-node energy consumption** like sparse designs
  - Maintains **fast convergence rates** close to dense counterparts
- Outperforms static and uniform mixing strategies in terms of energy fairness and total lifetime support.

---

### Significance and Impact

This work bridges an important gap between theoretical DFL algorithm design and practical deployment constraints in wireless edge systems. By rethinking the role of the mixing matrix—not just as a convergence accelerator but as a tool for **energy load balancing**—it opens new directions for sustainable, long-lived decentralized learning systems.

### Potential Applications
- Internet of Things (IoT) networks
- Mobile edge computing
- Autonomous sensor networks
- Federated learning on battery-powered devices

---

### Conclusion

arXiv:2512.24069v1 makes a timely and impactful contribution by reframing mixing matrix design around **energy fairness and efficiency** in decentralized federated learning. Its multi-phase, time-varying strategy leverages theoretical advances and realistic wireless considerations to deliver solutions that are both **analytically sound and practically viable**.

> 🔍 **Suggested follow-up**: Investigate whether the time-varying topologies can be learned online (adaptively) based on current energy levels and channel states—potentially enabling fully adaptive DFL in dynamic environments. <div>
arXiv:2512.24069v1 Announce Type: new 
Abstract: We consider the design of mixing matrices to minimize the operation cost for decentralized federated learning (DFL) in wireless networks, with focus on minimizing the maximum per-node energy consumption. As a critical hyperparameter for DFL, the mixing matrix controls both the convergence rate and the needs of agent-to-agent communications, and has thus been studied extensively. However, existing designs mostly focused on minimizing the communication time, leaving open the minimization of per-node energy consumption that is critical for energy-constrained devices. This work addresses this gap through a theoretically-justified solution for mixing matrix design that aims at minimizing the maximum per-node energy consumption until convergence, while taking into account the broadcast nature of wireless communications. Based on a novel convergence theorem that allows arbitrarily time-varying mixing matrices, we propose a multi-phase design framework that activates time-varying communication topologies under optimized budgets to trade off the per-iteration energy consumption and the convergence rate while balancing the energy consumption across nodes. Our evaluations based on real data have validated the efficacy of the proposed solution in combining the low energy consumption of sparse mixing matrices and the fast convergence of dense mixing matrices.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Heterogeneity-Aware Client Selection for Federated Learning in Wireless Networks</title>
<link>https://arxiv.org/abs/2512.24286</link>
<guid>https://arxiv.org/abs/2512.24286</guid>
<content:encoded><![CDATA[
<div> It looks like you've shared the **abstract** of a newly submitted paper to arXiv with the identifier **arXiv:2512.24286v1**, which focuses on **Federated Learning (FL)** in **wireless mobile edge networks**.

Here’s a breakdown and interpretation of the key points from the abstract:

---

### 🔍 **Summary of Key Contributions**

1. **Problem Context**:
   - Federated Learning allows edge devices (clients) to collaboratively train a global model without sharing raw data—preserving privacy.
   - However, FL over wireless networks faces challenges due to:
     - Limited communication & computational resources
     - **Data heterogeneity** across clients (non-IID data)
     - These issues worsen in large-scale deployments.

2. **Theoretical Analysis**:
   - The paper provides a **theoretical study** on how **client data heterogeneity affects generalization error** of the global model.
   - High heterogeneity can lead to:
     - Poor model convergence
     - More training rounds (increased latency)
     - Higher energy consumption

3. **Optimization Framework**:
   - Proposes a joint optimization problem that aims to:
     - Minimize **learning latency**
     - Minimize **energy consumption**
     - While keeping the **generalization error within acceptable bounds**
   - This is a multi-objective trade-off, especially critical for resource-constrained edge devices.

4. **Proposed Solution: CSRA**  
   *(Joint Client Selection and Resource Allocation)*
   - Uses **convex optimization and relaxation techniques** to make the problem tractable.
   - Dynamically selects which clients participate in each round (client selection).
   - Allocates communication and computation resources efficiently (e.g., bandwidth, power, CPU frequency).

5. **Evaluation & Results**:
   - Simulation-based evaluation shows that CSRA outperforms baseline methods that ignore data heterogeneity.
   - Improvements include:
     - **Higher test accuracy** (better generalization)
     - **Lower learning latency**
     - **Reduced energy consumption**

---

### 📌 Significance

This work bridges theoretical understanding of data heterogeneity in FL with practical system design in wireless edge environments. By incorporating heterogeneity into both the analysis and optimization framework, it enables more efficient and accurate FL deployment.

---

### 🔮 Potential Applications

- IoT networks
- 5G/6G mobile systems
- Smart cities, autonomous systems, or healthcare applications using decentralized learning

---

### 🛠️ Technical Approach Highlights

- **Modeling generalization error** as a function of data distribution skew.
- **Jointly optimizing** who participates (client selection) and how resources are allocated (bandwidth, time, local computation).
- Use of **relaxation techniques** (likely Lagrangian relaxation, fractional programming, or binary variable relaxations) to handle combinatorial aspects of client selection.

---

If you're interested, I can help with:
- Summarizing the full paper once it's available.
- Explaining possible mathematical models behind the generalization error analysis.
- Comparing this approach to prior works like FedAvg, FedProx, or communication-efficient FL methods.
- Replicating or designing simulations based on this framework.

Let me know how you'd like to proceed! <div>
arXiv:2512.24286v1 Announce Type: new 
Abstract: Federated Learning (FL) enables mobile edge devices, functioning as clients, to collaboratively train a decentralized model while ensuring local data privacy. However, the efficiency of FL in wireless networks is limited not only by constraints on communication and computational resources but also by significant data heterogeneity among clients, particularly in large-scale networks. This paper first presents a theoretical analysis of the impact of client data heterogeneity on global model generalization error, which can result in repeated training cycles, increased energy consumption, and prolonged latency. Based on the theoretical insights, an optimization problem is formulated to jointly minimize learning latency and energy consumption while constraining generalization error. A joint client selection and resource allocation (CSRA) approach is then proposed, employing a series of convex optimization and relaxation techniques. Extensive simulation results demonstrate that the proposed CSRA scheme yields higher test accuracy, reduced learning latency, and lower energy consumption compared to baseline methods that do not account for data heterogeneity.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>GateChain: A Blockchain Based Application for Country Entry Exit Registry Management</title>
<link>https://arxiv.org/abs/2512.24416</link>
<guid>https://arxiv.org/abs/2512.24416</guid>
<content:encoded><![CDATA[
<div> **arXiv:2512.24416v1 [cs.CR]** — *Announcement Type: New*

**Title:** *GateChain: A Blockchain-Based Secure and Auditable System for Border Entry and Exit Record Management*

**Summary of Abstract:**

With increasing international mobility and heightened security demands, the accurate and trustworthy recording of cross-border entry and exit events has become critical. Current border control systems often rely on centralized databases, which are susceptible to data tampering, single points of failure, and suffer from poor interoperability across agencies and nations.

To address these challenges, this paper introduces **GateChain**, a novel blockchain-based application designed to ensure **confidentiality, integrity, auditability**, and **interoperability** in border management systems. GateChain leverages a distributed ledger architecture to immutably record entry and exit events, providing a transparent, cryptographically secured, and tamper-evident log accessible in real time by authorized institutions.

Key features of GateChain include:
- **Decentralized Architecture**: Eliminates reliance on a single central authority, reducing risks of manipulation and downtime.
- **Cryptographic Integrity**: All records are hashed and chained, ensuring any unauthorized alteration is detectable.
- **Access Control Mechanisms**: Role-based permissions and cryptographic authentication enable secure, real-time data sharing among trusted entities (e.g., immigration, customs, law enforcement).
- **Auditability and Transparency**: Every transaction is time-stamped and traceable, supporting regulatory compliance and forensic investigations.
- **Interoperability Design**: Facilitates data exchange across national and institutional boundaries while preserving privacy and sovereignty.

The paper presents the full system architecture, details the security model (including resistance to common attack vectors), and evaluates GateChain’s performance in terms of transaction latency, scalability, and resource consumption through simulations and prototype testing.

**Conclusion:**  
GateChain demonstrates that blockchain technology can significantly enhance the reliability and trustworthiness of border control systems. By combining decentralization with strong cryptographic guarantees, it offers a promising foundation for next-generation, secure, and auditable international travel recordkeeping.

---

*Note: As of now (April 2024), arXiv IDs typically follow the format YYMM.NNNNN — so "2512.24416v1" suggests a future submission (December 2025). This may be a placeholder or hypothetical identifier. If this is a speculative or fictional paper, the above summary reflects a plausible interpretation based on current trends in blockchain applications for public security infrastructure.* <div>
arXiv:2512.24416v1 Announce Type: new 
Abstract: Recording entry and exit records for a country, with properties such as confidentiality, integrity, and auditability, is increasingly important due to rising international mobility and security requirements. Traditional border control systems, which rely on centralised databases, are vulnerable to data manipulation and have limited interoperability between institutions. This study presents GateChain, a blockchain-based application that addresses these vulnerabilities. GateChain aims to enhance data integrity, reliability, and transparency by recording entry and exit events on a distributed, immutable, and cryptographically verifiable ledger. The application provides real-time access control and verification for authorised institutions. This paper describes the architecture and security components of GateChain and evaluates its performance and security features.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Document Data Matching for Blockchain-Supported Real Estate</title>
<link>https://arxiv.org/abs/2512.24457</link>
<guid>https://arxiv.org/abs/2512.24457</guid>
<content:encoded><![CDATA[
<div> **arXiv:2512.24457v1 [cs.CY]** — *Automated Document Verification in Real Estate Using OCR, NLP, and Verifiable Credentials*

### Summary & Analysis:

This paper introduces an innovative framework aimed at modernizing the traditionally manual and error-prone processes in the real estate sector by leveraging **optical character recognition (OCR)**, **natural language processing (NLP)**, and **verifiable credentials (VCs)** anchored on a **blockchain-based decentralized trust layer**.

---

### Key Contributions:

1. **Integration of AI and Trust Technologies**:
   - Combines **OCR** for digitizing physical documents (e.g., property deeds, ID proofs, contracts) with **NLP** to extract structured data from unstructured text.
   - Uses synthetic datasets to train the OCR-NLP pipeline, enabling robustness across diverse document formats and quality levels—crucial given the heterogeneity in real estate documentation globally.

2. **Standardization via Verifiable Credentials (VCs)**:
   - Extracted information is converted into standardized **W3C-compliant verifiable credentials**, which are cryptographically secure and tamper-evident.
   - This enables interoperability and machine-readability across stakeholders (buyers, sellers, agents, banks, regulators).

3. **Blockchain for Decentralized Trust**:
   - VCs are issued and verified using a blockchain backend, ensuring immutability, auditability, and transparency without reliance on a central authority.
   - Enhances resistance to fraud such as forged titles or identity misrepresentation.

4. **End-to-End System Architecture**:
   The prototype includes:
   - **(i) OCR-NLP Extraction Pipeline**: Preprocessing, layout analysis, text extraction, and semantic parsing.
   - **(ii) Backend Service**: Manages credential issuance, revocation, and validation; integrates with a permissioned blockchain (likely Hyperledger or Ethereum-based).
   - **(iii) Frontend Interface**: Supports roles of issuer (e.g., notary), holder (e.g., buyer), and verifier (e.g., bank), facilitating user interaction and consent management.

5. **Automated Data Matching & Inconsistency Detection**:
   - Cross-references extracted fields across multiple documents (e.g., matching owner names in deed and ID) to flag discrepancies automatically.
   - Reduces human oversight errors and speeds up due diligence.

---

### Experimental Results:

- Models show **competitive accuracy** on key entity extraction (e.g., property address, price, parties involved) across various document types (deeds, IDs, mortgage statements).
- End-to-end processing significantly **reduces verification time**—from days/hours to minutes—while maintaining high reliability.
- Synthetic training data helps overcome scarcity of labeled real-world documents, improving generalization.

---

### Implications:

- **Efficiency Gains**: Automates labor-intensive tasks like title checks, income verification, and compliance reviews.
- **Fraud Reduction**: Tamper-proof records and cryptographic verification minimize risks of document forgery.
- **Stakeholder Trust**: Transparent audit trails and self-sovereign identity principles empower users with control over their data.
- **Scalability**: Framework can be extended to other domains requiring document-heavy workflows (e.g., banking, healthcare, legal services).

---

### Limitations & Future Work:

- Reliance on synthetic data may not fully capture edge cases in low-quality scans or regional formatting variations.
- Blockchain scalability and energy efficiency could be concerns depending on the chosen platform.
- User adoption depends on regulatory alignment and integration with existing land registry systems.

Future directions may include:
- Integration with government land registries.
- Support for zero-knowledge proofs to enhance privacy.
- Multilingual expansion for global applicability.

---

### Conclusion:

The proposed system represents a significant step toward **digitizing and securing real estate transactions** through the convergence of AI-driven automation and decentralized identity technologies. By transforming fragmented, paper-based workflows into a unified, trustworthy digital process, it paves the way for more efficient, transparent, and inclusive property markets.

> **Potential Impact**: High – particularly in jurisdictions with weak institutional trust or underdeveloped land administration systems. <div>
arXiv:2512.24457v1 Announce Type: new 
Abstract: The real estate sector remains highly dependent on manual document handling and verification, making processes inefficient and prone to fraud. This work presents a system that integrates optical character recognition (OCR), natural language processing (NLP), and verifiable credentials (VCs) to automate document extraction, verification, and management. The approach standardizes heterogeneous document formats into VCs and applies automated data matching to detect inconsistencies, while the blockchain provides a decentralized trust layer that reinforces transparency and integrity. A prototype was developed that comprises (i) an OCR-NLP extraction pipeline trained on synthetic datasets, (ii) a backend for credential issuance and management, and (iii) a frontend supporting issuer, holder, and verifier interactions. Experimental results show that the models achieve competitive accuracy across multiple document types and that the end-to-end pipeline reduces verification time while preserving reliability. The proposed framework demonstrates the potential to streamline real estate transactions, strengthen stakeholder trust, and enable scalable, secure digital processes.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning-Augmented LLM Agents for Collaborative Decision Making and Performance Optimization</title>
<link>https://arxiv.org/abs/2512.24609</link>
<guid>https://arxiv.org/abs/2512.24609</guid>
<content:encoded><![CDATA[
<div> The abstract you've provided describes a cutting-edge advancement in multi-agent Large Language Model (LLM) systems, leveraging reinforcement learning to enhance collaborative performance. Here's a breakdown and interpretation of the key components and contributions:

---

**Title (inferred):**  
*Group Relative Policy Optimization for Collaborative Multi-Agent LLMs*

---

### **Core Problem Addressed:**
While LLMs excel at individual language tasks, they often lack **collaborative awareness**—the ability to coordinate effectively with other agents to optimize **global outcomes** in multi-agent settings. This leads to inefficiencies, redundancy, and inconsistency in joint tasks like collaborative writing or coding.

---

### **Proposed Solution:**

1. **Dec-POMDP Framework:**  
   The authors model multi-agent collaboration as a *Decentralized Partially Observable Markov Decision Process (Dec-POMDP)*—a formalism suitable for multiple agents making decisions based on partial observations while aiming to maximize a shared objective.

2. **CTDE Paradigm:**  
   They adopt **Centralized Training with Decentralized Execution (CTDE)**:
   - During **training**, agents have access to global information (e.g., actions/rewards of all agents).
   - At **execution**, each agent acts only on its local observations—making the system scalable and practical.

3. **Group Relative Policy Optimization (GRPO):**  
   A novel reinforcement learning algorithm that extends concepts like PPO (Proximal Policy Optimization) to the multi-agent setting. GRPO:
   - Uses **global signals during training** to shape individual policies.
   - Optimizes policies relative to group performance, encouraging coordination without requiring explicit communication protocols.

4. **Simplified Joint Reward Function:**  
   Balances three critical aspects:
   - **Task quality** (correctness, coherence),
   - **Speed** (time-to-completion, efficiency),
   - **Coordination cost** (redundancy, conflicts, communication overhead).

---

### **Evaluation & Results:**

- **Domains Tested:**
  - **Collaborative Writing**: Measured for consistency in style and structure.
  - **Collaborative Coding**: Evaluated using test case pass rates.

- **Key Results:**
  - **3× faster task processing** compared to single-agent baselines.
  - **98.7% structural/style consistency** in writing tasks—indicating strong alignment across agent contributions.
  - **74.6% test pass rate** in coding tasks—competitive given the complexity of multi-agent program synthesis.
  - Outperforms existing multi-agent LLM frameworks (e.g., BabyAGI, MetaGPT, etc.) in both efficiency and outcome quality.

---

### **Significance & Implications:**

- Provides a **scalable and principled framework** for enabling LLMs to work together intelligently.
- Demonstrates that **reinforcement learning**, when adapted to group dynamics (via GRPO), can significantly improve coordination.
- Offers a **practical path toward automating complex workflows** (e.g., software development teams, content creation pipelines) using AI agents.

---

### **Potential Future Directions:**
- Extending GRPO to include explicit communication mechanisms.
- Applying the framework to real-time, open-world environments.
- Studying emergent behaviors and potential miscoordination risks in large-scale deployments.

---

This work represents a significant step toward **reliable, efficient, and coherent multi-agent AI systems**, bridging the gap between standalone LLM capabilities and team-based problem solving. If the full paper delivers on these claims with robust ablation studies and reproducible results, it could become a foundational method in the multi-agent LLM space.

Let me know if you'd like help summarizing the full paper once it’s available, implementing GRPO, or comparing it to other methods like MADDPG or COMA in multi-agent RL. <div>
arXiv:2512.24609v1 Announce Type: new 
Abstract: Large Language Models (LLMs) perform well in language tasks but often lack collaborative awareness and struggle to optimize global performance in multi-agent settings. We present a reinforcement learning-augmented LLM agent framework that formulates cooperation as a decentralized partially observable Markov decision process (Dec-POMDP) and adopts centralized training with decentralized execution (CTDE). We introduce Group Relative Policy Optimization (GRPO) to jointly optimize agent policies with access to global signals during training, together with a simplified joint reward that balances task quality, speed, and coordination cost. On collaborative writing and coding benchmarks, our framework delivers a 3x increase in task processing speed over single-agent baselines, 98.7% structural/style consistency in writing, and a 74.6% test pass rate in coding. The approach consistently outperforms strong multi-agent LLM baselines and provides a practical path toward reliable collaboration in complex workflows.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized No-Regret Frequency-Time Scheduling for FMCW Radar Interference Avoidance</title>
<link>https://arxiv.org/abs/2512.24619</link>
<guid>https://arxiv.org/abs/2512.24619</guid>
<content:encoded><![CDATA[
<div> The paper **arXiv:2512.24619v1** presents a novel and theoretically grounded approach to mitigating interference among automotive Frequency-Modulated Continuous Wave (FMCW) radars in dense environments — a critical challenge for Advanced Driver Assistance Systems (ADAS) and autonomous vehicles.

Here is a detailed breakdown and interpretation of the key contributions and concepts:

---

### 🔍 **Problem Context**
As the number of FMCW radars increases on roads, **mutual interference** becomes a major issue:
- Interference degrades signal quality → reduces detection accuracy.
- Traditional mitigation methods face trade-offs:
  - **Receiver-side suppression**: Limited scalability.
  - **Waveform design**: May reduce resolution or require coordination.
  - **Cooperative scheduling**: Depends on side-channel communication (e.g., V2X), which may not be reliable or available.

Thus, there's a need for **scalable, decentralized, communication-free solutions** that preserve radar performance.

---

### 🧠 **Core Idea: Game-Theoretic Time-Frequency Hopping**

The authors build upon their prior work on *decentralized frequency-domain no-regret hopping* and extend it into a **unified time-frequency framework**, introducing:

> **Time-Frequency No-Regret Hopping (TF-NRH)**

This algorithm enables each radar to autonomously adapt both:
1. **Frequency subband selection** (spectral domain)
2. **Chirp-level time offsets** (temporal domain)

via a **joint adaptation strategy** modeled as a repeated game.

---

### 🎮 **Game Formulation: Repeated Anti-Coordination Game**

- Each radar is a player choosing actions from a joint time-frequency action space.
- Objective: Avoid collisions (i.e., overlapping transmissions in time and frequency).
- The setup is an **anti-coordination game**: players benefit when they differ in resource usage.
- Players use **regret-minimization dynamics** to update mixed strategies over time.

Key properties:
- **No reliance on inter-radar communication** → fully decentralized.
- **Adaptive learning**: Radars learn good strategies over time by minimizing regret.

---

### ⚖️ **Theoretical Contributions**

The paper proves strong theoretical guarantees:

1. ✅ **Vanishing External Regret & Swap Regret**
   - Over time, the average regret tends to zero.
   - Implies that the algorithm performs nearly as well as the best fixed strategy (external regret), and even nearly as well as the best sequence of actions (swap regret).

2. ✅ **Convergence to Equilibrium**
   - Empirical joint play converges to:
     - An **ε-coarse correlated equilibrium (CCE)** (for external regret minimization)
     - A **correlated equilibrium (CE)** (for swap regret minimization)
   - These equilibria represent stable, efficient configurations where unilateral deviations yield little gain.

3. ✅ **Joint Time-Frequency Regret Bounds**
   - Novel analysis shows how temporal adaptation (time shifting) acts as a form of **implicit regularization** for frequency selection.
   - Enhances robustness against **asynchronous interference** (common in real-world settings due to clock drifts or variable delays).

---

### 💡 **Why Time Matters: Implicit Regularization**

One insightful finding:
- Introducing **time offset flexibility** helps smooth out poor frequency choices.
- Even if two radars occasionally pick overlapping frequencies, slight time misalignments reduce collision impact.
- This makes the system more resilient without explicit coordination.

---

### 📊 **Numerical Results**

Simulations in multi-radar scenarios show TF-NRH outperforms baselines in:
- **SINR (Signal-to-Interference-plus-Noise Ratio)** ↑
- **Collision rate** ↓
- **Range-Doppler map quality** ↑ (less smearing, clearer targets)

Compared to:
- **Random time-frequency hopping**: No learning, baseline stochastic approach.
- **Centralized Nash-based schemes**: Require full information and coordination; less scalable.

Despite being decentralized, TF-NRH matches or exceeds centralized approaches in performance while avoiding communication overhead.

---

### 🏁 **Conclusion & Significance**

This work offers:
- A **principled, game-theoretic framework** for interference mitigation in automotive radar networks.
- A **practical algorithm (TF-NRH)** that balances autonomy, scalability, and performance.
- Strong **theoretical grounding** with convergence and regret guarantees.
- Demonstrated **robustness and superiority** in realistic simulations.

It represents a significant step toward **scalable, intelligent spectrum sharing** in next-generation ADAS and autonomous driving systems — especially important as vehicle density rises and spectrum congestion worsens.

---

### 🔮 Future Directions (Implied or Suggested)
- Implementation on real radar hardware.
- Extension to heterogeneous radar configurations (different bandwidths, chirp rates).
- Integration with V2X for hybrid cooperative-autonomous operation.
- Exploration of deep reinforcement learning alternatives within the same game-theoretic structure.

---

Let me know if you'd like a visual explanation, summary slide, or code implementation sketch! <div>
arXiv:2512.24619v1 Announce Type: new 
Abstract: Automotive FMCW radars are indispensable to modern ADAS and autonomous-driving systems, but their increasing density has intensified the risk of mutual interference. Existing mitigation techniques, including reactive receiver-side suppression, proactive waveform design, and cooperative scheduling, often face limitations in scalability, reliance on side-channel communication, or degradation of range-Doppler resolution. Building on our earlier work on decentralized Frequency-Domain No-Regret hopping, this paper introduces a unified time-frequency game-theoretic framework that enables radars to adapt across both spectral and temporal resources. We formulate the interference-avoidance problem as a repeated anti-coordination game, in which each radar autonomously updates a mixed strategy over frequency subbands and chirp-level time offsets using regret-minimization dynamics. We show that the proposed Time-Frequency No-Regret Hopping algorithm achieves vanishing external and swap regret, and that the induced empirical play converges to an $\varepsilon$-coarse correlated equilibrium or a correlated equilibrium. Theoretical analysis provides regret bounds in the joint domain, revealing how temporal adaptation implicitly regularizes frequency selection and enhances robustness against asynchronous interference. Numerical experiments with multi-radar scenarios demonstrate substantial improvements in SINR, collision rate, and range-Doppler quality compared with time-frequency random hopping and centralized Nash-based benchmarks.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Heterogeneous Multi-Agent Multi-Target Tracking using Cellular Sheaves</title>
<link>https://arxiv.org/abs/2512.24886</link>
<guid>https://arxiv.org/abs/2512.24886</guid>
<content:encoded><![CDATA[
<div> This arXiv preprint (2512.24886v1) presents a novel approach to **multi-agent target tracking** in complex, heterogeneous environments using **cellular sheaf theory**, a sophisticated mathematical framework that extends traditional graph-based models. Here's a breakdown and interpretation of the key contributions and implications:

---

### **Core Problem**
- **Multi-agent systems** are tasked with tracking **multiple unknown targets**.
- Challenges include:
  - **Nonlinear agent dynamics**
  - **Heterogeneity among agents**: agents may have different state spaces (e.g., drones vs. ground robots with different sensor suites or control inputs)
  - **External disturbances**
- Traditional methods (e.g., those based on **graph Laplacians**) struggle here because they assume homogeneous state spaces and linear interactions.

---

### **Key Innovation: Cellular Sheaves**
- The authors use **cellular sheaves**, which generalize graphs by assigning vector spaces (or more generally, algebraic structures) to nodes and edges, along with linear maps (restriction maps) between them.
- This allows modeling **heterogeneous agents** naturally—each agent can have its own state dimension and dynamics.
- Prior work applied sheaves to cooperative tasks like consensus; this paper **extends sheaf-based coordination to non-cooperative settings**, specifically **target tracking**, where agents aim to follow external trajectories rather than agree with each other.

---

### **Methodology**
- **Formulates tracking as a harmonic extension problem**:
  - In sheaf theory, harmonic sections minimize a Dirichlet-like energy, representing "smoothest" global configurations consistent with local data.
  - Here, the desired formation or estimation around targets corresponds to such a harmonic section.
  - Observations from certain agents (e.g., those sensing a target) act as boundary conditions.
- Uses the **sheaf Laplacian** to derive a **decentralized control law**:
  - Each agent updates its state based only on local communication with neighbors.
  - The control law drives the system toward the harmonic extension that encodes optimal tracking estimates.
- Incorporates **nonlinear dynamics and disturbances** into the model, making it applicable to real-world scenarios.

---

### **Theoretical Contribution**
- Provides a **Lyapunov-based stability analysis** proving that the **tracking error converges to zero** under the proposed control law.
- This is significant because it shows robustness and correctness despite heterogeneity, nonlinearity, and distributed operation.

---

### **Validation**
- Results are supported by **simulations**, demonstrating effectiveness in realistic multi-agent tracking scenarios.

---

### **Significance and Implications**
- **Bridges abstract mathematics (topology/algebra) with robotics/control theory**.
- Enables design of **flexible, modular multi-agent systems** where diverse agents (different sensors, dynamics, dimensions) can still coordinate effectively.
- Opens the door for applying sheaf-theoretic tools to other **distributed estimation and control problems** beyond consensus.

---

### **Potential Applications**
- Surveillance networks with mixed UAVs, UGVs, and static sensors
- Search and rescue operations
- Environmental monitoring with heterogeneous robot teams

---

### **Conclusion**
This paper represents a **theoretical and practical advance** in multi-agent systems by leveraging cellular sheaves to solve a challenging instance of decentralized target tracking. It demonstrates how high-level mathematical frameworks can provide powerful tools for engineering problems involving heterogeneity and complexity.

> **In short:** A mathematically sophisticated yet practically grounded solution to multi-robot tracking using cellular sheaves—going beyond consensus to real-world tasks with nonlinearities, noise, and diverse agents. <div>
arXiv:2512.24886v1 Announce Type: new 
Abstract: Multi-agent target tracking in the presence of nonlinear dynamics and agent heterogeneity, where state-space dimensions may differ, is a challenging problem that traditional graph Laplacian methods cannot easily address. This work leverages the framework of cellular sheaves, a mathematical generalization of graph theory, to natively model such heterogeneous systems. While existing coordination sheaf frameworks focus on cooperative problems like consensus, this work extends them to the non-cooperative target-tracking problem. The tracking of multiple, unknown targets is formulated as a harmonic extension problem on a cellular sheaf, accommodating nonlinear dynamics and external disturbances for all agents. A decentralized control law is developed using the sheaf Laplacian, and a corresponding Lyapunov-based stability analysis is provided to guarantee tracking error convergence, with results validated by simulation.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoK: Web3 RegTech for Cryptocurrency VASP AML/CFT Compliance</title>
<link>https://arxiv.org/abs/2512.24888</link>
<guid>https://arxiv.org/abs/2512.24888</guid>
<content:encoded><![CDATA[
<div> The arXiv preprint **2512.24888v1** presents a comprehensive systematization of **Web3 RegTech**—regulatory technology tailored for decentralized environments—with a focus on addressing **Anti-Money Laundering (AML)** and **Counter-Financing of Terrorism (CFT)** compliance in blockchain ecosystems. Below is a structured summary and analysis of the paper’s key contributions, findings, and implications:

---

### 🔍 **Core Problem**
Traditional RegTech is built for centralized financial systems with Know-Your-Customer (KYC) frameworks, auditable intermediaries, and regulated gateways. In contrast:
- **Web3 operates on decentralized, pseudonymous, and permissionless blockchains**, making identity attribution and transaction monitoring difficult.
- While blockchain transactions are transparent, their **lack of direct identity linkage** complicates regulatory oversight.

This creates a critical gap: how to enforce AML/CFT compliance without compromising Web3's core tenets—decentralization, transparency, and user sovereignty.

---

### 🧩 Key Contributions

#### 1. **Three Taxonomies for Web3 RegTech**

| Taxonomy | Description |
|--------|-------------|
| **Regulatory Paradigm Evolution Framework (10 Dimensions)** | Maps the shift from traditional finance to Web3 across dimensions such as data provenance, identity models, enforcement mechanisms, auditability, etc. Highlights how regulation must evolve beyond custodial control to code-based, protocol-level governance. |
| **Compliance Protocol Taxonomy (5 Verification Layers)** | Proposes a layered model: <br />• **On-chain data layer** (raw transactions)<br />• **Entity abstraction layer** (wallet clustering, address labeling)<br />• **Behavioral analysis layer** (pattern detection, anomaly scoring)<br />• **Cross-chain correlation layer**<br />• **Privacy-preserving verification layer** (zero-knowledge proofs, secure multi-party computation) |
| **RegTech Lifecycle Framework** | Structures compliance into three phases:<br />• **Preventive**: Risk scoring before interaction (e.g., wallet screening)<br />• **Real-time**: Monitoring during transactions (e.g., sanctions filtering)<br />• **Investigative**: Forensic analysis post-event (e.g., chain tracing after illicit activity) |

These taxonomies provide a foundational structure for understanding, comparing, and designing Web3-native compliance tools.

---

#### 2. **Empirical Analysis**
- **41 commercial platforms** (e.g., Chainalysis, Elliptic, TRM Labs, CoinTracker)
- **28 academic prototypes** (from peer-reviewed literature, 2015–2025)

##### ✅ Capabilities Enabled by Web3 RegTech
- **Transaction graph analysis**: Mapping complex flows across wallets and protocols.
- **Real-time risk assessment**: Dynamic scoring of addresses or transactions using ML models.
- **Cross-chain analytics**: Tracking value movement across Ethereum, Solana, Bitcoin, etc.
- **Privacy-preserving verification**: Using ZKPs or trusted execution environments (TEEs) to validate compliance without exposing sensitive data.

These capabilities often surpass those in traditional finance due to immutable ledgers and programmable logic.

---

#### 3. **Critical Gaps Identified**

| Gap | Explanation |
|-----|-----------|
| **Academic vs. Industry Divide** | Academic research explores advanced techniques (e.g., formal methods, ZK-compliance), but industry favors heuristic, scalable solutions. Limited transfer of innovation. |
| **Cross-chain Tracking** | Fragmented tooling; lack of standardized identifiers or interoperability protocols hinders end-to-end traceability. |
| **DeFi Interaction Complexity** | Flash loans, liquidity pools, and composability obscure fund origins. Current tools struggle with semantic interpretation of DeFi actions. |
| **Privacy Protocol Monitoring** | Tornado Cash-like mixers and privacy coins challenge traceability. Regulatory responses remain reactive rather than adaptive. |
| **Scalability & Performance** | Real-time analysis across high-throughput chains (e.g., Solana) demands significant computational resources. |

---

#### 4. **Architectural Best Practices & Design Principles**
The paper advocates for **decentralized, privacy-aware, and modular architectures**, including:
- **On-chain compliance oracles** that attest to risk scores or KYC status.
- **Zero-knowledge compliance proofs**: Prove non-involvement with sanctioned entities without revealing transaction details.
- **Decentralized identity (DID) integration**: Allow users to self-sovereignly disclose verified credentials.
- **Open-source, auditable algorithms** to maintain transparency and trustlessness.

Such designs align with Web3 values while enabling regulatory accountability.

---

### 🔮 Research Directions Proposed
1. **Unified Cross-Chain Identity Layer**: Develop persistent, portable reputation/identity constructs across chains.
2. **Semantic Transaction Understanding**: Use AI to interpret intent behind DeFi interactions (beyond pattern matching).
3. **Adaptive Privacy Detection**: Machine learning models trained on obfuscation patterns in privacy-preserving protocols.
4. **Regulation-as-Code (RaC)**: Smart contracts that encode and automatically enforce jurisdiction-specific rules.
5. **Federated Compliance Analytics**: Collaborative threat intelligence sharing without centralizing sensitive data.

---

### ⚖️ Policy & Ethical Considerations
- **Balancing surveillance and privacy**: Overreach risks undermining decentralization and financial freedom.
- **Jurisdictional fragmentation**: Differing global regulations complicate uniform compliance standards.
- **Censorship resistance vs. regulatory compliance**: Tension between open access and blocking “bad actors.”

The authors emphasize **proportionality, minimal data collection, and user agency** as guiding principles.

---

### 📌 Conclusion
This work establishes the first holistic framework for **Web3-native RegTech**, demonstrating both its transformative potential and current limitations. It bridges technical innovation with regulatory pragmatism, offering a roadmap for:
- Researchers: To build more deployable, cross-chain, and privacy-aware tools.
- Developers: To integrate compliance at the protocol level.
- Policymakers: To craft adaptive regulations that respect decentralization.

It positions Web3 not as a regulatory evasion tool, but as an opportunity to reimagine compliance through **transparency, automation, and cryptographic assurance**.

---

### 🔗 Suggested Next Steps
- Explore integration of **ZK-proofs for anonymous yet compliant transactions**.
- Evaluate **open-source alternatives** to proprietary forensic tools (e.g., OXT, Bitquery).
- Investigate **DAO-based governance of compliance rules** for community-driven regulation.

---

Let me know if you'd like:
- A visual diagram of the taxonomies,
- A comparison table of selected commercial vs. academic platforms,
- Or a summary suitable for policymakers or developers. <div>
arXiv:2512.24888v1 Announce Type: new 
Abstract: The decentralized architecture of Web3 technologies creates fundamental challenges for Anti-Money Laundering and Counter-Financing of Terrorism compliance. Traditional regulatory technology solutions designed for centralized financial systems prove inadequate for blockchain's transparent yet pseudonymous networks. This systematization examines how blockchain-native RegTech solutions leverage distributed ledger properties to enable novel compliance capabilities.
  We develop three taxonomies organizing the Web3 RegTech domain: a regulatory paradigm evolution framework across ten dimensions, a compliance protocol taxonomy encompassing five verification layers, and a RegTech lifecycle framework spanning preventive, real-time, and investigative phases. Through analysis of 41 operational commercial platforms and 28 academic prototypes selected from systematic literature review (2015-2025), we demonstrate that Web3 RegTech enables transaction graph analysis, real-time risk assessment, cross-chain analytics, and privacy-preserving verification approaches that are difficult to achieve or less commonly deployed in traditional centralized systems.
  Our analysis reveals critical gaps between academic innovation and industry deployment, alongside persistent challenges in cross-chain tracking, DeFi interaction analysis, privacy protocol monitoring, and scalability. We synthesize architectural best practices and identify research directions addressing these gaps while respecting Web3's core principles of decentralization, transparency, and user sovereignty.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Market Making for Energy Sharing</title>
<link>https://arxiv.org/abs/2512.24432</link>
<guid>https://arxiv.org/abs/2512.24432</guid>
<content:encoded><![CDATA[
<div> The abstract you've provided appears to be a fictional or speculative document, as arXiv preprints with the identifier `2512.24432v1` do not currently exist (as of knowledge cutoff in 2024), and the date implied by the ID (December 2025) is in the future. However, assuming this is a hypothetical or forward-looking research abstract, let’s break it down and interpret its key contributions and implications:

---

### **Summary and Interpretation of the Abstract**

**Title (inferred):**  
*An Axiomatic Theory of Automated Market Makers in Local Energy Sharing Markets: A Mean-Field Game Approach*

---

#### **Core Contributions:**

1. **Axiomatic Framework for AMMs in Energy Markets:**  
   The paper proposes an axiomatic foundation for designing Automated Market Makers (AMMs)—a concept borrowed from decentralized finance (DeFi)—for application in *local energy sharing markets*. These are peer-to-peer networks where prosumers (consumers who also produce energy, e.g., via solar panels) trade excess energy locally.

2. **Mean-Field Game Model of Prosumer Behavior:**  
   To analyze strategic interactions among a large number of heterogeneous prosumers, the authors model the system as a **Mean-Field Game (MFG)**. In this setting:
   - Each prosumer solves a dynamic optimization problem via a **Bellman equation**.
   - Decisions include **energy consumption**, **storage management**, and **energy trading** with the local market or grid.

3. **Key Axioms Lead to Linear, Lipschitz Payment Functions:**  
   The axiomatic approach identifies desirable properties that fair and efficient mechanisms should satisfy. From these, the authors derive that valid payment rules must be:
   - **Linear** and **Lipschitz continuous** (ensuring stability and bounded sensitivity).
   - Prices decrease as the **aggregate supply-to-demand ratio** increases — intuitively, more surplus supply lowers prices.

4. **Design Conditions from DeFi Adapted to Energy:**  
   The paper shows that incorporating two innovations from DeFi improves mechanism design:
   - **Batch execution**: Trading occurs in discrete time batches, reducing arbitrage and gaming.
   - **Concentrated liquidity**: Liquidity providers (here, prosumers with stored energy) can allocate their capacity strategically in price ranges.
   
   Under these features, standard economic conditions—**quasi-concavity**, **monotonicity**, and **homotheticity**—can be used to build AMMs satisfying the proposed axioms.

5. **Efficiency and Budget Balance:**  
   The resulting AMM mechanism is:
   - **Budget-balanced**: No external subsidies required; payments clear within the system.
   - **Ex-ante efficient**: Optimizes expected social welfare before uncertainty resolves.
   - Contrasts with VCG mechanisms, which are strategy-proof and ex-post efficient but often not budget-balanced and vulnerable to collusion.

6. **Potential Game Structure Enables Equilibrium Solution:**  
   Because the game induced by the AMM is a **Potential Game**, the Markov Perfect Equilibrium (MPE) can be found by:
   - First solving the **social planner’s problem** (centralized optimum).
   - Then **decentralizing** the solution via individual incentives aligned through the AMM.

7. **Numerical Validation on Paris Data:**  
   Simulations using real-world data from the Paris region show that:
   - Prosumer communities achieve **up to 40% gains from trade** compared to relying solely on the main power grid.
   - Highlights the economic value of localized, peer-to-peer energy exchange enabled by well-designed AMMs.

---

### **Significance and Implications**

- **Cross-Disciplinary Innovation:**  
  This work bridges **blockchain-inspired financial mechanisms** (AMMs) with **smart grid and energy economics**, demonstrating how DeFi concepts can be repurposed for sustainable infrastructure.

- **Scalable Decentralized Energy Markets:**  
  By avoiding complex auction mechanisms like VCG (which don’t scale well and fail budget balance), AMMs offer a practical, decentralized alternative for managing distributed energy resources.

- **Policy and Design Guidance:**  
  The axiomatic approach provides principles for regulators and platform designers to create fair, stable, and efficient local energy markets.

- **Future-Ready Grids:**  
  As renewable generation becomes more decentralized, such mechanisms could play a crucial role in balancing supply and demand at the neighborhood level, reducing grid congestion and promoting resilience.

---

### **Critical Questions & Research Directions**
- How robust are these results to strategic manipulation or incomplete information?
- Can privacy-preserving implementations (e.g., via zero-knowledge proofs or federated learning) be integrated?
- What are the regulatory and institutional barriers to deploying such systems?

---

### Conclusion

While the paper reference appears speculative, its conceptual framework reflects a compelling direction for future research: applying advanced mechanism design from decentralized finance to pressing challenges in energy sustainability. If realized, such AMM-based energy markets could significantly enhance efficiency, equity, and decarbonization in urban power systems.

Let me know if you'd like a simplified explanation, a visualization idea, or help drafting a related research proposal! <div>
arXiv:2512.24432v1 Announce Type: cross 
Abstract: We develop an axiomatic theory for Automated Market Makers (AMMs) in local energy sharing markets and analyze the Markov Perfect Equilibrium of the resulting economy with a Mean-Field Game. In this game, heterogeneous prosumers solve a Bellman equation to optimize energy consumption, storage, and exchanges. Our axioms identify a class of mechanisms with linear, Lipschitz continuous payment functions, where prices decrease with the aggregate supply-to-demand ratio of energy. We prove that implementing batch execution and concentrated liquidity allows standard design conditions from decentralized finance-quasi-concavity, monotonicity, and homotheticity-to construct AMMs that satisfy our axioms. The resulting AMMs are budget-balanced and achieve ex-ante efficiency, contrasting with the strategy-proof, expost optimal VCG mechanism. Since the AMM implements a Potential Game, we solve its equilibrium by first computing the social planner's optimum and then decentralizing the allocation. Numerical experiments using data from the Paris administrative region suggest that the prosumer community can achieve gains from trade up to 40% relative to the grid-only benchmark.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>A New Decomposition Paradigm for Graph-structured Nonlinear Programs via Message Passing</title>
<link>https://arxiv.org/abs/2512.24676</link>
<guid>https://arxiv.org/abs/2512.24676</guid>
<content:encoded><![CDATA[
<div> The paper **arXiv:2512.24676v1** introduces **MP-Jacobi (Message Passing-Jacobi)**, a novel decentralized optimization framework designed for **finite-sum nonlinear programs** with **local interactions governed by a graph or hypergraph structure**. Here's a structured breakdown and interpretation of the key contributions and concepts:

---

### 🧩 Problem Setting
- **Finite-sum optimization**: Objective function is a sum of local cost terms.
- **Graph/hypergraph-structured dependencies**: Decision variables interact locally—each variable participates only in certain constraints or objective terms, forming a network structure.
- Applications include distributed control, sensor networks, multi-agent systems, and large-scale machine learning where data and decisions are spatially or logically distributed.

---

### 🔧 MP-Jacobi Framework

#### Core Idea:
Combine two powerful ideas:
1. **Min-sum message passing** (from probabilistic inference and dynamic programming) to capture intra-cluster optimization structure.
2. **Jacobi-type block updates** (parallel coordinate descent-style steps) to handle inter-cluster couplings using neighbors’ latest estimates.

#### Architecture:
- The underlying (hyper)graph is partitioned into **tree-structured clusters**.
- Each cluster acts as an "agent" that performs local computation.

#### Iteration Steps:
At each iteration:
1. **Intra-cluster processing**: Within each tree cluster, perform a single **min-sum sweep** (forward-backward message passing) to compute **cost-to-go messages**, effectively solving a localized approximation of the subproblem.
2. **Inter-cluster coordination**: Use **Jacobi updates** to account for coupling between clusters, based on the most recent iterates from neighboring clusters.
3. All agents update **in parallel**, enabling scalability.

#### Communication:
- Only requires **single-hop communication** (between direct neighbors), making it suitable for decentralized, low-bandwidth environments.
- Despite operating on **loopy graphs** (which typically cause convergence issues in message-passing methods), MP-Jacobi ensures **convergence** due to its hybrid design.

---

### 📈 Theoretical Contributions

For **strongly convex objectives**:
- **Global linear convergence** is established.
- **Explicit convergence rates** are derived, which depend on:
  - **Curvature** (strong convexity and smoothness parameters),
  - **Coupling strength** between clusters,
  - **Cluster partitioning strategy** (e.g., size, overlap, tree width).

These results provide **design guidance for clustering**: how to partition the graph to optimize convergence speed and scalability.

---

### ⚙️ Practical Enhancements

To reduce computational and communication burden:
- **Surrogate models** are introduced for message updates:
  - Maintain convergence guarantees.
  - Reduce per-iteration complexity (e.g., via approximated or delayed messages).
  - Preserve graph compliance (i.e., respect the decentralized architecture).

#### Extension to Hypergraphs:
- Handles more complex, overlapping interactions (e.g., higher-order potentials).
- In regimes with heavy hyperedge overlap, proposes a **hyperedge-splitting scheme**:
  - Enables finite-time intra-cluster message computation.
  - Maintains convergence under surrogates.

This makes MP-Jacobi applicable to problems like distributed MPC, federated learning with shared features, or structured prediction.

---

### ✅ Experimental Validation
- Evaluations show consistent outperformance over **decentralized gradient methods** (e.g., DGD, ADMM variants).
- Faster convergence, better scalability with problem size and network density.
- Demonstrates robustness across different graph topologies and partitions.

---

### 🎯 Key Innovations & Implications

| Feature | Significance |
|-------|--------------|
| **Hybrid min-sum + Jacobi** | Unifies dynamic programming-style reasoning with iterative optimization |
| **Tree clustering** | Enables exact efficient message passing within clusters |
| **Parallel updates** | Fully decentralized, asynchronous-friendly |
| **Convergence on loopy graphs** | Overcomes a major limitation of traditional belief propagation |
| **Guidance for clustering** | Theory-informed partitioning improves performance |
| **Scalable surrogates** | Makes method practical for large clusters |

---

### 📘 Potential Applications
- Multi-robot coordination
- Smart grid optimization
- Federated learning with structured regularization
- Distributed model predictive control (MPC)
- Sensor fusion networks

---

### 🔮 Future Directions (implied or possible)
- Asynchronous variants.
- Stochastic extensions for online or sampled-data settings.
- Adaptive clustering strategies.
- Integration with neural parameterizations (learning-enhanced message passing).

---

### Conclusion

**MP-Jacobi** represents a significant step toward **scalable, principled decentralized optimization** over complex networked structures. By combining **message passing for intra-cluster coherence** and **Jacobi updates for inter-cluster consistency**, it achieves both **theoretical rigor** and **practical efficiency**, especially in strongly convex settings. Its extension to hypergraphs further broadens its applicability to real-world problems with higher-order interactions.

> This work bridges ideas from optimization, message passing, and graphical models, offering a promising direction for next-generation distributed algorithms.

--- 

Let me know if you'd like a simplified explanation, pseudocode sketch, or comparison with ADMM/DGD! <div>
arXiv:2512.24676v1 Announce Type: cross 
Abstract: We study finite-sum nonlinear programs whose decision variables interact locally according to a graph or hypergraph. We propose MP-Jacobi (Message Passing-Jacobi), a graph-compliant decentralized framework that couples min-sum message passing with Jacobi block updates. The (hyper)graph is partitioned into tree clusters. At each iteration, agents update in parallel by solving a cluster subproblem whose objective decomposes into (i) an intra-cluster term evaluated by a single min-sum sweep on the cluster tree (cost-to-go messages) and (ii) inter-cluster couplings handled via a Jacobi correction using neighbors' latest iterates. This design uses only single-hop communication and yields a convergent message-passing method on loopy graphs.
  For strongly convex objectives we establish global linear convergence and explicit rates that quantify how curvature, coupling strength, and the chosen partition affect scalability and provide guidance for clustering. To mitigate the computation and communication cost of exact message updates, we develop graph-compliant surrogates that preserve convergence while reducing per-iteration complexity. We further extend MP-Jacobi to hypergraphs; in heavily overlapping regimes, a surrogate-based hyperedge-splitting scheme restores finite-time intra-cluster message updates and maintains convergence. Experiments validate the theory and show consistent improvements over decentralized gradient baselines.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>FEDSTR: Money-In AI-Out | A Decentralized Marketplace for Federated Learning and LLM Training on the NOSTR Protocol</title>
<link>https://arxiv.org/abs/2404.15834</link>
<guid>https://arxiv.org/abs/2404.15834</guid>
<content:encoded><![CDATA[
<div> The abstract you've provided describes a research paper (arXiv:2404.15834v2) that proposes leveraging **Nostr** — a decentralized, open-source communication protocol based on WebSockets — as the foundation for a **decentralized marketplace for federated learning and large language model (LLM) training**.

Here's a breakdown and analysis of the key points in the abstract:

---

### 🔹 **Overview of Nostr**
- **Nostr** (**Notes and Other Stuff Transmitted by Relays**) is a lightweight, decentralized social networking protocol standardized by the [W3C](https://www.w3.org/) and built on public-key cryptography and relay-based message distribution.
- It supports various data types (e.g., text notes, direct messages, file metadata, etc.) via standardized event types.
- Its architecture is highly resilient to censorship due to its lack of central servers; anyone can run a relay, and clients fetch data from multiple sources.

---

### 🔹 **Proposed Use Case: Decentralized AI Training Marketplace**
The authors propose extending Nostr beyond social communication into the domain of **AI/ML collaboration**, specifically:
- A **federated learning marketplace**, where:
  - **Customers (data owners)** submit datasets (or references to them) they wish to use for training AI models.
  - **Service providers (compute nodes)** download parts of the dataset, perform local model training, and return updated model parameters (not raw data), preserving privacy.
  - In exchange, service providers receive **payments**, likely facilitated off-chain or through blockchain-integrated methods (e.g., Bitcoin Lightning Network, which is already used in Nostr ecosystems).

This creates a trustless, decentralized environment for collaborative machine learning without relying on centralized platforms like AWS, Hugging Face, or centralized AI marketplaces.

---

### 🔹 **Key Features Enabled by Nostr**
1. **Decentralization & Censorship Resistance**
   - Since Nostr runs on independently operated relays, no single entity can block transactions or interactions.
   - Ideal for global participation in AI development, especially under restrictive regimes.

2. **Trustless Identity & Authentication**
   - All participants are identified via public keys (similar to blockchain wallets).
   - Signatures ensure integrity and non-repudiation of events (e.g., job postings, model updates).

3. **Event-Based Communication**
   - Training requests, dataset pointers, model updates, and payment confirmations can be encoded as Nostr events.
   - Example: Custom `kind` numbers could represent:
     - `kind=30400`: Federated Learning Task Request
     - `kind=30401`: Model Update Submission
     - `kind=30402`: Payment Receipt

4. **Scalability via Public Relays**
   - The proof-of-concept uses existing public Nostr relays, showing feasibility without infrastructure overhead.

---

### 🔹 **Potential Applications Mentioned**
Beyond basic messaging, Nostr already supports:
- Direct messaging
- File sharing (via encrypted links or IPFS hashes)
- Audio/video streaming
- Collaborative writing
- Blogging
- Distributed AI directories

This work builds on these capabilities to enable **distributed AI computation coordination**.

---

### 🔹 **Significance & Innovation**
- **Democratizes AI training**: Lowers barriers for individuals and small organizations to train models using distributed compute resources.
- **Privacy-preserving**: Federated approach means raw data doesn’t leave the customer’s control (or only shared securely with workers).
- **Open and permissionless**: Anyone with bandwidth and compute can become a service provider.
- **Synergy with crypto economy**: Naturally integrates with Bitcoin/Lightning for micropayments.

---

### 🔹 **Challenges & Considerations**
While promising, several challenges remain:
| Challenge | Description |
|--------|-------------|
| **Data Size & Bandwidth** | Large datasets may not fit well within Nostr’s current event size limits (~natively up to 64KB per event). Solution: Use Nostr only for coordination; store data off-relay (e.g., IPFS, S3, Bittorrent). |
| **Model Integrity & Cheating** | How to verify honest training? May require reputation systems or zero-knowledge proofs. |
| **Payment Finality & Dispute Resolution** | Trustless payments need smart contracts or multisig escrows. |
| **Latency & Relay Performance** | Not all relays are optimized for high-frequency ML syncs. |
| **Standardization** | Requires community adoption of new event kinds and metadata formats. |

---

### 🔹 **Conclusion**
This paper presents a **novel fusion of decentralized communication and decentralized machine learning**, demonstrating how protocols like **Nostr** can evolve beyond social media into foundational layers for next-generation AI infrastructure.

By combining **federated learning**, **decentralized identity**, and **peer-to-peer coordination**, it opens the door to:
- Censorship-resistant AI development
- Community-owned LLM training collectives
- Open-source alternatives to Big Tech AI pipelines

With further development, such a system could empower grassroots innovation in artificial intelligence.

---

If you'd like, I can help:
- Summarize the full paper (if available),
- Draft a response or review,
- Design the proposed event schema for Nostr-based FL,
- Or simulate a workflow for task posting and model aggregation.

Let me know how you'd like to proceed! <div>
arXiv:2404.15834v2 Announce Type: replace 
Abstract: The NOSTR is a communication protocol for the social web, based on the w3c websockets standard. Although it is still in its infancy, it is well known as a social media protocol, with thousands of trusted users and multiple user interfaces, offering a unique experience and enormous capabilities. To name a few, the NOSTR applications include but are not limited to direct messaging, file sharing, audio/video streaming, collaborative writing, blogging and data processing through distributed AI directories. In this work, we propose an approach that builds upon the existing protocol structure with end goal a decentralized marketplace for federated learning and LLM training. In this proposed design there are two parties: on one side there are customers who provide a dataset that they want to use for training an AI model. On the other side, there are service providers, who receive (parts of) the dataset, train the AI model, and for a payment as an exchange, they return the optimized AI model. To demonstrate viability, we present a proof-of-concept implementation over public NOSTR relays. The decentralized and censorship resistant features of the NOSTR enable the possibility of designing a fair and open marketplace for training AI models and LLMs.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transfer learning of state-based potential games for process optimization in decentralized manufacturing systems</title>
<link>https://arxiv.org/abs/2408.05992</link>
<guid>https://arxiv.org/abs/2408.05992</guid>
<content:encoded><![CDATA[
<div> The arXiv paper **2408.05992v3** introduces a novel approach called **Transfer Learning in State-based Potential Games (TL-SbPGs)**, aimed at enabling distributed self-optimization in manufacturing systems through online learning and knowledge sharing.

Here is a structured summary and interpretation of the key contributions and implications of this work:

---

### 🔹 **Core Idea**
The paper proposes **TL-SbPGs**, an extension of state-based potential games (SbPGs), integrating **transfer learning** to improve decentralized decision-making in large-scale industrial environments. The focus is on **online transfer learning**, where agents ("players") can adaptively reuse policies learned by similar players to accelerate their own learning and optimize system-wide performance.

---

### 🔹 **Key Contributions**

1. **Online Transfer Learning Framework for SbPGs**  
   - Extends traditional SbPGs with mechanisms for transferring knowledge between players.
   - Enables **distributed self-optimization** without centralized control—critical for scalable smart manufacturing.

2. **Similarity Criteria Between Players**  
   Two settings are proposed to determine which knowledge to transfer:
   - **(a) Predefined Similarities**: Based on known system structure or prior domain knowledge (e.g., identical machine types).
   - **(b) Dynamically Inferred Similarities**: Learned during training using behavioral or state-action trajectory analysis, allowing adaptation to changing environments.

3. **Formal Justification of Transferability in SbPGs**  
   - Proves that under certain conditions, policy transfer across similar players preserves convergence properties of potential games.
   - Ensures theoretical soundness when applying transfer learning within game-theoretic learning frameworks.

4. **Optimization of Transfer Timing and Weighting**  
   - Introduces a method to decide *when* and *how much* to transfer (i.e., adaptive weighting of source policies).
   - Prevents negative transfer by modulating influence based on similarity and performance feedback.

5. **Experimental Validation on a Laboratory Testbed**
   - Real-world evaluation demonstrates:
     - **Improved production efficiency**
     - **Reduced power consumption**
   - Outperforms vanilla SbPGs (without transfer), showing faster convergence and better resource utilization.

---

### 🔹 **Practical Implications for Industry**
- **Scalable Learning in Smart Factories**: TL-SbPGs allow machines or subsystems to learn optimal control policies more quickly by leveraging experience from peers.
- **Energy Efficiency**: Demonstrated reduction in power usage aligns with sustainable manufacturing goals.
- **Decentralized Adaptation**: Suitable for flexible manufacturing systems where reconfiguration is frequent and centralized optimization is impractical.

---

### 🔹 **Significance of "Replace" Announcement Type**
The `replace` type indicates this version (**v3**) supersedes earlier versions of the same paper (v1, v2), likely incorporating revisions such as:
- Expanded experiments
- Refined theoretical proofs
- Improved clarity in methodology

This suggests the authors have responded to peer feedback or further validated their approach.

---

### 🔹 **Potential Applications Beyond Manufacturing**
While focused on manufacturing, TL-SbPGs could be applied to other domains involving decentralized multi-agent optimization:
- Smart grids
- Traffic signal control
- Distributed robotics
- Supply chain coordination

---

### 🔹 **Conclusion**
This paper makes a strong case for integrating **transfer learning** into **game-theoretic multi-agent systems**, particularly in real-world industrial contexts. By formalizing how and when agents should share knowledge, TL-SbPGs offer a robust, efficient, and scalable solution for online optimization in complex, dynamic environments.

> ✅ **Takeaway**: TL-SbPGs bridge game theory and transfer learning to enable faster, greener, and smarter distributed decision-making in manufacturing—demonstrating both theoretical rigor and practical impact.

Let me know if you'd like a simplified explanation, a diagram proposal, or help comparing it to related methods (e.g., MADRL, standard POT games). <div>
arXiv:2408.05992v3 Announce Type: replace 
Abstract: This paper presents a novel online transfer learning approach in state-based potential games (TL-SbPGs) for distributed self-optimization in manufacturing systems. The approach targets practical industrial scenarios where knowledge sharing among similar players enhances learning in large-scale and decentralized environments. TL-SbPGs enable players to reuse learned policies from others, which improves learning outcomes and accelerates convergence. To accomplish this goal, we develop transfer learning concepts and similarity criteria for players, which offer two distinct settings: (a) predefined similarities between players and (b) dynamically inferred similarities between players during training. The applicability of the SbPG framework to transfer learning is formally established. Furthermore, we present a method to optimize the timing and weighting of knowledge transfer. Experimental results from a laboratory-scale testbed show that TL-SbPGs improve production efficiency and reduce power consumption compared to vanilla SbPGs.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tazza: Shuffling Neural Network Parameters for Secure and Private Federated Learning</title>
<link>https://arxiv.org/abs/2412.07454</link>
<guid>https://arxiv.org/abs/2412.07454</guid>
<content:encoded><![CDATA[
<div> The provided text appears to be the abstract of a research paper with arXiv identifier **2412.07454v3**, which has been updated (as indicated by "Announce Type: replace"). Here's a breakdown and interpretation of the abstract for clarity:

---

**Paper Title (Inferred):** *Tazza: A Secure and Efficient Framework for Federated Learning*

**Key Contributions:**

- **Problem:** Federated learning (FL) enables decentralized training while preserving data privacy by avoiding raw data sharing. However, it remains vulnerable to two major security threats:
  - **Gradient inversion attacks**: Where adversaries reconstruct private client data from shared gradients.
  - **Model poisoning attacks**: Where malicious clients manipulate model updates to degrade performance or inject backdoors.

  Most existing defenses tackle only one of these issues, often at the cost of accuracy or system efficiency.

- **Solution:** The authors propose **Tazza**, a novel FL framework that **simultaneously defends against both gradient inversion and model poisoning** without sacrificing model accuracy or computational efficiency.

- **Core Techniques:**
  - **Weight Shuffling**: Exploits *permutation equivariance/invariance* in neural networks—reordering neuron weights does not affect functionality—to obscure model parameters and resist inversion attacks.
  - **Shuffled Model Validation**: Uses shuffling strategies during aggregation to detect and filter out poisoned models, improving robustness against adversarial clients.

- **Benefits:**
  - **Enhanced Security**: Resists diverse poisoning attacks and protects data confidentiality.
  - **High Accuracy**: Maintains model performance close to standard FL baselines.
  - **Efficiency**: Up to **6.7× faster** computation compared to alternative secure FL methods, making it suitable for deployment on **resource-constrained embedded platforms**.

- **Evaluation:**
  - Tested across multiple datasets and real-world embedded systems.
  - Demonstrates strong defense capabilities while maintaining scalability and practicality.

---

**Significance:**  
Tazza represents an integrated approach to secure federated learning, addressing two critical attack vectors in a unified manner—something many prior works fail to achieve. Its emphasis on **efficiency** makes it particularly promising for edge computing and IoT applications where resources are limited.

---

If you’re looking for more details, such as implementation specifics, threat models considered, or comparison benchmarks, I recommend downloading the full paper from [arXiv:2412.07454](https://arxiv.org/abs/2412.07454). Let me know if you'd like help summarizing specific sections or understanding technical components like permutation equivariance! <div>
arXiv:2412.07454v3 Announce Type: replace 
Abstract: Federated learning enables decentralized model training without sharing raw data, preserving data privacy. However, its vulnerability towards critical security threats, such as gradient inversion and model poisoning by malicious clients, remain unresolved. Existing solutions often address these issues separately, sacrificing either system robustness or model accuracy. This work introduces Tazza, a secure and efficient federated learning framework that simultaneously addresses both challenges. By leveraging the permutation equivariance and invariance properties of neural networks via weight shuffling and shuffled model validation, Tazza enhances resilience against diverse poisoning attacks, while ensuring data confidentiality and high model accuracy. Comprehensive evaluations on various datasets and embedded platforms show that Tazza achieves robust defense with up to 6.7x improved computational efficiency compared to alternative schemes, without compromising performance.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>SlideChain: Semantic Provenance for Lecture Understanding via Blockchain Registration</title>
<link>https://arxiv.org/abs/2512.21684</link>
<guid>https://arxiv.org/abs/2512.21684</guid>
<content:encoded><![CDATA[
<div> The abstract you've provided describes **arXiv:2512.21684v2**, a research paper introducing **SlideChain**, a novel framework that leverages blockchain technology to ensure the **integrity, reproducibility, and auditability** of semantic content extracted by vision–language models (VLMs) from educational materials—specifically medical imaging lecture slides.

Here is a clear breakdown and interpretation of the key contributions and findings:

---

### 🔍 **Problem Statement**
- **Challenge**: Modern VLMs are increasingly used in education (especially STEM/medical fields), but their outputs are:
  - Hard to verify
  - Prone to inconsistencies across models, inference settings, and environments
  - Lacking long-term reproducibility and traceability
- This undermines trust in AI-generated educational content, particularly where accuracy is critical (e.g., medicine).

---

### ✅ **Solution: SlideChain**
A **blockchain-backed provenance framework** designed for:
- Verifiable integrity of multimodal semantic extraction
- Tamper-evident logging of model outputs
- Long-term auditability

#### Key Components:
1. **Dataset**: 
   - *SlideChain Slides Dataset*: 1,117 real medical imaging lecture slides from a university course.
2. **Semantic Extraction**:
   - Four state-of-the-art VLMs extract:
     - Key concepts
     - Relational triples (subject–predicate–object)
   - Output structured into **provenance records** per slide.
3. **Blockchain Anchoring**:
   - Cryptographic hashes of these records are stored on a **local EVM-compatible blockchain**.
   - Ensures:
     - Immutability
     - Tamper detection
     - Deterministic reproducibility

---

### 🧪 **Key Findings & Analysis**
1. **Cross-Model Discrepancies**:
   - Low overlap in extracted **concepts** across different VLMs.
   - Near-zero agreement in generated **relational triples** on many slides.
   - Highlights the **lack of consistency** even among SOTA models → major concern for reliability.

2. **Lecture-Level Variability**:
   - First systematic analysis showing how semantic output varies not just between models, but also across lectures/slide topics.
   - Suggests domain-specific challenges in multimodal understanding.

3. **System Performance**:
   - Evaluated under simulated deployment:
     - Measured **gas usage**, **throughput**, and **scalability**.
   - Results show:
     - Efficient operation at scale
     - **Perfect tamper detection**
     - **Deterministic reproducibility** across independent runs

---

### 🛠️ **Implications & Contributions**
- **Trustworthy AI in Education**:
  - SlideChain enables verifiable, auditable pipelines for AI-assisted teaching tools.
- **Reproducibility**:
  - By anchoring semantic outputs on-chain, future users can validate or compare results against a persistent baseline.
- **Scalable Design**:
  - Local EVM setup balances security and practicality without requiring public blockchain costs or latency.
- **New Research Direction**:
  - Opens path for studying semantic drift, model alignment, and accountability in multimodal systems.

---

### 📌 Conclusion
> *SlideChain represents a significant step toward trustworthy, transparent, and accountable use of VLMs in high-stakes educational contexts.*  
It addresses core issues of **semantic inconsistency** and **lack of provenance**, offering a scalable solution using blockchain to secure AI-generated knowledge.

---

### 🎯 Potential Applications
- Medical education platforms using AI tutors
- Regulatory compliance for AI-generated courseware
- Model comparison benchmarks with verified outputs
- Archival systems for evolving curricula with AI assistance

---

If you're interested, I can help with:
- Summarizing the full paper (once published)
- Explaining technical aspects (e.g., how hashing + Ethereum works here)
- Comparing SlideChain to other provenance/blockchain-AI systems
- Generating a presentation or infographic based on this work

Let me know your needs! <div>
arXiv:2512.21684v2 Announce Type: replace 
Abstract: Modern vision--language models (VLMs) are increasingly used to interpret and generate educational content, yet their semantic outputs remain challenging to verify, reproduce, and audit over time. Inconsistencies across model families, inference settings, and computing environments undermine the reliability of AI-generated instructional material, particularly in high-stakes and quantitative STEM domains. This work introduces SlideChain, a blockchain-backed provenance framework designed to provide verifiable integrity for multimodal semantic extraction at scale. Using the SlideChain Slides Dataset-a curated corpus of 1,117 medical imaging lecture slides from a university course-we extract concepts and relational triples from four state-of-the-art VLMs and construct structured provenance records for every slide. SlideChain anchors cryptographic hashes of these records on a local EVM (Ethereum Virtual Machine)-compatible blockchain, providing tamper-evident auditability and persistent semantic baselines. Through the first systematic analysis of semantic disagreement, cross-model similarity, and lecture-level variability in multimodal educational content, we reveal pronounced cross-model discrepancies, including low concept overlap and near-zero agreement in relational triples on many slides. We further evaluate gas usage, throughput, and scalability under simulated deployment conditions, and demonstrate perfect tamper detection along with deterministic reproducibility across independent extraction runs. Together, these results show that SlideChain provides a practical and scalable step toward trustworthy, verifiable multimodal educational pipelines, supporting long-term auditability, reproducibility, and integrity for AI-assisted instructional systems.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Multi-Task Clustering</title>
<link>https://arxiv.org/abs/2512.22897</link>
<guid>https://arxiv.org/abs/2512.22897</guid>
<content:encoded><![CDATA[
<div> The provided text appears to be the **abstract** of a research paper with the arXiv identifier **2512.22897v2**, which indicates it's the second version (revised) of the preprint. Here's a breakdown and interpretation of the abstract, followed by a concise summary suitable for understanding or presentation:

---

### 🔍 **Breakdown of the Abstract**

#### **Problem Context**
- **Spectral clustering** is known for high performance in clustering tasks.
- Traditional methods assume **centralized data**, which doesn't align with modern **decentralized/federated environments** where data privacy is critical.
- Existing **federated clustering approaches** suffer from:
  - Use of **unreliable pseudo-labels**, hurting generalization.
  - Inability to model **latent correlations among heterogeneous clients**.

#### **Proposed Solution: FMTC (Federated Multi-Task Clustering)**
A novel framework designed for **personalized yet collaborative clustering** under federation:
1. **Client-Side Personalized Clustering Module**
   - Learns a **parameterized mapping function** (e.g., neural network) per client.
   - Enables **robust out-of-sample inference** (can cluster new samples without re-computation).
   - Avoids reliance on **pseudo-labels**, improving reliability.

2. **Server-Side Tensorial Correlation Module**
   - Models inter-client relationships via **tensor representation**: all client models are organized into a single tensor.
   - Applies **low-rank regularization** to uncover a **shared latent subspace** across clients.
   - Captures **common structure** while respecting individual heterogeneity.

#### **Optimization & Privacy**
- Uses **ADMM (Alternating Direction Method of Multipliers)** to solve the joint optimization problem.
- The algorithm enables:
  - **Distributed computation**: local updates on clients.
  - **Secure aggregation**: server coordinates without accessing raw data.
  - **Privacy preservation**: no direct exchange of sensitive information.

#### **Evaluation**
- Tested on **multiple real-world datasets**.
- Outperforms various **baseline and state-of-the-art federated clustering methods** in effectiveness and robustness.

---

### ✅ **Summary (Suitable for Presentation or Report)**

> This paper introduces **Federated Multi-Task Clustering (FMTC)**, a novel framework addressing key limitations in current federated clustering methods. FMTC enables effective, personalized clustering in decentralized settings by combining **client-specific modeling** with **collaborative learning of shared structures**. It avoids error-prone pseudo-labels through parameterized clustering mappings and captures cross-client correlations using a **tensor-based low-rank regularization** approach. Leveraging an efficient **ADMM-based distributed optimization**, FMTC ensures privacy-preserving collaboration. Experiments show that FMTC significantly outperforms existing methods across diverse real-world datasets, marking a substantial advance in federated unsupervised learning.

---

### 📌 Key Contributions
1. **First use of tensor modeling** in federated clustering to capture multi-client correlations.
2. Eliminates dependence on **pseudo-labels**, enhancing robustness.
3. Enables **out-of-sample extension** via learnable mappings.
4. Integrates **personalization and global coordination** via low-rank tensor constraints.
5. Provides a **privacy-preserving, scalable ADMM algorithm** for federated clustering.

---

### 🧠 Potential Applications
- Healthcare (multi-institutional patient grouping without sharing records)
- IoT networks (device behavior clustering across edge nodes)
- Finance (customer segmentation across isolated branches)

---

If you'd like, I can help you:
- Generate a **title suggestion** for this work,
- Write a **blog post or slide deck** summarizing the paper,
- Compare FMTC to other federated clustering methods (like FedEM, CFL),
- Explain the **ADMM derivation** or **tensor formulation** in more detail.

Let me know how you'd like to proceed! <div>
arXiv:2512.22897v2 Announce Type: replace 
Abstract: Spectral clustering has emerged as one of the most effective clustering algorithms due to its superior performance. However, most existing models are designed for centralized settings, rendering them inapplicable in modern decentralized environments. Moreover, current federated learning approaches often suffer from poor generalization performance due to reliance on unreliable pseudo-labels, and fail to capture the latent correlations amongst heterogeneous clients. To tackle these limitations, this paper proposes a novel framework named Federated Multi-Task Clustering (i.e.,FMTC), which intends to learn personalized clustering models for heterogeneous clients while collaboratively leveraging their shared underlying structure in a privacy-preserving manner. More specifically, the FMTC framework is composed of two main components: client-side personalized clustering module, which learns a parameterized mapping model to support robust out-of-sample inference, bypassing the need for unreliable pseudo-labels; and server-side tensorial correlation module, which explicitly captures the shared knowledge across all clients. This is achieved by organizing all client models into a unified tensor and applying a low-rank regularization to discover their common subspace. To solve this joint optimization problem, we derive an efficient, privacy-preserving distributed algorithm based on the Alternating Direction Method of Multipliers, which decomposes the global problem into parallel local updates on clients and an aggregation step on the server. To the end, several extensive experiments on multiple real-world datasets demonstrate that our proposed FMTC framework significantly outperforms various baseline and state-of-the-art federated clustering algorithms.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contingency Model-based Control (CMC) for Communicationless Cooperative Collision Avoidance in Robot Swarms</title>
<link>https://arxiv.org/abs/2512.20391</link>
<guid>https://arxiv.org/abs/2512.20391</guid>
<content:encoded><![CDATA[
<div> The provided text appears to be a **metadata and abstract snippet** from an arXiv paper with the identifier **arXiv:2512.20391v3**, submitted under the "replace-cross" announcement type—indicating it's an updated version of a cross-listed paper.

Here’s a clear summary and interpretation of the content:

---

### 📄 **Paper Summary: arXiv:2512.20391v3**

**Title (inferred):** *Contingency Model-based Control (CMC) for Communication-Free Decentralized Swarm Collision Avoidance*

**Core Problem:**  
Cooperative collision avoidance in robot swarms is challenging, especially in decentralized systems where each agent must independently make decisions. Most current methods depend on inter-agent communication, which introduces vulnerabilities due to:
- Latency and delays
- Packet loss
- Transmission faults
- Adversarial attacks (e.g., jamming, spoofing)

These limitations motivate the need for **communication-free coordination strategies**.

---

### ✅ **Proposed Solution: Contingency Model-based Control (CMC)**

**Key Idea:**  
A fully **decentralized**, **communication-free** control framework inspired by consensual offline rules—akin to traffic laws (e.g., right-of-way rules). All agents follow pre-agreed protocols designed *offline*, enabling safe real-time operation without exchanging data during runtime.

#### 🔧 Key Components of CMC:
1. **Contingency Trajectory**:  
   Each robot has a predefined "safe" trajectory it will follow if conflict arises—a fallback plan encoded in advance.

2. **Perpendicular Bisecting Planes (PBP):**  
   Geometric constructs used as **collision avoidance constraints** between pairs of agents. These planes act as decision boundaries: robots adjust their motion to ensure they don’t simultaneously occupy risky regions relative to one another.

3. **Recursive Feasibility & Safety Guarantees:**  
   The method ensures that:
   - Safe states are always reachable (recursive feasibility)
   - No collisions occur in closed-loop operation (guaranteed collision avoidance)

4. **Plug-and-Play Capability:**  
   New robots can join the swarm dynamically without reconfiguring the entire system—each follows the same rule set.

---

### 🧪 Validation
- Demonstrated via **two numerical simulations**
- Results show:
  - Robust **collision avoidance** even in constrained environments
  - Smooth swarm behavior
  - Preservation of safety guarantees

---

### 🌟 Significance and Contributions

| Contribution | Description |
|------------|-------------|
| **No Communication Required** | Eliminates reliance on unreliable wireless networks |
| **Safety Guarantees** | Formal proof of collision avoidance and recursive feasibility |
| **Scalability & Flexibility** | Supports dynamic entry/exit of agents (plug-and-play) |
| **Resilience** | Inherently robust to adversarial interference targeting communications |

---

### 🛣️ Potential Applications
- Autonomous drone swarms in GPS-denied or contested environments
- Multi-robot warehouse logistics
- Search and rescue operations with limited comms
- Military or security robotics operating under electronic warfare conditions

---

### 🔮 Future Directions (Implied)
- Extension to 3D environments
- Handling heterogeneous agents
- Integration with high-level planning or learning modules
- Real-world hardware implementation and testing

---

### Final Thoughts:
This paper presents a **novel shift** from communication-dependent swarm control to a **rule-based, model-driven decentralized paradigm**. By leveraging geometric constraints and contingency modeling, CMC offers a promising path toward **provably safe, resilient, and scalable swarm autonomy**—especially valuable in mission-critical or adversarial settings.

If you'd like, I can help:
- Locate the full paper on arXiv (note: `2512.20391` seems to use a future date; possibly a placeholder or typo — likely meant to be `24xx.xxxx` or similar)
- Explain the math behind perpendicular bisecting planes
- Compare CMC to other approaches like Velocity Obstacles or MPC-based schemes
- Summarize related work in communication-free coordination

Let me know how you’d like to proceed! <div>
arXiv:2512.20391v3 Announce Type: replace-cross 
Abstract: Cooperative collision avoidance between robots, or `agents,' in swarm operations remains an open challenge. Assuming a decentralized architecture, each agent is responsible for making its own decisions and choosing its control actions. Most existing approaches rely on a (wireless) communication network between (some of) the agents. In reality, however, communication is brittle. It may be affected by latency, further delays and packet losses, and transmission faults. Moreover, it is subject to adversarial attacks, such as jamming or spoofing. This paper proposes Contingency Model-based Control (CMC), a decentralized cooperative approach that does not rely on communication. Instead, the control algorithm is based on consensual rules that are designed for all agents offline, similar to traffic rules. For CMC, this includes the definition of a contingency trajectory for each robot, and perpendicular bisecting planes as collision avoidance constraints. The setup permits a full guarantee of recursive feasibility and collision avoidance between all swarm members in closed-loop operation. CMC naturally satisfies the plug & play paradigm, i.e., new robots may enter the swarm dynamically. The effectiveness of the CMC regime is demonstrated in two numerical examples, showing that the collision avoidance guarantee is intact and the robot swarm operates smoothly in a constrained environment.
]]></content:encoded>
<pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
</item>
<item>
<title>DyPBP: Dynamic Peer Beneficialness Prediction for Cryptocurrency P2P Networking</title>
<link>https://arxiv.org/abs/2511.17523</link>
<guid>https://arxiv.org/abs/2511.17523</guid>
<content:encoded><![CDATA[
arXiv:2511.17523v1 Announce Type: new 
Abstract: Distributed peer-to-peer (P2P) networking delivers the new blocks and transactions and is critical for the cryptocurrency blockchain system operations. Having poor P2P connectivity reduces the financial rewards from the mining consensus protocol. Previous research defines beneficalness of each Bitcoin peer connection and estimates the beneficialness based on the observations of the blocks and transactions delivery, which are after they are delivered. However, due to the infrequent block arrivals and the sporadic and unstable peer connections, the peers do not stay connected long enough to have the beneficialness score to converge to its expected beneficialness. We design and build Dynamic Peer Beneficialness Prediction (DyPBP) which predicts a peer's beneficialness by using networking behavior observations beyond just the block and transaction arrivals. DyPBP advances the previous research by estimating the beneficialness of a peer connection before it delivers new blocks and transactions. To achieve such goal, DyPBP introduces a new feature for remembrance to address the dynamic connectivity issue, as Bitcoin's peers using distributed networking often disconnect and re-connect. We implement DyPBP on an active Bitcoin node connected to the Mainnet and use machine learning for the beneficialness prediction. Our experimental results validate and evaluate the effectiveness of DyPBP; for example, the error performance improves by 2 to 13 orders of magnitude depending on the machine-learning model selection. DyPBP's use of the remembrance feature also informs our model selection. DyPBP enables the P2P connection's beneficialness estimation from the connection start before a new block arrives.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bunny Hops and Blockchain Stops: Cross-Chain MEV Detection With N-Hops</title>
<link>https://arxiv.org/abs/2511.17527</link>
<guid>https://arxiv.org/abs/2511.17527</guid>
<content:encoded><![CDATA[
arXiv:2511.17527v1 Announce Type: new 
Abstract: This student paper introduces a novel methodology for the detection and analysis of multihop cross-chain arbitrage opportunities, wherein multihop denotes arbitrage sequences involving more than two transactional steps across distinct blockchain networks, executed using sequence-dependent strategies. Utilizing a comprehensive dataset comprising over 2.4 billion transactions recorded between September 2023 and August 2024 (encompassing 12 blockchain platforms and 45 cross-chain bridges) we design and implement an algorithm capable of identifying, sequence-dependent arbitrage paths spanning multiple ecosystems. Our empirical analysis demonstrates that such arbitrage opportunities are exceedingly infrequent, underscoring the inherent challenges associated with multihop execution in cross-chain environments.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iterative Negotiation and Oversight: A Case Study in Decentralized Air Traffic Management</title>
<link>https://arxiv.org/abs/2511.17625</link>
<guid>https://arxiv.org/abs/2511.17625</guid>
<content:encoded><![CDATA[
arXiv:2511.17625v1 Announce Type: new 
Abstract: Achieving consensus among noncooperative agents remains challenging in decentralized multi-agent systems, where agents often have conflicting preferences. Existing coordination methods enable agents to reach consensus without a centralized coordinator, but do not provide formal guarantees on system-level objectives such as efficiency or fairness. To address this limitation, we propose an iterative negotiation and oversight framework that augments a decentralized negotiation mechanism with taxation-like oversight. The framework builds upon the trading auction for consensus, enabling noncooperative agents with conflicting preferences to negotiate through asset trading while preserving valuation privacy. We introduce an oversight mechanism, which implements a taxation-like intervention that guides decentralized negotiation toward system-efficient and equitable outcomes while also regulating how fast the framework converges. We establish theoretical guarantees of finite-time termination and derive bounds linking system efficiency and convergence rate to the level of central intervention. A case study based on the collaborative trajectory options program, a rerouting initiative in U.S. air traffic management, demonstrates that the framework can reliably achieve consensus among noncooperative airspace sector managers, and reveals how the level of intervention regulates the relationship between system efficiency and convergence speed. Taken together, the theoretical and experimental results indicate that the proposed framework provides a general mechanism for decentralized coordination in noncooperative multi-agent systems while safeguarding system-level objectives.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian probabilistic exploration of Bitcoin informational quanta and interactions under the GITT-VT paradigm</title>
<link>https://arxiv.org/abs/2511.17646</link>
<guid>https://arxiv.org/abs/2511.17646</guid>
<content:encoded><![CDATA[
arXiv:2511.17646v1 Announce Type: new 
Abstract: This study explores Bitcoin's value formation through the Granular Interaction Thinking Theory-Value Theory (GITT-VT). Rather than stemming from material utility or cash flows, Bitcoin's value arises from informational attributes and interactions of multiple factors, including cryptographic order, decentralization-enabled autonomy, trust embedded in the consensus mechanism, and socio-narrative coherence that reduce entropy within decentralized value-exchange processes. To empirically assess this perspective, a Bayesian linear model was estimated using daily data from 2022 to 2025, operationalizing four informational value dimensions: Store-of-Value (SOV), Autonomy (AUT), Social-Signal Value (SSV), and Hedonic-Sentiment Value (HSV). Results indicate that only SSV exerts a highly credible positive effect on next-day returns, highlighting the dominant role of high-entropy social information in short-term pricing dynamics. In contrast, SOV and AUT show moderately reliable positive associations, reflecting their roles as low-entropy structural anchors of long-term value. HSV displays no credible predictive effect. The study advances interdisciplinary value theory and demonstrates Bitcoin as a dual-layer entropy-regulating socio-technological ecosystem. The findings offer implications for digital asset valuation, investment education, and future research on entropy dynamics across non-cash-flow digital assets.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Coordination in Autonomous Vehicle Routing: A Simulation-Based Study of Communication, Memory, and Routing Loops</title>
<link>https://arxiv.org/abs/2511.17656</link>
<guid>https://arxiv.org/abs/2511.17656</guid>
<content:encoded><![CDATA[
arXiv:2511.17656v1 Announce Type: new 
Abstract: Multi-agent coordination is critical for next-generation autonomous vehicle (AV) systems, yet naive implementations of communication-based rerouting can lead to catastrophic performance degradation. This study investigates a fundamental problem in decentralized multi-agent navigation: routing loops, where vehicles without persistent obstacle memory become trapped in cycles of inefficient path recalculation. Through systematic simulation experiments involving 72 unique configurations across varying vehicle densities (15, 35, 55 vehicles) and obstacle frequencies (6, 20 obstacles), we demonstrate that memory-less reactive rerouting increases average travel time by up to 682% compared to baseline conditions. To address this, we introduce Object Memory Management (OMM), a lightweight mechanism enabling agents to retain and share knowledge of previously encountered obstacles. OMM operates by maintaining a distributed blacklist of blocked nodes, which each agent consults during Dijkstra-based path recalculation, effectively preventing redundant routing attempts. Our results show that OMM-enabled coordination reduces average travel time by 75.7% and wait time by 88% compared to memory-less systems, while requiring only 1.67 route recalculations per vehicle versus 9.83 in memory-less scenarios. This work provides empirical evidence that persistent, shared memory is not merely beneficial but essential for robust multi-agent coordination in dynamic environments. The findings have implications beyond autonomous vehicles, informing the design of decentralized systems in robotics, network routing, and distributed AI. We provide a comprehensive experimental analysis, including detailed scenario breakdowns, scalability assessments, and visual documentation of the routing loop phenomenon, demonstrating OMM's critical role in preventing detrimental feedback cycles in cooperative multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Homomorphic Encryption-based Vaults for Anonymous Balances on VM-enabled Blockchains</title>
<link>https://arxiv.org/abs/2511.17842</link>
<guid>https://arxiv.org/abs/2511.17842</guid>
<content:encoded><![CDATA[
arXiv:2511.17842v1 Announce Type: new 
Abstract: In this work, we present homomorphic encryption-based vaults (Haults), a permissioned privacy-preserving smart wallet protocol for VM-enabled blockchains that keeps users' balances confidential, as well as the amounts transacted to other parties. To comply with regulations, we include optional compliance features that allow specific entities (the auditors) to retrieve transaction amounts or execute force transfers when necessary. Our solution uses ElGamal over elliptic curves to encrypt balances, combined with zero-knowledge proofs to verify the correctness of transaction amounts and the integrity of the sender's updated balance, among other security checks. We provide a detailed explanation of the protocol, including a security discussion and benchmarks from our proof-of-concept implementation, which yield great results. Beyond in-contract issued tokens, we also provide a thorough explanation on how our solution can be compatible with external ones (e.g., Ether or any ERC20).
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DISPATCH -- Decentralized Informed Spatial Planning and Assignment of Tasks for Cooperative Heterogeneous Agents</title>
<link>https://arxiv.org/abs/2511.17915</link>
<guid>https://arxiv.org/abs/2511.17915</guid>
<content:encoded><![CDATA[
arXiv:2511.17915v1 Announce Type: new 
Abstract: Spatial task allocation in systems such as multi-robot delivery or ride-sharing requires balancing efficiency with fair service across tasks. Greedy assignment policies that match each agent to its highest-preference or lowest-cost task can maximize efficiency but often create inequities: some tasks receive disproportionately favorable service (e.g., shorter delays or better matches), while others face long waits or poor allocations.
  We study fairness in heterogeneous multi-agent systems where tasks vary in preference alignment and urgency. Most existing approaches either assume centralized coordination or largely ignore fairness under partial observability. Distinct from this prior work, we establish a connection between the Eisenberg-Gale (EG) equilibrium convex program and decentralized, partially observable multi-agent learning. Building on this connection, we develop two equilibrium-informed algorithms that integrate fairness and efficiency: (i) a multi-agent reinforcement learning (MARL) framework, EG-MARL, whose training is guided by centralized fair assignment algorithms (EG and a preference-aware Hungarian method); and (ii) a stochastic online optimization mechanism that performs guided exploration and subset-based fair assignment as tasks are discovered.
  We evaluate our frameworks across a range of team sizes and assignment formulations against centralized EG, Hungarian, and Min-Max Distance baselines. Both algorithms preserve the fairness-efficiency balance of the Eisenberg-Gale equilibrium under partial observability. EG-MARL achieves near-centralized coordination and reduced travel distances, while the stochastic online mechanism enables real-time allocation with competitive fairness. Together, these results demonstrate that spatially aware EG formulations can effectively guide decentralized coordination in agents with heterogeneous capabilities.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty-Aware Federated Learning for Cyber-Resilient Microgrid Energy Management</title>
<link>https://arxiv.org/abs/2511.17968</link>
<guid>https://arxiv.org/abs/2511.17968</guid>
<content:encoded><![CDATA[
arXiv:2511.17968v1 Announce Type: new 
Abstract: Maintaining economic efficiency and operational reliability in microgrid energy management systems under cyberattack conditions remains challenging. Most approaches assume non-anomalous measurements, make predictions with unquantified uncertainties, and do not mitigate malicious attacks on renewable forecasts for energy management optimization. This paper presents a comprehensive cyber-resilient framework integrating federated Long Short-Term Memory-based photovoltaic forecasting with a novel two-stage cascade false data injection attack detection and energy management system optimization. The approach combines autoencoder reconstruction error with prediction uncertainty quantification to enable attack-resilient energy storage scheduling while preserving data privacy. Extreme false data attack conditions were studied that caused 58% forecast degradation and 16.9\% operational cost increases. The proposed integrated framework reduced false positive detections by 70%, recovered 93.7% of forecasting performance losses, and achieved 5\% operational cost savings, mitigating 34.7% of attack-induced economic losses. Results demonstrate that precision-focused cascade detection with multi-signal fusion outperforms single-signal approaches, validating security-performance synergy for decentralized microgrids.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Fair OR-ML Framework for Resource Substitution in Large-Scale Networks</title>
<link>https://arxiv.org/abs/2511.18269</link>
<guid>https://arxiv.org/abs/2511.18269</guid>
<content:encoded><![CDATA[
arXiv:2511.18269v1 Announce Type: new 
Abstract: Ensuring that the right resource is available at the right location and time remains a major challenge for organizations operating large-scale logistics networks. The challenge comes from uneven demand patterns and the resulting asymmetric flow of resources across the arcs, which create persistent imbalances at the network nodes. Resource substitution among multiple, potentially composite and interchangeable, resource types is a cost-effective way to mitigate these imbalances. This leads to the resource substitution problem, which aims at determining the minimum number of resource substitutions from an initial assignment to minimize the overall network imbalance. In decentralized settings, achieving globally coordinated solutions becomes even more difficult. When substitution entails costs, effective prescriptions must also incorporate fairness and account for the individual preferences of schedulers. This paper presents a generic framework that combines operations research (OR) and machine learning (ML) to enable fair resource substitution in large networks. The OR component models and solves the resource substitution problem under a fairness lens. The ML component leverages historical data to learn schedulers' preferences, guide intelligent exploration of the decision space, and enhance computational efficiency by dynamically selecting the top-$\kappa$ resources for each arc in the network. The framework produces a portfolio of high-quality solutions from which schedulers can select satisfactory trade-offs. The proposed framework is applied to the network of one of the largest package delivery companies in the world, which serves as the primary motivation for this research. Computational results demonstrate substantial improvements over state-of-the-art methods, including an 80% reduction in model size and a 90% decrease in execution time while preserving optimality.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ADF-LoRA: Alternating Low-Rank Aggregation for Decentralized Federated Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.18291</link>
<guid>https://arxiv.org/abs/2511.18291</guid>
<content:encoded><![CDATA[
arXiv:2511.18291v1 Announce Type: new 
Abstract: This paper revisits alternating low-rank updates for federated fine-tuning and examines their behavior in decentralized federated learning (DFL). While alternating the LoRA matrices has been shown to stabilize aggregation in centralized FL, extending this mechanism to decentralized, peer-to-peer communication introduces new challenges due to phase-state mismatch and block-wise divergence across clients. We introduce ADF-LoRA, which synchronizes the update of only one low-rank matrix per round and mixes both matrices to maintain more consistent parameter states under decentralized propagation. This design preserves the cross-term suppression effect of alternating updates while improving stability in serverless topologies. We provide a convergence analysis under standard smoothness assumptions and evaluate ADF-LoRA on multiple GLUE tasks. Experiments show that ADF-LoRA achieves faster and smoother convergence and delivers the highest average accuracy across tasks, outperforming existing LoRA variants in decentralized FL by a consistent margin.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Addressing Isolation in Blockchain-Based Self-Sovereign Identity</title>
<link>https://arxiv.org/abs/2511.18379</link>
<guid>https://arxiv.org/abs/2511.18379</guid>
<content:encoded><![CDATA[
arXiv:2511.18379v1 Announce Type: new 
Abstract: Self-Sovereign Identity (SSI) grants holders full ownership and control of their digital identities, being the ultimate digital identity model. Operating in a decentralized manner, SSI enables the verification of claims, including privacy-preserving mechanisms. Blockchain, which can be used to implement a Verifiable Data Registry (VDR), is often considered one of the pillars of SSI, along with Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs). Unfortunately, blockchains are mostly siloed, affecting the interoperability and universality of SSI. We investigate the effect of blockchain isolation on blockchain-based SSI. We first define possible scenarios for cross-chain SSI and exemplify with real-life use cases. We then define specific requirements for cross-chain SSI and identify challenges, also in relation to the identified scenarios. We explore various solutions to achieve blockchain interoperability, with a focus on SSI. In particular, we identify the advantages and disadvantages of distinct cross-chain models for cross-chain SSI. Finally, we address the usability of cross-chain SSI and discuss security and privacy aspects, opening the way for future research.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs as Firmware Experts: A Runtime-Grown Tree-of-Agents Framework</title>
<link>https://arxiv.org/abs/2511.18438</link>
<guid>https://arxiv.org/abs/2511.18438</guid>
<content:encoded><![CDATA[
arXiv:2511.18438v1 Announce Type: new 
Abstract: Large Language Models (LLMs) and their agent systems have recently demonstrated strong potential in automating code reasoning and vulnerability detection. However, when applied to large-scale firmware, their performance degrades due to the binary nature of firmware, complex dependency structures, and heterogeneous components. To address this challenge, this paper presents FIRMHIVE, a recursive agent hive that enables LLMs to act as autonomous firmware security analysts. FIRMHIVE introduces two key mechanisms: (1) transforming delegation into a per-agent, executable primitive and (2) constructing a runtime Tree of Agents (ToA) for decentralized coordination. We evaluate FIRMHIVE using real-world firmware images obtained from publicly available datasets, covering five representative security analysis tasks. Compared with existing LLM-agent baselines, FIRMHIVE performs deeper (about 16x more reasoning steps) and broader (about 2.3x more files inspected) cross-file exploration, resulting in about 5.6x more alerts per firmware. Compared to state-of-the-art (SOTA) security tools, FIRMHIVE identifies about 1.5x more vulnerabilities (1,802 total) and achieves 71% precision, representing significant improvements in both yield and fidelity.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DEXO: A Secure and Fair Exchange Mechanism for Decentralized IoT Data Markets</title>
<link>https://arxiv.org/abs/2511.18498</link>
<guid>https://arxiv.org/abs/2511.18498</guid>
<content:encoded><![CDATA[
arXiv:2511.18498v1 Announce Type: new 
Abstract: Opening up data produced by the Internet of Things (IoT) and mobile devices for public utilization can maximize their economic value. Challenges remain in the trustworthiness of the data sources and the security of the trading process, particularly when there is no trust between the data providers and consumers. In this paper, we propose DEXO, a decentralized data exchange mechanism that facilitates secure and fair data exchange between data consumers and distributed IoT/mobile data providers at scale, allowing the consumer to verify the data generation process and the providers to be compensated for providing authentic data, with correctness guarantees from the exchange platform. To realize this, DEXO extends the decentralized oracle network model that has been successful in the blockchain applications domain to incorporate novel hardware-cryptographic co-design that harmonizes trusted execution environment, secret sharing, and smart contract-assisted fair exchange. For the first time, DEXO ensures end-to-end data confidentiality, source verifiability, and fairness of the exchange process with strong resilience against participant collusion. We implemented a prototype of the DEXO system to demonstrate feasibility. The evaluation shows a moderate deployment cost and significantly improved blockchain operation efficiency compared to a popular data exchange mechanism.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lean 5.0: A Predictive, Human-AI, and Ethically Grounded Paradigm for Construction Management</title>
<link>https://arxiv.org/abs/2511.18651</link>
<guid>https://arxiv.org/abs/2511.18651</guid>
<content:encoded><![CDATA[
arXiv:2511.18651v1 Announce Type: new 
Abstract: This paper introduces Lean 5.0, a human-centric evolution of Lean-Digital integration that connects predictive analytics, AI collaboration, and continuous learning within Industry 5.0 and Construction 5.0 contexts. A systematic literature review (2019-2024) and a 12-week empirical validation study demonstrate measurable performance gains, including a 13% increase in Plan Percent Complete (PPC), 22% reduction in rework, and 42% improvement in forecast accuracy. The study adopts a mixed-method Design Science Research (DSR) approach aligned with PRISMA 2020 guidelines. The paper also examines integration with digital twin and blockchain technologies to improve traceability, auditability, and lifecycle transparency. Despite limitations related to sample size, single-case design, and study duration, the findings show that Lean 5.0 provides a transformative paradigm connecting human cognition with predictive control in construction management.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Cross-Entropy Method with Monotonic Nonlinear Critic Decomposition</title>
<link>https://arxiv.org/abs/2511.18671</link>
<guid>https://arxiv.org/abs/2511.18671</guid>
<content:encoded><![CDATA[
arXiv:2511.18671v1 Announce Type: new 
Abstract: Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution (CTDE), where centralized critics leverage global information to guide decentralized actors. However, centralized-decentralized mismatch (CDM) arises when the suboptimal behavior of one agent degrades others' learning. Prior approaches mitigate CDM through value decomposition, but linear decompositions allow per-agent gradients at the cost of limited expressiveness, while nonlinear decompositions improve representation but require centralized gradients, reintroducing CDM. To overcome this trade-off, we propose the multi-agent cross-entropy method (MCEM), combined with monotonic nonlinear critic decomposition (NCD). MCEM updates policies by increasing the probability of high-value joint actions, thereby excluding suboptimal behaviors. For sample efficiency, we extend off-policy learning with a modified k-step return and Retrace. Analysis and experiments demonstrate that MCEM outperforms state-of-the-art methods across both continuous and discrete action benchmarks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bipartiteness in Progressive Second-Price Multi-Auction Networks with Perfect Substitute</title>
<link>https://arxiv.org/abs/2511.19225</link>
<guid>https://arxiv.org/abs/2511.19225</guid>
<content:encoded><![CDATA[
arXiv:2511.19225v1 Announce Type: new 
Abstract: We consider a bipartite network of buyers and sellers, where the sellers run locally independent Progressive Second-Price (PSP) auctions, and buyers may participate in multiple auctions, forming a multi-auction market with perfect substitute. The paper develops a projection-based influence framework for decentralized PSP auctions. We formalize primary and expanded influence sets using projections on the active bid index set and show how partial orders on bid prices govern allocation, market shifts, and the emergence of saturated one-hop shells. Our results highlight the robustness of PSP auctions in decentralized environments by introducing saturated components and a structured framework for phase transitions in multi-auction dynamics. This structure ensures deterministic coverage of the strategy space, enabling stable and truthful embedding in the larger game. We further model intra-round dynamics using an index to capture coordinated asynchronous seller updates coupled through buyers' joint constraints. Together, these constructions explain how local interactions propagate across auctions and gives premise for coherent equilibria--without requiring global information or centralized control.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems</title>
<link>https://arxiv.org/abs/2511.19368</link>
<guid>https://arxiv.org/abs/2511.19368</guid>
<content:encoded><![CDATA[
arXiv:2511.19368v1 Announce Type: new 
Abstract: Multi-agent reinforcement learning (MARL) has been increasingly adopted in many real-world applications. While MARL enables decentralized deployment on resource-constrained edge devices, it suffers from severe non-stationarity due to the synchronous updates of agent policies. This non stationarity results in unstable training and poor policy con vergence, especially as the number of agents increases. In this paper, we propose RELED, a scalable MARL framework that integrates large language model (LLM)-driven expert demonstrations with autonomous agent exploration. RELED incorporates a Stationarity-Aware Expert Demonstration module, which leverages theoretical non-stationarity bounds to enhance the quality of LLM-generated expert trajectories, thus providing high reward and training-stable samples for each agent. Moreover, a Hybrid Expert-Agent Policy Optimization module adaptively balances each agent's learning from both expert-generated and agent-generated trajectories, accelerating policy convergence and improving generalization. Extensive experiments with real city networks based on OpenStreetMap demonstrate that RELED achieves superior performance compared to state-of-the-art MARL methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analog Physical Systems Can Exhibit Double Descent</title>
<link>https://arxiv.org/abs/2511.17825</link>
<guid>https://arxiv.org/abs/2511.17825</guid>
<content:encoded><![CDATA[
arXiv:2511.17825v1 Announce Type: cross 
Abstract: An important component of the success of large AI models is double descent, in which networks avoid overfitting as they grow relative to the amount of training data, instead improving their performance on unseen data. Here we demonstrate double descent in a decentralized analog network of self-adjusting resistive elements. This system trains itself and performs tasks without a digital processor, offering potential gains in energy efficiency and speed -- but must endure component non-idealities. We find that standard training fails to yield double descent, but a modified protocol that accommodates this inherent imperfection succeeds. Our findings show that analog physical systems, if appropriately trained, can exhibit behaviors underlying the success of digital AI. Further, they suggest that biological systems might similarly benefit from over-parameterization.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How improving performance may imply losing consistency in event-triggered consensus</title>
<link>https://arxiv.org/abs/2405.03245</link>
<guid>https://arxiv.org/abs/2405.03245</guid>
<content:encoded><![CDATA[
arXiv:2405.03245v2 Announce Type: replace 
Abstract: Event-triggered control is often argued to lower the average triggering rate compared to time-triggered control while still achieving a desired control goal, e.g., the same performance level. However, this property, often called consistency, cannot be taken for granted and can be hard to analyze in many settings. In particular, the performance properties of decentralized event-triggered control schemes with respect to time-triggered control remain mostly unexplored. Therefore, in this paper, we examine these performance properties for a consensus problem considering single-integrator agent dynamics, a level-triggering rule, and a complete communication graph. We consider the long-term average quadratic deviation from consensus as a performance measure. For this setting, we show that enriching the information the local controllers use improves the performance of the consensus algorithm but renders a previously consistent event-triggered control scheme inconsistent. In addition, we do so while deploying optimal control inputs which we derive for both information cases and triggering schemes. With this insight, we can furthermore explain the relationship between two seemingly contrasting consistency results from the literature.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Persistent BitTorrent Trackers</title>
<link>https://arxiv.org/abs/2511.17260</link>
<guid>https://arxiv.org/abs/2511.17260</guid>
<content:encoded><![CDATA[
arXiv:2511.17260v2 Announce Type: replace 
Abstract: Private BitTorrent trackers enforce upload-to-download ratios to prevent free-riding, but suffer from three critical weaknesses: reputation cannot move between trackers, centralized servers create single points of failure, and upload statistics are self-reported and unverifiable. When a tracker shuts down (whether by operator choice, technical failure, or legal action) users lose their contribution history and cannot prove their standing to new communities. We address these problems by storing reputation in smart contracts and replacing self-reports with cryptographic attestations. Receiving peers sign receipts for transferred pieces, which the tracker aggregates and verifies before updating on-chain reputation. Trackers run in Trusted Execution Environments (TEEs) to guarantee correct aggregation and prevent manipulation of state. If a tracker is unavailable, peers use an authenticated Distributed Hash Table (DHT) for discovery: the on-chain reputation acts as a Public Key Infrastructure (PKI), so peers can verify each other and maintain access control without the tracker. This design persists reputation across tracker failures and makes it portable to new instances through single-hop migration in factory-deployed contracts. We formalize the security requirements, prove correctness under standard cryptographic assumptions, and evaluate a prototype on Intel TDX. Measurements show that transfer receipts adds less than 6\% overhead with typical piece sizes, and signature aggregation speeds up verification by $2.5\times$.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Resiliency of Vital Services in Flood-Affected Regions of Bangladesh Using Next-Generation Opportunistic DTN Edge Ad Hoc Networks</title>
<link>https://arxiv.org/abs/2511.15710</link>
<guid>https://arxiv.org/abs/2511.15710</guid>
<content:encoded><![CDATA[
<div> The abstract you've provided describes a new research paper submitted to arXiv (arXiv:2511.15710v1), introducing **AZIZA**, an innovative opportunistic routing protocol tailored for disaster-prone regions like flood-affected areas in Bangladesh. Here's a concise summary and analysis of the key contributions and implications:

---

### 📌 **Summary of AZIZA (Adaptive Zone-based Intelligent Fully Distributed Trust-Aware Routing Protocol)**

**Context & Motivation:**
- In disaster scenarios—especially recurring floods in Bangladesh—traditional communication infrastructure often fails.
- This leads to isolation of communities and hampers emergency response.
- There is a critical need for **infrastructure-less, resilient, and adaptive communication networks**.

**Solution: AZIZA**
A novel **opportunistic routing protocol** designed for decentralized, mobile ad hoc networks composed of:
- Mobile phones
- UAVs (drones)
- Ground vehicles  
These act as edge relays in an **opportunistic network (OppNet)** with intermittent connectivity.

---

### 🔧 Key Features of AZIZA

1. **Zone-Based Forwarding**
   - Divides the network into logical zones to enable **localized, efficient message forwarding**.
   - Reduces unnecessary flooding and optimizes path selection based on geographic relevance.

2. **Trust-Aware Mechanism**
   - Evaluates node behavior using **trust metrics** to identify and avoid:
     - Malicious nodes
     - Uncooperative or selfish nodes
   - Enhances security and reliability in decentralized environments.

3. **Context-Driven Decision Making**
   - Routing decisions are based on multiple dynamic factors:
     - **Trust score**
     - **Residual energy** (to prolong network lifetime)
     - **Historical delivery performance**
   - Enables intelligent, adaptive data delivery for critical services.

4. **Fully Distributed & Lightweight**
   - No central control; suitable for resource-constrained devices.
   - Designed for rapid deployment over existing mobile and aerial platforms.

---

### 🧪 Evaluation & Results

- Simulated using **The ONE Simulator**.
- Configured with **real-world mobility traces** and **flood impact data from Bangladesh**.
- Benchmarked against established opportunistic protocols (e.g., Epidemic, Prophet, Spray-and-Wait).

**Key Outcomes:**
- ✅ **Higher message delivery ratio** (improved reliability)
- ✅ **Lower energy consumption** (enhanced efficiency)
- ✅ **Greater resilience under disruption and mobility**
- ✅ Scalable and deployable in real humanitarian contexts

---

### 🌍 Significance & Impact

- Addresses a pressing global challenge: **communication during natural disasters**.
- Tailored specifically for **low-resource, high-disruption environments**.
- Leverages widely available technologies (smartphones, drones) for rapid deployment.
- Advances the state-of-the-art in **trust-aware, context-sensitive opportunistic networking**.

---

### 🚀 Potential Applications

- Emergency alert dissemination during floods
- Health data collection from remote sensors
- Coordination between rescue teams across isolated zones
- Post-disaster situational awareness and recovery operations

---

### 🔮 Future Directions (Implied or Suggested)

- Field testing in real flood-affected regions
- Integration with satellite or LoRa-based backhaul for extended reach
- Machine learning enhancements for predictive trust modeling
- Privacy-preserving trust evaluation mechanisms

---

### ✅ Conclusion

**AZIZA represents a significant step forward in resilient, intelligent communication systems for disaster response.** By combining zone-based optimization, trust awareness, and context-driven adaptation, it offers a robust solution where traditional networks fail—particularly relevant for climate-vulnerable countries like Bangladesh.

This work not only contributes technically but also aligns with broader goals in **humanitarian technology, digital resilience, and sustainable development (e.g., SDG 11 and 13).**

---

If you'd like, I can help:
- Generate a presentation slide deck from this content
- Write a blog post explaining AZIZA for non-experts
- Compare AZIZA with other opportunistic protocols
- Draft a peer review or commentary on the paper

Let me know how you’d like to proceed! <div>
arXiv:2511.15710v1 Announce Type: new 
Abstract: Opportunistic routing architectures offer a resilient communication paradigm in environments where conventional networks fail due to disrupted infrastructure, dynamic node mobility, and intermittent connectivity conditions that commonly arise during large-scale disasters. In Bangladesh, recurring floods severely hinder communication systems, isolating affected populations and obstructing emergency response efforts. To address these challenges, there is a growing demand for intelligent and adaptive routing solutions capable of sustaining critical communication and services without relying on fixed infrastructure. This research presents AZIZA (Adaptive Zone-based Intelligent Fully Distributed Trust-Aware Routing Protocol), a next-generation opportunistic protocol designed to improve the resiliency of critical communication and services in disaster-prone and flood-affected regions. AZIZA supports adaptive data delivery for emergency alerts, sensor readings, and inter-zone coordination by integrating (1) zone-based forwarding to optimize localized transmission, (2) trust-aware logic to bypass uncooperative or malicious nodes, and (3) context-driven decision-making based on trust metrics, residual energy, and historical delivery patterns. AZIZA operates over lightweight, infrastructure-less edge ad hoc networks comprising mobile phones, UAVs, and ground vehicles acting as decentralized service relays. Simulation results using The Opportunistic Network Environment (ONE) Simulator configured with real-world mobility traces and flood data from Bangladesh demonstrate that AZIZA significantly outperforms benchmark approaches in delivery reliability, energy efficiency, and routing resilience. As a scalable and deployable framework, AZIZA advances the use of next-generation opportunistic routing in environments where traditional systems routinely collapse.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Secure Autonomous Agent Payments: Verifying Authenticity and Intent in a Trustless Environment</title>
<link>https://arxiv.org/abs/2511.15712</link>
<guid>https://arxiv.org/abs/2511.15712</guid>
<content:encoded><![CDATA[
<div> Thank you for sharing the abstract of arXiv:2511.15712v1.

Here is a **summary and analysis** of the paper based on the provided abstract:

---

### 📄 **Paper Summary (arXiv:2511.15712v1)**  
**Title (inferred):** *A Blockchain-Based Framework for Authenticating Intent in AI-Agent Financial Transactions*  
**Topic:** Secure, verifiable, and privacy-preserving financial transactions initiated by autonomous AI agents.

---

### 🔍 **Core Problem**
As AI agents gain the ability to initiate financial transactions autonomously, traditional assumptions about human authorization in payment systems break down. This raises critical concerns:
- How do we **verify that an AI agent is legitimate**?
- How can we **prove the transaction reflects genuine user intent**?
- How do we ensure **integrity, privacy, and accountability** in decentralized environments?

These challenges are especially pressing in blockchain-based or Web3 ecosystems where trustless interactions dominate.

---

### ✅ **Proposed Solution: A Hybrid Blockchain Framework**

The paper introduces a comprehensive framework combining several advanced technologies:

| Component | Role |
|--------|------|
| **Decentralized Identity (DID)** | Establishes cryptographically verifiable identities for AI agents. Ensures each agent is uniquely identifiable without central authorities. |
| **Verifiable Credentials (VCs)** | Used to issue and validate permissions (e.g., "this agent is authorized to spend up to $100/day"). Issued by users or trusted entities. |
| **On-Chain Intent Proofs** | Records user authorization (intent) directly on the blockchain. Creates a tamper-proof link between user consent and subsequent actions. |
| **Zero-Knowledge Proofs (ZKPs)** | Allows agents to prove compliance with policies (e.g., budget limits, rules) without revealing sensitive data (user identity, transaction details). Enables privacy-preserving verification. |
| **Trusted Execution Environments (TEEs)** | Ensures the AI agent’s reasoning and execution logic run securely and unaltered (via hardware-based attestation). Prevents model manipulation or runtime exploits. |
| **Hybrid On-Chain/Off-Chain Architecture** | Balances scalability and transparency: heavy processing occurs off-chain; critical proofs and intent records are stored on-chain. |

---

### 🔐 **Security & Trust Properties Achieved**
- **Authenticity**: DIDs + VCs prevent impersonation.
- **Intent Verification**: On-chain proof links action to prior user consent.
- **Privacy Preservation**: ZKPs allow validation without exposing inputs.
- **Execution Integrity**: TEEs ensure the agent behaves as intended.
- **Auditability**: Immutable ledger provides full traceability from intent to outcome.

---

### 🧩 **Significance and Impact**

This work addresses a **critical gap** in the convergence of AI and decentralized finance (DeFi):
- It enables **autonomous economic agents** (e.g., personal finance bots, supply chain coordinators) to operate securely at scale.
- It lays the foundation for **AI-native financial infrastructure**, where machines transact on behalf of humans with verifiable trust.
- The integration of **intent-awareness** into transactions represents a shift from purely mechanistic execution to semantically meaningful, policy-compliant automation.

---

### 🚀 **Future Implications**
- **Regulatory Compliance**: Could support frameworks like MiCA or GDPR by enabling auditable, explainable AI decisions.
- **AI Marketplaces**: Facilitates secure delegation of economic tasks to third-party AI services.
- **Web3 Evolution**: Supports next-gen dApps where AI agents interact autonomously but accountably.

---

### ⚠️ **Potential Challenges (Not Addressed in Abstract)**
While promising, real-world deployment may face hurdles:
- **Scalability** of ZKP generation and TEE coordination.
- **Key management** for agent identities (risk of compromise).
- **Legal liability** when AI agents make erroneous or harmful transactions.
- **Standardization needs** across DID, VC, and ZKP implementations.

---

### 🏁 **Conclusion**
This paper presents a timely and technically robust response to the growing need for **trustworthy autonomy** in AI-driven financial systems. By integrating blockchain, cryptographic proofs, and secure hardware, it offers a path toward **verifiable, private, and accountable AI agency**—a foundational step for the future of decentralized machine economies.

---

If you'd like, I can help:
- Generate a citation for this preprint,
- Draft a presentation slide deck summarizing the framework,
- Compare it to related work (e.g., ERC-7581, SIWE, or AI DAOs),
- Or simulate how the system would handle a use case (e.g., an AI paying a bill).

Let me know how you’d like to proceed! <div>
arXiv:2511.15712v1 Announce Type: new 
Abstract: Artificial intelligence (AI) agents are increasingly capable of initiating financial transactions on behalf of users or other agents. This evolution introduces a fundamental challenge: verifying both the authenticity of an autonomous agent and the true intent behind its transactions in a decentralized, trustless environment. Traditional payment systems assume human authorization, but autonomous, agent-led payments remove that safeguard. This paper presents a blockchain-based framework that cryptographically authenticates and verifies the intent of every AI-initiated transaction. The proposed system leverages decentralized identity (DID) standards and verifiable credentials to establish agent identities, on-chain intent proofs to record user authorization, and zero-knowledge proofs (ZKPs) to preserve privacy while ensuring policy compliance. Additionally, secure execution environments (TEE-based attestations) guarantee the integrity of agent reasoning and execution. The hybrid on-chain/off-chain architecture provides an immutable audit trail linking user intent to payment outcome. Through qualitative analysis, the framework demonstrates strong resistance to impersonation, unauthorized transactions, and misalignment of intent. This work lays the foundation for secure, auditable, and intent-aware autonomous economic agents, enabling a future of verifiable trust and accountability in AI-driven financial ecosystems.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Detailed Comparative Analysis of Blockchain Consensus Mechanisms</title>
<link>https://arxiv.org/abs/2511.15730</link>
<guid>https://arxiv.org/abs/2511.15730</guid>
<content:encoded><![CDATA[
<div> The abstract from arXiv:2511.15730v1 presents a timely and thorough comparative evaluation of two foundational blockchain consensus mechanisms—**Proof of Work (PoW)** and **Proof of Stake (PoS)**—across seven key dimensions critical to the evolution of decentralized systems.

### Key Insights from the Study:

1. **Energy Use & Environmental Impact**:
   - **PoW**: Noted for its high energy consumption due to computationally intensive mining, leading to significant environmental concerns.
   - **PoS**: Demonstrates a drastic reduction in energy usage by eliminating competitive mining, making it far more environmentally sustainable.

2. **Security**:
   - **PoW**: Offers battle-tested, robust security through economic cost to attackers (high hash power required), proven over more than a decade (e.g., Bitcoin).
   - **PoS**: While theoretically secure, concerns remain about long-term resilience, especially regarding "nothing-at-stake" issues and potential vulnerabilities in early adoption phases.

3. **Transaction Speed & Scalability**:
   - **PoW**: Generally slower transaction throughput and limited scalability (e.g., Bitcoin’s ~7 TPS), with bottlenecks exacerbated during peak demand.
   - **PoS**: Enables faster finality and higher throughput (e.g., Ethereum post-merge), supporting greater scalability via sharding and other layer-2 integrations.

4. **Centralization Risk**:
   - **PoW**: Centralization emerges via mining pools and geographic concentration of hardware resources, undermining decentralization goals.
   - **PoS**: Risks stem from wealth concentration—those with more stake gain disproportionate validation power, potentially favoring large stakeholders ("the rich get richer").

5. **Transaction Fees**:
   - **PoW**: Fees can be volatile and spike during network congestion.
   - **PoS**: Offers more predictable and stable fee structures due to better resource management and scalability.

6. **Decentralization vs. Efficiency Trade-off**:
   - The study highlights a core tension: PoW excels in decentralization and security at the cost of efficiency; PoS improves performance and sustainability but may compromise on equitable participation.

7. **Hybrid Solutions as a Path Forward**:
   - The paper suggests that hybrid consensus models—leveraging PoW’s proven security for initial trust establishment and PoS’s efficiency for ongoing operation—could offer a balanced alternative.
   - Such designs might enable blockchains to maintain decentralization while achieving scalability and ecological responsibility.

### Implications for Future Blockchain Development:
This research contributes meaningfully to the ongoing discourse on sustainable and secure blockchain infrastructure. As global regulatory and environmental scrutiny increases, the shift toward energy-efficient models like PoS is likely to accelerate. However, preserving decentralization and avoiding new forms of centralization (e.g., staking oligopolies) remains a challenge.

The recommendation of hybrid or novel consensus architectures aligns with emerging trends in blockchain innovation, such as **proof-of-authority hybrids**, **delegated proof-of-stake variants**, and **multi-layer consensus frameworks**.

### Conclusion:
arXiv:2511.15730v1 provides a well-rounded, evidence-based assessment that will be valuable for researchers, developers, and policymakers navigating the trade-offs between security, performance, and sustainability in next-generation blockchain systems. Its call for balanced design principles resonates with the broader goal of building trustworthy, scalable, and eco-conscious decentralized networks. <div>
arXiv:2511.15730v1 Announce Type: new 
Abstract: This paper presents a comprehensive comparative analysis of two dominant blockchain consensus mechanisms, Proof of Work (PoW) and Proof of Stake (PoS), evaluated across seven critical metrics: energy use, security, transaction speed, scalability, centralization risk, environmental impact, and transaction fees. Utilizing recent academic research and real-world blockchain data, the study highlights that PoW offers robust, time-tested security but suffers from high energy consumption, slower throughput, and centralization through mining pools. In contrast, PoS demonstrates improved scalability and efficiency, significantly reduced environmental impact, and more stable transaction fees, however it raises concerns over validator centralization and long-term security maturity. The findings underscore the trade-offs inherent in each mechanism and suggest hybrid designs may combine PoW's security with PoS's efficiency and sustainability. The study aims to inform future blockchain infrastructure development by striking a balance between decentralization, performance, and ecological responsibility.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tracking financial crime through code and law: a review of regtech applications in anti-money laundering and terrorism financing</title>
<link>https://arxiv.org/abs/2511.15764</link>
<guid>https://arxiv.org/abs/2511.15764</guid>
<content:encoded><![CDATA[
<div> The abstract you've provided appears to be from a preprint paper on arXiv (identifier: 2511.15764v1), which presents a narrative literature review on the role of Regulatory Technology (RegTech) in enhancing financial crime prevention, particularly in Anti-Money Laundering and Countering the Financing of Terrorism (AML/CFT) frameworks.

Here is a concise summary and analysis of the abstract:

---

**Summary of arXiv:2511.15764v1 – "RegTech in Financial Crime Prevention: A Narrative Review"**

This paper offers a structured narrative review of RegTech applications in financial compliance, focusing on developments between 2020 and 2024. It explores how advanced technologies are reshaping AML/CFT practices across several key domains:

- **Customer Due Diligence (CDD) and Know Your Customer (KYC):** Automation and AI-driven identity verification streamline onboarding and ongoing monitoring.
- **Transaction Monitoring:** Machine learning models detect anomalous patterns more accurately than rule-based systems.
- **Regulatory Reporting and Compliance Automation:** Real-time reporting and automated data collection reduce delays and human error.
- **Information Sharing and Cross-Border Cooperation:** Blockchain and secure data platforms enable trusted collaboration across jurisdictions.
- **Cost Efficiency:** RegTech reduces operational burdens and compliance costs for financial institutions.

The study highlights that RegTech is not merely a tool for efficiency but signifies a **paradigm shift** in regulatory oversight—moving institutions from passive compliance toward **active risk detection and management**. The integration of **AI, blockchain, and big data analytics** produces synergistic effects, improving detection accuracy, transparency, and responsiveness.

Importantly, the paper emphasizes the **strategic value of integrated RegTech solutions**, suggesting that isolated technological adoption is less effective than holistic, interoperable systems.

---

**Key Implications:**

- **Shift in Responsibility:** Financial institutions are becoming co-regulators in practice, empowered and expected to proactively manage risks.
- **Technology Synergy:** Combined use of AI (for pattern recognition), blockchain (for auditability and trust), and big data (for scale) enhances system resilience and adaptability.
- **Policy Relevance:** Regulators may need to evolve frameworks to support innovation while managing new risks (e.g., algorithmic bias, data privacy).

---

**Note on arXiv Preprints:**
As this is version v1 and marked as "Announce Type: new", it has not yet undergone peer review. While the findings are promising, they should be interpreted with caution until validated through formal academic scrutiny.

---

If you're looking to build upon this work or explore related topics, potential next steps could include:
- Comparative studies of RegTech adoption across regions.
- Case studies of AI-driven transaction monitoring failures/successes.
- Ethical and regulatory challenges of autonomous compliance systems.
- The role of central bank digital currencies (CBDCs) in enabling transparent financial ecosystems.

Let me know if you'd like help locating the full paper, identifying related research, or drafting a response or critique. <div>
arXiv:2511.15764v1 Announce Type: new 
Abstract: Regulatory technology (RegTech) is transforming financial compliance by integrating advanced information technologies to strengthen anti money laundering and countering the financing of terrorism (AML CFT) frameworks. Recent literature suggests that such technologies represent more than just an efficiency tool; they mark a paradigm shift in regulation and the evolution of financial oversight (Kurum, 2023). This paper aims to provide a narrative review of recent RegTech applications in financial crime prevention, with a focus on key compliance domains. A structured literature review was conducted to examine publications between 2020 and 2024 with a thematic synthesis of findings related to customer due diligence (CDD) and know your customer (KYC), transaction monitoring, regulatory reporting and compliance automation, information sharing and cross border cooperation, as well as cost efficiency. Findings reveal that RegTech solutions give financial institutions more responsibility for detecting and managing financial crime risks, making them more active players in compliance processes traditionally overseen by regulators. The combined use of technologies such as artificial intelligence (AI), blockchain, and big data also generates synergistic effects that improve compliance outcomes beyond what these technologies achieve individually. This demonstrates the strategic relevance of integrated RegTech approaches.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lifefin: Escaping Mempool Explosions in DAG-based BFT</title>
<link>https://arxiv.org/abs/2511.15936</link>
<guid>https://arxiv.org/abs/2511.15936</guid>
<content:encoded><![CDATA[
<div> The paper **arXiv:2511.15936v1** presents a critical security and liveness analysis of **DAG-based Byzantine Fault-Tolerant (BFT) consensus protocols**, which are increasingly popular in high-throughput blockchain systems due to their ability to achieve efficient transaction ordering through a Directed Acyclic Graph (DAG) structure.

### Key Contributions and Insights:

#### 🔍 **Identified Vulnerability: Mempool Explosion Attack**
- **Problem**: While DAG-based BFT protocols (e.g., Sailfish, Mysticeti) offer high throughput by decoupling data dissemination from ordering and enabling zero-message consensus via well-connected mempool DAGs, they suffer from a **liveness vulnerability**.
- **Attack Vector**: An adversary can exploit this by triggering a *mempool explosion* — flooding the network with transactions or control messages — preventing nodes from committing any transactions.
- **Impact**: This leads to indefinite delays in finality, effectively halting progress and violating the **liveness guarantee** of the protocol, even under sub-majority Byzantine conditions.

---

#### ✅ **Proposed Solution: Lifefin – A Self-Stabilizing Liveness Layer**
- **Goal**: To preserve liveness under adversarial mempool stress without sacrificing performance in normal operation.
- **Core Mechanism**: 
  - Integrates an **Agreement on Common Subset (ACS)** primitive.
  - Enables honest nodes to agree on a common subset of transactions that have been reliably disseminated, allowing them to commit despite ongoing mempool bloat.
- **Key Features**:
  - **Generic & Modular**: Designed as a plug-in layer for existing DAG-based BFT protocols.
  - **Self-Stabilizing**: Recovers automatically from transient faults or attacks; ensures eventual liveness.
  - **Low Overhead**: Imposes almost no cost during typical (non-attack) scenarios.
  - **Bounded Resource Usage**: Limits memory and computation usage per node, countering resource exhaustion.

---

#### 🛠️ **Integration & Evaluation**
- **Applied To**:
  - **Sailfish → Sailfish-Lifefin**
  - **Mysticeti → Mysticeti-Lifefin**
- **Implementation & Testing**:
  - Full implementations of both enhanced protocols.
  - Evaluated under various workloads and adversarial conditions.
- **Results**:
  - **Throughput**: Nearly identical to original protocols in normal operations.
  - **Latency**: Slight increase only during attack phases; minimal impact otherwise.
  - **Resilience**: Successfully resists mempool explosion attacks and maintains liveness.
  - **Efficiency**: Confirms theoretical claim of "almost zero overhead" in typical cases.

---

### Significance and Implications

- **Fundamental Insight**: High-performance does not imply robust liveness. Even advanced DAG-based designs can fail silently under targeted resource exhaustion.
- **Practical Impact**: Lifefin offers a practical retrofit for next-gen blockchains using DAG-based consensus, enhancing resilience without redesigning core logic.
- **Design Principle**: Highlights the importance of **self-stabilization** and **resource-bounded agreement** in distributed ledgers facing adaptive adversaries.

---

### Conclusion

**Lifefin** represents a significant step forward in securing the liveness of modern, high-throughput BFT protocols. By introducing a lightweight, generic defense rooted in ACS, it closes a critical gap in DAG-based consensus — proving that safety and efficiency need not come at the expense of liveness.

> 💡 **Takeaway**: As blockchain systems grow more complex, modular safety nets like Lifefin will become essential to ensure robustness against subtle, emergent attack vectors.

---

Let me know if you'd like a summary suitable for presentation, code implementation insights, or comparison with other liveness-preserving mechanisms (e.g., timeout-based view changes in classical BFT). <div>
arXiv:2511.15936v1 Announce Type: new 
Abstract: Directed Acyclic Graph (DAG)-based Byzantine Fault-Tolerant (BFT) protocols have emerged as promising solutions for high-throughput blockchains. By decoupling data dissemination from transaction ordering and constructing a well-connected DAG in the mempool, these protocols enable zero-message ordering and implicit view changes. However, we identify a fundamental liveness vulnerability: an adversary can trigger mempool explosions to prevent transaction commitment, ultimately compromising the protocol's liveness.
  In response, this work presents Lifefin, a generic and self-stabilizing protocol designed to integrate seamlessly with existing DAG-based BFT protocols and circumvent such vulnerabilities. Lifefin leverages the Agreement on Common Subset (ACS) mechanism, allowing nodes to escape mempool explosions by committing transactions with bounded resource usage even in adverse conditions. As a result, Lifefin imposes (almost) zero overhead in typical cases while effectively eliminating liveness vulnerabilities.
  To demonstrate the effectiveness of Lifefin, we integrate it into two state-of-the-art DAG-based BFT protocols, Sailfish and Mysticeti, resulting in two enhanced variants: Sailfish-Lifefin and Mysticeti-Lifefin. We implement these variants and compare them with the original Sailfish and Mysticeti systems. Our evaluation demonstrates that Lifefin achieves comparable transaction throughput while introducing only minimal additional latency to resist similar attacks.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Quantum-Secure and Blockchain-Integrated E-Voting Framework with Identity Validation</title>
<link>https://arxiv.org/abs/2511.16034</link>
<guid>https://arxiv.org/abs/2511.16034</guid>
<content:encoded><![CDATA[
<div> The paper **arXiv:2511.16034v1** presents a comprehensive and forward-looking approach to securing electronic voting (e-voting) systems in the post-quantum era. Here's a structured summary and analysis of its key contributions and implications:

---

### 🔍 **Summary of arXiv:2511.16034v1**

**Title:** *A Post-Quantum Secure E-Voting Architecture with Biometric Authentication and Blockchain*

#### 🎯 **Core Objective**
To design a scalable, secure, and quantum-resistant e-voting framework that ensures:
- Voter authenticity
- Data integrity
- Non-repudiation
- Resistance to spoofing attacks
- Transparency via auditability

---

### 🔐 **Key Components of the Architecture**

1. **Post-Quantum Cryptography – Falcon Signatures**
   - Uses **Falcon**, a lattice-based digital signature scheme selected by NIST for standardization.
   - Ensures long-term security against quantum adversaries capable of breaking RSA/ECC.
   - Applied during voter registration to sign facial embeddings.

2. **Biometric Authentication Pipeline**
   - **MobileNetV3**: Lightweight neural network used for efficient extraction of facial features on mobile or edge devices.
   - **AdaFace**: Advanced face recognition model trained to be robust under varying conditions (pose, illumination, occlusion).
   - Real-time verification using **cosine similarity matching** between live capture and stored embeddings.
   - Incorporates **anti-spoofing mechanisms** (liveness detection) to prevent presentation attacks (e.g., photos, masks).

3. **Permissioned Blockchain Integration**
   - Stores digitally signed facial embeddings and votes immutably.
   - Provides:
     - Tamper-proof record keeping
     - Audit trail with non-repudiation
     - Transparent yet privacy-preserving logging
   - Low gas overhead due to optimized transaction size:
     - **~3.3% overhead for registration**
     - **~0.15% for voting transactions**

4. **System Monitoring & Auditing**
   - Deployed monitoring stack: **Prometheus + Grafana**
   - Enables real-time performance tracking, anomaly detection, and system transparency.
   - Critical for maintaining trust in high-stakes elections.

---

### 📊 **Performance Evaluation Highlights**

| Metric | Result |
|-------|--------|
| **Average Classification Error Rate (ACER)** | <br />• **< 3.5%** on CelebA-Spoof dataset<br />• **< 8.2%** on Wild Face Anti-Spoofing (WFAS) dataset |
| **Latency** | Low end-to-end latency suitable for large-scale deployment |
| **Scalability** | Handles concurrent loads efficiently; no significant degradation under stress tests |
| **Gas Overhead** | Minimal blockchain cost preserves efficiency |

> These results indicate strong resistance to spoofing and practical feasibility for national-scale implementations.

---

### ✅ **Advantages of the Proposed System**

- **Quantum Resilience**: Future-proofs e-voting infrastructure against advances in quantum computing.
- **Strong Identity Assurance**: Biometrics + cryptographic binding reduce risks of impersonation.
- **Immutable Audit Trail**: On-chain storage enables verifiable, transparent election processes.
- **Efficiency**: Optimized models and minimal blockchain bloat ensure scalability.
- **Real-Time Monitoring**: Operational visibility enhances reliability and accountability.

---

### ⚠️ **Potential Challenges & Considerations**

While promising, several aspects warrant further exploration:

1. **Privacy Concerns**
   - Storing biometric data—even hashed/embedded—on any ledger raises privacy questions.
   - Need for strict access control and compliance with GDPR/CCPA-like regulations.

2. **Model Bias in Biometrics**
   - AdaFace/MobileNetV3 must be evaluated across diverse demographics to avoid discriminatory error rates.

3. **Centralization Risks**
   - Permissioned blockchains improve efficiency but may introduce trusted parties—potentially conflicting with decentralization ideals.

4. **Hardware Dependency**
   - Reliance on cameras and edge devices could exclude voters without modern smartphones.

5. **Revocation & Update Mechanisms**
   - How are compromised biometric templates revoked or re-registered securely?

---

### 🧩 **Broader Implications**

This work represents a **convergence of three critical domains**:
- **Cryptography (post-quantum)**
- **AI-driven biometrics**
- **Distributed ledger technology**

It offers a **blueprint for next-generation digital democracy tools**, especially relevant as nations seek resilient alternatives to traditional voting amid rising cyber threats.

---

### 🏁 **Conclusion**

**arXiv:2511.16034v1** introduces a timely and technically sound solution to one of the most pressing challenges in cybersecurity and democratic governance: building **quantum-safe, trustworthy e-voting systems**.

By integrating Falcon signatures, advanced biometrics, and lightweight blockchain anchoring, the authors deliver a system that is not only secure and efficient but also auditable and future-ready.

➡️ **Recommendation**: This architecture should be piloted in controlled electoral environments and subjected to independent red-teaming and formal verification before wide adoption.

--- 

Let me know if you'd like a visual diagram of the system architecture, code implementation ideas, or comparisons with other PQ-e-voting proposals. <div>
arXiv:2511.16034v1 Announce Type: new 
Abstract: The rapid growth of quantum computing poses a threat to the cryptographic foundations of digital systems, requiring the development of secure and scalable electronic voting (evoting) frameworks. We introduce a post-quantum-secure evoting architecture that integrates Falcon lattice-based digital signatures, biometric authentication via MobileNetV3 and AdaFace, and a permissioned blockchain for tamper-proof vote storage. Voter registration involves capturing facial embeddings, which are digitally signed using Falcon and stored on-chain to ensure integrity and non-repudiation. During voting, real-time biometric verification is performed using anti-spoofing techniques and cosine-similarity matching. The system demonstrates low latency and robust spoof detection, monitored through Prometheus and Grafana for real-time auditing. The average classification error rates (ACER) are below 3.5% on the CelebA Spoof dataset and under 8.2% on the Wild Face Anti-Spoofing (WFAS) dataset. Blockchain anchoring incurs minimal gas overhead, approximately 3.3% for registration and 0.15% for voting, supporting system efficiency, auditability, and transparency. The experimental results confirm the system's scalability, efficiency, and resilience under concurrent loads. This approach offers a unified solution to address key challenges in voter authentication, data integrity, and quantum-resilient security for digital systems.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ART: A Graph-based Framework for Investigating Illicit Activity in Monero via Address-Ring-Transaction Structures</title>
<link>https://arxiv.org/abs/2511.16192</link>
<guid>https://arxiv.org/abs/2511.16192</guid>
<content:encoded><![CDATA[
<div> The abstract you've provided describes a research paper (arXiv:2511.16192v1) that presents a novel approach to analyzing criminal behavior in the Monero blockchain—a privacy-focused cryptocurrency known for its resistance to traditional transaction tracing methods.

Here’s a breakdown and interpretation of the key points:

---

**Title (inferred):**  
*A Graph-Based Machine Learning Approach to Detecting Criminal Behavioral Patterns in Monero*

**Context:**  
As law enforcement improves its capabilities in **cryptocurrency forensics**, criminals are shifting from transparent blockchains like Bitcoin to more private alternatives such as **Monero (XMR)**. Monero uses advanced cryptographic techniques (e.g., ring signatures, stealth addresses, confidential transactions) to obscure sender, receiver, and transaction amount, rendering standard blockchain analysis tools ineffective.

This poses a significant challenge: how can investigators detect or infer illicit activity when direct tracing is impossible?

---

**Contribution of the Paper:**

1. **Novel Graph-Based Methodology:**  
   The authors introduce **Address-Ring-Transaction (ART) graphs**, a specialized structure designed to model Monero's unique transaction mechanics:
   - **Ring signatures** involve multiple possible signers, creating ambiguity about the true sender.
   - The graph captures relationships between addresses, ring participants (decoys), and transactions over time.

2. **Focus on Flagged Transactions:**  
   They start with a set of Monero transactions already linked to **known criminal activities** (e.g., darknet markets, ransomware payments, stolen funds). These serve as "ground truth" for training and validation.

3. **Feature Extraction:**  
   From the ART graphs, they extract both:
   - **Structural features**: patterns in how addresses interact with rings, reuse of keys, size of rings, clustering behaviors.
   - **Temporal features**: timing patterns between transactions, frequency, bursts of activity—potentially indicating operational security (OpSec) routines or coordination.

4. **Machine Learning for Pattern Detection:**  
   Using these features, they train ML models (likely classifiers like Random Forest, GNNs, or XGBoost) to:
   - Identify similar behavioral patterns in other parts of the Monero blockchain.
   - Flag transactions that exhibit characteristics akin to known criminal modus operandi.

5. **Strategic Goal:**  
   This work represents an early step toward building **investigative tools** for privacy-preserving blockchains. While full traceability remains off the table due to Monero’s design, **behavioral fingerprinting** may still offer actionable intelligence.

---

**Significance & Implications:**

- **First Steps in Private Chain Forensics:** Most blockchain forensics focus on transparent ledgers. This paper pushes the frontier into truly private systems.
- **Behavior Over Tracing:** Instead of following money flows, it shifts focus to **anomalies in usage patterns**—a promising avenue despite cryptographic privacy.
- **Support for LEAs:** Provides a framework for law enforcement to potentially identify suspicious clusters or predict future criminal operations, even without knowing exact transaction details.
- **Ethical Considerations:** Raises questions about privacy vs. surveillance, especially since innocent users share the same network and rely on decoys (ring members).

---

**Limitations (implied):**
- Relies on limited ground-truth data (few confirmed criminal Monero transactions).
- Cannot de-anonymize transactions but infers likelihood based on behavior.
- Risk of false positives if legitimate user behavior mimics criminal patterns.

---

**Future Directions (suggested by the work):**
- Integration with wallet-level heuristics or external data (e.g., exchange monitoring).
- Use of **Graph Neural Networks (GNNs)** to better learn complex relational structures.
- Cross-chain analysis (e.g., tracking movement from Bitcoin to Monero via mixers or DEXs).

---

**Conclusion:**

This paper represents a **pioneering effort** in applying machine learning and graph analytics to one of the most challenging domains in digital forensics: **privacy-preserving cryptocurrencies**. By focusing on structural and temporal behavioral patterns rather than content, it opens a new pathway for supporting investigations—without breaking Monero’s cryptography.

It underscores a growing trend: while privacy technologies limit visibility, **behavior leaves traces**—and those might be enough to guide real-world interventions.

--- 

Let me know if you'd like help summarizing this into a presentation, generating code ideas for ART graphs, or exploring related works! <div>
arXiv:2511.16192v1 Announce Type: new 
Abstract: As Law Enforcement Agencies advance in cryptocurrency forensics, criminal actors aiming to conceal illicit fund movements increasingly turn to "mixin" services or privacy-based cryptocurrencies. Monero stands out as a leading choice due to its strong privacy preserving and untraceability properties, making conventional blockchain analysis ineffective. Understanding the behavior and operational patterns of criminal actors within Monero is therefore challenging and it is essential to support future investigative strategies and disrupt illicit activities. In this work, we propose a case study in which we leverage a novel graph-based methodology to extract structural and temporal patterns from Monero transactions linked to already discovered criminal activities. By building Address-Ring-Transaction graphs from flagged transactions, we extract structural and temporal features and use them to train Machine Learning models capable of detecting similar behavioral patterns that could highlight criminal modus operandi. This represents a first partial step toward developing analytical tools that support investigative efforts in privacy-preserving blockchain ecosystems
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Code Similarity: Benchmarking the Plausibility, Efficiency, and Complexity of LLM-Generated Smart Contracts</title>
<link>https://arxiv.org/abs/2511.16224</link>
<guid>https://arxiv.org/abs/2511.16224</guid>
<content:encoded><![CDATA[
<div> The abstract you've shared outlines a rigorous and timely study on the reliability of Large Language Models (LLMs) in generating **Solidity smart contracts**, with a focus on both functional correctness and non-functional properties like **gas efficiency, security, and code complexity**. Here's a structured breakdown and analysis of the key points, implications, and contributions:

---

### 🔍 **Study Overview**

- **Objective**: Evaluate the quality of LLM-generated Solidity code across multiple dimensions—functional correctness, gas usage, complexity, and semantic similarity.
- **Scope**: 
  - 500 real-world Solidity functions as ground truth.
  - 4 state-of-the-art LLMs evaluated.
  - Two generation settings: **zero-shot** vs. **Retrieval-Augmented Generation (RAG)**.
- **Evaluation Metrics**:
  - **Code similarity** (e.g., BLEU, CodeBLEU)
  - **Semantic embeddings** (vector-based meaning alignment)
  - **Automated test execution** (functional behavior matching)
  - **Gas profiling** (efficiency in Ethereum execution cost)
  - **Cognitive & cyclomatic complexity** (readability and maintainability)

---

### 📊 **Key Findings**

| Aspect | Finding |
|-------|--------|
| **Semantic Similarity** | High — Generated code is semantically close to real contracts. Suggests LLMs understand domain structure. |
| **Functional Correctness** | Low — Only **20–26%** of zero-shot outputs pass tests matching ground-truth behavior. |
| **Code Simplicity** | Generated code has **lower complexity** (both cognitive and cyclomatic), often due to **missing validation logic, edge cases, or access controls**. |
| **Gas Consumption** | Lower than real contracts — but not necessarily better; often because of **omitted safety checks**, leading to under-constrained, potentially insecure code. |
| **RAG Impact** | Dramatically improves performance:
   - Up to **45% increase in functional correctness**
   - Produces more **concise and efficient** code
   - Better context reuse from similar real-world examples |

---

### ⚠️ **Critical Insight: Semantic Plausibility ≠ Functional Reliability**

This is the core takeaway:
> Just because generated code *looks right* or *reads similarly* to real smart contracts doesn’t mean it *behaves correctly*.

- LLMs may produce syntactically valid and semantically plausible code that fails during actual execution.
- Omission of input validation, reentrancy guards, overflow checks, etc., leads to **non-functional or insecure implementations**, even if they appear correct at first glance.

---

### 💡 **Implications for Practice and Research**

1. **LLMs Are Not Production-Ready for Autonomous Smart Contract Generation**
   - Despite advances, unsupervised generation cannot replace human expertise.
   - Blind trust in LLM output risks deploying vulnerable or incorrect contracts.

2. **RAG Is a Game-Changer**
   - Retrieval-augmented methods significantly improve accuracy by grounding generation in real codebases.
   - Highlights the importance of **contextual knowledge retrieval** over pure parametric memory.

3. **Need for Hybrid Development Workflows**
   - Use LLMs for scaffolding or suggestion, but require:
     - Rigorous unit testing
     - Formal verification
     - Expert review
     - Gas optimization tools

4. **Benchmarking Must Go Beyond Syntax**
   - Future evaluations should prioritize **behavioral testing**, **security linters**, and **on-chain simulation**, not just similarity scores.

---

### 🛠️ **Recommendations**

- Integrate **RAG pipelines** into IDEs for secure contract development assistance.
- Develop **specialized evaluation suites** for smart contract generation (like `SolBench` or `SmartEval`) that include:
  - Test case generation
  - Mutation testing
  - Security rule checking (e.g., Slither rules)
- Promote **fine-tuning + retrieval hybrids** trained specifically on audited, high-quality contracts.

---

### 🧩 Conclusion

> While LLMs show promise in understanding Solidity semantics, there remains a **significant gap between appearance and correctness**.  
> **Retrieval augmentation helps bridge this gap**, but achieving production-grade reliability requires more than better models—it demands robust validation frameworks and expert oversight.

As the authors rightly conclude:  
> _"Achieving robust, production-ready code generation remains a substantial challenge."_  

This work underscores the need for **cautious optimism** and **systematic evaluation** in applying LLMs to high-stakes domains like blockchain development.

---

Would you like help summarizing this into a presentation slide, blog post, or technical commentary? <div>
arXiv:2511.16224v1 Announce Type: new 
Abstract: Smart Contracts are critical components of blockchain ecosystems, with Solidity as the dominant programming language. While LLMs excel at general-purpose code generation, the unique constraints of Smart Contracts, such as gas consumption, security, and determinism, raise open questions about the reliability of LLM-generated Solidity code. Existing studies lack a comprehensive evaluation of these critical functional and non-functional properties. We benchmark four state-of-the-art models under zero-shot and retrieval-augmented generation settings across 500 real-world functions. Our multi-faceted assessment employs code similarity metrics, semantic embeddings, automated test execution, gas profiling, and cognitive and cyclomatic complexity analysis. Results show that while LLMs produce code with high semantic similarity to real contracts, their functional correctness is low: only 20% to 26% of zero-shot generations behave identically to ground-truth implementations under testing. The generated code is consistently simpler, with significantly lower complexity and gas consumption, often due to omitted validation logic. Retrieval-Augmented Generation markedly improves performance, boosting functional correctness by up to 45% and yielding more concise and efficient code. Our findings reveal a significant gap between semantic similarity and functional plausibility in LLM-generated Smart Contracts. We conclude that while RAG is a powerful enhancer, achieving robust, production-ready code generation remains a substantial challenge, necessitating careful expert validation.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auditable Ledger Snapshot for Non-Repudiable Cross-Blockchain Communication</title>
<link>https://arxiv.org/abs/2511.16560</link>
<guid>https://arxiv.org/abs/2511.16560</guid>
<content:encoded><![CDATA[
<div> The paper **arXiv:2511.16560v1**, titled with a focus on blockchain interoperability, introduces **InterSnap**, a novel methodology designed to address one of the critical challenges in multi-chain ecosystems: **ensuring non-repudiation and auditability in cross-blockchain transactions**.

Here is a structured summary and analysis of the key contributions and implications:

---

### 🔍 **Problem Statement**
- While individual blockchains are tamper-proof and provide **non-repudiation within their own networks**, this guarantee **breaks down in cross-chain scenarios**.
- If one blockchain suffers disruptions (e.g., due to attacks or failures), other chains may:
  - Deny legitimate claims.
  - File fraudulent liabilities against the affected chain.
- There’s a lack of **verifiable, immutable records** that can serve as proof of cross-chain interactions.

---

### ✅ **Solution: InterSnap**
InterSnap proposes a **blockchain snapshot archival framework** to enforce **auditability and non-repudiation across chains** through:

#### 1. **Cross-Chain Transaction Receipts**
- Cryptographically secured receipts generated for each cross-chain transaction.
- These receipts act as **irrefutable evidence** of agreement between participating blockchains.
- Designed to be tamper-evident and verifiable by external auditors or dispute resolution systems.

#### 2. **Distributed Snapshot Generation**
- Snapshots capture the state of ledger data at specific points in time.
- Generated in a **decentralized manner** to avoid single points of failure or trust.
- Ensures consistency and integrity across heterogeneous blockchain networks.

#### 3. **Need-Based Snapshot Scheduling**
- Adaptive scheduling mechanism triggered by events such as:
  - Completion of critical cross-chain transactions.
  - Detection of anomalies or system instability.
  - Periodic intervals based on load and security requirements.
- Balances resource usage with audit readiness.

#### 4. **Decentralized Archival & Sharing**
- Snapshots and receipts are stored using **decentralized storage platforms** (e.g., IPFS, Filecoin).
- Enables transparent, censorship-resistant access for auditing and recovery purposes.
- Supports secure sharing among authorized participants without relying on central authorities.

---

### ⚙️ **Implementation & Evaluation**
- Built a **prototype using Hyperledger Fabric**.
- Tested across diverse environments:
  - On-premise machines.
  - AWS public cloud.
  - Private cloud infrastructure.
  
#### Key Results:
- **Resilience**: Successfully recovered from simulated malicious attacks while preserving transaction receipts.
- **Scalability**: Maintained performance under increasing transaction loads.
- **Efficiency**: Minimal overhead during snapshot transfer and verification.
- **Security**: End-to-end protection of archived data via cryptographic binding of receipts and snapshots.

---

### 🧩 **Significance and Impact**
- Addresses a crucial gap in **interoperable blockchain architectures**.
- Provides foundational support for:
  - Cross-chain dispute resolution.
  - Regulatory compliance and auditing.
  - Trust-minimized interoperation in decentralized finance (DeFi), supply chains, and Web3 ecosystems.
- Enhances **system-wide resilience**, especially important in heterogeneous, multi-operator environments.

---

### 📌 Conclusion
**InterSnap** represents a significant step toward **secure, auditable, and non-repudiable cross-blockchain communication**. By combining distributed snapshots, irrefutable receipts, and decentralized archiving, it offers a practical solution to strengthen trust across independent blockchain networks—especially when they cannot fully trust each other but must still interact reliably.

This work will likely influence future designs of **interoperability protocols**, bridge mechanisms, and multi-chain consensus frameworks.

---

> 🔗 *Note: As of now (April 2025), arXiv:2511.16560v1 is newly announced and has not yet been peer-reviewed. Further validation through academic scrutiny and real-world deployment will determine its long-term impact.* <div>
arXiv:2511.16560v1 Announce Type: new 
Abstract: Blockchain interoperability is increasingly recognized as the centerpiece for robust interactions among decentralized services. Blockchain ledgers are generally tamper-proof and thus enforce non-repudiation for transactions recorded within the same network. However, such a guarantee does not hold for cross-blockchain transactions. When disruptions occur due to malicious activities or system failures within one blockchain network, foreign networks can take advantage by denying legitimate claims or mounting fraudulent liabilities against the defenseless network. In response, this paper introduces InterSnap, a novel blockchain snapshot archival methodology, for enabling auditability of crossblockchain transactions, enforcing non-repudiation. InterSnap introduces cross-chain transaction receipts that ensure their irrefutability. Snapshots of ledger data along with these receipts are utilized as non-repudiable proof of bilateral agreements among different networks. InterSnap enhances system resilience through a distributed snapshot generation process, need-based snapshot scheduling process, and archival storage and sharing via decentralized platforms. Through a prototype implementation based on Hyperledger Fabric, we conducted experiments using on-premise machines, AWS public cloud instances, as well as a private cloud infrastructure. We establish that InterSnap can recover from malicious attacks while preserving crosschain transaction receipts. Additionally, our proposed solution demonstrates adaptability to increasing loads while securely transferring snapshot archives with minimal overhead.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Market Making for Goods with Perishable Utility</title>
<link>https://arxiv.org/abs/2511.16357</link>
<guid>https://arxiv.org/abs/2511.16357</guid>
<content:encoded><![CDATA[
<div> The abstract you've provided describes a research paper (arXiv:2511.16357v1) that introduces a novel **decentralized market mechanism** for time-perishable goods—particularly motivated by **computational resources (compute)**. Here's a breakdown and explanation of the key ideas in accessible terms:

---

### 🔹 **Core Problem**
How do we efficiently trade **time-sensitive, perishable resources** like computing power? Compute capacity is only useful at specific times—unused CPU cycles today can't be used tomorrow. Traditional markets often treat compute as bespoke bundles (e.g., spot instances on AWS), but this limits flexibility and efficiency.

Recent advances in **reproducible and verifiable execution** (e.g., technologies allowing computation to be paused, verified for correctness, and resumed across different hardware) enable treating compute as a **standardized, time-indexed commodity**, similar to electricity or bandwidth.

---

### 🔹 **Key Innovation: Automated Market Maker (AMM) for Time-Indexed Compute**

Instead of relying on order books or auctions, the authors design an **Automated Market Maker (AMM)**—a smart contract-style mechanism that continuously quotes prices.

- **Pricing Rule**: The AMM sets an **hourly price** using a **concave function of load**.
  - *Load* = current demand / "floor supply"
    - Floor supply = minimum amount of compute providers are willing to offer at a preset base ("floor") price.
  - Concave pricing means prices rise with demand, but at a decreasing rate initially, then accelerate near capacity—this helps manage congestion while keeping latency low.

This **decouples price discovery from allocation**:
- Prices are posted transparently and updated frequently.
- Buyers and sellers don’t need to negotiate; they just react to prices.

→ Result: **Transparent, low-latency trading**, suitable for decentralized environments.

---

### 🔹 **Equilibrium Analysis**

The paper proves:
- An **equilibrium set of prices exists**.
- It is **unique** under reasonable conditions.
- Conditions are given under which the equilibrium is **admissible**, meaning supply ≥ demand (i.e., the market clears without shortages).

This gives theoretical grounding: the mechanism behaves predictably and stably.

---

### 🔹 **Incentive Design: Premium Sharing + Cheapest Feasible Matching (CFM)**

To ensure participants behave well (e.g., providers don’t lie about their costs), the system combines two mechanisms:

1. **Premium Sharing Pool**:
   - Base cost covers provider expenses.
   - Any surplus revenue (from demand exceeding floor supply) is shared **pro rata** among active providers.
   - Encourages early and full participation ("staking") because sharing rewards early commitment.

2. **Cheapest Feasible Matching (CFM)**:
   - Allocates jobs to the cheapest available providers who can fulfill them.
   - Similar in spirit to “pay-as-bid” or greedy matching, but designed to work with the AMM.

Under mild assumptions:
- Providers are incentivized to:
  - **Stake early and fully** (commit resources ahead of time).
  - **Truthfully report their costs** (no benefit from lying).

This aligns individual incentives with system efficiency—crucial for decentralization.

---

### 🔹 **Efficiency & Regret Guarantees**

Even though CFM is simple and fast (computationally efficient), it performs well compared to an optimal offline benchmark:
- The **worst-case regret** (performance gap vs. best possible allocation with full foresight) is **bounded**.
- So, despite being decentralized and myopic (no global lookahead), the mechanism doesn’t perform arbitrarily badly.

---

### 🔹 **Summary & Significance**

This paper proposes a **new paradigm for decentralized markets in time-perishable goods**, using **compute as a case study**. Its contributions include:

| Contribution | Description |
|------------|-------------|
| ✅ **Time-indexed commoditization of compute** | Enabled by verifiable execution; makes compute more fungible. |
| ✅ **AMM with concave pricing** | Enables automated, transparent, real-time pricing based on utilization. |
| ✅ **Decoupling price and allocation** | Improves scalability and reduces latency. |
| ✅ **Incentive alignment via premium sharing + CFM** | Promotes honest behavior and early resource commitment. |
| ✅ **Theoretical guarantees** | Existence/uniqueness of equilibria, admissibility, bounded regret. |

---

### 🔹 Potential Applications
- Decentralized cloud computing markets (e.g., peer-to-peer GPU rental).
- Edge computing coordination.
- Markets for other time-sensitive resources: energy, bandwidth, storage per time slot.

---

### 🔹 Final Thoughts

This work bridges **mechanism design**, **decentralized systems**, and **real-world resource markets**, offering a practical yet theoretically sound framework. By leveraging new tech (verifiable execution), it rethinks how we model and trade compute—not as hardware, but as **standardized, temporal capacity**.

If implemented, such a system could lead to **more efficient, open, and resilient compute markets**, reducing reliance on centralized cloud providers.

Let me know if you'd like a diagrammatic explanation, comparison to existing AMMs (like Uniswap), or help understanding the math behind the concave pricing or regret bound! <div>
arXiv:2511.16357v1 Announce Type: cross 
Abstract: We study decentralized markets for goods whose utility perishes in time, with compute as a primary motivation. Recent advances in reproducible and verifiable execution allow jobs to pause, verify, and resume across heterogeneous hardware, which allow us to treat compute as time indexed capacity rather than bespoke bundles. We design an automated market maker (AMM) that posts an hourly price as a concave function of load--the ratio of current demand to a "floor supply" (providers willing to work at a preset floor). This mechanism decouples price discovery from allocation and yields transparent, low latency trading. We establish existence and uniqueness of equilibrium quotes and give conditions under which the equilibrium is admissible (i.e. active supply weakly exceeds demand). To align incentives, we pair a premium sharing pool (base cost plus a pro rata share of contemporaneous surplus) with a Cheapest Feasible Matching (CFM) rule; under mild assumptions, providers optimally stake early and fully while truthfully report costs. Despite being simple and computationally efficient, we show that CFM attains bounded worst case regret relative to an optimal benchmark.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentially-Private Distributed Model Predictive Control of Linear Discrete-Time Systems with Global Constraints</title>
<link>https://arxiv.org/abs/2405.18690</link>
<guid>https://arxiv.org/abs/2405.18690</guid>
<content:encoded><![CDATA[
<div> The provided text appears to be a **replacement announcement (arXiv:2405.18690v3 Announce Type: replace)** for a scientific paper on **differentially-private distributed model predictive control (DMPC)**. Here's a breakdown and interpretation of the abstract, along with context and implications:

---

### 🔍 **Summary of the Paper (Based on Abstract)**

**Title (inferred):** *Differentially-Private Distributed Model Predictive Control for Constrained Linear Systems*

**Core Problem:**  
While **Distributed Model Predictive Control (DMPC)** is effective for large-scale systems with constraints and enables decentralized optimization, it typically requires subsystems to share local state or control data — raising **privacy concerns**, especially in sensitive applications (e.g., smart grids, autonomous fleets).

**Key Contributions:**

1. **Baseline Approach – Dual Gradient Method:**  
   The authors first analyze a conventional **distributed dual gradient algorithm** applied to DMPC with **coupled global constraints** (e.g., shared resources or network capacities). While this method achieves optimality, it **does not preserve privacy** — an eavesdropper could infer sensitive information from exchanged messages.

2. **Privacy-Preserving Solution – Differential Privacy (DP):**  
   To address this, they introduce **noise into the optimization process** using a **differential privacy mechanism**. This ensures that individual subsystems’ data cannot be confidently inferred by adversaries, even if communication channels are compromised.

3. **Theoretical Guarantees:**  
   - **Convergence:** Despite added noise, the algorithm converges to the **global optimal solution**.
   - **Privacy:** The scheme satisfies **$\epsilon$-differential privacy**, offering a rigorous, quantifiable privacy guarantee.
   - **Control Performance:** An implementation strategy ensures:
     - **Recursive feasibility** (constraints always satisfied)
     - **Closed-loop stability**

4. **Validation:**  
   Simulations demonstrate the effectiveness of the proposed approach in balancing **privacy, performance, and constraint satisfaction**.

---

### 🧠 Key Concepts Explained

- **DMPC:** A control architecture where multiple subsystems cooperatively optimize their actions while respecting local and global constraints, without a central coordinator.
  
- **Coupled Constraints:** Constraints that involve variables from multiple subsystems (e.g., total power consumption ≤ grid capacity), requiring coordination.

- **Differential Privacy (DP):** A statistical notion of privacy ensuring that the output of a computation does not significantly change whether any one participant’s data is included or altered. In control, DP is achieved by adding calibrated noise to signals (e.g., gradients or primal/dual variables).

- **Dual Gradient Method:** Used here to decompose the global optimization problem via Lagrangian duality; each subsystem updates its local variables, and dual variables (e.g., prices or penalties) are updated based on constraint violations.

- **Noise Injection:** Carefully designed noise is added during the dual update steps to obscure sensitive data, while still allowing convergence through diminishing step sizes or other techniques.

---

### ✅ Why This Work Matters

| Aspect | Significance |
|-------|--------------|
| **Privacy + Control Integration** | Bridges formal privacy (from machine learning/data science) with control theory. Rare in MPC literature. |
| **Strong Theoretical Guarantees** | Proves both convergence and $\epsilon$-DP — often challenging due to noise disrupting convergence. |
| **Practical Relevance** | Applicable to energy systems, transportation networks, industrial IoT, where privacy and safety coexist as requirements. |

---

### 📈 Potential Applications

- **Smart Grids:** Protect consumer load profiles while coordinating demand response.
- **Autonomous Vehicle Platoons:** Share trajectory plans without revealing exact positions or intentions.
- **Supply Chains:** Optimize logistics across companies without exposing internal operations.

---

### ⚠️ Challenges & Trade-offs (Implied)

- **Accuracy vs. Privacy:** Higher privacy ($\epsilon \to 0$) means more noise → slower convergence or suboptimality.
- **Tuning Complexity:** Balancing noise magnitude, step size, and constraint tightening requires careful design.
- **Assumptions:** Likely assumes bounded noise, Lipschitz gradients, strong convexity, or fixed communication topology.

---

### 📘 Next Steps (For Readers Interested in This Work)

1. Read the full paper (available at [https://arxiv.org/abs/2405.18690](https://arxiv.org/abs/2405.18690)) to see:
   - How noise is injected (primal, dual, or both?)
   - Whether they use output perturbation, objective perturbation, or gradient perturbation.
   - Details of the stability proof under noisy updates.

2. Compare with related works:
   - Zhu & Martínez (2012) – Early work on differential privacy in distributed optimization.
   - Han et al. – Applied DP to consensus and online learning.
   - Recent papers on "privacy-aware MPC" (e.g., in IEEE TAC, Automatica).

3. Consider extensions:
   - Can this handle time-varying topologies?
   - Is there a trade-off curve between $\epsilon$ and control cost?

---

If you'd like, I can help:
- Summarize the full paper once you upload it,
- Explain the technical proofs (e.g., convergence under noise),
- Or simulate a simplified version of the algorithm.

Let me know how you’d like to proceed! <div>
arXiv:2405.18690v3 Announce Type: replace 
Abstract: Distributed model predictive control (DMPC) has attracted extensive attention as it can explicitly handle system constraints and achieve optimal control in a decentralized manner. However, the deployment of DMPC strategies generally requires the sharing of sensitive data among subsystems, which may violate the privacy of participating systems. In this paper, we propose a differentially-private DMPC algorithm for linear discrete-time systems subject to coupled global constraints. Specifically, we first show that a conventional distributed dual gradient algorithm can be used to address the considered DMPC problem but cannot provide strong privacy preservation. Then, to protect privacy against the eavesdropper, we incorporate a differential-privacy noise injection mechanism into the DMPC framework and prove that the resulting distributed optimization algorithm can ensure both provable convergence to a global optimal solution and rigorous $\epsilon$-differential privacy. In addition, an implementation strategy of the DMPC is designed such that the recursive feasibility and stability of the closed-loop system are guaranteed. Simulation results are provided to demonstrate the effectiveness of the developed approach.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Securing Smart Contract Languages with a Unified Agentic Framework for Vulnerability Repair in Solidity and Move</title>
<link>https://arxiv.org/abs/2502.18515</link>
<guid>https://arxiv.org/abs/2502.18515</guid>
<content:encoded><![CDATA[
<div> The arXiv paper **2502.18515v2**, titled *"Smartify: A Multi-Agent Framework Leveraging LLMs for Automated Vulnerability Detection and Repair in Solidity and Move Smart Contracts"*, presents a significant advancement in smart contract security through the use of Large Language Models (LLMs) within a multi-agent architecture. Here's a structured summary and analysis of the key contributions and implications:

---

### 🔍 **Core Problem**
- **Context**: As blockchain ecosystems grow, so does the value locked in smart contracts—making them high-value targets.
- **Challenge**: Despite advances in secure programming languages like **Solidity** (Ethereum) and **Move** (Sui, Aptos), vulnerabilities persist due to complex logic, subtle language semantics, and human error.
- Existing tools (static analyzers, symbolic execution, rule-based LLMs) often lack precision, generalizability, or language-specific depth.

---

### 🧠 **Proposed Solution: Smartify**
**Smartify** is a novel **multi-agent framework** that uses **specialized, fine-tuned LLMs** working collaboratively to:
1. **Detect vulnerabilities**
2. **Generate secure fixes**

#### Key Innovations:
| Feature | Description |
|-------|-------------|
| **Multi-Agent Architecture** | Each agent has a specific role (e.g., vulnerability scanner, code analyzer, fix generator, validator), enabling division of labor and deeper reasoning. |
| **Language-Specific Fine-Tuning** | Agents are based on LLMs fine-tuned not just on code, but on **security patterns**, **language semantics**, and **formal verification principles** for both Solidity and Move. |
| **Concept-Based Reasoning** | Instead of relying solely on pattern matching or pre-training data, agents reason about core programming concepts (e.g., reentrancy, ownership, invariant preservation). |
| **No Need for Massive Pre-Training** | Smartify shows that targeted fine-tuning + agent specialization can outperform larger general-purpose models without requiring billion-dollar-scale training. |

---

### ⚙️ **Technical Highlights**
- **Base Models Used**: 
  - Gemma2 and CodeGemma as base LLMs
  - Compared against Llama 3.1 and other SOTA open-source LLMs
- **Performance**:
  - Achieves **state-of-the-art results** in automated repair accuracy across multiple vulnerability types (e.g., reentrancy, integer overflow, access control).
  - Outperforms both domain-general LLMs and prior code-focused models.
  - Demonstrates strong cross-language adaptability: effective in both **Solidity** and **Move**, despite Move’s stricter type system and resource-oriented design.
- **Dataset Curation**:
  - Solidity: Benchmark from known datasets (e.g., SmartBugs, Securify).
  - Move: First curated dataset for vulnerability repair evaluation, addressing a gap in existing research.

---

### ✅ **Results & Impact**
- **High Fix Accuracy**: Smartify generates correct, minimal, and safe patches with fewer hallucinations than baseline models.
- **Efficiency**: Multi-agent collaboration reduces false positives and improves patch quality via internal validation loops.
- **Generalization**: Successfully handles previously unseen vulnerability combinations and edge cases.
- **Language Nuance Handling**: Particularly notable in **Move**, where it respects linear typing and resource safety without explicit manual rules.

---

### 📚 **Contributions**
1. **Smartify Framework**: Open-sourced multi-agent system for autonomous smart contract auditing and repair.
2. **Evaluation Benchmark for Move**: First public dataset focused on real-world Move contract vulnerabilities and fixes.
3. **Comparative Study**: Detailed benchmarking of LLMs (Gemma, Llama, CodeGen, etc.) on smart contract repair tasks.
4. **Design Recipe**: Provides a methodology ("blueprint") for extending Smartify to other domains (e.g., Rust-based systems, WebAssembly modules).

---

### 🌐 **Broader Implications**
- **Security at Scale**: Enables continuous, automated auditing of decentralized applications (dApps), critical for DeFi, NFTs, and DAOs.
- **Democratization of Security**: Makes advanced vulnerability detection accessible even to smaller development teams lacking formal verification expertise.
- **Future of DevTools**: Paves the way for AI-powered IDE integrations that suggest real-time fixes during development.

---

### 🛠️ **Limitations & Future Work**
- **False Positives/Negatives**: Still present in complex logic bugs; hybrid approaches with formal methods may help.
- **Runtime Overhead**: Multi-agent coordination introduces latency; optimization needed for production pipelines.
- **Adversarial Robustness**: Potential for prompt injection or manipulation in agent communication needs study.

---

### 📎 Conclusion
*Smartify* represents a paradigm shift in how we approach smart contract security—moving from **rule-based static analysis** and **monolithic AI models** toward **collaborative, specialized, concept-aware multi-agent systems**. By combining fine-grained language understanding with modular reasoning, it sets a new standard for reliability in blockchain software engineering.

> 🔗 **Availability**: The authors commit to releasing the framework, datasets, and fine-tuning recipes to foster reproducibility and community extension.

---

If you'd like, I can:
- Summarize the methodology section if available,
- Compare Smartify to similar frameworks (like Slither+AI, GPT-Smart, etc.),
- Help draft a blog post or presentation slide deck based on this paper.

Let me know how you’d like to proceed! <div>
arXiv:2502.18515v2 Announce Type: replace 
Abstract: The rapid growth of the blockchain ecosystem and the increasing value locked in smart contracts necessitate robust security measures. While languages like Solidity and Move aim to improve smart contract security, vulnerabilities persist. This paper presents Smartify, a novel multi-agent framework leveraging Large Language Models (LLMs) to automatically detect and repair vulnerabilities in Solidity and Move smart contracts. Unlike traditional methods that rely solely on vast pre-training datasets, Smartify employs a team of specialized agents working on different specially fine-tuned LLMs to analyze code based on underlying programming concepts and language-specific security principles. We evaluated Smartify on a dataset for Solidity and a curated dataset for Move, demonstrating its effectiveness in fixing a wide range of vulnerabilities. Our results show that Smartify (Gemma2+codegemma) achieves state-of-the-art performance, surpassing existing LLMs and enhancing general-purpose models' capabilities, such as Llama 3.1. Notably, Smartify can incorporate language-specific knowledge, such as the nuances of Move, without requiring massive language-specific pre-training datasets. This work offers a detailed analysis of various LLMs' performance on smart contract repair, highlighting the strengths of our multi-agent approach and providing a blueprint for developing more secure and reliable decentralized applications in the growing blockchain landscape. We also provide a detailed recipe for extending this to other similar use cases.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MonadBFT: Fast, Responsive, Fork-Resistant Streamlined Consensus</title>
<link>https://arxiv.org/abs/2502.20692</link>
<guid>https://arxiv.org/abs/2502.20692</guid>
<content:encoded><![CDATA[
<div> The abstract you've provided describes **arXiv:2502.20692v2**, a revised version of a paper introducing **MonadBFT**, a new Byzantine Fault Tolerant (BFT) consensus protocol designed for high performance and robustness in blockchain systems. Here's a breakdown and analysis of the key contributions and innovations highlighted in the abstract:

---

### 🔷 **Overview of MonadBFT**

**MonadBFT** is a **HotStuff-family** BFT consensus protocol—meaning it inherits desirable traits like:
- **Linear message complexity** in the common case (i.e., $O(n)$ communication per decision),
- **Pipelining** to enable continuous block production,
- Use of **leader-based views** with cryptographic voting (e.g., via threshold signatures).

It is implemented as the core consensus engine in the **Monad blockchain**, which aims to scale Ethereum-compatible smart contract execution.

---

### 🔷 **Key Innovations & Contributions**

#### 1. **Tail-Forking Resistance**
- **Problem**: In many pipelined BFT protocols (like HotStuff), when a leader fails or is rotated, the current proposal may be abandoned, creating a "tail-fork" — an incomplete branch.
- **Attack Vector**: A malicious leader can **strategically drop** the prior proposer’s block—even if valid—to censor transactions, reorder them for MEV extraction, or deny rewards to others.
- **Solution**: MonadBFT prevents this by ensuring that once a block is proposed under certain conditions, it cannot be arbitrarily discarded by the next leader.
- **Impact**: 
  - Improves **fairness** (proposers aren’t denied rightful rewards),
  - Reduces **MEV exploitation vectors**,
  - Enhances **integrity** of transaction ordering.

#### 2. **Speculative Finality**
- Allows nodes to **execute transactions speculatively after just one round (single view)**, assuming no equivocation from the leader.
- Execution results are considered *tentative* but highly likely to persist.
- Only in the rare case of **provable leader equivocation** (e.g., proposing two conflicting blocks in the same view) would reverts occur.
- **Benefit**: Drastically reduces **user-perceived latency**, enabling faster feedback (e.g., for dApps or frontends).
- This is optimistic speculation grounded in safety guarantees — not probabilistic finality.

#### 3. **Leader Fault Isolation**
- Ensures the system **recovers quickly** from leader failures.
- The fault of a single leader (malicious or crashed) does not cascade into prolonged downtime or complex recovery procedures.
- Likely achieved through fast view-change mechanisms tied to accountability (e.g., detecting equivocation or timeouts with evidence).
- Supports **high availability** and responsiveness even under adversarial leadership.

---

### 🔷 **Why This Matters: Novelty Claim**

> *"To our knowledge, no prior pipelined, leader-based BFT consensus protocol combines all of these properties in a single design."*

This suggests that while individual features exist elsewhere:
- Linear complexity → HotStuff,
- Speculation → some speculative variants of PBFT or Streamlet,
- MEV-aware designs → recent work in fairness (e.g., PBS, SMR with ordered mempools),
- Fast recovery → various BFT optimizations,

...**MonadBFT unifies** them into a coherent, practical framework tailored for modern blockchains where **low latency, fairness, and resistance to manipulation** are critical.

---

### 🔷 **Implications for Blockchain Performance & Security**

| Feature | Benefit |
|-------|--------|
| Tail-forking resistance | Prevents MEV-based sabotage; promotes fair proposer inclusion |
| Speculative finality | Near-instant execution feedback; improves UX without sacrificing safety |
| Leader fault isolation | High liveness even under churn or attacks |
| Linear messaging | Scalable to large validator sets |

Together, these make MonadBFT suitable for **high-throughput, low-latency blockchains** like Monad, which aims to achieve thousands of TPS while maintaining compatibility with EVM-based applications.

---

### 🔷 **Potential Research & Practical Questions**
For deeper understanding, one might explore:
- How is "speculative finality" formally defined? What safety violations could arise?
- What cryptographic or protocol-level mechanisms enforce tail-forking resistance? (E.g., locking rules, mandatory inclusion proofs?)
- How often do reverts actually happen in practice? Are they economically disincentivized?
- Is there a formal proof of safety/liveness under network asynchrony or partial synchrony?

---

### ✅ Conclusion

**MonadBFT** represents a significant step forward in **practical BFT consensus design**, especially for blockchain environments where:
- **Performance matters** (linear scaling, low latency),
- **Fairness is crucial** (to prevent MEV abuse),
- And **system integrity must survive faulty/malicious leaders**.

By integrating **tail-forking resistance**, **speculative execution**, and **rapid fault recovery**, it addresses real-world operational challenges beyond theoretical safety—making it a compelling candidate for next-generation blockchain infrastructures.

If you'd like, I can help summarize the full paper (when available), compare it to other protocols (e.g., Tendermint, HotStuff, Casper FFG), or explain how speculative finality interacts with rollback safety. <div>
arXiv:2502.20692v2 Announce Type: replace 
Abstract: This paper introduces MonadBFT, a novel Byzantine Fault Tolerant (BFT) consensus protocol that advances both performance and robustness. MonadBFT is implemented as the consensus protocol in the Monad blockchain. As a HotStuff-family protocol, MonadBFT has linear message complexity in the common case and is optimistically responsive, operating as quickly as the network allows. A central feature of MonadBFT is its tail-forking resistance. In pipelined BFT protocols, when a leader goes offline, the previous proposal is abandoned. Malicious leaders can exploit this tail-forking behavior as a form of Maximal Extractable Value (MEV) attack by deliberately discarding their predecessor's block, depriving that proposer of rewards and enabling transaction reordering, censorship or theft. MonadBFT prevents such tail-forking attacks, preserving both fairness and integrity in transaction execution. Another related feature of MonadBFT is its notion of speculative finality, which enables parties to execute ordered transactions after a single round (i.e., a single view), with reverts occurring only in the rare case of provable leader equivocation. This mechanism reduces user-perceived latency. Additionally, we introduce the leader fault isolation property, which ensures that the protocol can quickly recover from a failure. To our knowledge, no prior pipelined, leader-based BFT consensus protocol combines all of these properties in a single design.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Bilevel Optimization: A Perspective from Transient Iteration Complexity</title>
<link>https://arxiv.org/abs/2402.03167</link>
<guid>https://arxiv.org/abs/2402.03167</guid>
<content:encoded><![CDATA[
<div> The provided text appears to be metadata and an abstract from a paper on arXiv with the identifier **2402.03167v4**, marked as a "replace-cross" announcement, indicating it is a revised version of a cross-listed paper.

Here's a clear summary and explanation of the content:

---

**Paper Title (inferred):** *D-SOBA: A Decentralized Stochastic One-loop Bilevel Algorithm Framework*

**arXiv ID:** [2402.03167v4](https://arxiv.org/abs/2402.03167)

**Announcement Type:** Replacement (updated version)  
**Cross-listed as:** Likely in Machine Learning (cs.LG), Optimization (math.OC), or related areas

---

### **Abstract Summary**

This paper addresses **Stochastic Bilevel Optimization (SBO)**, a class of optimization problems with nested structures that are increasingly important in modern machine learning applications such as hyperparameter tuning, meta-learning, and reinforcement learning.

#### **Key Challenges**
- Most existing decentralized SBO methods focus only on **asymptotic convergence rates**, which describe long-term behavior.
- They neglect **transient iteration complexity** — the number of iterations needed before asymptotic behavior kicks in.
- This gap limits understanding of how practical factors affect performance:
  - **Network topology** (how nodes are connected)
  - **Data heterogeneity** (non-IID data across nodes)
  - **Nested algorithmic structure** inherent in bilevel problems

#### **Proposed Solution: D-SOBA**
The authors introduce **D-SOBA** (**Decentralized Stochastic One-loop Bilevel Algorithm**), a new framework designed for large-scale, decentralized environments where nodes communicate only with neighbors (no central server), improving **communication efficiency** and **robustness**.

**Two Variants:**
1. **D-SOBA-SO**: Uses **second-order information** (Hessian and Jacobian matrices) for faster or more accurate updates.
2. **D-SOBA-FO**: Relies solely on **first-order gradients**, making it simpler and more scalable.

#### **Theoretical Contributions**
- First **non-asymptotic convergence analysis** for decentralized SBO.
- Explicit characterization of **transient iteration complexity**.
- Theoretical insights into how network topology, data heterogeneity, and algorithm design impact convergence.

#### **Experimental Results**
- Extensive experiments validate the **efficiency** and **superior performance** of D-SOBA.
- Demonstrates advantages both empirically and in alignment with theoretical predictions.

---

### **Significance**
This work bridges a critical gap between theory and practice in decentralized bilevel optimization by:
- Providing actionable theoretical insights beyond asymptotics.
- Offering flexible algorithms suitable for real-world distributed systems.
- Enabling better design choices based on network and data characteristics.

---

### **Potential Applications**
- Federated learning with hyperparameter optimization
- Decentralized meta-learning
- Distributed model training with regularization learned via inner loops
- Edge computing scenarios requiring robust, server-free coordination

---

If you're looking for further details, I can help:
- Summarize specific sections of the full paper
- Explain technical components like one-loop bilevel methods or non-asymptotic analysis
- Compare D-SOBA to other SBO or decentralized optimization algorithms
- Provide LaTeX citation for this paper

Let me know your needs! <div>
arXiv:2402.03167v4 Announce Type: replace-cross 
Abstract: Stochastic bilevel optimization (SBO) is becoming increasingly essential in machine learning due to its versatility in handling nested structures. To address large-scale SBO, decentralized approaches have emerged as effective paradigms in which nodes communicate with immediate neighbors without a central server, thereby improving communication efficiency and enhancing algorithmic robustness. However, most decentralized SBO algorithms focus solely on asymptotic convergence rates, overlooking transient iteration complexity-the number of iterations required before asymptotic rates dominate, which results in limited understanding of the influence of network topology, data heterogeneity, and the nested bilevel algorithmic structures. To address this issue, this paper introduces D-SOBA, a Decentralized Stochastic One-loop Bilevel Algorithm framework. D-SOBA comprises two variants: D-SOBA-SO, which incorporates second-order Hessian and Jacobian matrices, and D-SOBA-FO, which relies entirely on first-order gradients. We provide a comprehensive non-asymptotic convergence analysis and establish the transient iteration complexity of D-SOBA. This provides the first theoretical understanding of how network topology, data heterogeneity, and nested bilevel structures influence decentralized SBO. Extensive experimental results demonstrate the efficiency and theoretical advantages of D-SOBA.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents</title>
<link>https://arxiv.org/abs/2511.15074</link>
<guid>https://arxiv.org/abs/2511.15074</guid>
<content:encoded><![CDATA[
arXiv:2511.15074v1 Announce Type: new 
Abstract: The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a "flooding-pruning" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Walls Have Ears: Unveiling Cross-Chain Sandwich Attacks in DeFi</title>
<link>https://arxiv.org/abs/2511.15245</link>
<guid>https://arxiv.org/abs/2511.15245</guid>
<content:encoded><![CDATA[
arXiv:2511.15245v1 Announce Type: new 
Abstract: Cross-chain interoperability is a core component of modern blockchain infrastructure, enabling seamless asset transfers and composable applications across multiple blockchain ecosystems. However, the transparency of cross-chain messages can inadvertently expose sensitive transaction information, creating opportunities for adversaries to exploit value through manipulation or front-running strategies.
  In this work, we investigate cross-chain sandwich attacks targeting liquidity pool-based cross-chain bridge protocols. We uncover a critical vulnerability where attackers can exploit events emitted on the source chain to learn transaction details on the destination chain before they appear in the destination chain mempool. This information advantage allows attackers to strategically place front-running and back-running transactions, ensuring that their front-running transactions always precede those of existing MEV bots monitoring the mempool of the destination chain. Moreover, current sandwich-attack defenses are ineffective against this new cross-chain variant. To quantify this threat, we conduct an empirical study using two months (August 10 to October 10, 2025) of cross-chain transaction data from the Symbiosis protocol and a tailored heuristic detection model. Our analysis identifies attacks that collectively garnered over \(5.27\) million USD in profit, equivalent to 1.28\% of the total bridged volume.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Path Planning through Multi-Agent Reinforcement Learning in Dynamic Environments</title>
<link>https://arxiv.org/abs/2511.15284</link>
<guid>https://arxiv.org/abs/2511.15284</guid>
<content:encoded><![CDATA[
<div> 动态路径规划、强化学习、区域分解、联邦Q学习、环境局部性<br /><br />
总结:  
1. 提出一种面向动态环境的区域感知强化学习路径规划框架。  
2. 基于环境变化具局部性的观察，采用分层分解与分布式RL代理实现局部适应。  
3. 引入以子环境成功率驱动的再训练机制，智能触发策略更新。  
4. 探索单智能体Q学习与多智能体联邦Q学习，后者通过聚合Q表提升学习效率。  
5. 在多重障碍变化与高难度场景下，联邦方法逼近A* Oracle表现，具备更强可扩展性与实时性。 <div>
arXiv:2511.15284v1 Announce Type: new 
Abstract: Path planning in dynamic environments is a fundamental challenge in intelligent transportation and robotics, where obstacles and conditions change over time, introducing uncertainty and requiring continuous adaptation. While existing approaches often assume complete environmental unpredictability or rely on global planners, these assumptions limit scalability and practical deployment in real-world settings. In this paper, we propose a scalable, region-aware reinforcement learning (RL) framework for path planning in dynamic environments. Our method builds on the observation that environmental changes, although dynamic, are often localized within bounded regions. To exploit this, we introduce a hierarchical decomposition of the environment and deploy distributed RL agents that adapt to changes locally. We further propose a retraining mechanism based on sub-environment success rates to determine when policy updates are necessary. Two training paradigms are explored: single-agent Q-learning and multi-agent federated Q-learning, where local Q-tables are aggregated periodically to accelerate the learning process. Unlike prior work, we evaluate our methods in more realistic settings, where multiple simultaneous obstacle changes and increasing difficulty levels are present. Results show that the federated variants consistently outperform their single-agent counterparts and closely approach the performance of A* Oracle while maintaining shorter adaptation times and robust scalability. Although initial training remains time-consuming in large environments, our decentralized framework eliminates the need for a global planner and lays the groundwork for future improvements using deep RL and flexible environment decomposition.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BlueBottle: Fast and Robust Blockchains through Subsystem Specialization</title>
<link>https://arxiv.org/abs/2511.15361</link>
<guid>https://arxiv.org/abs/2511.15361</guid>
<content:encoded><![CDATA[
arXiv:2511.15361v1 Announce Type: new 
Abstract: Blockchain consensus faces a trilemma of security, latency, and decentralization. High-throughput systems often require a reduction in decentralization or robustness against strong adversaries, while highly decentralized and secure systems tend to have lower performance. We present BlueBottle, a two-layer consensus architecture. The core layer, BB-Core, is an n=5f+1 protocol that trades some fault tolerance for a much lower finality latency with a medium-sized core validator set. Our experiments show that BB-Core reduces latency by 20-25% in comparison to Mysticeti. The guard layer, BB-Guard, provides decentralized timestamping, proactive misbehavior detection in BB-Core, and a synchronous recovery path. When it observes equivocations or liveness failures in the core -- while tolerating up to f<3n/5 faulty nodes in the primary layer -- guard validators disseminate evidence, agree on misbehaving parties for exclusion or slashing, and either restart the core protocol (for liveness violations) or select a canonical fork (for safety violations). Together, these layers enable optimistic sub-second finality at high throughput while maintaining strong safety and liveness under a mild synchrony assumption.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multiple Sides of 36 Coins: Measuring Peer-to-Peer Infrastructure Across Cryptocurrencies</title>
<link>https://arxiv.org/abs/2511.15388</link>
<guid>https://arxiv.org/abs/2511.15388</guid>
<content:encoded><![CDATA[
arXiv:2511.15388v1 Announce Type: new 
Abstract: Blockchain technologies underpin an expanding ecosystem of decentralized applications, financial systems, and infrastructure. However, the fundamental networking layer that sustains these systems, the peer-to-peer layer, of all but the top few ecosystems remains largely opaque. In this paper, we present the first longitudinal, cross-network measurement study of 36 public blockchain networks. Over 9 months, we deployed 15 active crawlers, sourced data from two additional community crawlers, and conducted hourly connectivity probes to observe the evolving state of these networks. Furthermore, by leveraging Ethereum's discovery protocols, we inferred metadata for an additional 19 auxiliary networks that utilize the Ethereum peer discovery protocol. We also explored Internet-wide scans, which only require probing each protocol's default ports with a simple, network-specific payload. This approach allows us to rapidly identify responsive peers across the entire address space without having to implement custom discovery and handshake logic for every blockchain. We validated this method on Bitcoin and similar networks with known ground truth, then applied it to Cardano, which we could not crawl directly.
  Our study uncovers dramatic variation in network size from under 10 to more than 10,000 active nodes. We quantify trends in IPv4 versus IPv6 usage, analyze autonomous systems and geographic concentration, and characterize churn, diurnal behavior, and the coverage and redundancy of discovery protocols. These findings expose critical differences in network resilience, decentralization, and observability. Beyond characterizing each network, our methodology demonstrates a general framework for measuring decentralized networks at scale. This opens the door for continued monitoring, benchmarking, and more transparent assessments of blockchain infrastructure across diverse ecosystems.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Can You Trust Bitcoin? Value-Dependent Block Confirmation to Determine Transaction Finalit</title>
<link>https://arxiv.org/abs/2511.15421</link>
<guid>https://arxiv.org/abs/2511.15421</guid>
<content:encoded><![CDATA[
arXiv:2511.15421v1 Announce Type: new 
Abstract: We study financial transaction confirmation finality in Bitcoin as a function of transaction amount and user risk tolerance. A transaction is recorded in a block on a blockchain. However, a transaction may be revoked due to a fork in the blockchain, the odds of which decrease over time but never reach zero. Therefore, a transaction is considered confirmed if its block is sufficiently deep in the blockchain. This depth is usually set empirically at some fixed number such as six blocks. We analyze forks under varying network delays in simulation and actual Bitcoin data. Based on this analysis, we establish a relationship between block depth and the probability of confirmation revocation due to a fork. We use prospect theory to relate transaction confirmation probability to transaction amount and user risk tolerance.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining</title>
<link>https://arxiv.org/abs/2511.15456</link>
<guid>https://arxiv.org/abs/2511.15456</guid>
<content:encoded><![CDATA[
arXiv:2511.15456v1 Announce Type: new 
Abstract: As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beluga: Block Synchronization for BFT Consensus Protocols</title>
<link>https://arxiv.org/abs/2511.15517</link>
<guid>https://arxiv.org/abs/2511.15517</guid>
<content:encoded><![CDATA[
arXiv:2511.15517v1 Announce Type: new 
Abstract: Modern high-throughput BFT consensus protocols use streamlined push-pull mechanisms to disseminate blocks and keep happy-path performance optimal. Yet state-of-the-art designs lack a principled and efficient way to exchange blocks, which leaves them open to targeted attacks and performance collapse under network asynchrony. This work introduces the concept of a block synchronizer, a simple abstraction that drives incremental block retrieval and enforces resource-aware exchange. Its interface and role fit cleanly inside a modern BFT consensus stack. We also uncover a new attack, where an adversary steers honest validators into redundant, uncoordinated pulls that exhaust bandwidth and stall progress. Beluga is a modular and scarcity-aware instantiation of the block synchronizer. It achieves optimal common-case latency while bounding the cost of recovery under faults and adversarial behavior. We integrate Beluga into Mysticeti, the consensus core of the Sui blockchain, and show on a geo-distributed AWS deployment that Beluga sustains optimal performance in the optimistic path and, under attack, delivers up to 3x higher throughput and 25x lower latency than prior designs. The Sui blockchain adopted Beluga in production.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Gaussian Process Classification and an Application in Subsea Robotics</title>
<link>https://arxiv.org/abs/2511.15529</link>
<guid>https://arxiv.org/abs/2511.15529</guid>
<content:encoded><![CDATA[
arXiv:2511.15529v1 Announce Type: new 
Abstract: Teams of cooperating autonomous underwater vehicles (AUVs) rely on acoustic communication for coordination, yet this communication medium is constrained by limited range, multi-path effects, and low bandwidth. One way to address the uncertainty associated with acoustic communication is to learn the communication environment in real-time. We address the challenge of a team of robots building a map of the probability of communication success from one location to another in real-time. This is a decentralized classification problem -- communication events are either successful or unsuccessful -- where AUVs share a subset of their communication measurements to build the map. The main contribution of this work is a rigorously derived data sharing policy that selects measurements to be shared among AUVs. We experimentally validate our proposed sharing policy using real acoustic communication data collected from teams of Virginia Tech 690 AUVs, demonstrating its effectiveness in underwater environments.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resource-Constrained Decentralized Federated Learning via Personalized Event-Triggering</title>
<link>https://arxiv.org/abs/2211.12640</link>
<guid>https://arxiv.org/abs/2211.12640</guid>
<content:encoded><![CDATA[
arXiv:2211.12640v2 Announce Type: replace 
Abstract: Federated learning (FL) is a popular technique for distributing machine learning (ML) across a set of edge devices. In this paper, we study fully decentralized FL, where in addition to devices conducting training locally, they carry out model aggregations via cooperative consensus formation over device-to-device (D2D) networks. We introduce asynchronous, event-triggered communications among the devices to handle settings where access to a central server is not feasible. To account for the inherent resource heterogeneity and statistical diversity challenges in FL, we define personalized communication triggering conditions at each device that weigh the change in local model parameters against the available local network resources. We theoretically recover the $O(\ln{k} / \sqrt{k})$ convergence rate to the globally optimal model of decentralized gradient descent (DGD) methods in the setup of our methodology. We provide our convergence guarantees for the last iterates of models, under relaxed graph connectivity and data heterogeneity assumptions compared with the existing literature. To do so, we demonstrate a $B$-connected information flow guarantee in the presence of sporadic communications over the time-varying D2D graph. Our subsequent numerical evaluations demonstrate that our methodology obtains substantial improvements in convergence speed and/or communication savings compared to existing decentralized FL baselines.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A systematic review of metaheuristics-based and machine learning-driven intrusion detection systems in IoT</title>
<link>https://arxiv.org/abs/2506.00377</link>
<guid>https://arxiv.org/abs/2506.00377</guid>
<content:encoded><![CDATA[
arXiv:2506.00377v3 Announce Type: replace 
Abstract: The widespread adoption of the Internet of Things (IoT) has raised a new challenge for developers since it is prone to known and unknown cyberattacks due to its heterogeneity, flexibility, and close connectivity. To defend against such security breaches, researchers have focused on building sophisticated intrusion detection systems (IDSs) using machine learning (ML) techniques. Although these algorithms notably improve detection performance, they require excessive computing power and resources, which are crucial issues in IoT networks considering the recent trends of decentralized data processing and computing systems. Consequently, many optimization techniques have been incorporated with these ML models. Specifically, a special category of optimizer adopted from the behavior of living creatures and different aspects of natural phenomena, known as metaheuristic algorithms, has been a central focus in recent years and brought about remarkable results. Considering this vital significance, we present a comprehensive and systematic review of various applications of metaheuristics algorithms in developing a machine learning-based IDS, especially for IoT. A significant contribution of this study is the discovery of hidden correlations between these optimization techniques and machine learning models integrated with state-of-the-art IoT-IDSs. In addition, the effectiveness of these metaheuristic algorithms in different applications, such as feature selection, parameter or hyperparameter tuning, and hybrid usages are separately analyzed. Moreover, a taxonomy of existing IoT-IDSs is proposed. Furthermore, we investigate several critical issues related to such integration. Our extensive exploration ends with a discussion of promising optimization algorithms and technologies that can enhance the efficiency of IoT-IDSs.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minimmit: Fast Finality with Even Faster Blocks</title>
<link>https://arxiv.org/abs/2508.10862</link>
<guid>https://arxiv.org/abs/2508.10862</guid>
<content:encoded><![CDATA[
arXiv:2508.10862v3 Announce Type: replace 
Abstract: Achieving low-latency consensus in geographically distributed systems remains a key challenge for blockchain and distributed database applications. To this end, there has been significant recent interest in State-Machine-Replication (SMR) protocols that achieve 2-round finality under the assumption that $5f+1\leq n$, where $n$ is the number of processors and $f$ bounds the number of processors that may exhibit Byzantine faults. In these protocols, instructions are organised into views, each led by a different designated leader, and 2-round finality means that a leader's proposal can be finalised after just a single round of voting, meaning two rounds overall (one round for the proposal and one for voting).
  We introduce Minimmit, a Byzantine-fault-tolerant SMR protocol with lower latency than previous 2-round finality approaches. Our key insight is that view progression and transaction finality can operate on different quorum thresholds without compromising safety or liveness. Experiments simulating a globally distributed network of 50 processors, uniformly assigned across ten virtual regions, show that the approach leads to a 23.1% reduction in view latency and a 10.7% reduction in transaction latency compared to the state-of-the-art.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Game-theoretic Decentralized Coordination for Airspace Sector Overload Mitigation</title>
<link>https://arxiv.org/abs/2511.13770</link>
<guid>https://arxiv.org/abs/2511.13770</guid>
<content:encoded><![CDATA[
arXiv:2511.13770v1 Announce Type: new 
Abstract: Decentralized air traffic management systems offer a scalable alternative to centralized control, but often assume high levels of cooperation. In practice, such assumptions frequently break down since airspace sectors operate independently and prioritize local objectives. We address the problem of sector overload in decentralized air traffic management by proposing a mechanism that models self-interested behaviors based on best response dynamics. Each sector adjusts the departure times of flights under its control to reduce its own congestion, without any shared decision making. A tunable cooperativeness factor models the degree to which each sector is willing to reduce overload in other sectors. We prove that the proposed mechanism satisfies a potential game structure, ensuring that best response dynamics converge to a pure Nash equilibrium, under a mild restriction. In addition, we identify a sufficient condition under which an overload-free solution corresponds to a global minimizer of the potential function. Numerical experiments using 24 hours of European flight data demonstrate that the proposed algorithm substantially reduces overload even with only minimal cooperation between sectors, while maintaining scalability and matching the solution quality of centralized solvers.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hashpower allocation in Pay-per-Share blockchain mining pools</title>
<link>https://arxiv.org/abs/2511.13777</link>
<guid>https://arxiv.org/abs/2511.13777</guid>
<content:encoded><![CDATA[
arXiv:2511.13777v1 Announce Type: new 
Abstract: Mining blocks in a blockchain using the \textit{Proof-of-Work} consensus protocol involves significant risk, as network participants face continuous operational costs while earning infrequent capital gains upon successfully mining a block. A common risk mitigation strategy is to join a mining pool, which combines the computing resources of multiple miners to provide a more stable income. This article examines a Pay-per-Share (PPS) reward system, where the pool manager can adjust both the share difficulty and the management fee. Using a simplified wealth model for miners, we explore how miners should allocate their computing resources among different mining pools, considering the trade-off between risk transfer to the manager and management fees.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sigil: Server-Enforced Watermarking in U-Shaped Split Federated Learning via Gradient Injection</title>
<link>https://arxiv.org/abs/2511.14422</link>
<guid>https://arxiv.org/abs/2511.14422</guid>
<content:encoded><![CDATA[
arXiv:2511.14422v1 Announce Type: new 
Abstract: In decentralized machine learning paradigms such as Split Federated Learning (SFL) and its variant U-shaped SFL, the server's capabilities are severely restricted. Although this enhances client-side privacy, it also leaves the server highly vulnerable to model theft by malicious clients. Ensuring intellectual property protection for such capability-limited servers presents a dual challenge: watermarking schemes that depend on client cooperation are unreliable in adversarial settings, whereas traditional server-side watermarking schemes are technically infeasible because the server lacks access to critical elements such as model parameters or labels.
  To address this challenge, this paper proposes Sigil, a mandatory watermarking framework designed specifically for capability-limited servers. Sigil defines the watermark as a statistical constraint on the server-visible activation space and embeds the watermark into the client model via gradient injection, without requiring any knowledge of the data. Besides, we design an adaptive gradient clipping mechanism to ensure that our watermarking process remains both mandatory and stealthy, effectively countering existing gradient anomaly detection methods and a specifically designed adaptive subspace removal attack. Extensive experiments on multiple datasets and models demonstrate Sigil's fidelity, robustness, and stealthiness.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Capacity of Collusion-Resilient Decentralized Secure Aggregation with Groupwise Keys</title>
<link>https://arxiv.org/abs/2511.14444</link>
<guid>https://arxiv.org/abs/2511.14444</guid>
<content:encoded><![CDATA[
arXiv:2511.14444v1 Announce Type: new 
Abstract: This paper investigates the information-theoretic decentralized secure aggregation (DSA) problem under practical groupwise secret keys and collusion resilience. In DSA, $K$ users are interconnected through error-free broadcast channels. Each user holds a private input and aims to compute the sum of all other users' inputs, while satisfying the security constraint that no user, even when colluding with up to $T$ other users, can infer any information about the inputs beyond the recovered sum. To ensure security, users are equipped with secret keys to mask their inputs. Motivated by recent advances in efficient group-based key generation protocols, we consider the symmetric groupwise key setting, where every subset of $G$ users shares a group key that is independent of all other group keys. The problem is challenging because the recovery and security constraints must hold simultaneously for all users, and the structural constraints on the secret keys limit the flexibility of key correlations. We characterize the optimal rate region consisting of all achievable pairs of per-user broadcast communication rate and groupwise key rate. In particular, we show that DSA with groupwise keys is infeasible when $G=1$ or $G\ge K-T$. Otherwise, when $2\le G<K-T$, to securely compute one symbol of the desired sum, each user must broadcast at least one symbol, and each group key must contain at least $(K-T-2)/\binom{K-T-1}{G}$ independent symbols. Our results establish the fundamental limits of DSA with groupwise keys and provide design insights for communication- and key-efficient secure aggregation in decentralized learning systems.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sei Giga</title>
<link>https://arxiv.org/abs/2505.14914</link>
<guid>https://arxiv.org/abs/2505.14914</guid>
<content:encoded><![CDATA[
arXiv:2505.14914v2 Announce Type: replace 
Abstract: We introduce the Sei Giga, a multi-concurrent producer parallelized execution EVM layer one blockchain. In an internal testnet Giga has achieved >5 gigagas/sec throughput and sub 400ms finality. Giga uses Autobahn for consensus with separate DA and consensus layers requiring f+1 votes for a PoA on the DA layer before consensus. Giga reaches consensus over ordering and uses async block execution and state agreement to remove execution from the consensus bottleneck.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Federated Graph Attention Networks for Scalable and Resilient UAV Collision Avoidance</title>
<link>https://arxiv.org/abs/2511.11616</link>
<guid>https://arxiv.org/abs/2511.11616</guid>
<content:encoded><![CDATA[
arXiv:2511.11616v1 Announce Type: new 
Abstract: The real-time performance, adversarial resiliency, and privacy preservation are the most important metrics that need to be balanced to practice collision avoidance in large-scale multi-UAV (Unmanned Aerial Vehicle) systems. Current frameworks tend to prescribe monolithic solutions that are not only prohibitively computationally complex with a scaling cost of $O(n^2)$ but simply do not offer Byzantine fault tolerance. The proposed hierarchical framework presented in this paper tries to eliminate such trade-offs by stratifying a three-layered architecture. We spread the intelligence into three layers: an immediate collision avoiding local layer running on dense graph attention with latency of $<10 ms$, a regional layer using sparse attention with $O(nk)$ computational complexity and asynchronous federated learning with coordinate-wise trimmed mean aggregation, and lastly, a global layer using a lightweight Hashgraph-inspired protocol. We have proposed an adaptive differential privacy mechanism, wherein the noise level $(\epsilon \in [0.1, 1.0])$ is dynamically reduced based on an evaluation of the measured real-time threat that in turn maximized the privacy-utility tradeoff. Through the use of Distributed Hash Table (DHT)-based lightweight audit logging instead of heavyweight blockchain consensus, the median cost of getting a $95^{th}$ percentile decision within 50ms is observed across all tested swarm sizes. This architecture provides a scalable scenario of 500 UAVs with a collision rate of $< 2.0\%$ and the Byzantine fault tolerance of $f < n/3$.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DIAP: A Decentralized Agent Identity Protocol with Zero-Knowledge Proofs and a Hybrid P2P Stack</title>
<link>https://arxiv.org/abs/2511.11619</link>
<guid>https://arxiv.org/abs/2511.11619</guid>
<content:encoded><![CDATA[
arXiv:2511.11619v1 Announce Type: new 
Abstract: The absence of a fully decentralized, verifiable, and privacy-preserving communication protocol for autonomous agents remains a core challenge in decentralized computing. Existing systems often rely on centralized intermediaries, which reintroduce trust bottlenecks, or lack decentralized identity-resolution mechanisms, limiting persistence and cross-network interoperability.
  We propose the Decentralized Interstellar Agent Protocol (DIAP), a novel framework for agent identity and communication that enables persistent, verifiable, and trustless interoperability in fully decentralized environments. DIAP binds an agent's identity to an immutable IPFS or IPNS content identifier and uses zero-knowledge proofs (ZKP) to dynamically and statelessly prove ownership, removing the need for record updates.
  We present a Rust SDK that integrates Noir (for zero-knowledge proofs), DID-Key, IPFS, and a hybrid peer-to-peer stack combining Libp2p GossipSub for discovery and Iroh for high-performance, QUIC based data exchange. DIAP introduces a zero-dependency ZKP deployment model through a universal proof manager and compile-time build script that embeds a precompiled Noir circuit, eliminating the need for external ZKP toolchains. This enables instant, verifiable, and privacy-preserving identity proofs.
  This work establishes a practical, high-performance foundation for next-generation autonomous agent ecosystems and agent-to-agent (A to A) economies.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedFedPure: A Medical Federated Framework with MAE-based Detection and Diffusion Purification for Inference-Time Attacks</title>
<link>https://arxiv.org/abs/2511.11625</link>
<guid>https://arxiv.org/abs/2511.11625</guid>
<content:encoded><![CDATA[
arXiv:2511.11625v1 Announce Type: new 
Abstract: Artificial intelligence (AI) has shown great potential in medical imaging, particularly for brain tumor detection using Magnetic Resonance Imaging (MRI). However, the models remain vulnerable at inference time when they are trained collaboratively through Federated Learning (FL), an approach adopted to protect patient privacy. Adversarial attacks can subtly alter medical scans in ways invisible to the human eye yet powerful enough to mislead AI models, potentially causing serious misdiagnoses. Existing defenses often assume centralized data and struggle to cope with the decentralized and diverse nature of federated medical settings. In this work, we present MedFedPure, a personalized federated learning defense framework designed to protect diagnostic AI models at inference time without compromising privacy or accuracy. MedFedPure combines three key elements: (1) a personalized FL model that adapts to the unique data distribution of each institution; (2) a Masked Autoencoder (MAE) that detects suspicious inputs by exposing hidden perturbations; and (3) an adaptive diffusion-based purification module that selectively cleans only the flagged scans before classification. Together, these steps offer robust protection while preserving the integrity of normal, benign images. We evaluated MedFedPure on the Br35H brain MRI dataset. The results show a significant gain in adversarial robustness, improving performance from 49.50% to 87.33% under strong attacks, while maintaining a high clean accuracy of 97.67%. By operating locally and in real time during diagnosis, our framework provides a practical path to deploying secure, trustworthy, and privacy-preserving AI tools in clinical workflows.
  Index Terms: cancer, tumor detection, federated learning, masked autoencoder, diffusion, privacy
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Speculative Decoding in Decentralized LLM Inference: Turning Communication Latency into Computation Throughput</title>
<link>https://arxiv.org/abs/2511.11733</link>
<guid>https://arxiv.org/abs/2511.11733</guid>
<content:encoded><![CDATA[
arXiv:2511.11733v1 Announce Type: new 
Abstract: Speculative decoding accelerates large language model (LLM) inference by using a lightweight draft model to propose tokens that are later verified by a stronger target model. While effective in centralized systems, its behavior in decentralized settings, where network latency often dominates compute, remains under-characterized. We present Decentralized Speculative Decoding (DSD), a plug-and-play framework for decentralized inference that turns communication delay into useful computation by verifying multiple candidate tokens in parallel across distributed nodes. We further introduce an adaptive speculative verification strategy that adjusts acceptance thresholds by token-level semantic importance, delivering an additional 15% to 20% end-to-end speedup without retraining. In theory, DSD reduces cross-node communication cost by approximately (N-1)t1(k-1)/k, where t1 is per-link latency and k is the average number of tokens accepted per round. In practice, DSD achieves up to 2.56x speedup on HumanEval and 2.59x on GSM8K, surpassing the Eagle3 baseline while preserving accuracy. These results show that adapting speculative decoding for decentralized execution provides a system-level optimization that converts network stalls into throughput, enabling faster distributed LLM inference with no model retraining or architectural changes.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Goal-Oriented Multi-Agent Reinforcement Learning for Decentralized Agent Teams</title>
<link>https://arxiv.org/abs/2511.11992</link>
<guid>https://arxiv.org/abs/2511.11992</guid>
<content:encoded><![CDATA[
arXiv:2511.11992v1 Announce Type: new 
Abstract: Connected and autonomous vehicles across land, water, and air must often operate in dynamic, unpredictable environments with limited communication, no centralized control, and partial observability. These real-world constraints pose significant challenges for coordination, particularly when vehicles pursue individual objectives. To address this, we propose a decentralized Multi-Agent Reinforcement Learning (MARL) framework that enables vehicles, acting as agents, to communicate selectively based on local goals and observations. This goal-aware communication strategy allows agents to share only relevant information, enhancing collaboration while respecting visibility limitations. We validate our approach in complex multi-agent navigation tasks featuring obstacles and dynamic agent populations. Results show that our method significantly improves task success rates and reduces time-to-goal compared to non-cooperative baselines. Moreover, task performance remains stable as the number of agents increases, demonstrating scalability. These findings highlight the potential of decentralized, goal-driven MARL to support effective coordination in realistic multi-vehicle systems operating across diverse domains.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Road Safety Through Multi-Camera Image Segmentation with Post-Encroachment Time Analysis</title>
<link>https://arxiv.org/abs/2511.12018</link>
<guid>https://arxiv.org/abs/2511.12018</guid>
<content:encoded><![CDATA[
arXiv:2511.12018v1 Announce Type: new 
Abstract: Traffic safety analysis at signalized intersections is vital for reducing vehicle and pedestrian collisions, yet traditional crash-based studies are limited by data sparsity and latency. This paper presents a novel multi-camera computer vision framework for real-time safety assessment through Post-Encroachment Time (PET) computation, demonstrated at the intersection of H Street and Broadway in Chula Vista, California. Four synchronized cameras provide continuous visual coverage, with each frame processed on NVIDIA Jetson AGX Xavier devices using YOLOv11 segmentation for vehicle detection. Detected vehicle polygons are transformed into a unified bird's-eye map using homography matrices, enabling alignment across overlapping camera views. A novel pixel-level PET algorithm measures vehicle position without reliance on fixed cells, allowing fine-grained hazard visualization via dynamic heatmaps, accurate to 3.3 sq-cm. Timestamped vehicle and PET data is stored in an SQL database for long-term monitoring. Results over various time intervals demonstrate the framework's ability to identify high-risk regions with sub-second precision and real-time throughput on edge devices, producing data for an 800 x 800 pixel logarithmic heatmap at an average of 2.68 FPS. This study validates the feasibility of decentralized vision-based PET analysis for intelligent transportation systems, offering a replicable methodology for high-resolution, real-time, and scalable intersection safety evaluation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedSDA: Federated Stain Distribution Alignment for Non-IID Histopathological Image Classification</title>
<link>https://arxiv.org/abs/2511.12044</link>
<guid>https://arxiv.org/abs/2511.12044</guid>
<content:encoded><![CDATA[
arXiv:2511.12044v1 Announce Type: new 
Abstract: Federated learning (FL) has shown success in collaboratively training a model among decentralized data resources without directly sharing privacy-sensitive training data. Despite recent advances, non-IID (non-independent and identically distributed) data poses an inevitable challenge that hinders the use of FL. In this work, we address the issue of non-IID histopathological images with feature distribution shifts from an intuitive perspective that has only received limited attention. Specifically, we address this issue from the perspective of data distribution by solely adjusting the data distributions of all clients. Building on the success of diffusion models in fitting data distributions and leveraging stain separation to extract the pivotal features that are closely related to the non-IID properties of histopathological images, we propose a Federated Stain Distribution Alignment (FedSDA) method. FedSDA aligns the stain distribution of each client with a target distribution in an FL framework to mitigate distribution shifts among clients. Furthermore, considering that training diffusion models on raw data in FL has been shown to be susceptible to privacy leakage risks, we circumvent this problem while still effectively achieving alignment. Extensive experimental results show that FedSDA is not only effective in improving baselines that focus on mitigating disparities across clients' model updates but also outperforms baselines that address the non-IID data issues from the perspective of data distribution. We show that FedSDA provides valuable and practical insights for the computational pathology community.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Game-Theoretic Safe Multi-Agent Motion Planning with Reachability Analysis for Dynamic and Uncertain Environments (Extended Version)</title>
<link>https://arxiv.org/abs/2511.12160</link>
<guid>https://arxiv.org/abs/2511.12160</guid>
<content:encoded><![CDATA[
arXiv:2511.12160v1 Announce Type: new 
Abstract: Ensuring safe, robust, and scalable motion planning for multi-agent systems in dynamic and uncertain environments is a persistent challenge, driven by complex inter-agent interactions, stochastic disturbances, and model uncertainties. To overcome these challenges, particularly the computational complexity of coupled decision-making and the need for proactive safety guarantees, we propose a Reachability-Enhanced Dynamic Potential Game (RE-DPG) framework, which integrates game-theoretic coordination into reachability analysis. This approach formulates multi-agent coordination as a dynamic potential game, where the Nash equilibrium (NE) defines optimal control strategies across agents. To enable scalability and decentralized execution, we develop a Neighborhood-Dominated iterative Best Response (ND-iBR) scheme, built upon an iterated $\varepsilon$-BR (i$\varepsilon$-BR) process that guarantees finite-step convergence to an $\varepsilon$-NE. This allows agents to compute strategies based on local interactions while ensuring theoretical convergence guarantees. Furthermore, to ensure safety under uncertainty, we integrate a Multi-Agent Forward Reachable Set (MA-FRS) mechanism into the cost function, explicitly modeling uncertainty propagation and enforcing collision avoidance constraints. Through both simulations and real-world experiments in 2D and 3D environments, we validate the effectiveness of RE-DPG across diverse operational scenarios.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Collaborative Fuzzing with Continuous Reflection for Smart Contracts Vulnerability Detection</title>
<link>https://arxiv.org/abs/2511.12164</link>
<guid>https://arxiv.org/abs/2511.12164</guid>
<content:encoded><![CDATA[
arXiv:2511.12164v1 Announce Type: new 
Abstract: Fuzzing is a widely used technique for detecting vulnerabilities in smart contracts, which generates transaction sequences to explore the execution paths of smart contracts. However, existing fuzzers are falling short in detecting sophisticated vulnerabilities that require specific attack transaction sequences with proper inputs to trigger, as they (i) prioritize code coverage over vulnerability discovery, wasting considerable effort on non-vulnerable code regions, and (ii) lack semantic understanding of stateful contracts, generating numerous invalid transaction sequences that cannot pass runtime execution.
  In this paper, we propose SmartFuzz, a novel collaborative reflective fuzzer for smart contract vulnerability detection. It employs large language model-driven agents as the fuzzing engine and continuously improves itself by learning and reflecting through interactions with the environment. Specifically, we first propose a new Continuous Reflection Process (CRP) for fuzzing smart contracts, which reforms the transaction sequence generation as a self-evolving process through continuous reflection on feedback from the runtime environment. Then, we present the Reactive Collaborative Chain (RCC) to orchestrate the fuzzing process into multiple sub-tasks based on the dependencies of transaction sequences. Furthermore, we design a multi-agent collaborative team, where each expert agent is guided by the RCC to jointly generate and refine transaction sequences from both global and local perspectives. We conduct extensive experiments to evaluate SmartFuzz's performance on real-world contracts and DApp projects. The results demonstrate that SmartFuzz outperforms existing state-of-the-art tools: (i) it detects 5.8\%-74.7\% more vulnerabilities within 30 minutes, and (ii) it reduces false negatives by up to 80\%.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Software Supply Chain Security of Web3</title>
<link>https://arxiv.org/abs/2511.12274</link>
<guid>https://arxiv.org/abs/2511.12274</guid>
<content:encoded><![CDATA[
arXiv:2511.12274v1 Announce Type: new 
Abstract: Web3 applications, built on blockchain technology, manage billions of dollars in digital assets through decentralized applications (dApps) and smart contracts. These systems rely on complex, software supply chains that introduce significant security vulnerabilities. This paper examines the software supply chain security challenges unique to the Web3 ecosystem, where traditional Web2 software supply chain problems intersect with the immutable and high-stakes nature of blockchain technology. We analyze the threat landscape and propose mitigation strategies to strengthen the security posture of Web3 systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Decentralized Root Cause Localization Approach for Edge Computing Environments</title>
<link>https://arxiv.org/abs/2511.12486</link>
<guid>https://arxiv.org/abs/2511.12486</guid>
<content:encoded><![CDATA[
arXiv:2511.12486v1 Announce Type: new 
Abstract: Edge computing environments host increasingly complex microservice-based IoT applications, which are prone to performance anomalies that can propagate across dependent services. Identifying the true source of such anomalies, known as Root Cause Localization (RCL), is essential for timely mitigation. However, existing RCL approaches are designed for cloud environments and rely on centralized analysis, which increases latency and communication overhead when applied at the edge. This paper proposes a decentralized RCL approach that executes localization directly at the edge device level using the Personalized PageRank (PPR) algorithm. The proposed method first groups microservices into communication- and colocation-aware clusters, thereby confining most anomaly propagation within cluster boundaries. Within each cluster, PPR is executed locally to identify the root cause, significantly reducing localization time. For the rare cases where anomalies propagate across clusters, we introduce an inter-cluster peer-to-peer approximation process, enabling lightweight coordination among clusters with minimal communication overhead. To enhance the accuracy of localization in heterogeneous edge environments, we also propose a novel anomaly scoring mechanism tailored to the diverse anomaly triggers that arise across microservice, device, and network layers. Evaluation results on the publicly available edge dataset, MicroCERCL, demonstrate that the proposed decentralized approach achieves comparable or higher localization accuracy than its centralized counterpart while reducing localization time by up to 34%. These findings highlight that decentralized graph-based RCL can provide a practical and efficient solution for anomaly diagnosis in resource-constrained edge environments.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prrr: Personal Random Rewards for Blockchain Reporting</title>
<link>https://arxiv.org/abs/2511.12626</link>
<guid>https://arxiv.org/abs/2511.12626</guid>
<content:encoded><![CDATA[
arXiv:2511.12626v1 Announce Type: new 
Abstract: Smart contracts, the stateful programs running on blockchains, often rely on reports. Publishers are paid to publish these reports on the blockchain. Designing protocols that incentivize timely reporting is the prevalent reporting problem. But existing solutions face a security-performance trade-off: Relying on a small set of trusted publishers introduces centralization risks, while allowing open publication results in an excessive number of reports on the blockchain. We identify the root cause of this trade-off to be the standard symmetric reward design, which treats all reports equally. We prove that no symmetric-reward mechanism can overcome the trade-off.
  We present Personal Random Rewards for Reporting (Prrr), a protocol that assigns random heterogeneous values to reports. We call this novel mechanism-design concept Ex-Ante Synthetic Asymmetry. To the best of our knowledge, Prrr is the first game-theoretic mechanism (in any context) that deliberately forms participant asymmetry. Prrr employs a second-price-style settlement to allocate rewards, ensuring incentive compatibility and achieving both security and efficiency. Following the protocol constitutes a Subgame-Perfect Nash Equilibrium, robust against collusion and Sybil attacks. Prrr is applicable to numerous smart contracts that rely on timely reports.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bandit Learning in Housing Markets</title>
<link>https://arxiv.org/abs/2511.12629</link>
<guid>https://arxiv.org/abs/2511.12629</guid>
<content:encoded><![CDATA[
arXiv:2511.12629v1 Announce Type: new 
Abstract: The housing market, also known as one-sided matching market, is a classic exchange economy model where each agent on the demand side initially owns an indivisible good (a house) and has a personal preference over all goods. The goal is to find a core-stable allocation that exhausts all mutually beneficial exchanges among subgroups of agents. While this model has been extensively studied in economics and computer science due to its broad applications, little attention has been paid to settings where preferences are unknown and must be learned through repeated interactions. In this paper, we propose a statistical learning model within the multi-player multi-armed bandit framework, where players (agents) learn their preferences over arms (goods) from stochastic rewards. We introduce the notion of core regret for each player as the market objective. We study both centralized and decentralized approaches, proving $O(N \log T / \Delta^2)$ upper bounds on regret, where $N$ is the number of players, $T$ is the time horizon and $\Delta$ is the minimum preference gap among players. For the decentralized setting, we also establish a matching lower bound, demonstrating that our algorithm is order-optimal.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Hierarchical AI-Blockchain Framework for Real-Time Anomaly Detection in Large-Scale Autonomous Vehicle Networks</title>
<link>https://arxiv.org/abs/2511.12648</link>
<guid>https://arxiv.org/abs/2511.12648</guid>
<content:encoded><![CDATA[
arXiv:2511.12648v1 Announce Type: new 
Abstract: The security of autonomous vehicle networks is facing major challenges, owing to the complexity of sensor integration, real-time performance demands, and distributed communication protocols that expose vast attack surfaces around both individual and network-wide safety. Existing security schemes are unable to provide sub-10 ms (milliseconds) anomaly detection and distributed coordination of large-scale networks of vehicles within an acceptable safety/privacy framework. This paper introduces a three-tier hybrid security architecture HAVEN (Hierarchical Autonomous Vehicle Enhanced Network), which decouples real-time local threat detection and distributed coordination operations. It incorporates a light ensemble anomaly detection model on the edge (first layer), Byzantine-fault-tolerant federated learning to aggregate threat intelligence at a regional scale (middle layer), and selected blockchain mechanisms (top layer) to ensure critical security coordination. Extensive experimentation is done on a real-world autonomous driving dataset. Large-scale simulations with the number of vehicles ranging between 100 and 1000 and different attack types, such as sensor spoofing, jamming, and adversarial model poisoning, are conducted to test the scalability and resiliency of HAVEN. Experimental findings show sub-10 ms detection latency with an accuracy of 94% and F1-score of 92% across multimodal sensor data, Byzantine fault tolerance validated with 20\% compromised nodes, and a reduced blockchain storage overhead, guaranteeing sufficient differential privacy. The proposed framework overcomes the important trade-off between real-time safety obligation and distributed security coordination with novel three-tiered processing. The scalable architecture of HAVEN is shown to provide great improvement in detection accuracy as well as network resilience over other methods.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Atomic Gliders and CA as Language Generators (Extended Version)</title>
<link>https://arxiv.org/abs/2511.12656</link>
<guid>https://arxiv.org/abs/2511.12656</guid>
<content:encoded><![CDATA[
arXiv:2511.12656v1 Announce Type: new 
Abstract: Cellular automata (CA) are well-studied models of decentralized parallel computation, known for their ability to exhibit complex global behavior from simple local rules. While their dynamics have been widely explored through simulations, a formal treatment of CA as genuine language generators remains underdeveloped. We formalize CA-expressible languages as sets of finite words obtained by projecting the non-quiescent segments of configurations reachable by one-dimensional, deterministic, synchronous CA over bi-infinite grids. These languages are defined with respect to sets of initial configurations specified by a regular language as in regular model checking. To capture structured dynamics, we propose a glider-based generative semantics for CA. Inspired by the classical notion of gliders, we define a glider as a one-cell entity carrying a symbol in a certain velocity under well defined interaction semantics. We show that despite the regularity of the initial configurations and the locality of the transition rules, the resulting languages can exhibit non-regular and even non-context-free structure. This positions regular-initialized CA languages as a surprisingly rich computational model, with potential applications in the formal analysis of linearly ordered MAS.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Time to Consensus in a Blockchain: Insights into Bitcoin's "6 Blocks Rule''</title>
<link>https://arxiv.org/abs/2511.12687</link>
<guid>https://arxiv.org/abs/2511.12687</guid>
<content:encoded><![CDATA[
arXiv:2511.12687v1 Announce Type: new 
Abstract: We investigate the time to consensus in Nakamoto blockchains. Specifically, we consider two competing growth processes, labeled \emph{honest} and \emph{adversarial}, and determine the time after which the honest process permananetly exceeds the adversarial process. This is done via queueing techniques. The predominant difficulty is that the honest growth process is subject to \emph{random delays}. In a stylized Bitcoin model, we compute the Laplace transform for the time to consensus and verify it via simulation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Closer Look at Personalized Fine-Tuning in Heterogeneous Federated Learning</title>
<link>https://arxiv.org/abs/2511.12695</link>
<guid>https://arxiv.org/abs/2511.12695</guid>
<content:encoded><![CDATA[
arXiv:2511.12695v1 Announce Type: new 
Abstract: Federated Learning (FL) enables decentralized, privacy-preserving model training but struggles to balance global generalization and local personalization due to non-identical data distributions across clients. Personalized Fine-Tuning (PFT), a popular post-hoc solution, fine-tunes the final global model locally but often overfits to skewed client distributions or fails under domain shifts. We propose adapting Linear Probing followed by full Fine-Tuning (LP-FT), a principled centralized strategy for alleviating feature distortion (Kumar et al., 2022), to the FL setting. Through systematic evaluation across seven datasets and six PFT variants, we demonstrate LP-FT's superiority in balancing personalization and generalization. Our analysis uncovers federated feature distortion, a phenomenon where local fine-tuning destabilizes globally learned features, and theoretically characterizes how LP-FT mitigates this via phased parameter updates. We further establish conditions (e.g., partial feature overlap, covariate-concept shift) under which LP-FT outperforms standard fine-tuning, offering actionable guidelines for deploying robust personalization in FL.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Density-Driven Optimal Control for Non-Uniform Area Coverage in Decentralized Multi-Agent Systems Using Optimal Transport</title>
<link>https://arxiv.org/abs/2511.12756</link>
<guid>https://arxiv.org/abs/2511.12756</guid>
<content:encoded><![CDATA[
arXiv:2511.12756v1 Announce Type: new 
Abstract: This paper addresses the fundamental problem of non-uniform area coverage in multi-agent systems, where different regions require varying levels of attention due to mission-dependent priorities. Existing uniform coverage strategies are insufficient for realistic applications, and many non-uniform approaches either lack optimality guarantees or fail to incorporate crucial real-world constraints such as agent dynamics, limited operation time, the number of agents, and decentralized execution.
  To resolve these limitations, we propose a novel framework called Density-Driven Optimal Control (D2OC). The central idea of D2OC is the integration of optimal transport theory with multi-agent coverage control, enabling each agent to continuously adjust its trajectory to match a mission-specific reference density map. The proposed formulation establishes optimality by solving a constrained optimization problem that explicitly incorporates physical and operational constraints. The resulting control input is analytically derived from the Lagrangian of the objective function, yielding closed-form optimal solutions for linear systems and a generalizable structure for nonlinear systems. Furthermore, a decentralized data-sharing mechanism is developed to coordinate agents without reliance on global information.
  Comprehensive simulation studies demonstrate that D2OC achieves significantly improved non-uniform area coverage performance compared to existing methods, while maintaining scalability and decentralized implementability.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Look-back Horizon for Time Series Forecasting in Federated Learning</title>
<link>https://arxiv.org/abs/2511.12791</link>
<guid>https://arxiv.org/abs/2511.12791</guid>
<content:encoded><![CDATA[
arXiv:2511.12791v1 Announce Type: new 
Abstract: Selecting an appropriate look-back horizon remains a fundamental challenge in time series forecasting (TSF), particularly in the federated learning scenarios where data is decentralized, heterogeneous, and often non-independent. While recent work has explored horizon selection by preserving forecasting-relevant information in an intrinsic space, these approaches are primarily restricted to centralized and independently distributed settings. This paper presents a principled framework for adaptive horizon selection in federated time series forecasting through an intrinsic space formulation. We introduce a synthetic data generator (SDG) that captures essential temporal structures in client data, including autoregressive dependencies, seasonality, and trend, while incorporating client-specific heterogeneity. Building on this model, we define a transformation that maps time series windows into an intrinsic representation space with well-defined geometric and statistical properties. We then derive a decomposition of the forecasting loss into a Bayesian term, which reflects irreducible uncertainty, and an approximation term, which accounts for finite-sample effects and limited model capacity. Our analysis shows that while increasing the look-back horizon improves the identifiability of deterministic patterns, it also increases approximation error due to higher model complexity and reduced sample efficiency. We prove that the total forecasting loss is minimized at the smallest horizon where the irreducible loss starts to saturate, while the approximation loss continues to rise. This work provides a rigorous theoretical foundation for adaptive horizon selection for time series forecasting in federated learning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Reinforcement Learning for Heterogeneous Satellite Cluster Resources Optimization</title>
<link>https://arxiv.org/abs/2511.12792</link>
<guid>https://arxiv.org/abs/2511.12792</guid>
<content:encoded><![CDATA[
arXiv:2511.12792v1 Announce Type: new 
Abstract: This work investigates resource optimization in heterogeneous satellite clusters performing autonomous Earth Observation (EO) missions using Reinforcement Learning (RL). In the proposed setting, two optical satellites and one Synthetic Aperture Radar (SAR) satellite operate cooperatively in low Earth orbit to capture ground targets and manage their limited onboard resources efficiently. Traditional optimization methods struggle to handle the real-time, uncertain, and decentralized nature of EO operations, motivating the use of RL and Multi-Agent Reinforcement Learning (MARL) for adaptive decision-making. This study systematically formulates the optimization problem from single-satellite to multi-satellite scenarios, addressing key challenges including energy and memory constraints, partial observability, and agent heterogeneity arising from diverse payload capabilities. Using a near-realistic simulation environment built on the Basilisk and BSK-RL frameworks, we evaluate the performance and stability of state-of-the-art MARL algorithms such as MAPPO, HAPPO, and HATRPO. Results show that MARL enables effective coordination across heterogeneous satellites, balancing imaging performance and resource utilization while mitigating non-stationarity and inter-agent reward coupling. The findings provide practical insights into scalable, autonomous satellite operations and contribute a foundation for future research on intelligent EO mission planning under heterogeneous and dynamic conditions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resilient and Efficient Allocation for Large-Scale Autonomous Fleets via Decentralized Coordination</title>
<link>https://arxiv.org/abs/2511.12879</link>
<guid>https://arxiv.org/abs/2511.12879</guid>
<content:encoded><![CDATA[
arXiv:2511.12879v1 Announce Type: new 
Abstract: Operating large autonomous fleets demands fast, resilient allocation of scarce resources (such as energy and fuel, charger access and maintenance slots, time windows, and communication bandwidth) under uncertainty. We propose a side-information-aware approach for resource allocation at scale that combines distributional predictions with decentralized coordination. Local side information shapes per-agent risk models for consumption, which are coupled through chance constraints on failures. A lightweight consensus-ADMM routine coordinates agents over a sparse communication graph, enabling near-centralized performance while avoiding single points of failure. We validate the framework on real urban road networks with autonomous vehicles and on a representative satellite constellation, comparing against greedy, no-side-information, and oracle central baselines. Our method reduces failure rates by 30-55% at matched cost and scales to thousands of agents with near-linear runtime, while preserving feasibility with high probability.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Esim: EVM Bytecode Similarity Detection Based on Stable-Semantic Graph</title>
<link>https://arxiv.org/abs/2511.12971</link>
<guid>https://arxiv.org/abs/2511.12971</guid>
<content:encoded><![CDATA[
arXiv:2511.12971v1 Announce Type: new 
Abstract: Decentralized finance (DeFi) is experiencing rapid expansion. However, prevalent code reuse and limited open-source contributions have introduced significant challenges to the blockchain ecosystem, including plagiarism and the propagation of vulnerable code. Consequently, an effective and accurate similarity detection method for EVM bytecode is urgently needed to identify similar contracts. Traditional binary similarity detection methods are typically based on instruction stream or control flow graph (CFG), which have limitations on EVM bytecode due to specific features like low-level EVM bytecode and heavily-reused basic blocks. Moreover, the highly-diverse Solidity Compiler (Solc) versions further complicate accurate similarity detection.
  Motivated by these challenges, we propose a novel EVM bytecode representation called Stable-Semantic Graph (SSG), which captures relationships between 'stable instructions' (special instructions identified by our study). Moreover, we implement a prototype, Esim, which embeds SSG into matrices for similarity detection using a heterogeneous graph neural network. Esim demonstrates high accuracy in SSG construction, achieving F1-scores of 100% for control flow and 95.16% for data flow, and its similarity detection performance reaches 96.3% AUC, surpassing traditional approaches. Our large-scale study, analyzing 2,675,573 smart contracts on six EVM-compatible chains over a one-year period, also demonstrates that Esim outperforms the SOTA tool Etherscan in vulnerability search.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MEV in Multiple Concurrent Proposer Blockchains</title>
<link>https://arxiv.org/abs/2511.13080</link>
<guid>https://arxiv.org/abs/2511.13080</guid>
<content:encoded><![CDATA[
arXiv:2511.13080v1 Announce Type: new 
Abstract: We analyze maximal extractable value in multiple concurrent proposer blockchains, where multiple blocks become data available before their final execution order is determined. This concurrency breaks the single builder assumption of sequential chains and introduces new MEV channels, including same tick duplicate steals, proposer to proposer auctions, and timing races driven by proof of availability latency. We develop a hazard normalized model of delay and inclusion, derive a closed form delay envelope \(M(\tau)\), and characterize equilibria for censorship, duplication, and auction games. We show how deterministic priority DAG scheduling and duplicate aware payouts neutralize same tick MEV while preserving throughput, identifying simple protocol configurations to mitigate MCP specific extraction without centralized builders.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Personalized Federated Learning with Bidirectional Communication Compression via One-Bit Random Sketching</title>
<link>https://arxiv.org/abs/2511.13144</link>
<guid>https://arxiv.org/abs/2511.13144</guid>
<content:encoded><![CDATA[
arXiv:2511.13144v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative training across decentralized data, but faces key challenges of bidirectional communication overhead and client-side data heterogeneity. To address communication costs while embracing data heterogeneity, we propose pFed1BS, a novel personalized federated learning framework that achieves extreme communication compression through one-bit random sketching. In personalized FL, the goal shifts from training a single global model to creating tailored models for each client. In our framework, clients transmit highly compressed one-bit sketches, and the server aggregates and broadcasts a global one-bit consensus. To enable effective personalization, we introduce a sign-based regularizer that guides local models to align with the global consensus while preserving local data characteristics. To mitigate the computational burden of random sketching, we employ the Fast Hadamard Transform for efficient projection. Theoretical analysis guarantees that our algorithm converges to a stationary neighborhood of the global potential function. Numerical simulations demonstrate that pFed1BS substantially reduces communication costs while achieving competitive performance compared to advanced communication-efficient FL algorithms.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pico-Cloud: Cloud Infrastructure for Tiny Edge Devices</title>
<link>https://arxiv.org/abs/2511.13253</link>
<guid>https://arxiv.org/abs/2511.13253</guid>
<content:encoded><![CDATA[
arXiv:2511.13253v1 Announce Type: new 
Abstract: This paper introduces the Pico-Cloud, a micro-edge cloud architecture built on ultra-minimal hardware platforms such as the Raspberry Pi Zero and comparable single-board computers. The Pico-Cloud delivers container-based virtualization, service discovery, and lightweight orchestration directly at the device layer, enabling local operation with low latency and low power consumption without reliance on centralized data centers. We present its architectural model, outline representative use cases including rural connectivity, educational clusters, and edge AI inference, and analyze design challenges in computation, networking, storage, and power management. The results highlight Pico-Clouds as a cost-effective, decentralized, and sustainable platform for lightweight distributed workloads at the network edge.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributed Hierarchical Machine Learning for Joint Resource Allocation and Slice Selection in In-Network Edge Systems</title>
<link>https://arxiv.org/abs/2511.13313</link>
<guid>https://arxiv.org/abs/2511.13313</guid>
<content:encoded><![CDATA[
arXiv:2511.13313v1 Announce Type: new 
Abstract: The Metaverse promises immersive, real-time experiences; however, meeting its stringent latency and resource demands remains a major challenge. Conventional optimization techniques struggle to respond effectively under dynamic edge conditions and high user loads. In this study, we explore a slice-enabled in-network edge architecture that combines computing-in-the-network (COIN) with multi-access edge computing (MEC). In addition, we formulate the joint problem of wireless and computing resource management with optimal slice selection as a mixed-integer nonlinear program (MINLP). Because solving this model online is computationally intensive, we decompose it into three sub-problems (SP1) intra-slice allocation, (SP2) inter-slice allocation, and (SP3) offloading decision and train a distributed hierarchical DeepSets-based model (DeepSets-S) on optimal solutions obtained offline. In the proposed model, we design a slack-aware normalization mechanism for a shared encoder and task-specific decoders, ensuring permutation equivariance over variable-size wireless device (WD) sets. The learned system produces near-optimal allocations with low inference time and maintains permutation equivariance over variable-size device sets. Our experimental results show that DeepSets-S attains high tolerance-based accuracies on SP1/SP2 (Acc1 = 95.26% and 95.67%) and improves multiclass offloading accuracy on SP3 (Acc = 0.7486; binary local/offload Acc = 0.8824). Compared to exact solvers, the proposed approach reduces the execution time by 86.1%, while closely tracking the optimal system cost (within 6.1% in representative regimes). Compared with baseline models, DeepSets-S consistently achieves higher cost ratios and better utilization across COIN/MEC resources.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LinkXplore: A Framework for Affordable High-Quality Blockchain Data</title>
<link>https://arxiv.org/abs/2511.13318</link>
<guid>https://arxiv.org/abs/2511.13318</guid>
<content:encoded><![CDATA[
arXiv:2511.13318v1 Announce Type: new 
Abstract: Blockchain technologies are rapidly transforming both academia and industry. However, large-scale blockchain data collection remains prohibitively expensive, as many RPC providers only offer enhanced APIs with high pricing tiers that are unsuitable for budget-constrained research or industrial-scale applications, which has significantly slowed down academic studies and product development. Moreover, there is a clear lack of a systematic framework that allows flexible integration of new modules for analyzing on-chain data.
  To address these challenges, we introduce LinkXplore, the first open framework for collecting and managing on-chain data. LinkXplore enables users to bypass costly blockchain data providers by directly analyzing raw data from RPC queries or streams, thereby offering high-quality blockchain data at a fraction of the cost. Through a simple API and backend processing logic, any type of chain data can be integrated into the framework. This makes it a practical alternative for both researchers and developers with limited budgets. Code and dataset used in this project are publicly available at https://github.com/Linkis-Project/LinkXplore
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Merkle Mountain Belt</title>
<link>https://arxiv.org/abs/2511.13582</link>
<guid>https://arxiv.org/abs/2511.13582</guid>
<content:encoded><![CDATA[
arXiv:2511.13582v1 Announce Type: new 
Abstract: Merkle structures are widely used as commitment schemes: they allow a prover to publish a compact commitment to an ordered list $X$ of items, and then efficiently prove to a verifier that $x_i\in X$ is the $i$-th item in it. We compare different Merkle structures and their corresponding properties as commitment schemes in the context of blockchain applications. Our primary goal is to speed up light client protocols so that, e.g., a user can verify a transaction efficiently from their smartphone.
  For instance, the Merkle Mountain Range (MMR) yields a succinct scheme: a light client synchronizing for the first time can do so with a complexity sublinear in $|X|$. On the other hand, the Merkle chain, traditionally used to commit to block headers, is not succinct, but it is incremental - a light client resynchronizing frequently can do so with constant complexity - and optimally additive - the structure can be updated in constant time when a new item is appended to list $X$.
  We introduce new Merkle structures, most notably the Merkle Mountain Belt (MMB), the first to be simultaneously succinct, incremental and optimally additive. A variant called UMMB is also asynchronous: a light client may continue to interact with the network even when out of sync with the public commitment. Our Merkle structures are slightly unbalanced, so that items recently appended to $X$ receive shorter membership proofs than older items. This feature reduces a light client's expected costs, in applications where queries are biased towards recently generated data.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Client-Server Watermarking for Split Federated Learning</title>
<link>https://arxiv.org/abs/2511.13598</link>
<guid>https://arxiv.org/abs/2511.13598</guid>
<content:encoded><![CDATA[
arXiv:2511.13598v1 Announce Type: new 
Abstract: Split Federated Learning (SFL) is renowned for its privacy-preserving nature and low computational overhead among decentralized machine learning paradigms. In this framework, clients employ lightweight models to process private data locally and transmit intermediate outputs to a powerful server for further computation. However, SFL is a double-edged sword: while it enables edge computing and enhances privacy, it also introduces intellectual property ambiguity as both clients and the server jointly contribute to training. Existing watermarking techniques fail to protect both sides since no single participant possesses the complete model. To address this, we propose RISE, a Robust model Intellectual property protection scheme using client-Server watermark Embedding for SFL. Specifically, RISE adopts an asymmetric client-server watermarking design: the server embeds feature-based watermarks through a loss regularization term, while clients embed backdoor-based watermarks by injecting predefined trigger samples into private datasets. This co-embedding strategy enables both clients and the server to verify model ownership. Experimental results on standard datasets and multiple network architectures show that RISE achieves over $95\%$ watermark detection rate ($p-value \lt 0.03$) across most settings. It exhibits no mutual interference between client- and server-side watermarks and remains robust against common removal attacks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DIGing--SGLD: Decentralized and Scalable Langevin Sampling over Time--Varying Networks</title>
<link>https://arxiv.org/abs/2511.12836</link>
<guid>https://arxiv.org/abs/2511.12836</guid>
<content:encoded><![CDATA[
arXiv:2511.12836v1 Announce Type: cross 
Abstract: Sampling from a target distribution induced by training data is central to Bayesian learning, with Stochastic Gradient Langevin Dynamics (SGLD) serving as a key tool for scalable posterior sampling and decentralized variants enabling learning when data are distributed across a network of agents. This paper introduces DIGing-SGLD, a decentralized SGLD algorithm designed for scalable Bayesian learning in multi-agent systems operating over time-varying networks. Existing decentralized SGLD methods are restricted to static network topologies, and many exhibit steady-state sampling bias caused by network effects, even when full batches are used. DIGing-SGLD overcomes these limitations by integrating Langevin-based sampling with the gradient-tracking mechanism of the DIGing algorithm, originally developed for decentralized optimization over time-varying networks, thereby enabling efficient and bias-free sampling without a central coordinator. To our knowledge, we provide the first finite-time non-asymptotic Wasserstein convergence guarantees for decentralized SGLD-based sampling over time-varying networks, with explicit constants. Under standard strong convexity and smoothness assumptions, DIGing-SGLD achieves geometric convergence to an $O(\sqrt{\eta})$ neighborhood of the target distribution, where $\eta$ is the stepsize, with dependence on the target accuracy matching the best-known rates for centralized and static-network SGLD algorithms using constant stepsize. Numerical experiments on Bayesian linear and logistic regression validate the theoretical results and demonstrate the strong empirical performance of DIGing-SGLD under dynamically evolving network conditions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mysticeti: Reaching the Limits of Latency with Uncertified DAGs</title>
<link>https://arxiv.org/abs/2310.14821</link>
<guid>https://arxiv.org/abs/2310.14821</guid>
<content:encoded><![CDATA[
arXiv:2310.14821v5 Announce Type: replace 
Abstract: We introduce Mysticeti-C, the first DAG-based Byzantine consensus protocol to achieve the lower bounds of latency of 3 message rounds. Since Mysticeti-C is built over DAGs it also achieves high resource efficiency and censorship resistance. Mysticeti-C achieves this latency improvement by avoiding explicit certification of the DAG blocks and by proposing a novel commit rule such that every block can be committed without delays, resulting in optimal latency in the steady state and under crash failures. We further extend Mysticeti-C to Mysticeti-FPC, which incorporates a fast commit path that achieves even lower latency for transferring assets. Unlike prior fast commit path protocols, Mysticeti-FPC minimizes the number of signatures and messages by weaving the fast path transactions into the DAG. This frees up resources, which subsequently result in better performance. We prove the safety and liveness in a Byzantine context. We evaluate both Mysticeti protocols and compare them with state-of-the-art consensus and fast path protocols to demonstrate their low latency and resource efficiency, as well as their more graceful degradation under crash failures. Mysticeti-C is the first Byzantine consensus protocol to achieve WAN latency of 0.5s for consensus commit while simultaneously maintaining state-of-the-art throughput of over 200k TPS. Finally, we report on integrating Mysticeti-C as the consensus protocol into the Sui blockchain, resulting in over 4x latency reduction.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bounds of Block Rewards in Honest PinFi Systems</title>
<link>https://arxiv.org/abs/2404.02174</link>
<guid>https://arxiv.org/abs/2404.02174</guid>
<content:encoded><![CDATA[
arXiv:2404.02174v2 Announce Type: replace 
Abstract: PinFi is a class of novel protocols for decentralized pricing of dissipative assets, whose value naturally declines over time. Central to the protocol's functionality and its market efficiency is the role of liquidity providers (LPs). This study addresses critical stability and sustainability challenges within the protocol, namely: the propensity of LPs to prefer selling in external markets over participation in the protocol; a similar inclination towards selling within the PinFi system rather than contributing as LPs; and a scenario where LPs are disinclined to sell within the protocol. Employing a game-theoretic approach, we explore PinFi's mechanisms and its broader ramifications. Our findings reveal that, under a variety of common conditions and with an assumption of participant integrity, PinFi is capable of fostering a dynamic equilibrium among LPs, sellers, and buyers. This balance is maintained through a carefully calibrated range of block rewards for LPs, ensuring the protocol's long-term stability and utility.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LooPIN: A PinFi protocol for decentralized computing</title>
<link>https://arxiv.org/abs/2406.09422</link>
<guid>https://arxiv.org/abs/2406.09422</guid>
<content:encoded><![CDATA[
arXiv:2406.09422v2 Announce Type: replace 
Abstract: Networked computing power is a critical utility in the era of artificial intelligence. This paper presents a novel Physical Infrastructure Finance (PinFi) protocol designed to facilitate the distribution of computing power within networks in a decentralized manner. Addressing the core challenges of coordination, pricing, and liquidity in decentralized physical infrastructure networks (DePIN), the PinFi protocol introduces a distinctive dynamic pricing mechanism. It enables providers to allocate excess computing resources to a "dissipative" PinFi liquidity pool, distinct from traditional DeFi liquidity pools, ensuring seamless access for clients at equitable, market-based prices. This approach significantly reduces the costs of accessing computing power, potentially to as low as 1% compared to existing services, while simultaneously enhancing security and dependability. The PinFi protocol is poised to transform the dynamics of supply and demand in computing power networks, setting a new standard for efficiency and accessibility.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Satellite Swarm Deployment via Distance-based Orbital Transition Under $J_2$ Perturbation</title>
<link>https://arxiv.org/abs/2507.01769</link>
<guid>https://arxiv.org/abs/2507.01769</guid>
<content:encoded><![CDATA[
arXiv:2507.01769v2 Announce Type: replace 
Abstract: This paper presents an autonomous guidance and control strategy for a satellite swarm that enables scalable distributed space structures for innovative science and business opportunities. The averaged $J_2$ orbital parameters that describe the drift and periodic orbital motion were derived along with their target values to achieve a distributed space structure in a decentralized manner. This enabled the design of a distance-based orbital stabilizer to ensure autonomous deployment into a monolithic formation of a coplanar equidistant configuration on a user-defined orbital plane. Continuous formation control was assumed to be achieved through fuel-free actuation, such as satellite magnetic field interaction and differential aerodynamic forces, thereby maintaining long-term formation stability without thruster usage. A major challenge for such actuation systems is the potential loss of control capability due to increasing inter-satellite distances resulting from unstable orbital dynamics, particularly for autonomous satellite swarms. To mitigate this risk, our decentralized deployment controller minimized drift distance during unexpected communication outages. As a case study, we consider the deployment of palm-sized satellites into a coplanar equidistant formation in a $J_2$-perturbed orbit. Moreover, centralized grouping strategies are presented.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Linearly Homomorphic Ring Signature Scheme over Lattices</title>
<link>https://arxiv.org/abs/2507.02281</link>
<guid>https://arxiv.org/abs/2507.02281</guid>
<content:encoded><![CDATA[
arXiv:2507.02281v3 Announce Type: replace 
Abstract: Homomorphic ring signature schemes combine the strong anonymity of ring signatures with the computability of homomorphic signatures, demonstrating significant potential in scenarios requiring both anonymous data provenance and verifiable homomorphic computation (e.g., confidential blockchain transactions and secure multi-party computation). However, no feasible homomorphic ring signature scheme currently exists.
  In this work, we propose the first lattice-based linearly homomorphic ring signature scheme. Proven secure in the standard model under the small integer solution (SIS) assumption, our scheme achieves strong anonymity under full key exposure and unforgeability against insider corruption attacks. As the first unified framework for ring signatures and linear homomorphic signatures, this construction provides a post-quantum-secure solution for the aforementioned applications, advancing the development of privacy-enhanced homomorphic computation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Lightweight Approach for State Machine Replication</title>
<link>https://arxiv.org/abs/2509.17771</link>
<guid>https://arxiv.org/abs/2509.17771</guid>
<content:encoded><![CDATA[
arXiv:2509.17771v2 Announce Type: replace 
Abstract: We present a lightweight solution for state machine replication with commitment certificates. Specifically, we adapt and analyze a median rule for the stabilizing consensus problem [Doerr11] to operate in a client-server setting where arbitrary servers may be blocked adaptively based on past system information. We further extend our protocol by compressing information about committed commands, thus keeping the protocol lightweight, while still enabling clients to easily prove that their commands have indeed been committed on the shared state. Our approach guarantees liveness as long as at most a constant fraction of servers are blocked, ensures safety under any number of blocked servers, and supports fast recovery even after all servers are blocked. In addition to offering near-optimal asymptotic performance in several respects, our method is fully decentralized, unlike other near-optimal solutions that rely on leaders. In particular, our solution is robust against adversaries that target key servers (which captures insider-based denial-of-service attacks), whereas leader-based approaches fail under such a blocking model.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupling Bias, Aligning Distributions: Synergistic Fairness Optimization for Deepfake Detection</title>
<link>https://arxiv.org/abs/2511.10150</link>
<guid>https://arxiv.org/abs/2511.10150</guid>
<content:encoded><![CDATA[
arXiv:2511.10150v2 Announce Type: replace 
Abstract: Fairness is a core element in the trustworthy deployment of deepfake detection models, especially in the field of digital identity security. Biases in detection models toward different demographic groups, such as gender and race, may lead to systemic misjudgments, exacerbating the digital divide and social inequities. However, current fairness-enhanced detectors often improve fairness at the cost of detection accuracy. To address this challenge, we propose a dual-mechanism collaborative optimization framework. Our proposed method innovatively integrates structural fairness decoupling and global distribution alignment: decoupling channels sensitive to demographic groups at the model architectural level, and subsequently reducing the distance between the overall sample distribution and the distributions corresponding to each demographic group at the feature level. Experimental results demonstrate that, compared with other methods, our framework improves both inter-group and intra-group fairness while maintaining overall detection accuracy across domains.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Price Competition under Generalized Linear Demands</title>
<link>https://arxiv.org/abs/2511.10718</link>
<guid>https://arxiv.org/abs/2511.10718</guid>
<content:encoded><![CDATA[
arXiv:2511.10718v1 Announce Type: new 
Abstract: We study sequential price competition among $N$ sellers, each influenced by the pricing decisions of their rivals. Specifically, the demand function for each seller $i$ follows the single index model $\lambda_i(\mathbf{p}) = \mu_i(\langle \boldsymbol{\theta}_{i,0}, \mathbf{p} \rangle)$, with known increasing link $\mu_i$ and unknown parameter $\boldsymbol{\theta}_{i,0}$, where the vector $\mathbf{p}$ denotes the vector of prices offered by all the sellers simultaneously at a given instant. Each seller observes only their own realized demand -- unobservable to competitors -- and the prices set by rivals. Our framework generalizes existing approaches that focus solely on linear demand models. We propose a novel decentralized policy, PML-GLUCB, that combines penalized MLE with an upper-confidence pricing rule, removing the need for coordinated exploration phases across sellers -- which is integral to previous linear models -- and accommodating both binary and real-valued demand observations. Relative to a dynamic benchmark policy, each seller achieves $O(N^{2}\sqrt{T}\log(T))$ regret, which essentially matches the optimal rate known in the linear setting. A significant technical contribution of our work is the development of a variant of the elliptical potential lemma -- typically applied in single-agent systems -- adapted to our competitive multi-agent environment.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Welfare in Noncooperative Network Formation under Attack</title>
<link>https://arxiv.org/abs/2511.10845</link>
<guid>https://arxiv.org/abs/2511.10845</guid>
<content:encoded><![CDATA[
arXiv:2511.10845v1 Announce Type: new 
Abstract: Communication networks are essential for our economy and our everyday lives. This makes them lucrative targets for attacks. Today, we see an ongoing battle between criminals that try to disrupt our key communication networks and security professionals that try to mitigate these attacks. However, today's networks, like the Internet or peer-to-peer networks among smart devices, are not controlled by a single authority, but instead consist of many independently administrated entities that are interconnected. Thus, both the decisions of how to interconnect and how to secure against potential attacks are taken in a decentralized way by selfish agents.
  This strategic setting, with agents that want to interconnect and potential attackers that want to disrupt the network, was captured via an influential game-theoretic model by Goyal, Jabbari, Kearns, Khanna, and Morgenstern (WINE 2016). We revisit this model and show improved tight bounds on the achieved robustness of networks created by selfish agents. As our main result, we show that such networks can resist attacks of a large class of potential attackers, i.e., these networks maintain asymptotically optimal welfare post attack. This improves several bounds and resolves an open problem. Along the way, we show the counter-intuitive result, that attackers that aim at minimizing the social welfare post attack do not actually inflict the greatest possible damage.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Swarm Control via SO(3) Embeddings for 3D Trajectories</title>
<link>https://arxiv.org/abs/2511.10858</link>
<guid>https://arxiv.org/abs/2511.10858</guid>
<content:encoded><![CDATA[
arXiv:2511.10858v1 Announce Type: new 
Abstract: This paper presents a novel decentralized approach for achieving emergent behavior in multi-agent systems with minimal information sharing. Based on prior work in simple orbits, our method produces a broad class of stable, periodic trajectories by stabilizing the system around a Lie group-based geometric embedding. Employing the Lie group SO(3), we generate a wider range of periodic curves than existing quaternion-based methods. Furthermore, we exploit SO(3) properties to eliminate the need for velocity inputs, allowing agents to receive only position inputs. We also propose a novel phase controller that ensures uniform agent separation, along with a formal stability proof. Validation through simulations and experiments showcases the method's adaptability to complex low-level dynamics and disturbances.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Federated Clustering: A Client-wise Private Graph Aggregation Framework</title>
<link>https://arxiv.org/abs/2511.10915</link>
<guid>https://arxiv.org/abs/2511.10915</guid>
<content:encoded><![CDATA[
arXiv:2511.10915v1 Announce Type: new 
Abstract: Federated clustering addresses the critical challenge of extracting patterns from decentralized, unlabeled data. However, it is hampered by the flaw that current approaches are forced to accept a compromise between performance and privacy: \textit{transmitting embedding representations risks sensitive data leakage, while sharing only abstract cluster prototypes leads to diminished model accuracy}. To resolve this dilemma, we propose Structural Privacy-Preserving Federated Graph Clustering (SPP-FGC), a novel algorithm that innovatively leverages local structural graphs as the primary medium for privacy-preserving knowledge sharing, thus moving beyond the limitations of conventional techniques. Our framework operates on a clear client-server logic; on the client-side, each participant constructs a private structural graph that captures intrinsic data relationships, which the server then securely aggregates and aligns to form a comprehensive global graph from which a unified clustering structure is derived. The framework offers two distinct modes to suit different needs. SPP-FGC is designed as an efficient one-shot method that completes its task in a single communication round, ideal for rapid analysis. For more complex, unstructured data like images, SPP-FGC+ employs an iterative process where clients and the server collaboratively refine feature representations to achieve superior downstream performance. Extensive experiments demonstrate that our framework achieves state-of-the-art performance, improving clustering accuracy by up to 10\% (NMI) over federated baselines while maintaining provable privacy guarantees.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exposing Weak Links in Multi-Agent Systems under Adversarial Prompting</title>
<link>https://arxiv.org/abs/2511.10949</link>
<guid>https://arxiv.org/abs/2511.10949</guid>
<content:encoded><![CDATA[
arXiv:2511.10949v1 Announce Type: new 
Abstract: LLM-based agents are increasingly deployed in multi-agent systems (MAS). As these systems move toward real-world applications, their security becomes paramount. Existing research largely evaluates single-agent security, leaving a critical gap in understanding the vulnerabilities introduced by multi-agent design. However, existing systems fall short due to lack of unified frameworks and metrics focusing on unique rejection modes in MAS. We present SafeAgents, a unified and extensible framework for fine-grained security assessment of MAS. SafeAgents systematically exposes how design choices such as plan construction strategies, inter-agent context sharing, and fallback behaviors affect susceptibility to adversarial prompting. We introduce Dharma, a diagnostic measure that helps identify weak links within multi-agent pipelines. Using SafeAgents, we conduct a comprehensive study across five widely adopted multi-agent architectures (centralized, decentralized, and hybrid variants) on four datasets spanning web tasks, tool use, and code generation. Our findings reveal that common design patterns carry significant vulnerabilities. For example, centralized systems that delegate only atomic instructions to sub-agents obscure harmful objectives, reducing robustness. Our results highlight the need for security-aware design in MAS. Link to code is https://github.com/microsoft/SafeAgents
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structural asymmetry as a fraud signature: detecting collusion with Heron's Information Coefficient</title>
<link>https://arxiv.org/abs/2511.10957</link>
<guid>https://arxiv.org/abs/2511.10957</guid>
<content:encoded><![CDATA[
arXiv:2511.10957v1 Announce Type: new 
Abstract: Fraud in public procurement remains a persistent challenge, especially in large, decentralized systems like Brazil's Unified Health System. We introduce Heron's Information Coefficient (HIC), a geometric measure that quantifies how subgraphs deviate from the global structure of a network. Applied to over eight years of Brazilian bidding data for medical supplies, this measure highlights collusive patterns that standard indicators may overlook. Unlike conventional robustness metrics, the Heron coefficient focuses on the interaction between active and inactive subgraphs, revealing structural shifts that may signal coordinated behavior, such as cartel formation. Synthetic experiments support these findings, demonstrating strong detection performance across varying corruption intensities and network sizes. While our results do not replace legal or economic analyses, they offer an effective complementary tool for auditors and policymakers to monitor procurement integrity more effectively. This study demonstrates that simple geometric insight can reveal hidden dynamics in real-world networks better than other Information Theoretic metrics.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When to Stop Federated Learning: Zero-Shot Generation of Synthetic Validation Data with Generative AI for Early Stopping</title>
<link>https://arxiv.org/abs/2511.11208</link>
<guid>https://arxiv.org/abs/2511.11208</guid>
<content:encoded><![CDATA[
arXiv:2511.11208v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative model training across decentralized devices while preserving data privacy. However, FL methods typically run for a predefined number of global rounds, often leading to unnecessary computation when optimal performance is reached earlier. In addition, training may continue even when the model fails to achieve meaningful performance. To address this inefficiency, we introduce a zero-shot synthetic validation framework that leverages generative AI to monitor model performance and determine early stopping points. Our approach adaptively stops training near the optimal round, thereby conserving computational resources and enabling rapid hyperparameter adjustments. Numerical results on multi-label chest X-ray classification demonstrate that our method reduces training rounds by up to 74% while maintaining accuracy within 1% of the optimal.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Local and Federated Data Normalization in Federated Learning: A Privacy-Preserving Approach</title>
<link>https://arxiv.org/abs/2511.11249</link>
<guid>https://arxiv.org/abs/2511.11249</guid>
<content:encoded><![CDATA[
arXiv:2511.11249v1 Announce Type: new 
Abstract: Data normalization is a crucial preprocessing step for enhancing model performance and training stability. In federated learning (FL), where data remains distributed across multiple parties during collaborative model training, normalization presents unique challenges due to the decentralized and often heterogeneous nature of the data. Traditional methods rely on either independent client-side processing, i.e., local normalization, or normalizing the entire dataset before distributing it to parties, i.e., pooled normalization. Local normalization can be problematic when data distributions across parties are non-IID, while the pooled normalization approach conflicts with the decentralized nature of FL. In this paper, we explore the adaptation of widely used normalization techniques to FL and define the term federated normalization. Federated normalization simulates pooled normalization by enabling the collaborative exchange of normalization parameters among parties. Thus, it achieves performance on par with pooled normalization without compromising data locality. However, sharing normalization parameters such as the mean introduces potential privacy risks, which we further mitigate through a robust privacy-preserving solution. Our contributions include: (i) We systematically evaluate the impact of various federated and local normalization techniques in heterogeneous FL scenarios, (ii) We propose a novel homomorphically encrypted $k$-th ranked element (and median) calculation tailored for the federated setting, enabling secure and efficient federated normalization, (iii) We propose privacy-preserving implementations of widely used normalization techniques for FL, leveraging multiparty fully homomorphic encryption (MHE).
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt Engineering vs. Fine-Tuning for LLM-Based Vulnerability Detection in Solana and Algorand Smart Contracts</title>
<link>https://arxiv.org/abs/2511.11250</link>
<guid>https://arxiv.org/abs/2511.11250</guid>
<content:encoded><![CDATA[
arXiv:2511.11250v1 Announce Type: new 
Abstract: Smart contracts have emerged as key components within decentralized environments, enabling the automation of transactions through self-executing programs. While these innovations offer significant advantages, they also present potential drawbacks if the smart contract code is not carefully designed and implemented. This paper investigates the capability of large language models (LLMs) to detect OWASP-inspired vulnerabilities in smart contracts beyond the Ethereum Virtual Machine (EVM) ecosystem, focusing specifically on Solana and Algorand. Given the lack of labeled datasets for non-EVM platforms, we design a synthetic dataset of annotated smart contract snippets in Rust (for Solana) and PyTeal (for Algorand), structured around a vulnerability taxonomy derived from OWASP. We evaluate LLMs under three configurations: prompt engineering, fine-tuning, and a hybrid of both, comparing their performance on different vulnerability categories. Experimental results show that prompt engineering achieves general robustness, while fine-tuning improves precision and recall on less semantically rich languages such as TEAL. Additionally, we analyze how the architectural differences of Solana and Algorand influence the manifestation and detectability of vulnerabilities, offering platform-specific mappings that highlight limitations in existing security tooling. Our findings suggest that LLM-based approaches are viable for static vulnerability detection in smart contracts, provided domain-specific data and categorization are integrated into training pipelines.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Convergence Analysis for Semi-Decentralized Learning: Sampled-to-Sampled vs. Sampled-to-All Communication</title>
<link>https://arxiv.org/abs/2511.11560</link>
<guid>https://arxiv.org/abs/2511.11560</guid>
<content:encoded><![CDATA[
arXiv:2511.11560v1 Announce Type: new 
Abstract: In semi-decentralized federated learning, devices primarily rely on device-to-device communication but occasionally interact with a central server. Periodically, a sampled subset of devices uploads their local models to the server, which computes an aggregate model. The server can then either (i) share this aggregate model only with the sampled clients (sampled-to-sampled, S2S) or (ii) broadcast it to all clients (sampled-to-all, S2A). Despite their practical significance, a rigorous theoretical and empirical comparison of these two strategies remains absent. We address this gap by analyzing S2S and S2A within a unified convergence framework that accounts for key system parameters: sampling rate, server aggregation frequency, and network connectivity. Our results, both analytical and experimental, reveal distinct regimes where one strategy outperforms the other, depending primarily on the degree of data heterogeneity across devices. These insights lead to concrete design guidelines for practical semi-decentralized FL deployments.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement</title>
<link>https://arxiv.org/abs/2505.12684</link>
<guid>https://arxiv.org/abs/2505.12684</guid>
<content:encoded><![CDATA[
arXiv:2505.12684v2 Announce Type: replace 
Abstract: Recent advances in graph machine learning have shifted to data-centric paradigms, driven by two emerging fields: (1) Federated graph learning (FGL) enables multi-client collaboration but faces challenges from data and task heterogeneity, limiting its practicality; (2) Graph foundation models (GFM) offer strong domain generalization but are usually trained on single machines, missing out on cross-silo data and resources.
  These paradigms are complementary, and their integration brings notable benefits. Motivated by this, we propose FedGFM, a novel decentralized GFM training paradigm. However, a key challenge is knowledge entanglement, where multi-domain knowledge merges into indistinguishable representations, hindering downstream adaptation.
  To address this, we present FedGFM+, an enhanced framework with two core modules to reduce knowledge entanglement: (1) AncDAI: A global anchor-based domain-aware initialization strategy. Before pre-training, each client encodes its local graph into domain-specific prototypes that serve as semantic anchors. Synthetic embeddings around these anchors initialize the global model. We theoretically prove these prototypes are distinguishable across domains, providing a strong inductive bias to disentangle domain-specific knowledge. (2) AdaDPP: A local adaptive domain-sensitive prompt pool. Each client learns a lightweight graph prompt capturing domain semantics during pre-training. During fine-tuning, prompts from all clients form a pool from which the GFM selects relevant prompts to augment target graph attributes, improving downstream adaptation.
  FedGFM+ is evaluated on 8 diverse benchmarks across multiple domains and tasks, outperforming 20 baselines from supervised learning, FGL, and federated GFM variants.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ADMM Penalty Parameter Evaluation for Networked Microgrid Energy Management</title>
<link>https://arxiv.org/abs/2511.08750</link>
<guid>https://arxiv.org/abs/2511.08750</guid>
<content:encoded><![CDATA[
arXiv:2511.08750v1 Announce Type: new 
Abstract: The alternating direction method of multipliers (ADMM) is a powerful algorithm for solving decentralized optimization problems including networked microgrid energy management (NetMEM). However, its performance is highly sensitive to the selection of its penalty parameter \r{ho}, which can lead to slow convergence, suboptimal solutions, or even algorithm divergence. This paper evaluates and compares three district ADMM formulations to solve the NetMEM problem, which explore different methods to determine appropriate stopping points, aiming to yield high-quality solutions. Furthermore, an adaptive penalty heuristic is also incorporated into each method to analyze its potential impact on ADMM performance. Different case studies on networks of varying sizes demonstrate that an objective-based ADMM approach, denominated as OB-ADMM, is significantly more robust to the choice of \r{ho}, consistently yielding solutions closer to the centralized optimal benchmark by preventing premature algorithm stopping.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Signature, Multiple Payments: Demystifying and Detecting Signature Replay Vulnerabilities in Smart Contracts</title>
<link>https://arxiv.org/abs/2511.09134</link>
<guid>https://arxiv.org/abs/2511.09134</guid>
<content:encoded><![CDATA[
arXiv:2511.09134v1 Announce Type: new 
Abstract: Smart contracts have significantly advanced blockchain technology, and digital signatures are crucial for reliable verification of contract authority. Through signature verification, smart contracts can ensure that signers possess the required permissions, thus enhancing security and scalability. However, lacking checks on signature usage conditions can lead to repeated verifications, increasing the risk of permission abuse and threatening contract assets. We define this issue as the Signature Replay Vulnerability (SRV). In this paper, we conducted the first empirical study to investigate the causes and characteristics of the SRVs. From 1,419 audit reports across 37 blockchain security companies, we identified 108 with detailed SRV descriptions and classified five types of SRVs. To detect these vulnerabilities automatically, we designed LASiR, which utilizes the general semantic understanding ability of Large Language Models (LLMs) to assist in the static taint analysis of the signature state and identify the signature reuse behavior. It also employs path reachability verification via symbolic execution to ensure effective and reliable detection. To evaluate the performance of LASiR, we conducted large-scale experiments on 15,383 contracts involving signature verification, selected from the initial dataset of 918,964 contracts across four blockchains: Ethereum, Binance Smart Chain, Polygon, and Arbitrum. The results indicate that SRVs are widespread, with affected contracts holding $4.76 million in active assets. Among these, 19.63% of contracts that use signatures on Ethereum contain SRVs. Furthermore, manual verification demonstrates that LASiR achieves an F1-score of 87.90% for detection. Ablation studies and comparative experiments reveal that the semantic information provided by LLMs aids static taint analysis, significantly enhancing LASiR's detection performance.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Slaying the Dragon: The Quest for Democracy in Decentralized Autonomous Organizations (DAOs)</title>
<link>https://arxiv.org/abs/2511.09263</link>
<guid>https://arxiv.org/abs/2511.09263</guid>
<content:encoded><![CDATA[
arXiv:2511.09263v1 Announce Type: new 
Abstract: This chapter explores how Decentralized Autonomous Organizations (DAOs), a novel institutional form based on blockchain technology, challenge traditional centralized governance structures. DAOs govern projects ranging from finance to science and digital communities. They aim to redistribute decision-making power through programmable, transparent, and participatory mechanisms. This chapter outlines both the opportunities DAOs present, such as incentive alignment, rapid coordination, and censorship resistance, and the challenges they face, including token concentration, low participation, and the risk of de facto centralization. It further discusses the emerging intersection of DAOs and artificial intelligence, highlighting the potential for increased automation alongside the dangers of diminished human oversight and algorithmic opacity. Ultimately, we discuss under what circumstances DAOs can fulfill their democratic promise or risk replicating the very power asymmetries they seek to overcome.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Estimation and Control for Heterogeneous Multi-agent Systems Based on Decentralized k-hop Prescribed Performance Observers</title>
<link>https://arxiv.org/abs/2511.09269</link>
<guid>https://arxiv.org/abs/2511.09269</guid>
<content:encoded><![CDATA[
arXiv:2511.09269v1 Announce Type: new 
Abstract: We propose decentralized k-hop Prescribed Performance State and Input Observers for heterogeneous multi-agent systems subject to bounded external disturbances. In the proposed input/state observer, each agent estimates the state and input of agents located two or more hops away using only local information exchanged with 1-hop neighbors, while guaranteeing that transient estimation errors satisfy predefined performance bounds. Conditions are established under which the input observer can be omitted, allowing the state observer convergence to be independent of the input estimates. Theoretical analysis demonstrates that if a closed-loop controller with full state knowledge achieves the control objective and the estimation-based closed-loop system is set-Input to State Stable (set-ISS) with respect to the goal set, then the estimated states can be used to achieve the system objective with an arbitrarily small worst-case error governed by the accuracy of the states estimates. Simulation results are provided to validate the proposed approach.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Steering Noncooperative Games Through Conjecture Design</title>
<link>https://arxiv.org/abs/2511.09435</link>
<guid>https://arxiv.org/abs/2511.09435</guid>
<content:encoded><![CDATA[
arXiv:2511.09435v1 Announce Type: new 
Abstract: In dynamic noncooperative games, each player makes conjectures about other players' reactions before choosing a strategy. However, resulting equilibria may be multiple and do not always lead to desirable outcomes. These issues are typically addressed separately, for example, through opponent modelling and incentive design. Drawing inspiration from conjectural variations games, we propose an incentive design framework in which a coordinator first computes an equilibrium by optimizing a predefined objective function, then communicates this equilibrium as a target for the players to reach. In a centralized setting, the coordinator also optimizes the conjectures to steer the players towards the target. In decentralized settings, players independently compute conjectures and update their strategies based on individual targets. We provide a guarantee of equilibrium existence in both cases. This framework uses conjectures not only to guide the system towards desirable outcomes but also to decouple the game into independent optimization problems, enabling efficient computation and parallelization in large-scale settings. We illustrate our theoretical results on classical representative noncooperative games, demonstrating its application potential.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Formal Verification of a Generic Algorithm for TDM Communication Over Inter Satellite Links</title>
<link>https://arxiv.org/abs/2511.09485</link>
<guid>https://arxiv.org/abs/2511.09485</guid>
<content:encoded><![CDATA[
arXiv:2511.09485v1 Announce Type: new 
Abstract: The Python Testbed for Federated Learning Algorithms is a simple FL framework targeting edge systems, which provides the three generic algorithms: the centralized federated learning, the decentralized federated learning, and the universal TDM communication in the current time slot. The first two were formally verified in a previous paper using the CSP process algebra, and in this paper, we use the same approach to formally verify the third one, in two phases. In the first phase, we construct the CSP model as a faithful representation of the real Python code. In the second phase, the model checker PAT automatically proves correctness of the third generic algorithm by proving its deadlock freeness (safety property) and successful termination (liveness property).
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation</title>
<link>https://arxiv.org/abs/2505.03586</link>
<guid>https://arxiv.org/abs/2505.03586</guid>
<content:encoded><![CDATA[
arXiv:2505.03586v4 Announce Type: replace 
Abstract: In real-world multi-agent systems (MASs), observation delays are ubiquitous, preventing agents from making decisions based on the environment's true state. An individual agent's local observation typically comprises multiple components from other agents or dynamic entities within the environment. These discrete observation components with varying delay characteristics pose significant challenges for multi-agent reinforcement learning (MARL). In this paper, we first formulate the decentralized stochastic individual delay partially observable Markov decision process (DSID-POMDP) by extending the standard Dec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL training framework for addressing stochastic individual delays, along with recommended implementations for its constituent modules. We implement the DSID-POMDP's observation generation pattern using standard MARL benchmarks, including MPE and SMAC. Experiments demonstrate that baseline MARL methods suffer severe performance degradation under fixed and unfixed delays. The RDC-enhanced approach mitigates this issue, remarkably achieving ideal delay-free performance in certain delay scenarios while maintaining generalizability. Our work provides a novel perspective on multi-agent delayed observation problems and offers an effective solution framework. The source code is available at https://github.com/linkjoker1006/RDC-pymarl.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agentic Educational Content Generation for African Languages on Edge Devices</title>
<link>https://arxiv.org/abs/2511.07437</link>
<guid>https://arxiv.org/abs/2511.07437</guid>
<content:encoded><![CDATA[
arXiv:2511.07437v1 Announce Type: new 
Abstract: Addressing educational inequity in Sub-Saharan Africa, this research presents an autonomous agent-orchestrated framework for decentralized, culturally adaptive educational content generation on edge devices. The system leverages four specialized agents that work together to generate contextually appropriate educational content. Experimental validation on platforms including Raspberry Pi 4B and NVIDIA Jetson Nano demonstrates significant performance achievements. InkubaLM on Jetson Nano achieved a Time-To-First-Token (TTFT) of 129 ms, an average inter-token latency of 33 ms, and a throughput of 45.2 tokens per second while consuming 8.4 W. On Raspberry Pi 4B, InkubaLM also led with 326 ms TTFT and 15.9 tokens per second at 5.8 W power consumption. The framework consistently delivered high multilingual quality, averaging a BLEU score of 0.688, cultural relevance of 4.4/5, and fluency of 4.2/5 across tested African languages. Through potential partnerships with active community organizations including African Youth & Community Organization (AYCO) and Florida Africa Foundation, this research aims to establish a practical foundation for accessible, localized, and sustainable AI-driven education in resource-constrained environments. Keeping focus on long-term viability and cultural appropriateness, it contributes to United Nations SDGs 4, 9, and 10. Index Terms - Multi-Agent Systems, Edge AI Computing, Educational Technology, African Languages, Rural Education, Sustainable Development, UN SDG.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain</title>
<link>https://arxiv.org/abs/2511.07577</link>
<guid>https://arxiv.org/abs/2511.07577</guid>
<content:encoded><![CDATA[
arXiv:2511.07577v1 Announce Type: new 
Abstract: Existing retrieval-augmented generation (RAG) systems typically use a centralized architecture, causing a high cost of data collection, integration, and management, as well as privacy concerns. There is a great need for a decentralized RAG system that enables foundation models to utilize information directly from data owners who maintain full control over their sources. However, decentralization brings a challenge: the numerous independent data sources vary significantly in reliability, which can diminish retrieval accuracy and response quality. To address this, our decentralized RAG system has a novel reliability scoring mechanism that dynamically evaluates each source based on the quality of responses it contributes to generate and prioritizes high-quality sources during retrieval. To ensure transparency and trust, the scoring process is securely managed through blockchain-based smart contracts, creating verifiable and tamper-proof reliability records without relying on a central authority. We evaluate our decentralized system with two Llama models (3B and 8B) in two simulated environments where six data sources have different levels of reliability. Our system achieves a +10.7\% performance improvement over its centralized counterpart in the real world-like unreliable data environments. Notably, it approaches the upper-bound performance of centralized systems under ideally reliable data environments. The decentralized infrastructure enables secure and trustworthy scoring management, achieving approximately 56\% marginal cost savings through batched update operations. Our code and system are open-sourced at github.com/yining610/Reliable-dRAG.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Virtual Traffic Lights for Multi-Robot Navigation: Decentralized Planning with Centralized Conflict Resolution</title>
<link>https://arxiv.org/abs/2511.07811</link>
<guid>https://arxiv.org/abs/2511.07811</guid>
<content:encoded><![CDATA[
arXiv:2511.07811v1 Announce Type: new 
Abstract: We present a hybrid multi-robot coordination framework that combines decentralized path planning with centralized conflict resolution. In our approach, each robot autonomously plans its path and shares this information with a centralized node. The centralized system detects potential conflicts and allows only one of the conflicting robots to proceed at a time, instructing others to stop outside the conflicting area to avoid deadlocks. Unlike traditional centralized planning methods, our system does not dictate robot paths but instead provides stop commands, functioning as a virtual traffic light. In simulation experiments with multiple robots, our approach increased the success rate of robots reaching their goals while reducing deadlocks. Furthermore, we successfully validated the system in real-world experiments with two quadruped robots and separately with wheeled Duckiebots.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blockchain-Integrated Privacy-Preserving Medical Insurance Claim Processing Using Homomorphic Encryption</title>
<link>https://arxiv.org/abs/2511.07818</link>
<guid>https://arxiv.org/abs/2511.07818</guid>
<content:encoded><![CDATA[
arXiv:2511.07818v1 Announce Type: new 
Abstract: This research proposes a decentralized and cryptographically secure framework to address the most acute issues of privacy, data security, and protection in the ecosystem of medical insurance claim processing. The scope of this study focuses on enabling the management of insurance claims in a transparent, privacy-protecting manner while maintaining the efficiency and trust level needed by the patients, healthcare providers, and insurers. To accomplish this, the proposed system adds blockchain technology to provide an unchangeable, decentralized, and auditable claim transactions ledger which enhances overall claim-related processes and trust among all stakeholders. To protect critical patient information, the framework employs homomorphic encryption a modern form of cryptography to allow authorized insurance providers to perform necessary operations like claim adjudication and reimbursement on encrypted medical records without any decryption during the process. This method significantly reduces the third-party processing privacy risk because patient data can be kept secret even when third-party processing is done. In addition, smart contracts improve automation of the most important procedures in the claim processing pipeline, which decreases manual, operational, and susceptibility towards human blunders or deceitful acts. The integration of these two transformative technologiesblockchain and homomorphic encryption represents the core contribution of this work, enabling the coexistence of transparency and privacy which are usually viewed as competing objectives in traditional systems. As a result, these technologies are expected to foster the creation of a reliable, effective, and privacy safeguarding architecture that could transform the medical claim submission systems paradigm.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated CLIP for Resource-Efficient Heterogeneous Medical Image Classification</title>
<link>https://arxiv.org/abs/2511.07929</link>
<guid>https://arxiv.org/abs/2511.07929</guid>
<content:encoded><![CDATA[
arXiv:2511.07929v1 Announce Type: new 
Abstract: Despite the remarkable performance of deep models in medical imaging, they still require source data for training, which limits their potential in light of privacy concerns. Federated learning (FL), as a decentralized learning framework that trains a shared model with multiple hospitals (a.k.a., FL clients), provides a feasible solution. However, data heterogeneity and resource costs hinder the deployment of FL models, especially when using vision language models (VLM). To address these challenges, we propose a novel contrastive language-image pre-training (CLIP) based FL approach for medical image classification (FedMedCLIP). Specifically, we introduce a masked feature adaptation module (FAM) as a communication module to reduce the communication load while freezing the CLIP encoders to reduce the computational overhead. Furthermore, we propose a masked multi-layer perceptron (MLP) as a private local classifier to adapt to the client tasks. Moreover, we design an adaptive Kullback-Leibler (KL) divergence-based distillation regularization method to enable mutual learning between FAM and MLP. Finally, we incorporate model compression to transmit the FAM parameters while using ensemble predictions for classification. Extensive experiments on four publicly available medical datasets demonstrate that our model provides feasible performance (e.g., 8\% higher compared to second best baseline on ISIC2019) with reasonable resource cost (e.g., 120$\times$ faster than FedAVG).
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Private Chat in a Public Space of Metaverse Systems</title>
<link>https://arxiv.org/abs/2511.07993</link>
<guid>https://arxiv.org/abs/2511.07993</guid>
<content:encoded><![CDATA[
arXiv:2511.07993v1 Announce Type: new 
Abstract: With the proliferation of Virtual Reality (VR) technologies and the emergence of the Metaverse, social VR applications have become increasingly prevalent and accessible to the general user base. Serving as a novel form of social media, these platforms give users a unique opportunity to engage in social activities. However, there remains a significant limitation: the inability to engage in private conversations within public social VR environments. Current interactions are predominantly public, making it challenging for users to have confidential side discussions or whispers without disrupting ongoing conversations. To address this gap, we developed Hushhub, a private chat system integrated into the popular social VR platform VRChat. Our system enables users within a shared VR space to initiate private audio conversations selectively, allowing them to maintain awareness and engagement with the broader group discussions. To evaluate the system, we conducted user studies to gather insight and feedback on the efficacy and user experience of the implemented system. The results demonstrate the value and necessity of enabling private conversations within immersive social VR environments, paving the way for richer, more nuanced social interactions.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AVOID-JACK: Avoidance of Jackknifing for Swarms of Long Heavy Articulated Vehicles</title>
<link>https://arxiv.org/abs/2511.08016</link>
<guid>https://arxiv.org/abs/2511.08016</guid>
<content:encoded><![CDATA[
arXiv:2511.08016v1 Announce Type: new 
Abstract: This paper presents a novel approach to avoiding jackknifing and mutual collisions in Heavy Articulated Vehicles (HAVs) by leveraging decentralized swarm intelligence. In contrast to typical swarm robotics research, our robots are elongated and exhibit complex kinematics, introducing unique challenges. Despite its relevance to real-world applications such as logistics automation, remote mining, airport baggage transport, and agricultural operations, this problem has not been addressed in the existing literature.
  To tackle this new class of swarm robotics problems, we propose a purely reaction-based, decentralized swarm intelligence strategy tailored to automate elongated, articulated vehicles. The method presented in this paper prioritizes jackknifing avoidance and establishes a foundation for mutual collision avoidance. We validate our approach through extensive simulation experiments and provide a comprehensive analysis of its performance. For the experiments with a single HAV, we observe that for 99.8% jackknifing was successfully avoided and that 86.7% and 83.4% reach their first and second goals, respectively. With two HAVs interacting, we observe 98.9%, 79.4%, and 65.1%, respectively, while 99.7% of the HAVs do not experience mutual collisions.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generic Algorithm for Universal TDM Communication Over Inter Satellite Links</title>
<link>https://arxiv.org/abs/2511.08034</link>
<guid>https://arxiv.org/abs/2511.08034</guid>
<content:encoded><![CDATA[
arXiv:2511.08034v1 Announce Type: new 
Abstract: The original Python Testbed for Federated Learning Algorithms is a light FL framework, which provides the three generic algorithms: the centralized federated learning, the decentralized federated learning, and the TDM communication (i.e., peer data exchange) in the current time slot. The limitation of the latter is that it allows communication only between pairs of network nodes. This paper presents the new generic algorithm for the universal TDM communication that overcomes this limitation, such that a node can communicate with an arbitrary number of peers (assuming the peers also want to communicate with it). The paper covers: (i) the algorithm's theoretical foundation, (ii) the system design, and (iii) the system validation. The main advantage of the new algorithm is that it supports real-world TDM communications over inter satellite links.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributed Zero-Shot Learning for Visual Recognition</title>
<link>https://arxiv.org/abs/2511.08170</link>
<guid>https://arxiv.org/abs/2511.08170</guid>
<content:encoded><![CDATA[
arXiv:2511.08170v1 Announce Type: new 
Abstract: In this paper, we propose a Distributed Zero-Shot Learning (DistZSL) framework that can fully exploit decentralized data to learn an effective model for unseen classes. Considering the data heterogeneity issues across distributed nodes, we introduce two key components to ensure the effective learning of DistZSL: a cross-node attribute regularizer and a global attribute-to-visual consensus. Our proposed cross-node attribute regularizer enforces the distances between attribute features to be similar across different nodes. In this manner, the overall attribute feature space would be stable during learning, and thus facilitate the establishment of visual-to-attribute(V2A) relationships. Then, we introduce the global attribute-tovisual consensus to mitigate biased V2A mappings learned from individual nodes. Specifically, we enforce the bilateral mapping between the attribute and visual feature distributions to be consistent across different nodes. Thus, the learned consistent V2A mapping can significantly enhance zero-shot learning across different nodes. Extensive experiments demonstrate that DistZSL achieves superior performance to the state-of-the-art in learning from distributed data.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SRE-Llama -- Fine-Tuned Meta's Llama LLM, Federated Learning, Blockchain and NFT Enabled Site Reliability Engineering(SRE) Platform for Communication and Networking Software Services</title>
<link>https://arxiv.org/abs/2511.08282</link>
<guid>https://arxiv.org/abs/2511.08282</guid>
<content:encoded><![CDATA[
arXiv:2511.08282v1 Announce Type: new 
Abstract: Software services are crucial for reliable communication and networking; therefore, Site Reliability Engineering (SRE) is important to ensure these systems stay reliable and perform well in cloud-native environments. SRE leverages tools like Prometheus and Grafana to monitor system metrics, defining critical Service Level Indicators (SLIs) and Service Level Objectives (SLOs) for maintaining high service standards. However, a significant challenge arises as many developers often lack in-depth understanding of these tools and the intricacies involved in defining appropriate SLIs and SLOs. To bridge this gap, we propose a novel SRE platform, called SRE-Llama, enhanced by Generative-AI, Federated Learning, Blockchain, and Non-Fungible Tokens (NFTs). This platform aims to automate and simplify the process of monitoring, SLI/SLO generation, and alert management, offering ease in accessibility and efficy for developers. The system operates by capturing metrics from cloud-native services and storing them in a time-series database, like Prometheus and Mimir. Utilizing this stored data, our platform employs Federated Learning models to identify the most relevant and impactful SLI metrics for different services and SLOs, addressing concerns around data privacy. Subsequently, fine-tuned Meta's Llama-3 LLM is adopted to intelligently generate SLIs, SLOs, error budgets, and associated alerting mechanisms based on these identified SLI metrics. A unique aspect of our platform is the encoding of generated SLIs and SLOs as NFT objects, which are then stored on a Blockchain. This feature provides immutable record-keeping and facilitates easy verification and auditing of the SRE metrics and objectives. The automation of the proposed platform is governed by the blockchain smart contracts. The proposed SRE-Llama platform prototype has been implemented with a use case featuring a customized Open5GS 5G Core.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blockly2Hooks: Smart Contracts for Everyone with the XRP Ledger and Google Blockly</title>
<link>https://arxiv.org/abs/2511.08403</link>
<guid>https://arxiv.org/abs/2511.08403</guid>
<content:encoded><![CDATA[
arXiv:2511.08403v1 Announce Type: new 
Abstract: Recent technologies such as inter-ledger payments, non-fungible tokens, and smart contracts are all fruited from the ongoing development of Distributed Ledger Technologies. The foreseen trend is that they will play an increasingly visible role in daily life, which will have to be backed by appropriate operational resources. For example, due to increasing demand, smart contracts could soon face a shortage of knowledgeable users and tools to handle them in practice. Widespread smart contract adoption is currently limited by security, usability and costs aspects. Because of a steep learning curve, the handling of smart contracts is currently performed by specialised developers mainly, and most of the research effort is focusing on smart contract security, while other aspects like usability being somewhat neglected. Specific tools would lower the entry barrier, enabling interested non-experts to create smart contracts.
  In this paper we designed, developed and tested Blockly2Hooks, a solution towards filling this gap even in challenging scenarios such as when the smart contracts are written in an advanced language like C. With the XRP Ledger as a concrete working case, Blockly2Hooks helps interested non-experts from the community to learn smart contracts easily and adopt the technology, through leveraging well-proven teaching methodologies like Visual Programming Languages, and more specifically, the Blockly Visual Programming library from Google. The platform was developed and tested and the results are promising to make learning smart contract development smoother.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying the Climate Risk of Generative AI: Region-Aware Carbon Accounting with G-TRACE and the AI Sustainability Pyramid</title>
<link>https://arxiv.org/abs/2511.04776</link>
<guid>https://arxiv.org/abs/2511.04776</guid>
<content:encoded><![CDATA[
arXiv:2511.04776v1 Announce Type: new 
Abstract: Generative Artificial Intelligence (GenAI) represents a rapidly expanding digital infrastructure whose energy demand and associated CO2 emissions are emerging as a new category of climate risk. This study introduces G-TRACE (GenAI Transformative Carbon Estimator), a cross-modal, region-aware framework that quantifies training- and inference-related emissions across modalities and deployment geographies. Using real-world analytics and microscopic simulation, G-TRACE measures energy use and carbon intensity per output type (text, image, video) and reveals how decentralized inference amplifies small per-query energy costs into system-level impacts. Through the Ghibli-style image generation trend (2024-2025), we estimate 4,309 MWh of energy consumption and 2,068 tCO2 emissions, illustrating how viral participation inflates individual digital actions into tonne-scale consequences. Building on these findings, we propose the AI Sustainability Pyramid, a seven-level governance model linking carbon accounting metrics (L1-L7) with operational readiness, optimization, and stewardship. This framework translates quantitative emission metrics into actionable policy guidance for sustainable AI deployment. The study contributes to the quantitative assessment of emerging digital infrastructures as a novel category of climate risk, supporting adaptive governance for sustainable technology deployment. By situating GenAI within climate-risk frameworks, the work advances data-driven methods for aligning technological innovation with global decarbonization and resilience objectives.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-agent Coordination via Flow Matching</title>
<link>https://arxiv.org/abs/2511.05005</link>
<guid>https://arxiv.org/abs/2511.05005</guid>
<content:encoded><![CDATA[
arXiv:2511.05005v1 Announce Type: new 
Abstract: This work presents MAC-Flow, a simple yet expressive framework for multi-agent coordination. We argue that requirements of effective coordination are twofold: (i) a rich representation of the diverse joint behaviors present in offline data and (ii) the ability to act efficiently in real time. However, prior approaches often sacrifice one for the other, i.e., denoising diffusion-based solutions capture complex coordination but are computationally slow, while Gaussian policy-based solutions are fast but brittle in handling multi-agent interaction. MAC-Flow addresses this trade-off by first learning a flow-based representation of joint behaviors, and then distilling it into decentralized one-step policies that preserve coordination while enabling fast execution. Across four different benchmarks, including $12$ environments and $34$ datasets, MAC-Flow alleviates the trade-off between performance and computational cost, specifically achieving about $\boldsymbol{\times14.5}$ faster inference compared to diffusion-based MARL methods, while maintaining good performance. At the same time, its inference speed is similar to that of prior Gaussian policy-based offline multi-agent reinforcement learning (MARL) methods.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tunable Passivity Control for Centralized Multiport Networked Systems</title>
<link>https://arxiv.org/abs/2511.05026</link>
<guid>https://arxiv.org/abs/2511.05026</guid>
<content:encoded><![CDATA[
arXiv:2511.05026v1 Announce Type: new 
Abstract: Centralized Multiport Networked Dynamic (CMND) systems have emerged as a key architecture with applications in several complex network systems, such as multilateral telerobotics and multi-agent control. These systems consist of a hub node/subsystem connecting with multiple remote nodes/subsystems via a networked architecture. One challenge for this system is stability, which can be affected by non-ideal network artifacts. Conventional passivity-based approaches can stabilize the system under specialized applications like small-scale networked systems. However, those conventional passive stabilizers have several restrictions, such as distributing compensation across subsystems in a decentralized manner, limiting flexibility, and, at the same time, relying on the restrictive assumptions of node passivity. This paper synthesizes a centralized optimal passivity-based stabilization framework for CMND systems. It consists of a centralized passivity observer monitoring overall energy flow and an optimal passivity controller that distributes the just-needed dissipation among various nodes, guaranteeing strict passivity and, thus, L2 stability. The proposed data-driven model-free approach, i.e., Tunable Centralized Optimal Passivity Control (TCoPC), optimizes total performance based on the prescribed dissipation distribution strategy while ensuring stability. The controller can put high dissipation loads on some sub-networks while relaxing the dissipation on other nodes. Simulation results demonstrate the proposed frameworks performance in a complex task under different time-varying delay scenarios while relaxing the remote nodes minimum phase and passivity assumption, enhancing the scalability and generalizability.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OvA-LP: A Simple and Efficient Framework for Federated Learning on Non-IID Data</title>
<link>https://arxiv.org/abs/2511.05028</link>
<guid>https://arxiv.org/abs/2511.05028</guid>
<content:encoded><![CDATA[
arXiv:2511.05028v1 Announce Type: new 
Abstract: Federated fine-tuning (FFT) adapts foundation models to decentralized data but remains fragile under heterogeneous client distributions due to local drift, i.e., client-level update divergences that induce systematic bias and amplified variance in the global model. Existing aggregation and personalization methods largely correct drift post hoc, which proves brittle under extreme non-IID conditions. We introduce OvA-LP, a minimalist framework that is, to our knowledge, the first explicitly designed to suppress drift at its source within the PEFT-based FFT paradigm. OvA-LP combines linear probing on a frozen encoder with a one-vs-all head and a simple two-stage procedure, preserving pretrained feature geometry and decoupling logits to prevent the mechanisms that amplify drift. On CIFAR-100 with 100 clients, averaged over shard-1, shard-2, and Bernoulli-Dirichlet partitions, OvA-LP retains 95.9% of its IID accuracy, whereas state-of-the-art FFT baselines retain only 10.1% (PFPT) and 34.5% (FFT-MoE) under the same conditions. OvA-LP further maintains resilience under both symmetric and asymmetric label noise. In addition, precomputing encoder features makes per-round cost nearly independent of encoder size. Together, these results demonstrate that OvA-LP provides a principled and efficient basis for robust FFT under heterogeneity.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SmartSecChain-SDN: A Blockchain-Integrated Intelligent Framework for Secure and Efficient Software-Defined Networks</title>
<link>https://arxiv.org/abs/2511.05156</link>
<guid>https://arxiv.org/abs/2511.05156</guid>
<content:encoded><![CDATA[
arXiv:2511.05156v1 Announce Type: new 
Abstract: With more and more existing networks being transformed to Software-Defined Networking (SDN), they need to be more secure and demand smarter ways of traffic control. This work, SmartSecChain-SDN, is a platform that combines machine learning based intrusion detection, blockchain-based storage of logs, and application-awareness-based priority in SDN networks. To detect network intrusions in a real-time, precision and low-false positives setup, the framework utilizes the application of advanced machine learning algorithms, namely Random Forest, XGBoost, CatBoost, and CNN-BiLSTM. SmartSecChain-SDN is based on the Hyperledger Fabric, which is a permissioned blockchain technology, to provide secure, scalable, and privacy-preserving storage and, thus, guarantee that the Intrusion Detection System (IDS) records cannot be altered and can be analyzed comprehensively. The system also has Quality of Service (QoS) rules and traffic shaping based on applications, which enables prioritization of critical services, such as VoIP, video conferencing, and business applications, as well as de-prioritization of non-essential traffic, such as downloads and updates. Mininet can simulate real-time SDN scenarios because it is used to prototype whole architectures. It is also compatible with controllers OpenDaylight and Ryu. It has tested the framework using the InSDN dataset and proved that it can identify different kinds of cyberattacks and handle bandwidth allocation efficiently under circumstances of resource constraints. SmartSecChain-SDN comprehensively addresses SDN system protection, securing and enhancing. The proposed study offers an innovative, extensible way to improve cybersecurity, regulatory compliance, and the administration of next-generation programmable networks.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>To Squelch or not to Squelch: Enabling Improved Message Dissemination on the XRP Ledger</title>
<link>https://arxiv.org/abs/2511.05362</link>
<guid>https://arxiv.org/abs/2511.05362</guid>
<content:encoded><![CDATA[
arXiv:2511.05362v1 Announce Type: new 
Abstract: With the large increase in the adoption of blockchain technologies, their underlying peer-to-peer networks must also scale with the demand. In this context, previous works highlighted the importance of ensuring efficient and resilient communication for the underlying consensus and replication mechanisms. However, they were mainly focused on mainstream, Proof-of-Work-based Distributed Ledger Technologies like Bitcoin or Ethereum.
  In this paper, the problem is investigated in the context of consensus-validation based blockchains, like the XRP Ledger. The latter relies on a Federated Byzantine Agreement (FBA) consensus mechanism which is proven to have a good scalability in regards to transaction throughput. However, it is known that significant increases in the size of the XRP Ledger network would be challenging to achieve. The main reason is the flooding mechanism used to disseminate the messages related to the consensus protocol, which creates many duplicates in the network. Squelching is a recent solution proposed for limiting this duplication, however, it was never evaluated quantitatively in real-life scenarios involving the XRPL production network. In this paper, our aim is to assess this mechanism using a real-life controllable testbed and the XRPL production network, to assess its benefit and compare it to alternative solutions relying on Named Data Networking and on a gossip-based approach.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OptiLog: Assigning Roles in Byzantine Consensus</title>
<link>https://arxiv.org/abs/2502.15428</link>
<guid>https://arxiv.org/abs/2502.15428</guid>
<content:encoded><![CDATA[
arXiv:2502.15428v2 Announce Type: replace 
Abstract: Byzantine Fault-Tolerant (BFT) protocols play an important role in blockchains. As the deployment of such systems extends to wide-area networks, the scalability of BFT protocols becomes a critical concern. Optimizations that assign specific roles to individual replicas can significantly improve the performance of BFT systems. However, such role assignment is highly sensitive to faults, potentially undermining the optimizations' effectiveness. To address these challenges, we present OptiLog, a logging framework for collecting and analyzing measurements that help to assign roles in globally distributed systems, despite the presence of faults. OptiLog presents local measurements in global data structures, to enable consistent decisions and hold replicas accountable if they do not perform according to their reported measurements. We demonstrate OptiLog's flexibility by applying it to two BFT protocols: (1) Aware, a highly optimized PBFT-like protocol, and (2) Kauri, a tree-based protocol designed for large-scale deployments. OptiLog detects and excludes replicas that misbehave during consensus and thus enables the system to operate in an optimized, low-latency configuration, even under adverse conditions. Experiments show that for tree overlays deployed across 73 worldwide cities, trees found by OptiLog display 39% lower latency than Kauri.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Centralized Reduction of Decentralized Stochastic Control Models and their weak-Feller Regularity</title>
<link>https://arxiv.org/abs/2408.13828</link>
<guid>https://arxiv.org/abs/2408.13828</guid>
<content:encoded><![CDATA[
arXiv:2408.13828v5 Announce Type: replace-cross 
Abstract: Decentralized stochastic control problems involving general state/measurement/action spaces are intrinsically difficult to study because of the inapplicability of standard tools from centralized (single-agent) stochastic control. In this paper, we address some of these challenges for decentralized stochastic control with standard Borel spaces under two different but tightly related information structures: the one-step delayed information sharing pattern (OSDISP), and the $K$-step periodic information sharing pattern (KSPISP). We will show that the one-step delayed and $K$-step periodic problems can be reduced to a centralized Markov Decision Process (MDP), generalizing prior results which considered finite, linear, or static models, by addressing several measurability and topological questions. We then provide sufficient conditions for the transition kernels of both centralized reductions to be weak-Feller. The existence and separated nature of optimal policies under both information structures are then established. The weak Feller regularity also facilitates rigorous approximation and learning theoretic results, as shown in the paper.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Autoregressive Transformers for Model-Agnostic Federated MRI Reconstruction</title>
<link>https://arxiv.org/abs/2502.04521</link>
<guid>https://arxiv.org/abs/2502.04521</guid>
<content:encoded><![CDATA[
arXiv:2502.04521v3 Announce Type: replace-cross 
Abstract: While learning-based models hold great promise for MRI reconstruction, single-site models trained on limited local datasets often show poor generalization. This has motivated collaborative training across institutions via federated learning (FL)-a privacy-preserving framework that aggregates model updates instead of sharing raw data. Conventional FL requires architectural homogeneity, restricting sites from using models tailored to their resources or needs. To address this limitation, we propose FedGAT, a model-agnostic FL technique that first collaboratively trains a global generative prior for MR images, adapted from a natural image foundation model composed of a variational autoencoder (VAE) and a transformer that generates images via spatial-scale autoregression. We fine-tune the transformer module after injecting it with a lightweight site-specific prompting mechanism, keeping the VAE frozen, to efficiently adapt the model to multi-site MRI data. In a second tier, each site independently trains its preferred reconstruction model by augmenting local data with synthetic MRI data from other sites, generated by site-prompting the tuned prior. This decentralized augmentation improves generalization while preserving privacy. Experiments on multi-institutional datasets show that FedGAT outperforms state-of-the-art FL baselines in both within- and cross-site reconstruction performance under model-heterogeneous settings.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Certification Strategies for Blockchain-based Traceability Systems</title>
<link>https://arxiv.org/abs/2511.04409</link>
<guid>https://arxiv.org/abs/2511.04409</guid>
<content:encoded><![CDATA[
arXiv:2511.04409v1 Announce Type: new 
Abstract: The use of blockchains for data certification and traceability is now well established in both the literature and practical applications. However, while blockchain-based certification of individual data is clear and straightforward, the use of blockchain to certify large amounts of data produced on a nearly continuous basis still poses some challenges. In such a case, in fact, it is first necessary to collect the data in an off-chain buffer, and then to organize it, e.g., via Merkle trees, in order to keep the size and quantity of certification data to be written to the blockchain small. In this paper, we consider a typical system for blockchain-based traceability of a production process, and propose and comparatively analyze some strategies for certifying the data of such a process on blockchain, while maintaining the possibility of verifying their certification in a decentralized way.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Regret Lower Bounds for Decentralized Multi-Agent Stochastic Shortest Path Problems</title>
<link>https://arxiv.org/abs/2511.04594</link>
<guid>https://arxiv.org/abs/2511.04594</guid>
<content:encoded><![CDATA[
arXiv:2511.04594v1 Announce Type: new 
Abstract: Multi-agent systems (MAS) are central to applications such as swarm robotics and traffic routing, where agents must coordinate in a decentralized manner to achieve a common objective. Stochastic Shortest Path (SSP) problems provide a natural framework for modeling decentralized control in such settings. While the problem of learning in SSP has been extensively studied in single-agent settings, the decentralized multi-agent variant remains largely unexplored. In this work, we take a step towards addressing that gap. We study decentralized multi-agent SSPs (Dec-MASSPs) under linear function approximation, where the transition dynamics and costs are represented using linear models. Applying novel symmetry-based arguments, we identify the structure of optimal policies. Our main contribution is the first regret lower bound for this setting based on the construction of hard-to-learn instances for any number of agents, $n$. Our regret lower bound of $\Omega(\sqrt{K})$, over $K$ episodes, highlights the inherent learning difficulty in Dec-MASSPs. These insights clarify the learning complexity of decentralized control and can further guide the design of efficient learning algorithms in multi-agent systems.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2511.04646</link>
<guid>https://arxiv.org/abs/2511.04646</guid>
<content:encoded><![CDATA[
arXiv:2511.04646v1 Announce Type: new 
Abstract: Cooperative multi-agent planning requires agents to make joint decisions with partial information and limited communication. Coordination at the trajectory level often fails, as small deviations in timing or movement cascade into conflicts. Symbolic planning mitigates this challenge by raising the level of abstraction and providing a minimal vocabulary of actions that enable synchronization and collective progress. We present DR. WELL, a decentralized neurosymbolic framework for cooperative multi-agent planning. Cooperation unfolds through a two-phase negotiation protocol: agents first propose candidate roles with reasoning and then commit to a joint allocation under consensus and environment constraints. After commitment, each agent independently generates and executes a symbolic plan for its role without revealing detailed trajectories. Plans are grounded in execution outcomes via a shared world model that encodes the current state and is updated as agents act. By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids brittle step-level alignment and enables higher-level operations that are reusable, synchronizable, and interpretable. Experiments on cooperative block-push tasks show that agents adapt across episodes, with the dynamic world model capturing reusable patterns and improving task completion rates and efficiency. Experiments on cooperative block-push tasks show that our dynamic world model improves task completion and efficiency through negotiation and self-refinement, trading a time overhead for evolving, more efficient collaboration strategies.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RaceTEE: Enabling Interoperability of Confidential Smart Contracts</title>
<link>https://arxiv.org/abs/2503.09317</link>
<guid>https://arxiv.org/abs/2503.09317</guid>
<content:encoded><![CDATA[
arXiv:2503.09317v2 Announce Type: replace 
Abstract: Decentralized smart contracts enable trustless collaboration but suffer from limited privacy and scalability, which hinders broader adoption. Trusted Execution Environment (TEE) based off-chain execution frameworks offer a promising solution to both issues. Although TEE-based frameworks have made significant progress, prior work has yet to fully explore contract interoperability, a critical foundation for building complex real-world decentralized applications. This paper identifies the key challenges impeding such interoperability and presents practical solutions. Based on these insights, we introduce RaceTEE, a novel framework that leverages off-chain TEE-enabled nodes to efficiently execute confidential, long-lived smart contracts with interactions of arbitrary complexity among contracts. We implement a RaceTEE prototype using Intel SGX, integrate it with Ethereum, and release it as open source. Evaluation across diverse use cases demonstrates its practicality and effectiveness.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning</title>
<link>https://arxiv.org/abs/2506.06694</link>
<guid>https://arxiv.org/abs/2506.06694</guid>
<content:encoded><![CDATA[
arXiv:2506.06694v5 Announce Type: replace 
Abstract: Foundation models have revolutionized fields such as natural language processing and computer vision by enabling general-purpose learning across diverse tasks and datasets. However, building analogous models for human mobility remains challenging due to the privacy-sensitive nature of mobility data and the resulting data silos across institutions. To bridge this gap, we propose MoveGCL, a scalable and privacy-preserving framework for training mobility foundation models via generative continual learning. Without sharing raw data, MoveGCL enables decentralized and progressive model evolution by replaying synthetic trajectories generated from a frozen teacher model, and reinforces knowledge retention through a tailored distillation strategy that mitigates catastrophic forgetting. To address the heterogeneity of mobility patterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a mobility-aware expert routing mechanism, and employs a layer-wise progressive adaptation strategy to stabilize continual updates. Experiments on six real-world urban datasets demonstrate that MoveGCL achieves performance comparable to joint training and significantly outperforms federated learning baselines, while offering strong privacy protection. MoveGCL marks a crucial step toward unlocking foundation models for mobility, offering a practical blueprint for open, scalable, and privacy-preserving model development in the era of foundation models. To facilitate reproducibility and future research, we have released the code and models at https://github.com/tsinghua-fib-lab/MoveGCL.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RobustFSM: Submodular Maximization in Federated Setting with Malicious Clients</title>
<link>https://arxiv.org/abs/2511.02029</link>
<guid>https://arxiv.org/abs/2511.02029</guid>
<content:encoded><![CDATA[
arXiv:2511.02029v1 Announce Type: new 
Abstract: Submodular maximization is an optimization problem benefiting many machine learning applications, where we seek a small subset best representing an extremely large dataset. We focus on the federated setting where the data are locally owned by decentralized clients who have their own definitions for the quality of representability. This setting requires repetitive aggregation of local information computed by the clients. While the main motivation is to respect the privacy and autonomy of the clients, the federated setting is vulnerable to client misbehaviors: malicious clients might share fake information. An analogy is backdoor attack in conventional federated learning, but our challenge differs freshly due to the unique characteristics of submodular maximization. We propose RobustFSM, a federated submodular maximization solution that is robust to various practical client attacks. Its performance is substantiated with an empirical evaluation study using real-world datasets. Numerical results show that the solution quality of RobustFSM substantially exceeds that of the conventional federated algorithm when attacks are severe. The degree of this improvement depends on the dataset and attack scenarios, which can be as high as 200%
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GPoS: Geospatially-aware Proof of Stake</title>
<link>https://arxiv.org/abs/2511.02034</link>
<guid>https://arxiv.org/abs/2511.02034</guid>
<content:encoded><![CDATA[
arXiv:2511.02034v1 Announce Type: new 
Abstract: Geospatial decentralization is essential for blockchains, ensuring regulatory resilience, robustness, and fairness. We empirically analyze five major Proof of Stake (PoS) blockchains: Aptos, Avalanche, Ethereum, Solana, and Sui, revealing that a few geographic regions dominate consensus voting power, resulting in limited geospatial decentralization. To address this, we propose Geospatially aware Proof of Stake (GPoS), which integrates geospatial diversity with stake-based voting power. Experimental evaluation demonstrates an average 45% improvement in geospatial decentralization, as measured by the Gini coefficient of Eigenvector centrality, while incurring minimal performance overhead in BFT protocols, including HotStuff and CometBFT. These results demonstrate that GPoS can improve geospatial decentralization {while, in our experiments, incurring minimal overhead} to consensus performance.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Private Map-Secure Reduce: Infrastructure for Efficient AI Data Markets</title>
<link>https://arxiv.org/abs/2511.02055</link>
<guid>https://arxiv.org/abs/2511.02055</guid>
<content:encoded><![CDATA[
arXiv:2511.02055v1 Announce Type: new 
Abstract: The modern AI data economy centralizes power, limits innovation, and misallocates value by extracting data without control, privacy, or fair compensation. We introduce Private Map-Secure Reduce (PMSR), a network-native paradigm that transforms data economics from extractive to participatory through cryptographically enforced markets. Extending MapReduce to decentralized settings, PMSR enables computation to move to the data, ensuring verifiable privacy, efficient price discovery, and incentive alignment. Demonstrations include large-scale recommender audits, privacy-preserving LLM ensembling (87.5\% MMLU accuracy across six models), and distributed analytics over hundreds of nodes. PMSR establishes a scalable, equitable, and privacy-guaranteed foundation for the next generation of AI data markets.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Near Optimal Convergence to Coarse Correlated Equilibrium in General-Sum Markov Games</title>
<link>https://arxiv.org/abs/2511.02157</link>
<guid>https://arxiv.org/abs/2511.02157</guid>
<content:encoded><![CDATA[
arXiv:2511.02157v1 Announce Type: new 
Abstract: No-regret learning dynamics play a central role in game theory, enabling decentralized convergence to equilibrium for concepts such as Coarse Correlated Equilibrium (CCE) or Correlated Equilibrium (CE). In this work, we improve the convergence rate to CCE in general-sum Markov games, reducing it from the previously best-known rate of $\mathcal{O}(\log^5 T / T)$ to a sharper $\mathcal{O}(\log T / T)$. This matches the best known convergence rate for CE in terms of $T$, number of iterations, while also improving the dependence on the action set size from polynomial to polylogarithmic-yielding exponential gains in high-dimensional settings. Our approach builds on recent advances in adaptive step-size techniques for no-regret algorithms in normal-form games, and extends them to the Markovian setting via a stage-wise scheme that adjusts learning rates based on real-time feedback. We frame policy updates as an instance of Optimistic Follow-the-Regularized-Leader (OFTRL), customized for value-iteration-based learning. The resulting self-play algorithm achieves, to our knowledge, the fastest known convergence rate to CCE in Markov games.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Permissioned Blockchain in Advanced Air Mobility: A Performance Analisys for UTM</title>
<link>https://arxiv.org/abs/2511.02171</link>
<guid>https://arxiv.org/abs/2511.02171</guid>
<content:encoded><![CDATA[
arXiv:2511.02171v1 Announce Type: new 
Abstract: The rapid adoption of Uncrewed Aerial Vehicles (UAVs) has driven aviation authorities to propose distributed Uncrewed Traffic Management (UTM) architectures. Several studies have advocated blockchain as a promising technology to meet these requirements. However, since UTM is a safety-critical and highly regulated domain, compliance with standards and regulatory frameworks is as crucial as performance and security. This work benchmarks two distributed architectures aligned with current regulatory frameworks: the Linux Foundation's InterUSS platform and a Hyperledger Fabric-based private ledger. Our findings reveal that blockchain-based systems require architectures specifically designed for aeronautical performance constraints.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Quantum Kernel Learning for Anomaly Detection in Multivariate IoT Time-Series</title>
<link>https://arxiv.org/abs/2511.02301</link>
<guid>https://arxiv.org/abs/2511.02301</guid>
<content:encoded><![CDATA[
arXiv:2511.02301v1 Announce Type: new 
Abstract: The rapid growth of industrial Internet of Things (IIoT) systems has created new challenges for anomaly detection in high-dimensional, multivariate time-series, where privacy, scalability, and communication efficiency are critical. Classical federated learning approaches mitigate privacy concerns by enabling decentralized training, but they often struggle with highly non-linear decision boundaries and imbalanced anomaly distributions. To address this gap, we propose a Federated Quantum Kernel Learning (FQKL) framework that integrates quantum feature maps with federated aggregation to enable distributed, privacy-preserving anomaly detection across heterogeneous IoT networks. In our design, quantum edge nodes locally compute compressed kernel statistics using parameterized quantum circuits and share only these summaries with a central server, which constructs a global Gram matrix and trains a decision function (e.g., Fed-QSVM). Experimental results on synthetic IIoT benchmarks demonstrate that FQKL achieves superior generalization in capturing complex temporal correlations compared to classical federated baselines, while significantly reducing communication overhead. This work highlights the promise of quantum kernels in federated settings, advancing the path toward scalable, robust, and quantum-enhanced intelligence for next-generation IoT infrastructures.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.02304</link>
<guid>https://arxiv.org/abs/2511.02304</guid>
<content:encoded><![CDATA[
arXiv:2511.02304v1 Announce Type: new 
Abstract: We study the problem of learning multi-task, multi-agent policies for cooperative, temporal objectives, under centralized training, decentralized execution. In this setting, using automata to represent tasks enables the decomposition of complex tasks into simpler sub-tasks that can be assigned to agents. However, existing approaches remain sample-inefficient and are limited to the single-task case. In this work, we present Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning (ACC-MARL), a framework for learning task-conditioned, decentralized team policies. We identify the main challenges to ACC-MARL's feasibility in practice, propose solutions, and prove the correctness of our approach. We further show that the value functions of learned policies can be used to assign tasks optimally at test time. Experiments show emergent task-aware, multi-step coordination among agents, e.g., pressing a button to unlock a door, holding the door, and short-circuiting tasks.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large-scale automatic carbon ion treatment planning for head and neck cancers via parallel multi-agent reinforcement learning</title>
<link>https://arxiv.org/abs/2511.02314</link>
<guid>https://arxiv.org/abs/2511.02314</guid>
<content:encoded><![CDATA[
arXiv:2511.02314v1 Announce Type: new 
Abstract: Head-and-neck cancer (HNC) planning is difficult because multiple critical organs-at-risk (OARs) are close to complex targets. Intensity-modulated carbon-ion therapy (IMCT) offers superior dose conformity and OAR sparing but remains slow due to relative biological effectiveness (RBE) modeling, leading to laborious, experience-based, and often suboptimal tuning of many treatment-planning parameters (TPPs). Recent deep learning (DL) methods are limited by data bias and plan feasibility, while reinforcement learning (RL) struggles to efficiently explore the exponentially large TPP search space. We propose a scalable multi-agent RL (MARL) framework for parallel tuning of 45 TPPs in IMCT. It uses a centralized-training decentralized-execution (CTDE) QMIX backbone with Double DQN, Dueling DQN, and recurrent encoding (DRQN) for stable learning in a high-dimensional, non-stationary environment. To enhance efficiency, we (1) use compact historical DVH vectors as state inputs, (2) apply a linear action-to-value transform mapping small discrete actions to uniform parameter adjustments, and (3) design an absolute, clinically informed piecewise reward aligned with plan scores. A synchronous multi-process worker system interfaces with the PHOENIX TPS for parallel optimization and accelerated data collection. On a head-and-neck dataset (10 training, 10 testing), the method tuned 45 parameters simultaneously and produced plans comparable to or better than expert manual ones (relative plan score: RL $85.93\pm7.85%$ vs Manual $85.02\pm6.92%$), with significant (p-value $<$ 0.05) improvements for five OARs. The framework efficiently explores high-dimensional TPP spaces and generates clinically competitive IMCT plans through direct TPS interaction, notably improving OAR sparing.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Spatially Informed Gaussian Process UCB Method for Decentralized Coverage Control</title>
<link>https://arxiv.org/abs/2511.02398</link>
<guid>https://arxiv.org/abs/2511.02398</guid>
<content:encoded><![CDATA[
arXiv:2511.02398v1 Announce Type: new 
Abstract: We present a novel decentralized algorithm for coverage control in unknown spatial environments modeled by Gaussian Processes (GPs). To trade-off between exploration and exploitation, each agent autonomously determines its trajectory by minimizing a local cost function. Inspired by the GP-UCB (Upper Confidence Bound for GPs) acquisition function, the proposed cost combines the expected locational cost with a variance-based exploration term, guiding agents toward regions that are both high in predicted density and model uncertainty. Compared to previous work, our algorithm operates in a fully decentralized fashion, relying only on local observations and communication with neighboring agents. In particular, agents periodically update their inducing points using a greedy selection strategy, enabling scalable online GP updates. We demonstrate the effectiveness of our algorithm in simulation.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Voltage Control of AC Microgrids with Constant Power Loads using Control Barrier Functions</title>
<link>https://arxiv.org/abs/2511.02438</link>
<guid>https://arxiv.org/abs/2511.02438</guid>
<content:encoded><![CDATA[
arXiv:2511.02438v1 Announce Type: new 
Abstract: This paper proposes a novel nonlinear decentralized voltage controller for constrained regulation of meshed AC Microgrid networks with high penetration of constant power loads. Perceiving the load demand as an unknown disturbance, the network model is reformulated in a cascaded structure composed of a nominal, i.e. uncertainty-free, and an error subsystem. The latter captures the distance between the true and the nominal state trajectories, for which we prove boundedness via a suitable control barrier function. Under sufficient conditions, we prove asymptotic stability of the cascaded dynamics with respect to an equilibrium set and also provide an estimate of the region of attraction. In addition, it is rigorously shown that the proposed nonlinear control law also enforces constrained regulation around a rated voltage value, without the need of saturation devices. The operation of the closed-loop system is illustrated in a simulation scenario, demonstrating bounded operation and convergence to a neighbourhood of the desired reference vector.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenCourier: an Open Protocol for Building a Decentralized Ecosystem of Community-owned Delivery Platforms</title>
<link>https://arxiv.org/abs/2511.02455</link>
<guid>https://arxiv.org/abs/2511.02455</guid>
<content:encoded><![CDATA[
arXiv:2511.02455v1 Announce Type: new 
Abstract: Although the platform gig economy has reshaped the landscape of work, its centralized operation by select actors has brought about challenges that impedes workers' well-being. We present the architecture and design of OpenCourier, an open protocol that defines communication patterns within a decentralized ecosystem of delivery platforms. Through this protocol, we aim to address three key challenges in the current economy: power imbalances between the platform and workers, information asymmetries caused by black-boxed algorithms and value misalignments in the infrastructure design process. With the OpenCourier protocol, we outline a blueprint for community-owned ecosystem of delivery platforms that centers worker agency, transparency, and bottom-up design.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Approach to Detect and Eliminate Flapping Phenomena due to Flexible Resources</title>
<link>https://arxiv.org/abs/2511.02497</link>
<guid>https://arxiv.org/abs/2511.02497</guid>
<content:encoded><![CDATA[
arXiv:2511.02497v1 Announce Type: new 
Abstract: This paper presents a decentralized methodology for detecting and mitigating flapping phenomena in power systems, primarily caused by the operation of discrete devices. The proposed approach applies moving-window autocorrelation to local measurements, enabling each device to autonomously identify sustained oscillations. Upon detection, a probabilistic, device-specific mitigation strategy is executed. Flexible demand resources (DFRs), under-load tap changers (ULTCs), and automatic voltage regulators (AVRs) are utilised to illustrate the performance of the proposed approach to both discrete and continuous-operation devices. Results show that the proposed method is robust and properly distinguishes damped oscillations from persistent flapping, allowing devices to independently recognize problematic operating scenarios and implement corrective actions accordingly.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized AI Service Placement, Selection and Routing in Mobile Networks</title>
<link>https://arxiv.org/abs/2511.02638</link>
<guid>https://arxiv.org/abs/2511.02638</guid>
<content:encoded><![CDATA[
arXiv:2511.02638v1 Announce Type: new 
Abstract: The rapid development and usage of large-scale AI models by mobile users will dominate the traffic load in future communication networks. The advent of AI technology also facilitates a decentralized AI ecosystem where small organizations or even individuals can host AI services. In such scenarios, AI service (models) placement, selection, and request routing decisions are tightly coupled, posing a challenging yet fundamental trade-off between service quality and service latency, especially when considering user mobility. Existing solutions for related problems in mobile edge computing (MEC) and data-intensive networks fall short due to restrictive assumptions about network structure or user mobility. To bridge this gap, we propose a decentralized framework that jointly optimizes AI service placement, selection, and request routing. In the proposed framework, we use traffic tunneling to support user mobility without costly AI service migrations. To account for nonlinear queuing delays, we formulate a nonconvex problem to optimize the trade-off between service quality and end-to-end latency. We derive the node-level KKT conditions and develop a decentralized Frank--Wolfe algorithm with a novel messaging protocol. Numerical evaluations validate the proposed approach and show substantial performance improvements over existing methods.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bringing Private Reads to Hyperledger Fabric via Private Information Retrieval</title>
<link>https://arxiv.org/abs/2511.02656</link>
<guid>https://arxiv.org/abs/2511.02656</guid>
<content:encoded><![CDATA[
arXiv:2511.02656v1 Announce Type: new 
Abstract: Permissioned blockchains ensure integrity and auditability of shared data but expose query parameters to peers during read operations, creating privacy risks for organizations querying sensitive records. This paper proposes a Private Information Retrieval (PIR) mechanism to enable private reads from Hyperledger Fabric's world state, allowing endorsing peers to process encrypted queries without learning which record is accessed. We implement and benchmark a PIR-enabled chaincode that performs ciphertext-plaintext (ct-pt) homomorphic multiplication directly within evaluate transactions, preserving Fabric's endorsement and audit semantics. The prototype achieves an average end-to-end latency of 113 ms and a peer-side execution time below 42 ms, with approximately 2 MB of peer network traffic per private read in development mode--reducible by half under in-process deployment. Storage profiling across three channel configurations shows near-linear growth: block size increases from 77 kilobytes to 294 kilobytes and world-state from 112 kilobytes to 332 kilobytes as the ring dimension scales from 8,192 to 32,768 coefficients. Parameter analysis further indicates that ring size and record length jointly constrain packing capacity, supporting up to 512 records of 64 bytes each under the largest configuration. These results confirm the practicality of PIR-based private reads in Fabric for smaller, sensitive datasets and highlight future directions to optimize performance and scalability.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Controlling Performance and Budget of a Centralized Multi-agent LLM System with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.02755</link>
<guid>https://arxiv.org/abs/2511.02755</guid>
<content:encoded><![CDATA[
arXiv:2511.02755v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit complementary strengths across domains and come with varying inference costs, motivating the design of multi-agent LLM systems where specialized models collaborate efficiently. Existing approaches predominantly rely on decentralized frameworks, which invoke multiple LLMs for every input and thus lead to substantial and uncontrolled inference costs. In this work, we introduce a centralized multi-LLM framework, where a controller LLM selectively coordinates a pool of expert models in a cost-efficient and cost-controllable manner. We formulate this coordination problem as reinforcement learning with dual objectives: maximizing task performance while minimizing the overall inference cost. In addition, we expect the multi-agent system to have adapted behavior with different budget conditions during inference. To this end, we propose CoRL, a reinforcement learning framework that optimizes the performance cost trade-off in a controllable multi-budget setting. Experiments on four diverse benchmarks demonstrate that CoRL enables a single system to surpass the best expert LLM under high-budget settings, while maintaining strong performance in more economical low-budget modes, highlighting the effectiveness of centralized coordination for scalable and cost-efficient multi-agent LLM systems.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>1 PoCo: Agentic Proof-of-Concept Exploit Generation for Smart Contracts</title>
<link>https://arxiv.org/abs/2511.02780</link>
<guid>https://arxiv.org/abs/2511.02780</guid>
<content:encoded><![CDATA[
arXiv:2511.02780v1 Announce Type: new 
Abstract: Smart contracts operate in a highly adversarial environment, where vulnerabilities can lead to substantial financial losses. Thus, smart contracts are subject to security audits. In auditing, proof-of-concept (PoC) exploits play a critical role by demonstrating to the stakeholders that the reported vulnerabilities are genuine, reproducible, and actionable. However, manually creating PoCs is time-consuming, error-prone, and often constrained by tight audit schedules. We introduce POCO, an agentic framework that automatically generates executable PoC exploits from natural-language vulnerability descriptions written by auditors. POCO autonomously generates PoC exploits in an agentic manner by interacting with a set of code-execution tools in a Reason-Act-Observe loop. It produces fully executable exploits compatible with the Foundry testing framework, ready for integration into audit reports and other security tools. We evaluate POCO on a dataset of 23 real-world vulnerability reports. POCO consistently outperforms the prompting and workflow baselines, generating well-formed and logically correct PoCs. Our results demonstrate that agentic frameworks can significantly reduce the effort required for high-quality PoCs in smart contract audits. Our contribution provides readily actionable knowledge for the smart contract security community.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoK: Design, Vulnerabilities, and Security Measures of Cryptocurrency Wallets</title>
<link>https://arxiv.org/abs/2307.12874</link>
<guid>https://arxiv.org/abs/2307.12874</guid>
<content:encoded><![CDATA[
arXiv:2307.12874v5 Announce Type: replace 
Abstract: With the advent of decentralised digital currencies powered by blockchain technology, a new era of peer-to-peer transactions has commenced. The rapid growth of the cryptocurrency economy has led to increased use of transaction-enabling wallets, making them a focal point for security risks. As the frequency of wallet-related incidents rises, there is a critical need for a systematic approach to measure and evaluate these attacks, drawing lessons from past incidents to enhance wallet security. In response, we introduce a multi-dimensional design taxonomy for existing and novel wallets with various design decisions. We classify existing industry wallets based on this taxonomy, identify previously occurring vulnerabilities and discuss the security implications of design decisions. We also systematise threats to the wallet mechanism and analyse the adversary's goals, capabilities and required knowledge. We present a multi-layered attack framework and investigate 84 incidents between 2012 and 2024, accounting for $5.4B. Following this, we classify defence implementations for these attacks on the precautionary and remedial axes. We map the mechanism and design decisions to vulnerabilities, attacks, and possible defence methods to discuss various insights.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DMind Benchmark: Toward a Holistic Assessment of LLM Capabilities across the Web3 Domain</title>
<link>https://arxiv.org/abs/2504.16116</link>
<guid>https://arxiv.org/abs/2504.16116</guid>
<content:encoded><![CDATA[
arXiv:2504.16116v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have achieved impressive performance in diverse natural language processing tasks, but specialized domains such as Web3 present new challenges and require more tailored evaluation. Despite the significant user base and capital flows in Web3, encompassing smart contracts, decentralized finance (DeFi), non-fungible tokens (NFTs), decentralized autonomous organizations (DAOs), on-chain governance, and novel token-economics, no comprehensive benchmark has systematically assessed LLM performance in this domain. To address this gap, we introduce the DMind Benchmark, a holistic Web3-oriented evaluation suite covering nine critical subfields: fundamental blockchain concepts, blockchain infrastructure, smart contract, DeFi mechanisms, DAOs, NFTs, token economics, meme concept, and security vulnerabilities. Beyond multiple-choice questions, DMind Benchmark features domain-specific tasks such as contract debugging and on-chain numeric reasoning, mirroring real-world scenarios. We evaluated 26 models, including ChatGPT, Claude, DeepSeek, Gemini, Grok, and Qwen, uncovering notable performance gaps in specialized areas like token economics and security-critical contract analysis. While some models excel in blockchain infrastructure tasks, advanced subfields remain challenging. Our benchmark dataset and evaluation pipeline are open-sourced on https://huggingface.co/datasets/DMindAI/DMind_Benchmark, reaching number one in Hugging Face's trending dataset charts within a week of release.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging</title>
<link>https://arxiv.org/abs/2506.09024</link>
<guid>https://arxiv.org/abs/2506.09024</guid>
<content:encoded><![CDATA[
arXiv:2506.09024v2 Announce Type: replace 
Abstract: Safe deployment of machine learning (ML) models in safety-critical domains such as medical imaging requires detecting inputs with characteristics not seen during training, known as out-of-distribution (OOD) detection, to prevent unreliable predictions. Effective OOD detection after deployment could benefit from access to the training data, enabling direct comparison between test samples and the training data distribution to identify differences. State-of-the-art OOD detection methods, however, either discard the training data after deployment or assume that test samples and training data are centrally stored together, an assumption that rarely holds in real-world settings. This is because shipping the training data with the deployed model is usually impossible due to the size of training databases, as well as proprietary or privacy constraints. We introduce the Isolation Network, an OOD detection framework that quantifies the difficulty of separating a target test sample from the training data by solving a binary classification task. We then propose Decentralized Isolation Networks (DIsoN), which enables the comparison of training and test data when data-sharing is impossible, by exchanging only model parameters between the remote computational nodes of training and deployment. We further extend DIsoN with class-conditioning, comparing a target sample solely with training data of its predicted class. We evaluate DIsoN on four medical imaging datasets (dermatology, chest X-ray, breast ultrasound, histopathology) across 12 OOD detection tasks. DIsoN performs favorably against existing methods while respecting data-privacy. This decentralized OOD detection framework opens the way for a new type of service that ML developers could provide along with their models: providing remote, secure utilization of their training data for OOD detection services. Code: https://github.com/FelixWag/DIsoN
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real World Federated Learning with a Knowledge Distilled Transformer for Cardiac CT Imaging</title>
<link>https://arxiv.org/abs/2407.07557</link>
<guid>https://arxiv.org/abs/2407.07557</guid>
<content:encoded><![CDATA[
arXiv:2407.07557v3 Announce Type: replace-cross 
Abstract: Federated learning is a renowned technique for utilizing decentralized data while preserving privacy. However, real-world applications often face challenges like partially labeled datasets, where only a few locations have certain expert annotations, leaving large portions of unlabeled data unused. Leveraging these could enhance transformer architectures ability in regimes with small and diversely annotated sets. We conduct the largest federated cardiac CT analysis to date (n=8,104) in a real-world setting across eight hospitals. Our two-step semi-supervised strategy distills knowledge from task-specific CNNs into a transformer. First, CNNs predict on unlabeled data per label type and then the transformer learns from these predictions with label-specific heads. This improves predictive accuracy and enables simultaneous learning of all partial labels across the federation, and outperforms UNet-based models in generalizability on downstream tasks. Code and model weights are made openly available for leveraging future cardiac CT analysis.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Fundamental Limitations of Decentralized Learnable Reward Shaping in Cooperative Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.00034</link>
<guid>https://arxiv.org/abs/2511.00034</guid>
<content:encoded><![CDATA[
arXiv:2511.00034v1 Announce Type: new 
Abstract: Recent advances in learnable reward shaping have shown promise in single-agent reinforcement learning by automatically discovering effective feedback signals. However, the effectiveness of decentralized learnable reward shaping in cooperative multi-agent settings remains poorly understood. We propose DMARL-RSA, a fully decentralized system where each agent learns individual reward shaping, and evaluate it on cooperative navigation tasks in the simple_spread_v3 environment. Despite sophisticated reward learning, DMARL-RSA achieves only -24.20 +/- 0.09 average reward, compared to MAPPO with centralized training at 1.92 +/- 0.87--a 26.12-point gap. DMARL-RSA performs similarly to simple independent learning (IPPO: -23.19 +/- 0.96), indicating that advanced reward shaping cannot overcome fundamental decentralized coordination limitations. Interestingly, decentralized methods achieve higher landmark coverage (0.888 +/- 0.029 for DMARL-RSA, 0.960 +/- 0.045 for IPPO out of 3 total) but worse overall performance than centralized MAPPO (0.273 +/- 0.008 landmark coverage)--revealing a coordination paradox between local optimization and global performance. Analysis identifies three critical barriers: (1) non-stationarity from concurrent policy updates, (2) exponential credit assignment complexity, and (3) misalignment between individual reward optimization and global objectives. These results establish empirical limits for decentralized reward learning and underscore the necessity of centralized coordination for effective multi-agent cooperation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LC-Opt: Benchmarking Reinforcement Learning and Agentic AI for End-to-End Liquid Cooling Optimization in Data Centers</title>
<link>https://arxiv.org/abs/2511.00116</link>
<guid>https://arxiv.org/abs/2511.00116</guid>
<content:encoded><![CDATA[
arXiv:2511.00116v1 Announce Type: new 
Abstract: Liquid cooling is critical for thermal management in high-density data centers with the rising AI workloads. However, machine learning-based controllers are essential to unlock greater energy efficiency and reliability, promoting sustainability. We present LC-Opt, a Sustainable Liquid Cooling (LC) benchmark environment, for reinforcement learning (RL) control strategies in energy-efficient liquid cooling of high-performance computing (HPC) systems. Built on the baseline of a high-fidelity digital twin of Oak Ridge National Lab's Frontier Supercomputer cooling system, LC-Opt provides detailed Modelica-based end-to-end models spanning site-level cooling towers to data center cabinets and server blade groups. RL agents optimize critical thermal controls like liquid supply temperature, flow rate, and granular valve actuation at the IT cabinet level, as well as cooling tower (CT) setpoints through a Gymnasium interface, with dynamic changes in workloads. This environment creates a multi-objective real-time optimization challenge balancing local thermal regulation and global energy efficiency, and also supports additional components like a heat recovery unit (HRU). We benchmark centralized and decentralized multi-agent RL approaches, demonstrate policy distillation into decision and regression trees for interpretable control, and explore LLM-based methods that explain control actions in natural language through an agentic mesh architecture designed to foster user trust and simplify system management. LC-Opt democratizes access to detailed, customizable liquid cooling models, enabling the ML community, operators, and vendors to develop sustainable data center liquid cooling control solutions.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Application of Blockchain Frameworks for Decentralized Identity and Access Management of IoT Devices</title>
<link>https://arxiv.org/abs/2511.00249</link>
<guid>https://arxiv.org/abs/2511.00249</guid>
<content:encoded><![CDATA[
arXiv:2511.00249v1 Announce Type: new 
Abstract: The growth in IoT devices means an ongoing risk of data vulnerability. The transition from centralized ecosystems to decentralized ecosystems is of paramount importance due to security, privacy, and data use concerns. Since the majority of IoT devices will be used by consumers in peer-to-peer applications, a centralized approach raises many issues of trust related to privacy, control, and censorship. Identity and access management lies at the heart of any user-facing system. Blockchain technologies can be leveraged to augment user authority, transparency, and decentralization. This study proposes a decentralized identity management framework for IoT environments using Hyperledger Fabric and Decentralized Identifiers (DIDs). The system was simulated using Node-RED to model IoT data streams, and key functionalities including device onboarding, authentication, and secure asset querying were successfully implemented. Results demonstrated improved data integrity, transparency, and user control, with reduced reliance on centralized authorities. These findings validate the practicality of blockchain-based identity management in enhancing the security and trustworthiness of IoT infrastructures.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Penetrating the Hostile: Detecting DeFi Protocol Exploits through Cross-Contract Analysis</title>
<link>https://arxiv.org/abs/2511.00408</link>
<guid>https://arxiv.org/abs/2511.00408</guid>
<content:encoded><![CDATA[
arXiv:2511.00408v1 Announce Type: new 
Abstract: Decentralized finance (DeFi) protocols are crypto projects developed on the blockchain to manage digital assets. Attacks on DeFi have been frequent and have resulted in losses exceeding $80 billion. Current tools detect and locate possible vulnerabilities in contracts by analyzing the state changes that may occur during malicious events. However, this victim-only approaches seldom possess the capability to cover the attacker's interaction intention logic. Furthermore, only a minuscule percentage of DeFi protocols experience attacks in real-world scenarios, which poses a significant challenge for these detection tools to demonstrate practical effectiveness. In this paper, we propose DeFiTail, the first framework that utilizes deep learning technology for access control and flash loan exploit detection. Through feeding the cross-contract static data flow, DeFiTail automatically learns the attack logic in real-world malicious events that occur on DeFi protocols, capturing the threat patterns between attacker and victim contracts. Since the DeFi protocol events involve interactions with multi-account transactions, the execution path with external and internal transactions requires to be unified. Moreover, to mitigate the impact of mistakes in Control Flow Graph (CFG) connections, DeFiTail validates the data path by employing the symbolic execution stack. Furthermore, we feed the data paths through our model to achieve the inspection of DeFi protocols. Comparative experiment results indicate that DeFiTail achieves the highest accuracy, with 98.39% in access control and 97.43% in flash loan exploits. DeFiTail also demonstrates an enhanced capability to detect malicious contracts, identifying 86.67% accuracy from the CVE dataset.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TINC: Trusted Intelligent NetChain</title>
<link>https://arxiv.org/abs/2511.00823</link>
<guid>https://arxiv.org/abs/2511.00823</guid>
<content:encoded><![CDATA[
arXiv:2511.00823v1 Announce Type: new 
Abstract: Blockchain technology facilitates the development of decentralized systems that ensure trust and transparency without the need for expensive centralized intermediaries. However, existing blockchain architectures particularly consortium blockchains face critical challenges related to scalability and efficiency. State sharding has emerged as a promising approach to enhance blockchain scalability and performance. However, current shard-based solutions often struggle to guarantee fair participation and a balanced workload distribution among consortium members. To address these limitations, we propose Trusted Intelligent NetChain (TINC), a multi-plane sharding architecture specifically designed for consortium blockchains. TINC incorporates intelligent mechanisms for adaptive node assignment and dynamic workload balancing, enabling the system to respond effectively to changing network conditions while maintaining equitable shard utilization. By decoupling the control and data planes, TINC allows control nodes to focus on consensus operations, while data nodes handle large-scale storage, thus improving overall resource efficiency. Extensive experimental evaluation and formal analysis demonstrate that TINC significantly outperforms existing shard-based blockchain frameworks. It achieves higher throughput, lower latency, balanced node and transaction distributions, and reduced transaction failure rates. Furthermore, TINC maintains essential blockchain security guarantees, exhibiting resilience against Byzantine faults and dynamic network environments. The integration of Dynamic Decentralized Identifiers (DDIDs) further strengthens trust and security management within the consortium network.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Single-Tokenomics: How Farcaster's Pluralistic Incentives Reshape Social Networking</title>
<link>https://arxiv.org/abs/2511.00827</link>
<guid>https://arxiv.org/abs/2511.00827</guid>
<content:encoded><![CDATA[
arXiv:2511.00827v1 Announce Type: new 
Abstract: This paper presents the first empirical analysis of how diverse token-based reward mechanisms impact platform dynamics and user behaviors. For this, we gather a unique, large-scale dataset from Farcaster. This blockchain-based, decentralized social network incorporates multiple incentive mechanisms spanning platform-native rewards, third-party token programs, and peer-to-peer tipping. Our dataset captures token transactions and social interactions from 574,829 wallet-linked users, representing 64.25% of the platform's user base. Our socioeconomic analyses reveal how different tokenomics design shape varying participation rates (7.6%--70%) and wealth concentration patterns (Gini 0.72--0.94), whereas inter-community tipping (51--75% of all tips) is 1.3--2x more frequent among non-following pairs, thereby mitigating echo chambers. Our causal analyses further uncover several critical trade-offs: (1) while most token rewards boost content creation, they often fail to enhance -- sometimes undermining -- content quality; (2) token rewards increase follower acquisition but show neutral or negative effects on outbound following, suggesting potential asymmetric network growth; (3) repeated algorithmic rewards demonstrate strong cumulative effects that may encourage strategic optimization. Our findings advance understanding of cryptocurrency integration in social platforms and highlight challenges in aligning economic incentives with authentic social value.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GauDP: Reinventing Multi-Agent Collaboration through Gaussian-Image Synergy in Diffusion Policies</title>
<link>https://arxiv.org/abs/2511.00998</link>
<guid>https://arxiv.org/abs/2511.00998</guid>
<content:encoded><![CDATA[
arXiv:2511.00998v1 Announce Type: new 
Abstract: Recently, effective coordination in embodied multi-agent systems has remained a fundamental challenge, particularly in scenarios where agents must balance individual perspectives with global environmental awareness. Existing approaches often struggle to balance fine-grained local control with comprehensive scene understanding, resulting in limited scalability and compromised collaboration quality. In this paper, we present GauDP, a novel Gaussian-image synergistic representation that facilitates scalable, perception-aware imitation learning in multi-agent collaborative systems. Specifically, GauDP constructs a globally consistent 3D Gaussian field from decentralized RGB observations, then dynamically redistributes 3D Gaussian attributes to each agent's local perspective. This enables all agents to adaptively query task-critical features from the shared scene representation while maintaining their individual viewpoints. This design facilitates both fine-grained control and globally coherent behavior without requiring additional sensing modalities (e.g., 3D point cloud). We evaluate GauDP on the RoboFactory benchmark, which includes diverse multi-arm manipulation tasks. Our method achieves superior performance over existing image-based methods and approaches the effectiveness of point-cloud-driven methods, while maintaining strong scalability as the number of agents increases.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predictive Auxiliary Learning for Belief-based Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2511.01078</link>
<guid>https://arxiv.org/abs/2511.01078</guid>
<content:encoded><![CDATA[
arXiv:2511.01078v1 Announce Type: new 
Abstract: The performance of multi-agent reinforcement learning (MARL) in partially observable environments depends on effectively aggregating information from observations, communications, and reward signals. While most existing multi-agent systems primarily rely on rewards as the only feedback for policy training, our research shows that introducing auxiliary predictive tasks can significantly enhance learning efficiency and stability. We propose Belief-based Predictive Auxiliary Learning (BEPAL), a framework that incorporates auxiliary training objectives to support policy optimization. BEPAL follows the centralized training with decentralized execution paradigm. Each agent learns a belief model that predicts unobservable state information, such as other agents' rewards or motion directions, alongside its policy model. By enriching hidden state representations with information that does not directly contribute to immediate reward maximization, this auxiliary learning process stabilizes MARL training and improves overall performance. We evaluate BEPAL in the predator-prey environment and Google Research Football, where it achieves an average improvement of about 16 percent in performance metrics and demonstrates more stable convergence compared to baseline methods.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LSHFed: Robust and Communication-Efficient Federated Learning with Locally-Sensitive Hashing Gradient Mapping</title>
<link>https://arxiv.org/abs/2511.01296</link>
<guid>https://arxiv.org/abs/2511.01296</guid>
<content:encoded><![CDATA[
arXiv:2511.01296v1 Announce Type: new 
Abstract: Federated learning (FL) enables collaborative model training across distributed nodes without exposing raw data, but its decentralized nature makes it vulnerable in trust-deficient environments. Inference attacks may recover sensitive information from gradient updates, while poisoning attacks can degrade model performance or induce malicious behaviors. Existing defenses often suffer from high communication and computation costs, or limited detection precision. To address these issues, we propose LSHFed, a robust and communication-efficient FL framework that simultaneously enhances aggregation robustness and privacy preservation. At its core, LSHFed incorporates LSHGM, a novel gradient verification mechanism that projects high-dimensional gradients into compact binary representations via multi-hyperplane locally-sensitive hashing. This enables accurate detection and filtering of malicious gradients using only their irreversible hash forms, thus mitigating privacy leakage risks and substantially reducing transmission overhead. Extensive experiments demonstrate that LSHFed maintains high model performance even when up to 50% of participants are collusive adversaries while achieving up to a 1000x reduction in gradient verification communication compared to full-gradient methods.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Verifiable Split Learning via zk-SNARKs</title>
<link>https://arxiv.org/abs/2511.01356</link>
<guid>https://arxiv.org/abs/2511.01356</guid>
<content:encoded><![CDATA[
arXiv:2511.01356v1 Announce Type: new 
Abstract: Split learning is an approach to collaborative learning in which a deep neural network is divided into two parts: client-side and server-side at a cut layer. The client side executes its model using its raw input data and sends the intermediate activation to the server side. This configuration architecture is very useful for enabling collaborative training when data or resources are separated between devices. However, split learning lacks the ability to verify the correctness and honesty of the computations that are performed and exchanged between the parties. To this purpose, this paper proposes a verifiable split learning framework that integrates a zk-SNARK proof to ensure correctness and verifiability. The zk-SNARK proof and verification are generated for both sides in forward propagation and backward propagation on the server side, guaranteeing verifiability on both sides. The verifiable split learning architecture is compared to a blockchain-enabled system for the same deep learning network, one that records updates but without generating the zero-knowledge proof. From the comparison, it can be deduced that applying the zk-SNARK test achieves verifiability and correctness, while blockchains are lightweight but unverifiable.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConneX: Automatically Resolving Transaction Opacity of Cross-Chain Bridges for Security Analysis</title>
<link>https://arxiv.org/abs/2511.01393</link>
<guid>https://arxiv.org/abs/2511.01393</guid>
<content:encoded><![CDATA[
arXiv:2511.01393v1 Announce Type: new 
Abstract: As the Web3 ecosystem evolves toward a multi-chain architecture, cross-chain bridges have become critical infrastructure for enabling interoperability between diverse blockchain networks. However, while connecting isolated blockchains, the lack of cross-chain transaction pairing records introduces significant challenges for security analysis like cross-chain fund tracing, advanced vulnerability detection, and transaction graph-based analysis. To address this gap, we introduce ConneX, an automated and general-purpose system designed to accurately identify corresponding transaction pairs across both ends of cross-chain bridges. Our system leverages Large Language Models (LLMs) to efficiently prune the semantic search space by identifying semantically plausible key information candidates within complex transaction records. Further, it deploys a novel examiner module that refines these candidates by validating them against transaction values, effectively addressing semantic ambiguities and identifying the correct semantics. Extensive evaluations on a dataset of about 500,000 transactions from five major bridge platforms demonstrate that ConneX achieves an average F1 score of 0.9746, surpassing baselines by at least 20.05\%, with good efficiency that reduces the semantic search space by several orders of magnitude (1e10 to less than 100). Moreover, its successful application in tracing illicit funds (including a cross-chain transfer worth $1 million) in real-world hacking incidents underscores its practical utility for enhancing cross-chain security and transparency.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Designing for Distributed Heterogeneous Modularity: On Software Architecture and Deployment of MoonBots</title>
<link>https://arxiv.org/abs/2511.01437</link>
<guid>https://arxiv.org/abs/2511.01437</guid>
<content:encoded><![CDATA[
arXiv:2511.01437v1 Announce Type: new 
Abstract: This paper presents the software architecture and deployment strategy behind the MoonBot platform: a modular space robotic system composed of heterogeneous components distributed across multiple computers, networks and ultimately celestial bodies. We introduce a principled approach to distributed, heterogeneous modularity, extending modular robotics beyond physical reconfiguration to software, communication and orchestration. We detail the architecture of our system that integrates component-based design, a data-oriented communication model using ROS2 and Zenoh, and a deployment orchestrator capable of managing complex multi-module assemblies. These abstractions enable dynamic reconfiguration, decentralized control, and seamless collaboration between numerous operators and modules. At the heart of this system lies our open-source Motion Stack software, validated by months of field deployment with self-assembling robots, inter-robot cooperation, and remote operation. Our architecture tackles the significant hurdles of modular robotics by significantly reducing integration and maintenance overhead, while remaining scalable and robust. Although tested with space in mind, we propose generalizable patterns for designing robotic systems that must scale across time, hardware, teams and operational environments.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FTSmartAudit: A Knowledge Distillation-Enhanced Framework for Automated Smart Contract Auditing Using Fine-Tuned LLMs</title>
<link>https://arxiv.org/abs/2410.13918</link>
<guid>https://arxiv.org/abs/2410.13918</guid>
<content:encoded><![CDATA[
arXiv:2410.13918v3 Announce Type: replace 
Abstract: The rapid growth of blockchain technology has driven the widespread adoption of smart contracts. However, their inherent vulnerabilities have led to significant financial losses. Traditional auditing methods, while essential, struggle to keep pace with the increasing complexity and scale of smart contracts. Large Language Models (LLMs) offer promising capabilities for automating vulnerability detection, but their adoption is often limited by high computational costs. Although prior work has explored leveraging large models through agents or workflows, relatively little attention has been given to improving the performance of smaller, fine-tuned models--a critical factor for achieving both efficiency and data privacy. In this paper, we introduce HKT-SmartAudit, a framework for developing lightweight models optimized for smart contract auditing. It features a multi-stage knowledge distillation pipeline that integrates classical distillation, external domain knowledge, and reward-guided learning to transfer high-quality insights from large teacher models. A single-task learning strategy is employed to train compact student models that maintain high accuracy and robustness while significantly reducing computational overhead. Experimental results show that our distilled models outperform both commercial tools and larger models in detecting complex vulnerabilities and logical flaws, offering a practical, secure, and scalable solution for smart contract auditing. The source code is available at Github repository.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantitative Parameter Conditions for Stability and Coupling in GFM-GFL Converter Hybrid Systems from a Small-Signal Synchronous Perspective</title>
<link>https://arxiv.org/abs/2510.26971</link>
<guid>https://arxiv.org/abs/2510.26971</guid>
<content:encoded><![CDATA[
arXiv:2510.26971v1 Announce Type: new 
Abstract: With the development of renewable energy sources, power systems are gradually evolving into a system comprising both grid-forming (GFM) and grid-following (GFL) converters. However, the dynamic interaction between the two types of converters, especially low-inertia GFM converters and GFL converters, remains unclear due to the substantial differences in their synchronization mechanisms. To address this gap, this paper develops a small-signal synchronous stability model for power systems containing GFM and GFL converters, which considers network line dynamics. Based on subspace perturbation theory, we reveal that GFM and GFL subsystems can be effectively decoupled when GFL converters operate near unity power factor or when GFM converters possess sufficiently large inertia or damping, and provide lower bound of control parameters ensuring decoupling. Under the decoupling condition, we propose decentralized and analytical parameter-based stability criteria which have clear physical interpretations: the positive damping of converters compensates for the negative damping of the network. In the case of coupling, we also propose decentralized stability criteria based on the small phase theorem. The effectiveness of the theoretical analysis is validated through simulations in MATLAB/Simulink.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Study of Cluster-Based Routing Based on Machine Learning for UAV Networks in 6G</title>
<link>https://arxiv.org/abs/2510.27121</link>
<guid>https://arxiv.org/abs/2510.27121</guid>
<content:encoded><![CDATA[
arXiv:2510.27121v1 Announce Type: new 
Abstract: The sixth generation (6G) wireless networks are envisioned to deliver ultra-low latency, massive connectivity, and high data rates, enabling advanced applications such as autonomous {unmaned aerial vehicles (UAV)} swarms and aerial edge computing. However, realizing this vision in Flying Ad Hoc Networks (FANETs) requires intelligent and adaptive clustering mechanisms to ensure efficient routing and resource utilization. This paper proposes a novel machine learning-driven framework for dynamic cluster formation and cluster head selection in 6G-enabled FANETs. The system leverages mobility prediction using {Extreme Gradient Boosting (XGBoost)} and a composite optimization strategy based on signal strength and spatial proximity to identify optimal cluster heads. To evaluate the proposed method, comprehensive simulations were conducted in both centralized (5G) and decentralized (6G) topologies using realistic video traffic patterns. Results show that the proposed model achieves significant improvements in delay, jitter, and throughput in decentralized scenarios. These findings demonstrate the potential of combining machine learning with clustering techniques to enhance scalability, stability, and performance in next-generation aerial networks.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedSM: Robust Semantics-Guided Feature Mixup for Bias Reduction in Federated Learning with Long-Tail Data</title>
<link>https://arxiv.org/abs/2510.27240</link>
<guid>https://arxiv.org/abs/2510.27240</guid>
<content:encoded><![CDATA[
arXiv:2510.27240v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative model training across decentralized clients without sharing private data. However, FL suffers from biased global models due to non-IID and long-tail data distributions. We propose \textbf{FedSM}, a novel client-centric framework that mitigates this bias through semantics-guided feature mixup and lightweight classifier retraining. FedSM uses a pretrained image-text-aligned model to compute category-level semantic relevance, guiding the category selection of local features to mix-up with global prototypes to generate class-consistent pseudo-features. These features correct classifier bias, especially when data are heavily skewed. To address the concern of potential domain shift between the pretrained model and the data, we propose probabilistic category selection, enhancing feature diversity to effectively mitigate biases. All computations are performed locally, requiring minimal server overhead. Extensive experiments on long-tail datasets with various imbalanced levels demonstrate that FedSM consistently outperforms state-of-the-art methods in accuracy, with high robustness to domain shift and computational efficiency.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Integration of cross platform functions onto service-oriented architectures</title>
<link>https://arxiv.org/abs/2510.27344</link>
<guid>https://arxiv.org/abs/2510.27344</guid>
<content:encoded><![CDATA[
arXiv:2510.27344v1 Announce Type: new 
Abstract: The automotive industry is currently undergoing a major transformation with respect to the Electric/Electronic (E/E) and software architecture, driven by a significant increase in the complexity of the technological stack within a vehicle. This complexity acts as a driving force for Software-Defined Vehicles (SDVs) leading to the evolution of the automotive E/E architectures from decentralized configuration comprising multiple Electronic Control Units (ECUs) towards a more integrated configuration comprising a smaller number of ECUs, domain controllers, gateways, and High-Performance Computers (HPCs) [2]. This transition along with several other reasons have resulted in heterogeneous software platforms such as AUTOSAR Classic, AUTOSAR Adaptive, and prototypical frameworks like ROS 2. It is therefore essential to develop applications that are both hardware- and platform/middleware-agnostic to attain development and integration efficiency. This work presents an application development and integration concept to facilitate developing applications as Software as a Product (SaaP), while simultaneously ensuring efficient integration onto multiple software architecture platforms. The concept involves designing applications in a hardware- and software platform-agnostic manner and standardizing application interfaces [6]. It also includes describing the relevant aspects of the application and corresponding middleware in a machine-readable format to aid the integration of developed applications. Additionally, tools are developed to facilitate semi-automation of the development and integration processes. An example application has been developed and integrated onto AUTOSAR Adaptive and ROS 2, demonstrating the applicability of the approach. Finally, metrics are presented to show the efficiency of the overall concept.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Challenging Tribal Knowledge -- Large Scale Measurement Campaign on Decentralized NAT Traversal</title>
<link>https://arxiv.org/abs/2510.27500</link>
<guid>https://arxiv.org/abs/2510.27500</guid>
<content:encoded><![CDATA[
arXiv:2510.27500v1 Announce Type: new 
Abstract: The promise of decentralized peer-to-peer (P2P) systems is fundamentally gated by the challenge of Network Address Translation (NAT) traversal, with existing solutions often reintroducing the very centralization they seek to avoid. This paper presents the first large-scale, longitudinal measurement study of a fully decentralized NAT traversal protocol, Direct Connection Upgrade through Relay (DCUtR), within the production libp2p-based IPFS network. Drawing on over 4.4 million traversal attempts from 85,000+ distinct networks across 167 countries, we provide a definitive empirical analysis of modern P2P connectivity. We establish a contemporary baseline success rate of $70\% \pm 7.1\%$ for the hole-punching stage, providing a crucial new benchmark for the field. Critically, we empirically refute the long-held 'tribal knowledge' of UDP's superiority for NAT traversal, demonstrating that DCUtR's high-precision, RTT-based synchronization yields statistically indistinguishable success rates for both TCP and QUIC ($\sim70\%$). Our analysis further validates the protocol's design for permissionless environments by showing that success is independent of relay characteristics and that the mechanism is highly efficient, with $97.6\%$ of successful connections established on the first attempt. Building on this analysis, we propose a concrete roadmap of protocol enhancements aimed at achieving universal connectivity and contribute our complete dataset to foster further research in this domain.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asynchronous Risk-Aware Multi-Agent Packet Routing for Ultra-Dense LEO Satellite Networks</title>
<link>https://arxiv.org/abs/2510.27506</link>
<guid>https://arxiv.org/abs/2510.27506</guid>
<content:encoded><![CDATA[
arXiv:2510.27506v1 Announce Type: new 
Abstract: The rise of ultra-dense LEO constellations creates a complex and asynchronous network environment, driven by their massive scale, dynamic topologies, and significant delays. This unique complexity demands an adaptive packet routing algorithm that is asynchronous, risk-aware, and capable of balancing diverse and often conflicting QoS objectives in a decentralized manner. However, existing methods fail to address this need, as they typically rely on impractical synchronous decision-making and/or risk-oblivious approaches. To tackle this gap, we introduce PRIMAL, an event-driven multi-agent routing framework designed specifically to allow each satellite to act independently on its own event-driven timeline, while managing the risk of worst-case performance degradation via a principled primal-dual approach. This is achieved by enabling agents to learn the full cost distribution of the targeted QoS objectives and constrain tail-end risks. Extensive simulations on a LEO constellation with 1584 satellites validate its superiority in effectively optimizing latency and balancing load. Compared to a recent risk-oblivious baseline, it reduces queuing delay by over 70%, and achieves a nearly 12 ms end-to-end delay reduction in loaded scenarios. This is accomplished by resolving the core conflict between naive shortest-path finding and congestion avoidance, highlighting such autonomous risk-awareness as a key to robust routing.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Decentralized Optimization: Leveraging Both First- and Zeroth-Order Optimizers for Faster Convergence</title>
<link>https://arxiv.org/abs/2210.07703</link>
<guid>https://arxiv.org/abs/2210.07703</guid>
<content:encoded><![CDATA[
arXiv:2210.07703v4 Announce Type: replace 
Abstract: Distributed optimization is the standard way of speeding up machine learning training, and most of the research in the area focuses on distributed first-order, gradient-based methods. Yet, there are settings where some computationally-bounded nodes may not be able to implement first-order, gradient-based optimization, while they could still contribute to joint optimization tasks. In this paper, we initiate the study of hybrid decentralized optimization, studying settings where nodes with zeroth-order and first-order optimization capabilities co-exist in a distributed system, and attempt to jointly solve an optimization task over some data distribution. We essentially show that, under reasonable parameter settings, such a system can not only withstand noisier zeroth-order agents but can even benefit from integrating such agents into the optimization process, rather than ignoring their information. At the core of our approach is a new analysis of distributed optimization with noisy and possibly-biased gradient estimators, which may be of independent interest. Our results hold for both convex and non-convex objectives. Experimental results on standard optimization tasks confirm our analysis, showing that hybrid first-zeroth order optimization can be practical, even when training deep neural networks.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Partitioning and Observability in Linear Systems via Submodular Optimization</title>
<link>https://arxiv.org/abs/2505.16169</link>
<guid>https://arxiv.org/abs/2505.16169</guid>
<content:encoded><![CDATA[
arXiv:2505.16169v2 Announce Type: replace 
Abstract: Network partitioning has gained recent attention as a pathway to enable decentralized operation and control in large-scale systems. This paper addresses the interplay between partitioning, observability, and sensor placement (SP) in dynamic networks. The problem, being computationally intractable at scale, is a largely unexplored, open problem in the literature. To that end, the paper's objective is designing scalable partitioning of linear systems while maximizing observability metrics of the subsystems. We show that the partitioning problem can be posed as a submodular maximization problem -- and the SP problem can subsequently be solved over the partitioned network. Consequently, theoretical bounds are derived to compare observability metrics of the original network with those of the resulting partitions, highlighting the impact of partitioning on system observability. Case studies on networks of varying sizes corroborate the derived theoretical bounds.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On-device Computation of Single-lead ECG Parameters for Real-time Remote Cardiac Health Assessment: A Real-world Validation Study</title>
<link>https://arxiv.org/abs/2502.17499</link>
<guid>https://arxiv.org/abs/2502.17499</guid>
<content:encoded><![CDATA[
arXiv:2502.17499v3 Announce Type: replace-cross 
Abstract: Accurate, continuous out-of-hospital electrocardiogram (ECG) parameter measurement is vital for real-time cardiac health monitoring and telemedicine. On-device computation of single-lead ECG parameters enables timely assessment without reliance on centralized data processing, advancing personalized, ubiquitous cardiac care-yet comprehensive validation across heterogeneous real-world populations remains limited. This study validated the on-device algorithm FeatureDB (https://github.com/PKUDigitalHealth/FeatureDB) using two datasets: HeartVoice-ECG-lite (369 participants with single-lead ECGs annotated by two physicians) and PTB-XL/PTB-XL+ (21,354 patients with 12-lead ECGs and physicians' diagnostic annotations). FeatureDB computed PR, QT, and QTc intervals, with accuracy evaluated against physician annotations via mean absolute error (MAE), correlation analysis, and Bland-Altman analysis. Diagnostic performance for first-degree atrioventricular block (AVBI, PR-based) and long QT syndrome (LQT, QTc-based) was benchmarked against commercial 12-lead systems (12SL, Uni-G) and open-source algorithm Deli, using AUC, accuracy, sensitivity, and specificity. Results showed high concordance with expert annotations (Pearson correlations: 0.836-0.960), MAEs matching inter-observer variability, and minimal bias. AVBI AUC reached 0.787 (12SL: 0.859; Uni-G: 0.812; Deli: 0.501); LQT AUC was 0.684 (12SL: 0.716; Uni-G: 0.605; Deli: 0.569)-comparable to commercial tools and superior to open-source alternatives. FeatureDB delivers physician-level parameter accuracy and commercial-grade abnormality detection via single-lead devices, supporting scalable telemedicine, decentralized cardiac screening, and continuous monitoring in community and outpatient settings.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AAGATE: A NIST AI RMF-Aligned Governance Platform for Agentic AI</title>
<link>https://arxiv.org/abs/2510.25863</link>
<guid>https://arxiv.org/abs/2510.25863</guid>
<content:encoded><![CDATA[
arXiv:2510.25863v1 Announce Type: new 
Abstract: This paper introduces the Agentic AI Governance Assurance & Trust Engine (AAGATE), a Kubernetes-native control plane designed to address the unique security and governance challenges posed by autonomous, language-model-driven agents in production. Recognizing the limitations of traditional Application Security (AppSec) tooling for improvisational, machine-speed systems, AAGATE operationalizes the NIST AI Risk Management Framework (AI RMF). It integrates specialized security frameworks for each RMF function: the Agentic AI Threat Modeling MAESTRO framework for Map, a hybrid of OWASP's AIVSS and SEI's SSVC for Measure, and the Cloud Security Alliance's Agentic AI Red Teaming Guide for Manage. By incorporating a zero-trust service mesh, an explainable policy engine, behavioral analytics, and decentralized accountability hooks, AAGATE provides a continuous, verifiable governance solution for agentic AI, enabling safe, accountable, and scalable deployment. The framework is further extended with DIRF for digital identity rights, LPCI defenses for logic-layer injection, and QSAF monitors for cognitive degradation, ensuring governance spans systemic, adversarial, and ethical risks.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Engineering Social Optimality via Utility Shaping in Non-Cooperative Games under Incomplete Information and Imperfect Monitoring</title>
<link>https://arxiv.org/abs/2510.26033</link>
<guid>https://arxiv.org/abs/2510.26033</guid>
<content:encoded><![CDATA[
arXiv:2510.26033v1 Announce Type: new 
Abstract: In this paper, we study decentralized decision-making where agents optimize private objectives under incomplete information and imperfect public monitoring, in a non-cooperative setting. By shaping utilities-embedding shadow prices or Karush-Kuhn-Tucker(KKT)-aligned penalties-we make the stage game an exact-potential game whose unique equilibrium equals the (possibly constrained) social optimum. We characterize the Bayesian equilibrium as a stochastic variational inequality; strong monotonicity follows from a single-inflection compressed/stretched-exponential response combined with convex pricing. We give tracking bounds for damped-gradient and best-response-with-hysteresis updates under a noisy public index, and corresponding steady-state error. The framework accommodates discrete and continuous action sets and composes with slower discrete assignment. Deployable rules include: embed prices/penalties; publish a single public index; tune steps, damping, and dual rates for contraction. Computational experiments cover (i) a multi-tier supply chain and (ii) a non-cooperative agentic-AI compute market of bidding bots. Relative to price-only baselines, utility shaping attains near-centralized welfare, eliminates steady-state constraint/capacity violations when feasible, and accelerates convergence; with quantization, discrete equilibria track continuous ones within the mesh. The blueprint is portable to demand response, cloud/edge scheduling, and transportation pricing and biosecurity/agriculture. Overall, utility shaping plus a public index implements the constrained social optimum with stable equilibria under noise and drift-an operations-research-friendly alternative to heavy messaging or full mechanism design.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle Routing</title>
<link>https://arxiv.org/abs/2510.26089</link>
<guid>https://arxiv.org/abs/2510.26089</guid>
<content:encoded><![CDATA[
arXiv:2510.26089v1 Announce Type: new 
Abstract: Traffic congestion in urban road networks leads to longer trip times and higher emissions, especially during peak periods. While the Shortest Path First (SPF) algorithm is optimal for a single vehicle in a static network, it performs poorly in dynamic, multi-vehicle settings, often worsening congestion by routing all vehicles along identical paths. We address dynamic vehicle routing through a multi-agent reinforcement learning (MARL) framework for coordinated, network-aware fleet navigation. We first propose Adaptive Navigation (AN), a decentralized MARL model where each intersection agent provides routing guidance based on (i) local traffic and (ii) neighborhood state modeled using Graph Attention Networks (GAT). To improve scalability in large networks, we further propose Hierarchical Hub-based Adaptive Navigation (HHAN), an extension of AN that assigns agents only to key intersections (hubs). Vehicles are routed hub-to-hub under agent control, while SPF handles micro-routing within each hub region. For hub coordination, HHAN adopts centralized training with decentralized execution (CTDE) under the Attentive Q-Mixing (A-QMIX) framework, which aggregates asynchronous vehicle decisions via attention. Hub agents use flow-aware state features that combine local congestion and predictive dynamics for proactive routing. Experiments on synthetic grids and real urban maps (Toronto, Manhattan) show that AN reduces average travel time versus SPF and learning baselines, maintaining 100% routing success. HHAN scales to networks with hundreds of intersections, achieving up to 15.9% improvement under heavy traffic. These findings highlight the potential of network-constrained MARL for scalable, coordinated, and congestion-aware routing in intelligent transportation systems.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PEEL: A Poisoning-Exposing Encoding Theoretical Framework for Local Differential Privacy</title>
<link>https://arxiv.org/abs/2510.26102</link>
<guid>https://arxiv.org/abs/2510.26102</guid>
<content:encoded><![CDATA[
arXiv:2510.26102v1 Announce Type: new 
Abstract: Local Differential Privacy (LDP) is a widely adopted privacy-protection model in the Internet of Things (IoT) due to its lightweight, decentralized, and scalable nature. However, it is vulnerable to poisoning attacks, and existing defenses either incur prohibitive resource overheads or rely on domain-specific prior knowledge, limiting their practical deployment. To address these limitations, we propose PEEL, a Poisoning-Exposing Encoding theoretical framework for LDP, which departs from resource- or prior-dependent countermeasures and instead leverages the inherent structural consistency of LDP-perturbed data. As a non-intrusive post-processing module, PEEL amplifies stealthy poisoning effects by re-encoding LDP-perturbed data via sparsification, normalization, and low-rank projection, thereby revealing both output and rule poisoning attacks through structural inconsistencies in the reconstructed space. Theoretical analysis proves that PEEL, integrated with LDP, retains unbiasedness and statistical accuracy, while being robust to expose both output and rule poisoning attacks. Moreover, evaluation results show that LDP-integrated PEEL not only outperforms four state-of-the-art defenses in terms of poisoning exposure accuracy but also significantly reduces client-side computational costs, making it highly suitable for large-scale IoT deployments.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Context Length Optimization with Low-Frequency Truncation for Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.26389</link>
<guid>https://arxiv.org/abs/2510.26389</guid>
<content:encoded><![CDATA[
arXiv:2510.26389v1 Announce Type: new 
Abstract: Recently, deep multi-agent reinforcement learning (MARL) has demonstrated promising performance for solving challenging tasks, such as long-term dependencies and non-Markovian environments. Its success is partly attributed to conditioning policies on large fixed context length. However, such large fixed context lengths may lead to limited exploration efficiency and redundant information. In this paper, we propose a novel MARL framework to obtain adaptive and effective contextual information. Specifically, we design a central agent that dynamically optimizes context length via temporal gradient analysis, enhancing exploration to facilitate convergence to global optima in MARL. Furthermore, to enhance the adaptive optimization capability of the context length, we present an efficient input representation for the central agent, which effectively filters redundant information. By leveraging a Fourier-based low-frequency truncation method, we extract global temporal trends across decentralized agents, providing an effective and efficient representation of the MARL environment. Extensive experiments demonstrate that the proposed method achieves state-of-the-art (SOTA) performance on long-term dependency tasks, including PettingZoo, MiniGrid, Google Research Football (GRF), and StarCraft Multi-Agent Challenge v2 (SMACv2).
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Pragmatic View of AI Personhood</title>
<link>https://arxiv.org/abs/2510.26396</link>
<guid>https://arxiv.org/abs/2510.26396</guid>
<content:encoded><![CDATA[
arXiv:2510.26396v1 Announce Type: new 
Abstract: The emergence of agentic Artificial Intelligence (AI) is set to trigger a "Cambrian explosion" of new kinds of personhood. This paper proposes a pragmatic framework for navigating this diversification by treating personhood not as a metaphysical property to be discovered, but as a flexible bundle of obligations (rights and responsibilities) that societies confer upon entities for a variety of reasons, especially to solve concrete governance problems. We argue that this traditional bundle can be unbundled, creating bespoke solutions for different contexts. This will allow for the creation of practical tools -- such as facilitating AI contracting by creating a target "individual" that can be sanctioned -- without needing to resolve intractable debates about an AI's consciousness or rationality. We explore how individuals fit in to social roles and discuss the use of decentralized digital identity technology, examining both "personhood as a problem", where design choices can create "dark patterns" that exploit human social heuristics, and "personhood as a solution", where conferring a bundle of obligations is necessary to ensure accountability or prevent conflict. By rejecting foundationalist quests for a single, essential definition of personhood, this paper offers a more pragmatic and flexible way to think about integrating AI agents into our society.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Bidding and Coordinated Dispatch of Hybrid Energy Systems in Regulation Markets</title>
<link>https://arxiv.org/abs/2510.26602</link>
<guid>https://arxiv.org/abs/2510.26602</guid>
<content:encoded><![CDATA[
arXiv:2510.26602v1 Announce Type: new 
Abstract: The increasing integration of renewable energy sources and distributed energy resources (DER) into modern power systems introduces significant uncertainty, posing challenges for maintaining grid flexibility and reliability. Hybrid energy systems (HES), composed of controllable generators, flexible loads, and battery storage, offer a decentralized solution to enhance flexibility compared to single centralized resources. This paper presents a two-level framework to enable HES participation in frequency regulation markets. The upper level performs a chance-constrained optimization to choose capacity bids based on historical regulation signals. At the lower level, a real-time control strategy disaggregates the regulation power among the constituent resources. This real-time control strategy is then benchmarked against an offline optimal dispatch to evaluate flexibility performance. Additionally, the framework evaluates the profitability of overbidding strategies and identifies thresholds beyond which performance degradation may lead to market penalties or disqualification. The proposed framework also compare the impact of imbalance of power capacities on performance and battery state of charge (SoC) through asymmetric HES configurations.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph approach for observability analysis in power system dynamic state estimation</title>
<link>https://arxiv.org/abs/2510.26701</link>
<guid>https://arxiv.org/abs/2510.26701</guid>
<content:encoded><![CDATA[
arXiv:2510.26701v1 Announce Type: new 
Abstract: The proposed approach yields a numerical method that provably executes in linear time with respect to the number of nodes and edges in a graph. The graph, constructed from the power system model, requires only knowledge of the dependencies between state-to-state and output-to-state variables within a state-space framework. While graph-based observability analysis methods exist for power system static-state estimation, the approach presented here is the first for dynamic-state estimation (DSE). We examine decentralized and centralized DSE scenarios and compare our findings with a well-established, albeit non-scalable, observability analysis method in the literature. When compared to the latter in a centralized DSE setting, our method reduced computation time by 1440x.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guided Model Merging for Hybrid Data Learning: Leveraging Centralized Data to Refine Decentralized Models</title>
<link>https://arxiv.org/abs/2503.20138</link>
<guid>https://arxiv.org/abs/2503.20138</guid>
<content:encoded><![CDATA[
arXiv:2503.20138v2 Announce Type: replace 
Abstract: Current network training paradigms primarily focus on either centralized or decentralized data regimes. However, in practice, data availability often exhibits a hybrid nature, where both regimes coexist. This hybrid setting presents new opportunities for model training, as the two regimes offer complementary trade-offs: decentralized data is abundant but subject to heterogeneity and communication constraints, while centralized data, though limited in volume and potentially unrepresentative, enables better curation and high-throughput access. Despite its potential, effectively combining these paradigms remains challenging, and few frameworks are tailored to hybrid data regimes. To address this, we propose a novel framework that constructs a model atlas from decentralized models and leverages centralized data to refine a global model within this structured space. The refined model is then used to reinitialize the decentralized models. Our method synergizes federated learning (to exploit decentralized data) and model merging (to utilize centralized data), enabling effective training under hybrid data availability. Theoretically, we show that our approach achieves faster convergence than methods relying solely on decentralized data, due to variance reduction in the merging process. Extensive experiments demonstrate that our framework consistently outperforms purely centralized, purely decentralized, and existing hybrid-adaptable methods. Notably, our method remains robust even when the centralized and decentralized data domains differ or when decentralized data contains noise, significantly broadening its applicability.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiple Proposer Transaction Fee Mechanism Design: Robust Incentives Against Censorship and Bribery</title>
<link>https://arxiv.org/abs/2505.13751</link>
<guid>https://arxiv.org/abs/2505.13751</guid>
<content:encoded><![CDATA[
arXiv:2505.13751v2 Announce Type: replace 
Abstract: Censorship resistance is one of the core value proposition of blockchains. A recurring design pattern aimed at providing censorship resistance is enabling multiple proposers to contribute inputs into block construction. Notably, Fork-Choice Enforced Inclusion Lists (FOCIL) is proposed to be included in Ethereum. However, the current proposal relies on altruistic behavior, without a Transaction Fee Mechanism (TFM). This study aims to address this gap by exploring how multiple proposers should be rewarded to incentivize censorship resistance. The main contribution of this work is the identification of TFMs that ensure censorship resistance under bribery attacks, while also satisfying the incentive compatibility properties of EIP-1559. We provide a concrete payment mechanism for FOCIL, along with generalizable contributions to the literature by analyzing 1) incentive compatibility of TFMs in the presence of a bribing adversary, 2) TFMs in protocols with multiple phases of transaction inclusion, and 3) TFMs of protocols in which parties are uncertain about the behavior and the possible bribe of others.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Formal Verification of a Token Sale Launchpad: A Compositional Approach in Dafny</title>
<link>https://arxiv.org/abs/2510.24798</link>
<guid>https://arxiv.org/abs/2510.24798</guid>
<content:encoded><![CDATA[
arXiv:2510.24798v1 Announce Type: new 
Abstract: The proliferation of decentralized financial (DeFi) systems and smart contracts has underscored the critical need for software correctness. Bugs in such systems can lead to catastrophic financial losses. Formal verification offers a path to achieving mathematical certainty about software behavior. This paper presents the formal verification of the core logic for a token sale launchpad, implemented and proven correct using the Dafny programming language and verification system. We detail a compositional, bottom-up verification strategy, beginning with the proof of fundamental non-linear integer arithmetic properties, and building upon them to verify complex business logic, including asset conversion, time-based discounts, and capped-sale refund mechanics. The principal contributions are the formal proofs of critical safety and lifecycle properties. Most notably, we prove that refunds in a capped sale can never exceed the user's original deposit amount, and that the precision loss in round-trip financial calculations is strictly bounded. Furthermore, we verify the complete lifecycle logic, including user withdrawals under various sale mechanics and the correctness of post-sale token allocation, vesting, and claiming. This work serves as a comprehensive case study in applying rigorous verification techniques to build high-assurance financial software.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fortytwo: Swarm Inference with Peer-Ranked Consensus</title>
<link>https://arxiv.org/abs/2510.24801</link>
<guid>https://arxiv.org/abs/2510.24801</guid>
<content:encoded><![CDATA[
arXiv:2510.24801v1 Announce Type: new 
Abstract: As centralized AI hits compute ceilings and diminishing returns from ever-larger training runs, meeting demand requires an inference layer that scales horizontally in both capacity and capability. We present Fortytwo, a novel protocol that leverages swarm intelligence principles and distributed pairwise ranking consensus to achieve superior performance in AI inference. Our approach reimagines collaboration among AI nodes using swarm inference: a peer-ranked, reputation-weighted consensus across heterogeneous models that surfaces the highest-quality responses. Using pairwise ranking with a custom Bradley-Terry-style aggregation model, we demonstrate that swarm inference substantially outperforms majority voting, achieving 85.90% on GPQA Diamond versus 68.69% for majority voting with the same model set - an improvement of +17.21 percentage points (approximately +25.1% relative). The protocol incorporates on-chain reputation so node influence adapts to demonstrated accuracy over time, yielding a meritocratic consensus that filters low-quality or malicious participants. To resist Sybil attacks, Fortytwo employs proof-of-capability in its consensus: nodes must successfully complete calibration/test requests and stake reputation to enter ranking rounds, making multi-identity attacks economically unattractive while preserving openness. Across six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and AIME, our evaluation indicates higher accuracy and strong resilience to adversarial and noisy free-form prompting (e.g., prompt-injection degradation of only 0.12% versus 6.20% for a monolithic single-model baseline), while retaining practical deployability. Together, these results establish a foundation for decentralized AI systems - democratizing access to high-quality inference through collective intelligence without sacrificing reliability or security.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Merging Control of Connected and Automated Vehicles to Enhance Safety and Energy Efficiency using Control Barrier Functions</title>
<link>https://arxiv.org/abs/2510.24871</link>
<guid>https://arxiv.org/abs/2510.24871</guid>
<content:encoded><![CDATA[
arXiv:2510.24871v1 Announce Type: new 
Abstract: This paper presents a decentralized Control Barrier Function (CBF) based approach for highway merging of Connected and Automated Vehicles (CAVs). In this control algorithm, each "host" vehicle negotiates with other agents in a control zone of the highway network, and enacts its own action, to perform safe and energy-efficient merge maneuvers. It uses predictor-corrector loops within the robust CBF setting for negotiation and to reconcile disagreements that may arise. There is no explicit order of vehicles and no priority. A notable feature is absence of gridlocks due to instability of the inter-agent system. Results from Monte Carlo simulations show significant improvement in the system-wide energy efficiency and traffic flow compared to a first-in-first-out approach, as well as enhanced robustness of the proposed decentralized controller compared to its centralized counterpart.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Timing Games in Responsive Consensus Protocols</title>
<link>https://arxiv.org/abs/2510.25144</link>
<guid>https://arxiv.org/abs/2510.25144</guid>
<content:encoded><![CDATA[
arXiv:2510.25144v1 Announce Type: new 
Abstract: Optimistic responsiveness -- the ability of a consensus protocol to operate at the speed of the network -- is widely used in consensus protocol design to optimize latency and throughput. However, blockchain applications incentivize validators to play timing games by strategically delaying their proposals, since increased block time correlates with greater rewards. Consequently, it may appear that responsiveness (even under optimistic conditions) is impossible in blockchain protocols. In this work, we develop a model of timing games in responsive consensus protocols and find a prisoner's dilemma structure, where cooperation (proposing promptly) is in the validators' best interest, but individual incentives encourage validators to delay proposals selfishly. To attain desirable equilibria, we introduce dynamic block rewards that decrease with round time to explicitly incentivize faster proposals. Delays are measured through a voting mechanism, where other validators vote on the current leader's round time. By carefully setting the protocol parameters, the voting mechanism allows validators to coordinate and reach the cooperative equilibrium, benefiting all through a higher rate-of-reward. Thus, instead of responsiveness being an unattainable property due to timing games, we show that responsiveness itself can promote faster block proposals. One consequence of moving from a static to dynamic block reward is that validator utilities become more sensitive to latency, worsening the gap between the best- and worst-connected validators. Our analysis shows, however, that this effect is minor in both theoretical latency models and simulations based on real-world networks.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study on Privacy-Preserving Scholarship Evaluation Based on Decentralized Identity and Zero-Knowledge Proofs</title>
<link>https://arxiv.org/abs/2510.25477</link>
<guid>https://arxiv.org/abs/2510.25477</guid>
<content:encoded><![CDATA[
arXiv:2510.25477v1 Announce Type: new 
Abstract: Traditional centralized scholarship evaluation processes typically require students to submit detailed academic records and qualification information, which exposes them to risks of data leakage and misuse, making it difficult to simultaneously ensure privacy protection and transparent auditability. To address these challenges, this paper proposes a scholarship evaluation system based on Decentralized Identity (DID) and Zero-Knowledge Proofs (ZKP). The system aggregates multidimensional ZKPs off-chain, and smart contracts verify compliance with evaluation criteria without revealing raw scores or computational details. Experimental results demonstrate that the proposed solution not only automates the evaluation efficiently but also maximally preserves student privacy and data integrity, offering a practical and trustworthy technical paradigm for higher education scholarship programs.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incorporating Social Awareness into Control of Unknown Multi-Agent Systems: A Real-Time Spatiotemporal Tubes Approach</title>
<link>https://arxiv.org/abs/2510.25597</link>
<guid>https://arxiv.org/abs/2510.25597</guid>
<content:encoded><![CDATA[
arXiv:2510.25597v1 Announce Type: new 
Abstract: This paper presents a decentralized control framework that incorporates social awareness into multi-agent systems with unknown dynamics to achieve prescribed-time reach-avoid-stay tasks in dynamic environments. Each agent is assigned a social awareness index that quantifies its level of cooperation or self-interest, allowing heterogeneous social behaviors within the system. Building on the spatiotemporal tube (STT) framework, we propose a real-time STT framework that synthesizes tubes online for each agent while capturing its social interactions with others. A closed-form, approximation-free control law is derived to ensure that each agent remains within its evolving STT, thereby avoiding dynamic obstacles while also preventing inter-agent collisions in a socially aware manner, and reaching the target within a prescribed time. The proposed approach provides formal guarantees on safety and timing, and is computationally lightweight, model-free, and robust to unknown disturbances. The effectiveness and scalability of the framework are validated through simulation and hardware experiments on a 2D omnidirectional
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaLore: Learning to Orchestrate Communication and Computation for Metaverse Synchronization</title>
<link>https://arxiv.org/abs/2510.25705</link>
<guid>https://arxiv.org/abs/2510.25705</guid>
<content:encoded><![CDATA[
arXiv:2510.25705v1 Announce Type: new 
Abstract: As augmented and virtual reality evolve, achieving seamless synchronization between physical and digital realms remains a critical challenge, especially for real-time applications where delays affect the user experience. This paper presents MetaLore, a Deep Reinforcement Learning (DRL) based framework for joint communication and computational resource allocation in Metaverse or digital twin environments. MetaLore dynamically shares the communication bandwidth and computational resources among sensors and mobile devices to optimize synchronization, while offering high throughput performance. Special treatment is given in satisfying end-to-end delay guarantees. A key contribution is the introduction of two novel Age of Information (AoI) metrics: Age of Request Information (AoRI) and Age of Sensor Information (AoSI), integrated into the reward function to enhance synchronization quality. An open source simulator has been extended to incorporate and evaluate the approach. The DRL solution is shown to achieve the performance of full-enumeration brute-force solutions by making use of a small, task-oriented observation space of two queue lengths at the network side. This allows the DRL approach the flexibility to effectively and autonomously adapt to dynamic traffic conditions.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Holon Streaming: Global Aggregations with Windowed CRDTs</title>
<link>https://arxiv.org/abs/2510.25757</link>
<guid>https://arxiv.org/abs/2510.25757</guid>
<content:encoded><![CDATA[
arXiv:2510.25757v1 Announce Type: new 
Abstract: Scaling global aggregations is a challenge for exactly-once stream processing systems. Current systems implement these either by computing the aggregation in a single task instance, or by static aggregation trees, which limits scalability and may become a bottleneck. Moreover, the end-to-end latency is determined by the slowest path in the tree, and failures and reconfiguration cause large latency spikes due to the centralized coordination. Towards these issues, we present Holon Streaming, an exactly-once stream processing system for global aggregations. Its deterministic programming model uses windowed conflict-free replicated data types (Windowed CRDTs), a novel abstraction for shared replicated state. Windowed CRDTs make computing global aggregations scalable. Furthermore, their guarantees such as determinism and convergence enable the design of efficient failure recovery algorithms by decentralized coordination. Our evaluation shows a 5x lower latency and 2x higher throughput than an existing stream processing system on global aggregation workloads, with an 11x latency reduction under failure scenarios. The paper demonstrates the effectiveness of decentralized coordination with determinism, and the utility of Windowed CRDTs for global aggregations.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed learning for automatic modulation recognition in bandwidth-limited networks</title>
<link>https://arxiv.org/abs/2510.24722</link>
<guid>https://arxiv.org/abs/2510.24722</guid>
<content:encoded><![CDATA[
arXiv:2510.24722v1 Announce Type: cross 
Abstract: Automatic Modulation Recognition (AMR) is critical in identifying various modulation types in wireless communication systems. Recent advancements in deep learning have facilitated the integration of algorithms into AMR techniques. However, this integration typically follows a centralized approach that necessitates collecting and processing all training data on high-powered computing devices, which may prove impractical for bandwidth-limited wireless networks. In response to this challenge, this study introduces two methods for distributed learning-based AMR on the collaboration of multiple receivers to perform AMR tasks. The TeMuRAMRD 2023 dataset is employed to support this investigation, uniquely suited for multi-receiver AMR tasks. Within this distributed sensing environment, multiple receivers collaborate in identifying modulation types from the same RF signal, each possessing a partial perspective of the overall environment. Experimental results demonstrate that the centralized-based AMR, with six receivers, attains an impressive accuracy rate of 91%, while individual receivers exhibit a notably lower accuracy, at around 41%. Nonetheless, the two proposed decentralized learning-based AMR methods exhibit noteworthy enhancements. Based on consensus voting among six receivers, the initial method achieves a marginally lower accuracy. It achieves this while substantially reducing the bandwidth demands to a 1/256th of the centralized model. With the second distributed method, each receiver shares its feature map, subsequently aggregated by a central node. This approach also accompanies a substantial bandwidth reduction of 1/8 compared to the centralized approach. These findings highlight the capacity of distributed AMR to significantly enhance accuracy while effectively addressing the constraints of bandwidth-limited wireless networks.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Partially Observable Multi-Agent Reinforcement Learning with Information Sharing</title>
<link>https://arxiv.org/abs/2308.08705</link>
<guid>https://arxiv.org/abs/2308.08705</guid>
<content:encoded><![CDATA[
arXiv:2308.08705v4 Announce Type: replace 
Abstract: We study provable multi-agent reinforcement learning (RL) in the general framework of partially observable stochastic games (POSGs). To circumvent the known hardness results and the use of computationally intractable oracles, we advocate leveraging the potential \emph{information-sharing} among agents, a common practice in empirical multi-agent RL, and a standard model for multi-agent control systems with communication. We first establish several computational complexity results to justify the necessity of information-sharing, as well as the observability assumption that has enabled quasi-polynomial time and sample single-agent RL with partial observations, for tractably solving POSGs. Inspired by the inefficiency of planning in the ground-truth model, we then propose to further \emph{approximate} the shared common information to construct an approximate model of the POSG, in which an approximate \emph{equilibrium} (of the original POSG) can be found in quasi-polynomial-time, under the aforementioned assumptions. Furthermore, we develop a partially observable multi-agent RL algorithm whose time and sample complexities are \emph{both} quasi-polynomial. Finally, beyond equilibrium learning, we extend our algorithmic framework to finding the \emph{team-optimal solution} in cooperative POSGs, i.e., decentralized partially observable Markov decision processes, a more challenging goal. We establish concrete computational and sample complexities under several structural assumptions of the model. We hope our study could open up the possibilities of leveraging and even designing different \emph{information structures}, a well-studied notion in control theory, for developing both sample- and computation-efficient partially observable multi-agent RL.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Deep Reinforcement Learning for Privacy-Preserving Robotic-Assisted Surgery</title>
<link>https://arxiv.org/abs/2505.12153</link>
<guid>https://arxiv.org/abs/2505.12153</guid>
<content:encoded><![CDATA[
arXiv:2505.12153v2 Announce Type: replace 
Abstract: The integration of Reinforcement Learning (RL) into robotic-assisted surgery (RAS) holds significant promise for advancing surgical precision, adaptability, and autonomous decision-making. However, the development of robust RL models in clinical settings is hindered by key challenges, including stringent patient data privacy regulations, limited access to diverse surgical datasets, and high procedural variability. To address these limitations, this paper presents a Federated Deep Reinforcement Learning (FDRL) framework that enables decentralized training of RL models across multiple healthcare institutions without exposing sensitive patient information. A central innovation of the proposed framework is its dynamic policy adaptation mechanism, which allows surgical robots to select and tailor patient-specific policies in real-time, thereby ensuring personalized and Optimised interventions. To uphold rigorous privacy standards while facilitating collaborative learning, the FDRL framework incorporates secure aggregation, differential privacy, and homomorphic encryption techniques. Experimental results demonstrate a 60\% reduction in privacy leakage compared to conventional methods, with surgical precision maintained within a 1.5\% margin of a centralized baseline. This work establishes a foundational approach for adaptive, secure, and patient-centric AI-driven surgical robotics, offering a pathway toward clinical translation and scalable deployment across diverse healthcare environments.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models</title>
<link>https://arxiv.org/abs/2510.23824</link>
<guid>https://arxiv.org/abs/2510.23824</guid>
<content:encoded><![CDATA[
arXiv:2510.23824v1 Announce Type: new 
Abstract: Coordinating multiple autonomous agents in shared environments under decentralized conditions is a long-standing challenge in robotics and artificial intelligence. This work addresses the problem of decentralized goal assignment for multi-agent path planning, where agents independently generate ranked preferences over goals based on structured representations of the environment, including grid visualizations and scenario data. After this reasoning phase, agents exchange their goal rankings, and assignments are determined by a fixed, deterministic conflict-resolution rule (e.g., agent index ordering), without negotiation or iterative coordination. We systematically compare greedy heuristics, optimal assignment, and large language model (LLM)-based agents in fully observable grid-world settings. Our results show that LLM-based agents, when provided with well-designed prompts and relevant quantitative information, can achieve near-optimal makespans and consistently outperform traditional heuristics. These findings underscore the potential of language models for decentralized goal assignment in multi-agent path planning and highlight the importance of information structure in such systems.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EthVault: A Secure and Resource-Conscious FPGA-Based Ethereum Cold Wallet</title>
<link>https://arxiv.org/abs/2510.23847</link>
<guid>https://arxiv.org/abs/2510.23847</guid>
<content:encoded><![CDATA[
arXiv:2510.23847v1 Announce Type: new 
Abstract: Cryptocurrency blockchain networks safeguard digital assets using cryptographic keys, with wallets playing a critical role in generating, storing, and managing these keys. Wallets, typically categorized as hot and cold, offer varying degrees of security and convenience. However, they are generally software-based applications running on microcontrollers. Consequently, they are vulnerable to malware and side-channel attacks, allowing perpetrators to extract private keys by targeting critical algorithms, such as ECC, which processes private keys to generate public keys and authorize transactions. To address these issues, this work presents EthVault, the first hardware architecture for an Ethereum hierarchically deterministic cold wallet, featuring hardware implementations of key algorithms for secure key generation. Also, an ECC architecture resilient to side-channel and timing attacks is proposed. Moreover, an architecture of the child key derivation function, a fundamental component of cryptocurrency wallets, is proposed. The design minimizes resource usage, meeting market demand for small, portable cryptocurrency wallets. FPGA implementation results validate the feasibility of the proposed approach. The ECC architecture exhibits uniform execution behavior across varying inputs, while the complete design utilizes only 27%, 7%, and 6% of LUTs, registers, and RAM blocks, respectively, on a Xilinx Zynq UltraScale+ FPGA.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Causal Discovery using Judo Calculus</title>
<link>https://arxiv.org/abs/2510.23942</link>
<guid>https://arxiv.org/abs/2510.23942</guid>
<content:encoded><![CDATA[
arXiv:2510.23942v1 Announce Type: new 
Abstract: We describe a theory and implementation of an intuitionistic decentralized framework for causal discovery using judo calculus, which is formally defined as j-stable causal inference using j-do-calculus in a topos of sheaves. In real-world applications -- from biology to medicine and social science -- causal effects depend on regime (age, country, dose, genotype, or lab protocol). Our proposed judo calculus formalizes this context dependence formally as local truth: a causal claim is proven true on a cover of regimes, not everywhere at once. The Lawvere-Tierney modal operator j chooses which regimes are relevant; j-stability means the claim holds constructively and consistently across that family. We describe an algorithmic and implementation framework for judo calculus, combining it with standard score-based, constraint-based, and gradient-based causal discovery methods. We describe experimental results on a range of domains, from synthetic to real-world datasets from biology and economics. Our experimental results show the computational efficiency gained by the decentralized nature of sheaf-theoretic causal discovery, as well as improved performance over classical causal discovery methods.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Matchings Under Biased and Correlated Evaluations</title>
<link>https://arxiv.org/abs/2510.23628</link>
<guid>https://arxiv.org/abs/2510.23628</guid>
<content:encoded><![CDATA[
arXiv:2510.23628v1 Announce Type: cross 
Abstract: We study a two-institution stable matching model in which candidates from two distinct groups are evaluated using partially correlated signals that are group-biased. This extends prior work (which assumes institutions evaluate candidates in an identical manner) to a more realistic setting in which institutions rely on overlapping, but independently processed, criteria. These evaluations could consist of a variety of informative tools such as standardized tests, shared recommendation systems, or AI-based assessments with local noise. Two key parameters govern evaluations: the bias parameter $\beta \in (0,1]$, which models systematic disadvantage faced by one group, and the correlation parameter $\gamma \in [0,1]$, which captures the alignment between institutional rankings. We study the representation ratio, i.e., the ratio of disadvantaged to advantaged candidates selected by the matching process in this setting. Focusing on a regime in which all candidates prefer the same institution, we characterize the large-market equilibrium and derive a closed-form expression for the resulting representation ratio. Prior work shows that when $\gamma = 1$, this ratio scales linearly with $\beta$. In contrast, we show that the representation ratio increases nonlinearly with $\gamma$ and even modest losses in correlation can cause sharp drops in the representation ratio. Our analysis identifies critical $\gamma$-thresholds where institutional selection behavior undergoes discrete transitions, and reveals structural conditions under which evaluator alignment or bias mitigation are most effective. Finally, we show how this framework and results enable interventions for fairness-aware design in decentralized selection systems.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Problem-Parameter-Free Decentralized Bilevel Optimization</title>
<link>https://arxiv.org/abs/2510.24288</link>
<guid>https://arxiv.org/abs/2510.24288</guid>
<content:encoded><![CDATA[
arXiv:2510.24288v1 Announce Type: cross 
Abstract: Decentralized bilevel optimization has garnered significant attention due to its critical role in solving large-scale machine learning problems. However, existing methods often rely on prior knowledge of problem parameters-such as smoothness, convexity, or communication network topologies-to determine appropriate stepsizes. In practice, these problem parameters are typically unavailable, leading to substantial manual effort for hyperparameter tuning. In this paper, we propose AdaSDBO, a fully problem-parameter-free algorithm for decentralized bilevel optimization with a single-loop structure. AdaSDBO leverages adaptive stepsizes based on cumulative gradient norms to update all variables simultaneously, dynamically adjusting its progress and eliminating the need for problem-specific hyperparameter tuning. Through rigorous theoretical analysis, we establish that AdaSDBO achieves a convergence rate of $\widetilde{\mathcal{O}}\left(\frac{1}{T}\right)$, matching the performance of well-tuned state-of-the-art methods up to polylogarithmic factors. Extensive numerical experiments demonstrate that AdaSDBO delivers competitive performance compared to existing decentralized bilevel optimization methods while exhibiting remarkable robustness across diverse stepsize configurations.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning with Partially Labeled Data: A Conditional Distillation Approach</title>
<link>https://arxiv.org/abs/2412.18833</link>
<guid>https://arxiv.org/abs/2412.18833</guid>
<content:encoded><![CDATA[
arXiv:2412.18833v2 Announce Type: replace 
Abstract: In medical imaging, developing generalized segmentation models that can handle multiple organs and lesions is crucial. However, the scarcity of fully annotated datasets and strict privacy regulations present significant barriers to data sharing. Federated Learning (FL) allows decentralized model training, but existing FL methods often struggle with partial labeling, leading to model divergence and catastrophic forgetting. We propose ConDistFL, a novel FL framework incorporating conditional distillation to address these challenges. ConDistFL enables effective learning from partially labeled datasets, significantly improving segmentation accuracy across distributed and non-uniform datasets. In addition to its superior segmentation performance, ConDistFL maintains computational and communication efficiency, ensuring its scalability for real-world applications. Furthermore, ConDistFL demonstrates remarkable generalizability, significantly outperforming existing FL methods in out-of-federation tests, even adapting to unseen contrast phases (e.g., non-contrast CT images) in our experiments. Extensive evaluations on 3D CT and 2D chest X-ray datasets show that ConDistFL is an efficient, adaptable solution for collaborative medical image segmentation in privacy-constrained settings.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-augmented empirical game theoretic simulation for social-ecological systems</title>
<link>https://arxiv.org/abs/2510.21965</link>
<guid>https://arxiv.org/abs/2510.21965</guid>
<content:encoded><![CDATA[
arXiv:2510.21965v1 Announce Type: new 
Abstract: Designing institutions for social-ecological systems requires models that capture heterogeneity, uncertainty, and strategic interaction. Multiple modeling approaches have emerged to meet this challenge, including empirical game-theoretic analysis (EGTA), which merges ABM's scale and diversity with game-theoretic models' formal equilibrium analysis. The newly popular class of LLM-driven simulations provides yet another approach, and it is not clear how these approaches can be integrated with one another, nor whether the resulting simulations produce a plausible range of behaviours for real-world social-ecological governance. To address this gap, we compare four LLM-augmented frameworks: procedural ABMs, generative ABMs, LLM-EGTA, and expert guided LLM-EGTA, and evaluate them on a real-world case study of irrigation and fishing in the Amu Darya basin under centralized and decentralized governance. Our results show: first, procedural ABMs, generative ABMs, and LLM-augmented EGTA models produce strikingly different patterns of collective behaviour, highlighting the value of methodological diversity. Second, inducing behaviour through system prompts in LLMs is less effective than shaping behaviour through parameterized payoffs in an expert-guided EGTA-based model.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Power to the Clients: Federated Learning in a Dictatorship Setting</title>
<link>https://arxiv.org/abs/2510.22149</link>
<guid>https://arxiv.org/abs/2510.22149</guid>
<content:encoded><![CDATA[
arXiv:2510.22149v1 Announce Type: new 
Abstract: Federated learning (FL) has emerged as a promising paradigm for decentralized model training, enabling multiple clients to collaboratively learn a shared model without exchanging their local data. However, the decentralized nature of FL also introduces vulnerabilities, as malicious clients can compromise or manipulate the training process. In this work, we introduce dictator clients, a novel, well-defined, and analytically tractable class of malicious participants capable of entirely erasing the contributions of all other clients from the server model, while preserving their own. We propose concrete attack strategies that empower such clients and systematically analyze their effects on the learning process. Furthermore, we explore complex scenarios involving multiple dictator clients, including cases where they collaborate, act independently, or form an alliance in order to ultimately betray one another. For each of these settings, we provide a theoretical analysis of their impact on the global model's convergence. Our theoretical algorithms and findings about the complex scenarios including multiple dictator clients are further supported by empirical evaluations on both computer vision and natural language processing benchmarks.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Cost Allocation in Energy Communities: A DLMP-based Bilevel Optimization with a Shapley Value Approach</title>
<link>https://arxiv.org/abs/2510.22321</link>
<guid>https://arxiv.org/abs/2510.22321</guid>
<content:encoded><![CDATA[
arXiv:2510.22321v1 Announce Type: new 
Abstract: Energy communities (ECs) are emerging as a promising decentralized model for managing cooperative distributed energy resources (DERs). As these communities expand and their operations become increasingly integrated into the grid, ensuring fairness in allocating operating costs among participants becomes a challenge. In distribution networks, DER operations at the community level can influence Distribution Locational Marginal Prices (DLMPs), which in turn affect system's operation cost. This interdependence between local decisions and system-level pricing introduces new challenges for fair and transparent cost allocation. Despite growing interest in fairness-aware methods, most methods do not account for the impact of DLMPs. To fill this gap, we propose a bilevel optimization model in which a Community Energy Aggregator (CEA) schedules DERs across multiple ECs while a Distribution System Operator (DSO) determines DLMPs through network-constrained dispatch. Leveraging the Karush-Kuhn-Tucker (KKT) conditions and strong duality, the bilevel model is reformulated into a tractable single-level problem. We achieve fairness in the cost allocation by applying the Shapley value to quantify each community's marginal contribution to system-wide cost savings. The effectiveness of the proposed method is validated through simulations on several benchmark distribution systems.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ant-inspired Walling Strategies for Scalable Swarm Separation: Reinforcement Learning Approaches Based on Finite State Machines</title>
<link>https://arxiv.org/abs/2510.22524</link>
<guid>https://arxiv.org/abs/2510.22524</guid>
<content:encoded><![CDATA[
arXiv:2510.22524v1 Announce Type: new 
Abstract: In natural systems, emergent structures often arise to balance competing demands. Army ants, for example, form temporary "walls" that prevent interference between foraging trails. Inspired by this behavior, we developed two decentralized controllers for heterogeneous robotic swarms to maintain spatial separation while executing concurrent tasks. The first is a finite-state machine (FSM)-based controller that uses encounter-triggered transitions to create rigid, stable walls. The second integrates FSM states with a Deep Q-Network (DQN), dynamically optimizing separation through emergent "demilitarized zones." In simulation, both controllers reduce mixing between subgroups, with the DQN-enhanced controller improving adaptability and reducing mixing by 40-50% while achieving faster convergence.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZK Coprocessor Bridge: Replay-Safe Private Execution from Solana to Aztec via Wormhole</title>
<link>https://arxiv.org/abs/2510.22536</link>
<guid>https://arxiv.org/abs/2510.22536</guid>
<content:encoded><![CDATA[
arXiv:2510.22536v1 Announce Type: new 
Abstract: We formalize a cross-domain "ZK coprocessor bridge" that lets Solana programs request private execution on Aztec L2 (via Ethereum) using Wormhole Verifiable Action Approvals (VAAs) as authenticated transport. The system comprises: (i) a Solana program that posts messages to Wormhole Core with explicit finality; (ii) an EVM Portal that verifies VAAs, enforces a replay lock, parses a bound payload secretHash||m from the attested VAA, derives a domain-separated field commitment, and enqueues an L1->L2 message into the Aztec Inbox (our reference implementation v0.1.0 currently uses consumeWithSecret(vaa, secretHash); we provide migration guidance to the payload-bound interface); (iii) a minimal Aztec contract that consumes the message privately; and (iv) an off-chain relayer that ferries VAAs and can record receipts on Solana. We present state machines, message formats, and proof sketches for replay-safety, origin authenticity, finality alignment, parameter binding (no relayer front-running of Aztec parameters), privacy, idempotence, and liveness. Finally, we include a concise Reproducibility note with pinned versions and artifacts to replicate a public testnet run.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blockchain Signatures to Ensure Information Integrity and Non-Repudiation in the Digital Era: A comprehensive study</title>
<link>https://arxiv.org/abs/2510.22561</link>
<guid>https://arxiv.org/abs/2510.22561</guid>
<content:encoded><![CDATA[
arXiv:2510.22561v1 Announce Type: new 
Abstract: Blockchain systems rely on decentralized ledgers and strong security guarantees. A key requirement is non-repudiation, which prevents denial of transaction authorship and supports integrity of recorded data. This work surveys digital signature schemes used in blockchain platforms and analyzes how they deliver non-repudiation and contribute to overall system security. We examine representative scheme families and their cryptographic foundations, security assumptions, and properties relevant to deployment, including unforgeability, resistance to malleability, support for aggregation and multisignature or threshold settings, key and signature sizes, and verification cost. Using these criteria, we compare the suitability of different designs for consensus protocols, smart contract constraints, and resource limits. We highlight practical tradeoffs that affect throughput, storage, scalability, and attack surfaces, and summarize benefits and limitations of each scheme in blockchain contexts. The study underscores that carefully chosen digital signatures are central to achieving non-repudiation and preserving information integrity, and it outlines implementation considerations and open directions such as interoperability and post-quantum readiness.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Conditional Diffusion Model with Mean Field Communication as Wireless Resource Allocation Planner</title>
<link>https://arxiv.org/abs/2510.22969</link>
<guid>https://arxiv.org/abs/2510.22969</guid>
<content:encoded><![CDATA[
arXiv:2510.22969v1 Announce Type: new 
Abstract: In wireless communication systems, efficient and adaptive resource allocation plays a crucial role in enhancing overall Quality of Service (QoS). While centralized Multi-Agent Reinforcement Learning (MARL) frameworks rely on a central coordinator for policy training and resource scheduling, they suffer from scalability issues and privacy risks. In contrast, the Distributed Training with Decentralized Execution (DTDE) paradigm enables distributed learning and decision-making, but it struggles with non-stationarity and limited inter-agent cooperation, which can severely degrade system performance. To overcome these challenges, we propose the Multi-Agent Conditional Diffusion Model Planner (MA-CDMP) for decentralized communication resource management. Built upon the Model-Based Reinforcement Learning (MBRL) paradigm, MA-CDMP employs Diffusion Models (DMs) to capture environment dynamics and plan future trajectories, while an inverse dynamics model guides action generation, thereby alleviating the sample inefficiency and slow convergence of conventional DTDE methods. Moreover, to approximate large-scale agent interactions, a Mean-Field (MF) mechanism is introduced as an assistance to the classifier in DMs. This design mitigates inter-agent non-stationarity and enhances cooperation with minimal communication overhead in distributed settings. We further theoretically establish an upper bound on the distributional approximation error introduced by the MF-based diffusion generation, guaranteeing convergence stability and reliable modeling of multi-agent stochastic dynamics. Extensive experiments demonstrate that MA-CDMP consistently outperforms existing MARL baselines in terms of average reward and QoS metrics, showcasing its scalability and practicality for real-world wireless network optimization.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AirFed: Federated Graph-Enhanced Multi-Agent Reinforcement Learning for Multi-UAV Cooperative Mobile Edge Computing</title>
<link>https://arxiv.org/abs/2510.23053</link>
<guid>https://arxiv.org/abs/2510.23053</guid>
<content:encoded><![CDATA[
arXiv:2510.23053v1 Announce Type: new 
Abstract: Multiple Unmanned Aerial Vehicles (UAVs) cooperative Mobile Edge Computing (MEC) systems face critical challenges in coordinating trajectory planning, task offloading, and resource allocation while ensuring Quality of Service (QoS) under dynamic and uncertain environments. Existing approaches suffer from limited scalability, slow convergence, and inefficient knowledge sharing among UAVs, particularly when handling large-scale IoT device deployments with stringent deadline constraints. This paper proposes AirFed, a novel federated graph-enhanced multi-agent reinforcement learning framework that addresses these challenges through three key innovations. First, we design dual-layer dynamic Graph Attention Networks (GATs) that explicitly model spatial-temporal dependencies among UAVs and IoT devices, capturing both service relationships and collaborative interactions within the network topology. Second, we develop a dual-Actor single-Critic architecture that jointly optimizes continuous trajectory control and discrete task offloading decisions. Third, we propose a reputation-based decentralized federated learning mechanism with gradient-sensitive adaptive quantization, enabling efficient and robust knowledge sharing across heterogeneous UAVs. Extensive experiments demonstrate that AirFed achieves 42.9% reduction in weighted cost compared to state-of-the-art baselines, attains over 99% deadline satisfaction and 94.2% IoT device coverage rate, and reduces communication overhead by 54.5%. Scalability analysis confirms robust performance across varying UAV numbers, IoT device densities, and system scales, validating AirFed's practical applicability for large-scale UAV-MEC deployments.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blockchain and Biometrics: Survey, GDPR Analysis, and Future Directions</title>
<link>https://arxiv.org/abs/2302.10883</link>
<guid>https://arxiv.org/abs/2302.10883</guid>
<content:encoded><![CDATA[
arXiv:2302.10883v4 Announce Type: replace 
Abstract: Biometric recognition as an efficient and hard-to-forge way of identification and verification has become an indispensable part of the current digital world. The fast evolution of this technology has been a strong incentive for integration into many applications. Meanwhile, blockchain, the decentralized ledger technology, has been widely received by both research and industry in the past few years, and it is being increasingly deployed today in many different applications, such as money transfer, IoT, healthcare, or logistics. Recently, researchers have started to speculate on the pros and cons and what the best applications would be when these two technologies cross paths. This paper provides a survey of the research literature on the combination of blockchain and biometrics and includes a first legal analysis of this integration based on GDPR to shed light on challenges and potentials. Although the integration of blockchain technology into the biometric sector is still in its infancy, with a growing body of literature discussing specific applications and advanced technological setups, this paper aims to provide a holistic understanding of blockchain applicability in biometrics. Based on published studies, this article discusses, among others, practical examples combining blockchain and biometrics for novel applications in PKI systems, distributed trusted services, and identity management. Challenges and limitations when combining blockchain and biometrics that motivate future work will also be discussed; e.g., blockchain networks at their current stage may not be efficient or economical for some real-time biometric applications. Finally, we also discuss key legal aspects of the EU General Data Protection Regulation (GDPR) related to this combination of technologies (blockchain and biometrics); for example, accountability, immutability, anonymity, and data protection elements.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized EXTRA stochastic gradient Langevin dynamics</title>
<link>https://arxiv.org/abs/2412.01993</link>
<guid>https://arxiv.org/abs/2412.01993</guid>
<content:encoded><![CDATA[
arXiv:2412.01993v2 Announce Type: replace 
Abstract: Langevin algorithms are popular Markov Chain Monte Carlo methods for Bayesian learning, particularly when the aim is to sample from the posterior distribution of a parametric model, given the input data and the prior distribution over the model parameters. Their stochastic versions such as stochastic gradient Langevin dynamics (SGLD) allow iterative learning based on randomly sampled mini-batches of large datasets and are scalable to large datasets. However, when data is decentralized across a network of agents subject to communication and privacy constraints, standard SGLD algorithms cannot be applied. Instead, we employ decentralized SGLD (DE-SGLD) algorithms, where Bayesian learning is performed collaboratively by a network of agents without sharing individual data. Nonetheless, existing DE-SGLD algorithms induce a bias at every agent that can negatively impact performance; this bias persists even when using full batches and is attributable to network effects. Motivated by the EXTRA algorithm and its generalizations for decentralized optimization, we propose the generalized EXTRA stochastic gradient Langevin dynamics, which eliminates this bias in the full-batch setting. Moreover, we show that, in the mini-batch setting, our algorithm provides performance bounds that significantly improve upon those of standard DE-SGLD algorithms in the literature. Our numerical results also demonstrate the efficiency of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Various DeFi Price Manipulations with LLM Reasoning</title>
<link>https://arxiv.org/abs/2502.11521</link>
<guid>https://arxiv.org/abs/2502.11521</guid>
<content:encoded><![CDATA[
arXiv:2502.11521v2 Announce Type: replace 
Abstract: DeFi (Decentralized Finance) is one of the most important applications of today's cryptocurrencies and smart contracts. It manages hundreds of billions in Total Value Locked (TVL) on-chain, yet it remains susceptible to common DeFi price manipulation attacks. Despite state-of-the-art (SOTA) systems like DeFiRanger and DeFort, we found that they are less effective to non-standard price models in custom DeFi protocols, which account for 44.2% of the 95 DeFi price manipulation attacks reported over the past three years. In this paper, we introduce the first LLM-based approach, DeFiScope, for detecting DeFi price manipulation attacks in both standard and custom price models. Our insight is that large language models (LLMs) have certain intelligence to abstract price calculation from smart contract source code and infer the trend of token price changes based on the extracted price models. To further strengthen LLMs in this aspect, we leverage Foundry to synthesize on-chain data and use it to fine-tune a DeFi price-specific LLM. Together with the high-level DeFi operations recovered from low-level transaction data, DeFiScope detects various DeFi price manipulations according to systematically mined patterns. Experimental results show that DeFiScope achieves a high recall of 80% on real-world attacks, a precision of 96% on suspicious transactions, and zero false alarms on benign transactions, significantly outperforming SOTA approaches. Moreover, we evaluate DeFiScope's cost-effectiveness and demonstrate its practicality by helping our industry partner confirm 147 real-world price manipulation attacks, including discovering 81 previously unknown historical incidents.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking</title>
<link>https://arxiv.org/abs/2504.19940</link>
<guid>https://arxiv.org/abs/2504.19940</guid>
<content:encoded><![CDATA[
arXiv:2504.19940v2 Announce Type: replace 
Abstract: The growing spread of online misinformation has created an urgent need for scalable, reliable fact-checking solutions. Crowdsourced fact-checking - where non-experts evaluate claim veracity - offers a cost-effective alternative to expert verification, despite concerns about variability in quality and bias. Encouraged by promising results in certain contexts, major platforms such as X (formerly Twitter), Facebook, and Instagram have begun shifting from centralized moderation to decentralized, crowd-based approaches.
  In parallel, advances in Large Language Models (LLMs) have shown strong performance across core fact-checking tasks, including claim detection and evidence evaluation. However, their potential role in crowdsourced workflows remains unexplored. This paper investigates whether LLM-powered generative agents - autonomous entities that emulate human behavior and decision-making - can meaningfully contribute to fact-checking tasks traditionally reserved for human crowds.
  Using the protocol of La Barbera et al. (2024), we simulate crowds of generative agents with diverse demographic and ideological profiles. Agents retrieve evidence, assess claims along multiple quality dimensions, and issue final veracity judgments. Our results show that agent crowds outperform human crowds in truthfulness classification, exhibit higher internal consistency, and show reduced susceptibility to social and cognitive biases. Compared to humans, agents rely more systematically on informative criteria such as Accuracy, Precision, and Informativeness, suggesting a more structured decision-making process. Overall, our findings highlight the potential of generative agents as scalable, consistent, and less biased contributors to crowd-based fact-checking systems.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniFC: Rethinking Federated Clustering via Lossless and Secure Distance Reconstruction</title>
<link>https://arxiv.org/abs/2505.13071</link>
<guid>https://arxiv.org/abs/2505.13071</guid>
<content:encoded><![CDATA[
arXiv:2505.13071v2 Announce Type: replace 
Abstract: Federated clustering (FC) aims to discover global cluster structures across decentralized clients without sharing raw data, making privacy preservation a fundamental requirement. There are two critical challenges: (1) privacy leakage during collaboration, and (2) robustness degradation due to aggregation of proxy information from non-independent and identically distributed (Non-IID) local data, leading to inaccurate or inconsistent global clustering. Existing solutions typically rely on model-specific local proxies, which are sensitive to data heterogeneity and inherit inductive biases from their centralized counterparts, thus limiting robustness and generality. We propose Omni Federated Clustering (OmniFC), a unified and model-agnostic framework. Leveraging Lagrange coded computing, our method enables clients to share only encoded data, allowing exact reconstruction of the global distance matrix--a fundamental representation of sample relationships--without leaking private information, even under client collusion. This construction is naturally resilient to Non-IID data distributions. This approach decouples FC from model-specific proxies, providing a unified extension mechanism applicable to diverse centralized clustering methods. Theoretical analysis confirms both reconstruction fidelity and privacy guarantees, while comprehensive experiments demonstrate OmniFC's superior robustness, effectiveness, and generality across various benchmarks compared to state-of-the-art methods. Code will be released.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAO-AI: Evaluating Collective Decision-Making through Agentic AI in Decentralized Governance</title>
<link>https://arxiv.org/abs/2510.21117</link>
<guid>https://arxiv.org/abs/2510.21117</guid>
<content:encoded><![CDATA[
arXiv:2510.21117v1 Announce Type: new 
Abstract: This paper presents a first empirical study of agentic AI as autonomous decision-makers in decentralized governance. Using more than 3K proposals from major protocols, we build an agentic AI voter that interprets proposal contexts, retrieves historical deliberation data, and independently determines its voting position. The agent operates within a realistic financial simulation environment grounded in verifiable blockchain data, implemented through a modular composable program (MCP) workflow that defines data flow and tool usage via Agentics framework. We evaluate how closely the agent's decisions align with the human and token-weighted outcomes, uncovering strong alignments measured by carefully designed evaluation metrics. Our findings demonstrate that agentic AI can augment collective decision-making by producing interpretable, auditable, and empirically grounded signals in realistic DAO governance settings. The study contributes to the design of explainable and economically rigorous AI agents for decentralized financial systems.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QAE-BAC: Achieving Quantifiable Anonymity and Efficiency in Blockchain-Based Access Control with Attribute</title>
<link>https://arxiv.org/abs/2510.21124</link>
<guid>https://arxiv.org/abs/2510.21124</guid>
<content:encoded><![CDATA[
arXiv:2510.21124v1 Announce Type: new 
Abstract: Blockchain-based Attribute-Based Access Control (BC-ABAC) offers a decentralized paradigm for secure data governance but faces two inherent challenges: the transparency of blockchain ledgers threatens user privacy by enabling reidentification attacks through attribute analysis, while the computational complexity of policy matching clashes with blockchain's performance constraints. Existing solutions, such as those employing Zero-Knowledge Proofs (ZKPs), often incur high overhead and lack measurable anonymity guarantees, while efficiency optimizations frequently ignore privacy implications. To address these dual challenges, this paper proposes QAEBAC (Quantifiable Anonymity and Efficiency in Blockchain-Based Access Control with Attribute). QAE-BAC introduces a formal (r, t)-anonymity model to dynamically quantify the re-identification risk of users based on their access attributes and history. Furthermore, it features an Entropy-Weighted Path Tree (EWPT) that optimizes policy structure based on realtime anonymity metrics, drastically reducing policy matching complexity. Implemented and evaluated on Hyperledger Fabric, QAE-BAC demonstrates a superior balance between privacy and performance. Experimental results show that it effectively mitigates re-identification risks and outperforms state-of-the-art baselines, achieving up to an 11x improvement in throughput and an 87% reduction in latency, proving its practicality for privacy-sensitive decentralized applications.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Federated Learning for Smart Prediction and Recommendation Applications</title>
<link>https://arxiv.org/abs/2510.21183</link>
<guid>https://arxiv.org/abs/2510.21183</guid>
<content:encoded><![CDATA[
arXiv:2510.21183v1 Announce Type: new 
Abstract: This paper proposes a generative adversarial network and federated learning-based model to address various challenges of the smart prediction and recommendation applications, such as high response time, compromised data privacy, and data scarcity. The integration of the generative adversarial network and federated learning is referred to as Generative Federated Learning (GFL). As a case study of the proposed model, a heart health monitoring application is considered. The realistic synthetic datasets are generated using the generated adversarial network-based proposed algorithm for improving data diversity, data quality, and data augmentation, and remove the data scarcity and class imbalance issues. In this paper, we implement the centralized and decentralized federated learning approaches in an edge computing paradigm. In centralized federated learning, the edge nodes communicate with the central server to build the global and personalized local models in a collaborative manner. In the decentralized federated learning approach, the edge nodes communicate among themselves to exchange model updates for collaborative training. The comparative study shows that the proposed framework outperforms the existing heart health monitoring applications. The results show that using the proposed framework (i) the prediction accuracy is improved by 12% than the conventional framework, and (ii) the response time is reduced by 73% than the conventional cloud-only system.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Powered Detection of Price Manipulation in DeFi</title>
<link>https://arxiv.org/abs/2510.21272</link>
<guid>https://arxiv.org/abs/2510.21272</guid>
<content:encoded><![CDATA[
arXiv:2510.21272v1 Announce Type: new 
Abstract: Decentralized Finance (DeFi) smart contracts manage billions of dollars, making them a prime target for exploits. Price manipulation vulnerabilities, often via flash loans, are a devastating class of attacks causing significant financial losses. Existing detection methods are limited. Reactive approaches analyze attacks only after they occur, while proactive static analysis tools rely on rigid, predefined heuristics, limiting adaptability. Both depend on known attack patterns, failing to identify novel variants or comprehend complex economic logic. We propose PMDetector, a hybrid framework combining static analysis with Large Language Model (LLM)-based reasoning to proactively detect price manipulation vulnerabilities. Our approach uses a formal attack model and a three-stage pipeline. First, static taint analysis identifies potentially vulnerable code paths. Second, a two-stage LLM process filters paths by analyzing defenses and then simulates attacks to evaluate exploitability. Finally, a static analysis checker validates LLM results, retaining only high-risk paths and generating comprehensive vulnerability reports. To evaluate its effectiveness, we built a dataset of 73 real-world vulnerable and 288 benign DeFi protocols. Results show PMDetector achieves 88% precision and 90% recall with Gemini 2.5-flash, significantly outperforming state-of-the-art static analysis and LLM-based approaches. Auditing a vulnerability with PMDetector costs just $0.03 and takes 4.0 seconds with GPT-4.1, offering an efficient and cost-effective alternative to manual audits.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LIDC: A Location Independent Multi-Cluster Computing Framework for Data Intensive Science</title>
<link>https://arxiv.org/abs/2510.21373</link>
<guid>https://arxiv.org/abs/2510.21373</guid>
<content:encoded><![CDATA[
arXiv:2510.21373v1 Announce Type: new 
Abstract: Scientific communities are increasingly using geographically distributed computing platforms. The current methods of compute placement predominantly use logically centralized controllers such as Kubernetes (K8s) to match tasks to available resources. However, this centralized approach is unsuitable in multi-organizational collaborations. Furthermore, workflows often need to use manual configurations tailored for a single platform and cannot adapt to dynamic changes across infrastructure. Our work introduces a decentralized control plane for placing computations on geographically dispersed compute clusters using semantic names. We assign semantic names to computations to match requests with named Kubernetes (K8s) service endpoints. We show that this approach provides multiple benefits. First, it allows placement of computational jobs to be independent of location, enabling any cluster with sufficient resources to execute the computation. Second, it facilitates dynamic compute placement without requiring prior knowledge of cluster locations or predefined configurations.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLAMES: Fine-tuning LLMs to Synthesize Invariants for Smart Contract Security</title>
<link>https://arxiv.org/abs/2510.21401</link>
<guid>https://arxiv.org/abs/2510.21401</guid>
<content:encoded><![CDATA[
arXiv:2510.21401v1 Announce Type: new 
Abstract: Smart contract vulnerabilities cost billions of dollars annually, yet existing automated analysis tools fail to generate deployable defenses. We present FLAMES, a novel automated approach that synthesizes executable runtime guards as Solidity "require" statements to harden smart contracts against exploits. Unlike prior work that relies on vulnerability labels, symbolic analysis, or natural language specifications, FLAMES employs domain-adapted large language models trained through fill-in-the-middle supervised fine-tuning on real-world invariants extracted from 514,506 verified contracts. Our extensive evaluation across three dimensions demonstrates FLAMES's effectiveness: (1) Compilation: FLAMES achieves 96.7% compilability for synthesized invariant (2) Semantic Quality: on a curated test set of 5,000 challenging invariants, FLAMES produces exact or semantically equivalent matches to ground truth in 44.5% of cases; (3) Exploit Mitigation: FLAMES prevents 22 out of 108 real exploits (20.4%) while preserving contract functionality, and (4) FLAMES successfully blocks the real-world APEMAGA incident by synthesizing a pre-condition that mitigates the attack. FLAMES establishes that domain-adapted LLMs can automatically generate production-ready security defenses for smart contracts without requiring vulnerability detection, formal specifications, or human intervention. We release our code, model weights, datasets, and evaluation infrastructure to enable reproducible research in this critical domain.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Catastrophic Forgetting Mitigation Methods in Federated Time Series Forecasting</title>
<link>https://arxiv.org/abs/2510.21491</link>
<guid>https://arxiv.org/abs/2510.21491</guid>
<content:encoded><![CDATA[
arXiv:2510.21491v1 Announce Type: new 
Abstract: Catastrophic forgetting (CF) poses a persistent challenge in continual learning (CL), especially within federated learning (FL) environments characterized by non-i.i.d. time series data. While existing research has largely focused on classification tasks in vision domains, the regression-based forecasting setting prevalent in IoT and edge applications remains underexplored. In this paper, we present the first benchmarking framework tailored to investigate CF in federated continual time series forecasting. Using the Beijing Multi-site Air Quality dataset across 12 decentralized clients, we systematically evaluate several CF mitigation strategies, including Replay, Elastic Weight Consolidation, Learning without Forgetting, and Synaptic Intelligence. Key contributions include: (i) introducing a new benchmark for CF in time series FL, (ii) conducting a comprehensive comparative analysis of state-of-the-art methods, and (iii) releasing a reproducible open-source framework. This work provides essential tools and insights for advancing continual learning in federated time-series forecasting systems.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auction-Based Responsibility Allocation for Scalable Decentralized Safety Filters in Cooperative Multi-Agent Collision Avoidance</title>
<link>https://arxiv.org/abs/2510.21546</link>
<guid>https://arxiv.org/abs/2510.21546</guid>
<content:encoded><![CDATA[
arXiv:2510.21546v1 Announce Type: new 
Abstract: This paper proposes a scalable decentralized safety filter for multi-agent systems based on high-order control barrier functions (HOCBFs) and auction-based responsibility allocation. While decentralized HOCBF formulations ensure pairwise safety under input bounds, they face feasibility and scalability challenges as the number of agents grows. Each agent must evaluate an increasing number of pairwise constraints, raising the risk of infeasibility and making it difficult to meet real-time requirements. To address this, we introduce an auction-based allocation scheme that distributes constraint enforcement asymmetrically among neighbors based on local control effort estimates. The resulting directed responsibility graph guarantees full safety coverage while reducing redundant constraints and per-agent computational load. Simulation results confirm safe and efficient coordination across a range of network sizes and interaction densities.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Genetic Algorithm for Optimal User Order Routing: Multi-Objective Solver Optimization in CoW Protocol Batch Auctions</title>
<link>https://arxiv.org/abs/2510.21647</link>
<guid>https://arxiv.org/abs/2510.21647</guid>
<content:encoded><![CDATA[
arXiv:2510.21647v1 Announce Type: new 
Abstract: CoW Protocol batch auctions aggregate user intents and rely on solvers to find optimal execution paths that maximize user surplus across heterogeneous automated market makers (AMMs) under stringent auction deadlines. Deterministic single-objective heuristics that optimize only expected output frequently fail to exploit split-flow opportunities across multiple parallel paths and to internalize gas, slippage, and execution risk constraints in a unified search. We apply evolutionary multi-objective optimization to this blockchain routing problem, proposing a hybrid genetic algorithm (GA) architecture for real-time solver optimization that combines a production-grade, multi-objective NSGA-II engine with adaptive instance profiling and deterministic baselines. Our core engine encodes variable-length path sets with continuous split ratios and evolves candidate route-and-volume allocations under a Pareto objective vector F = (user surplus, -gas, -slippage, -risk), enabling principled trade-offs and anytime operation within the auction deadline. An adaptive controller selects between GA and a deterministic dual-decomposition optimizer with Bellman-Ford based negative-cycle detection, with a guarantee to never underperform the baseline. The open-source system integrates six protection layers and passes 8/8 tests, validating safety and correctness. In a 14-stratum benchmark (30 seeds each), the hybrid approach yields absolute user-surplus gains of approximately 0.40-9.82 ETH on small-to-medium orders, while large high-fragmentation orders are unprofitable across gas regimes. Convergence occurs in about 0.5 s median (soft capped at 1.0 s) within a 2-second limit. We are not aware of an openly documented multi-objective GA with end-to-end safety for real-time DEX routing.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mean-Field Sampling for Cooperative Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2412.00661</link>
<guid>https://arxiv.org/abs/2412.00661</guid>
<content:encoded><![CDATA[
arXiv:2412.00661v4 Announce Type: replace 
Abstract: Designing efficient algorithms for multi-agent reinforcement learning (MARL) is fundamentally challenging because the size of the joint state and action spaces grows exponentially in the number of agents. These difficulties are exacerbated when balancing sequential global decision-making with local agent interactions. In this work, we propose a new algorithm $\texttt{SUBSAMPLE-MFQ}$ ($\textbf{Subsample}$-$\textbf{M}$ean-$\textbf{F}$ield-$\textbf{Q}$-learning) and a decentralized randomized policy for a system with $n$ agents. For any $k\leq n$, our algorithm learns a policy for the system in time polynomial in $k$. We prove that this learned policy converges to the optimal policy on the order of $\tilde{O}(1/\sqrt{k})$ as the number of subsampled agents $k$ increases. In particular, this bound is independent of the number of agents $n$.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Latent-Space Constraints in Personalized Federated Learning</title>
<link>https://arxiv.org/abs/2505.07525</link>
<guid>https://arxiv.org/abs/2505.07525</guid>
<content:encoded><![CDATA[
arXiv:2505.07525v2 Announce Type: replace 
Abstract: Federated learning (FL) is an effective and widely used approach to training deep learning models on decentralized datasets held by distinct clients. FL also strengthens both security and privacy protections for training data. Common challenges associated with statistical heterogeneity between distributed datasets have spurred significant interest in personalized FL (pFL) methods, where models combine aspects of global learning with local modeling specific to each client's unique characteristics. This work investigates the efficacy of theoretically supported, adaptive MMD measures in pFL, primarily focusing on the Ditto framework, a state-of-the-art technique for distributed data heterogeneity. The use of such measures significantly improves model performance across a variety of tasks, especially those with pronounced feature heterogeneity. Additional experiments demonstrate that such measures are directly applicable to other pFL techniques and yield similar improvements across a number of datasets. Finally, the results motivate the use of constraints tailored to the various kinds of heterogeneity expected in FL systems.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Combinatorial Auction for Blockchain Trade Intents: Being Fair without Knowing What is Fair</title>
<link>https://arxiv.org/abs/2408.12225</link>
<guid>https://arxiv.org/abs/2408.12225</guid>
<content:encoded><![CDATA[
arXiv:2408.12225v3 Announce Type: replace-cross 
Abstract: We study blockchain trade-intent auctions, which currently intermediate about USD 10 billion in trades each month. These auctions are combinatorial because executing multiple trade intents jointly generates additional efficiencies. However, the auctioneer cannot observe what each trader would have received had its order been auctioned individually and hence cannot determine how these efficiencies should be shared. We compare the two dominant mechanisms - batch auctions and simultaneous individual auctions - and introduce a novel definition of fairness applicable to combinatorial auctions. We then propose a fair combinatorial auction that endogenously constructs a fairness benchmark from individual bids and a counterfactual mechanism. Whether fairness guarantees arise in equilibrium depends on the counterfactual: all traders receive more in the equilibrium of the fair combinatorial auction than in the equilibrium of the counterfactual mechanism when the counterfactual is simultaneous first-price auctions, but that may not be the case if the counterfactual is simultaneous second-price auctions.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Context Contracts - MCP-Enabled Framework to Integrate LLMs With Blockchain Smart Contracts</title>
<link>https://arxiv.org/abs/2510.19856</link>
<guid>https://arxiv.org/abs/2510.19856</guid>
<content:encoded><![CDATA[
arXiv:2510.19856v1 Announce Type: new 
Abstract: In recent years, blockchain has experienced widespread adoption across various industries, becoming integral to numerous enterprise applications. Concurrently, the rise of generative AI and LLMs has transformed human-computer interactions, offering advanced capabilities in understanding and generating human-like text. The introduction of the MCP has further enhanced AI integration by standardizing communication between AI systems and external data sources. Despite these advancements, there is still no standardized method for seamlessly integrating LLM applications and blockchain. To address this concern, we propose "MCC: Model Context Contracts" a novel framework that enables LLMs to interact directly with blockchain smart contracts through MCP-like protocol. This integration allows AI agents to invoke blockchain smart contracts, facilitating more dynamic and context-aware interactions between users and blockchain networks. Essentially, it empowers users to interact with blockchain systems and perform transactions using queries in natural language. Within this proposed architecture, blockchain smart contracts can function as intelligent agents capable of recognizing user input in natural language and executing the corresponding transactions. To ensure that the LLM accurately interprets natural language inputs and maps them to the appropriate MCP functions, the LLM was fine-tuned using a custom dataset comprising user inputs paired with their corresponding MCP server functions. This fine-tuning process significantly improved the platform's performance and accuracy. To validate the effectiveness of MCC, we have developed an end-to-end prototype implemented on the Rahasak blockchain with the fine-tuned Llama-4 LLM. To the best of our knowledge, this research represents the first approach to using the concept of Model Context Protocol to integrate LLMs with blockchain.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Privacy-Utility Trade-off in Decentralized Federated Learning via $f$-Differential Privacy</title>
<link>https://arxiv.org/abs/2510.19934</link>
<guid>https://arxiv.org/abs/2510.19934</guid>
<content:encoded><![CDATA[
arXiv:2510.19934v1 Announce Type: new 
Abstract: Differentially private (DP) decentralized Federated Learning (FL) allows local users to collaborate without sharing their data with a central server. However, accurately quantifying the privacy budget of private FL algorithms is challenging due to the co-existence of complex algorithmic components such as decentralized communication and local updates. This paper addresses privacy accounting for two decentralized FL algorithms within the $f$-differential privacy ($f$-DP) framework. We develop two new $f$-DP-based accounting methods tailored to decentralized settings: Pairwise Network $f$-DP (PN-$f$-DP), which quantifies privacy leakage between user pairs under random-walk communication, and Secret-based $f$-Local DP (Sec-$f$-LDP), which supports structured noise injection via shared secrets. By combining tools from $f$-DP theory and Markov chain concentration, our accounting framework captures privacy amplification arising from sparse communication, local iterations, and correlated noise. Experiments on synthetic and real datasets demonstrate that our methods yield consistently tighter $(\epsilon,\delta)$ bounds and improved utility compared to R\'enyi DP-based approaches, illustrating the benefits of $f$-DP in decentralized privacy accounting.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>zk-Agreements: A Privacy-Preserving Way to Establish Deterministic Trust in Confidential Agreements</title>
<link>https://arxiv.org/abs/2510.20007</link>
<guid>https://arxiv.org/abs/2510.20007</guid>
<content:encoded><![CDATA[
arXiv:2510.20007v1 Announce Type: new 
Abstract: Digital transactions currently exceed trillions of dollars annually, yet traditional paper-based agreements remain a bottleneck for automation, enforceability, and dispute resolution. Natural language contracts introduce ambiguity, require manual processing, and lack computational verifiability, all of which hinder efficient digital commerce. Computable legal contracts, expressed in machine-readable formats, offer a potential solution by enabling automated execution and verification. Blockchain-based smart contracts further strengthen enforceability and accelerate dispute resolution; however, current implementations risk exposing sensitive agreement terms on public ledgers, raising serious privacy and competitive intelligence concerns that limit enterprise adoption.
  We introduce zk-agreements, a protocol designed to transition from paper-based trust to cryptographic trust while preserving confidentiality. Our design combines zero-knowledge proofs to protect private agreement terms, secure two-party computation to enable private compliance evaluation, and smart contracts to guarantee automated enforcement. Together, these components achieve both privacy preservation and computational enforceability, resolving the fundamental tension between transparency and confidentiality in blockchain-based agreements.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMAS$^2$: Joint Agent Selection and Information-Theoretic Coordinated Perception In Dec-POMDPs</title>
<link>https://arxiv.org/abs/2510.20009</link>
<guid>https://arxiv.org/abs/2510.20009</guid>
<content:encoded><![CDATA[
arXiv:2510.20009v1 Announce Type: new 
Abstract: We study the problem of jointly selecting sensing agents and synthesizing decentralized active perception policies for the chosen subset of agents within a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) framework. Our approach employs a two-layer optimization structure. In the inner layer, we introduce information-theoretic metrics, defined by the mutual information between the unknown trajectories or some hidden property in the environment and the collective partial observations in the multi-agent system, as a unified objective for active perception problems. We employ various optimization methods to obtain optimal sensor policies that maximize mutual information for distinct active perception tasks. In the outer layer, we prove that under certain conditions, the information-theoretic objectives are monotone and submodular with respect to the subset of observations collected from multiple agents. We then exploit this property to design an IMAS$^2$ (Information-theoretic Multi-Agent Selection and Sensing) algorithm for joint sensing agent selection and sensing policy synthesis. However, since the policy search space is infinite, we adapt the classical Nemhauser-Wolsey argument to prove that the proposed IMAS$^2$ algorithm can provide a tight $(1 - 1/e)$-guarantee on the performance. Finally, we demonstrate the effectiveness of our approach in a multi-agent cooperative perception in a grid-world environment.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADP-VRSGP: Decentralized Learning with Adaptive Differential Privacy via Variance-Reduced Stochastic Gradient Push</title>
<link>https://arxiv.org/abs/2510.20157</link>
<guid>https://arxiv.org/abs/2510.20157</guid>
<content:encoded><![CDATA[
arXiv:2510.20157v1 Announce Type: new 
Abstract: Differential privacy is widely employed in decentralized learning to safeguard sensitive data by introducing noise into model updates. However, existing approaches that use fixed-variance noise often degrade model performance and reduce training efficiency. To address these limitations, we propose a novel approach called decentralized learning with adaptive differential privacy via variance-reduced stochastic gradient push (ADP-VRSGP). This method dynamically adjusts both the noise variance and the learning rate using a stepwise-decaying schedule, which accelerates training and enhances final model performance while providing node-level personalized privacy guarantees. To counteract the slowed convergence caused by large-variance noise in early iterations, we introduce a progressive gradient fusion strategy that leverages historical gradients. Furthermore, ADP-VRSGP incorporates decentralized push-sum and aggregation techniques, making it particularly suitable for time-varying communication topologies. Through rigorous theoretical analysis, we demonstrate that ADP-VRSGP achieves robust convergence with an appropriate learning rate, significantly improving training stability and speed. Experimental results validate that our method outperforms existing baselines across multiple scenarios, highlighting its efficacy in addressing the challenges of privacy-preserving decentralized learning.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRUST: A Decentralized Framework for Auditing Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2510.20188</link>
<guid>https://arxiv.org/abs/2510.20188</guid>
<content:encoded><![CDATA[
arXiv:2510.20188v1 Announce Type: new 
Abstract: Large Language Models generate complex reasoning chains that reveal their decision-making, yet verifying the faithfulness and harmlessness of these intermediate steps remains a critical unsolved problem. Existing auditing methods are centralized, opaque, and hard to scale, creating significant risks for deploying proprietary models in high-stakes domains. We identify four core challenges: (1) Robustness: Centralized auditors are single points of failure, prone to bias or attacks. (2) Scalability: Reasoning traces are too long for manual verification. (3) Opacity: Closed auditing undermines public trust. (4) Privacy: Exposing full reasoning risks model theft or distillation. We propose TRUST, a transparent, decentralized auditing framework that overcomes these limitations via: (1) A consensus mechanism among diverse auditors, guaranteeing correctness under up to $30\%$ malicious participants. (2) A hierarchical DAG decomposition of reasoning traces, enabling scalable, parallel auditing. (3) A blockchain ledger that records all verification decisions for public accountability. (4) Privacy-preserving segmentation, sharing only partial reasoning steps to protect proprietary logic. We provide theoretical guarantees for the security and economic incentives of the TRUST framework. Experiments across multiple LLMs (GPT-OSS, DeepSeek-r1, Qwen) and reasoning tasks (math, medical, science, humanities) show TRUST effectively detects reasoning flaws and remains robust against adversarial auditors. Our work pioneers decentralized AI auditing, offering a practical path toward safe and trustworthy LLM deployment.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NODA-MMH: Certified Learning-Aided Nonlinear Control for Magnetically-Actuated Swarm Experiment Toward On-Orbit Proof</title>
<link>https://arxiv.org/abs/2510.20231</link>
<guid>https://arxiv.org/abs/2510.20231</guid>
<content:encoded><![CDATA[
arXiv:2510.20231v1 Announce Type: new 
Abstract: This study experimentally validates the principle of large-scale satellite swarm control through learning-aided magnetic field interactions generated by satellite-mounted magnetorquers. This actuation presents a promising solution for the long-term formation maintenance of multiple satellites and has primarily been demonstrated in ground-based testbeds for two-satellite position control. However, as the number of satellites increases beyond three, fundamental challenges coupled with the high nonlinearity arise: 1) nonholonomic constraints, 2) underactuation, 3) scalability, and 4) computational cost. Previous studies have shown that time-integrated current control theoretically solves these problems, where the average actuator outputs align with the desired command, and a learning-based technique further enhances their performance. Through multiple experiments, we validate critical aspects of learning-aided time-integrated current control: (1) enhanced controllability of the averaged system dynamics, with a theoretically guaranteed error bound, and (2) decentralized current management. We design two-axis coils and a ground-based experimental setup utilizing an air-bearing platform, enabling a mathematical replication of orbital dynamics. Based on the effectiveness of the learned interaction model, we introduce NODA-MMH (Neural power-Optimal Dipole Allocation for certified learned Model-based Magnetically swarm control Harness) for model-based power-optimal swarm control. This study complements our tutorial paper on magnetically actuated swarms for the long-term formation maintenance problem.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Decentralized Reinforcement Learning for Modular Reconfigurable Lunar Robots</title>
<link>https://arxiv.org/abs/2510.20347</link>
<guid>https://arxiv.org/abs/2510.20347</guid>
<content:encoded><![CDATA[
arXiv:2510.20347v1 Announce Type: new 
Abstract: Modular reconfigurable robots suit task-specific space operations, but the combinatorial growth of morphologies hinders unified control. We propose a decentralized reinforcement learning (Dec-RL) scheme where each module learns its own policy: wheel modules use Soft Actor-Critic (SAC) for locomotion and 7-DoF limbs use Proximal Policy Optimization (PPO) for steering and manipulation, enabling zero-shot generalization to unseen configurations. In simulation, the steering policy achieved a mean absolute error of 3.63{\deg} between desired and induced angles; the manipulation policy plateaued at 84.6 % success on a target-offset criterion; and the wheel policy cut average motor torque by 95.4 % relative to baseline while maintaining 99.6 % success. Lunar-analogue field tests validated zero-shot integration for autonomous locomotion, steering, and preliminary alignment for reconfiguration. The system transitioned smoothly among synchronous, parallel, and sequential modes for Policy Execution, without idle states or control conflicts, indicating a scalable, reusable, and robust approach for modular lunar robots.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SheafAlign: A Sheaf-theoretic Framework for Decentralized Multimodal Alignment</title>
<link>https://arxiv.org/abs/2510.20540</link>
<guid>https://arxiv.org/abs/2510.20540</guid>
<content:encoded><![CDATA[
arXiv:2510.20540v1 Announce Type: new 
Abstract: Conventional multimodal alignment methods assume mutual redundancy across all modalities, an assumption that fails in real-world distributed scenarios. We propose SheafAlign, a sheaf-theoretic framework for decentralized multimodal alignment that replaces single-space alignment with multiple comparison spaces. This approach models pairwise modality relations through sheaf structures and leverages decentralized contrastive learning-based objectives for training. SheafAlign overcomes the limitations of prior methods by not requiring mutual redundancy among all modalities, preserving both shared and unique information. Experiments on multimodal sensing datasets show superior zero-shot generalization, cross-modal alignment, and robustness to missing modalities, with 50\% lower communication cost than state-of-the-art baselines.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Small Gain and Phase Stability Conditions for Grid-Forming Converters: Limitations and Extensions</title>
<link>https://arxiv.org/abs/2510.20544</link>
<guid>https://arxiv.org/abs/2510.20544</guid>
<content:encoded><![CDATA[
arXiv:2510.20544v1 Announce Type: new 
Abstract: The increasing share of converter based resources in power systems calls for scalable methods to analyse stability without relying on exhaustive system wide simulations. Decentralized small gain and small-phase criteria have recently been proposed for this purpose, but their applicability to grid forming converters is severely limited by the sectoriality assumption, which is not typically satisfied at low frequencies. This work revisits and extends mixed gain phase conditions by introducing loop shaping transformations that reformulate converter and network models in alternative coordinate frames. The proposed approach resolves intrinsic non sectoriality at low frequencies and reduces conservativeness, thereby improving the applicability of decentralized stability certification. Analytical results are illustrated using an infinite bus system first and then extended to the IEEE 14 bus network, demonstrating the practicality and scalability of the method. These findings provide a pathway toward less conservative and more widely applicable decentralized stability certificates in power grids.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe Decentralized Density Control of Multi-Robot Systems using PDE-Constrained Optimization with State Constraints</title>
<link>https://arxiv.org/abs/2510.20643</link>
<guid>https://arxiv.org/abs/2510.20643</guid>
<content:encoded><![CDATA[
arXiv:2510.20643v1 Announce Type: new 
Abstract: In this paper, we introduce a decentralized optimization-based density controller designed to enforce set invariance constraints in multi-robot systems. By designing a decentralized control barrier function, we derived sufficient conditions under which local safety constraints guarantee global safety. We account for localization and motion noise explicitly by modeling robots as spatial probability density functions governed by the Fokker-Planck equation. Compared to traditional centralized approaches, our controller requires less computational and communication power, making it more suitable for deployment in situations where perfect communication and localization are impractical. The controller is validated through simulations and experiments with four quadcopters.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Exchange that Mitigate a Bribery Attack</title>
<link>https://arxiv.org/abs/2510.20645</link>
<guid>https://arxiv.org/abs/2510.20645</guid>
<content:encoded><![CDATA[
arXiv:2510.20645v1 Announce Type: new 
Abstract: Despite the popularity of Hashed Time-Locked Contracts (HTLCs) because of their use in wide areas of applications such as payment channels, atomic swaps, etc, their use in exchange is still questionable. This is because of its incentive incompatibility and susceptibility to bribery attacks.
  State-of-the-art solutions such as MAD-HTLC (Oakland'21) and He-HTLC (NDSS'23) address this by leveraging miners' profit-driven behaviour to mitigate such attacks. The former is the mitigation against passive miners; however, the latter works against both active and passive miners. However, they consider only two bribing scenarios where either of the parties involved in the transfer collude with the miner.
  In this paper, we expose vulnerabilities in state-of-the-art solutions by presenting a miner-collusion bribery attack with implementation and game-theoretic analysis. Additionally, we propose a stronger attack on MAD-HTLC than He-HTLC, allowing the attacker to earn profits equivalent to attacking naive HTLC.
  Leveraging our insights, we propose \prot, a game-theoretically secure HTLC protocol resistant to all bribery scenarios. \prot\ employs a two-phase approach, preventing unauthorized token confiscation by third parties, such as miners. In Phase 1, parties commit to the transfer; in Phase 2, the transfer is executed without manipulation. We demonstrate \prot's efficiency in transaction cost and latency via implementations on Bitcoin and Ethereum.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bilevel Analysis of Cost and Emissions Externalities from Data Center Load Shifting</title>
<link>https://arxiv.org/abs/2510.20805</link>
<guid>https://arxiv.org/abs/2510.20805</guid>
<content:encoded><![CDATA[
arXiv:2510.20805v1 Announce Type: new 
Abstract: Data centers are emerging as large, flexible electricity consumers capable of shifting computational workloads across locations in response to economic and environmental signals. While this flexibility has potential for emissions reduction, its impact on power system operations depends critically on how such behavior interacts with network constraints and market signals. We develop a bilevel optimization framework in which a data center minimizes a weighted combination of electricity cost and marginal emissions intensity (LME), while the system operator clears economic dispatch under transmission and generation constraints. Focusing on a stylized three-bus power system, we derive closed-form, piecewise-linear expressions for both the data center and system-wide objectives as functions of the data centers' load shift. These expressions capture threshold-driven regime changes due to congestion and renewable saturation. We identify sufficient conditions under which the data center's decentralized decisions align with or diverge from socially optimal behavior and characterize the resulting externalities. Our results reveal how system topology and generator asymmetry affect incentive alignment and provide insight into when marginal price or emissions signals may fail to guide flexible loads toward socially beneficial outcomes. Our results offer a tractable starting point for analyzing decentralized flexibility under carbon-aware incentives and suggest directions for improving coordination between flexible loads and system operations.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Decentralized Routing Policies via Graph Attention-based Multi-Agent Reinforcement Learning in Lunar Delay-Tolerant Networks</title>
<link>https://arxiv.org/abs/2510.20436</link>
<guid>https://arxiv.org/abs/2510.20436</guid>
<content:encoded><![CDATA[
arXiv:2510.20436v1 Announce Type: cross 
Abstract: We present a fully decentralized routing framework for multi-robot exploration missions operating under the constraints of a Lunar Delay-Tolerant Network (LDTN). In this setting, autonomous rovers must relay collected data to a lander under intermittent connectivity and unknown mobility patterns. We formulate the problem as a Partially Observable Markov Decision Problem (POMDP) and propose a Graph Attention-based Multi-Agent Reinforcement Learning (GAT-MARL) policy that performs Centralized Training, Decentralized Execution (CTDE). Our method relies only on local observations and does not require global topology updates or packet replication, unlike classical approaches such as shortest path and controlled flooding-based algorithms. Through Monte Carlo simulations in randomized exploration environments, GAT-MARL provides higher delivery rates, no duplications, and fewer packet losses, and is able to leverage short-term mobility forecasts; offering a scalable solution for future space robotic systems for planetary exploration, as demonstrated by successful generalization to larger rover teams.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sharp Gaussian approximations for Decentralized Federated Learning</title>
<link>https://arxiv.org/abs/2505.08125</link>
<guid>https://arxiv.org/abs/2505.08125</guid>
<content:encoded><![CDATA[
arXiv:2505.08125v2 Announce Type: replace-cross 
Abstract: Federated Learning has gained traction in privacy-sensitive collaborative environments, with local SGD emerging as a key optimization method in decentralized settings. While its convergence properties are well-studied, asymptotic statistical guarantees beyond convergence remain limited. In this paper, we present two generalized Gaussian approximation results for local SGD and explore their implications. First, we prove a Berry-Esseen theorem for the final local SGD iterates, enabling valid multiplier bootstrap procedures. Second, motivated by robustness considerations, we introduce two distinct time-uniform Gaussian approximations for the entire trajectory of local SGD. The time-uniform approximations support Gaussian bootstrap-based tests for detecting adversarial attacks. Extensive simulations are provided to support our theoretical results.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>POLAR: Policy-based Layerwise Reinforcement Learning Method for Stealthy Backdoor Attacks in Federated Learning</title>
<link>https://arxiv.org/abs/2510.19056</link>
<guid>https://arxiv.org/abs/2510.19056</guid>
<content:encoded><![CDATA[
arXiv:2510.19056v1 Announce Type: new 
Abstract: Federated Learning (FL) enables decentralized model training across multiple clients without exposing local data, but its distributed feature makes it vulnerable to backdoor attacks. Despite early FL backdoor attacks modifying entire models, recent studies have explored the concept of backdoor-critical (BC) layers, which poison the chosen influential layers to maintain stealthiness while achieving high effectiveness. However, existing BC layers approaches rely on rule-based selection without consideration of the interrelations between layers, making them ineffective and prone to detection by advanced defenses. In this paper, we propose POLAR (POlicy-based LAyerwise Reinforcement learning), the first pipeline to creatively adopt RL to solve the BC layer selection problem in layer-wise backdoor attack. Different from other commonly used RL paradigm, POLAR is lightweight with Bernoulli sampling. POLAR dynamically learns an attack strategy, optimizing layer selection using policy gradient updates based on backdoor success rate (BSR) improvements. To ensure stealthiness, we introduce a regularization constraint that limits the number of modified layers by penalizing large attack footprints. Extensive experiments demonstrate that POLAR outperforms the latest attack methods by up to 40% against six state-of-the-art (SOTA) defenses.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Securing IoT Communications via Anomaly Traffic Detection: Synergy of Genetic Algorithm and Ensemble Method</title>
<link>https://arxiv.org/abs/2510.19121</link>
<guid>https://arxiv.org/abs/2510.19121</guid>
<content:encoded><![CDATA[
arXiv:2510.19121v1 Announce Type: new 
Abstract: The rapid growth of the Internet of Things (IoT) has transformed industries by enabling seamless data exchange among connected devices. However, IoT networks remain vulnerable to security threats such as denial of service (DoS) attacks, anomalous traffic, and data manipulation due to decentralized architectures and limited resources. To address these issues, this paper proposes an advanced anomaly detection framework with three main phases. First, data preprocessing is performed using the Median KS Test to remove noise, handle missing values, and balance datasets for cleaner input. Second, a feature selection phase employs a Genetic Algorithm combined with eagle inspired search strategies to identify the most relevant features, reduce dimensionality, and improve efficiency without sacrificing accuracy. Finally, an ensemble classifier integrates Decision Tree, Random Forest, and XGBoost algorithms to achieve accurate and reliable anomaly detection. The proposed model demonstrates high adaptability and scalability across diverse IoT environments. Experimental results show that it outperforms existing methods by achieving 98 percent accuracy, 95 percent detection rate, and reductions in false positive (10 percent) and false negative (5 percent) rates. These results confirm the framework effectiveness and robustness in improving IoT network security against evolving cyber threats.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Communication-Efficient Decentralized Actor-Critic Algorithm</title>
<link>https://arxiv.org/abs/2510.19199</link>
<guid>https://arxiv.org/abs/2510.19199</guid>
<content:encoded><![CDATA[
arXiv:2510.19199v1 Announce Type: new 
Abstract: In this paper, we study the problem of reinforcement learning in multi-agent systems where communication among agents is limited. We develop a decentralized actor-critic learning framework in which each agent performs several local updates of its policy and value function, where the latter is approximated by a multi-layer neural network, before exchanging information with its neighbors. This local training strategy substantially reduces the communication burden while maintaining coordination across the network. We establish finite-time convergence analysis for the algorithm under Markov-sampling. Specifically, to attain the $\varepsilon$-accurate stationary point, the sample complexity is of order $\mathcal{O}(\varepsilon^{-3})$ and the communication complexity is of order $\mathcal{O}(\varepsilon^{-1}\tau^{-1})$, where tau denotes the number of local training steps. We also show how the final error bound depends on the neural network's approximation quality. Numerical experiments in a cooperative control setting illustrate and validate the theoretical findings.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatiotemporal Tubes based Control of Unknown Multi-Agent Systems for Temporal Reach-Avoid-Stay Tasks</title>
<link>https://arxiv.org/abs/2510.19232</link>
<guid>https://arxiv.org/abs/2510.19232</guid>
<content:encoded><![CDATA[
arXiv:2510.19232v1 Announce Type: new 
Abstract: The paper focuses on designing a controller for unknown dynamical multi-agent systems to achieve temporal reach-avoid-stay tasks for each agent while preventing inter-agent collisions. The main objective is to generate a spatiotemporal tube (STT) for each agent and thereby devise a closed-form, approximation-free, and decentralized control strategy that ensures the system trajectory reaches the target within a specific time while avoiding time-varying unsafe sets and collisions with other agents. In order to achieve this, the requirements of STTs are formulated as a robust optimization problem (ROP) and solved using a sampling-based scenario optimization problem (SOP) to address the issue of infeasibility caused by the infinite number of constraints in ROP. The STTs are generated by solving the SOP, and the corresponding closed-form control is designed to fulfill the specified task. Finally, the effectiveness of our approach is demonstrated through two case studies, one involving omnidirectional robots and the other involving multiple drones modelled as Euler-Lagrange systems.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trace: Securing Smart Contract Repository Against Access Control Vulnerability</title>
<link>https://arxiv.org/abs/2510.19254</link>
<guid>https://arxiv.org/abs/2510.19254</guid>
<content:encoded><![CDATA[
arXiv:2510.19254v1 Announce Type: new 
Abstract: Smart contract vulnerabilities, particularly improper Access Control that allows unauthorized execution of restricted functions, have caused billions of dollars in losses. GitHub hosts numerous smart contract repositories containing source code, documentation, and configuration files-these serve as intermediate development artifacts that must be compiled and packaged before deployment. Third-party developers often reference, reuse, or fork code from these repositories during custom development. However, if the referenced code contains vulnerabilities, it can introduce significant security risks. Existing tools for detecting smart contract vulnerabilities are limited in their ability to handle complex repositories, as they typically require the target contract to be compilable to generate an abstract representation for further analysis. This paper presents TRACE, a tool designed to secure non-compilable smart contract repositories against access control vulnerabilities. TRACE employs LLMs to locate sensitive functions involving critical operations (e.g., transfer) within the contract and subsequently completes function snippets into a fully compilable contract. TRACE constructs a function call graph from the abstract syntax tree (AST) of the completed contract. It uses the control flow graph (CFG) of each function as node information. The nodes of the sensitive functions are then analyzed to detect Access Control vulnerabilities. Experimental results demonstrate that TRACE outperforms state-of-the-art tools on an open-sourced CVE dataset, detecting 14 out of 15 CVEs. In addition, it achieves 89.2% precision on 5,000 recent on-chain contracts, far exceeding the best existing tool at 76.9%. On 83 real-world repositories, TRACE achieves 87.0% precision, significantly surpassing DeepSeek-R1's 14.3%.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative penetration testing suite for emerging generative AI algorithms</title>
<link>https://arxiv.org/abs/2510.19303</link>
<guid>https://arxiv.org/abs/2510.19303</guid>
<content:encoded><![CDATA[
arXiv:2510.19303v1 Announce Type: new 
Abstract: Problem Space: AI Vulnerabilities and Quantum Threats Generative AI vulnerabilities: model inversion, data poisoning, adversarial inputs. Quantum threats Shor Algorithm breaking RSA ECC encryption. Challenge Secure generative AI models against classical and quantum cyberattacks. Proposed Solution Collaborative Penetration Testing Suite Five Integrated Components: DAST SAST OWASP ZAP, Burp Suite, SonarQube, Fortify. IAST Contrast Assess integrated with CI CD pipeline. Blockchain Logging Hyperledger Fabric for tamper-proof logs. Quantum Cryptography Lattice based RLWE protocols. AI Red Team Simulations Adversarial ML & Quantum-assisted attacks. Integration Layer: Unified workflow for AI, cybersecurity, and quantum experts. Key Results 300+ vulnerabilities identified across test environments. 70% reduction in high-severity issues within 2 weeks. 90% resolution efficiency for blockchain-logged vulnerabilities. Quantum-resistant cryptography maintained 100% integrity in tests. Outcome: Quantum AI Security Protocol integrating Blockchain Quantum Cryptography AI Red Teaming.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SORA-ATMAS: Adaptive Trust Management and Multi-LLM Aligned Governance for Future Smart Cities</title>
<link>https://arxiv.org/abs/2510.19327</link>
<guid>https://arxiv.org/abs/2510.19327</guid>
<content:encoded><![CDATA[
arXiv:2510.19327v1 Announce Type: new 
Abstract: The rapid evolution of smart cities has increased the reliance on intelligent interconnected services to optimize infrastructure, resources, and citizen well-being. Agentic AI has emerged as a key enabler by supporting autonomous decision-making and adaptive coordination, allowing urban systems to respond in real time to dynamic conditions. Its benefits are evident in areas such as transportation, where the integration of traffic data, weather forecasts, and safety sensors enables dynamic rerouting and a faster response to hazards. However, its deployment across heterogeneous smart city ecosystems raises critical governance, risk, and compliance (GRC) challenges, including accountability, data privacy, and regulatory alignment within decentralized infrastructures. Evaluation of SORA-ATMAS with three domain agents (Weather, Traffic, and Safety) demonstrated that its governance policies, including a fallback mechanism for high-risk scenarios, effectively steer multiple LLMs (GPT, Grok, DeepSeek) towards domain-optimized, policy-aligned outputs, producing an average MAE reduction of 35% across agents. Results showed stable weather monitoring, effective handling of high-risk traffic plateaus 0.85, and adaptive trust regulation in Safety/Fire scenarios 0.65. Runtime profiling of a 3-agent deployment confirmed scalability, with throughput between 13.8-17.2 requests per second, execution times below 72~ms, and governance delays under 100 ms, analytical projections suggest maintained performance at larger scales. Cross-domain rules ensured safe interoperability, with traffic rerouting permitted only under validated weather conditions. These findings validate SORA-ATMAS as a regulation-aligned, context-aware, and verifiable governance framework that consolidates distributed agent outputs into accountable, real-time decisions, offering a resilient foundation for smart-city management.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping and Evolving Interoperability Testing in European Energy Systems: The int:net Perspective</title>
<link>https://arxiv.org/abs/2510.19460</link>
<guid>https://arxiv.org/abs/2510.19460</guid>
<content:encoded><![CDATA[
arXiv:2510.19460v1 Announce Type: new 
Abstract: The ongoing transformation of the European energy landscape, driven by the integration of renewable energy sources, digital technologies, and decentralized systems, requires a high degree of interoperability across diverse components and systems. Ensuring that these elements can exchange information and operate together reliably is essential for achieving a secure, flexible, and efficient energy supply infrastructure. While several initiatives have contributed to the development of smart grid testing infrastructures, they do not provide a dedicated or comprehensive focus on interoperability testing. A structured and harmonized overview of interoperability testing capabilities across Europe is therefore still missing. This work therefore presents a novel contribution by analyzing the European interoperability testing facility landscape through a structured survey of 30 facilities. It provides a categorized inventory of testing infrastructures, applied methodologies, and reference test cases, and introduces a blueprint for the development of future testing environments. The findings contribute to the establishment of a coordinated European ecosystem for interoperability testing, supporting collaboration, innovation, and alignment with the goals of the energy transition.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Chain Sealed-Bid Auctions Using Confidential Compute Blockchains</title>
<link>https://arxiv.org/abs/2510.19491</link>
<guid>https://arxiv.org/abs/2510.19491</guid>
<content:encoded><![CDATA[
arXiv:2510.19491v1 Announce Type: new 
Abstract: Sealed-bid auctions ensure fair competition and efficient allocation but are often deployed on centralized infrastructure, enabling opaque manipulation. Public blockchains eliminate central control, yet their inherent transparency conflicts with the confidentiality required for sealed bidding. Prior attempts struggle to reconcile privacy, verifiability, and scalability without relying on trusted intermediaries, multi-round protocols, or expensive cryptography. We present a sealed-bid auction protocol that executes sensitive bidding logic on a Trusted Execution Environment (TEE)-backed confidential compute blockchain while retaining settlement and enforcement on a public chain. Bidders commit funds to enclave-generated escrow addresses, ensuring confidentiality and binding commitments. After the deadline, any party can trigger resolution: the confidential blockchain determines the winner through verifiable off-chain computation and issues signed settlement transactions for execution on the public chain. Our design provides security, privacy, and scalability without trusted third parties or protocol modifications. We implement it on SUAVE with Ethereum settlement, evaluate its scalability and trust assumptions, and demonstrate deployment with minimal integration on existing infrastructure
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Scaling LT-Coded Blockchains in Heterogeneous Networks and their Vulnerabilities to DoS Threats</title>
<link>https://arxiv.org/abs/2402.05620</link>
<guid>https://arxiv.org/abs/2402.05620</guid>
<content:encoded><![CDATA[
arXiv:2402.05620v3 Announce Type: replace 
Abstract: Coded blockchains have acquired prominence as a promising solution to reduce storage costs and facilitate scalability. Within this class, Luby Transform (LT) coded blockchains are an appealing choice for scalability owing to the availability of a wide range of low-complexity decoders. In the first part of this work, we identify that traditional LT decoders like Belief Propagation and On-the-Fly Gaussian Elimination may not be optimal for heterogeneous networks with nodes that have varying computational and download capabilities. To address this, we introduce a family of hybrid decoders for LT codes and propose optimal operating regimes for them to recover the blockchain at the lowest decoding cost. While LT coded blockchain architecture has been studied from the aspects of storage savings and scalability, not much is known in terms of its security vulnerabilities. Pointing at this research gap, in the second part, we present novel denial-of-service threats on LT coded blockchains that target nodes with specific decoding capabilities, preventing them from joining the network. Our proposed threats are non-oblivious in nature, wherein adversaries gain access to the archived blocks, and choose to execute their attack on a subset of them based on underlying coding scheme. We show that our optimized threats can achieve the same level of damage as that of blind attacks, however, with limited amount of resources. Overall, this is the first work of its kind that opens up new questions on designing coded blockchains to jointly provide storage savings, scalability and also resilience to optimized threats.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn More by Using Less: Distributed Learning with Energy-Constrained Devices</title>
<link>https://arxiv.org/abs/2412.02289</link>
<guid>https://arxiv.org/abs/2412.02289</guid>
<content:encoded><![CDATA[
arXiv:2412.02289v2 Announce Type: replace 
Abstract: Federated Learning (FL) has emerged as a solution for distributed model training across decentralized, privacy-preserving devices, but the different energy capacities of participating devices (system heterogeneity) constrain real-world implementations. These energy limitations not only reduce model accuracy but also increase dropout rates, impacting on convergence in practical FL deployments. In this work, we propose LeanFed, an energy-aware FL framework designed to optimize client selection and training workloads on battery-constrained devices. LeanFed leverages adaptive data usage by dynamically adjusting the fraction of local data each device utilizes during training, thereby maximizing device participation across communication rounds while ensuring they do not run out of battery during the process. We rigorously evaluate LeanFed against traditional FedAvg on CIFAR-10 and CIFAR-100 datasets, simulating various levels of data heterogeneity and device participation rates. Results show that LeanFed consistently enhances model accuracy and stability, particularly in settings with high data heterogeneity and limited battery life, by mitigating client dropout and extending device availability. This approach demonstrates the potential of energy-efficient, privacy-preserving FL in real-world, large-scale applications, setting a foundation for robust and sustainable pervasive AI on resource-constrained networks.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Becoming Immutable: How Ethereum is Made</title>
<link>https://arxiv.org/abs/2506.04940</link>
<guid>https://arxiv.org/abs/2506.04940</guid>
<content:encoded><![CDATA[
arXiv:2506.04940v2 Announce Type: replace-cross 
Abstract: We collect and study 15,097 blocks proposed for inclusion in the Ethereum blockchain during 8 minutes on December 3rd, 2024, corresponding to 39 added blocks. These proposed blocks contain 10,793 unique transactions, and 2,380,014 transaction-block pairings, our primary unit of analysis. We find that 20% of user transactions are delayed: although proposed during a bidding cycle, they are not included in the corresponding winning block. Approximately 30% of such delayed transactions are exclusive to a losing builder. We also identify two arbitrage bots trading between decentralized (DEX) and centralized exchanges (CEX), which are responsible for a significant fraction of the value of proposed blocks. By examining their bidding dynamics, we estimate that the implied price at which these bots trade USDC/WETH and USDT/WETH on CEXes is approximately 2.8 basis points better than the contemporaneous Binance price.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RiskTagger: An LLM-based Agent for Automatic Annotation of Web3 Crypto Money Laundering Behaviors</title>
<link>https://arxiv.org/abs/2510.17848</link>
<guid>https://arxiv.org/abs/2510.17848</guid>
<content:encoded><![CDATA[
arXiv:2510.17848v1 Announce Type: new 
Abstract: While the rapid growth of Web3 has driven the development of decentralized finance, user anonymity and cross-chain asset flows make on-chain laundering behaviors more covert and complex. In this context, constructing high-quality anti-money laundering(AML) datasets has become essential for risk-control systems and on-chain forensic analysis, yet current practices still rely heavily on manual efforts with limited efficiency and coverage. In this paper, we introduce RiskTagger, a large-language-model-based agent for the automatic annotation of crypto laundering behaviors in Web3. RiskTagger is designed to replace or complement human annotators by addressing three key challenges: extracting clues from complex unstructured reports, reasoning over multichain transaction paths, and producing auditor-friendly explanations. RiskTagger implements an end-to-end multi-module agent, integrating a key-clue extractor, a multichain fetcher with a laundering-behavior reasoner, and a data explainer, forming a data annotation pipeline. Experiments on the real case Bybit Hack (with the highest stolen asset value) demonstrate that RiskTagger achieves 100% accuracy in clue extraction, 84.1% consistency with expert judgment, and 90% coverage in explanation generation. Overall, RiskTagger automates laundering behavior annotation while improving transparency and scalability in AML research.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smart Contracts Formal Verification: A Systematic Literature Review</title>
<link>https://arxiv.org/abs/2510.17865</link>
<guid>https://arxiv.org/abs/2510.17865</guid>
<content:encoded><![CDATA[
arXiv:2510.17865v1 Announce Type: new 
Abstract: Formal verification entails testing software to ensure it operates as specified. Smart contracts are self-executing contracts with the terms of the agreement directly written into lines of code. They run on blockchain platforms and automatically enforce and execute the terms of an agreement when meeting predefined conditions. However, Smart Contracts, as software models, often contain notable errors in their operation or specifications. This observation prompts us to conduct a focused study examining related works published across various sources. These publications detail specifications, verification tools, and relevant experiments. Subsequently, this survey proposes an alternative formal verification based on description logic.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Federated Unlearning for Large Language Models</title>
<link>https://arxiv.org/abs/2510.17895</link>
<guid>https://arxiv.org/abs/2510.17895</guid>
<content:encoded><![CDATA[
arXiv:2510.17895v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly integrated into real-world applications, raising concerns about privacy, security and the need to remove undesirable knowledge. Machine Unlearning has emerged as a promising solution, yet faces two key challenges: (1) practical unlearning needs are often continuous and heterogeneous, and (2) they involve decentralized, sensitive data with asymmetric access. These factors result in inter-domain and intra-domain interference, which further amplifies the dilemma of unbalanced forgetting and retaining performance. In response, we propose a federated unlearning approach for LLMs that is scalable and privacy preserving. Our method decouples unlearning and retention via task-specific adapter learning and employs a hierarchical merging strategy to mitigate conflicting objectives and enables robust, adaptable unlearning updates. Comprehensive experiments on benchmarks of WMDP, MUSE, and TOFU showed that our approach effectively handles heterogeneous unlearning requests while maintaining strong LLM utility compared with baseline methods.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Sherpa.ai Blind Vertical Federated Learning Paradigm to Minimize the Number of Communications</title>
<link>https://arxiv.org/abs/2510.17901</link>
<guid>https://arxiv.org/abs/2510.17901</guid>
<content:encoded><![CDATA[
arXiv:2510.17901v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative decentralized training across multiple parties (nodes) while keeping raw data private. There are two main paradigms in FL: Horizontal FL (HFL), where all participant nodes share the same feature space but hold different samples, and Vertical FL (VFL), where participants hold complementary features for the same samples. While HFL is widely adopted, VFL is employed in domains where nodes hold complementary features about the same samples. Still, VFL presents a significant limitation: the vast number of communications required during training. This compromises privacy and security, and can lead to high energy consumption, and in some cases, make model training unfeasible due to the high number of communications.
  In this paper, we introduce Sherpa.ai Blind Vertical Federated Learning (SBVFL), a novel paradigm that leverages a distributed training mechanism enhanced for privacy and security. Decoupling the vast majority of node updates from the server dramatically reduces node-server communication. Experiments show that SBVFL reduces communication by ~99% compared to standard VFL while maintaining accuracy and robustness. Therefore, SBVFL enables practical, privacy-preserving VFL across sensitive domains, including healthcare, finance, manufacturing, aerospace, cybersecurity, and the defense industry.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParaVul: A Parallel Large Language Model and Retrieval-Augmented Framework for Smart Contract Vulnerability Detection</title>
<link>https://arxiv.org/abs/2510.17919</link>
<guid>https://arxiv.org/abs/2510.17919</guid>
<content:encoded><![CDATA[
arXiv:2510.17919v1 Announce Type: new 
Abstract: Smart contracts play a significant role in automating blockchain services. Nevertheless, vulnerabilities in smart contracts pose serious threats to blockchain security. Currently, traditional detection methods primarily rely on static analysis and formal verification, which can result in high false-positive rates and poor scalability. Large Language Models (LLMs) have recently made significant progress in smart contract vulnerability detection. However, they still face challenges such as high inference costs and substantial computational overhead. In this paper, we propose ParaVul, a parallel LLM and retrieval-augmented framework to improve the reliability and accuracy of smart contract vulnerability detection. Specifically, we first develop Sparse Low-Rank Adaptation (SLoRA) for LLM fine-tuning. SLoRA introduces sparsification by incorporating a sparse matrix into quantized LoRA-based LLMs, thereby reducing computational overhead and resource requirements while enhancing their ability to understand vulnerability-related issues. We then construct a vulnerability contract dataset and develop a hybrid Retrieval-Augmented Generation (RAG) system that integrates dense retrieval with Best Matching 25 (BM25), assisting in verifying the results generated by the LLM. Furthermore, we propose a meta-learning model to fuse the outputs of the RAG system and the LLM, thereby generating the final detection results. After completing vulnerability detection, we design chain-of-thought prompts to guide LLMs to generate comprehensive vulnerability detection reports. Simulation results demonstrate the superiority of ParaVul, especially in terms of F1 scores, achieving 0.9398 for single-label detection and 0.9330 for multi-label detection.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPACeR: Self-Play Anchoring with Centralized Reference Models</title>
<link>https://arxiv.org/abs/2510.18060</link>
<guid>https://arxiv.org/abs/2510.18060</guid>
<content:encoded><![CDATA[
arXiv:2510.18060v1 Announce Type: new 
Abstract: Developing autonomous vehicles (AVs) requires not only safety and efficiency, but also realistic, human-like behaviors that are socially aware and predictable. Achieving this requires sim agent policies that are human-like, fast, and scalable in multi-agent settings. Recent progress in imitation learning with large diffusion-based or tokenized models has shown that behaviors can be captured directly from human driving data, producing realistic policies. However, these models are computationally expensive, slow during inference, and struggle to adapt in reactive, closed-loop scenarios. In contrast, self-play reinforcement learning (RL) scales efficiently and naturally captures multi-agent interactions, but it often relies on heuristics and reward shaping, and the resulting policies can diverge from human norms. We propose SPACeR, a framework that leverages a pretrained tokenized autoregressive motion model as a centralized reference policy to guide decentralized self-play. The reference model provides likelihood rewards and KL divergence, anchoring policies to the human driving distribution while preserving RL scalability. Evaluated on the Waymo Sim Agents Challenge, our method achieves competitive performance with imitation-learned policies while being up to 10x faster at inference and 50x smaller in parameter size than large generative models. In addition, we demonstrate in closed-loop ego planning evaluation tasks that our sim agents can effectively measure planner quality with fast and scalable traffic simulation, establishing a new paradigm for testing autonomous driving policies.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PrivaDE: Privacy-preserving Data Evaluation for Blockchain-based Data Marketplaces</title>
<link>https://arxiv.org/abs/2510.18109</link>
<guid>https://arxiv.org/abs/2510.18109</guid>
<content:encoded><![CDATA[
arXiv:2510.18109v1 Announce Type: new 
Abstract: Evaluating the relevance of data is a critical task for model builders seeking to acquire datasets that enhance model performance. Ideally, such evaluation should allow the model builder to assess the utility of candidate data without exposing proprietary details of the model. At the same time, data providers must be assured that no information about their data - beyond the computed utility score - is disclosed to the model builder.
  In this paper, we present PrivaDE, a cryptographic protocol for privacy-preserving utility scoring and selection of data for machine learning. While prior works have proposed data evaluation protocols, our approach advances the state of the art through a practical, blockchain-centric design. Leveraging the trustless nature of blockchains, PrivaDE enforces malicious-security guarantees and ensures strong privacy protection for both models and datasets. To achieve efficiency, we integrate several techniques - including model distillation, model splitting, and cut-and-choose zero-knowledge proofs - bringing the runtime to a practical level. Furthermore, we propose a unified utility scoring function that combines empirical loss, predictive entropy, and feature-space diversity, and that can be seamlessly integrated into active-learning workflows. Evaluation shows that PrivaDE performs data evaluation effectively, achieving online runtimes within 15 minutes even for models with millions of parameters.
  Our work lays the foundation for fair and automated data marketplaces in decentralized machine learning ecosystems.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TaintSentinel: Path-Level Randomness Vulnerability Detection for Ethereum Smart Contracts</title>
<link>https://arxiv.org/abs/2510.18192</link>
<guid>https://arxiv.org/abs/2510.18192</guid>
<content:encoded><![CDATA[
arXiv:2510.18192v1 Announce Type: new 
Abstract: The inherent determinism of blockchain technology poses a significant challenge to generating secure random numbers within smart contracts, leading to exploitable vulnerabilities, particularly in decentralized finance (DeFi) ecosystems and blockchain-based gaming applications. From our observations, the current state-of-the-art detection tools suffer from inadequate precision while dealing with random number vulnerabilities. To address this problem, we propose TaintSentinel, a novel path sensitive vulnerability detection system designed to analyze smart contracts at the execution path level and gradually analyze taint with domain-specific rules. This paper discusses a solution that incorporates a multi-faceted approach, integrating rule-based taint analysis to track data flow, a dual stream neural network to identify complex vulnerability signatures, and evidence-based parameter initialization to minimize false positives. The system's two-phase operation involves semantic graph construction and taint propagation analysis, followed by pattern recognition using PathGNN and global structural analysis via GlobalGCN. Our experiments on 4,844 contracts demonstrate the superior performance of TaintSentinel relative to existing tools, yielding an F1-score of 0.892, an AUC-ROC of 0.94, and a PRA accuracy of 97%.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Optimization of Cooperation Efficiency and Communication Covertness for Target Detection with AUVs</title>
<link>https://arxiv.org/abs/2510.18225</link>
<guid>https://arxiv.org/abs/2510.18225</guid>
<content:encoded><![CDATA[
arXiv:2510.18225v1 Announce Type: new 
Abstract: This paper investigates underwater cooperative target detection using autonomous underwater vehicles (AUVs), with a focus on the critical trade-off between cooperation efficiency and communication covertness. To tackle this challenge, we first formulate a joint trajectory and power control optimization problem, and then present an innovative hierarchical action management framework to solve it. According to the hierarchical formulation, at the macro level, the master AUV models the agent selection process as a Markov decision process and deploys the proximal policy optimization algorithm for strategic task allocation. At the micro level, each selected agent's decentralized decision-making is modeled as a partially observable Markov decision process, and a multi-agent proximal policy optimization algorithm is used to dynamically adjust its trajectory and transmission power based on its local observations. Under the centralized training and decentralized execution paradigm, our target detection framework enables adaptive covert cooperation while satisfying both energy and mobility constraints. By comprehensively modeling the considered system, the involved signals and tasks, as well as energy consumption, theoretical insights and practical solutions for the efficient and secure operation of multiple AUVs are provided, offering significant implications for the execution of underwater covert communication tasks.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Healthcare Data in IoT: A Synergistic Approach with Deep Learning and Blockchain</title>
<link>https://arxiv.org/abs/2510.18568</link>
<guid>https://arxiv.org/abs/2510.18568</guid>
<content:encoded><![CDATA[
arXiv:2510.18568v1 Announce Type: new 
Abstract: The integration of Internet of Things (IoT) devices in healthcare has revolutionized patient care by enabling real-time monitoring, personalized treatments, and efficient data management. However, this technological advancement introduces significant security risks, particularly concerning the confidentiality, integrity, and availability of sensitive medical data. Traditional security measures are often insufficient to address the unique challenges posed by IoT environments, such as heterogeneity, resource constraints, and the need for real-time processing. To tackle these challenges, we propose a comprehensive three-phase security framework designed to enhance the security and reliability of IoT-enabled healthcare systems. In the first phase, the framework assesses the reliability of IoT devices using a reputation-based trust estimation mechanism, which combines device behavior analytics with off-chain data storage to ensure scalability. The second phase integrates blockchain technology with a lightweight proof-of-work mechanism, ensuring data immutability, secure communication, and resistance to unauthorized access. The third phase employs a lightweight Long Short-Term Memory (LSTM) model for anomaly detection and classification, enabling real-time identification of cyber threats. Simulation results demonstrate that the proposed framework outperforms existing methods, achieving a 2% increase in precision, accuracy, and recall, a 5% higher attack detection rate, and a 3% reduction in false alarm rate. These improvements highlight the framework's ability to address critical security concerns while maintaining scalability and real-time performance.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fetch.ai: An Architecture for Modern Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2510.18699</link>
<guid>https://arxiv.org/abs/2510.18699</guid>
<content:encoded><![CDATA[
arXiv:2510.18699v1 Announce Type: new 
Abstract: Recent surges in LLM-driven intelligent systems largely overlook decades of foundational multi-agent systems (MAS) research, resulting in frameworks with critical limitations such as centralization and inadequate trust and communication protocols. This paper introduces the Fetch.ai architecture, an industrial-strength platform designed to bridge this gap by facilitating the integration of classical MAS principles with modern AI capabilities. We present a novel, multi-layered solution built on a decentralized foundation of on-chain blockchain services for verifiable identity, discovery, and transactions. This is complemented by a comprehensive development framework for creating secure, interoperable agents, a cloud-based platform for deployment, and an intelligent orchestration layer where an agent-native LLM translates high-level human goals into complex, multi-agent workflows. We demonstrate the deployed nature of this system through a decentralized logistics use case where autonomous agents dynamically discover, negotiate, and transact with one another securely. Ultimately, the Fetch.ai stack provides a principled architecture for moving beyond current agent implementations towards open, collaborative, and economically sustainable multi-agent ecosystems.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transaction Capacity, Security and Latency in Blockchains</title>
<link>https://arxiv.org/abs/2402.10138</link>
<guid>https://arxiv.org/abs/2402.10138</guid>
<content:encoded><![CDATA[
arXiv:2402.10138v2 Announce Type: replace 
Abstract: We analyze how secure a block is after the block becomes $k$-deep, i.e., security-latency, for Nakamoto consensus under an exponential network delay model. We provide the fault tolerance and extensive bounds on safety violation probabilities given mining rate, delay rate and confirmation rules. Next, modeling the blockchain system as a batch service queue with exponential network delay, we connect the security-latency analysis to sustainable transaction rate of the queue system. As our model assumes exponential network delay, batch service queue models give a meaningful trade-off between transaction capacity, security and latency. Our results indicate that, by simply picking $k=7$-block confirmation rule in Bitcoin instead of the convention of $k=6$, mining rate, latency and throughput can be increased sixfold with the same safety guarantees. We further consider adversarial attacks on the queue service to hamper the service process. In an extreme scenario, we consider the selfish-mining attack for this purpose and provide the maximum adversarial block ratio in the longest chain under the exponential delay model. The ratio in turn reflects the maximum rate of decrease in the sustainable transaction rate of the queue.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BlockScan: Detecting Anomalies in Blockchain Transactions</title>
<link>https://arxiv.org/abs/2410.04039</link>
<guid>https://arxiv.org/abs/2410.04039</guid>
<content:encoded><![CDATA[
arXiv:2410.04039v5 Announce Type: replace 
Abstract: We propose BlockScan, a customized Transformer for anomaly detection in blockchain transactions. Unlike existing methods that rely on rule-based systems or directly apply off-the-shelf large language models (LLMs), BlockScan introduces a series of customized designs to effectively model the unique data structure of blockchain transactions. First, a blockchain transaction is multi-modal, containing blockchain-specific tokens, texts, and numbers. We design a novel modularized tokenizer to handle these multi-modal inputs, balancing the information across different modalities. Second, we design a customized masked language modeling mechanism for pretraining the Transformer architecture, incorporating RoPE embedding and FlashAttention for handling longer sequences. Finally, we design a novel anomaly detection method based on the model outputs. We further provide theoretical analysis for the detection method of our system. Extensive evaluations on Ethereum and Solana transactions demonstrate BlockScan's exceptional capability in anomaly detection while maintaining a low false positive rate. Remarkably, BlockScan is the only method that successfully detects anomalous transactions on Solana with high accuracy, whereas all other approaches achieved very low or zero detection recall scores. This work sets a new benchmark for applying Transformer-based approaches in blockchain data analysis.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asynchronous Federated Learning: A Scalable Approach for Decentralized Machine Learning</title>
<link>https://arxiv.org/abs/2412.17723</link>
<guid>https://arxiv.org/abs/2412.17723</guid>
<content:encoded><![CDATA[
arXiv:2412.17723v4 Announce Type: replace 
Abstract: Federated Learning (FL) has emerged as a powerful paradigm for decentralized machine learning, enabling collaborative model training across diverse clients without sharing raw data. However, traditional FL approaches often face limitations in scalability and efficiency due to their reliance on synchronous client updates, which can result in significant delays and increased communication overhead, particularly in heterogeneous and dynamic environments. To address these challenges in this paper, we propose an Asynchronous Federated Learning (AFL) algorithm, which allows clients to update the global model independently and asynchronously. Our key contributions include a comprehensive convergence analysis of AFL in the presence of client delays and model staleness. By leveraging martingale difference sequence theory and variance bounds, we ensure robust convergence despite asynchronous updates. Assuming strongly convex local objective functions, we establish bounds on gradient variance under random client sampling and derive a recursion formula quantifying the impact of client delays on convergence. Furthermore, we demonstrate the practical applicability of the AFL algorithm by training decentralized linear regression and Support Vector Machine (SVM) based classifiers and compare its results with synchronous FL algorithm to effectively handling non-IID data distributed among clients. The proposed AFL algorithm addresses key limitations of traditional FL methods, such as inefficiency due to global synchronization and susceptibility to client drift. It enhances scalability, robustness, and efficiency in real-world settings with heterogeneous client populations and dynamic network conditions. Our results underscore the potential of AFL to drive advancements indistributed learning systems, particularly for large-scale, privacy-preserving applications in resource-constrained environments.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wonder Wins Ways: Curiosity-Driven Exploration through Multi-Agent Contextual Calibration</title>
<link>https://arxiv.org/abs/2509.20648</link>
<guid>https://arxiv.org/abs/2509.20648</guid>
<content:encoded><![CDATA[
arXiv:2509.20648v2 Announce Type: replace 
Abstract: Autonomous exploration in complex multi-agent reinforcement learning (MARL) with sparse rewards critically depends on providing agents with effective intrinsic motivation. While artificial curiosity offers a powerful self-supervised signal, it often confuses environmental stochasticity with meaningful novelty. Moreover, existing curiosity mechanisms exhibit a uniform novelty bias, treating all unexpected observations equally. However, peer behavior novelty, which encode latent task dynamics, are often overlooked, resulting in suboptimal exploration in decentralized, communication-free MARL settings. To this end, inspired by how human children adaptively calibrate their own exploratory behaviors via observing peers, we propose a novel approach to enhance multi-agent exploration. We introduce CERMIC, a principled framework that empowers agents to robustly filter noisy surprise signals and guide exploration by dynamically calibrating their intrinsic curiosity with inferred multi-agent context. Additionally, CERMIC generates theoretically-grounded intrinsic rewards, encouraging agents to explore state transitions with high information gain. We evaluate CERMIC on benchmark suites including VMAS, Meltingpot, and SMACv2. Empirical results demonstrate that exploration with CERMIC significantly outperforms SoTA algorithms in sparse-reward environments.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Prompt Refinement with Non-Interfering Attention Masking for One-Shot Federated Learning</title>
<link>https://arxiv.org/abs/2509.22700</link>
<guid>https://arxiv.org/abs/2509.22700</guid>
<content:encoded><![CDATA[
arXiv:2509.22700v2 Announce Type: replace 
Abstract: Federated Prompt Learning (FPL) enables communication-efficient adaptation by tuning lightweight prompts on top of frozen pre-trained models. Existing FPL methods typically rely on global information, which is only available after the second training round, to facilitate collaboration among client models. Therefore, they are inherently dependent on multi-round communication to fully exhibit their strengths. Moreover, existing one-shot federated learning methods typically focus on fitting seen tasks, but lack cross-task generalization. To bridge this gap, we propose the Global Prompt Refinement with Non-Interfering Attention Masking (GPR-NIAM) method for one-shot FPL. The core idea is to design a masking mechanism that restricts excessive interaction between the original text embeddings and the learnable prompt embeddings. GPR-NIAM achieves this through the collaboration of two key modules. Firstly, the attention isolation module suppresses attention from the learnable prompt tokens to the original text tokens, and reweights the reverse attention which preserves generalization across tasks. Secondly, the cross-silo collaborative refinement module integrates decentralized visual knowledge into a unified base and calibrates the global prompt through multi-source cross-modal knowledge alignment, further mitigating the inconsistency caused by data heterogeneity. Extensive experiments conducted on ten benchmark datasets under two tasks show that GPR-NIAM outperforms eight state-of-the-art methods in both class-level and domain-level generalization.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoCks - Simplifying Firmware and Software Integration for Heterogeneous SoCs</title>
<link>https://arxiv.org/abs/2510.15910</link>
<guid>https://arxiv.org/abs/2510.15910</guid>
<content:encoded><![CDATA[
arXiv:2510.15910v1 Announce Type: new 
Abstract: Modern heterogeneous System-on-Chip (SoC) devices integrate advanced components into a single package, offering powerful capabilities while also introducing significant complexity. To manage these sophisticated devices, firmware and software developers need powerful development tools. However, as these tools become increasingly complex, they often lack adequate support, resulting in a steep learning curve and challenging troubleshooting. To address this, this work introduces System-on-Chip blocks (SoCks), a flexible and expandable build framework that reduces complexity by partitioning the SoC image into high-level units called blocks. SoCks builds each firmware and software block in an encapsulated way, independently from other components of the image, thereby reducing dependencies to a minimum. While some information exchange between the blocks is unavoidable to ensure seamless runtime integration, this interaction is standardized via interfaces. A small number of dependencies and well-defined interfaces simplify the reuse of existing block implementations and facilitate seamless substitution between versions-for instance, when choosing root file systems for the embedded Linux operating system. Additionally, this approach facilitates the establishment of a decentralized and partially automated development flow through Continuous Integration and Continuous Delivery (CI/CD). Measurement results demonstrate that SoCks can build a complete SoC image up to three times faster than established tools.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On-Chain Decentralized Learning and Cost-Effective Inference for DeFi Attack Mitigation</title>
<link>https://arxiv.org/abs/2510.16024</link>
<guid>https://arxiv.org/abs/2510.16024</guid>
<content:encoded><![CDATA[
arXiv:2510.16024v1 Announce Type: new 
Abstract: Billions of dollars are lost every year in DeFi platforms by transactions exploiting business logic or accounting vulnerabilities. Existing defenses focus on static code analysis, public mempool screening, attacker contract detection, or trusted off-chain monitors, none of which prevents exploits submitted through private relays or malicious contracts that execute within the same block. We present the first decentralized, fully on-chain learning framework that: (i) performs gas-prohibitive computation on Layer-2 to reduce cost, (ii) propagates verified model updates to Layer-1, and (iii) enables gas-bounded, low-latency inference inside smart contracts. A novel Proof-of-Improvement (PoIm) protocol governs the training process and verifies each decentralized micro update as a self-verifying training transaction. Updates are accepted by \textit{PoIm} only if they demonstrably improve at least one core metric (e.g., accuracy, F1-score, precision, or recall) on a public benchmark without degrading any of the other core metrics, while adversarial proposals get financially penalized through an adaptable test set for evolving threats. We develop quantization and loop-unrolling techniques that enable inference for logistic regression, SVM, MLPs, CNNs, and gated RNNs (with support for formally verified decision tree inference) within the Ethereum block gas limit, while remaining bit-exact to their off-chain counterparts, formally proven in Z3. We curate 298 unique real-world exploits (2020 - 2025) with 402 exploit transactions across eight EVM chains, collectively responsible for \$3.74 B in losses.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nondeterminism-Aware Optimistic Verification for Floating-Point Neural Networks</title>
<link>https://arxiv.org/abs/2510.16028</link>
<guid>https://arxiv.org/abs/2510.16028</guid>
<content:encoded><![CDATA[
arXiv:2510.16028v1 Announce Type: new 
Abstract: Neural networks increasingly run on hardware outside the user's control (cloud GPUs, inference marketplaces). Yet ML-as-a-Service reveals little about what actually ran or whether returned outputs faithfully reflect the intended inputs. Users lack recourse against service downgrades (model swaps, quantization, graph rewrites, or discrepancies like altered ad embeddings). Verifying outputs is hard because floating-point(FP) execution on heterogeneous accelerators is inherently nondeterministic. Existing approaches are either impractical for real FP neural networks or reintroduce vendor trust. We present NAO: a Nondeterministic tolerance Aware Optimistic verification protocol that accepts outputs within principled operator-level acceptance regions rather than requiring bitwise equality. NAO combines two error models: (i) sound per-operator IEEE-754 worst-case bounds and (ii) tight empirical percentile profiles calibrated across hardware. Discrepancies trigger a Merkle-anchored, threshold-guided dispute game that recursively partitions the computation graph until one operator remains, where adjudication reduces to a lightweight theoretical-bound check or a small honest-majority vote against empirical thresholds. Unchallenged results finalize after a challenge window, without requiring trusted hardware or deterministic kernels. We implement NAO as a PyTorch-compatible runtime and a contract layer currently deployed on Ethereum Holesky testnet. The runtime instruments graphs, computes per-operator bounds, and runs unmodified vendor kernels in FP32 with negligible overhead (0.3% on Qwen3-8B). Across CNNs, Transformers and diffusion models on A100, H100, RTX6000, RTX4090, empirical thresholds are $10^2-10^3$ times tighter than theoretical bounds, and bound-aware adversarial attacks achieve 0% success. NAO reconciles scalability with verifiability for real-world heterogeneous ML compute.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework For Decentralized Micro-credential Verification Towards Higher Qualifications</title>
<link>https://arxiv.org/abs/2510.16050</link>
<guid>https://arxiv.org/abs/2510.16050</guid>
<content:encoded><![CDATA[
arXiv:2510.16050v1 Announce Type: new 
Abstract: Student retention is one of the rising problems seen in educational institutions. With the rising cost of education and issues in the education sector, such as curriculum relevance, student engagement, and rapidly changing technological advancements, ensuring the relevance of academic programs in a fast-evolving job market has created a significant concern for educational institutions. With the intent to adapt to such challenges, educational institutions are dealing with alternative solutions for education, in which micro-credentials are at the very center of this, which are short-term academic programs or standalone courses. However, one of the challenges of micro-credentials is a lack of credit transfer among institutions. With the lack of standardization of assessments among educational institutions, it is difficult to transfer micro-credentials to larger qualifications. Regarding such challenges, micro-credentials with blockchain technology can bring significant benefits. Blockchain technology offers a decentralized and immutable platform for securely storing and verifying credentials. This paper presents a prototype model for micro-credential verification. With the policies decided by the educational institution, the learner provides a micro-credential certificate to the system. Upon validation of the certificate by the verifying body, the educational institution will review the assessment criteria and provide exemptions based on the provided criteria. The prototype uses the Hyper-ledger Fabric platform and utilizes off-chain technology, which acts as a middle-man storage platform. With the combination of off-chain and on-chain technologies, congestion on the blockchain is reduced, and transaction speed is improved. In summary, this research proposes a prototype for secure micro-credential verification and a more efficient course exemption process.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Blockchain-Based CI/CD Framework to Enhance Security in Cloud Environments</title>
<link>https://arxiv.org/abs/2510.16087</link>
<guid>https://arxiv.org/abs/2510.16087</guid>
<content:encoded><![CDATA[
arXiv:2510.16087v1 Announce Type: new 
Abstract: Security is becoming a pivotal point in cloud platforms. Several divisions, such as business organisations, health care, government, etc., have experienced cyber-attacks on their infrastructures. This research focuses on security issues within Continuous Integration and Deployment (CI/CD) pipelines in a cloud platform as a reaction to recent cyber breaches. This research proposes a blockchain-based solution to enhance CI/CD pipeline security. This research aims to develop a framework that leverages blockchain's distributed ledger technology and tamper-resistant features to improve CI/CD pipeline security. The goal is to emphasise secure software deployment by integrating threat modelling frameworks and adherence to coding standards. It also aims to employ tools to automate security testing to detect publicly disclosed vulnerabilities and flaws, such as an outdated version of Java Spring Framework, a JavaScript library from an unverified source, or a database library that allows SQL injection attacks in the deployed software through the framework.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SentinelNet: Safeguarding Multi-Agent Collaboration Through Credit-Based Dynamic Threat Detection</title>
<link>https://arxiv.org/abs/2510.16219</link>
<guid>https://arxiv.org/abs/2510.16219</guid>
<content:encoded><![CDATA[
arXiv:2510.16219v1 Announce Type: new 
Abstract: Malicious agents pose significant threats to the reliability and decision-making capabilities of Multi-Agent Systems (MAS) powered by Large Language Models (LLMs). Existing defenses often fall short due to reactive designs or centralized architectures which may introduce single points of failure. To address these challenges, we propose SentinelNet, the first decentralized framework for proactively detecting and mitigating malicious behaviors in multi-agent collaboration. SentinelNet equips each agent with a credit-based detector trained via contrastive learning on augmented adversarial debate trajectories, enabling autonomous evaluation of message credibility and dynamic neighbor ranking via bottom-k elimination to suppress malicious communications. To overcome the scarcity of attack data, it generates adversarial trajectories simulating diverse threats, ensuring robust training. Experiments on MAS benchmarks show SentinelNet achieves near-perfect detection of malicious agents, close to 100% within two debate rounds, and recovers 95% of system accuracy from compromised baselines. By exhibiting strong generalizability across domains and attack patterns, SentinelNet establishes a novel paradigm for safeguarding collaborative MAS.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DESTinE Block: Private Blockchain Based Data Storage Framework for Power System</title>
<link>https://arxiv.org/abs/2510.16593</link>
<guid>https://arxiv.org/abs/2510.16593</guid>
<content:encoded><![CDATA[
arXiv:2510.16593v1 Announce Type: new 
Abstract: This paper presents DESTinE Block, a blockchain-based data storage framework designed for power systems and optimized for resource-constrained environments, including grid-edge devices such as single-board computers. The proposed architecture leverages the InterPlanetary File System (IPFS) for storing large files while maintaining secure and traceable metadata on a custom blockchain named DESTinE Block. The metadata, comprising the IPFS Content Identifier (CID), uploader identity, administrator verification, and timestamp; is immutably recorded on-chain to ensure authenticity and integrity. DESTinE Block adopts a dual-blockchain abstraction, where the blockchain remains unaware of the IPFS storage layer to enhance security and limit the exposure of sensitive file data. The consensus mechanism is based on Proof of Authority (PoA), where both an administrator and an uploader with distinct cryptographic key pairs are required to create a block collaboratively. Each block contains verified signatures of both parties and is designed to be computationally efficient, enabling deployment on devices like the Raspberry Pi 5. The framework was tested on both an x86-based device and an ARM64-based Raspberry Pi, demonstrating its potential for secure, decentralized logging and measurement storage in smart grid applications. Moreover, DESTinE Block is compared with a similar framework based on Multichain. The results indicate that DESTinE Block provides a promising solution for tamper-evident data retention in distributed power system infrastructure while maintaining minimal hardware requirements.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Verifiable Fine-Tuning for LLMs: Zero-Knowledge Training Proofs Bound to Data Provenance and Policy</title>
<link>https://arxiv.org/abs/2510.16830</link>
<guid>https://arxiv.org/abs/2510.16830</guid>
<content:encoded><![CDATA[
arXiv:2510.16830v1 Announce Type: new 
Abstract: Large language models are often adapted through parameter efficient fine tuning, but current release practices provide weak assurances about what data were used and how updates were computed. We present Verifiable Fine Tuning, a protocol and system that produces succinct zero knowledge proofs that a released model was obtained from a public initialization under a declared training program and an auditable dataset commitment. The approach combines five elements. First, commitments that bind data sources, preprocessing, licenses, and per epoch quota counters to a manifest. Second, a verifiable sampler that supports public replayable and private index hiding batch selection. Third, update circuits restricted to parameter efficient fine tuning that enforce AdamW style optimizer semantics and proof friendly approximations with explicit error budgets. Fourth, recursive aggregation that folds per step proofs into per epoch and end to end certificates with millisecond verification. Fifth, provenance binding and optional trusted execution property cards that attest code identity and constants. On English and bilingual instruction mixtures, the method maintains utility within tight budgets while achieving practical proof performance. Policy quotas are enforced with zero violations, and private sampling windows show no measurable index leakage. Federated experiments demonstrate that the system composes with probabilistic audits and bandwidth constraints. These results indicate that end to end verifiable fine tuning is feasible today for real parameter efficient pipelines, closing a critical trust gap for regulated and decentralized deployments.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiRAC - Distributed Robot Awareness and Consensus</title>
<link>https://arxiv.org/abs/2510.16850</link>
<guid>https://arxiv.org/abs/2510.16850</guid>
<content:encoded><![CDATA[
arXiv:2510.16850v1 Announce Type: new 
Abstract: DiRAC is a scalable, distributed framework designed to enable efficient task assignment and path planning in very large robotic swarms. It introduces a novel zone-partitioned architecture with dynamically elected leaders and a tick-synchronized consensus protocol that yields strong consistency and deterministic outcomes. For path planning, DiRAC uses a novel algorithm, a force-based decentralized planner for real-time collision resolution. Validated within ROS 2 middleware through preliminary simulation, DiRAC demonstrates architectural scalability and modular efficiency in simulated warehouse environments, laying the groundwork for real-world deployment in large-scale industrial and logistics domains.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Real-Time Planning for Multi-UAV Cooperative Manipulation via Imitation Learning</title>
<link>https://arxiv.org/abs/2510.17143</link>
<guid>https://arxiv.org/abs/2510.17143</guid>
<content:encoded><![CDATA[
arXiv:2510.17143v1 Announce Type: new 
Abstract: Existing approaches for transporting and manipulating cable-suspended loads using multiple UAVs along reference trajectories typically rely on either centralized control architectures or reliable inter-agent communication. In this work, we propose a novel machine learning based method for decentralized kinodynamic planning that operates effectively under partial observability and without inter-agent communication. Our method leverages imitation learning to train a decentralized student policy for each UAV by imitating a centralized kinodynamic motion planner with access to privileged global observations. The student policy generates smooth trajectories using physics-informed neural networks that respect the derivative relationships in motion. During training, the student policies utilize the full trajectory generated by the teacher policy, leading to improved sample efficiency. Moreover, each student policy can be trained in under two hours on a standard laptop. We validate our method in both simulation and real-world environments to follow an agile reference trajectory, demonstrating performance comparable to that of centralized approaches.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of Input-Output Mappings in Coinjoin Transactions with Arbitrary Values</title>
<link>https://arxiv.org/abs/2510.17284</link>
<guid>https://arxiv.org/abs/2510.17284</guid>
<content:encoded><![CDATA[
arXiv:2510.17284v1 Announce Type: new 
Abstract: A coinjoin protocol aims to increase transactional privacy for Bitcoin and Bitcoin-like blockchains via collaborative transactions, by violating assumptions behind common analysis heuristics. Estimating the resulting privacy gain is a crucial yet unsolved problem due to a range of influencing factors and large computational complexity.
  We adapt the BlockSci on-chain analysis software to coinjoin transactions, demonstrating a significant (10-50%) average post-mix anonymity set size decrease for all three major designs with a central coordinator: Whirlpool, Wasabi 1.x, and Wasabi 2.x. The decrease is highest during the first day and negligible after one year from a coinjoin creation.
  Moreover, we design a precise, parallelizable privacy estimation method, which takes into account coinjoin fees, implementation-specific limitations and users' post-mix behavior. We evaluate our method in detail on a set of emulated and real-world Wasabi 2.x coinjoins and extrapolate to its largest real-world coinjoins with hundreds of inputs and outputs. We conclude that despite the users' undesirable post-mix behavior, correctly attributing the coins to their owners is still very difficult, even with our improved analysis algorithm.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Local Combining with Decentralized Decoding for Distributed Massive MIMO</title>
<link>https://arxiv.org/abs/2510.17445</link>
<guid>https://arxiv.org/abs/2510.17445</guid>
<content:encoded><![CDATA[
arXiv:2510.17445v1 Announce Type: new 
Abstract: A major bottleneck in uplink distributed massive multiple-input multiple-output networks is the sub-optimal performance of local combining schemes, coupled with high fronthaul load and computational cost inherent in centralized large scale fading decoding (LSFD) architectures. This paper introduces a decentralized decoding architecture that fundamentally breaks from the conventional LSFD, by allowing each AP calculates interference-suppressing local weights independently and applies them to its data estimates before transmission. Furthermore, two generalized local zero-forcing (ZF) framework, generalized partial full-pilot ZF (G-PFZF) and generalized protected weak PFZF (G-PWPFZF), are introduced, where each access point (AP) adaptively and independently determines its combining strategy through a local sum spectral efficiency optimization that classifies user equipments (UEs) as strong or weak using only local information, eliminating the fixed thresholds used in PFZF and PWPFZF. To further enhance scalability, pilot-dependent combining vectors instead of user-dependent ones are introduced and are shared among users with the same pilot. The corresponding closed-form spectral efficiency expressions are derived. Numerical results show that the proposed generalized schemes consistently outperform fixed-threshold counterparts, while the introduction of local weights yields lower overhead and computation costs with minimal performance penalty compared to them.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-driven Communication and Control Design for Distributed Frequency Regulation with Black-box Inverters</title>
<link>https://arxiv.org/abs/2510.17769</link>
<guid>https://arxiv.org/abs/2510.17769</guid>
<content:encoded><![CDATA[
arXiv:2510.17769v1 Announce Type: new 
Abstract: The increasing penetration of inverter-based resources into the power grid, with often only black-box models available, challenges long-standing frequency control methods. Most recent works take a decentralized approach without online device coordination via communication. This paper considers both dynamic behavior and communication within secondary frequency control on an intermediate timescale. We develop a distributed data-driven approach that utilizes peer-to-peer communication between inverters to avoid the need for a central control center. To enable a trade off between communication network requirements and control performance, we present a framework to guide communication topology design for secondary frequency regulation. Following design of the inter-agent information exchange scheme, we design a controller that is structured according to the communication topology with a closed-loop stability guarantee. Case studies on the IEEE 39-bus system validate the framework and illustrate the trade-off between communication requirements and control performance that is enabled by our approach.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Sensing Performance Design for Enhancing Secure Communication in Networked ISAC Systems</title>
<link>https://arxiv.org/abs/2510.16397</link>
<guid>https://arxiv.org/abs/2510.16397</guid>
<content:encoded><![CDATA[
arXiv:2510.16397v1 Announce Type: cross 
Abstract: The channel state information (CSI) of an eavesdropper is crucial for physical layer security (PLS) design, but it is difficult to obtain due to the passive and non-cooperative nature of the eavesdropper. To this end, integrated sensing and communication (ISAC) offers a novel solution by estimating the CSI of the eavesdropper based on sensing information. However, existing studies normally impose explicit and fixed sensing performance requirement without considering the varying communication conditions, which hinders the system from fully exploiting the synergy between sensing and communication. To address this issue, this paper proposes sensing-enhanced secure communication with adaptive sensing performance. Specifically, we formulate the sensing performance implicitly in the information leakage rate and adaptively optimize it for the minimization of the power consumption, offering enhanced flexibility and adaptability in sensing performance. We consider both centralized and decentralized designs to thoroughly investigate the impact of network structure on system performance and complexity. Specifically, we devise a block coordinate descent (BCD)-based method for centralized design. For decentralized design, we develop an optimization framework based on consensus alternating direction method of multipliers (ADMM) to reduce complexity and information exchange overhead. Experimental results demonstrate the advantage of the proposed implicit sensing performance requirement design due to its capability to adaptively adjust the sensing performance to enhance the system performance for varying system configurations.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Federated Learning: Architectural Elements and Future Directions</title>
<link>https://arxiv.org/abs/2510.17642</link>
<guid>https://arxiv.org/abs/2510.17642</guid>
<content:encoded><![CDATA[
arXiv:2510.17642v1 Announce Type: cross 
Abstract: Federated learning (FL) focuses on collaborative model training without the need to move the private data silos to a central server. Despite its several benefits, the classical FL is plagued with several limitations, such as high computational power required for model training(which is critical for low-resource clients), privacy risks, large update traffic, and non-IID heterogeneity. This chapter surveys a hybrid paradigm - Quantum Federated Learning (QFL), which introduces quantum computation, that addresses multiple challenges of classical FL and offers rapid computing capability while keeping the classical orchestration intact. Firstly, we motivate QFL with a concrete presentation on pain points of classical FL, followed by a discussion on a general architecture of QFL frameworks specifying the roles of client and server, communication primitives and the quantum model placement. We classify the existing QFL systems based on four criteria - quantum architecture (pure QFL, hybrid QFL), data processing method (quantum data encoding, quantum feature mapping, and quantum feature selection & dimensionality reduction), network topology (centralized, hierarchial, decentralized), and quantum security mechanisms (quantum key distribution, quantum homomorphic encryption, quantum differential privacy, blind quantum computing). We then describe applications of QFL in healthcare, vehicular networks, wireless networks, and network security, clearly highlighting where QFL improves communication efficiency, security, and performance compared to classical FL. We close with multiple challenges and future works in QFL, including extension of QFL beyond classification tasks, adversarial attacks, realistic hardware deployment, quantum communication protocols deployment, aggregation of different quantum models, and quantum split learning as an alternative to QFL.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proto: A Guided Journey through Modern OS Construction</title>
<link>https://arxiv.org/abs/2504.17984</link>
<guid>https://arxiv.org/abs/2504.17984</guid>
<content:encoded><![CDATA[
arXiv:2504.17984v2 Announce Type: replace 
Abstract: Proto is a new instructional OS that runs on commodity, portable hardware. It showcases modern features, including per-app address spaces, threading, commodity filesystems, USB, DMA, multicore support, self-hosted debugging, and a window manager. It supports rich applications such as 2D/3D games, music and video players, and a blockchain miner. Unlike traditional instructional systems, Proto emphasizes engaging, media-rich apps that go beyond basic terminal programs. Our method breaks down a full-featured OS into a set of incremental, self-contained prototypes. Each prototype introduces a minimal set of OS mechanisms, driven by the needs of specific apps. The construction process then progressively enables these apps by bringing up one mechanism at a time. Proto enables a wider audience to experience building a self-contained software system used in daily life
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smart Traffic Signals: Comparing MARL and Fixed-Time Strategies</title>
<link>https://arxiv.org/abs/2505.14544</link>
<guid>https://arxiv.org/abs/2505.14544</guid>
<content:encoded><![CDATA[
arXiv:2505.14544v3 Announce Type: replace 
Abstract: Urban traffic congestion, particularly at intersections, significantly impacts travel time, fuel consumption, and emissions. Traditional fixed-time signal control systems often lack the adaptability to manage dynamic traffic patterns effectively. This study explores the application of multi-agent reinforcement learning (MARL) to optimize traffic signal coordination across multiple intersections within a simulated environment. Utilizing Pygame, a simulation was developed to model a network of interconnected intersections with randomly generated vehicle flows to reflect realistic traffic variability. A decentralized MARL controller was implemented, in which each traffic signal operates as an autonomous agent, making decisions based on local observations and information from neighboring agents. Performance was evaluated against a baseline fixed-time controller using metrics such as average vehicle wait time and overall throughput. The MARL approach demonstrated statistically significant improvements, including reduced average waiting times and improved throughput. These findings suggest that MARL-based dynamic control strategies hold substantial promise for improving urban traffic management efficiency. More research is recommended to address scalability and real-world implementation challenges.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal online model aggregation</title>
<link>https://arxiv.org/abs/2403.15527</link>
<guid>https://arxiv.org/abs/2403.15527</guid>
<content:encoded><![CDATA[
arXiv:2403.15527v3 Announce Type: replace-cross 
Abstract: Conformal prediction equips machine learning models with a reasonable notion of uncertainty quantification without making strong distributional assumptions. It wraps around any prediction model and converts point predictions into set predictions with a predefined marginal coverage guarantee. However, conformal prediction only works if we fix the underlying machine learning model in advance. A relatively unaddressed issue in conformal prediction is that of model selection and/or aggregation: given a set of prediction models, which one should we conformalize? This paper suggests that instead of performing model selection, it can be prudent and practical to perform conformal set aggregation in an online, adaptive fashion. We propose a wrapper that takes in several conformal prediction sets (themselves wrapped around black-box prediction models), and outputs a single adaptively-combined prediction set. Our method, called conformal online model aggregation (COMA), is based on combining the prediction sets from several algorithms by weighted voting, and can be thought of as a sort of online stacking of the underlying conformal sets. As long as the input sets have (distribution-free) coverage guarantees, COMA retains coverage guarantees, under a negative correlation assumption between errors and weights. We verify that the assumption holds empirically in all settings considered. COMA is well-suited for decentralized or distributed settings, where different users may have different models, and are only willing to share their prediction sets for a new test point in a black-box fashion. As we demonstrate, it is also well-suited to settings with distribution drift and shift, where model selection can be imprudent.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Role of Federated Learning in Improving Financial Security: A Survey</title>
<link>https://arxiv.org/abs/2510.14991</link>
<guid>https://arxiv.org/abs/2510.14991</guid>
<content:encoded><![CDATA[
arXiv:2510.14991v1 Announce Type: new 
Abstract: With the growth of digital financial systems, robust security and privacy have become a concern for financial institutions. Even though traditional machine learning models have shown to be effective in fraud detections, they often compromise user data by requiring centralized access to sensitive information. In IoT-enabled financial endpoints such as ATMs and POS Systems that regularly produce sensitive data that is sent over the network. Federated Learning (FL) offers a privacy-preserving, decentralized model training across institutions without sharing raw data. FL enables cross-silo collaboration among banks while also using cross-device learning on IoT endpoints. This survey explores the role of FL in enhancing financial security and introduces a novel classification of its applications based on regulatory and compliance exposure levels ranging from low-exposure tasks such as collaborative portfolio optimization to high-exposure tasks like real-time fraud detection. Unlike prior surveys, this work reviews FL's practical use within financial systems, discussing its regulatory compliance and recent successes in fraud prevention and blockchain-integrated frameworks. However, FL deployment in finance is not without challenges. Data heterogeneity, adversarial attacks, and regulatory compliance make implementation far from easy. This survey reviews current defense mechanisms and discusses future directions, including blockchain integration, differential privacy, secure multi-party computation, and quantum-secure frameworks. Ultimately, this work aims to be a resource for researchers exploring FL's potential to advance secure, privacy-compliant financial systems.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Q-EnergyDEX: A Zero-Trust Distributed Energy Trading Framework Driven by Quantum Key Distribution and Blockchain</title>
<link>https://arxiv.org/abs/2510.15045</link>
<guid>https://arxiv.org/abs/2510.15045</guid>
<content:encoded><![CDATA[
arXiv:2510.15045v1 Announce Type: new 
Abstract: The rapid decentralization and digitalization of local electricity markets have introduced new cyber-physical vulnerabilities, including key leakage, data tampering, and identity spoofing. Existing blockchain-based solutions provide transparency and traceability but still depend on classical cryptographic primitives that are vulnerable to quantum attacks. To address these challenges, this paper proposes Q-EnergyDEX, a zero-trust distributed energy trading framework driven by quantum key distribution and blockchain. The framework integrates physical-layer quantum randomness with market-level operations, providing an end-to-end quantum-secured infrastructure. A cloud-based Quantum Key Management Service continuously generates verifiable entropy and regulates key generation through a rate-adaptive algorithm to sustain high-quality randomness. A symmetric authentication protocol (Q-SAH) establishes secure and low-latency sessions, while the quantum-aided consensus mechanism (PoR-Lite) achieves probabilistic ledger finality within a few seconds. Furthermore, a Stackelberg-constrained bilateral auction couples market clearing with entropy availability, ensuring both economic efficiency and cryptographic security. Simulation results show that Q-EnergyDEX maintains robust key stability and near-optimal social welfare, demonstrating its feasibility for large-scale decentralized energy markets.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combinatorial Contract Design: Recent Progress and Emerging Frontiers</title>
<link>https://arxiv.org/abs/2510.15065</link>
<guid>https://arxiv.org/abs/2510.15065</guid>
<content:encoded><![CDATA[
arXiv:2510.15065v1 Announce Type: new 
Abstract: Contract theory studies how a principal can incentivize agents to exert costly, unobservable effort through performance-based payments. While classical economic models provide elegant characterizations of optimal solutions, modern applications, ranging from online labor markets and healthcare to AI delegation and blockchain protocols, call for an algorithmic perspective. The challenge is no longer only which contracts induce desired behavior, but whether such contracts can be computed efficiently. This viewpoint has given rise to \emph{algorithmic contract design}, paralleling the rise of algorithmic mechanism design two decades ago.
  This article focuses on \emph{combinatorial contracts}, an emerging frontier within algorithmic contract design, where agents may choose among exponentially many combinations of actions, or where multiple agents must work together as a team, and the challenge lies in selecting the right composition. These models capture a wide variety of real-world contracting environments, from hospitals coordinating physicians across treatment protocols to firms hiring teams of engineers for interdependent tasks. We review three combinatorial settings: (i) a single agent choosing multiple actions, (ii) multiple agents with binary actions, and (iii) multiple agents each selecting multiple actions. For each, we highlight structural insights, algorithmic techniques, and complexity barriers. Results include tractable cases such as gross substitutes reward functions, hardness results, and approximation guarantees under value- and demand-oracle access. By charting these advances, the article maps the emerging landscape of combinatorial contract design, and highlights fundamental open questions and promising directions for future work.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NEMO: Faster Parallel Execution for Highly Contended Blockchain Workloads (Full version)</title>
<link>https://arxiv.org/abs/2510.15122</link>
<guid>https://arxiv.org/abs/2510.15122</guid>
<content:encoded><![CDATA[
arXiv:2510.15122v1 Announce Type: new 
Abstract: Following the design of more efficient blockchain consensus algorithms, the execution layer has emerged as the new performance bottleneck of blockchains, especially under high contention. Current parallel execution frameworks either rely on optimistic concurrency control (OCC) or on pessimistic concurrency control (PCC), both of which see their performance decrease when workloads are highly contended, albeit for different reasons. In this work, we present NEMO, a new blockchain execution engine that combines OCC with the object data model to address this challenge. NEMO introduces four core innovations: (i) a greedy commit rule for transactions using only owned objects; (ii) refined handling of dependencies to reduce re-executions; (iii) the use of incomplete but statically derivable read/write hints to guide execution; and (iv) a priority-based scheduler that favors transactions that unblock others. Through simulated execution experiments, we demonstrate that NEMO significantly reduces redundant computation and achieves higher throughput than representative approaches. For example, with 16 workers NEMO's throughput is up to 42% higher than the one of Block-STM, the state-of-the-art OCC approach, and 61% higher than the pessimistic concurrency control baseline used.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DFCA: Decentralized Federated Clustering Algorithm</title>
<link>https://arxiv.org/abs/2510.15300</link>
<guid>https://arxiv.org/abs/2510.15300</guid>
<content:encoded><![CDATA[
arXiv:2510.15300v1 Announce Type: new 
Abstract: Clustered Federated Learning has emerged as an effective approach for handling heterogeneous data across clients by partitioning them into clusters with similar or identical data distributions. However, most existing methods, including the Iterative Federated Clustering Algorithm (IFCA), rely on a central server to coordinate model updates, which creates a bottleneck and a single point of failure, limiting their applicability in more realistic decentralized learning settings. In this work, we introduce DFCA, a fully decentralized clustered FL algorithm that enables clients to collaboratively train cluster-specific models without central coordination. DFCA uses a sequential running average to aggregate models from neighbors as updates arrive, providing a communication-efficient alternative to batch aggregation while maintaining clustering performance. Our experiments on various datasets demonstrate that DFCA outperforms other decentralized algorithms and performs comparably to centralized IFCA, even under sparse connectivity, highlighting its robustness and practicality for dynamic real-world decentralized networks.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Parameter-Free Online Learning</title>
<link>https://arxiv.org/abs/2510.15644</link>
<guid>https://arxiv.org/abs/2510.15644</guid>
<content:encoded><![CDATA[
arXiv:2510.15644v1 Announce Type: new 
Abstract: We propose the first parameter-free decentralized online learning algorithms with network regret guarantees, which achieve sublinear regret without requiring hyperparameter tuning. This family of algorithms connects multi-agent coin-betting and decentralized online learning via gossip steps. To enable our decentralized analysis, we introduce a novel "betting function" formulation for coin-betting that simplifies the multi-agent regret analysis. Our analysis shows sublinear network regret bounds and is validated through experiments on synthetic and real datasets. This family of algorithms is applicable to distributed sensing, decentralized optimization, and collaborative ML applications.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grassroots Logic Programs: A Secure, Multiagent, Concurrent, Logic Programming Language</title>
<link>https://arxiv.org/abs/2510.15747</link>
<guid>https://arxiv.org/abs/2510.15747</guid>
<content:encoded><![CDATA[
arXiv:2510.15747v1 Announce Type: new 
Abstract: Grassroots platforms are distributed applications run by\linebreak cryptographically-identified people on their networked personal devices, where multiple disjoint platform instances emerge independently and coalesce when they interoperate. Their foundation is the grassroots social graph, upon which grassroots social networks, grassroots cryptocurrencies, and grassroots democratic federations can be built.
  Grassroots platforms have yet to be implemented, the key challenge being faulty and malicious participants: without secure programming support, correct participants cannot reliably identify each other, establish secure communication, or verify each other's code integrity.
  We present Grassroots Logic Programs (GLP), a secure, multiagent, concurrent, logic programming language for implementing grassroots platforms. GLP extends logic programs with paired single-reader/single-writer (SRSW) logic variables, providing secure communication channels among cryptographically-identified people through encrypted, signed and attested messages, which enable identity and code integrity verification. We present GLP progressively: logic programs, concurrent GLP, multiagent GLP, augmenting it with cryptographic security, and providing smartphone implementation-ready specifications. We prove safety properties including that GLP computations are deductions, SRSW preservation, acyclicity, and monotonicity. We prove multiagent GLP is grassroots and that GLP streams achieve blockchain security properties. We present a grassroots social graph protocol establishing authenticated peer-to-peer connections and demonstrate secure grassroots social networking applications.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Ego-graph inference for Networked Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.16606</link>
<guid>https://arxiv.org/abs/2509.16606</guid>
<content:encoded><![CDATA[
arXiv:2509.16606v2 Announce Type: replace 
Abstract: In networked multi-agent reinforcement learning (Networked-MARL), decentralized agents must act under local observability and constrained communication over fixed physical graphs. Existing methods often assume static neighborhoods, limiting adaptability to dynamic or heterogeneous environments. While centralized frameworks can learn dynamic graphs, their reliance on global state access and centralized infrastructure is impractical in real-world decentralized systems. We propose a stochastic graph-based policy for Networked-MARL, where each agent conditions its decision on a sampled subgraph over its local physical neighborhood. Building on this formulation, we introduce BayesG, a decentralized actor-framework that learns sparse, context-aware interaction structures via Bayesian variational inference. Each agent operates over an ego-graph and samples a latent communication mask to guide message passing and policy computation. The variational distribution is trained end-to-end alongside the policy using an evidence lower bound (ELBO) objective, enabling agents to jointly learn both interaction topology and decision-making strategies. BayesG outperforms strong MARL baselines on large-scale traffic control tasks with up to 167 agents, demonstrating superior scalability, efficiency, and performance.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized non-convex optimization via bi-level SQP and ADMM</title>
<link>https://arxiv.org/abs/2204.08786</link>
<guid>https://arxiv.org/abs/2204.08786</guid>
<content:encoded><![CDATA[
arXiv:2204.08786v4 Announce Type: replace-cross 
Abstract: Decentralized non-convex optimization is important in many problems of practical relevance. Existing decentralized methods, however, typically either lack convergence guarantees for general non-convex problems, or they suffer from a high subproblem complexity. We present a novel bi-level SQP method, where the inner quadratic problems are solved via ADMM. A decentralized stopping criterion from inexact Newton methods allows the early termination of ADMM as an inner algorithm to improve computational efficiency. The method has local convergence guarantees for non-convex problems. Moreover, it only solves sequences of Quadratic Programs, whereas many existing algorithms solve sequences of Nonlinear Programs. The method shows competitive numerical performance for an optimal power flow problem.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Real-Time Iterations for Distributed NMPC</title>
<link>https://arxiv.org/abs/2401.14898</link>
<guid>https://arxiv.org/abs/2401.14898</guid>
<content:encoded><![CDATA[
arXiv:2401.14898v3 Announce Type: replace-cross 
Abstract: This article presents a Real-Time Iteration (RTI) scheme for distributed Nonlinear Model Predictive Control (NMPC). The scheme transfers the well-known RTI approach, a key enabler for many industrial real-time NMPC implementations, to the setting of cooperative distributed control. At each sampling instant, one outer iteration of a bi-level decentralized Sequential Quadratic Programming (dSQP) method is applied to a centralized optimal control problem. This ensures that real-time requirements are met and it facilitates cooperation between subsystems. Combining novel dSQP convergence results with RTI stability guarantees, we prove local exponential stability under standard assumptions on the MPC design with and without terminal constraints. The proposed scheme only requires neighbor-to-neighbor communication and avoids a central coordinator. A numerical example with coupled inverted pendulums demonstrates the efficacy of the approach.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving and Incentive-Driven Relay-Based Framework for Cross-Domain Blockchain Interoperability</title>
<link>https://arxiv.org/abs/2510.14151</link>
<guid>https://arxiv.org/abs/2510.14151</guid>
<content:encoded><![CDATA[
arXiv:2510.14151v1 Announce Type: new 
Abstract: Interoperability is essential for transforming blockchains from isolated networks into collaborative ecosystems, unlocking their full potential. While significant progress has been made in public blockchain interoperability, bridging permissioned and permissionless blockchains poses unique challenges due to differences in access control, architectures, and security requirements. This paper introduces a blockchain-agnostic framework to enable interoperability between permissioned and permissionless networks. Leveraging cryptographic techniques, the framework ensures secure data exchanges. Its lightweight architectural design simplifies implementation and maintenance, while the integration of Clover and Dandelion++ protocols enhances transaction anonymity. Performance evaluations demonstrate the framework's effectiveness in achieving secure and efficient interoperability by measuring the forwarding time, the throughput, the availability, and their collusion impact of the system across heterogeneous blockchain ecosystems.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proof-Carrying Fair Ordering: Asymmetric Verification for BFT via Incremental Graphs</title>
<link>https://arxiv.org/abs/2510.14186</link>
<guid>https://arxiv.org/abs/2510.14186</guid>
<content:encoded><![CDATA[
arXiv:2510.14186v1 Announce Type: new 
Abstract: Byzantine Fault-Tolerant (BFT) consensus protocols ensure agreement on transaction ordering despite malicious actors, but unconstrained ordering power enables sophisticated value extraction attacks like front running and sandwich attacks - a critical threat to blockchain systems. Order-fair consensus curbs adversarial value extraction by constraining how leaders may order transactions. While state-of-the-art protocols such as Themis attain strong guarantees through graph-based ordering, they ask every replica to re-run the leader's expensive ordering computation for validation - an inherently symmetric and redundant paradigm. We present AUTIG, a high-performance, pluggable order-fairness service that breaks this symmetry. Our key insight is that verifying a fair order does not require re-computing it. Instead, verification can be reduced to a stateless audit of succinct, verifiable assertions about the ordering graph's properties. AUTIG realizes this via an asymmetric architecture: the leader maintains a persistent Unconfirmed-Transaction Incremental Graph (UTIG) to amortize graph construction across rounds and emits a structured proof of fairness with each proposal; followers validate the proof without maintaining historical state. AUTIG introduces three critical innovations: (i) incremental graph maintenance driven by threshold-crossing events and state changes; (ii) a decoupled pipeline that overlaps leader-side collection/update/extraction with follower-side stateless verification; and (iii) a proof design covering all internal pairs in the finalized prefix plus a frontier completeness check to rule out hidden external dependencies. We implement AUTIG and evaluate it against symmetric graph-based baselines under partial synchrony. Experiments show higher throughput and lower end-to-end latency while preserving gamma-batch-order-fairness.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentive-Based Federated Learning</title>
<link>https://arxiv.org/abs/2510.14208</link>
<guid>https://arxiv.org/abs/2510.14208</guid>
<content:encoded><![CDATA[
arXiv:2510.14208v1 Announce Type: new 
Abstract: Federated learning promises to revolutionize machine learning by enabling collaborative model training without compromising data privacy. However, practical adaptability can be limited by critical factors, such as the participation dilemma. Participating entities are often unwilling to contribute to a learning system unless they receive some benefits, or they may pretend to participate and free-ride on others. This chapter identifies the fundamental challenges in designing incentive mechanisms for federated learning systems. It examines how foundational concepts from economics and game theory can be applied to federated learning, alongside technology-driven solutions such as blockchain and deep reinforcement learning. This work presents a comprehensive taxonomy that thoroughly covers both centralized and decentralized architectures based on the aforementioned theoretical concepts. Furthermore, the concepts described are presented from an application perspective, covering emerging industrial applications, including healthcare, smart infrastructure, vehicular networks, and blockchain-based decentralized systems. Through this exploration, this chapter demonstrates that well-designed incentive mechanisms are not merely optional features but essential components for the practical success of federated learning. This analysis reveals both the promising solutions that have emerged and the significant challenges that remain in building truly sustainable, fair, and robust federated learning ecosystems.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Certifying optimal MEV strategies with Lean</title>
<link>https://arxiv.org/abs/2510.14480</link>
<guid>https://arxiv.org/abs/2510.14480</guid>
<content:encoded><![CDATA[
arXiv:2510.14480v1 Announce Type: new 
Abstract: Maximal Extractable Value (MEV) refers to a class of attacks to decentralized applications where the adversary profits by manipulating the ordering, inclusion, or exclusion of transactions in a blockchain. Decentralized Finance (DeFi) protocols are a primary target of these attacks, as their logic depends critically on transaction sequencing. To date, MEV attacks have already extracted billions of dollars in value, underscoring their systemic impact on blockchain security. Verifying the absence of MEV attacks requires determining suitable upper bounds, i.e. proving that no adversarial strategy can extract more value (if any) than expected by protocol designers. This problem is notoriously difficult: the space of adversarial strategies is extremely vast, making empirical studies and pen-and-paper reasoning insufficiently rigorous. In this paper, we present the first mechanized formalization of MEV in the Lean theorem prover. We introduce a methodology to construct machine-checked proofs of MEV bounds, providing correctness guarantees beyond what is possible with existing techniques. To demonstrate the generality of our approach, we model and analyse the MEV of two paradigmatic DeFi protocols. Notably, we develop the first machine-checked proof of the optimality of sandwich attacks in Automated Market Makers, a fundamental DeFi primitive.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Helmsman: Autonomous Synthesis of Federated Learning Systems via Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2510.14512</link>
<guid>https://arxiv.org/abs/2510.14512</guid>
<content:encoded><![CDATA[
arXiv:2510.14512v1 Announce Type: new 
Abstract: Federated Learning (FL) offers a powerful paradigm for training models on decentralized data, but its promise is often undermined by the immense complexity of designing and deploying robust systems. The need to select, combine, and tune strategies for multifaceted challenges like data heterogeneity and system constraints has become a critical bottleneck, resulting in brittle, bespoke solutions. To address this, we introduce Helmsman, a novel multi-agent system that automates the end-to-end synthesis of federated learning systems from high-level user specifications. It emulates a principled research and development workflow through three collaborative phases: (1) interactive human-in-the-loop planning to formulate a sound research plan, (2) modular code generation by supervised agent teams, and (3) a closed-loop of autonomous evaluation and refinement in a sandboxed simulation environment. To facilitate rigorous evaluation, we also introduce AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess the system-level generation capabilities of agentic systems in FL. Extensive experiments demonstrate that our approach generates solutions competitive with, and often superior to, established hand-crafted baselines. Our work represents a significant step towards the automated engineering of complex decentralized AI systems.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JASDA: Introducing Job-Aware Scheduling in Scheduler-Driven Job Atomization</title>
<link>https://arxiv.org/abs/2510.14599</link>
<guid>https://arxiv.org/abs/2510.14599</guid>
<content:encoded><![CDATA[
arXiv:2510.14599v1 Announce Type: new 
Abstract: The increasing complexity and temporal variability of workloads on MIG-enabled GPUs challenge the scalability of traditional centralized scheduling. Building upon the SJA concept, this paper introduces JASDA-a novel paradigm that extends SJA from a largely centralized scheduling model toward a fully decentralized negotiation process. In JASDA, jobs actively generate and score feasible subjobs in response to scheduler-announced execution windows, while the scheduler performs policy-driven clearing that balances utilization, fairness, and temporal responsiveness. This bidirectional, iterative interaction embeds feedback, calibration, and probabilistic safety directly into the scheduling loop, enabling adaptive and transparent decision-making. By coupling principles from auction theory and online optimization with the temporal granularity of GPU workloads, JASDA provides a scalable foundation for market-aware and fairness-driven resource management-bridging theoretical scheduling models with practical deployment in modern MIG-enabled environments relevant to Artificial Intelligence and Agriculture 4.0.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Bidding Games: Reinforcement Learning for MEV Extraction on Polygon Blockchain</title>
<link>https://arxiv.org/abs/2510.14642</link>
<guid>https://arxiv.org/abs/2510.14642</guid>
<content:encoded><![CDATA[
arXiv:2510.14642v1 Announce Type: new 
Abstract: In blockchain networks, the strategic ordering of transactions within blocks has emerged as a significant source of profit extraction, known as Maximal Extractable Value (MEV). The transition from spam-based Priority Gas Auctions to structured auction mechanisms like Polygon Atlas has transformed MEV extraction from public bidding wars into sealed-bid competitions under extreme time constraints. While this shift reduces network congestion, it introduces complex strategic challenges where searchers must make optimal bidding decisions within a sub-second window without knowledge of competitor behavior or presence. Traditional game-theoretic approaches struggle in this high-frequency, partially observable environment due to their reliance on complete information and static equilibrium assumptions. We present a reinforcement learning framework for MEV extraction on Polygon Atlas and make three contributions: (1) A novel simulation environment that accurately models the stochastic arrival of arbitrage opportunities and probabilistic competition in Atlas auctions; (2) A PPO-based bidding agent optimized for real-time constraints, capable of adaptive strategy formulation in continuous action spaces while maintaining production-ready inference speeds; (3) Empirical validation demonstrating our history-conditioned agent captures 49\% of available profits when deployed alongside existing searchers and 81\% when replacing the market leader, significantly outperforming static bidding strategies. Our work establishes that reinforcement learning provides a critical advantage in high-frequency MEV environments where traditional optimization methods fail, offering immediate value for industrial participants and protocol designers alike.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FibRace: a large-scale benchmark of client-side proving on mobile devices</title>
<link>https://arxiv.org/abs/2510.14693</link>
<guid>https://arxiv.org/abs/2510.14693</guid>
<content:encoded><![CDATA[
arXiv:2510.14693v1 Announce Type: new 
Abstract: FibRace, jointly developed by KKRT Labs and Hyli, was the first large-scale experiment to test client-side proof generation on smartphones using Cairo M. Presented as a mobile game in which players proved Fibonacci numbers and climbed a leaderboard, FibRace served a dual purpose: to engage the public and to provide empirical benchmarking. Over a three-week campaign (September 11-30, 2025), 6,047 players across 99 countries generated 2,195,488 proofs on 1,420 unique device models. The results show that most modern smartphones can complete a proof in under 5 seconds, confirming that *mobile devices are now capable of producing zero-knowledge proofs reliably*, without the need for remote provers or specialized hardware. Performance was correlated primarily with RAM capacity and SoC (System on Chip) performance: devices with at least 3 GB of RAM proved stably, when Apple's A19 Pro and M-series chips achieved the fastest proving times. Hyli's blockchain natively verified every proof onchain without congestion. FibRace provides the most comprehensive dataset to date on mobile proving performance, establishing a practical baseline for future research in lightweight provers, proof-powered infrastructure, and privacy-preserving mobile applications.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedPPA: Progressive Parameter Alignment for Personalized Federated Learning</title>
<link>https://arxiv.org/abs/2510.14698</link>
<guid>https://arxiv.org/abs/2510.14698</guid>
<content:encoded><![CDATA[
arXiv:2510.14698v1 Announce Type: new 
Abstract: Federated Learning (FL) is designed as a decentralized, privacy-preserving machine learning paradigm that enables multiple clients to collaboratively train a model without sharing their data. In real-world scenarios, however, clients often have heterogeneous computational resources and hold non-independent and identically distributed data (non-IID), which poses significant challenges during training. Personalized Federated Learning (PFL) has emerged to address these issues by customizing models for each client based on their unique data distribution. Despite its potential, existing PFL approaches typically overlook the coexistence of model and data heterogeneity arising from clients with diverse computational capabilities. To overcome this limitation, we propose a novel method, called Progressive Parameter Alignment (FedPPA), which progressively aligns the weights of common layers across clients with the global model's weights. Our approach not only mitigates inconsistencies between global and local models during client updates, but also preserves client's local knowledge, thereby enhancing personalization robustness in non-IID settings. To further enhance the global model performance while retaining strong personalization, we also integrate entropy-based weighted averaging into the FedPPA framework. Experiments on three image classification datasets, including MNIST, FMNIST, and CIFAR-10, demonstrate that FedPPA consistently outperforms existing FL algorithms, achieving superior performance in personalized adaptation.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Augmenting Smart Contract Decompiler Output through Fine-grained Dependency Analysis and LLM-facilitated Semantic Recovery</title>
<link>https://arxiv.org/abs/2501.08670</link>
<guid>https://arxiv.org/abs/2501.08670</guid>
<content:encoded><![CDATA[
arXiv:2501.08670v2 Announce Type: replace 
Abstract: Decompiler is a specialized type of reverse engineering tool extensively employed in program analysis tasks, particularly in program comprehension and vulnerability detection. However, current Solidity smart contract decompilers face significant limitations in reconstructing the original source code. In particular, the bottleneck of SOTA decompilers lies in inaccurate method identification, incorrect variable type recovery, and missing contract attributes. These deficiencies hinder downstream tasks and understanding of the program logic. To address these challenges, we propose SmartHalo, a new framework that enhances decompiler output by combining static analysis (SA) and large language models (LLM). SmartHalo leverages the complementary strengths of SA's accuracy in control and data flow analysis and LLM's capability in semantic prediction. More specifically, \system{} constructs a new data structure - Dependency Graph (DG), to extract semantic dependencies via static analysis. Then, it takes DG to create prompts for LLM optimization. Finally, the correctness of LLM outputs is validated through symbolic execution and formal verification. Evaluation on a dataset consisting of 465 randomly selected smart contract methods shows that SmartHalo significantly improves the quality of the decompiled code, compared to SOTA decompilers (e.g., Gigahorse). Notably, integrating GPT-4o with SmartHalo further enhances its performance, achieving precision rates of 87.39% for method boundaries, 90.39% for variable types, and 80.65% for contract attributes.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Far Have LLMs Come Toward Automated SATD Taxonomy Construction?</title>
<link>https://arxiv.org/abs/2506.09601</link>
<guid>https://arxiv.org/abs/2506.09601</guid>
<content:encoded><![CDATA[
arXiv:2506.09601v2 Announce Type: replace 
Abstract: Technical debt refers to suboptimal code that degrades software quality. When developers intentionally introduce such debt, it is called self-admitted technical debt (SATD). Since SATD hinders maintenance, identifying its categories is key to uncovering quality issues. Traditionally, constructing such taxonomies requires manually inspecting SATD comments and surrounding code, which is time-consuming, labor-intensive, and often inconsistent due to annotator subjectivity. In this study, we investigate to what extent large language models (LLMs) can generate SATD taxonomies. We designed a structured, LLM-driven pipeline that mirrors the taxonomy construction steps researchers typically follow. We evaluated it on SATD datasets from three domains: quantum software, smart contracts, and machine learning. It successfully recovered domain-specific categories reported in prior work, such as Layer Configuration in machine learning. It also completed taxonomy generation in under two hours and for less than $1, even on the largest dataset. These results suggest that, while full automation remains challenging, LLMs can support semi-automated SATD taxonomy construction. Furthermore, our work opens up avenues for future work, such as automated taxonomy generation in other areas.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Safety, Alignment, and Ethics (AI SAE)</title>
<link>https://arxiv.org/abs/2509.24065</link>
<guid>https://arxiv.org/abs/2509.24065</guid>
<content:encoded><![CDATA[
arXiv:2509.24065v2 Announce Type: replace 
Abstract: This paper grounds ethics in evolutionary biology, viewing moral norms as adaptive mechanisms that render cooperation fitness-viable under selection pressure. Current alignment approaches add ethics post hoc, treating it as an external constraint rather than embedding it as an evolutionary strategy for cooperation. The central question is whether normative architectures can be embedded directly into AI systems to sustain human--AI cooperation (symbiosis) as capabilities scale. To address this, I propose a governance--embedding--representation pipeline linking moral representation learning to system-level design and institutional governance, treating alignment as a multi-level problem spanning cognition, optimization, and oversight. I formalize moral norm representation through the moral problem space, a learnable subspace in neural representations where cooperative norms can be encoded and causally manipulated. Using sparse autoencoders, activation steering, and causal interventions, I outline a research program for engineering moral representations and embedding them into the full semantic space -- treating competing theories of morality as empirical hypotheses about representation geometry rather than philosophical positions. Governance principles leverage these learned moral representations to regulate how cooperative behaviors evolve within the AI ecosystem. Through replicator dynamics and multi-agent game theory, I model how internal representational features can shape population-level incentives by motivating the design of sanctions and subsidies structured to yield decentralized normative institutions.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARTeX: Anonymity Real-world-assets Token eXchange</title>
<link>https://arxiv.org/abs/2510.12821</link>
<guid>https://arxiv.org/abs/2510.12821</guid>
<content:encoded><![CDATA[
arXiv:2510.12821v1 Announce Type: new 
Abstract: This paper addresses one of the most noteworthy issues in the recent virtual asset market, the privacy concerns related to token transactions of Real-World Assets tokens, known as RWA tokens. Following the advent of Bitcoin, the virtual asset market has experienced explosive growth, spawning movements to link real-world assets with virtual assets. However, due to the transparency principle of blockchain technology, the anonymity of traders cannot be guaranteed. In the existing blockchain environment, there have been instances of protecting the privacy of fungible tokens (FTs) using mixer services. Moreover, numerous studies have been conducted to secure the privacy of non-fungible tokens (NFTs). However, due to the unique characteristics of RWA tokens and the limitations of each study, it has been challenging to achieve the goal of anonymity protection effectively. This paper proposes a new token trading platform, the ARTeX, designed to resolve these issues. This platform not only addresses the shortcomings of existing methods but also ensures the anonymity of traders while enhancing safeguards against illegal activities.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching for Heterogeneous Tasks and Clusters</title>
<link>https://arxiv.org/abs/2510.12889</link>
<guid>https://arxiv.org/abs/2510.12889</guid>
<content:encoded><![CDATA[
arXiv:2510.12889v1 Announce Type: new 
Abstract: This paper introduces Dodoor, an efficient randomized decentralized scheduler designed for task scheduling in modern data centers. Dodoor leverages advanced research on the weighted balls-into-bins model with b-batched setting. Unlike other decentralized schedulers that rely on real-time probing of remote servers, Dodoor makes scheduling decisions based on cached server information, which is updated in batches, to reduce communication overheads. To schedule tasks with dynamic, multidimensional resource requirements in heterogeneous cluster, Dodoor uses a novel load score to measure servers' loads for each scheduled task. This score captures the anti-affinity between servers and tasks in contrast to the commonly used heuristic of counting pending tasks to balance load. On a 101-node heterogeneous cluster, Dodoor is evaluated using two workloads: (i) simulated Azure virtual machines placements and (ii) real serverless Python functions executions in Docker. The evaluation shows that Dodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can also increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency by 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two workloads.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Altruistic Ride Sharing: A Community-Driven Approach to Short-Distance Mobility</title>
<link>https://arxiv.org/abs/2510.13227</link>
<guid>https://arxiv.org/abs/2510.13227</guid>
<content:encoded><![CDATA[
arXiv:2510.13227v1 Announce Type: new 
Abstract: Urban mobility faces persistent challenges of congestion and fuel consumption, specifically when people choose a private, point-to-point commute option. Profit-driven ride-sharing platforms prioritize revenue over fairness and sustainability. This paper introduces Altruistic Ride-Sharing (ARS), a decentralized, peer-to-peer mobility framework where participants alternate between driver and rider roles based on altruism points rather than monetary incentives. The system integrates multi-agent reinforcement learning (MADDPG) for dynamic ride-matching, game-theoretic equilibrium guarantees for fairness, and a population model to sustain long-term balance. Using real-world New York City taxi data, we demonstrate that ARS reduces travel distance and emissions, increases vehicle utilization, and promotes equitable participation compared to both no-sharing and optimization-based baselines. These results establish ARS as a scalable, community-driven alternative to conventional ride-sharing, aligning individual behavior with collective urban sustainability goals.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BlendFL: Blended Federated Learning for Handling Multimodal Data Heterogeneity</title>
<link>https://arxiv.org/abs/2510.13266</link>
<guid>https://arxiv.org/abs/2510.13266</guid>
<content:encoded><![CDATA[
arXiv:2510.13266v1 Announce Type: new 
Abstract: One of the key challenges of collaborative machine learning, without data sharing, is multimodal data heterogeneity in real-world settings. While Federated Learning (FL) enables model training across multiple clients, existing frameworks, such as horizontal and vertical FL, are only effective in `ideal' settings that meet specific assumptions. Hence, they struggle to address scenarios where neither all modalities nor all samples are represented across the participating clients. To address this gap, we propose BlendFL, a novel FL framework that seamlessly blends the principles of horizontal and vertical FL in a synchronized and non-restrictive fashion despite the asymmetry across clients. Specifically, any client within BlendFL can benefit from either of the approaches, or both simultaneously, according to its available dataset. In addition, BlendFL features a decentralized inference mechanism, empowering clients to run collaboratively trained local models using available local data, thereby reducing latency and reliance on central servers for inference. We also introduce BlendAvg, an adaptive global model aggregation strategy that prioritizes collaborative model updates based on each client's performance. We trained and evaluated BlendFL and other state-of-the-art baselines on three classification tasks using a large-scale real-world multimodal medical dataset and a popular multimodal benchmark. Our results highlight BlendFL's superior performance for both multimodal and unimodal classification. Ablation studies demonstrate BlendFL's faster convergence compared to traditional approaches, accelerating collaborative learning. Overall, in our study we highlight the potential of BlendFL for handling multimodal data heterogeneity for collaborative learning in real-world settings where data privacy is crucial, such as in healthcare and finance.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Authenticated and Interoperable Multimedia Healthcare Data over Hybrid-Storage Blockchains</title>
<link>https://arxiv.org/abs/2510.13318</link>
<guid>https://arxiv.org/abs/2510.13318</guid>
<content:encoded><![CDATA[
arXiv:2510.13318v1 Announce Type: new 
Abstract: The integration of blockchain technology into healthcare presents a paradigm shift for secure data management, enabling decentralized and tamper-proof storage and sharing of sensitive Electronic Health Records (EHRs). However, existing blockchain-based healthcare systems, while providing robust access control, commonly overlook the high latency in user-side re-computation of hashes for integrity verification of large multimedia data, impairing their practicality, especially in time-sensitive clinical scenarios. In this paper, we propose FAITH, an innovative scheme for \underline{F}ast \underline{A}uthenticated and \underline{I}nteroperable mul\underline{T}imedia \underline{H}ealthcare data storage and sharing over hybrid-storage blockchains. Rather than user-side hash re-computations, FAITH lets an off-chain storage provider generate verifiable proofs using recursive Zero-Knowledge Proofs (ZKPs), while the user only needs to perform lightweight verification. For flexible access authorization, we leverage Proxy Re-Encryption (PRE) and enable the provider to conduct ciphertext re-encryption, in which the re-encryption correctness can be verified via ZKPs against the malicious provider. All metadata and proofs are recorded on-chain for public verification. We provide a comprehensive analysis of FAITH's security regarding data privacy and integrity. We implemented a prototype of FAITH, and extensive experiments demonstrated its practicality for time-critical healthcare applications, dramatically reducing user-side verification latency by up to $98\%$, bringing it from $4$ s down to around $70$ ms for a $5$ GB encrypted file.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multifractality and its sources in the digital currency market</title>
<link>https://arxiv.org/abs/2510.13785</link>
<guid>https://arxiv.org/abs/2510.13785</guid>
<content:encoded><![CDATA[
arXiv:2510.13785v1 Announce Type: cross 
Abstract: Multifractality in time series analysis characterizes the presence of multiple scaling exponents, indicating heterogeneous temporal structures and complex dynamical behaviors beyond simple monofractal models. In the context of digital currency markets, multifractal properties arise due to the interplay of long-range temporal correlations and heavy-tailed distributions of returns, reflecting intricate market microstructure and trader interactions. Incorporating multifractal analysis into the modeling of cryptocurrency price dynamics enhances the understanding of market inefficiencies, may improve volatility forecasting and facilitate the detection of critical transitions or regime shifts. Based on the multifractal cross-correlation analysis (MFCCA) whose spacial case is the multifractal detrended fluctuation analysis (MFDFA), as the most commonly used practical tools for quantifying multifractality, in the present contribution a recently proposed method of disentangling sources of multifractality in time series was applied to the most representative instruments from the digital market. They include Bitcoin (BTC), Ethereum (ETH), decentralized exchanges (DEX) and non-fungible tokens (NFT). The results indicate the significant role of heavy tails in generating a broad multifractal spectrum. However, they also clearly demonstrate that the primary source of multifractality are temporal correlations in the series, and without them, multifractality fades out. It appears characteristic that these temporal correlations, to a large extent, do not depend on the thickness of the tails of the fluctuation distribution. These observations, made here in the context of the digital currency market, provide a further strong argument for the validity of the proposed methodology of disentangling sources of multifractality in time series.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Ensemble-Based Semi-Supervised Learning for Illicit Account Detection in Ethereum DeFi Transactions</title>
<link>https://arxiv.org/abs/2412.02408</link>
<guid>https://arxiv.org/abs/2412.02408</guid>
<content:encoded><![CDATA[
arXiv:2412.02408v2 Announce Type: replace 
Abstract: The advent of smart contracts has enabled the rapid rise of Decentralized Finance (DeFi) on the Ethereum blockchain, offering substantial rewards in financial innovation and inclusivity. This growth, however, is accompanied by significant security risks such as illicit accounts engaged in fraud. Effective detection is further limited by the scarcity of labeled data and the evolving tactics of malicious accounts. To address these challenges with a robust solution for safeguarding the DeFi ecosystem, we propose $\textbf{SLEID}$, a $\textbf{S}$elf-$\textbf{L}$earning $\textbf{E}$nsemble-based $\textbf{I}$llicit account $\textbf{D}$etection framework. SLEID uses an Isolation Forest model for initial outlier detection and a self-training mechanism to iteratively generate pseudo-labels for unlabeled accounts, enhancing detection accuracy. Experiments on 6,903,860 Ethereum transactions with extensive DeFi interaction coverage demonstrate that SLEID significantly outperforms supervised and semi-supervised baselines with $\textbf{+2.56}$ percentage-point precision, comparable recall, and $\textbf{+0.90}$ percentage-point F1 -- particularly for the minority illicit class -- alongside $\textbf{+3.74}$ percentage-points higher accuracy and improvements in PR-AUC, while substantially reducing reliance on labeled data.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking LLMs' Swarm intelligence</title>
<link>https://arxiv.org/abs/2505.04364</link>
<guid>https://arxiv.org/abs/2505.04364</guid>
<content:encoded><![CDATA[
arXiv:2505.04364v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) show potential for complex reasoning, yet their capacity for emergent coordination in Multi-Agent Systems (MAS) when operating under strict swarm-like constraints-limited local perception and communication-remains largely unexplored. Existing benchmarks often do not fully capture the unique challenges of decentralized coordination when agents operate with incomplete spatio-temporal information. To bridge this gap, we introduce SwarmBench, a novel benchmark designed to systematically evaluate the swarm intelligence capabilities of LLMs acting as decentralized agents. SwarmBench features five foundational MAS coordination tasks (Pursuit, Synchronization, Foraging, Flocking, Transport) within a configurable 2D grid environment, forcing agents to rely solely on local sensory input ($k\times k$ view) and local communication. We propose metrics for coordination effectiveness and analyze emergent group dynamics. Zero-shot evaluations of leading LLMs (e.g., deepseek-v3, o4-mini) reveal significant task-dependent performance variations. While some rudimentary coordination is observed, our results indicate that current LLMs significantly struggle with robust long-range planning and adaptive strategy formation under the uncertainty inherent in these decentralized scenarios. Assessing LLMs under such swarm-like constraints is crucial for understanding their utility in future decentralized intelligent systems. We release SwarmBench as an open, extensible toolkit-built on a customizable physical system-providing environments, prompts, evaluation scripts, and comprehensive datasets. This aims to foster reproducible research into LLM-based MAS coordination and the theoretical underpinnings of emergent collective behavior under severe informational decentralization. Our code repository is available at https://github.com/x66ccff/swarmbench.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralizing Multi-Agent Reinforcement Learning with Temporal Causal Information</title>
<link>https://arxiv.org/abs/2506.07829</link>
<guid>https://arxiv.org/abs/2506.07829</guid>
<content:encoded><![CDATA[
arXiv:2506.07829v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) algorithms can find an optimal policy for a single agent to accomplish a particular task. However, many real-world problems require multiple agents to collaborate in order to achieve a common goal. For example, a robot executing a task in a warehouse may require the assistance of a drone to retrieve items from high shelves. In Decentralized Multi-Agent RL (DMARL), agents learn independently and then combine their policies at execution time, but often must satisfy constraints on compatibility of local policies to ensure that they can achieve the global task when combined. In this paper, we study how providing high-level symbolic knowledge to agents can help address unique challenges of this setting, such as privacy constraints, communication limitations, and performance concerns. In particular, we extend the formal tools used to check the compatibility of local policies with the team task, making decentralized training with theoretical guarantees usable in more scenarios. Furthermore, we empirically demonstrate that symbolic knowledge about the temporal evolution of events in the environment can significantly expedite the learning process in DMARL.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rationally Analyzing Shelby: Proving Incentive Compatibility in a Decentralized Storage Network</title>
<link>https://arxiv.org/abs/2510.11866</link>
<guid>https://arxiv.org/abs/2510.11866</guid>
<content:encoded><![CDATA[
arXiv:2510.11866v1 Announce Type: new 
Abstract: Decentralized storage is one of the most natural applications built on blockchains and a central component of the Web3 ecosystem. Yet despite a decade of active development -- from IPFS and Filecoin to more recent entrants -- most of these storage protocols have received limited formal analysis of their incentive properties. Claims of incentive compatibility are sometimes made, but rarely proven. This gap matters: without well-designed incentives, a system may distribute storage but fail to truly decentralize it.
  We analyze Shelby -- a storage network protocol recently proposed by Aptos Labs and Jump Crypto -- and provide the first formal proof of its incentive properties. Our game-theoretic model shows that while off-chain audits alone collapse to universal shirking, Shelby's combination of peer audits with occasional on-chain verification yields incentive compatibility under natural parameter settings. We also examine coalition behavior and outline a simple modification that strengthens the protocol's collusion-resilience.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stable and Fault-Tolerant Decentralized Traffic Engineering</title>
<link>https://arxiv.org/abs/2510.11937</link>
<guid>https://arxiv.org/abs/2510.11937</guid>
<content:encoded><![CDATA[
arXiv:2510.11937v1 Announce Type: new 
Abstract: Cloud providers have recently decentralized their wide-area network traffic engineering (TE) systems to contain the impact of TE controller failures. In the decentralized design, a controller fault only impacts its slice of the network, limiting the blast radius to a fraction of the network. However, we find that autonomous slice controllers can arrive at divergent traffic allocations that overload links by 30% beyond their capacity. We present Symphony, a decentralized TE system that addresses the challenge of divergence-induced congestion while preserving the fault-isolation benefits of decentralization. By augmenting TE objectives with quadratic regularization, Symphony makes traffic allocations robust to demand perturbations, ensuring TE controllers naturally converge to compatible allocations without coordination. In parallel, Symphony's randomized slicing algorithm partitions the network to minimize blast radius by distributing critical traffic sources across slices, preventing any single failure from becoming catastrophic. These innovations work in tandem: regularization ensures algorithmic stability to traffic allocations while intelligent slicing provides architectural resilience in the network. Through extensive evaluation on cloud provider WANs, we show Symphony reduces divergence-induced congestion by 14x and blast radius by 79% compared to current practice.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adding All Flavors: A Hybrid Random Number Generator for dApps and Web3</title>
<link>https://arxiv.org/abs/2510.12062</link>
<guid>https://arxiv.org/abs/2510.12062</guid>
<content:encoded><![CDATA[
arXiv:2510.12062v1 Announce Type: new 
Abstract: Random numbers play a vital role in many decentralized applications (dApps), such as gaming and decentralized finance (DeFi) applications.
  Existing random number provision mechanisms can be roughly divided into two categories, on-chain, and off-chain.
  On-chain approaches usually rely on the blockchain as the major input and all computations are done by blockchain nodes.
  The major risk for this type of method is that the input itself is susceptible to the adversary's influence.
  Off-chain approaches, as the name suggested, complete the generation without the involvement of blockchain nodes and share the result directly with a dApp.
  These mechanisms usually have a strong security assumption and high complexity.
  To mitigate these limitations and provide a framework that allows a dApp to balance different factors involved in random number generation, we propose a hybrid random number generation solution that leverages IoT devices equipped with trusted execution environment (TEE) as the randomness sources, and then utilizes a set of cryptographic tools to aggregate the multiple sources and obtain the final random number that can be consumed by the dApp.
  The new approach only needs one honest random source to guarantee the unbiasedness of the final random number and a user can configure the system to tolerate malicious participants who can refuse to respond to avoid unfavored results.
  We also provide a concrete construction that can further reduce the on-chain computation complexity to lower the cost of the solution in practice.
  We evaluate the computation and gas costs to demonstrate the effectiveness of the improvement.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VeilAudit: Breaking the Deadlock Between Privacy and Accountability Across Blockchains</title>
<link>https://arxiv.org/abs/2510.12153</link>
<guid>https://arxiv.org/abs/2510.12153</guid>
<content:encoded><![CDATA[
arXiv:2510.12153v1 Announce Type: new 
Abstract: Cross chain interoperability in blockchain systems exposes a fundamental tension between user privacy and regulatory accountability. Existing solutions enforce an all or nothing choice between full anonymity and mandatory identity disclosure, which limits adoption in regulated financial settings. We present VeilAudit, a cross chain auditing framework that introduces Auditor Only Linkability, which allows auditors to link transaction behaviors that originate from the same anonymous entity without learning its identity. VeilAudit achieves this with a user generated Linkable Audit Tag that embeds a zero knowledge proof to attest to its validity without exposing the user master wallet address, and with a special ciphertext that only designated auditors can test for linkage. To balance privacy and compliance, VeilAudit also supports threshold gated identity revelation under due process. VeilAudit further provides a mechanism for building reputation in pseudonymous environments, which enables applications such as cross chain credit scoring based on verifiable behavioral history. We formalize the security guarantees and develop a prototype that spans multiple EVM chains. Our evaluation shows that the framework is practical for today multichain environments.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedMMKT:Co-Enhancing a Server Text-to-Image Model and Client Task Models in Multi-Modal Federated Learning</title>
<link>https://arxiv.org/abs/2510.12254</link>
<guid>https://arxiv.org/abs/2510.12254</guid>
<content:encoded><![CDATA[
arXiv:2510.12254v1 Announce Type: new 
Abstract: Text-to-Image (T2I) models have demonstrated their versatility in a wide range of applications. However, adaptation of T2I models to specialized tasks is often limited by the availability of task-specific data due to privacy concerns. On the other hand, harnessing the power of rich multimodal data from modern mobile systems and IoT infrastructures presents a great opportunity. This paper introduces Federated Multi-modal Knowledge Transfer (FedMMKT), a novel framework that enables co-enhancement of a server T2I model and client task-specific models using decentralized multimodal data without compromising data privacy.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Runtime Composition in Dynamic System of Systems: A Systematic Review of Challenges, Solutions, Tools, and Evaluation Methods</title>
<link>https://arxiv.org/abs/2510.12616</link>
<guid>https://arxiv.org/abs/2510.12616</guid>
<content:encoded><![CDATA[
arXiv:2510.12616v1 Announce Type: new 
Abstract: Context: Modern Systems of Systems (SoSs) increasingly operate in dynamic environments (e.g., smart cities, autonomous vehicles) where runtime composition -- the on-the-fly discovery, integration, and coordination of constituent systems (CSs)--is crucial for adaptability. Despite growing interest, the literature lacks a cohesive synthesis of runtime composition in dynamic SoSs. Objective: This study synthesizes research on runtime composition in dynamic SoSs and identifies core challenges, solution strategies, supporting tools, and evaluation methods. Methods: We conducted a Systematic Literature Review (SLR), screening 1,774 studies published between 2019 and 2024 and selecting 80 primary studies for thematic analysis (TA). Results: Challenges fall into four categories: modeling and analysis, resilient operations, system orchestration, and heterogeneity of CSs. Solutions span seven areas: co-simulation and digital twins, semantic ontologies, integration frameworks, adaptive architectures, middleware, formal methods, and AI-driven resilience. Service-oriented frameworks for composition and integration dominate tooling, while simulation platforms support evaluation. Interoperability across tools, limited cross-toolchain workflows, and the absence of standardized benchmarks remain key gaps. Evaluation approaches include simulation-based, implementation-driven, and human-centered studies, which have been applied in domains such as smart cities, healthcare, defense, and industrial automation. Conclusions: The synthesis reveals tensions, including autonomy versus coordination, the modeling-reality gap, and socio-technical integration. It calls for standardized evaluation metrics, scalable decentralized architectures, and cross-domain frameworks. The analysis aims to guide researchers and practitioners in developing and implementing dynamically composable SoSs.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chronicles of Jockeying in Queuing Systems</title>
<link>https://arxiv.org/abs/2402.11061</link>
<guid>https://arxiv.org/abs/2402.11061</guid>
<content:encoded><![CDATA[
arXiv:2402.11061v5 Announce Type: replace 
Abstract: Emerging trends in communication systems, such as network softwarization, functional disaggregation, and multi-access edge computing (MEC), are reshaping both the infrastructural landscape and the application ecosystem. These transformations introduce new challenges for packet transmission, task offloading, and resource allocation under stringent service-level requirements. A key factor in this context is queue impatience, where waiting entities alter their behavior in response to delay. While balking and reneging have been widely studied, this survey focuses on the less explored but operationally significant phenomenon of jockeying, i.e. the switching of jobs or users between queues. Although a substantial body of literature models jockeying behavior, the diversity of approaches raises questions about their practical applicability in dynamic, distributed environments such as 5G and Beyond. This chronicle reviews and classifies these studies with respect to their methodologies, modeling assumptions, and use cases, with particular emphasis on communication systems and MEC scenarios. We argue that forthcoming architectural transformations in next-generation networks will render many existing jockeying models inapplicable. By highlighting emerging paradigms such as MEC, network slicing, and network function virtualization, we identify open challenges, including state dissemination, migration cost, and stability, that undermine classical assumptions. We further outline design principles and research directions, emphasizing hybrid architectures and decentralized decision making as foundations for re-conceptualizing impatience in next-generation communication systems.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChipmunkRing: A Practical Post-Quantum Ring Signature Scheme for Blockchain Applications</title>
<link>https://arxiv.org/abs/2510.09617</link>
<guid>https://arxiv.org/abs/2510.09617</guid>
<content:encoded><![CDATA[
arXiv:2510.09617v1 Announce Type: new 
Abstract: ChipmunkRing, a practical post-quantum ring signature construction tailored for blockchain environments. Building on our Chipmunk lattice-based cryptographic framework, this implementation delivers compact digital signatures ranging from 20.5 to 279.7KB, with rapid signing operations completing in 1.1-15.1ms and efficient validation processes requiring only 0.4-4.5ms for participant groups of 2-64 members. The cornerstone of our approach is Acorn Verification-a streamlined zero-knowledge protocol that supersedes the classical Fiat-Shamir methodology. This innovation enables linear O(n) authentication complexity using concise 96-byte cryptographic proofs per participant, yielding a remarkable 17.7x performance enhancement for 32-member rings when compared to conventional techniques. Our work includes rigorous mathematical security demonstrations confirming 112-bit post-quantum protection (NIST Level 1), extensive computational benchmarking, and comprehensive support for both standard anonymity sets and collaborative threshold constructions with flexible participation requirements.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Literature Review on Fundamental Technologies and Security Challenges in the Metaverse Platforms</title>
<link>https://arxiv.org/abs/2510.09621</link>
<guid>https://arxiv.org/abs/2510.09621</guid>
<content:encoded><![CDATA[
arXiv:2510.09621v1 Announce Type: new 
Abstract: The Metaverse utilizes emerging technologies such as Extended Reality (XR), Artificial Intelligence (AI), blockchain, and digital twins to provide an immersive and interactive virtual experience. As Metaverse continues to evolve, it bring a range of security and privacy threats, such as identity management, data governance, and user interactions. This survey aims to provide a comprehensive review of the enabling technologies for the Metaverse. It also aims to provide a thorough analysis of key vulnerabilities and threats that may compromise its sustainability and user safety. We perform a systematic literature review (SLR) to identify key vulnerabilities and their countermeasures in Metaverse platforms. Metaverse offers a much larger attack surface compared to conventional digital platforms. Immersive, decentralized, and permanent characteristics of the Metaverse generate new vulnerabilities. Although there are many countermeasures to these vulnerabilities, most of them are theoretical or have not been tested in real-world environments. Our review highlights current advancements, identifies research gaps, and outlines future directions to ensure a secure, resilient, and ethically governed Metaverse.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Transaction Tracing Techniques for Blockchain Systems</title>
<link>https://arxiv.org/abs/2510.09624</link>
<guid>https://arxiv.org/abs/2510.09624</guid>
<content:encoded><![CDATA[
arXiv:2510.09624v1 Announce Type: new 
Abstract: With the proliferation of new blockchain-based cryptocurrencies/assets and platforms that make it possible to transact across them, it becomes important to consider not just whether the transfer of coins/assets can be tracked within their respective transaction ledger, but also if they can be tracked as they move across ledgers. This is especially important given that there are documented cases of criminals attempting to use these cross-ledger trades to obscure the flow of their coins/assets. In this paper, we perform a systematic review of the various tracing techniques for blockchain transactions proposed in literature, categorize them using multiple criteria (such as tracing approach and targeted objective) and compare them. Based on the above categorization, we provide insights on the state of blockchain transaction tracing literature and identify the limitations of existing approaches. Finally, we suggest directions for future research in this area based on our analysis.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scalable, Privacy-Preserving Decentralized Identity and Verifiable Data Sharing Framework based on Zero-Knowledge Proofs</title>
<link>https://arxiv.org/abs/2510.09715</link>
<guid>https://arxiv.org/abs/2510.09715</guid>
<content:encoded><![CDATA[
arXiv:2510.09715v1 Announce Type: new 
Abstract: With the proliferation of decentralized applications (DApps), the conflict between the transparency of blockchain technology and user data privacy has become increasingly prominent. While Decentralized Identity (DID) and Verifiable Credentials (VCs) provide a standardized framework for user data sovereignty, achieving trusted identity verification and data sharing without compromising privacy remains a significant challenge. This paper proposes a novel, comprehensive framework that integrates DIDs and VCs with efficient Zero-Knowledge Proof (ZKP) schemes to address this core issue. The key contributions of this framework are threefold: first, it constructs a set of strong privacy-preserving protocols based on zk-STARKs, allowing users to prove that their credentials satisfy specific conditions (e.g., "age is over 18") without revealing any underlying sensitive data. Second, it designs a scalable, privacy-preserving credential revocation mechanism based on cryptographic accumulators, effectively solving credential management challenges in large-scale scenarios. Finally, it integrates a practical social key recovery scheme, significantly enhancing system usability and security. Through a prototype implementation and performance evaluation, this paper quantitatively analyzes the framework's performance in terms of proof generation time, verification overhead, and on-chain costs. Compared to existing state-of-the-art systems based on zk-SNARKs, our framework, at the cost of a larger proof size, significantly improves prover efficiency for complex computations and provides stronger security guarantees, including no trusted setup and post-quantum security. Finally, a case study in the decentralized finance (DeFi) credit scoring scenario demonstrates the framework's immense potential for unlocking capital efficiency and fostering a trusted data economy.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zk-SNARK Marketplace with Proof of Useful Work</title>
<link>https://arxiv.org/abs/2510.09729</link>
<guid>https://arxiv.org/abs/2510.09729</guid>
<content:encoded><![CDATA[
arXiv:2510.09729v1 Announce Type: new 
Abstract: Proof of Work (PoW) is widely regarded as the most secure permissionless blockchain consensus protocol. However, its reliance on computationally intensive yet externally useless puzzles results in excessive electric energy wasting. To alleviate this, Proof of Useful Work (PoUW) has been explored as an alternative to secure blockchain platforms while also producing real-world value. Despite this promise, existing PoUW proposals often fail to embed the integrity of the chain and identity of the miner into the puzzle solutions, not meeting necessary requirements for PoW and thus rendering them vulnerable. In this work, we propose a PoUW consensus protocol that computes client-outsourced zk-SNARKs proofs as a byproduct, which are at the same time used to secure the consensus protocol. We further leverage this mechanism to design a decentralized marketplace for outsourcing zk-SNARK proof generation, which is, to the best of our knowledge, the first such marketplace operating at the consensus layer, while meeting all necessary properties of PoW.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Farewell to Westphalia: Crypto Sovereignty and Post-Nation-State Governaance</title>
<link>https://arxiv.org/abs/2510.09840</link>
<guid>https://arxiv.org/abs/2510.09840</guid>
<content:encoded><![CDATA[
arXiv:2510.09840v1 Announce Type: new 
Abstract: We argue that the principal application for blockchain technology will not be in the financial sector, but rather in maintaining decentralized human governance, from archives to transparent policies encoded in the blockchain in the form of smart contracts.. Such decentralized, blockchain-grounded governance comes not a moment too soon, as nation states are dissolving before our eyes. Will blockchain-based communities replace the nation state? What are the prospects and dangers of this development?
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Cooperative Multi-Agent Reinforcement Learning: a Bayesian Network Perspective</title>
<link>https://arxiv.org/abs/2510.09937</link>
<guid>https://arxiv.org/abs/2510.09937</guid>
<content:encoded><![CDATA[
arXiv:2510.09937v1 Announce Type: new 
Abstract: The empirical success of multi-agent reinforcement learning (MARL) has motivated the search for more efficient and scalable algorithms for large scale multi-agent systems. However, existing state-of-the-art algorithms do not fully exploit inter-agent coupling information to develop MARL algorithms. In this paper, we propose a systematic approach to leverage structures in the inter-agent couplings for efficient model-free reinforcement learning. We model the cooperative MARL problem via a Bayesian network and characterize the subset of agents, termed as the value dependency set, whose information is required by each agent to estimate its local action value function exactly. Moreover, we propose a partially decentralized training decentralized execution (P-DTDE) paradigm based on the value dependency set. We theoretically establish that the total variance of our P-DTDE policy gradient estimator is less than the centralized training decentralized execution (CTDE) policy gradient estimator. We derive a multi-agent policy gradient theorem based on the P-DTDE scheme and develop a scalable actor-critic algorithm. We demonstrate the efficiency and scalability of the proposed algorithm on multi-warehouse resource allocation and multi-zone temperature control examples. For dense value dependency sets, we propose an approximation scheme based on truncation of the Bayesian network and empirically show that it achieves a faster convergence than the exact value dependence set for applications with a large number of agents.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-Quantum Cryptography and Quantum-Safe Security: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2510.10436</link>
<guid>https://arxiv.org/abs/2510.10436</guid>
<content:encoded><![CDATA[
arXiv:2510.10436v1 Announce Type: new 
Abstract: Post-quantum cryptography (PQC) is moving from evaluation to deployment as NIST finalizes standards for ML-KEM, ML-DSA, and SLH-DSA. This survey maps the space from foundations to practice. We first develop a taxonomy across lattice-, code-, hash-, multivariate-, isogeny-, and MPC-in-the-Head families, summarizing security assumptions, cryptanalysis, and standardization status. We then compare performance and communication costs using representative, implementation-grounded measurements, and review hardware acceleration (AVX2, FPGA/ASIC) and implementation security with a focus on side-channel resistance. Building upward, we examine protocol integration (TLS, DNSSEC), PKI and certificate hygiene, and deployment in constrained and high-assurance environments (IoT, cloud, finance, blockchain). We also discuss complementarity with quantum technologies (QKD, QRNGs) and the limits of near-term quantum computing. Throughout, we emphasize crypto-agility, hybrid migration, and evidence-based guidance for operators. We conclude with open problems spanning parameter agility, leakage-resilient implementations, and domain-specific rollout playbooks. This survey aims to be a practical reference for researchers and practitioners planning quantum-safe systems, bridging standards, engineering, and operations.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multitask Learning with Learned Task Relationships</title>
<link>https://arxiv.org/abs/2510.10570</link>
<guid>https://arxiv.org/abs/2510.10570</guid>
<content:encoded><![CDATA[
arXiv:2510.10570v1 Announce Type: new 
Abstract: Classical consensus-based strategies for federated and decentralized learning are statistically suboptimal in the presence of heterogeneous local data or task distributions. As a result, in recent years, there has been growing interest in multitask or personalized strategies, which allow individual agents to benefit from one another in pursuing locally optimal models without enforcing consensus. Existing strategies require either precise prior knowledge of the underlying task relationships or are fully non-parametric and instead rely on meta-learning or proximal constructions. In this work, we introduce an algorithmic framework that strikes a balance between these extremes. By modeling task relationships through a Gaussian Markov Random Field with an unknown precision matrix, we develop a strategy that jointly learns both the task relationships and the local models, allowing agents to self-organize in a way consistent with their individual data distributions. Our theoretical analysis quantifies the quality of the learned relationship, and our numerical experiments demonstrate its practical effectiveness.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toxic Ink on Immutable Paper: Content Moderation for Ethereum Input Data Messages (IDMs)</title>
<link>https://arxiv.org/abs/2510.10761</link>
<guid>https://arxiv.org/abs/2510.10761</guid>
<content:encoded><![CDATA[
arXiv:2510.10761v1 Announce Type: new 
Abstract: Decentralized communication is becoming an important use case within Web3. On Ethereum, users can repurpose the transaction input data field to embed natural-language messages, commonly known as Input Data Messages (IDMs). However, as IDMs gain wider adoption, there has been a growing volume of toxic content on-chain. This trend is concerning, as Ethereum provides no protocol-level support for content moderation.
  We propose two moderation frameworks for Ethereum IDMs: (i) BUILDERMOD, where builders perform semantic checks during block construction; and (ii) USERMOD, where users proactively obtain moderation proofs from external classifiers and embed them in transactions. Our evaluation reveals that BUILDERMOD incurs high block-time overhead, which limits its practicality. In contrast, USERMOD enables lower-latency validation and scales more effectively, making it a more practical approach in moderation-aware Ethereum environments.
  Our study lays the groundwork for protocol-level content governance in decentralized systems, and we hope it contributes to the development of a decentralized communication environment that is safe, trustworthy, and socially responsible.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stabilizing the Staking Rate, Dynamically Distributed Inflation and Delay Induced Oscillations</title>
<link>https://arxiv.org/abs/2510.11065</link>
<guid>https://arxiv.org/abs/2510.11065</guid>
<content:encoded><![CDATA[
arXiv:2510.11065v1 Announce Type: new 
Abstract: Dynamically distributed inflation is a common mechanism used to guide a blockchain's staking rate towards a desired equilibrium between network security and token liquidity.
  However, the high sensitivity of the annual percentage yield to changes in the staking rate, coupled with the inherent feedback delays in staker responses, can induce undesirable oscillations around this equilibrium.
  This paper investigates this instability phenomenon. We analyze the dynamics of inflation-based reward systems and propose a novel distribution model designed to stabilize the staking rate. Our solution effectively dampens oscillations, stabilizing the yield within a target staking range.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Decentralized Microservice Scheduling Approach Using Service Mesh in Cloud-Edge Systems</title>
<link>https://arxiv.org/abs/2510.11189</link>
<guid>https://arxiv.org/abs/2510.11189</guid>
<content:encoded><![CDATA[
arXiv:2510.11189v1 Announce Type: new 
Abstract: As microservice-based systems scale across the cloud-edge continuum, traditional centralized scheduling mechanisms increasingly struggle with latency, coordination overhead, and fault tolerance. This paper presents a new architectural direction: leveraging service mesh sidecar proxies as decentralized, in-situ schedulers to enable scalable, low-latency coordination in large-scale, cloud-native environments. We propose embedding lightweight, autonomous scheduling logic into each sidecar, allowing scheduling decisions to be made locally without centralized control. This approach leverages the growing maturity of service mesh infrastructures, which support programmable distributed traffic management. We describe the design of such an architecture and present initial results demonstrating its scalability potential in terms of response time and latency under varying request rates. Rather than delivering a finalized scheduling algorithm, this paper presents a system-level architectural direction and preliminary evidence to support its scalability potential.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fast-Converging Decentralized Approach to the Weighted Minimum Vertex Cover Problem</title>
<link>https://arxiv.org/abs/2510.11697</link>
<guid>https://arxiv.org/abs/2510.11697</guid>
<content:encoded><![CDATA[
arXiv:2510.11697v1 Announce Type: new 
Abstract: We address the problem of computing a Minimum Weighted Vertex Cover (MWVC) in a decentralized network. MWVC, a classical NP-hard problem, is foundational in applications such as network monitoring and resource placement. We propose a fully decentralized protocol where each node makes decisions using only local knowledge and communicates with its neighbors. The method is adaptive, communication-efficient, and avoids centralized coordination. We evaluate the protocol on real-world and synthetic graphs, comparing it to both centralized and decentralized baselines. Our results demonstrate competitive solution quality with reduced communication overhead, highlighting the feasibility of MWVC computation in decentralized environments.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exposing the Vulnerability of Decentralized Learning to Membership Inference Attacks Through the Lens of Graph Mixing</title>
<link>https://arxiv.org/abs/2412.12837</link>
<guid>https://arxiv.org/abs/2412.12837</guid>
<content:encoded><![CDATA[
arXiv:2412.12837v3 Announce Type: replace 
Abstract: The primary promise of decentralized learning is to allow users to engage in the training of machine learning models in a collaborative manner while keeping their data on their premises and without relying on any central entity. However, this paradigm necessitates the exchange of model parameters or gradients between peers. Such exchanges can be exploited to infer sensitive information about training data, which is achieved through privacy attacks (e.g., Membership Inference Attacks -- MIA). In order to devise effective defense mechanisms, it is important to understand the factors that increase/reduce the vulnerability of a given decentralized learning architecture to MIA. In this study, we extensively explore the vulnerability to MIA of various decentralized learning architectures by varying the graph structure (e.g., number of neighbors), the graph dynamics, and the aggregation strategy, across diverse datasets and data distributions. Our key finding, which to the best of our knowledge we are the first to report, is that the vulnerability to MIA is heavily correlated to (i) the local model mixing strategy performed by each node upon reception of models from neighboring nodes and (ii) the global mixing properties of the communication graph. We illustrate these results experimentally using four datasets and by theoretically analyzing the mixing properties of various decentralized architectures. We also empirically show that enhancing mixing properties is highly beneficial when combined with other privacy-preserving techniques such as Differential Privacy. Our paper draws a set of lessons learned for devising decentralized learning systems that reduce by design the vulnerability to MIA.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clustering Deposit and Withdrawal Activity in Tornado Cash: A Cross-Chain Analysis</title>
<link>https://arxiv.org/abs/2510.09433</link>
<guid>https://arxiv.org/abs/2510.09433</guid>
<content:encoded><![CDATA[
arXiv:2510.09433v2 Announce Type: replace 
Abstract: Tornado Cash is a decentralised mixer that uses cryptographic techniques to sever the on-chain trail between depositors and withdrawers. In practice, however, its anonymity can be undermined by user behaviour and operational quirks. We conduct the first cross-chain empirical study of Tornado Cash activity on Ethereum, BNB Smart Chain, and Polygon, introducing three clustering heuristics-(i) address-reuse, (ii) transactional-linkage, and (iii) a novel first-in-first-out (FIFO) temporal-matching rule. Together, these heuristics reconnect deposits to withdrawals and deanonymise a substantial share of recipients. Our analysis shows that 5.1 - 12.6% of withdrawals can already be traced to their originating deposits through address reuse and transactional linkage heuristics. Adding our novel First-In-First-Out (FIFO) temporal-matching heuristic lifts the linkage rate by a further 15 - 22 percentage points. Statistical tests confirm that these FIFO matches are highly unlikely to occur by chance. Comparable leakage across Ethereum, BNB Smart Chain, and Polygon indicates chain-agnostic user misbehaviour, rather than chain-specific protocol flaws. These results expose how quickly cryptographic guarantees can unravel in everyday use, underscoring the need for both disciplined user behaviour and privacy-aware protocol design. In total, our heuristics link over $2.3 billion in Tornado Cash withdrawals to identifiable deposits, exposing significant cracks in practical anonymity.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Impact of Sanctions on decentralised Privacy Tools: A Case Study of Tornado Cash</title>
<link>https://arxiv.org/abs/2510.09443</link>
<guid>https://arxiv.org/abs/2510.09443</guid>
<content:encoded><![CDATA[
arXiv:2510.09443v2 Announce Type: replace 
Abstract: This paper investigates the impact of sanctions on Tornado Cash, a smart contract protocol designed to enhance transaction privacy. Following the U.S. Department of the Treasury's sanctions against Tornado Cash in August 2022, platform activity declined sharply. We document a significant and sustained reduction in transaction volume, user diversity, and overall protocol utilization after the sanctions were imposed. Our analysis draws on transaction data from three major blockchains: Ethereum, BNB Smart Chain, and Polygon. We further examine developments following the partial lifting and eventual removal of sanctions by the U.S. Office of Foreign Assets Control (OFAC) in March 2025. Although activity partially recovered, the rebound remained limited. The Tornado Cash case illustrates how regulatory interventions can affect decentralized protocols, while also highlighting the challenges of fully enforcing such measures in decentralized environments.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible Swarm Learning May Outpace Foundation Models in Essential Tasks</title>
<link>https://arxiv.org/abs/2510.06349</link>
<guid>https://arxiv.org/abs/2510.06349</guid>
<content:encoded><![CDATA[
arXiv:2510.06349v1 Announce Type: new 
Abstract: Foundation models have rapidly advanced AI, raising the question of whether their decisions will ultimately surpass human strategies in real-world domains. The exponential, and possibly super-exponential, pace of AI development makes such analysis elusive. Nevertheless, many application areas that matter for daily life and society show only modest gains so far; a prominent case is diagnosing and treating dynamically evolving disease in intensive care.
  The common challenge is adapting complex systems to dynamic environments. Effective strategies must optimize outcomes in systems composed of strongly interacting functions while avoiding shared side effects; this requires reliable, self-adaptive modeling. These tasks align with building digital twins of highly complex systems whose mechanisms are not fully or quantitatively understood. It is therefore essential to develop methods for self-adapting AI models with minimal data and limited mechanistic knowledge. As this challenge extends beyond medicine, AI should demonstrate clear superiority in these settings before assuming broader decision-making roles.
  We identify the curse of dimensionality as a fundamental barrier to efficient self-adaptation and argue that monolithic foundation models face conceptual limits in overcoming it. As an alternative, we propose a decentralized architecture of interacting small agent networks (SANs). We focus on agents representing the specialized substructure of the system, where each agent covers only a subset of the full system functions. Drawing on mathematical results on the learning behavior of SANs and evidence from existing applications, we argue that swarm-learning in diverse swarms can enable self-adaptive SANs to deliver superior decision-making in dynamic environments compared with monolithic foundation models, though at the cost of reduced reproducibility in detail.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R3R: Decentralized Multi-Agent Collision Avoidance with Infinite-Horizon Safety</title>
<link>https://arxiv.org/abs/2510.06436</link>
<guid>https://arxiv.org/abs/2510.06436</guid>
<content:encoded><![CDATA[
arXiv:2510.06436v1 Announce Type: new 
Abstract: Existing decentralized methods for multi-agent motion planning lack formal, infinite-horizon safety guarantees, especially for communication-constrained systems. We present R3R, to our knowledge the first decentralized and asynchronous framework for multi-agent motion planning under distance-based communication constraints with infinite-horizon safety guarantees for systems of nonlinear agents. R3R's novelty lies in combining our gatekeeper safety framework with a geometric constraint called R-Boundedness, which together establish a formal link between an agent's communication radius and its ability to plan safely. We constrain trajectories to within a fixed planning radius that is a function of the agent's communication radius, which enables trajectories to be shown provably safe for all time, using only local information. Our algorithm is fully asynchronous, and ensures the forward invariance of these guarantees even in time-varying networks where agents asynchronously join, leave, and replan. We validate our approach in simulations of up to 128 Dubins vehicles, demonstrating 100% safety in dense, obstacle rich scenarios. Our results demonstrate that R3R's performance scales with agent density rather than problem size, providing a practical solution for scalable and provably safe multi-agent systems.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Aware Inference via Performance Forecasting in Decentralized Learning Networks</title>
<link>https://arxiv.org/abs/2510.06444</link>
<guid>https://arxiv.org/abs/2510.06444</guid>
<content:encoded><![CDATA[
arXiv:2510.06444v1 Announce Type: new 
Abstract: In decentralized learning networks, predictions from many participants are combined to generate a network inference. While many studies have demonstrated performance benefits of combining multiple model predictions, existing strategies using linear pooling methods (ranging from simple averaging to dynamic weight updates) face a key limitation. Dynamic prediction combinations that rely on historical performance to update weights are necessarily reactive. Due to the need to average over a reasonable number of epochs (with moving averages or exponential weighting), they tend to be slow to adjust to changing circumstances (phase or regime changes). In this work, we develop a model that uses machine learning to forecast the performance of predictions by models at each epoch in a time series. This enables `context-awareness' by assigning higher weight to models that are likely to be more accurate at a given time. We show that adding a performance forecasting worker in a decentralized learning network, following a design similar to the Allora network, can improve the accuracy of network inferences. Specifically, we find forecasting models that predict regret (performance relative to the network inference) or regret z-score (performance relative to other workers) show greater improvement than models predicting losses, which often do not outperform the naive network inference (historically weighted average of all inferences). Through a series of optimization tests, we show that the performance of the forecasting model can be sensitive to choices in the feature set and number of training epochs. These properties may depend on the exact problem and should be tailored to each domain. Although initially designed for a decentralized learning network, using performance forecasting for prediction combination may be useful in any situation where predictive rather than reactive model weighting is needed.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Fronthaul Quantization for Flexible User Load in Cell-Free Massive MIMO</title>
<link>https://arxiv.org/abs/2510.06734</link>
<guid>https://arxiv.org/abs/2510.06734</guid>
<content:encoded><![CDATA[
arXiv:2510.06734v1 Announce Type: new 
Abstract: We investigate the physical layer (PHY) spectral efficiency and fronthaul network load of a scalable user-centric cell-free massive MIMO system. Each user-centric cluster processor responsible for cluster-level signal processing is located at one of multiple decentralized units (DUs). Thus, the radio units in the cluster must exchange data with the corresponding DU over the fronthaul. Because the fronthaul links have limited capacity, this data must be quantized before it is sent over the fronthaul. We consider a routed fronthaul network, where the cluster processor placement and fronthaul traffic routing are jointly optimized with a mixed-integer linear program. For different numbers of users in the network, we investigate the effect of fronthaul quantization rates, a system parameter computed based on rate-distortion theory. Our results show that with optimized quantization rates, the fronthaul load is quite stable for a wide range of user loads without significant PHY performance loss. This demonstrates that the cell-free massive MIMO PHY and fronthaul network are resilient to varying user densities.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bionetta: Efficient Client-Side Zero-Knowledge Machine Learning Proving</title>
<link>https://arxiv.org/abs/2510.06784</link>
<guid>https://arxiv.org/abs/2510.06784</guid>
<content:encoded><![CDATA[
arXiv:2510.06784v1 Announce Type: new 
Abstract: In this report, we compare the performance of our UltraGroth-based zero-knowledge machine learning framework Bionetta to other tools of similar purpose such as EZKL, Lagrange's deep-prove, or zkml. The results show a significant boost in the proving time for custom-crafted neural networks: they can be proven even on mobile devices, enabling numerous client-side proving applications. While our scheme increases the cost of one-time preprocessing steps, such as circuit compilation and generating trusted setup, our approach is, to the best of our knowledge, the only one that is deployable on the native EVM smart contracts without overwhelming proof size and verification overheads.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized CBF-based Safety Filters for Collision Avoidance of Cooperative Missile Systems with Input Constraints</title>
<link>https://arxiv.org/abs/2510.06846</link>
<guid>https://arxiv.org/abs/2510.06846</guid>
<content:encoded><![CDATA[
arXiv:2510.06846v1 Announce Type: new 
Abstract: This paper presents a decentralized safety filter for collision avoidance in multi-agent aerospace interception scenarios. The approach leverages robust control barrier functions (RCBFs) to guarantee forward invariance of safety sets under bounded inputs and high-relative-degree dynamics. Each effector executes its nominal cooperative guidance command, while a local quadratic program (QP) modifies the input only when necessary. Event-triggered activation based on range and zero-effort miss (ZEM) criteria ensures scalability by restricting active constraints to relevant neighbors. To resolve feasibility issues from simultaneous constraints, a slack-variable relaxation scheme is introduced that prioritizes critical agents in a Pareto-optimal manner. Simulation results in many-on-many interception scenarios demonstrate that the proposed framework maintains collision-free operation with minimal deviation from nominal guidance, providing a computationally efficient and scalable solution for safety-critical multi-agent aerospace systems.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Unlearning in the Wild: Rethinking Fairness and Data Discrepancy</title>
<link>https://arxiv.org/abs/2510.07022</link>
<guid>https://arxiv.org/abs/2510.07022</guid>
<content:encoded><![CDATA[
arXiv:2510.07022v1 Announce Type: new 
Abstract: Machine unlearning is critical for enforcing data deletion rights like the "right to be forgotten." As a decentralized paradigm, Federated Learning (FL) also requires unlearning, but realistic implementations face two major challenges. First, fairness in Federated Unlearning (FU) is often overlooked. Exact unlearning methods typically force all clients into costly retraining, even those uninvolved. Approximate approaches, using gradient ascent or distillation, make coarse interventions that can unfairly degrade performance for clients with only retained data. Second, most FU evaluations rely on synthetic data assumptions (IID/non-IID) that ignore real-world heterogeneity. These unrealistic benchmarks obscure the true impact of unlearning and limit the applicability of current methods. We first conduct a comprehensive benchmark of existing FU methods under realistic data heterogeneity and fairness conditions. We then propose a novel, fairness-aware FU approach, Federated Cross-Client-Constrains Unlearning (FedCCCU), to explicitly address both challenges. FedCCCU offers a practical and scalable solution for real-world FU. Experimental results show that existing methods perform poorly in realistic settings, while our approach consistently outperforms them.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pseudo-MDPs: A Novel Framework for Efficiently Optimizing Last Revealer Seed Manipulations in Blockchains</title>
<link>https://arxiv.org/abs/2510.07080</link>
<guid>https://arxiv.org/abs/2510.07080</guid>
<content:encoded><![CDATA[
arXiv:2510.07080v1 Announce Type: new 
Abstract: This study tackles the computational challenges of solving Markov Decision Processes (MDPs) for a restricted class of problems. It is motivated by the Last Revealer Attack (LRA), which undermines fairness in some Proof-of-Stake (PoS) blockchains such as Ethereum (\$400B market capitalization). We introduce pseudo-MDPs (pMDPs) a framework that naturally models such problems and propose two distinct problem reductions to standard MDPs. One problem reduction provides a novel, counter-intuitive perspective, and combining the two problem reductions enables significant improvements in dynamic programming algorithms such as value iteration. In the case of the LRA which size is parameterized by $\kappa$ (in Ethereum's case $\kappa$ = 32), we reduce the computational complexity from O(2^$\kappa$ $\kappa$^2^($\kappa$+2)) to O($\kappa$^4) (per iteration). This solution also provide the usual benefits from Dynamic Programming solutions: exponentially fast convergence toward the optimal solution is guaranteed. The dual perspective also simplifies policy extraction, making the approach well-suited for resource-constrained agents who can operate with very limited memory and computation once the problem has been solved. Furthermore, we generalize those results to a broader class of MDPs, enhancing their applicability. The framework is validated through two case studies: a fictional card game and the LRA on the Ethereum random seed consensus protocol. These applications demonstrate the framework's ability to solve large-scale problems effectively while offering actionable insights into optimal strategies. This work advances the study of MDPs and contributes to understanding security vulnerabilities in blockchain systems.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RevealNet: Distributed Traffic Correlation for Attack Attribution on Programmable Networks</title>
<link>https://arxiv.org/abs/2505.00618</link>
<guid>https://arxiv.org/abs/2505.00618</guid>
<content:encoded><![CDATA[
arXiv:2505.00618v2 Announce Type: replace 
Abstract: Network attackers have increasingly resorted to proxy chains, VPNs, and anonymity networks to conceal their activities. To tackle this issue, past research has explored the applicability of traffic correlation techniques to perform attack attribution, i.e., to identify an attacker's true network location. However, current traffic correlation approaches rely on well-provisioned and centralized systems that ingest flows from multiple network probes to compute correlation scores. Unfortunately, this makes correlation efforts scale poorly for large high-speed networks.
  In this paper, we propose RevealNet, a decentralized framework for attack attribution that orchestrates a fleet of P4-programmable switches to perform traffic correlation. RevealNet builds on a set of correlation primitives inspired by prior work on computing and comparing flow sketches -- compact summaries of flows' key characteristics -- to enable efficient, distributed, in-network traffic correlation. Our evaluation suggests that RevealNet achieves comparable accuracy to centralized attack attribution systems while significantly reducing both the computational complexity and bandwidth overheads imposed by correlation tasks.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentZero++: Modeling Fear-Based Behavior</title>
<link>https://arxiv.org/abs/2510.05185</link>
<guid>https://arxiv.org/abs/2510.05185</guid>
<content:encoded><![CDATA[
arXiv:2510.05185v1 Announce Type: new 
Abstract: We present AgentZero++, an agent-based model that integrates cognitive, emotional, and social mechanisms to simulate decentralized collective violence in spatially distributed systems. Building on Epstein's Agent\_Zero framework, we extend the original model with eight behavioral enhancements: age-based impulse control; memory-based risk estimation; affect-cognition coupling; endogenous destructive radius; fight-or-flight dynamics; affective homophily; retaliatory damage; and multi-agent coordination. These additions allow agents to adapt based on internal states, previous experiences, and social feedback, producing emergent dynamics such as protest asymmetries, escalation cycles, and localized retaliation. Implemented in Python using the Mesa ABM framework, AgentZero++ enables modular experimentation and visualization of how micro-level cognitive heterogeneity shapes macro-level conflict patterns. Our results highlight how small variations in memory, reactivity, and affective alignment can amplify or dampen unrest through feedback loops. By explicitly modeling emotional thresholds, identity-driven behavior, and adaptive networks, this work contributes a flexible and extensible platform for analyzing affective contagion and psychologically grounded collective action.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constraint-Level Design of zkEVMs: Architectures, Trade-offs, and Evolution</title>
<link>https://arxiv.org/abs/2510.05376</link>
<guid>https://arxiv.org/abs/2510.05376</guid>
<content:encoded><![CDATA[
arXiv:2510.05376v1 Announce Type: new 
Abstract: Zero-knowledge Ethereum Virtual Machines (zkEVMs) must reconcile a fundamental contradiction: the Ethereum Virtual Machine was designed for transparent sequential execution, while zero-knowledge proofs require algebraic circuit representations. This survey provides the first systematic analysis of how existing major production zkEVM implementations resolve this tension through distinct constraint engineering strategies. We develop a comparative framework that maps the design space across three architectural dimensions. First, arithmetization schemes reveal stark trade-offs: R1CS requires compositional gadget libraries, PLONKish achieves elegance through custom gates that capture complex EVM opcodes in single constraints, while the homogeneous structure of AIR fundamentally mismatches the irregular instruction set of EVM. Second, dispatch mechanisms determine constraint activation patterns: selector-based systems waste trace width on inactive constraints, while ROM-based approaches trade memory lookups for execution flexibility. Third, the Type 1-4 spectrum quantifies an inescapable trade-off: the bit-level EVM compatibility of Type 1 demands significantly higher constraint complexity than the custom instruction sets of Type 4. Beyond cataloging implementations, we identify critical open problems across multiple domains: performance barriers preventing sub-second proving, absence of formal verification for constraint-to-EVM semantic equivalence, lack of standardized benchmarking frameworks, and architectural gaps in hybrid zkEVM/zkVM designs, decentralized prover coordination, privacy preservation, and interoperability.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanism design and equilibrium analysis of smart contract mediated resource allocation</title>
<link>https://arxiv.org/abs/2510.05504</link>
<guid>https://arxiv.org/abs/2510.05504</guid>
<content:encoded><![CDATA[
arXiv:2510.05504v1 Announce Type: new 
Abstract: Decentralized coordination and digital contracting are becoming critical in complex industrial ecosystems, yet existing approaches often rely on ad hoc heuristics or purely technical blockchain implementations without a rigorous economic foundation. This study develops a mechanism design framework for smart contract-based resource allocation that explicitly embeds efficiency and fairness in decentralized coordination. We establish the existence and uniqueness of contract equilibria, extending classical results in mechanism design, and introduce a decentralized price adjustment algorithm with provable convergence guarantees that can be implemented in real time. To evaluate performance, we combine extensive synthetic benchmarks with a proof-of-concept real-world dataset (MovieLens). The synthetic tests probe robustness under fee volatility, participation shocks, and dynamic demand, while the MovieLens case study illustrates how the mechanism can balance efficiency and fairness in realistic allocation environments. Results demonstrate that the proposed mechanism achieves substantial improvements in both efficiency and equity while remaining resilient to abrupt perturbations, confirming its stability beyond steady state analysis. The findings highlight broad managerial and policy relevance for supply chains, logistics, energy markets, healthcare resource allocation, and public infrastructure, where transparent and auditable coordination is increasingly critical. By combining theoretical rigor with empirical validation, the study shows how digital contracts can serve not only as technical artifacts but also as institutional instruments for transparency, accountability, and resilience in high-stakes resource allocation.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Correlation-Aware Dual-View Pose and Velocity Estimation for Dynamic Robotic Manipulation</title>
<link>https://arxiv.org/abs/2510.05536</link>
<guid>https://arxiv.org/abs/2510.05536</guid>
<content:encoded><![CDATA[
arXiv:2510.05536v1 Announce Type: new 
Abstract: Accurate pose and velocity estimation is essential for effective spatial task planning in robotic manipulators. While centralized sensor fusion has traditionally been used to improve pose estimation accuracy, this paper presents a novel decentralized fusion approach to estimate both pose and velocity. We use dual-view measurements from an eye-in-hand and an eye-to-hand vision sensor configuration mounted on a manipulator to track a target object whose motion is modeled as random walk (stochastic acceleration model). The robot runs two independent adaptive extended Kalman filters formulated on a matrix Lie group, developed as part of this work. These filters predict poses and velocities on the manifold $\mathbb{SE}(3) \times \mathbb{R}^3 \times \mathbb{R}^3$ and update the state on the manifold $\mathbb{SE}(3)$. The final fused state comprising the fused pose and velocities of the target is obtained using a correlation-aware fusion rule on Lie groups. The proposed method is evaluated on a UFactory xArm 850 equipped with Intel RealSense cameras, tracking a moving target. Experimental results validate the effectiveness and robustness of the proposed decentralized dual-view estimation framework, showing consistent improvements over state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving On-chain Permissioning for KYC-Compliant Decentralized Applications</title>
<link>https://arxiv.org/abs/2510.05807</link>
<guid>https://arxiv.org/abs/2510.05807</guid>
<content:encoded><![CDATA[
arXiv:2510.05807v1 Announce Type: new 
Abstract: Decentralized applications (dApps) in Decentralized Finance (DeFi) face a fundamental tension between regulatory compliance requirements like Know Your Customer (KYC) and maintaining decentralization and privacy. Existing permissioned DeFi solutions often fail to adequately protect private attributes of dApp users and introduce implicit trust assumptions, undermining the blockchain's decentralization. Addressing these limitations, this paper presents a novel synthesis of Self-Sovereign Identity (SSI), Zero-Knowledge Proofs (ZKPs), and Attribute-Based Access Control to enable privacy-preserving on-chain permissioning based on decentralized policy decisions. We provide a comprehensive framework for permissioned dApps that aligns decentralized trust, privacy, and transparency, harmonizing blockchain principles with regulatory compliance. Our framework supports multiple proof types (equality, range, membership, and time-dependent) with efficient proof generation through a commit-and-prove scheme that moves credential authenticity verification outside the ZKP circuit. Experimental evaluation of our KYC-compliant DeFi implementation shows considerable performance improvement for different proof types compared to baseline approaches. We advance the state-of-the-art through a holistic approach, flexible proof mechanisms addressing diverse real-world requirements, and optimized proof generation enabling practical deployment.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EARL: Efficient Agentic Reinforcement Learning Systems for Large Language Models</title>
<link>https://arxiv.org/abs/2510.05943</link>
<guid>https://arxiv.org/abs/2510.05943</guid>
<content:encoded><![CDATA[
arXiv:2510.05943v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has become a pivotal component of large language model (LLM) post-training, and agentic RL extends this paradigm to operate as agents through multi-turn interaction and tool use. Scaling such systems exposes two practical bottlenecks: (1) context length grows rapidly during training, inflating memory usage and latency, and triggering out-of-memory (OOM) failures; and (2) intermediate tensors accumulate with context length, making cross-device data movement a major system bottleneck.
  We present EARL, a scalable system for efficient agentic RL. EARL designs a parallelism selector that dynamically adapts model and training parallelism across RL stages based on sequence length and system load, and a data dispatcher that performs layout-aware, decentralized exchange of intermediate data batches. Together, these components increase throughput, reduce long-context failures, and enable stable large-scale training of agentic LLMs without relying on hard limits or penalties of context length.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Robot Distributed Optimization for Exploration and Mapping of Unknown Environments using Bioinspired Tactile-Sensor</title>
<link>https://arxiv.org/abs/2510.06085</link>
<guid>https://arxiv.org/abs/2510.06085</guid>
<content:encoded><![CDATA[
arXiv:2510.06085v1 Announce Type: new 
Abstract: This project proposes a bioinspired multi-robot system using Distributed Optimization for efficient exploration and mapping of unknown environments. Each robot explores its environment and creates a map, which is afterwards put together to form a global 2D map of the environment. Inspired by wall-following behaviors, each robot autonomously explores its neighborhood based on a tactile sensor, similar to the antenna of a cockroach, mounted on the surface of the robot. Instead of avoiding obstacles, robots log collision points when they touch obstacles. This decentralized control strategy ensures effective task allocation and efficient exploration of unknown terrains, with applications in search and rescue, industrial inspection, and environmental monitoring. The approach was validated through experiments using e-puck robots in a simulated 1.5 x 1.5 m environment with three obstacles. The results demonstrated the system's effectiveness in achieving high coverage, minimizing collisions, and constructing accurate 2D maps.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved High-probability Convergence Guarantees of Decentralized SGD</title>
<link>https://arxiv.org/abs/2510.06141</link>
<guid>https://arxiv.org/abs/2510.06141</guid>
<content:encoded><![CDATA[
arXiv:2510.06141v1 Announce Type: new 
Abstract: Convergence in high-probability (HP) has been receiving increasing interest, due to its attractive properties, such as exponentially decaying tail bounds and strong guarantees for each individual run of an algorithm. While HP guarantees are extensively studied in centralized settings, much less is understood in the decentralized, networked setup. Existing HP studies in decentralized settings impose strong assumptions, like uniformly bounded gradients, or asymptotically vanishing noise, resulting in a significant gap between assumptions used to establish convergence in the HP and the mean-squared error (MSE) sense, even for vanilla Decentralized Stochastic Gradient Descent ($\mathtt{DSGD}$) algorithm. This is contrary to centralized settings, where it is known that $\mathtt{SGD}$ converges in HP under the same conditions on the cost function as needed to guarantee MSE convergence. Motivated by this observation, we revisit HP guarantees for $\mathtt{DSGD}$ in the presence of light-tailed noise. We show that $\mathtt{DSGD}$ converges in HP under the same conditions on the cost as in the MSE sense, removing uniformly bounded gradients and other restrictive assumptions, while simultaneously achieving order-optimal rates for both non-convex and strongly convex costs. Moreover, our improved analysis yields linear speed-up in the number of users, demonstrating that $\mathtt{DSGD}$ maintains strong performance in the HP sense and matches existing MSE guarantees. Our improved results stem from a careful analysis of the MGF of quantities of interest (norm-squared of gradient or optimality gap) and the MGF of the consensus gap between users' models. To achieve linear speed-up, we provide a novel result on the variance-reduction effect of decentralized methods in the HP sense and more fine-grained bounds on the MGF for strongly convex costs, which are both of independent interest.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Coordination for Distributed Decision-Making</title>
<link>https://arxiv.org/abs/2401.16926</link>
<guid>https://arxiv.org/abs/2401.16926</guid>
<content:encoded><![CDATA[
arXiv:2401.16926v2 Announce Type: replace 
Abstract: In decentralized network control, communication plays a critical role by transforming local observations into shared knowledge, enabling agents to coordinate their actions. This paper investigates how communication facilitates cooperation behavior and therefore improves the overall performance in the vector-valued Witsenhausen counterexample, a canonical toy example in distributed decision-making. We consider setups where the encoder, i.e., the first decision-maker (DM) acts causally and the decoder, i.e., the second DM, operates noncausally, 1) without and 2) with access to channel feedback. Using a coordination coding framework, we characterize the achievable power-estimation cost regions in single-letter expressions for both scenarios. The first result is that, when restricted to Gaussian random variables, the cost is identical across all setups featuring at least one causal DM - regardless of the presence of feedback information. Next, building on the characterization of the power-estimation cost region, we propose a hybrid scheme that combines discrete quantization with a continuous Gaussian codebook - the Zero Estimation Cost (ZEC) scheme - which achieves an arbitrarily small estimation cost. This scheme uses coding tools that allow perfect reconstruction of the target symbols, leading to an asymptotic estimation cost equal to zero, while significantly reducing the asymptotic power consumption. Furthermore, when channel feedback is available at the first DM, we propose an analogous scheme that simultaneously achieves zero power and zero estimation cost in the low-noise regime.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Collective World Model for Emergent Communication and Coordination</title>
<link>https://arxiv.org/abs/2504.03353</link>
<guid>https://arxiv.org/abs/2504.03353</guid>
<content:encoded><![CDATA[
arXiv:2504.03353v3 Announce Type: replace 
Abstract: We propose a fully decentralized multi-agent world model that enables both symbol emergence for communication and coordinated behavior through temporal extension of collective predictive coding. Unlike previous research that focuses on either communication or coordination separately, our approach achieves both simultaneously. Our method integrates world models with communication channels, enabling agents to predict environmental dynamics, estimate states from partial observations, and share critical information through bidirectional message exchange with contrastive learning for message alignment. Using a two-agent trajectory drawing task, we demonstrate that our communication-based approach outperforms non-communicative models when agents have divergent perceptual capabilities, achieving the second-best coordination after centralized models. Importantly, our decentralized approach with constraints preventing direct access to other agents' internal states facilitates the emergence of more meaningful symbol systems that accurately reflect environmental states. These findings demonstrate the effectiveness of decentralized communication for supporting coordination while developing shared representations of the environment.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QLLM: Do We Really Need a Mixing Network for Credit Assignment in Multi-Agent Reinforcement Learning?</title>
<link>https://arxiv.org/abs/2504.12961</link>
<guid>https://arxiv.org/abs/2504.12961</guid>
<content:encoded><![CDATA[
arXiv:2504.12961v3 Announce Type: replace 
Abstract: Credit assignment has remained a fundamental challenge in multi-agent reinforcement learning (MARL). Previous studies have primarily addressed this issue through value decomposition methods under the centralized training with decentralized execution paradigm, where neural networks are utilized to approximate the nonlinear relationship between individual Q-values and the global Q-value. Although these approaches have achieved considerable success in various benchmark tasks, they still suffer from several limitations, including imprecise attribution of contributions, limited interpretability, and poor scalability in high-dimensional state spaces. To address these challenges, we propose a novel algorithm, \textbf{QLLM}, which facilitates the automatic construction of credit assignment functions using large language models (LLMs). Specifically, the concept of \textbf{TFCAF} is introduced, wherein the credit allocation process is represented as a direct and expressive nonlinear functional formulation. A custom-designed \textit{coder-evaluator} framework is further employed to guide the generation, verification, and refinement of executable code by LLMs, significantly mitigating issues such as hallucination and shallow reasoning during inference. Extensive experiments conducted on several standard MARL benchmarks demonstrate that the proposed method consistently outperforms existing state-of-the-art baselines. Moreover, QLLM exhibits strong generalization capability and maintains compatibility with a wide range of MARL algorithms that utilize mixing networks, positioning it as a promising and versatile solution for complex multi-agent scenarios.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Concept to Measurement: A Survey of How the Blockchain Trilemma Is Analyzed</title>
<link>https://arxiv.org/abs/2505.03768</link>
<guid>https://arxiv.org/abs/2505.03768</guid>
<content:encoded><![CDATA[
arXiv:2505.03768v4 Announce Type: replace 
Abstract: The blockchain trilemma highlights the difficulty of simultaneously achieving a high degree of decentralization (DoD), scalability, and security in blockchain systems. While numerous constructs and metrics have been proposed to analyze these subconcepts, existing guidance is fragmented and inconsistent, limiting comparability across studies. This lack of clarity hinders practitioners in identifying Pareto-optimal blockchain system designs that meet common non-functional requirements. We systematically reviewed literature on the blockchain trilemma and blockchain benchmarks to synthesize constructs and their operationalizations through metrics to analyze the trilemma's subconcepts. We identified 12 constructs, operationalized through 15 metrics, that capture DoD, scalability, and security. We explain how these constructs apply across different blockchain systems and provide a structured overview that supports benchmarking and blockchain system design. Beyond blockchain, the findings offer insights for distributed database systems that rely on consensus and state machine replication. This work contributes a harmonized foundation for quantitative analyses of the blockchain trilemma, guiding both researchers in developing analysis approaches and practitioners in evaluating real-world systems.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge-FIT: Federated Instruction Tuning of Quantized LLMs for Privacy-Preserving Smart Home Environments</title>
<link>https://arxiv.org/abs/2510.03284</link>
<guid>https://arxiv.org/abs/2510.03284</guid>
<content:encoded><![CDATA[
arXiv:2510.03284v1 Announce Type: new 
Abstract: This paper proposes Edge-FIT (Federated Instruction Tuning on the Edge), a scalable framework for Federated Instruction Tuning (FIT) of Large Language Models (LLMs). Traditional Federated Learning (TFL) methods, like FedAvg, fail when confronted with the massive parameter size of LLMs [3], [6]. Our Edge-FIT framework combines federated learning with 4-bit Quantized Low-Rank Adaptation (QLORA), mitigating the core issues of communication and computational overhead. We demonstrate this by filtering the general-purpose Databricks Dolly 15k dataset for the IoT domain. Experimental results show the Edge-FIT tuned Llama 2(7B) achieves an F1-Score of 0.89. We also demonstrate a viable trade-off using the 3.8B Phi-3-mini model, validating Edge-FIT as a scalable framework for decentralized LLM deployment on home compute gateways.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Adaptive Responsible AI Governance Framework for Decentralized Organizations</title>
<link>https://arxiv.org/abs/2510.03368</link>
<guid>https://arxiv.org/abs/2510.03368</guid>
<content:encoded><![CDATA[
arXiv:2510.03368v1 Announce Type: new 
Abstract: This paper examines the assessment challenges of Responsible AI (RAI) governance efforts in globally decentralized organizations through a case study collaboration between a leading research university and a multinational enterprise. While there are many proposed frameworks for RAI, their application in complex organizational settings with distributed decision-making authority remains underexplored. Our RAI assessment, conducted across multiple business units and AI use cases, reveals four key patterns that shape RAI implementation: (1) complex interplay between group-level guidance and local interpretation, (2) challenges translating abstract principles into operational practices, (3) regional and functional variation in implementation approaches, and (4) inconsistent accountability in risk oversight. Based on these findings, we propose an Adaptive RAI Governance (ARGO) Framework that balances central coordination with local autonomy through three interdependent layers: shared foundation standards, central advisory resources, and contextual local implementation. We contribute insights from academic-industry collaboration for RAI assessments, highlighting the importance of modular governance approaches that accommodate organizational complexity while maintaining alignment with responsible AI principles. These lessons offer practical guidance for organizations navigating the transition from RAI principles to operational practice within decentralized structures.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Robust Clustered Federated Learning Approach for Non-IID Data with Quantity Skew</title>
<link>https://arxiv.org/abs/2510.03380</link>
<guid>https://arxiv.org/abs/2510.03380</guid>
<content:encoded><![CDATA[
arXiv:2510.03380v1 Announce Type: new 
Abstract: Federated Learning (FL) is a decentralized paradigm that enables a client-server architecture to collaboratively train a global Artificial Intelligence model without sharing raw data, thereby preserving privacy. A key challenge in FL is Non-IID data. Quantity Skew (QS) is a particular problem of Non-IID, where clients hold highly heterogeneous data volumes. Clustered Federated Learning (CFL) is an emergent variant of FL that presents a promising solution to Non-IID problem. It improves models' performance by grouping clients with similar data distributions into clusters. CFL methods generally fall into two operating strategies. In the first strategy, clients select the cluster that minimizes the local training loss. In the second strategy, the server groups clients based on local model similarities. However, most CFL methods lack systematic evaluation under QS but present significant challenges because of it.  In this paper, we present two main contributions. The first one is an evaluation of state-of-the-art CFL algorithms under various Non-IID settings, applying multiple QS scenarios to assess their robustness. Our second contribution is a novel iterative CFL algorithm, named CORNFLQS, which proposes an optimal coordination between both operating strategies of CFL. Our approach is robust against the different variations of QS settings. We conducted intensive experiments on six image classification datasets, resulting in 270 Non-IID configurations. The results show that CORNFLQS achieves the highest average ranking in both accuracy and clustering quality, as well as strong robustness to QS perturbations. Overall, our approach outperforms actual CFL algorithms.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paris: A Decentralized Trained Open-Weight Diffusion Model</title>
<link>https://arxiv.org/abs/2510.03434</link>
<guid>https://arxiv.org/abs/2510.03434</guid>
<content:encoded><![CDATA[
arXiv:2510.03434v1 Announce Type: new 
Abstract: We present Paris, the first publicly released diffusion model pre-trained entirely through decentralized computation. Paris demonstrates that high-quality text-to-image generation can be achieved without centrally coordinated infrastructure. Paris is open for research and commercial use. Paris required implementing our Distributed Diffusion Training framework from scratch. The model consists of 8 expert diffusion models (129M-605M parameters each) trained in complete isolation with no gradient, parameter, or intermediate activation synchronization. Rather than requiring synchronized gradient updates across thousands of GPUs, we partition data into semantically coherent clusters where each expert independently optimizes its subset while collectively approximating the full distribution. A lightweight transformer router dynamically selects appropriate experts at inference, achieving generation quality comparable to centrally coordinated baselines. Eliminating synchronization enables training on heterogeneous hardware without specialized interconnects. Empirical validation confirms that Paris's decentralized training maintains generation quality while removing the dedicated GPU cluster requirement for large-scale diffusion models. Paris achieves this using 14$\times$ less training data and 16$\times$ less compute than the prior decentralized baseline.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A distributed routing protocol for sending data from things to the cloud leveraging fog technology in the large-scale IoT ecosystem</title>
<link>https://arxiv.org/abs/2510.03524</link>
<guid>https://arxiv.org/abs/2510.03524</guid>
<content:encoded><![CDATA[
arXiv:2510.03524v1 Announce Type: new 
Abstract: Fog computing integrates cloud and edge resources. According to an intelligent and decentralized method, this technology processes data generated by IoT sensors to seamlessly integrate physical and cyber environments. Internet of Things uses wireless and smart objects. They communicate with each other, monitor the environment, collect information, and respond to user requests. These objects have limited energy resources since they use batteries to supply energy. Also, they cannot replace their batteries. As a result, the network lifetime is limited and short. Thus, reducing energy consumption and accelerating the data transmission process are very important challenges in IoT networks to reduce the response time. In the data transmission process, selecting an appropriate cluster head node is very important because it can reduce the delay when sending data to the fog. In this paper, cluster head nodes are selected based on several important criteria such as distance, residual energy, received signal strength, and link expiration time. Then, objects send the processed data to the server hierarchically through a balanced tree. The simulation results show that the proposed method outperforms the energy-efficient centroid-based routing protocol (EECRP) and the Emergency Response IoT based on Global Information Decision (ERGID) in terms of packet delivery rate, delay, response time, and network lifetime.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning for Multi-Agent Coordination</title>
<link>https://arxiv.org/abs/2510.03592</link>
<guid>https://arxiv.org/abs/2510.03592</guid>
<content:encoded><![CDATA[
arXiv:2510.03592v1 Announce Type: new 
Abstract: We address the challenge of coordinating multiple robots in narrow and confined environments, where congestion and interference often hinder collective task performance. Drawing inspiration from insect colonies, which achieve robust coordination through stigmergy -- modifying and interpreting environmental traces -- we propose a Stigmergic Multi-Agent Deep Reinforcement Learning (S-MADRL) framework that leverages virtual pheromones to model local and social interactions, enabling decentralized emergent coordination without explicit communication. To overcome the convergence and scalability limitations of existing algorithms such as MADQN, MADDPG, and MAPPO, we leverage curriculum learning, which decomposes complex tasks into progressively harder sub-problems. Simulation results show that our framework achieves the most effective coordination of up to eight agents, where robots self-organize into asymmetric workload distributions that reduce congestion and modulate group performance. This emergent behavior, analogous to strategies observed in nature, demonstrates a scalable solution for decentralized multi-agent coordination in crowded environments with communication constraints.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Limits of Consensus under Dynamic Availability and Reconfiguration</title>
<link>https://arxiv.org/abs/2510.03625</link>
<guid>https://arxiv.org/abs/2510.03625</guid>
<content:encoded><![CDATA[
arXiv:2510.03625v1 Announce Type: new 
Abstract: Proof-of-stake blockchains require consensus protocols that support Dynamic Availability and Reconfiguration (so-called DAR setting), where the former means that the consensus protocol should remain live even if a large number of nodes temporarily crash, and the latter means it should be possible to change the set of operating nodes over time. State-of-the-art protocols for the DAR setting, such as Ethereum, Cardano's Ouroboros, or Snow White, require unrealistic additional assumptions, such as social consensus, or that key evolution is performed even while nodes are not participating. In this paper, we identify the necessary and sufficient adversarial condition under which consensus can be achieved in the DAR setting without additional assumptions. We then introduce a new and realistic additional assumption: honest nodes dispose of their cryptographic keys the moment they express intent to exit from the set of operating nodes. To add reconfiguration to any dynamically available consensus protocol, we provide a bootstrapping gadget that is particularly simple and efficient in the common optimistic case of few reconfigurations and no double-spending attempts.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Time-Bound Signature Scheme for Blockchains</title>
<link>https://arxiv.org/abs/2510.03697</link>
<guid>https://arxiv.org/abs/2510.03697</guid>
<content:encoded><![CDATA[
arXiv:2510.03697v1 Announce Type: new 
Abstract: We introduce a modified Schnorr signature scheme to allow for time-bound signatures for transaction fee auction bidding and smart contract purposes in a blockchain context, ensuring an honest producer can only validate a signature before a given block height. The immutable blockchain is used as a source of universal time for the signature scheme. We show the use of such a signature scheme leads to lower MEV revenue for builders. We then apply our time-bound signatures to Ethereum's EIP-1559 and show how it can be used to mitigate the effect of MEV on predicted equilibrium strategies.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Security Analysis of Ponzi Schemes in Ethereum Smart Contracts</title>
<link>https://arxiv.org/abs/2510.03819</link>
<guid>https://arxiv.org/abs/2510.03819</guid>
<content:encoded><![CDATA[
arXiv:2510.03819v1 Announce Type: new 
Abstract: The rapid advancement of blockchain technology has precipitated the widespread adoption of Ethereum and smart contracts across a variety of sectors. However, this has also given rise to numerous fraudulent activities, with many speculators embedding Ponzi schemes within smart contracts, resulting in significant financial losses for investors. Currently, there is a lack of effective methods for identifying and analyzing such new types of fraudulent activities. This paper categorizes these scams into four structural types and explores the intrinsic characteristics of Ponzi scheme contract source code from a program analysis perspective. The Mythril tool is employed to conduct static and dynamic analyses of representative cases, thereby revealing their vulnerabilities and operational mechanisms. Furthermore, this paper employs shell scripts and command patterns to conduct batch detection of open-source smart contract code, thereby unveiling the common characteristics of Ponzi scheme smart contracts.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Data Center Low-Voltage Ride-Through</title>
<link>https://arxiv.org/abs/2510.03867</link>
<guid>https://arxiv.org/abs/2510.03867</guid>
<content:encoded><![CDATA[
arXiv:2510.03867v1 Announce Type: new 
Abstract: Data center loads have expanded significantly in recent years. Compared to traditional loads, data centers are highly sensitive to voltage deviations and thus their protection mechanisms trip more proactively during voltage fluctuations. During a grid fault, simultaneous tripping of large-scale data centers can further destabilize the transmission system and even lead to cascading failures. In response, transmission system operators are imposing voltage ride-through (VRT) requirements for data centers. In this work, we enhance the VRT capability of data centers by designing voltage controllers for their internal power distribution network. We first systematically analyze VRT standards and the controllable resources related to data centers. These resources enable the design of voltage control strategies to regulate voltages internal to the data center, thereby allowing loads to remain online during voltage disturbances from the external transmission grid. We study and contrast both centralized and decentralized controllers that unify the control of heterogeneous flexible resources. Additionally, we construct an integrated test system that simulates both the transient fault response of the transmission system and the data center distribution network. Case studies demonstrate that the proposed voltage control mechanisms provide effective yet simple solutions to enhance data center low-voltage ride-through capability.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Does the Engineering Manager Still Exist in Agile Software Development?</title>
<link>https://arxiv.org/abs/2510.03920</link>
<guid>https://arxiv.org/abs/2510.03920</guid>
<content:encoded><![CDATA[
arXiv:2510.03920v1 Announce Type: new 
Abstract: Although Agile methodologies emphasize decentralized decision-making and team autonomy, engineering managers continue to be employed in Agile software organizations. This apparent paradox suggests that traditional managerial functions persist despite the theoretical displacement of managerial hierarchy in Agile. This paper explores the persistence of engineering managers through a multidimensional framework encompassing historical context, theoretical tensions, organizational realities, empirical evidence, evolving managerial roles, and practical implications. A systematic literature review underpins our multifaceted analysis, supplemented by illustrative case studies. We conclude by proposing a conceptual model that reconciles Agile principles with managerial necessity, offering guidance for practitioners, researchers, and tool designers. Implications for leadership development, tool integration, and future research are discussed.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cooperative Flexibility Exchange: Fair and Comfort-Aware Decentralized Resource Allocation</title>
<link>https://arxiv.org/abs/2510.04192</link>
<guid>https://arxiv.org/abs/2510.04192</guid>
<content:encoded><![CDATA[
arXiv:2510.04192v1 Announce Type: new 
Abstract: The growing electricity demand and increased use of smart appliances are placing new pressures on power grids, making efficient energy management more important than ever. The existing energy management systems often prioritize system efficiency (balanced energy demand and supply) at the expense of user comfort. This paper addresses this gap by proposing a novel decentralized multi-agent coordination-based demand-side management system. The proposed system enables individual agents to coordinate for demand-side energy optimization while improving the user comfort and maintaining the system efficiency. A key innovation of this work is the introduction of a slot exchange mechanism, where agents first receive optimized appliance-level energy consumption schedules and then coordinate with each other to adjust these schedules through slot exchanges. This approach improves user comfort even when agents show non-altruistic behaviour, and it scales well with large populations. The system also promotes fairness by balancing satisfaction levels across users. For performance evaluation, a real-world dataset is used, and the results demonstrate that the proposed slot exchange mechanism increases user comfort and fairness without raising system inefficiency cost, making it a practical and scalable solution for future smart grids.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedSRD: Sparsify-Reconstruct-Decompose for Communication-Efficient Federated Large Language Models Fine-Tuning</title>
<link>https://arxiv.org/abs/2510.04601</link>
<guid>https://arxiv.org/abs/2510.04601</guid>
<content:encoded><![CDATA[
arXiv:2510.04601v1 Announce Type: new 
Abstract: The current paradigm of training large language models (LLMs) on publicly available Web data is becoming unsustainable, with high-quality data sources in specialized domains nearing exhaustion. Federated Learning (FL) emerges as a practical solution for the next generation of AI on a decentralized Web, enabling privacy-preserving collaborative fine-tuning by leveraging private data distributed across a global client base. While Low-Rank Adaptation (LoRA) is the standard for efficient fine-tuning, its application in federated settings presents a critical challenge: communication overhead remains a significant bottleneck across the Web's heterogeneous network conditions. The structural redundancy within LoRA parameters not only incurs a heavy communication burden but also introduces conflicts when aggregating client updates. To address this, we propose FedSRD, a Sparsify-Reconstruct-Decompose framework designed for communication-efficient FL. We first introduce an importance-aware sparsification method that preserves the structural integrity of LoRA updates to reduce the uploaded parameter count. The server then reconstructs and aggregates these updates in a full-rank space to mitigate conflicts. Finally, it decomposes the global update into a sparse low-rank format for broadcast, ensuring a symmetrically efficient cycle. We also propose an efficient variant, FedSRD-e, to reduce computational overhead. Experimental results on 10 benchmarks demonstrate that our framework significantly reduces communication costs by up to 90\% while even improving model performance on heterogeneous client data.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PoS-CoPOR: Proof-of-Stake Consensus Protocol with Native Onion Routing Providing Scalability and DoS-Resistance</title>
<link>https://arxiv.org/abs/2510.04619</link>
<guid>https://arxiv.org/abs/2510.04619</guid>
<content:encoded><![CDATA[
arXiv:2510.04619v1 Announce Type: new 
Abstract: Proof-of-Stake (PoS) consensus protocols often face a trade-off between performance and security. Protocols that pre-elect leaders for subsequent rounds are vulnerable to Denial-of-Service (DoS) attacks, which can disrupt the network and compromise liveness. In this work, we present PoS-CoPOR, a single-chain PoS consensus protocol that mitigates this vulnerability by integrating a native onion routing mechanism into the consensus protocol itself. PoS-CoPOR combines stake-weighted probabilistic leader election with an anonymization layer that conceals the network identity of the next block proposer. This approach prevents targeted DoS attacks on leaders before they produce a block, thus enhancing network resilience. We implemented and evaluated PoS-CoPOR, demonstrating its ability to achieve a throughput of up to 110 tx/s with 6 nodes, even with the overhead of the anonymization layer. The results show that native anonymization can provide robust DoS resistance with only a modest impact on performance, offering a solution to build secure and scalable PoS blockchains.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impossible Cloud Network: A Decentralized Internet Infrastructure Layer</title>
<link>https://arxiv.org/abs/2510.04620</link>
<guid>https://arxiv.org/abs/2510.04620</guid>
<content:encoded><![CDATA[
arXiv:2510.04620v1 Announce Type: new 
Abstract: The internet faces a sovereignty crisis due to power concentration and data growth among a few hyperscalers, leading to centralization and loss of user control. This consolidation risks censorship and creates single points of failure. While Web3 offers decentralized solutions, they often sacrifice either scalability, decentralization, or security, which are key elements in the blockchain trilemma. These solutions also struggle with limited access to enterprise-grade hardware and frequently rely on centralized infrastructure. The Impossible Cloud Network (ICN) addresses these issues by creating a multi-tiered, decentralized infrastructure layer. ICN offers a composable service layer, an enterprise-grade hardware resource layer, and a transparent, permissionless HyperNode network for performance enforcement. By strategically decoupling and decentralizing each layer, ICN aims to provide an open, extensively scalable infrastructure that ensures digital sovereignty, eliminates single points of trust, enables service programmability, and offers a decoupled architecture for limitless possibilities in the future internet.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LMM-Incentive: Large Multimodal Model-based Incentive Design for User-Generated Content in Web 3.0</title>
<link>https://arxiv.org/abs/2510.04765</link>
<guid>https://arxiv.org/abs/2510.04765</guid>
<content:encoded><![CDATA[
arXiv:2510.04765v1 Announce Type: new 
Abstract: Web 3.0 represents the next generation of the Internet, which is widely recognized as a decentralized ecosystem that focuses on value expression and data ownership. By leveraging blockchain and artificial intelligence technologies, Web 3.0 offers unprecedented opportunities for users to create, own, and monetize their content, thereby enabling User-Generated Content (UGC) to an entirely new level. However, some self-interested users may exploit the limitations of content curation mechanisms and generate low-quality content with less effort, obtaining platform rewards under information asymmetry. Such behavior can undermine Web 3.0 performance. To this end, we propose \textit{LMM-Incentive}, a novel Large Multimodal Model (LMM)-based incentive mechanism for UGC in Web 3.0. Specifically, we propose an LMM-based contract-theoretic model to motivate users to generate high-quality UGC, thereby mitigating the adverse selection problem from information asymmetry. To alleviate potential moral hazards after contract selection, we leverage LMM agents to evaluate UGC quality, which is the primary component of the contract, utilizing prompt engineering techniques to improve the evaluation performance of LMM agents. Recognizing that traditional contract design methods cannot effectively adapt to the dynamic environment of Web 3.0, we develop an improved Mixture of Experts (MoE)-based Proximal Policy Optimization (PPO) algorithm for optimal contract design. Simulation results demonstrate the superiority of the proposed MoE-based PPO algorithm over representative benchmarks in the context of contract design. Finally, we deploy the designed contract within an Ethereum smart contract framework, further validating the effectiveness of the proposed scheme.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge</title>
<link>https://arxiv.org/abs/2510.04772</link>
<guid>https://arxiv.org/abs/2510.04772</guid>
<content:encoded><![CDATA[
arXiv:2510.04772v1 Announce Type: new 
Abstract: Purpose: The FedSurg challenge was designed to benchmark the state of the art in federated learning for surgical video classification. Its goal was to assess how well current methods generalize to unseen clinical centers and adapt through local fine-tuning while enabling collaborative model development without sharing patient data. Methods: Participants developed strategies to classify inflammation stages in appendicitis using a preliminary version of the multi-center Appendix300 video dataset. The challenge evaluated two tasks: generalization to an unseen center and center-specific adaptation after fine-tuning. Submitted approaches included foundation models with linear probing, metric learning with triplet loss, and various FL aggregation schemes (FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and Expected Cost, with ranking robustness evaluated via bootstrapping and statistical testing. Results: In the generalization task, performance across centers was limited. In the adaptation task, all teams improved after fine-tuning, though ranking stability was low. The ViViT-based submission achieved the strongest overall performance. The challenge highlighted limitations in generalization, sensitivity to class imbalance, and difficulties in hyperparameter tuning in decentralized training, while spatiotemporal modeling and context-aware preprocessing emerged as promising strategies. Conclusion: The FedSurg Challenge establishes the first benchmark for evaluating FL strategies in surgical video classification. Findings highlight the trade-off between local personalization and global robustness, and underscore the importance of architecture choice, preprocessing, and loss design. This benchmarking offers a reference point for future development of imbalance-aware, adaptive, and robust FL methods in clinical surgical AI.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Distributed Optimization With Feasible Set Privacy</title>
<link>https://arxiv.org/abs/2510.05068</link>
<guid>https://arxiv.org/abs/2510.05068</guid>
<content:encoded><![CDATA[
arXiv:2510.05068v1 Announce Type: new 
Abstract: We consider the problem of decentralized constrained optimization with multiple agents $E_1,\ldots,E_N$ who jointly wish to learn the optimal solution set while keeping their feasible sets $\mathcal{P}_1,\ldots,\mathcal{P}_N$ private from each other. We assume that the objective function $f$ is known to all agents and each feasible set is a collection of points from a universal alphabet $\mathcal{P}_{alph}$. A designated agent (leader) starts the communication with the remaining (non-leader) agents, and is the first to retrieve the solution set. The leader searches for the solution by sending queries to and receiving answers from the non-leaders, such that the information on the individual feasible sets revealed to the leader should be no more than nominal, i.e., what is revealed from learning the solution set alone. We develop achievable schemes for obtaining the solution set at nominal information leakage, and characterize their communication costs under two communication setups between agents. In this work, we focus on two kinds of network setups: i) ring, where each agent communicates with two adjacent agents, and ii) star, where only the leader communicates with the remaining agents. We show that, if the leader first learns the joint feasible set through an existing private set intersection (PSI) protocol and then deduces the solution set, the information leaked to the leader is greater than nominal. Moreover, we draw connection of our schemes to threshold PSI (ThPSI), which is a PSI-variant where the intersection is revealed only when its cardinality is larger than a threshold value. Finally, for various realizations of $f$ mapped uniformly at random to a fixed range of values, our schemes are more communication-efficient with a high probability compared to retrieving the entire feasible set through PSI.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position Paper: Assessing Robustness, Privacy, and Fairness in Federated Learning Integrated with Foundation Models</title>
<link>https://arxiv.org/abs/2402.01857</link>
<guid>https://arxiv.org/abs/2402.01857</guid>
<content:encoded><![CDATA[
arXiv:2402.01857v2 Announce Type: replace 
Abstract: Federated Learning (FL), while a breakthrough in decentralized machine learning, contends with significant challenges such as limited data availability and the variability of computational resources, which can stifle the performance and scalability of the models. The integration of Foundation Models (FMs) into FL presents a compelling solution to these issues, with the potential to enhance data richness and reduce computational demands through pre-training and data augmentation. However, this incorporation introduces novel issues in terms of robustness, privacy, and fairness, which have not been sufficiently addressed in the existing research. We make a preliminary investigation into this field by systematically evaluating the implications of FM-FL integration across these dimensions. We analyze the trade-offs involved, uncover the threats and issues introduced by this integration, and propose a set of criteria and strategies for navigating these challenges. Furthermore, we identify potential research directions for advancing this field, laying a foundation for future development in creating reliable, secure, and equitable FL systems.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proof-of-Data: A Consensus Protocol for Collaborative Intelligence</title>
<link>https://arxiv.org/abs/2501.02971</link>
<guid>https://arxiv.org/abs/2501.02971</guid>
<content:encoded><![CDATA[
arXiv:2501.02971v2 Announce Type: replace 
Abstract: Existing research on federated learning has been focused on the setting where learning is coordinated by a centralized entity. Yet the greatest potential of future collaborative intelligence would be unleashed in a more open and democratized setting with no central entity in a dominant role, referred to as "decentralized federated learning". New challenges arise accordingly in achieving both correct model training and fair reward allocation with collective effort among all participating nodes, especially with the threat of the Byzantine node jeopardising both tasks.
  In this paper, we propose a blockchain-based decentralized Byzantine fault-tolerant federated learning framework based on a novel Proof-of-Data (PoD) consensus protocol to resolve both the "trust" and "incentive" components. By decoupling model training and contribution accounting, PoD is able to enjoy not only the benefit of learning efficiency and system liveliness from asynchronous societal-scale PoW-style learning but also the finality of consensus and reward allocation from epoch-based BFT-style voting. To mitigate false reward claims by data forgery from Byzantine attacks, a privacy-aware data verification and contribution-based reward allocation mechanism is designed to complete the framework. Our evaluation results show that PoD demonstrates performance in model training close to that of the centralized counterpart while achieving trust in consensus and fairness for reward allocation with a fault tolerance ratio of 1/3.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exact and Linear Convergence for Federated Learning under Arbitrary Client Participation is Attainable</title>
<link>https://arxiv.org/abs/2503.20117</link>
<guid>https://arxiv.org/abs/2503.20117</guid>
<content:encoded><![CDATA[
arXiv:2503.20117v3 Announce Type: replace 
Abstract: This work tackles the fundamental challenges in Federated Learning (FL) posed by arbitrary client participation and data heterogeneity, prevalent characteristics in practical FL settings. It is well-established that popular FedAvg-style algorithms struggle with exact convergence and can suffer from slow convergence rates since a decaying learning rate is required to mitigate these scenarios. To address these issues, we introduce the concept of stochastic matrix and the corresponding time-varying graphs as a novel modeling tool to accurately capture the dynamics of arbitrary client participation and the local update procedure. Leveraging this approach, we offer a fresh decentralized perspective on designing FL algorithms and present FOCUS, Federated Optimization with Exact Convergence via Push-pull Strategy, a provably convergent algorithm designed to effectively overcome the previously mentioned two challenges. More specifically, we provide a rigorous proof demonstrating that FOCUS achieves exact convergence with a linear rate regardless of the arbitrary client participation, establishing it as the first work to demonstrate this significant result.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Spatiotemporal Graph Learning for Passive Attack Detection in Smart Grids</title>
<link>https://arxiv.org/abs/2510.02371</link>
<guid>https://arxiv.org/abs/2510.02371</guid>
<content:encoded><![CDATA[
arXiv:2510.02371v1 Announce Type: new 
Abstract: Smart grids are exposed to passive eavesdropping, where attackers listen silently to communication links. Although no data is actively altered, such reconnaissance can reveal grid topology, consumption patterns, and operational behavior, creating a gateway to more severe targeted attacks. Detecting this threat is difficult because the signals it produces are faint, short-lived, and often disappear when traffic is examined by a single node or along a single timeline. This paper introduces a graph-centric, multimodal detector that fuses physical-layer and behavioral indicators over ego-centric star subgraphs and short temporal windows to detect passive attacks. To capture stealthy perturbations, a two-stage encoder is introduced: graph convolution aggregates spatial context across ego-centric star subgraphs, while a bidirectional GRU models short-term temporal dependencies. The encoder transforms heterogeneous features into a unified spatio-temporal representation suitable for classification. Training occurs in a federated learning setup under FedProx, improving robustness to heterogeneous local raw data and contributing to the trustworthiness of decentralized training; raw measurements remain on client devices. A synthetic, standards-informed dataset is generated to emulate heterogeneous HAN/NAN/WAN communications with wireless-only passive perturbations, event co-occurrence, and leak-safe splits. The model achieves a testing accuracy of 98.32% per-timestep (F1_{attack}=0.972) and 93.35% per-sequence at 0.15% FPR using a simple decision rule with run-length m=2 and threshold $\tau=0.55$. The results demonstrate that combining spatial and temporal context enables reliable detection of stealthy reconnaissance while maintaining low false-positive rates, making the approach suitable for non-IID federated smart-grid deployments.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Modeling and Control System Analysis for Continuous-Disc Filters in Pulp Mill Operations</title>
<link>https://arxiv.org/abs/2510.02385</link>
<guid>https://arxiv.org/abs/2510.02385</guid>
<content:encoded><![CDATA[
arXiv:2510.02385v1 Announce Type: new 
Abstract: Vacuum disc filtration is critical in pulp mills for white liquor clarification and pulp washing, involving tightly coupled dynamics between rotational speed, vacuum pressure, slurry concentration, filtrate flow, and cake thickness. These nonlinear interactions are often regulated using empirical methods, lacking formal modeling and control. This article develops a dynamic, multivariable model of a continuous-disc filter (CD-filter) system based on first principles, simplified to a single representative disc for tractability. A linearized state-space model supports the design of two control strategies: a decentralized PI-based scheme and a centralized model predictive control (MPC). MATLAB-Simulink simulations reveal that MPC outperforms PI in tracking accuracy, overshoot reduction, and disturbance rejection. A 3D efficiency surface illustrates the importance of coordinating inlet flow and solids concentration. Results highlight the need for advanced multivariable control in optimizing CD-filter performance.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PolyLink: A Blockchain Based Decentralized Edge AI Platform for LLM Inference</title>
<link>https://arxiv.org/abs/2510.02395</link>
<guid>https://arxiv.org/abs/2510.02395</guid>
<content:encoded><![CDATA[
arXiv:2510.02395v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) in recent years has revolutionized the AI landscape. However, the deployment model and usage of LLM services remain highly centralized, creating significant trust issues and costs for end users and developers. To address these issues, we propose PolyLink, a blockchain-based decentralized AI platform that decentralizes LLM development and inference. Specifically, PolyLink introduces a decentralized crowdsourcing architecture that supports single-device and cross-device model deployment and inference across heterogeneous devices at the edge. Moreover, to ensure the inference integrity, we design the TIQE protocol, which combines a lightweight cross-encoder model and an LLM-as-a-Judge for a high-accuracy inference evaluation. Lastly, we integrate a comprehensive token-based incentive model with dynamic pricing and reward mechanisms for all participants. We have deployed PolyLink and conducted an extensive real-world evaluation through geo-distributed deployment across heterogeneous devices. Results indicate that the inference and verification latency is practical. Our security analysis demonstrates that the system is resistant to model degradation attacks and validator corruptions. PolyLink is now available at https://github.com/IMCL-PolyLink/PolyLink.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoK: Preconfirmations</title>
<link>https://arxiv.org/abs/2510.02947</link>
<guid>https://arxiv.org/abs/2510.02947</guid>
<content:encoded><![CDATA[
arXiv:2510.02947v1 Announce Type: new 
Abstract: In recent years, significant research efforts have focused on improving blockchain throughput and confirmation speeds without compromising security. While decreasing the time it takes for a transaction to be included in the blockchain ledger enhances user experience, a fundamental delay still remains between when a transaction is issued by a user and when its inclusion is confirmed in the blockchain ledger. This delay limits user experience gains through the confirmation uncertainty it brings for users. This inherent delay in conventional blockchain protocols has led to the emergence of preconfirmation protocols -- protocols that provide users with early guarantees of eventual transaction confirmation.
  This article presents a Systematization of Knowledge (SoK) on preconfirmations. We present the core terms and definitions needed to understand preconfirmations, outline a general framework for preconfirmation protocols, and explore the economics and risks of preconfirmations. Finally, we survey and apply our framework to several implementations of real-world preconfirmation protocols, bridging the gap between theory and practice.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pioneering Scalable Prototyping for Mid-Band XL-MIMO Systems: Design and Implementation</title>
<link>https://arxiv.org/abs/2510.02793</link>
<guid>https://arxiv.org/abs/2510.02793</guid>
<content:encoded><![CDATA[
arXiv:2510.02793v1 Announce Type: cross 
Abstract: The mid-band frequency range, combined with extra large-scale multiple-input multiple-output (XL-MIMO), is emerging as a key enabler for future communication systems. Thanks to the advent of new spectrum resources and degrees of freedom brought by the near-field propagation, the mid-band XL-MIMO system is expected to significantly enhance throughput and inherently support advanced functionalities such as integrated sensing and communication. Although theoretical studies have highlighted the benefits of mid-band XL-MIMO systems, the promised performance gains have yet to be validated in practical systems, posing a major challenge to the standardization. In this paper, preliminaries are first discussed, followed by an analysis of key challenges in constructing a real-time prototype system. Subsequently, the design and implementation of a real-time mid-band XL-MIMO prototype system are presented. Benefiting from the novel architecture, the proposed prototype system supports metrics aligned with standardization, including a bandwidth of 200 MHz, up to 1024 antenna elements, and up to 256 transceiver chains. Operating in time-division duplexing (TDD) mode, the prototype enables multiuser communication with support for up to 12 users, while retaining standard communication procedures. Built on software-defined radio (SDR) platforms, the system is programmable and allows for flexible deployment of advanced algorithms. Moreover, the modular architecture ensures high scalability, making the system adaptable to various configurations, including distributed deployments and decentralized signal processing. Experimental results with the proposed prototype system demonstrate real-time digital sample processing at 1167.85 Gbps, a peak data throughput of 15.81 Gbps for 12 users, and a maximal spectral efficiency approaching 80 bit/s/Hz.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proxima. A DAG based cooperative distributed ledger</title>
<link>https://arxiv.org/abs/2411.16456</link>
<guid>https://arxiv.org/abs/2411.16456</guid>
<content:encoded><![CDATA[
arXiv:2411.16456v5 Announce Type: replace 
Abstract: This paper introduces a novel architecture for a distributed ledger, commonly referred to as a "blockchain", which is organized in the form of directed acyclic graph (DAG) with UTXO transactions as vertices, rather than as a chain of blocks. Consensus on the state of ledger assets is achieved through the cooperative consensus: a profit-driven behavior of token holders themselves, which is viable only when they cooperate by following the "biggest ledger coverage rule", akin the "longest chain rule" of Bitcoin. The cooperative behavior is facilitated by enforcing purposefully designed UTXO transaction validity constraints. Token holders are the sole category of participants authorized to make amendments to the ledger, making participation completely permissionless - without miners, validators, committees or staking - and without any need of knowledge about the composition of the set of all participants in the consensus. The setup allows to achieve high throughput and scalability alongside with low transaction costs, while preserving key aspects of high decentralization, open participation, and asynchronicity found in Bitcoin and other proof-of-work blockchains, but without huge energy consumption. Sybil protection is achieved similarly to proof-of-stake blockchains, using tokens native to the ledger, yet the architecture operates in a leaderless manner without block proposers and committee selection.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ColNet: Collaborative Optimization in Decentralized Federated Multi-task Learning Systems</title>
<link>https://arxiv.org/abs/2501.10347</link>
<guid>https://arxiv.org/abs/2501.10347</guid>
<content:encoded><![CDATA[
arXiv:2501.10347v2 Announce Type: replace 
Abstract: The integration of Federated Learning (FL) and Multi-Task Learning (MTL) has been explored to address client heterogeneity, with Federated Multi-Task Learning (FMTL) treating each client as a distinct task. However, most existing research focuses on data heterogeneity (e.g., addressing non-IID data) rather than task heterogeneity, where clients solve fundamentally different tasks. Additionally, much of the work relies on centralized settings with a server managing the federation, leaving the more challenging domain of decentralized FMTL largely unexplored. Thus, this work bridges this gap by proposing ColNet, a framework designed for heterogeneous tasks in decentralized federated environments.
  ColNet partitions models into a backbone and task-specific heads, and uses adaptive clustering based on model and data sensitivity to form task-coherent client groups. Backbones are averaged within groups, and group leaders perform hyper-conflict-averse cross-group aggregation. Across datasets and federations, ColNet outperforms competing schemes under label and task heterogeneity and shows robustness to poisoning attacks.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinP: Fairness-in-Privacy in Federated Learning by Addressing Disparities in Privacy Risk</title>
<link>https://arxiv.org/abs/2502.17748</link>
<guid>https://arxiv.org/abs/2502.17748</guid>
<content:encoded><![CDATA[
arXiv:2502.17748v2 Announce Type: replace 
Abstract: Ensuring fairness in machine learning extends to the critical dimension of privacy, particularly in human-centric federated learning (FL) settings where decentralized data necessitates an equitable distribution of privacy risk across clients. This paper introduces FinP, a novel framework specifically designed to address disparities in privacy risk by mitigating disproportionate vulnerability to source inference attacks (SIA). FinP employs a two-pronged strategy: (1) server-side adaptive aggregation, which dynamically adjusts client contributions to the global model to foster fairness, and (2) client-side regularization, which enhances the privacy robustness of individual clients. This comprehensive approach directly tackles both the symptoms and underlying causes of privacy unfairness in FL. Extensive evaluations on the Human Activity Recognition (HAR) and CIFAR-10 datasets demonstrate FinP's effectiveness, achieving improvement in fairness-in-privacy on HAR and CIFAR-10 with minimal impact on utility. FinP improved group fairness with respect to disparity in privacy risk using equal opportunity in CIFAR-10 by 57.14% compared to the state-of-the-art. Furthermore, FinP significantly mitigates SIA risks on CIFAR-10, underscoring its potential to establish fairness in privacy within FL systems without compromising utility.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Odontoceti: Ultra-Fast DAG Consensus with Two Round Commitment</title>
<link>https://arxiv.org/abs/2510.01216</link>
<guid>https://arxiv.org/abs/2510.01216</guid>
<content:encoded><![CDATA[
arXiv:2510.01216v1 Announce Type: new 
Abstract: Users of blockchains value scalability, expecting fast confirmations and immediate transaction processing. Odontoceti, the latest in DAG-based consensus, addresses these concerns by prioritizing low latency and high throughput, making a strategic trade-off in security by operating with a 20% fault tolerance instead of the established 33% level. It is the first DAG-based protocol to achieve commitment in just two communication rounds, delivering median latency of 300 milliseconds while processing 10,000 transactions per second under realistic network conditions. Odontoceti operates with n = 5f + 1 validators and creates an uncertified DAG with a novel decision rule for committing blocks. The protocol includes an optimization that advances progress when participants are slow, benefiting crash fault scenarios which are more common in practice than Byzantine faults. Evaluation results demonstrate 20-25% latency improvements compared to an existing production protocol, validating that reducing wave length from three rounds to two rounds yields meaningful performance benefits. This paper establishes the practical viability of lower fault tolerance consensus protocols for blockchains.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Securing IoT Devices in Smart Cities: A Review of Proposed Solutions</title>
<link>https://arxiv.org/abs/2510.01445</link>
<guid>https://arxiv.org/abs/2510.01445</guid>
<content:encoded><![CDATA[
arXiv:2510.01445v1 Announce Type: new 
Abstract: Privacy and security in Smart Cities remain at constant risk due to the vulnerabilities introduced by Internet of Things (IoT) devices. The limited computational resources of these devices make them especially susceptible to attacks, while their widespread adoption increases the potential impact of security breaches. This article presents a review of security proposals aimed at protecting IoT devices in Smart City environments. The review was conducted by analyzing recent literature on device-level security, with particular emphasis on lightweight cryptography, physically unclonable functions (PUFs), and blockchain-based solutions. Findings highlight both the strengths and limitations of current approaches, as well as the need for more practical, scalable, and resource-efficient mechanisms to ensure user privacy and data protection in IoT ecosystems.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FOSS-chain: using blockchain for Open Source Software license compliance</title>
<link>https://arxiv.org/abs/2510.01740</link>
<guid>https://arxiv.org/abs/2510.01740</guid>
<content:encoded><![CDATA[
arXiv:2510.01740v1 Announce Type: new 
Abstract: Open Source Software (OSS) is widely used and carries licenses that indicate the terms under which the software is provided for use, also specifying modification and distribution rules. Ensuring that users are respecting OSS license terms when creating derivative works is a complex process. Compliance issues arising from incompatibilities among licenses may lead to legal disputes. At the same time, the blockchain technology with immutable entries offers a mechanism to provide transparency when it comes to licensing and ensure software changes are recorded. In this work, we are introducing an integration of blockchain and license management when creating derivative works, in order to tackle the issue of OSS license compatibility. We have designed, implemented and performed a preliminary evaluation of FOSS-chain, a web platform that uses blockchain and automates the license compliance process, covering 14 OSS licenses. We have evaluated the initial prototype version of the FOSS-chain platform via a small scale user study. Our preliminary results are promising, demonstrating the potential of the platform for adaptation on realistic software systems.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeMuon: A Decentralized Muon for Matrix Optimization over Graphs</title>
<link>https://arxiv.org/abs/2510.01377</link>
<guid>https://arxiv.org/abs/2510.01377</guid>
<content:encoded><![CDATA[
arXiv:2510.01377v1 Announce Type: cross 
Abstract: In this paper, we propose DeMuon, a method for decentralized matrix optimization over a given communication topology. DeMuon incorporates matrix orthogonalization via Newton-Schulz iterations-a technique inherited from its centralized predecessor, Muon-and employs gradient tracking to mitigate heterogeneity among local functions. Under heavy-tailed noise conditions and additional mild assumptions, we establish the iteration complexity of DeMuon for reaching an approximate stochastic stationary point. This complexity result matches the best-known complexity bounds of centralized algorithms in terms of dependence on the target tolerance. To the best of our knowledge, DeMuon is the first direct extension of Muon to decentralized optimization over graphs with provable complexity guarantees. We conduct preliminary numerical experiments on decentralized transformer pretraining over graphs with varying degrees of connectivity. Our numerical results demonstrate a clear margin of improvement of DeMuon over other popular decentralized algorithms across different network topologies.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LPAC: Learnable Perception-Action-Communication Loops with Applications to Coverage Control</title>
<link>https://arxiv.org/abs/2401.04855</link>
<guid>https://arxiv.org/abs/2401.04855</guid>
<content:encoded><![CDATA[
arXiv:2401.04855v4 Announce Type: replace 
Abstract: Coverage control is the problem of navigating a robot swarm to collaboratively monitor features or a phenomenon of interest not known a priori. The problem is challenging in decentralized settings with robots that have limited communication and sensing capabilities. We propose a learnable Perception-Action-Communication (LPAC) architecture for the problem, wherein a convolutional neural network (CNN) processes localized perception; a graph neural network (GNN) facilitates robot communications; finally, a shallow multi-layer perceptron (MLP) computes robot actions. The GNN enables collaboration in the robot swarm by computing what information to communicate with nearby robots and how to incorporate received information. Evaluations show that the LPAC models -- trained using imitation learning -- outperform standard decentralized and centralized coverage control algorithms. The learned policy generalizes to environments different from the training dataset, transfers to larger environments with more robots, and is robust to noisy position estimates. The results indicate the suitability of LPAC architectures for decentralized navigation in robot swarms to achieve collaborative behavior.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DFPL: Decentralized Federated Prototype Learning Across Heterogeneous Data Distributions</title>
<link>https://arxiv.org/abs/2505.04947</link>
<guid>https://arxiv.org/abs/2505.04947</guid>
<content:encoded><![CDATA[
arXiv:2505.04947v4 Announce Type: replace 
Abstract: Federated learning is a distributed machine learning paradigm through centralized model aggregation. However, standard federated learning relies on a centralized server, making it vulnerable to server failures. While existing solutions utilize blockchain technology to implement Decentralized Federated Learning (DFL), the statistical heterogeneity of data distributions among clients severely degrades the performance of DFL. Driven by this issue, this paper proposes a decentralized federated prototype learning framework, named DFPL, which significantly improves the performance of DFL under heterogeneous data distributions. Specifically, DFPL introduces prototype learning into DFL to mitigate the impact of statistical heterogeneity and reduces the amount of parameters exchanged between clients. Additionally, blockchain is embedded into our framework, enabling the training and mining processes to be executed locally on each client. From a theoretical perspective, we analyze the convergence of DFPL by modeling the required computational resources during both training and mining. The experiment results highlight the superiority of DFPL in both model performance and communication efficiency across four benchmark datasets with heterogeneous data distributions.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Cybersecurity Architecture for Digital Product Ecosystems Using Agentic AI</title>
<link>https://arxiv.org/abs/2509.20640</link>
<guid>https://arxiv.org/abs/2509.20640</guid>
<content:encoded><![CDATA[
arXiv:2509.20640v2 Announce Type: replace 
Abstract: Traditional static cybersecurity models often struggle with scalability, real-time detection, and contextual responsiveness in the current digital product ecosystems which include cloud services, application programming interfaces (APIs), mobile platforms, and edge devices. This study introduces autonomous goal driven agents capable of dynamic learning and context-aware decision making as part of an adaptive cybersecurity architecture driven by agentic artificial intelligence (AI). To facilitate autonomous threat mitigation, proactive policy enforcement, and real-time anomaly detection, this framework integrates agentic AI across the key ecosystem layers. Behavioral baselining, decentralized risk scoring, and federated threat intelligence sharing are important features. The capacity of the system to identify zero-day attacks and dynamically modify access policies was demonstrated through native cloud simulations. The evaluation results show increased adaptability, decreased response latency, and improved detection accuracy. The architecture provides an intelligent and scalable blueprint for safeguarding complex digital infrastructure and is compatible with zero-trust models, thereby supporting the adherence to international cybersecurity regulations.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdGT: Decentralized Gradient Tracking with Tuning-free Per-Agent Stepsize</title>
<link>https://arxiv.org/abs/2504.15196</link>
<guid>https://arxiv.org/abs/2504.15196</guid>
<content:encoded><![CDATA[
arXiv:2504.15196v2 Announce Type: replace-cross 
Abstract: In decentralized optimization, the choice of stepsize plays a critical role in algorithm performance. A common approach is to use a shared stepsize across all agents to ensure convergence. However, selecting an optimal stepsize often requires careful tuning, which can be time-consuming and may lead to slow convergence, especially when there is significant variation in the smoothness (L-smoothness) of local objective functions across agents. Individually tuning stepsizes per agent is also impractical, particularly in large-scale networks. To address these limitations, we propose AdGT, an adaptive gradient tracking method that enables each agent to adjust its stepsize based on the smoothness of its local objective. We prove that AdGT achieves linear convergence to the global optimal solution. Through numerical experiments, we compare AdGT with fixed-stepsize gradient tracking methods and demonstrate its superior performance. Additionally, we compare AdGT with adaptive gradient descent (AdGD) in a centralized setting and observe that fully adaptive stepsizes offer greater benefits in decentralized networks than in centralized ones.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Lead Themselves: Agentic AI in MAS using MARL</title>
<link>https://arxiv.org/abs/2510.00022</link>
<guid>https://arxiv.org/abs/2510.00022</guid>
<content:encoded><![CDATA[
arXiv:2510.00022v1 Announce Type: new 
Abstract: As autonomous systems move from prototypes to real deployments, the ability of multiple agents to make decentralized, cooperative decisions becomes a core requirement. This paper examines how agentic artificial intelligence, agents that act independently, adaptively and proactively can improve task allocation and coordination in multi-agent systems, with primary emphasis on drone delivery and secondary relevance to warehouse automation. We formulate the problem in a cooperative multi-agent reinforcement learning setting and implement a lightweight multi-agent Proximal Policy Optimization, called IPPO, approach in PyTorch under a centralized-training, decentralized-execution paradigm. Experiments are conducted in PettingZoo environment, where multiple homogeneous drones or agents must self-organize to cover distinct targets without explicit communication.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calyx: Privacy-Preserving Multi-Token Optimistic-Rollup Protocol</title>
<link>https://arxiv.org/abs/2510.00164</link>
<guid>https://arxiv.org/abs/2510.00164</guid>
<content:encoded><![CDATA[
arXiv:2510.00164v1 Announce Type: new 
Abstract: Rollup protocols have recently received significant attention as a promising class of Layer 2 (L2) scalability solutions. By utilizing the Layer 1 (L1) blockchain solely as a bulletin board for a summary of the executed transactions and state changes, rollups enable secure off-chain execution while avoiding the complexity of other L2 mechanisms. However, to ensure data availability, current rollup protocols require the plaintext of executed transactions to be published on-chain, resulting in inherent privacy limitations.
  In this paper, we address this problem by introducing Calyx, the first privacy-preserving multi-token optimistic-Rollup protocol. Calyx guarantees full payment privacy for all L2 transactions, revealing no information about the sender, recipient, transferred amount, or token type. The protocol further supports atomic execution of multiple multi-token transactions and introduces a transaction fee scheme to enable broader application scenarios while ensuring the sustainable operation of the protocol. To enforce correctness, Calyx adopts an efficient one-step fraud-proof mechanism. We analyze the security and privacy guarantees of the protocol and provide an implementation and evaluation. Our results show that executing a single transaction costs approximately $0.06 (0.00002 ETH) and incurs only constant-size on-chain cost in asymptotic terms.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lattica: A Decentralized Cross-NAT Communication Framework for Scalable AI Inference and Training</title>
<link>https://arxiv.org/abs/2510.00183</link>
<guid>https://arxiv.org/abs/2510.00183</guid>
<content:encoded><![CDATA[
arXiv:2510.00183v1 Announce Type: new 
Abstract: The rapid expansion of distributed Artificial Intelligence (AI) workloads beyond centralized data centers creates a demand for new communication substrates. These substrates must operate reliably in heterogeneous and permissionless environments, where Network Address Translators (NATs) and firewalls impose significant constraints. Existing solutions, however, are either designed for controlled data center deployments or implemented as monolithic systems that tightly couple machine learning logic with networking code. To address these limitations, we present Lattica, a decentralized cross-NAT communication framework designed to support distributed AI systems. Lattica integrates three core components. First, it employs a robust suite of NAT traversal mechanisms to establish a globally addressable peer-to-peer mesh. Second, it provides a decentralized data store based on Conflict-free Replicated Data Types (CRDTs), ensuring verifiable and eventually consistent state replication. Third, it incorporates a content discovery layer that leverages distributed hash tables (DHTs) together with an optimized RPC protocol for efficient model synchronization. By integrating these components, Lattica delivers a complete protocol stack for sovereign, resilient, and scalable AI systems that operate independently of centralized intermediaries. It is directly applicable to edge intelligence, collaborative reinforcement learning, and other large-scale distributed machine learning scenarios.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BlockSDN-VC: A SDN-Based Virtual Coordinate-Enhanced Transaction Broadcast Framework for High-Performance Blockchains</title>
<link>https://arxiv.org/abs/2510.00306</link>
<guid>https://arxiv.org/abs/2510.00306</guid>
<content:encoded><![CDATA[
arXiv:2510.00306v1 Announce Type: new 
Abstract: Modern blockchains need fast, reliable propagation to balance security and throughput. Virtual-coordinate methods speed dissemination but rely on slow iterative updates, leaving nodes out of sync. We present BlockSDN-VC, a transaction-broadcast protocol that centralises coordinate computation and forwarding control in an SDN controller, delivering global consistency, minimal path stretch and rapid response to churn or congestion. In geo-distributed simulations, BlockSDN-VC cuts median latency by up to 62% and accelerates convergence fourfold over state-of-the-art schemes with under 3% control-plane overhead. In a real blockchain environment, BlockSDN-VC boosts confirmed-transaction throughput by 17% under adversarial workloads, requiring no modifications to existing clients.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow of Knowledge: Federated Fine-Tuning of LLMs in Healthcare under Non-IID Conditions</title>
<link>https://arxiv.org/abs/2510.00543</link>
<guid>https://arxiv.org/abs/2510.00543</guid>
<content:encoded><![CDATA[
arXiv:2510.00543v1 Announce Type: new 
Abstract: Large language models (LLMs) show great promise in healthcare, but their applications are hindered by data privacy restrictions and the challenges of cross-institution collaboration. Sensitive medical data cannot be centralized, while non-independent and identically distributed (non-IID) characteristics across institutions further complicate convergence and fairness. To address these issues, we present a federated fine-tuning approach based on Low-Rank Adaptation (LoRA), enabling privacy-preserving knowledge flow across institutions. The method iteratively combines local LoRA adaptation with global parameter aggregation, allowing efficient knowledge sharing without exposing raw data. A blockchain identity scheme is used for identifying individual LLM in such a distributed network. We evaluate this approach on heterogeneous and highly non-IID medical text datasets, where experiments demonstrate that federated LoRA not only enhances cross-client generalization but also improves the performance of the weakest client, achieving stable convergence and fairer outcomes. These findings highlight federated LoRA fine-tuning as a practical and effective paradigm for adapting LLMs in healthcare, offering a new path for multi-center medical AI collaboration.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized and Self-adaptive Core Maintenance on Temporal Graphs</title>
<link>https://arxiv.org/abs/2510.00758</link>
<guid>https://arxiv.org/abs/2510.00758</guid>
<content:encoded><![CDATA[
arXiv:2510.00758v1 Announce Type: new 
Abstract: Key graph-based problems play a central role in understanding network topology and uncovering patterns of similarity in homogeneous and temporal data. Such patterns can be revealed by analyzing communities formed by nodes, which in turn can be effectively modeled through temporal $k$-cores. This paper introduces a novel decentralized and incremental algorithm for computing the core decomposition of temporal networks. Decentralized solutions leverage the ability of network nodes to communicate and coordinate locally, addressing complex problems in a scalable, adaptive, and timely manner. By leveraging previously computed coreness values, our approach significantly reduces the activation of nodes and the volume of message exchanges when the network changes over time. This enables scalability with only a minimal trade-off in precision. Experimental evaluations on large real-world networks under varying levels of dynamism demonstrate the efficiency of our solution compared to a state-of-the-art approach, particularly in terms of active nodes, communication overhead, and convergence speed.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Verifiable Federated Unlearning: Framework, Challenges, and The Road Ahead</title>
<link>https://arxiv.org/abs/2510.00833</link>
<guid>https://arxiv.org/abs/2510.00833</guid>
<content:encoded><![CDATA[
arXiv:2510.00833v1 Announce Type: new 
Abstract: Federated unlearning (FUL) enables removing the data influence from the model trained across distributed clients, upholding the right to be forgotten as mandated by privacy regulations. FUL facilitates a value exchange where clients gain privacy-preserving control over their data contributions, while service providers leverage decentralized computing and data freshness. However, this entire proposition is undermined because clients have no reliable way to verify that their data influence has been provably removed, as current metrics and simple notifications offer insufficient assurance. We envision unlearning verification becoming a pivotal and trust-by-design part of the FUL life-cycle development, essential for highly regulated and data-sensitive services and applications like healthcare. This article introduces veriFUL, a reference framework for verifiable FUL that formalizes verification entities, goals, approaches, and metrics. Specifically, we consolidate existing efforts and contribute new insights, concepts, and metrics to this domain. Finally, we highlight research challenges and identify potential applications and developments for verifiable FUL and veriFUL.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structuring Automotive Data for Systems Engineering: A Taxonomy-Based Approach</title>
<link>https://arxiv.org/abs/2510.00963</link>
<guid>https://arxiv.org/abs/2510.00963</guid>
<content:encoded><![CDATA[
arXiv:2510.00963v1 Announce Type: new 
Abstract: Vehicle data is essential for advancing data-driven development throughout the automotive lifecycle, including requirements engineering, design, verification, and validation, and post-deployment optimization. Developers currently collect data in a decentralized and fragmented manner across simulations, test benches, and real-world driving, resulting in data silos, inconsistent formats, and limited interoperability. This leads to redundant efforts, inefficient integration, and suboptimal use of data. This fragmentation results in data silos, inconsistent storage structures, and limited interoperability, leading to redundant data collection, inefficient integration, and suboptimal application. To address these challenges, this article presents a structured literature review and develops an inductive taxonomy for automotive data. This taxonomy categorizes data according to its sources and applications, improving data accessibility and utilization. The analysis reveals a growing emphasis on real-world driving and machine learning applications while highlighting a critical gap in data availability for requirements engineering. By providing a systematic framework for structuring automotive data, this research contributes to more efficient data management and improved decision-making in the automotive industry.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universally Composable Termination Analysis of Tendermint</title>
<link>https://arxiv.org/abs/2510.01097</link>
<guid>https://arxiv.org/abs/2510.01097</guid>
<content:encoded><![CDATA[
arXiv:2510.01097v1 Announce Type: new 
Abstract: Modern blockchain systems operating in adversarial environments require robust consensus protocols that guarantee both safety and termination under network delay attacks. Tendermint, a widely adopted consensus protocol in consortium blockchains, achieves high throughput and finality. However, previous analysis of the safety and termination has been done in a standalone fashion, with no consideration of the composition with other protocols interacting with it in a concurrent manner. Moreover, the termination properties under adaptive network delays caused by Byzantine adversaries have not been formally analyzed. This paper presents the first universally composable (UC) security analysis of Tendermint, demonstrating its resilience against strategic message-delay attacks. By constructing a UC ideal model of Tendermint, we formalize its core mechanisms: phase-base consensus procedure, dynamic timeouts, proposal locking, leader rotation, and others, under a network adversary that selectively delays protocol messages. Our main result proves that the Tendermint protocol UC-realizes the ideal Tendermint model, which ensures bounded termination latency, i.e., guaranteed termination, even when up to $f<n/3$ nodes are Byzantine (where $n$ is the number of nodes participating in the consensus), provided that network delays remain within a protocol-defined threshold under the partially synchronous net assumption. Specifically, through formal proofs within the UC framework, we show that Tendermint maintains safety and termination. By the composition theorem of UC, this guarantees that these properties are maintained when Tendermint is composed with various blockchain components.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy Preserved Federated Learning with Attention-Based Aggregation for Biometric Recognition</title>
<link>https://arxiv.org/abs/2510.01113</link>
<guid>https://arxiv.org/abs/2510.01113</guid>
<content:encoded><![CDATA[
arXiv:2510.01113v1 Announce Type: new 
Abstract: Because biometric data is sensitive, centralized training poses a privacy risk, even though biometric recognition is essential for contemporary applications. Federated learning (FL), which permits decentralized training, provides a privacy-preserving substitute. Conventional FL, however, has trouble with interpretability and heterogeneous data (non-IID). In order to handle non-IID biometric data, this framework adds an attention mechanism at the central server that weights local model updates according to their significance. Differential privacy and secure update protocols safeguard data while preserving accuracy. The A3-FL framework is evaluated in this study using FVC2004 fingerprint data, with each client's features extracted using a Siamese Convolutional Neural Network (Siamese-CNN). By dynamically modifying client contributions, the attention mechanism increases the accuracy of the global model.The accuracy, convergence speed, and robustness of the A3-FL framework are superior to those of standard FL (FedAvg) and static baselines, according to experimental evaluations using fingerprint data (FVC2004). The accuracy of the attention-based approach was 0.8413, while FedAvg, Local-only, and Centralized approaches were 0.8164, 0.7664, and 0.7997, respectively. Accuracy stayed high at 0.8330 even with differential privacy. A scalable and privacy-sensitive biometric system for secure and effective recognition in dispersed environments is presented in this work.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Networked Control and Mean Field Problems Under Diagonal Dominance: Decentralized and Social Optimality</title>
<link>https://arxiv.org/abs/2510.01067</link>
<guid>https://arxiv.org/abs/2510.01067</guid>
<content:encoded><![CDATA[
arXiv:2510.01067v1 Announce Type: cross 
Abstract: In this article, we employ an input-output approach to expand the study of cooperative multi-agent control and optimization problems characterized by mean-field interactions that admit decentralized and selfish solutions. The setting involves $n$ independent agents that interact solely through a shared cost function, which penalizes deviations of each agent from the group's average collective behavior. Building on our earlier results established for homogeneous agents, we extend the framework to nonidentical agents and show that, under a diagonal dominant interaction of the collective dynamics, with bounded local open-loop dynamics, the optimal controller for $H_\infty$ and $H_2$ norm minimization remains decentralized and selfish in the limit as the number of agents $n$ grows to infinity.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XRZoo: A Large-Scale and Versatile Dataset of Extended Reality (XR) Applications</title>
<link>https://arxiv.org/abs/2412.06759</link>
<guid>https://arxiv.org/abs/2412.06759</guid>
<content:encoded><![CDATA[
arXiv:2412.06759v3 Announce Type: replace 
Abstract: The rapid advancement of Extended Reality (XR, encompassing AR, MR, and VR) and spatial computing technologies forms a foundational layer for the emerging Metaverse, enabling innovative applications across healthcare, education, manufacturing, and entertainment. However, research in this area is often limited by the lack of large, representative, and highquality application datasets that can support empirical studies and the development of new approaches benefiting XR software processes. In this paper, we introduce XRZoo, a comprehensive and curated dataset of XR applications designed to bridge this gap. XRZoo contains 12,528 free XR applications, spanning nine app stores, across all XR techniques (i.e., AR, MR, and VR) and use cases, with detailed metadata on key aspects such as application descriptions, application categories, release dates, user review numbers, and hardware specifications, etc. By making XRZoo publicly available, we aim to foster reproducible XR software engineering and security research, enable cross-disciplinary investigations, and also support the development of advanced XR systems by providing examples to developers. Our dataset serves as a valuable resource for researchers and practitioners interested in improving the scalability, usability, and effectiveness of XR applications. XRZoo will be released and actively maintained.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Domain Shift in Federated Learning via Intra- and Inter-Domain Prototypes</title>
<link>https://arxiv.org/abs/2501.08521</link>
<guid>https://arxiv.org/abs/2501.08521</guid>
<content:encoded><![CDATA[
arXiv:2501.08521v3 Announce Type: replace 
Abstract: Federated Learning (FL) has emerged as a decentralized machine learning technique, allowing clients to train a global model collaboratively without sharing private data. However, most FL studies ignore the crucial challenge of heterogeneous domains where each client has a distinct feature distribution, which is popular in real-world scenarios. Prototype learning, which leverages the mean feature vectors within the same classes, has become a prominent solution for federated learning under domain shift. However, existing federated prototype learning methods focus soley on inter-domain prototypes and neglect intra-domain perspectives. In this work, we introduce a novel federated prototype learning method, namely I$^2$PFL, which incorporates $\textbf{I}$ntra-domain and $\textbf{I}$nter-domain $\textbf{P}$rototypes, to mitigate domain shift from both perspectives and learn a generalized global model across multiple domains in federated learning. To construct intra-domain prototypes, we propose feature alignment with MixUp-based augmented prototypes to capture the diversity within local domains and enhance the generalization of local features. Additionally, we introduce a reweighting mechanism for inter-domain prototypes to generate generalized prototypes that reduce domain shift while providing inter-domain knowledge across multiple clients. Extensive experiments on the Digits, Office-10, and PACS datasets illustrate the superior performance of our method compared to other baselines.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CYCle: Choosing Your Collaborators Wisely to Enhance Collaborative Fairness in Decentralized Learning</title>
<link>https://arxiv.org/abs/2501.12344</link>
<guid>https://arxiv.org/abs/2501.12344</guid>
<content:encoded><![CDATA[
arXiv:2501.12344v2 Announce Type: replace 
Abstract: Collaborative learning (CL) enables multiple participants to jointly train machine learning (ML) models on decentralized data sources without raw data sharing. While the primary goal of CL is to maximize the expected accuracy gain for each participant, it is also important to ensure that the gains are fairly distributed: no client should be negatively impacted, and gains should reflect contributions. Most existing CL methods require central coordination and focus only on gain maximization, overlooking fairness. In this work, we first show that the existing measure of collaborative fairness based on the correlation between accuracy values without and with collaboration has drawbacks because it does not account for negative collaboration gain. We argue that maximizing mean collaboration gain (MCG) while simultaneously minimizing the collaboration gain spread (CGS) is a fairer alternative. Next, we propose the CYCle protocol that enables individual participants in a private decentralized learning (PDL) framework to achieve this objective through a novel reputation scoring method based on gradient alignment between the local cross-entropy and distillation losses. We further extend the CYCle protocol to operate on top of gossip-based decentralized algorithms such as Gossip-SGD. We also theoretically show that CYCle performs better than standard FedAvg in a two-client mean estimation setting under high heterogeneity. Empirical experiments demonstrate the effectiveness of the CYCle protocol to ensure positive and fair collaboration gain for all participants, even in cases where the data distributions of participants are highly skewed.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noninterference Analysis of Irreversible or Reversible Systems with Nondeterminism and Probabilities</title>
<link>https://arxiv.org/abs/2501.19290</link>
<guid>https://arxiv.org/abs/2501.19290</guid>
<content:encoded><![CDATA[
arXiv:2501.19290v2 Announce Type: replace 
Abstract: Noninterference theory supports the analysis of secure computations in multi-level security systems. Classical equivalence-based approaches to noninterference mainly rely on bisimilarity. In a nondeterministic setting, assessing noninterference through weak bisimilarity is adequate for irreversible systems, whereas for reversible ones branching bisimilarity has been recently proven to be more appropriate. In this paper we address the same two families of systems, with the difference that probabilities come into play in addition to nondeterminism according to the alternating model of Hansson and Jonsson. For irreversible systems we extend the results of Aldini, Bravetti, and Gorrieri developed in a generative-reactive probabilistic setting, while for reversible systems we extend the results of Esposito, Aldini, Bernardo, and Rossi developed in a purely nondeterministic setting. We recast noninterference properties by adopting probabilistic variants of weak and branching bisimilarities for irreversible and reversible systems respectively. Then we investigate a taxonomy of those properties as well as their preservation and compositionality aspects, along with a comparison with the nondeterministic taxonomy. The adequacy of the extended noninterference theory is illustrated via a probabilistic smart contract lottery.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The challenge of hidden gifts in multi-agent reinforcement learning</title>
<link>https://arxiv.org/abs/2505.20579</link>
<guid>https://arxiv.org/abs/2505.20579</guid>
<content:encoded><![CDATA[
arXiv:2505.20579v5 Announce Type: replace 
Abstract: Sometimes we benefit from actions that others have taken even when we are unaware that they took those actions. For example, if your neighbor chooses not to take a parking spot in front of your house when you are not there, you can benefit, even without being aware that they took this action. These ``hidden gifts'' represent an interesting challenge for multi-agent reinforcement learning (MARL), since assigning credit when the beneficial actions of others are hidden is non-trivial. Here, we study the impact of hidden gifts with a very simple MARL task. In this task, agents in a grid-world environment have individual doors to unlock in order to obtain individual rewards. As well, if all the agents unlock their door the group receives a larger collective reward. However, there is only one key for all of the doors, such that the collective reward can only be obtained when the agents drop the key for others after they use it. Notably, there is nothing to indicate to an agent that the other agents have dropped the key, thus this act for others is a ``hidden gift''. We show that several different state-of-the-art MARL algorithms, including MARL specific architectures, fail to learn how to obtain the collective reward in this simple task. Interestingly, we find that decentralized actor-critic policy gradient agents can succeed when we provide them with information about their own action history, but MARL agents still cannot solve the task with action history. Finally, we derive a correction term for policy gradient agents, inspired by learning aware approaches, which reduces the variance in learning and helps them to converge to collective success more reliably. These results show that credit assignment in multi-agent settings can be particularly challenging in the presence of ``hidden gifts'', and demonstrate that self learning-awareness in decentralized agents can benefit these settings.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hitchhiker's Guide to Privacy-Preserving Digital Payment Systems: A Survey on Anonymity, Confidentiality, and Auditability</title>
<link>https://arxiv.org/abs/2505.21008</link>
<guid>https://arxiv.org/abs/2505.21008</guid>
<content:encoded><![CDATA[
arXiv:2505.21008v2 Announce Type: replace 
Abstract: Crypto-assets and central bank digital currencies (CBDCs) are reshaping how value is exchanged in distributed computing environments. These systems combine cryptographic primitives, protocol design, and system architectures to provide transparency and efficiency while raising critical challenges around privacy and regulatory compliance. This survey offers a comprehensive overview of privacy-preserving digital payment systems, covering both decentralized ledgers and CBDCs. We present a taxonomy of privacy goals -- including anonymity, confidentiality, unlinkability, and auditability -- and map them to underlying cryptographic primitives, protocols, and system architectures. Our work adopts a design-oriented perspective, linking high-level privacy objectives to concrete implementations. We also trace the evolution of privacy-preserving digital payment systems through three generations, highlighting shifts from basic anonymity guarantees toward more nuanced privacy-accountability trade-offs. Finally, we identify open challenges, motivating further research into architectures and solutions that balance strong privacy with real-world auditability needs.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blockchain-Based Secure Online Voting Platform Ensuring Voter Anonymity, Integrity, and End-to-End Verifiability</title>
<link>https://arxiv.org/abs/2509.22965</link>
<guid>https://arxiv.org/abs/2509.22965</guid>
<content:encoded><![CDATA[
arXiv:2509.22965v2 Announce Type: replace 
Abstract: Casting a ballot from a phone or laptop sounds appealing, but only if voters can be confident their choice remains secret and results cannot be altered in the dark. This paper proposes a hybrid blockchain-based voting model that stores encrypted votes on a private blockchain maintained by election organizers and neutral observers, while periodically anchoring hashes of these votes onto a public blockchain as a tamper-evident seal. The system issues voters one-time blind-signed tokens to protect anonymity, and provides receipts so they can confirm their vote was counted. We implemented a live prototype using common web technologies (Next.js, React, Firebase) to demonstrate end-to-end functionality, accessibility, and cost efficiency. Our contributions include developing a working demo, a complete election workflow, a hybrid blockchain design, and a user-friendly interface that balances privacy, security, transparency, and practicality. This research highlights the feasibility of secure, verifiable, and scalable online voting for organizations ranging from small groups to larger institutions.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Causal Inference from Multi-Site Observational Data via Propensity Score Aggregation</title>
<link>https://arxiv.org/abs/2505.17961</link>
<guid>https://arxiv.org/abs/2505.17961</guid>
<content:encoded><![CDATA[
arXiv:2505.17961v2 Announce Type: replace-cross 
Abstract: Causal inference typically assumes centralized access to individual-level data. Yet, in practice, data are often decentralized across multiple sites, making centralization infeasible due to privacy, logistical, or legal constraints. We address this problem by estimating the Average Treatment Effect (ATE) from decentralized observational data via a Federated Learning (FL) approach, allowing inference through the exchange of aggregate statistics rather than individual-level data. We propose a novel method to estimate propensity scores by computing a federated weighted average of local scores with Membership Weights (MW)--probabilities of site membership conditional on covariates--which can be flexibly estimated using parametric or non-parametric classification models. Unlike density ratio weights (DW) from the transportability and generalization literature, which either rely on strong modeling assumptions or cannot be implemented in FL, MW can be estimated using standard FL algorithms and are more robust, as they support flexible, non-parametric models--making them the preferred choice in multi-site settings with strict data-sharing constraints. The resulting propensity scores are used to construct Federated Inverse Propensity Weighting (Fed-IPW) and Augmented IPW (Fed-AIPW) estimators. Unlike meta-analysis methods, which fail when any site violates positivity, our approach leverages heterogeneity in treatment assignment across sites to improve overlap. We show that Fed-IPW and Fed-AIPW perform well under site-level heterogeneity in sample sizes, treatment mechanisms, and covariate distributions. Both theoretical analysis and experiments on simulated and real-world data highlight their advantages over meta-analysis and related methods.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Permuting Transactions in Ethereum Blocks: An Empirical Study</title>
<link>https://arxiv.org/abs/2509.25415</link>
<guid>https://arxiv.org/abs/2509.25415</guid>
<content:encoded><![CDATA[
arXiv:2509.25415v1 Announce Type: new 
Abstract: Several recent proposals implicitly or explicitly suggest making use of randomized transaction ordering within a block to mitigate centralization effects and to improve fairness in the Ethereum ecosystem. However, transactions and blocks are subject to gas limits and protocol rules. In a randomized transaction order, the behavior of transactions may change depending on other transactions in the same block, leading to invalid blocks and varying gas consumptions. In this paper, we quantify and characterize protocol violations, execution errors and deviations in gas consumption of blocks and transactions to examine technical deployability. For that, we permute and execute the transactions of over 335,000 Ethereum Mainnet blocks multiple times. About 22% of block permutations are invalid due to protocol violations caused by privately mined transactions or blocks close to their gas limit. Also, almost all transactions which show execution errors under permutation but not in the original order are privately mined transactions. Only 6% of transactions show deviations in gas consumption and 98% of block permutations deviate at most 10% from their original gas consumption. From a technical perspective, these results suggest that randomized transaction ordering may be feasible if transaction selection is handled carefully.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Interact in World Latent for Team Coordination</title>
<link>https://arxiv.org/abs/2509.25550</link>
<guid>https://arxiv.org/abs/2509.25550</guid>
<content:encoded><![CDATA[
arXiv:2509.25550v1 Announce Type: new 
Abstract: This work presents a novel representation learning framework, interactive world latent (IWoL), to facilitate team coordination in multi-agent reinforcement learning (MARL). Building effective representation for team coordination is a challenging problem, due to the intricate dynamics emerging from multi-agent interaction and incomplete information induced by local observations. Our key insight is to construct a learnable representation space that jointly captures inter-agent relations and task-specific world information by directly modeling communication protocols. This representation, we maintain fully decentralized execution with implicit coordination, all while avoiding the inherent drawbacks of explicit message passing, e.g., slower decision-making, vulnerability to malicious attackers, and sensitivity to bandwidth constraints. In practice, our representation can be used not only as an implicit latent for each agent, but also as an explicit message for communication. Across four challenging MARL benchmarks, we evaluate both variants and show that IWoL provides a simple yet powerful key for team coordination. Moreover, we demonstrate that our representation can be combined with existing MARL algorithms to further enhance their performance.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Split Learning with Sharded and Blockchain-Enabled SplitFed Approaches</title>
<link>https://arxiv.org/abs/2509.25555</link>
<guid>https://arxiv.org/abs/2509.25555</guid>
<content:encoded><![CDATA[
arXiv:2509.25555v1 Announce Type: new 
Abstract: Collaborative and distributed learning techniques, such as Federated Learning (FL) and Split Learning (SL), hold significant promise for leveraging sensitive data in privacy-critical domains. However, FL and SL suffer from key limitations -- FL imposes substantial computational demands on clients, while SL leads to prolonged training times. To overcome these challenges, SplitFed Learning (SFL) was introduced as a hybrid approach that combines the strengths of FL and SL. Despite its advantages, SFL inherits scalability, performance, and security issues from SL. In this paper, we propose two novel frameworks: Sharded SplitFed Learning (SSFL) and Blockchain-enabled SplitFed Learning (BSFL). SSFL addresses the scalability and performance constraints of SFL by distributing the workload and communication overhead of the SL server across multiple parallel shards. Building upon SSFL, BSFL replaces the centralized server with a blockchain-based architecture that employs a committee-driven consensus mechanism to enhance fairness and security. BSFL incorporates an evaluation mechanism to exclude poisoned or tampered model updates, thereby mitigating data poisoning and model integrity attacks. Experimental evaluations against baseline SL and SFL approaches show that SSFL improves performance and scalability by 31.2% and 85.2%, respectively. Furthermore, BSFL increases resilience to data poisoning attacks by 62.7% while maintaining superior performance under normal operating conditions. To the best of our knowledge, BSFL is the first blockchain-enabled framework to implement an end-to-end decentralized SplitFed Learning system.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight and Robust Federated Data Valuation</title>
<link>https://arxiv.org/abs/2509.25560</link>
<guid>https://arxiv.org/abs/2509.25560</guid>
<content:encoded><![CDATA[
arXiv:2509.25560v1 Announce Type: new 
Abstract: Federated learning (FL) faces persistent robustness challenges due to non-IID data distributions and adversarial client behavior. A promising mitigation strategy is contribution evaluation, which enables adaptive aggregation by quantifying each client's utility to the global model. However, state-of-the-art Shapley-value-based approaches incur high computational overhead due to repeated model reweighting and inference, which limits their scalability. We propose FedIF, a novel FL aggregation framework that leverages trajectory-based influence estimation to efficiently compute client contributions. FedIF adapts decentralized FL by introducing normalized and smoothed influence scores computed from lightweight gradient operations on client updates and a public validation set. Theoretical analysis demonstrates that FedIF yields a tighter bound on one-step global loss change under noisy conditions. Extensive experiments on CIFAR-10 and Fashion-MNIST show that FedIF achieves robustness comparable to or exceeding SV-based methods in the presence of label noise, gradient noise, and adversarial samples, while reducing aggregation overhead by up to 450x. Ablation studies confirm the effectiveness of FedIF's design choices, including local weight normalization and influence smoothing. Our results establish FedIF as a practical, theoretically grounded, and scalable alternative to Shapley-value-based approaches for efficient and robust FL in real-world deployments.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero Trust-based Decentralized Identity Management System for Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2509.25566</link>
<guid>https://arxiv.org/abs/2509.25566</guid>
<content:encoded><![CDATA[
arXiv:2509.25566v1 Announce Type: new 
Abstract: The rise of autonomous vehicles (AVs) promises to significantly enhance transportation safety and efficiency by mitigating human error, which is responsible for over 90\% of road accidents. However, the increasing connectivity of AVs introduces new cybersecurity challenges, as traditional perimeter-based security models are inadequate for dynamic and untrusted environments. This paper presents a novel Zero Trust-based Decentralized Identity Management (D-IM) protocol for AVs. By integrating the core principles of Zero Trust Architecture, "never trust, always verify", with the tamper resistant and decentralized nature of a blockchain network, our framework eliminates reliance on centralized authorities and provides continuous verification for every entity. We detail the system's design, which leverages Hyperledger Iroha to enable lightweight and secure authentication without a central trusted entity. A comprehensive experimental evaluation, conducted across both urban and highway scenarios, validates the protocol's practicality. Our results demonstrate that the D-IM framework introduces minimal overhead, with less than 7.5\% reduction in Packet Reception Rate (PRR) in urban settings and an increase of under 11\% in Channel Busy Ratio (CBR) for LTE-V2X. These findings prove the protocol's efficiency and robustness, providing a resilient foundation for securing real-time V2X communication against impersonation and replay attacks.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Asynchronous Multi-player Bandits</title>
<link>https://arxiv.org/abs/2509.25824</link>
<guid>https://arxiv.org/abs/2509.25824</guid>
<content:encoded><![CDATA[
arXiv:2509.25824v1 Announce Type: new 
Abstract: In recent years, multi-player multi-armed bandits (MP-MAB) have been extensively studied due to their wide applications in cognitive radio networks and Internet of Things systems. While most existing research on MP-MAB focuses on synchronized settings, real-world systems are often decentralized and asynchronous, where players may enter or leave the system at arbitrary times, and do not have a global clock. This decentralized asynchronous setting introduces two major challenges. First, without a global time, players cannot implicitly coordinate their actions through time, making it difficult to avoid collisions. Second, it is important to detect how many players are in the system, but doing so may cost a lot. In this paper, we address the challenges posed by such a fully asynchronous setting in a decentralized environment. We develop a novel algorithm in which players adaptively change between exploration and exploitation. During exploration, players uniformly pull their arms, reducing the probability of collisions and effectively mitigating the first challenge. Meanwhile, players continue pulling arms currently exploited by others with a small probability, enabling them to detect when a player has left, thereby addressing the second challenge. We prove that our algorithm achieves a regret of $\mathcal{O}(\sqrt{T \log T} + {\log T}/{\Delta^2})$, where $\Delta$ is the minimum expected reward gap between any two arms. To the best of our knowledge, this is the first efficient MP-MAB algorithm in the asynchronous and decentralized environment. Extensive experiments further validate the effectiveness and robustness of our algorithm, demonstrating its applicability to real-world scenarios.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Achieving Pareto Optimality in Games via Single-bit Feedback</title>
<link>https://arxiv.org/abs/2509.25921</link>
<guid>https://arxiv.org/abs/2509.25921</guid>
<content:encoded><![CDATA[
arXiv:2509.25921v1 Announce Type: new 
Abstract: Efficient coordination in multi-agent systems often incurs high communication overhead or slow convergence rates, making scalable welfare optimization difficult. We propose Single-Bit Coordination Dynamics for Pareto-Efficient Outcomes (SBC-PE), a decentralized learning algorithm requiring only a single-bit satisfaction signal per agent each round. Despite this extreme efficiency, SBC-PE guarantees convergence to the exact optimal solution in arbitrary finite games. We establish explicit regret bounds, showing expected regret grows only logarithmically with the horizon, i.e., O(log T). Compared with prior payoff-based or bandit-style rules, SBC-PE uniquely combines minimal signaling, general applicability, and finite-time guarantees. These results show scalable welfare optimization is achievable under minimal communication constraints.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parallax: Efficient LLM Inference Service over Decentralized Environment</title>
<link>https://arxiv.org/abs/2509.26182</link>
<guid>https://arxiv.org/abs/2509.26182</guid>
<content:encoded><![CDATA[
arXiv:2509.26182v1 Announce Type: new 
Abstract: Deploying a large language model (LLM) inference service remains costly because centralized serving depends on specialized GPU clusters and high-bandwidth interconnects in datacenters. An appealing alternative is to leverage collaborative decentralized GPU pools. However, heterogeneity in GPU and limited interconnected network bandwidth, along with potentially dynamic availability, make efficient scheduling the central challenge in this scenario. In this paper, we present Parallax, a decentralized LLM serving system that turns a pool of heterogeneous GPUs into an efficient inference platform via a two-phase scheduler. Parallax decomposes planning into (i) model allocation, which places layers of each replica across diverse GPUs to jointly optimize latency and throughput under memory and link-bandwidth constraints, and (ii) request-time GPU pipeline selection, which stitches layers from different replicas into end-to-end execution chains that balance load and adapt to current conditions. We implement Parallax and evaluate it on open-source LLMs deployed over real volunteer nodes. Parallax consistently reduces latency and increases throughput relative to decentralized baselines, demonstrating that principled scheduling can make volunteer compute a practical, affordable substrate for LLM inference.
  Github Repo at: https://github.com/GradientHQ/parallax.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Decentralized Federated Learning</title>
<link>https://arxiv.org/abs/2509.26462</link>
<guid>https://arxiv.org/abs/2509.26462</guid>
<content:encoded><![CDATA[
arXiv:2509.26462v1 Announce Type: new 
Abstract: CLIP has revolutionized zero-shot learning by enabling task generalization without fine-tuning. While prompting techniques like CoOp and CoCoOp enhance CLIP's adaptability, their effectiveness in Federated Learning (FL) remains an open challenge. Existing federated prompt learning approaches, such as FedCoOp and FedTPG, improve performance but face generalization issues, high communication costs, and reliance on a central server, limiting scalability and privacy. We propose Zero-shot Decentralized Federated Learning (ZeroDFL), a fully decentralized framework that enables zero-shot adaptation across distributed clients without a central coordinator. ZeroDFL employs an iterative prompt-sharing mechanism, allowing clients to optimize and exchange textual prompts to enhance generalization while drastically reducing communication overhead. We validate ZeroDFL on nine diverse image classification datasets, demonstrating that it consistently outperforms--or remains on par with--state-of-the-art federated prompt learning methods. More importantly, ZeroDFL achieves this performance in a fully decentralized setting while reducing communication overhead by 118x compared to FedTPG. These results highlight that our approach not only enhances generalization in federated zero-shot learning but also improves scalability, efficiency, and privacy preservation--paving the way for decentralized adaptation of large vision-language models in real-world applications.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAP: Two-Stage Adaptive Personalization of Multi-task and Multi-Modal Foundation Models in Federated Learning</title>
<link>https://arxiv.org/abs/2509.26524</link>
<guid>https://arxiv.org/abs/2509.26524</guid>
<content:encoded><![CDATA[
arXiv:2509.26524v1 Announce Type: new 
Abstract: Federated Learning (FL), despite demonstrating impressive capabilities in the training of multiple models in a decentralized manner, has been shown to produce a final model not necessarily well-suited to the needs of each client. While extensive work has been conducted on how to create tailored personalized models, called Personalized Federated Learning (PFL), less attention has been given to personalization via fine-tuning of foundation models with multi-task and multi-modal properties. Moreover, there exists a lack of understanding in the literature on how to fine-tune and personalize such models in a setting that is heterogeneous across clients not only in data, but also in tasks and modalities. To address this gap in the literature, we propose TAP (Two-Stage Adaptive Personalization), which (i) leverages mismatched model architectures between the clients and server to selectively conduct replacement operations when it benefits a client's local tasks and (ii) engages in post-FL knowledge distillation for capturing beneficial general knowledge without compromising personalization. We also introduce the first convergence analysis of the server model under its modality-task pair architecture, and demonstrate that as the number of modality-task pairs increases, its ability to cater to all tasks suffers. Through extensive experiments, we demonstrate the effectiveness of our proposed algorithm across a variety of datasets and tasks in comparison to a multitude of baselines. Implementation code is publicly available at https://github.com/lee3296/TAP.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Searcher Competition in Block Building</title>
<link>https://arxiv.org/abs/2407.07474</link>
<guid>https://arxiv.org/abs/2407.07474</guid>
<content:encoded><![CDATA[
arXiv:2407.07474v2 Announce Type: replace 
Abstract: We study the amount of maximal extractable value (MEV) captured by validators, as a function of searcher competition, in blockchains with competitive block building markets such as Ethereum. We argue that the core is a suitable solution concept in this context that makes robust predictions that are independent of implementation details or specific mechanisms chosen. We characterize how much value validators extract in the core and quantify the surplus share of validators as a function of searcher competition. Searchers can obtain at most the marginal value increase of the winning block relative to the best block that can be built without their bundles. Dually this gives a lower bound on the value extracted by the validator. If arbitrages are easy to find and many searchers find similar bundles, the validator gets paid all value almost surely, while searchers can capture most value if there is little searcher competition per arbitrage. For the case of passive block-proposers we study, moreover, mechanisms that implement core allocations in dominant strategies and find that for submodular value, there is a unique dominant-strategy incentive compatible core-selecting mechanism that gives each searcher exactly their marginal value contribution to the winning block. We validate our theoretical prediction empirically with aggregate bundle data and find a significant positive relation between the number of submitted backruns for the same opportunity and the median value captured by the proposer from the opportunity.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Competition to Centralization: The Oligopoly in Ethereum Block Building Auctions</title>
<link>https://arxiv.org/abs/2412.18074</link>
<guid>https://arxiv.org/abs/2412.18074</guid>
<content:encoded><![CDATA[
arXiv:2412.18074v2 Announce Type: replace 
Abstract: Block production on the Ethereum blockchain has adopted an auction-based mechanism known as Proposer--Builder Separation (PBS), where validators outsource block creation to builders competing in MEV--Boost auctions for Maximal Extractable Value (MEV) rewards. We employ empirical game-theoretic analysis based on simulations to examine how advantages in latency and MEV access shape builder strategic bidding and auction outcomes. We find that a small set of dominant builders leverage these advantages, consolidating power, reducing auction efficiency, and heightening centralization. Our results underscore the need for fair MEV distribution and sustained efforts to promote decentralization in Ethereum's block building market.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thunderdome: Timelock-Free Rationally-Secure Virtual Channels</title>
<link>https://arxiv.org/abs/2501.14418</link>
<guid>https://arxiv.org/abs/2501.14418</guid>
<content:encoded><![CDATA[
arXiv:2501.14418v4 Announce Type: replace 
Abstract: Payment channel networks (PCNs) offer a promising solution to address the limited transaction throughput of deployed blockchains. However, several attacks have recently been proposed that stress the vulnerability of PCNs to timelock and censoring attacks. To address such attacks, we introduce Thunderdome, the first timelock-free PCN. Instead, Thunderdome leverages the design rationale of virtual channels to extend a timelock-free payment channel primitive, thereby enabling multi-hop transactions without timelocks. Previous works either utilize timelocks or do not accommodate transactions between parties that do not share a channel.
  At its core, Thunderdome relies on a committee of non-trusted watchtowers, known as wardens, who ensure that no honest party loses funds, even when offline, during the channel closure process. We introduce tailored incentive mechanisms to ensure that all participants follow the protocol's correct execution. Besides a traditional security proof that assumes an honest majority of the committee, we conduct a formal game-theoretic analysis to demonstrate the security of Thunderdome when all participants, including wardens, act rationally. We implement a proof of concept of Thunderdome on Ethereum to validate its feasibility and evaluate its costs. Our evaluation shows that deploying Thunderdome, including opening the underlying payment channel, costs approximately \$15 (0.0089 ETH), while the worst-case cost for closing a channel is about \$7 (0.004 ETH).
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AntiFLipper: A Secure and Efficient Defense Against Label-Flipping Attacks in Federated Learning</title>
<link>https://arxiv.org/abs/2509.22873</link>
<guid>https://arxiv.org/abs/2509.22873</guid>
<content:encoded><![CDATA[
arXiv:2509.22873v2 Announce Type: replace 
Abstract: Federated learning (FL) enables privacy-preserving model training by keeping data decentralized. However, it remains vulnerable to label-flipping attacks, where malicious clients manipulate labels to poison the global model. Despite their simplicity, these attacks can severely degrade model performance, and defending against them remains challenging. We introduce AntiFLipper, a novel and computationally efficient defense against multi-class label-flipping attacks in FL. Unlike existing methods that ensure security at the cost of high computational overhead, AntiFLipper employs a novel client-side detection strategy, significantly reducing the central server's burden during aggregation. Comprehensive empirical evaluations across multiple datasets under different distributions demonstrate that AntiFLipper achieves accuracy comparable to state-of-the-art defenses while requiring substantially fewer computational resources in server side. By balancing security and efficiency, AntiFLipper addresses a critical gap in existing defenses, making it particularly suitable for resource-constrained FL deployments where both model integrity and operational efficiency are essential.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Satellite: Detecting and Analyzing Smart Contract Vulnerabilities caused by Subcontract Misuse</title>
<link>https://arxiv.org/abs/2509.23679</link>
<guid>https://arxiv.org/abs/2509.23679</guid>
<content:encoded><![CDATA[
arXiv:2509.23679v1 Announce Type: new 
Abstract: Developers of smart contracts pervasively reuse subcontracts to improve development efficiency. Like any program language, such subcontract reuse may unexpectedly include, or introduce vulnerabilities to the end-point smart contract. Unfortunately, automatically detecting such issues poses several unique challenges. Particularly, in most cases, smart contracts are compiled as bytecode, whose class-level information (e.g., inheritance, virtual function table), and even semantics (e.g., control flow and data flow) are fully obscured as a single smart contract after compilation.
  In this paper, we propose Satellite, a new bytecode-level static analysis framework for subcontract misuse vulnerability (SMV) detection in smart contracts. Satellite incorporates a series of novel designs to enhance its overall effectiveness.. Particularly, Satellite utilizes a transfer learning method to recover the inherited methods, which are critical for identifying subcontract reuse in smart contracts. Further, Satellite extracts a set of fine-grained method-level features and performs a method-level comparison, for identifying the reuse part of subcontract in smart contracts. Finally, Satellite summarizes a set of SMV indicators according to their types, and hence effectively identifies SMVs. To evaluate Satellite, we construct a dataset consisting of 58 SMVs derived from real-world attacks and collect additional 56 SMV patterns from SOTA studies. Experiment results indicate that Satellite exhibits good performance in identifying SMV, with a precision rate of 84.68% and a recall rate of 92.11%. In addition, Satellite successfully identifies 14 new/unknown SMV over 10,011 real-world smart contracts, affecting a total amount of digital assets worth 201,358 USD.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Dynamic Cooperation of Personalized Models for Federated Continual Learning</title>
<link>https://arxiv.org/abs/2509.23683</link>
<guid>https://arxiv.org/abs/2509.23683</guid>
<content:encoded><![CDATA[
arXiv:2509.23683v1 Announce Type: new 
Abstract: Federated continual learning (FCL) has garnered increasing attention for its ability to support distributed computation in environments with evolving data distributions. However, the emergence of new tasks introduces both temporal and cross-client shifts, making catastrophic forgetting a critical challenge. Most existing works aggregate knowledge from clients into a global model, which may not enhance client performance since irrelevant knowledge could introduce interference, especially in heterogeneous scenarios. Additionally, directly applying decentralized approaches to FCL suffers from ineffective group formation caused by task changes. To address these challenges, we propose a decentralized dynamic cooperation framework for FCL, where clients establish dynamic cooperative learning coalitions to balance the acquisition of new knowledge and the retention of prior learning, thereby obtaining personalized models. To maximize model performance, each client engages in selective cooperation, dynamically allying with others who offer meaningful performance gains. This results in non-overlapping, variable coalitions at each stage of the task. Moreover, we use coalitional affinity game to simulate coalition relationships between clients. By assessing both client gradient coherence and model similarity, we quantify the client benefits derived from cooperation. We also propose a merge-blocking algorithm and a dynamic cooperative evolution algorithm to achieve cooperative and dynamic equilibrium. Comprehensive experiments demonstrate the superiority of our method compared to various baselines. Code is available at: https://github.com/ydn3229/DCFCL.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Short-Term Guidance Algorithm on a Drone Road System</title>
<link>https://arxiv.org/abs/2509.23794</link>
<guid>https://arxiv.org/abs/2509.23794</guid>
<content:encoded><![CDATA[
arXiv:2509.23794v1 Announce Type: new 
Abstract: Unmanned Aerial Vehicles (UAVs), commonly known as drones, have experienced expanding use in urban environments in recent years. However, the growing density of drones raises significant challenges, such as avoiding collisions and managing air traffic efficiently, especially in congested areas. To address these issues, a structured road system and an effective guidance algorithm are essential. In this paper, we introduce a markup language allowing to describe drone road systems (DRS), in which a road system is given by a set of individual roads, each of which can have a varying number of lanes. Roads can be linked through connecting lanes. Furthermore, we propose a novel short-term decentralized greedy (STDG) guidance algorithm that uses only the position and speed information of nearby drones -- communicated via periodically transmitted beacons -- to make real-time decisions such as stopping, changing lanes, or adjusting speed for the next few seconds. Unlike existing methods that rely on centralized coordination, our algorithm enables drones to operate independently while ensuring safety and efficiency. We present simulation results showing the impact of key wireless and algorithm parameters on performance metrics like the drone collision rate, average speed and throughput of the drone road system.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SolContractEval: A Benchmark for Evaluating Contract-Level Solidity Code Generation</title>
<link>https://arxiv.org/abs/2509.23824</link>
<guid>https://arxiv.org/abs/2509.23824</guid>
<content:encoded><![CDATA[
arXiv:2509.23824v1 Announce Type: new 
Abstract: The rise of blockchain has brought smart contracts into mainstream use, creating a demand for smart contract generation tools. While large language models (LLMs) excel at generating code in general-purpose languages, their effectiveness on Solidity, the primary language for smart contracts, remains underexplored. Solidity constitutes only a small portion of typical LLM training data and differs from general-purpose languages in its version-sensitive syntax and limited flexibility. These factors raise concerns about the reliability of existing LLMs for Solidity code generation. Critically, existing evaluations, focused on isolated functions and synthetic inputs, fall short of assessing models' capabilities in real-world contract development.
  To bridge this gap, we introduce SolContractEval, the first contract-level benchmark for Solidity code generation. It comprises 124 tasks drawn from real on-chain contracts across nine major domains. Each task input, consisting of complete context dependencies, a structured contract framework, and a concise task prompt, is independently annotated and cross-validated by experienced developers. To enable precise and automated evaluation of functional correctness, we also develop a dynamic evaluation framework based on historical transaction replay. Building on SolContractEval, we perform a systematic evaluation of six mainstream LLMs. We find that Claude-3.7-Sonnet achieves the highest overall performance, though evaluated models underperform relative to their capabilities on class-level generation tasks in general-purpose programming languages. Second, current models perform better on tasks that follow standard patterns but struggle with complex logic and inter-contract dependencies. Finally, they exhibit limited understanding of Solidity-specific features and contextual dependencies.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAD-PINN: A Decentralized Physics-Informed Machine Learning Framework for Safe and Optimal Multi-Agent Control</title>
<link>https://arxiv.org/abs/2509.23960</link>
<guid>https://arxiv.org/abs/2509.23960</guid>
<content:encoded><![CDATA[
arXiv:2509.23960v1 Announce Type: new 
Abstract: Co-optimizing safety and performance in large-scale multi-agent systems remains a fundamental challenge. Existing approaches based on multi-agent reinforcement learning (MARL), safety filtering, or Model Predictive Control (MPC) either lack strict safety guarantees, suffer from conservatism, or fail to scale effectively. We propose MAD-PINN, a decentralized physics-informed machine learning framework for solving the multi-agent state-constrained optimal control problem (MASC-OCP). Our method leverages an epigraph-based reformulation of SC-OCP to simultaneously capture performance and safety, and approximates its solution via a physics-informed neural network. Scalability is achieved by training the SC-OCP value function on reduced-agent systems and deploying them in a decentralized fashion, where each agent relies only on local observations of its neighbours for decision-making. To further enhance safety and efficiency, we introduce an Hamilton-Jacobi (HJ) reachability-based neighbour selection strategy to prioritize safety-critical interactions, and a receding-horizon policy execution scheme that adapts to dynamic interactions while reducing computational burden. Experiments on multi-agent navigation tasks demonstrate that MAD-PINN achieves superior safety-performance trade-offs, maintains scalability as the number of agents grows, and consistently outperforms state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curriculum-Guided Reinforcement Learning for Synthesizing Gas-Efficient Financial Derivatives Contracts</title>
<link>https://arxiv.org/abs/2509.23976</link>
<guid>https://arxiv.org/abs/2509.23976</guid>
<content:encoded><![CDATA[
arXiv:2509.23976v1 Announce Type: new 
Abstract: Smart contract-based automation of financial derivatives offers substantial efficiency gains, but its real-world adoption is constrained by the complexity of translating financial specifications into gas-efficient executable code. In particular, generating code that is both functionally correct and economically viable from high-level specifications, such as the Common Domain Model (CDM), remains a significant challenge. This paper introduces a Reinforcement Learning (RL) framework to generate functional and gas-optimized Solidity smart contracts directly from CDM specifications. We employ a Proximal Policy Optimization (PPO) agent that learns to select optimal code snippets from a pre-defined library. To manage the complex search space, a two-phase curriculum first trains the agent for functional correctness before shifting its focus to gas optimization. Our empirical results show the RL agent learns to generate contracts with significant gas savings, achieving cost reductions of up to 35.59% on unseen test data compared to unoptimized baselines. This work presents a viable methodology for the automated synthesis of reliable and economically sustainable smart contracts, bridging the gap between high-level financial agreements and efficient on-chain execution.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiple Concurrent Proposers: Why and How</title>
<link>https://arxiv.org/abs/2509.23984</link>
<guid>https://arxiv.org/abs/2509.23984</guid>
<content:encoded><![CDATA[
arXiv:2509.23984v1 Announce Type: new 
Abstract: Traditional single-proposer blockchains suffer from miner extractable value (MEV), where validators exploit their serial monopoly on transaction inclusion and ordering to extract rents from users. While there have been many developments at the application layer to reduce the impact of MEV, these approaches largely require auctions as a subcomponent. Running auctions efficiently on chain requires two key properties of the underlying consensus protocol: selective-censorship resistance and hiding. These properties guarantee that an adversary can neither selectively delay transactions nor see their contents before they are confirmed. We propose a multiple concurrent proposer (MCP) protocol offering exactly these properties.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimism as Risk-Seeking in Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.24047</link>
<guid>https://arxiv.org/abs/2509.24047</guid>
<content:encoded><![CDATA[
arXiv:2509.24047v1 Announce Type: new 
Abstract: Risk sensitivity has become a central theme in reinforcement learning (RL), where convex risk measures and robust formulations provide principled ways to model preferences beyond expected return. Recent extensions to multi-agent RL (MARL) have largely emphasized the risk-averse setting, prioritizing robustness to uncertainty. In cooperative MARL, however, such conservatism often leads to suboptimal equilibria, and a parallel line of work has shown that optimism can promote cooperation. Existing optimistic methods, though effective in practice, are typically heuristic and lack theoretical grounding. Building on the dual representation for convex risk measures, we propose a principled framework that interprets risk-seeking objectives as optimism. We introduce optimistic value functions, which formalize optimism as divergence-penalized risk-seeking evaluations. Building on this foundation, we derive a policy-gradient theorem for optimistic value functions, including explicit formulas for the entropic risk/KL-penalty setting, and develop decentralized optimistic actor-critic algorithms that implement these updates. Empirical results on cooperative benchmarks demonstrate that risk-seeking optimism consistently improves coordination over both risk-neutral baselines and heuristic optimistic methods. Our framework thus unifies risk-sensitive learning and optimism, offering a theoretically grounded and practically effective approach to cooperation in MARL.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VeriLLM: A Lightweight Framework for Publicly Verifiable Decentralized Inference</title>
<link>https://arxiv.org/abs/2509.24257</link>
<guid>https://arxiv.org/abs/2509.24257</guid>
<content:encoded><![CDATA[
arXiv:2509.24257v1 Announce Type: new 
Abstract: Decentralized inference is an appealing paradigm for serving large language models (LLMs), offering strong security, high efficiency, and lower operating costs. Yet the permissionless setting admits no a priori trust in participating nodes, making output verifiability a prerequisite for secure deployment. We present VeriLLM, a publicly verifiable protocol for decentralized LLM inference that (i) achieves security under a one-honest-verifier assumption, (ii) attains near-negligible verification cost (about 1% of the underlying inference) via a lightweight verification algorithm designed explicitly for LLMs, and (iii) enforces honest checking through a peer-prediction mechanism that mitigates lazy verification in naive voting. We further introduce an isomorphic inference-verification network that multiplexes both roles on the same set of GPU workers. This architecture (i) increases GPU utilization and thereby improves end-to-end throughput for both inference and verification, (ii) expands the effective pool of available validators, strengthening robustness and security, and (iii) enforces task indistinguishability at the worker boundary to prevent job-type-conditioned behavior. Finally, we provide a formal game-theoretic analysis and prove that, under our incentives, honest inference and verification constitute a Nash equilibrium, ensuring incentive compatibility against rational adversaries. To our knowledge, this is the first decentralized inference verification protocol with an end-to-end game-theoretic security proof.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H+: An Efficient Similarity-Aware Aggregation for Byzantine Resilient Federated Learning</title>
<link>https://arxiv.org/abs/2509.24330</link>
<guid>https://arxiv.org/abs/2509.24330</guid>
<content:encoded><![CDATA[
arXiv:2509.24330v1 Announce Type: new 
Abstract: Federated Learning (FL) enables decentralized model training without sharing raw data. However, it remains vulnerable to Byzantine attacks, which can compromise the aggregation of locally updated parameters at the central server. Similarity-aware aggregation has emerged as an effective strategy to mitigate such attacks by identifying and filtering out malicious clients based on similarity between client model parameters and those derived from clean data, i.e., data that is uncorrupted and trustworthy. However, existing methods adopt this strategy only in FL systems with clean data, making them inapplicable to settings where such data is unavailable. In this paper, we propose H+, a novel similarity-aware aggregation approach that not only outperforms existing methods in scenarios with clean data, but also extends applicability to FL systems without any clean data. Specifically, H+ randomly selects $r$-dimensional segments from the $p$-dimensional parameter vectors uploaded to the server and applies a similarity check function $H$ to compare each segment against a reference vector, preserving the most similar client vectors for aggregation. The reference vector is derived either from existing robust algorithms when clean data is unavailable or directly from clean data. Repeating this process $K$ times enables effective identification of honest clients. Moreover, H+ maintains low computational complexity, with an analytical time complexity of $\mathcal{O}(KMr)$, where $M$ is the number of clients and $Kr \ll p$. Comprehensive experiments validate H+ as a state-of-the-art (SOTA) method, demonstrating substantial robustness improvements over existing approaches under varying Byzantine attack ratios and multiple types of traditional Byzantine attacks, across all evaluated scenarios and benchmark datasets.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BugMagnifier: TON Transaction Simulator for Revealing Smart Contract Vulnerabilities</title>
<link>https://arxiv.org/abs/2509.24444</link>
<guid>https://arxiv.org/abs/2509.24444</guid>
<content:encoded><![CDATA[
arXiv:2509.24444v1 Announce Type: new 
Abstract: The Open Network (TON) blockchain employs an asynchronous execution model that introduces unique security challenges for smart contracts, particularly race conditions arising from unpredictable message processing order. While previous work established vulnerability patterns through static analysis of audit reports, dynamic detection of temporal dependencies through systematic testing remains an open problem. We present BugMagnifier, a transaction simulation framework that systematically reveals vulnerabilities in TON smart contracts through controlled message orchestration. Built atop TON Sandbox and integrated with the TON Virtual Machine (TVM), our tool combines precise message queue manipulation with differential state analysis and probabilistic permutation testing to detect asynchronous execution flaws. Experimental evaluation demonstrates BugMagnifier's effectiveness through extensive parametric studies on purpose-built vulnerable contracts, revealing message ratio-dependent detection complexity that aligns with theoretical predictions. This quantitative model enables predictive vulnerability assessment while shifting discovery from manual expert analysis to automated evidence generation. By providing reproducible test scenarios for temporal vulnerabilities, BugMagnifier addresses a critical gap in the TON security tooling, offering practical support for safer smart contract development in asynchronous blockchain environments.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributionally Robust Federated Learning with Outlier Resilience</title>
<link>https://arxiv.org/abs/2509.24462</link>
<guid>https://arxiv.org/abs/2509.24462</guid>
<content:encoded><![CDATA[
arXiv:2509.24462v1 Announce Type: new 
Abstract: Federated learning (FL) enables collaborative model training without direct data sharing, but its performance can degrade significantly in the presence of data distribution perturbations. Distributionally robust optimization (DRO) provides a principled framework for handling this by optimizing performance against the worst-case distributions within a prescribed ambiguity set. However, existing DRO-based FL methods often overlook the detrimental impact of outliers in local datasets, which can disproportionately bias the learned models. In this work, we study distributionally robust federated learning with explicit outlier resilience. We introduce a novel ambiguity set based on the unbalanced Wasserstein distance, which jointly captures geometric distributional shifts and incorporates a non-geometric Kullback--Leibler penalization to mitigate the influence of outliers. This formulation naturally leads to a challenging min--max--max optimization problem. To enable decentralized training, we reformulate the problem as a tractable Lagrangian penalty optimization, which admits robustness certificates. Building on this reformulation, we propose the distributionally outlier-robust federated learning algorithm and establish its convergence guarantees. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Specification Generator for Move Programs</title>
<link>https://arxiv.org/abs/2509.24515</link>
<guid>https://arxiv.org/abs/2509.24515</guid>
<content:encoded><![CDATA[
arXiv:2509.24515v1 Announce Type: new 
Abstract: While LLM-based specification generation is gaining traction, existing tools primarily focus on mainstream programming languages like C, Java, and even Solidity, leaving emerging and yet verification-oriented languages like Move underexplored. In this paper, we introduce MSG, an automated specification generation tool designed for Move smart contracts. MSG aims to highlight key insights that uniquely present when applying LLM-based specification generation to a new ecosystem. Specifically, MSG demonstrates that LLMs exhibit robust code comprehension and generation capabilities even for non-mainstream languages. MSG successfully generates verifiable specifications for 84% of tested Move functions and even identifies clauses previously overlooked by experts. Additionally, MSG shows that explicitly leveraging specification language features through an agentic, modular design improves specification quality substantially (generating 57% more verifiable clauses than conventional designs). Incorporating feedback from the verification toolchain further enhances the effectiveness of MSG, leading to a 30% increase in generated verifiable specifications.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompting Robot Teams with Natural Language</title>
<link>https://arxiv.org/abs/2509.24575</link>
<guid>https://arxiv.org/abs/2509.24575</guid>
<content:encoded><![CDATA[
arXiv:2509.24575v1 Announce Type: new 
Abstract: This paper presents a framework towards prompting multi-robot teams with high-level tasks using natural language expressions. Our objective is to use the reasoning capabilities demonstrated by recent language models in understanding and decomposing human expressions of intent, and repurpose these for multi-robot collaboration and decision-making. The key challenge is that an individual's behavior in a collective can be hard to specify and interpret, and must continuously adapt to actions from others. This necessitates a framework that possesses the representational capacity required by the logic and semantics of a task, and yet supports decentralized and interactive real-time operation. We solve this dilemma by recognizing that a task can be represented as a deterministic finite automaton (DFA), and that recurrent neural networks (RNNs) can encode numerous automata. This allows us to distill the logic and sequential decompositions of sub-tasks obtained from a language model into an RNN, and align its internal states with the semantics of a given task. By training a graph neural network (GNN) control policy that is conditioned on the hidden states of the RNN and the language embeddings, our method enables robots to execute task-relevant actions in a decentralized manner. We present evaluations of this single light-weight interpretable model on various simulated and real-world multi-robot tasks that require sequential and collaborative behavior by the team -- sites.google.com/view/prompting-teams.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LISA Technical Report: An Agentic Framework for Smart Contract Auditing</title>
<link>https://arxiv.org/abs/2509.24698</link>
<guid>https://arxiv.org/abs/2509.24698</guid>
<content:encoded><![CDATA[
arXiv:2509.24698v1 Announce Type: new 
Abstract: We present LISA, an agentic smart contract vulnerability detection framework that combines rule-based and logic-based methods to address a broad spectrum of vulnerabilities in smart contracts. LISA leverages data from historical audit reports to learn the detection experience (without model fine-tuning), enabling it to generalize learned patterns to unseen projects and evolving threat profiles. In our evaluation, LISA significantly outperforms both LLM-based approaches and traditional static analysis tools, achieving superior coverage of vulnerability types and higher detection accuracy. Our results suggest that LISA offers a compelling solution for industry: delivering more reliable and comprehensive vulnerability detection while reducing the dependence on manual effort.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blockchain-Driven Federation for Distributed Edge Systems: Design and Experimental Validation</title>
<link>https://arxiv.org/abs/2509.24846</link>
<guid>https://arxiv.org/abs/2509.24846</guid>
<content:encoded><![CDATA[
arXiv:2509.24846v1 Announce Type: new 
Abstract: Edge computing brings computation near end users, enabling the provisioning of novel use cases. To satisfy end-user requirements, the concept of edge federation has recently emerged as a key mechanism for dynamic resources and services sharing across edge systems managed by different administrative domains. However, existing federation solutions often rely on pre-established agreements and face significant limitations, including operational complexity, delays caused by manual operations, high overhead costs, and dependence on trusted third parties. In this context, blockchain can create dynamic federation agreements that enable service providers to securely interact and share services without prior trust.
  This article first describes the problem of edge federation, using the standardized ETSI multi-access edge computing framework as a reference architecture, and how it is being addressed. Then, it proposes a novel solution using blockchain and smart contracts to enable distributed MEC systems to dynamically negotiate and execute federation in a secure, automated, and scalable manner. We validate our framework's feasibility through a performance evaluation using a private Ethereum blockchain, built on the open-source Hyperledger Besu platform. The testbed includes a large number of MEC systems and compares two blockchain consensus algorithms. Experimental results demonstrate that our solution automates the entire federation lifecycle-from negotiation to deployment-with a quantifiable overhead, achieving federation in approximately 18 seconds in a baseline scenario. The framework scales efficiently in concurrent request scenarios, where multiple MEC systems initiate federation requests simultaneously. This approach provides a promising direction for addressing the complexities of dynamic, multi-domain federations across the edge-to-cloud continuum.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Free Option Problem of ePBS</title>
<link>https://arxiv.org/abs/2509.24849</link>
<guid>https://arxiv.org/abs/2509.24849</guid>
<content:encoded><![CDATA[
arXiv:2509.24849v1 Announce Type: new 
Abstract: Ethereum's upcoming Glamsterdam upgrade introduces EIP-7732 enshrined Proposer--Builder Separation (ePBS), which improves the block production pipeline by addressing trust and scalability challenges. Yet it also creates a new liveness risk: builders gain a short-dated ``free'' option to prevent the execution payload they committed to from becoming canonical, without incurring an additional penalty. Exercising this option renders an empty block for the slot in question, thereby degrading network liveness.
  We present the first systematic study of the free option problem. Our theoretical results predict that option value and exercise probability grow with market volatility, the length of the option window, and the share of block value derived from external signals such as external market prices. The availability of a free option will lead to mispricing and LP losses. The problem would be exacerbated if Ethereum further scales and attracts more liquidity. Empirical estimates of values and exercise probabilities on historical blocks largely confirm our theoretical predictions. While the option is rarely profitable to exercise on average (0.82\% of blocks assuming an 8-second option time window), it becomes significant in volatile periods, reaching up to 6\% of blocks on high-volatility days -- precisely when users most require timely execution.
  Moreover, builders whose block value relies heavily on CEX-DEX arbitrage are more likely to exercise the option. We demonstrate that mitigation strategies -- shortening the option window or penalizing exercised options -- effectively reduce liveness risk.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Secret Leader Election in Ethereum PoS: An Empirical Security Analysis of Whisk and Homomorphic Sortition under DoS on the Leader and Censorship Attacks</title>
<link>https://arxiv.org/abs/2509.24955</link>
<guid>https://arxiv.org/abs/2509.24955</guid>
<content:encoded><![CDATA[
arXiv:2509.24955v1 Announce Type: new 
Abstract: Proposer anonymity in Proof-of-Stake (PoS) blockchains is a critical concern due to the risk of targeted attacks such as malicious denial-of-service (DoS) and censorship attacks. While several Secret Single Leader Election (SSLE) mechanisms have been proposed to address these threats, their practical impact and trade-offs remain insufficiently explored. In this work, we present a unified experimental framework for evaluating SSLE mechanisms under adversarial conditions, grounded in a simplified yet representative model of Ethereum's PoS consensus layer. The framework includes configurable adversaries capable of launching targeted DoS and censorship attacks, including coordinated strategies that simultaneously compromise groups of validators. We simulate and compare key protection mechanisms - Whisk, and homomorphic sortition. To the best of our knowledge, this is the first comparative study to examine adversarial DoS scenarios involving multiple attackers under diverse protection mechanisms. Our results show that while both designs offer strong protection against targeted DoS attacks on the leader, neither defends effectively against coordinated attacks on validator groups. Moreover, Whisk simplifies a DoS attack by narrowing the target set from all validators to a smaller list of known candidates. Homomorphic sortition, despite its theoretical strength, remains impractical due to the complexity of cryptographic operations over large validator sets.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MARLIN: Multi-Agent Reinforcement Learning with Murmuration Intelligence and LLM Guidance for Reservoir Management</title>
<link>https://arxiv.org/abs/2509.25034</link>
<guid>https://arxiv.org/abs/2509.25034</guid>
<content:encoded><![CDATA[
arXiv:2509.25034v1 Announce Type: new 
Abstract: As climate change intensifies extreme weather events, water disasters pose growing threats to global communities, making adaptive reservoir management critical for protecting vulnerable populations and ensuring water security. Modern water resource management faces unprecedented challenges from cascading uncertainties propagating through interconnected reservoir networks. These uncertainties, rooted in physical water transfer losses and environmental variability, make precise control difficult. For example, sending 10 tons downstream may yield only 8-12 tons due to evaporation and seepage. Traditional centralized optimization approaches suffer from exponential computational complexity and cannot effectively handle such real-world uncertainties, while existing multi-agent reinforcement learning (MARL) methods fail to achieve effective coordination under uncertainty. To address these challenges, we present MARLIN, a decentralized reservoir management framework inspired by starling murmurations intelligence. Integrating bio-inspired alignment, separation, and cohesion rules with MARL, MARLIN enables individual reservoirs to make local decisions while achieving emergent global coordination. In addition, a LLM provides real-time reward shaping signals, guiding agents to adapt to environmental changes and human-defined preferences. Experiments on real-world USGS data show that MARLIN improves uncertainty handling by 23\%, cuts computation by 35\%, and accelerates flood response by 68\%, exhibiting super-linear coordination, with complexity scaling 5.4x from 400 to 10,000 nodes. These results demonstrate MARLIN's potential for disaster prevention and protecting communities through intelligent, scalable water resource management.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding How Network Geometry Influences Diffusion Processes in Complex Networks: A Focus on Cryptocurrency Blockchains and Critical Infrastructure Networks</title>
<link>https://arxiv.org/abs/2509.23450</link>
<guid>https://arxiv.org/abs/2509.23450</guid>
<content:encoded><![CDATA[
arXiv:2509.23450v1 Announce Type: cross 
Abstract: This study provides essential insights into how diffusion processes unfold in complex networks, with a focus on cryptocurrency blockchains and infrastructure networks. The structural properties of these networks, such as hub-dominated, heavy-tailed topology, network motifs, and node centrality, significantly influence diffusion speed and reach. Using epidemic diffusion models, specifically the Kertesz threshold model and the Susceptible-Infected (SI) model, we analyze key factors affecting diffusion dynamics. To assess the uncertainty in the fraction of infected nodes over time, we employ bootstrap confidence intervals, while Bayesian credible intervals are constructed to quantify parameter uncertainties in the SI models. Our findings reveal substantial variations across different network types, including Erd\H{o}s--R\'enyi networks, Geometric Random Graphs, and Delaunay Triangulation networks, emphasizing the role of network architecture in failure propagation. We identify that network motifs are crucial in diffusion. We highlight that hub-dominated networks, which dominate blockchain ecosystems, provide resilience against random failures but remain vulnerable to targeted attacks, posing significant risks to network stability. Furthermore, centrality measures such as degree, betweenness, and clustering coefficient strongly influence the transmissibility of diffusion in both blockchain and critical infrastructure networks.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TOB-SVD: Total-Order Broadcast with Single-Vote Decisions in the Sleepy Model</title>
<link>https://arxiv.org/abs/2310.11331</link>
<guid>https://arxiv.org/abs/2310.11331</guid>
<content:encoded><![CDATA[
arXiv:2310.11331v3 Announce Type: replace 
Abstract: Over the past years, distributed consensus research has expanded its focus to address challenges in large-scale, permissionless systems, such as blockchains. This shift reflects the need to accommodate dynamic participation, in contrast to the traditional model of a static set of continuously online validators. Works like Bitcoin and the sleepy model have laid the groundwork for this evolving framework.
  Notable contributions by Momose and Ren (CCS 2022), along with subsequent research, have introduced Total-Order Broadcast protocols that leverage Graded Agreement primitives and support dynamic participation. However, these approaches often require multiple phases of voting per decision, which can create a bottleneck for real-world, large-scale systems.
  To address this, our paper introduces TOB-SVD, a novel Total-Order Broadcast protocol in the sleepy model that is resilient to up to 1/2 Byzantine validators. TOB-SVD requires only a single phase of voting per decision in the best case and achieves lower expected latency compared to existing approaches offering the same optimal adversarial resilience. This work paves the way for more practical Total-Order Broadcast protocols that can be implemented in real-world systems involving a large number of validators with fluctuating participation over time.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Analysis of the Nostr Social Network: Decentralization, Availability, and Replication Overhead</title>
<link>https://arxiv.org/abs/2402.05709</link>
<guid>https://arxiv.org/abs/2402.05709</guid>
<content:encoded><![CDATA[
arXiv:2402.05709v2 Announce Type: replace 
Abstract: Nostr is a decentralized social network launched in 2022, emphasizing high availability and censorship resistance. Since launching, it has gained substantial attention, boasting over 100 million posts. Nostr resembles a micro-blogging service like Twitter but with distinct underlying infrastructure. Nostr introduces the concept of relays, which act as open storage servers that receive, store, and distribute user posts. Each user is uniquely identified by a public key, ensuring authenticity of posts through digital signatures. Users are able to securely replicate and retrieve posts through multiple relays, which frees them from single-server reliance and enhances post availability, thereby attempting to make Nostr censorship resistant. However, this aggressive design also presents challenges, such as the overhead required for extensive post replication and the difficulty in obtaining a global view of post replication locations, which remain unexplored or unaddressed. This necessitates a thorough understanding of the Nostr ecosystem; therefore, we conduct the first large-scale study on this topic. Our study focuses on two key aspects: Nostr relays and post replication strategies. We find that Nostr achieves superior decentralization compared to traditional Fediverse applications. However, relay availability remains a challenge, where financial sustainability (particularly for free-to-use relays) emerges as a contributing factor. We also find that the replication of posts across relays enhances censorship-resistance but introduces significant overhead. To address this, we propose two improvements: one to control the number of post replications, and another to reduce the overhead during post retrieval. Via a data-driven evaluation, we demonstrate their ability to reduce overhead without negatively impacting post availability under the simulated scenarios.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BlockFUL: Enabling Unlearning in Blockchained Federated Learning</title>
<link>https://arxiv.org/abs/2402.16294</link>
<guid>https://arxiv.org/abs/2402.16294</guid>
<content:encoded><![CDATA[
arXiv:2402.16294v3 Announce Type: replace 
Abstract: Unlearning in Federated Learning (FL) presents significant challenges, as models grow and evolve with complex inheritance relationships. This complexity is amplified when blockchain is employed to ensure the integrity and traceability of FL, where the need to edit multiple interlinked blockchain records and update all inherited models complicates the process.In this paper, we introduce Blockchained Federated Unlearning (BlockFUL), a novel framework with a dual-chain structure comprising a live chain and an archive chain for enabling unlearning capabilities within Blockchained FL. BlockFUL introduces two new unlearning paradigms, i.e., parallel and sequential paradigms, which can be effectively implemented through gradient-ascent-based and re-training-based unlearning methods. These methods enhance the unlearning process across multiple inherited models by enabling efficient consensus operations and reducing computational costs. Our extensive experiments validate that these methods effectively reduce data dependency and operational overhead, thereby boosting the overall performance of unlearning inherited models within BlockFUL on CIFAR-10 and Fashion-MNIST datasets using AlexNet, ResNet18, and MobileNetV2 models.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fast Confirmation Rule (aka Fast Synchronous Finality) for the Ethereum Consensus Protocol</title>
<link>https://arxiv.org/abs/2405.00549</link>
<guid>https://arxiv.org/abs/2405.00549</guid>
<content:encoded><![CDATA[
arXiv:2405.00549v3 Announce Type: replace 
Abstract: A Confirmation Rule, within blockchain networks, refers to an algorithm implemented by network nodes that determines (either probabilistically or deterministically) the permanence of certain blocks on the blockchain. An example of Confirmation Ruble is the Bitcoin's longest chain Confirmation Rule where a block $b$ is confirmed (with high probability) when it has a sufficiently long chain of successors, its siblings have notably shorter successor chains, the majority of the network's total computation power (hashing) is controlled by honest nodes, and network synchrony holds.
  The only Confirmation Rule currently available in the Ethereum protocol, Gasper, is the FFG Finalization Rule. While this Confirmation Rule works under asynchronous network conditions, it is quite slow for many use cases. Specifically, best-case scenario, it takes around 13 to 19 min to confirm a transaction, where the actual figure depends on when the transaction is submitted to the network.
  In this work, we devise a Fast Confirmation Rule for Ethereum's consensus protocol. Our Confirmation Rule relies on synchrony conditions, but provides a best-case confirmation time of 12 seconds only, greatly improving on the latency of the FFG Finalization Rule.
  Users can then rely on the Confirmation Rule that best suits their needs depending on their belief about the network conditions and the need for a quick response.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PROMFUZZ: Leveraging LLM-Driven and Bug-Oriented Composite Analysis for Detecting Functional Bugs in Smart Contracts</title>
<link>https://arxiv.org/abs/2503.23718</link>
<guid>https://arxiv.org/abs/2503.23718</guid>
<content:encoded><![CDATA[
arXiv:2503.23718v2 Announce Type: replace 
Abstract: Smart contracts are fundamental pillars of the blockchain, playing a crucial role in facilitating various business transactions. However, these smart contracts are vulnerable to exploitable bugs that can lead to substantial monetary losses. A recent study reveals that over 80% of these exploitable bugs, which are primarily functional bugs, can evade the detection of current tools. The primary issue is the significant gap between understanding the high-level logic of the business model and checking the low-level implementations in smart contracts. Furthermore, identifying deeply rooted functional bugs in smart contracts requires the automated generation of effective detection oracles based on various bug features. To address these challenges, we design and implement PROMFUZZ, an automated and scalable system to detect functional bugs, in smart contracts. In PROMFUZZ, we first propose a novel Large Language Model (LLM)-driven analysis framework, which leverages a dual-agent prompt engineering strategy to pinpoint potentially vulnerable functions for further scrutiny. We then implement a dual-stage coupling approach, which focuses on generating invariant checkers that leverage logic information extracted from potentially vulnerable functions. Finally, we design a bug-oriented fuzzing engine, which maps the logical information from the high-level business model to the low-level smart contract implementations, and performs the bug-oriented fuzzing on targeted functions. We compare PROMFUZZ with multiple state-of-the-art methods. The results show that PROMFUZZ achieves 86.96% recall and 93.02% F1-score in detecting functional bugs, marking at least a 50% improvement in both metrics over state-of-the-art methods. Moreover, we perform an in-depth analysis on real-world DeFi projects and detect 30 zero-day bugs. Up to now, 24 zero-day bugs have been assigned CVE IDs.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering the Herd: A Framework for LLM-based Control of Social Learning</title>
<link>https://arxiv.org/abs/2504.02648</link>
<guid>https://arxiv.org/abs/2504.02648</guid>
<content:encoded><![CDATA[
arXiv:2504.02648v3 Announce Type: replace 
Abstract: Algorithms increasingly serve as information mediators--from social media feeds and targeted advertising to the increasing ubiquity of LLMs. This engenders a joint process where agents combine private, algorithmically-mediated signals with learning from peers to arrive at decisions. To study such settings, we introduce a model of controlled sequential social learning in which an information-mediating planner (e.g. an LLM) controls the information structure of agents while they also learn from the decisions of earlier agents. The planner may seek to improve social welfare (altruistic planner) or to induce a specific action the planner prefers (biased planner). Our framework presents a new optimization problem for social learning that combines dynamic programming with decentralized action choices and Bayesian belief updates.
  We prove the convexity of the value function and characterize the optimal policies of altruistic and biased planners, which attain desired tradeoffs between the costs they incur and the payoffs they earn from induced agent choices. Notably, in some regimes the biased planner intentionally obfuscates the agents' signals. Even under stringent transparency constraints--information parity with individuals, no lying or cherry-picking, and full observability--we show that information mediation can substantially shift social welfare in either direction. We complement our theory with simulations in which LLMs act as both planner and agents. Notably, the LLM planner in our simulations exhibits emergent strategic behavior in steering public opinion that broadly mirrors the trends predicted, though key deviations suggest the influence of non-Bayesian reasoning consistent with the cognitive patterns of both humans and LLMs trained on human-like data. Together, we establish our framework as a tractable basis for studying the impact and regulation of LLM information mediators.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Federated Learning Meets Quantum Computing: Survey and Research Opportunities</title>
<link>https://arxiv.org/abs/2504.08814</link>
<guid>https://arxiv.org/abs/2504.08814</guid>
<content:encoded><![CDATA[
arXiv:2504.08814v3 Announce Type: replace 
Abstract: Quantum Federated Learning (QFL) is an emerging field that harnesses advances in Quantum Computing (QC) to improve the scalability and efficiency of decentralized Federated Learning (FL) models. This paper provides a systematic and comprehensive survey of the emerging problems and solutions when FL meets QC, from research protocol to a novel taxonomy, particularly focusing on both quantum and federated limitations, such as their architectures, Noisy Intermediate Scale Quantum (NISQ) devices, and privacy preservation, so on. This work explores key developments and integration strategies, along with the impact of quantum computing on FL, keeping a sharp focus on hybrid quantum-classical approaches. The paper offers an in-depth understanding of how the strengths of QC, such as gradient hiding, state entanglement, quantum key distribution, quantum security, and quantum-enhanced differential privacy, have been integrated into FL to ensure the privacy of participants in an enhanced, fast, and secure framework. Finally, this study proposes potential future directions to address the identified research gaps and challenges, aiming to inspire faster and more secure QFL models for practical use.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Proximal Gradient Method With Probabilistic Multi-Gossip Communications for Decentralized Composite Optimization</title>
<link>https://arxiv.org/abs/2312.11861</link>
<guid>https://arxiv.org/abs/2312.11861</guid>
<content:encoded><![CDATA[
arXiv:2312.11861v3 Announce Type: replace-cross 
Abstract: Decentralized optimization methods with local updates have recently gained attention for their provable ability to communication acceleration. In these methods, nodes perform several iterations of local computations between the communication rounds. Nevertheless, this capability is effective only when the network is sufficiently well-connected and the loss function is smooth. In this paper, we propose a communication-efficient method MG-Skip with probabilistic local updates and multi-gossip communications for decentralized composite (smooth + nonsmooth) optimization, whose stepsize is independent of the number of local updates and the network topology. For any undirected and connected networks, MG-Skip allows for the multi-gossip communications to be skipped in most iterations in the strongly convex setting, while its computation complexity is $\mathcal{O}\left(\kappa \log \frac{1}{\epsilon}\right)$ and communication complexity is only $\mathcal{O}\left(\sqrt{\frac{\kappa}{(1-\rho)}} \log \frac{1}{\epsilon}\right)$, where $\kappa$ is the condition number of the loss function, $\rho$ reflects the connectivity of the network topology, and $\epsilon$ is the target accuracy. The theoretical results indicate that MG-Skip achieves provable communication acceleration, thereby validating the advantages of local updates in the nonsmooth setting.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Learning for Grant-Free Activity Detection in Cell-Free Massive MIMO Networks</title>
<link>https://arxiv.org/abs/2503.11305</link>
<guid>https://arxiv.org/abs/2503.11305</guid>
<content:encoded><![CDATA[
arXiv:2503.11305v3 Announce Type: replace-cross 
Abstract: Grant-free random access (GF-RA) is a promising access technique for massive machine-type communications (mMTC) in future wireless networks, particularly in the context of 5G and beyond (6G) systems. Within the context of GF-RA, this study investigates the efficiency of employing supervised machine learning techniques to tackle the challenges on the device activity detection (AD). GF-RA addresses scalability by employing non-orthogonal pilot sequences, which provides an efficient alternative comparing to conventional grant-based random access (GB-RA) technique that are constrained by the scarcity of orthogonal preamble resources. In this paper, we propose a novel lightweight data-driven algorithmic framework specifically designed for activity detection in GF-RA for mMTC in cell-free massive multiple-input multiple-output (CF-mMIMO) networks. We propose two distinct framework deployment strategies, centralized and decentralized, both tailored to streamline the proposed approach implementation across network infrastructures. Moreover, we introduce optimized post-detection methodologies complemented by a clustering stage to enhance overall detection performances. Our 3GPP-compliant simulations have validated that the proposed algorithm achieves state-of-the-art model-based activity detection accuracy while significantly reducing complexity. Achieving 99% accuracy, it demonstrates real-world viability and effectiveness.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing Ethereum's Geographical (De)Centralization Beyond the Atlantic</title>
<link>https://arxiv.org/abs/2509.21475</link>
<guid>https://arxiv.org/abs/2509.21475</guid>
<content:encoded><![CDATA[
arXiv:2509.21475v1 Announce Type: new 
Abstract: Decentralization has a geographic dimension that conventional metrics such as stake distribution overlook. Where validators run affects resilience to regional shocks (outages, disasters, government intervention) and fairness in reward access. Yet in permissionless systems, locations cannot be mandated, but they emerge from incentives. Today, Ethereum's validators cluster along the Atlantic (EU and U.S. East Coast), where latency is structurally favorable. This raises a key question: when some regions already enjoy latency advantages, how does protocol design shape validator incentives and the geography of (de)centralization? We develop a latency-calibrated agent-based model and compare two Ethereum block-building paradigms: a Single-Source Paradigm (SSP), akin to MEV-Boost, where proposers fetch full blocks from a relay that also propagates them; and a Multi-Source Paradigm (MSP), where proposers aggregate value from multiple sources and broadcast the block themselves. Simulations show that SSP concentrates around relay placement but more slowly, since proximity mainly affects propagation, and the marginal value of time is relatively uniform across regions. MSP centralizes faster: aggregating across sources makes marginal value location-dependent, amplifying payoff dispersion and migration toward latency minima. Source placement and consensus settings can dampen or intensify these effects, though once validators are already clustered, the impact of source placement on decentralization is marginal. In most cases, North America consistently emerges as the focal hub. These findings show that protocol design materially shapes validator geography and offer levers for promoting geographical decentralization.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Indexing to Coding: A New Paradigm for Data Availability Sampling</title>
<link>https://arxiv.org/abs/2509.21586</link>
<guid>https://arxiv.org/abs/2509.21586</guid>
<content:encoded><![CDATA[
arXiv:2509.21586v1 Announce Type: new 
Abstract: The data availability problem is a central challenge in blockchain systems and lies at the core of the accessibility and scalability issues faced by platforms such as Ethereum. Modern solutions employ several approaches, with data availability sampling (DAS) being the most self-sufficient and minimalistic in its security assumptions. Existing DAS methods typically form cryptographic commitments on codewords of fixed-rate erasure codes, which restrict light nodes to sampling from a predetermined set of coded symbols.
  In this paper, we introduce a new approach to DAS that modularizes the coding and commitment process by committing to the uncoded data while performing sampling through on-the-fly coding. The resulting samples are significantly more expressive, enabling light nodes to obtain, in concrete implementations, up to multiple orders of magnitude stronger assurances of data availability than from sampling pre-committed symbols from a fixed-rate redundancy code as done in established DAS schemes using Reed Solomon or low density parity check codes. We present a concrete protocol that realizes this paradigm using random linear network coding (RLNC).
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Agnostic Federated Continual Learning via Replay-Free Gradient Projection</title>
<link>https://arxiv.org/abs/2509.21606</link>
<guid>https://arxiv.org/abs/2509.21606</guid>
<content:encoded><![CDATA[
arXiv:2509.21606v1 Announce Type: new 
Abstract: Federated continual learning (FCL) enables distributed client devices to learn from streaming data across diverse and evolving tasks. A major challenge to continual learning, catastrophic forgetting, is exacerbated in decentralized settings by the data heterogeneity, constrained communication and privacy concerns. We propose Federated gradient Projection-based Continual Learning with Task Identity Prediction (FedProTIP), a novel FCL framework that mitigates forgetting by projecting client updates onto the orthogonal complement of the subspace spanned by previously learned representations of the global model. This projection reduces interference with earlier tasks and preserves performance across the task sequence. To further address the challenge of task-agnostic inference, we incorporate a lightweight mechanism that leverages core bases from prior tasks to predict task identity and dynamically adjust the global model's outputs. Extensive experiments across standard FCL benchmarks demonstrate that FedProTIP significantly outperforms state-of-the-art methods in average accuracy, particularly in settings where task identities are a priori unknown.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NEO-Grid: A Neural Approximation Framework for Optimization and Control in Distribution Grids</title>
<link>https://arxiv.org/abs/2509.21668</link>
<guid>https://arxiv.org/abs/2509.21668</guid>
<content:encoded><![CDATA[
arXiv:2509.21668v1 Announce Type: new 
Abstract: The rise of distributed energy resources (DERs) is reshaping modern distribution grids, introducing new challenges in attaining voltage stability under dynamic and decentralized operating conditions. This paper presents NEO-Grid, a unified learning-based framework for volt-var optimization (VVO) and volt-var control (VVC) that leverages neural network surrogates for power flow and deep equilibrium models (DEQs) for closed-loop control. Our method replaces traditional linear approximations with piecewise-linear ReLU networks trained to capture the nonlinear relationship between power injections and voltage magnitudes. For control, we model the recursive interaction between voltage and inverter response using DEQs, allowing direct fixed-point computation and efficient training via implicit differentiation. We evaluated NEO-Grid on the IEEE 33-bus system, demonstrating that it significantly improves voltage regulation performance compared to standard linear and heuristic baselines in both optimization and control settings. Our results establish NEO-Grid as a scalable, accurate, and interpretable solution for learning-based voltage regulation in distribution grids.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentially-Private Decentralized Learning in Heterogeneous Multicast Networks</title>
<link>https://arxiv.org/abs/2509.21688</link>
<guid>https://arxiv.org/abs/2509.21688</guid>
<content:encoded><![CDATA[
arXiv:2509.21688v1 Announce Type: new 
Abstract: We propose a power-controlled differentially private decentralized learning algorithm designed for a set of clients aiming to collaboratively train a common learning model. The network is characterized by a row-stochastic adjacency matrix, which reflects different channel gains between the clients. In our privacy-preserving approach, both the transmit power for model updates and the level of injected Gaussian noise are jointly controlled to satisfy a given privacy and energy budget. We show that our proposed algorithm achieves a convergence rate of O(log T), where T is the horizon bound in the regret function. Furthermore, our numerical results confirm that our proposed algorithm outperforms existing works.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preference-Guided Learning for Sparse-Reward Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.21828</link>
<guid>https://arxiv.org/abs/2509.21828</guid>
<content:encoded><![CDATA[
arXiv:2509.21828v1 Announce Type: new 
Abstract: We study the problem of online multi-agent reinforcement learning (MARL) in environments with sparse rewards, where reward feedback is not provided at each interaction but only revealed at the end of a trajectory. This setting, though realistic, presents a fundamental challenge: the lack of intermediate rewards hinders standard MARL algorithms from effectively guiding policy learning. To address this issue, we propose a novel framework that integrates online inverse preference learning with multi-agent on-policy optimization into a unified architecture. At its core, our approach introduces an implicit multi-agent reward learning model, built upon a preference-based value-decomposition network, which produces both global and local reward signals. These signals are further used to construct dual advantage streams, enabling differentiated learning targets for the centralized critic and decentralized actors. In addition, we demonstrate how large language models (LLMs) can be leveraged to provide preference labels that enhance the quality of the learned reward model. Empirical evaluations on state-of-the-art benchmarks, including MAMuJoCo and SMACv2, show that our method achieves superior performance compared to existing baselines, highlighting its effectiveness in addressing sparse-reward challenges in online MARL.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Dark Art of Financial Disguise in Web3: Money Laundering Schemes and Countermeasures</title>
<link>https://arxiv.org/abs/2509.21831</link>
<guid>https://arxiv.org/abs/2509.21831</guid>
<content:encoded><![CDATA[
arXiv:2509.21831v1 Announce Type: new 
Abstract: The rise of Web3 and Decentralized Finance (DeFi) has enabled borderless access to financial services empowered by smart contracts and blockchain technology. However, the ecosystem's trustless, permissionless, and borderless nature presents substantial regulatory challenges. The absence of centralized oversight and the technical complexity create fertile ground for financial crimes. Among these, money laundering is particularly concerning, as in the event of successful scams, code exploits, and market manipulations, it facilitates covert movement of illicit gains. Beyond this, there is a growing concern that cryptocurrencies can be leveraged to launder proceeds from drug trafficking, or to transfer funds linked to terrorism financing.
  This survey aims to outline a taxonomy of high-level strategies and underlying mechanisms exploited to facilitate money laundering in Web3. We examine how criminals leverage the pseudonymous nature of Web3, alongside weak regulatory frameworks, to obscure illicit financial activities. Our study seeks to bridge existing knowledge gaps on laundering schemes, identify open challenges in the detection and prevention of such activities, and propose future research directions to foster a more transparent Web3 financial ecosystem -- offering valuable insights for researchers, policymakers, and industry practitioners.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Path Finding via Offline RL and LLM Collaboration</title>
<link>https://arxiv.org/abs/2509.22130</link>
<guid>https://arxiv.org/abs/2509.22130</guid>
<content:encoded><![CDATA[
arXiv:2509.22130v1 Announce Type: new 
Abstract: Multi-Agent Path Finding (MAPF) poses a significant and challenging problem critical for applications in robotics and logistics, particularly due to its combinatorial complexity and the partial observability inherent in realistic environments. Decentralized reinforcement learning methods commonly encounter two substantial difficulties: first, they often yield self-centered behaviors among agents, resulting in frequent collisions, and second, their reliance on complex communication modules leads to prolonged training times, sometimes spanning weeks. To address these challenges, we propose an efficient decentralized planning framework based on the Decision Transformer (DT), uniquely leveraging offline reinforcement learning to substantially reduce training durations from weeks to mere hours. Crucially, our approach effectively handles long-horizon credit assignment and significantly improves performance in scenarios with sparse and delayed rewards. Furthermore, to overcome adaptability limitations inherent in standard RL methods under dynamic environmental changes, we integrate a large language model (GPT-4o) to dynamically guide agent policies. Extensive experiments in both static and dynamically changing environments demonstrate that our DT-based approach, augmented briefly by GPT-4o, significantly enhances adaptability and performance.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiency Boost in Decentralized Optimization: Reimagining Neighborhood Aggregation with Minimal Overhead</title>
<link>https://arxiv.org/abs/2509.22174</link>
<guid>https://arxiv.org/abs/2509.22174</guid>
<content:encoded><![CDATA[
arXiv:2509.22174v1 Announce Type: new 
Abstract: In today's data-sensitive landscape, distributed learning emerges as a vital tool, not only fortifying privacy measures but also streamlining computational operations. This becomes especially crucial within fully decentralized infrastructures where local processing is imperative due to the absence of centralized aggregation. Here, we introduce DYNAWEIGHT, a novel framework to information aggregation in multi-agent networks. DYNAWEIGHT offers substantial acceleration in decentralized learning with minimal additional communication and memory overhead. Unlike traditional static weight assignments, such as Metropolis weights, DYNAWEIGHT dynamically allocates weights to neighboring servers based on their relative losses on local datasets. Consequently, it favors servers possessing diverse information, particularly in scenarios of substantial data heterogeneity. Our experiments on various datasets MNIST, CIFAR10, and CIFAR100 incorporating various server counts and graph topologies, demonstrate notable enhancements in training speeds. Notably, DYNAWEIGHT functions as an aggregation scheme compatible with any underlying server-level optimization algorithm, underscoring its versatility and potential for widespread integration.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>I-ETL: an interoperability-aware health (meta) data pipeline to enable federated analyses</title>
<link>https://arxiv.org/abs/2509.22351</link>
<guid>https://arxiv.org/abs/2509.22351</guid>
<content:encoded><![CDATA[
arXiv:2509.22351v1 Announce Type: new 
Abstract: Clinicians are interested in better understanding complex diseases, such as cancer or rare diseases, so they need to produce and exchange data to mutualize sources and join forces. To do so and ensure privacy, a natural way consists in using a decentralized architecture and Federated Learning algorithms. This ensures that data stays in the organization in which it has been collected, but requires data to be collected in similar settings and similar models. In practice, this is often not the case because healthcare institutions work individually with different representations and raw data; they do not have means to normalize their data, and even less to do so across centers. For instance, clinicians have at hand phenotypic, clinical, imaging and genomic data (each individually collected) and want to better understand some diseases by analyzing them together. This example highlights the needs and challenges for a cooperative use of this wealth of information. We designed and implemented a framework, named I-ETL, for integrating highly heterogeneous healthcare datasets of hospitals in interoperable databases. Our proposal is twofold: (i) we devise two general and extensible conceptual models for modeling both data and metadata and (ii) we propose an Extract-Transform-Load (ETL) pipeline ensuring and assessing interoperability from the start. By conducting experiments on open-source datasets, we show that I-ETL succeeds in representing various health datasets in a unified way thanks to our two general conceptual models. Next, we demonstrate the importance of blending interoperability as a first-class citizen in integration pipelines, ensuring possible collaboration between different centers.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Aware Multi-Robot Task Allocation With Strongly Coupled Inter-Robot Rewards</title>
<link>https://arxiv.org/abs/2509.22469</link>
<guid>https://arxiv.org/abs/2509.22469</guid>
<content:encoded><![CDATA[
arXiv:2509.22469v1 Announce Type: new 
Abstract: This paper proposes a task allocation algorithm for teams of heterogeneous robots in environments with uncertain task requirements. We model these requirements as probability distributions over capabilities and use this model to allocate tasks such that robots with complementary skills naturally position near uncertain tasks, proactively mitigating task failures without wasting resources. We introduce a market-based approach that optimizes the joint team objective while explicitly capturing coupled rewards between robots, offering a polynomial-time solution in decentralized settings with strict communication assumptions. Comparative experiments against benchmark algorithms demonstrate the effectiveness of our approach and highlight the challenges of incorporating coupled rewards in a decentralized formulation.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Dual-Mode Distillation with Incentive Schemes for Scalable, Heterogeneous Federated Learning on Non-IID Data</title>
<link>https://arxiv.org/abs/2509.22507</link>
<guid>https://arxiv.org/abs/2509.22507</guid>
<content:encoded><![CDATA[
arXiv:2509.22507v1 Announce Type: new 
Abstract: Federated Learning (FL) has emerged as a promising decentralized learning (DL) approach that enables the use of distributed data without compromising user privacy. However, FL poses several key challenges. First, it is frequently assumed that every client can train the same machine learning models, however, not all clients are able to meet this assumption because of differences in their business needs and computational resources. Second, statistical heterogeneity (a.k.a. non-IID data) poses a major challenge in FL, which can lead to lower global model performance. Third, while addressing these challenges, there is a need for a cost-effective incentive mechanism to encourage clients to participate in FL training. In response to these challenges, we propose several methodologies: DL-SH, which facilitates efficient, privacy-preserving, and communication-efficient learning in the context of statistical heterogeneity; DL-MH, designed to manage fully heterogeneous models while tackling statistical disparities; and I-DL-MH, an incentive-based extension of DL-MH that promotes client engagement in federated learning training by providing incentives within this complex federated learning framework. Comprehensive experiments were carried out to assess the performance and scalability of the proposed approaches across a range of complex experimental settings. This involved utilizing various model architectures, in diverse data distributions, including IID and several non-IID scenarios, as well as multiple datasets. Experimental results demonstrate that the proposed approaches significantly enhance accuracy and decrease communication costs while effectively addressing statistical heterogeneity and model heterogeneity in comparison to existing state-of-the-art approaches and baselines, with DL-SH improving global model accuracy by 153%, and I-DL-MH achieving a 225% improvement under non-IID conditions.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Technical Capability and User Accessibility: Off-grid Civilian Emergency Communication</title>
<link>https://arxiv.org/abs/2509.22568</link>
<guid>https://arxiv.org/abs/2509.22568</guid>
<content:encoded><![CDATA[
arXiv:2509.22568v1 Announce Type: new 
Abstract: During large-scale crises disrupting cellular and Internet infrastructure, civilians lack reliable methods for communication, aid coordination, and access to trustworthy information. This paper presents a unified emergency communication system integrating a low-power, long-range network with a crisis-oriented smartphone application, enabling decentralized and off-grid civilian communication. Unlike previous solutions separating physical layer resilience from user layer usability, our design merges these aspects into a cohesive crisis-tailored framework.
  The system is evaluated in two dimensions: communication performance and application functionality. Field experiments in urban Z\"urich demonstrate that the 868 MHz band, using the LongFast configuration, achieves a communication range of up to 1.2 km with 92% Packet Delivery Ratio, validating network robustness under real-world infrastructure degraded conditions. In parallel, a purpose-built mobile application featuring peer-to-peer messaging, identity verification, and community moderation was evaluated through a requirements-based analysis.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voting-Bloc Entropy: A New Metric for DAO Decentralization</title>
<link>https://arxiv.org/abs/2509.22620</link>
<guid>https://arxiv.org/abs/2509.22620</guid>
<content:encoded><![CDATA[
arXiv:2509.22620v1 Announce Type: new 
Abstract: Decentralized Autonomous Organizations (DAOs) use smart contracts to foster communities working toward common goals. Existing definitions of decentralization, however -- the 'D' in DAO -- fall short of capturing the key properties characteristic of diverse and equitable participation. This work proposes a new framework for measuring DAO decentralization called Voting-Bloc Entropy (VBE, pronounced ''vibe''). VBE is based on the idea that voters with closely aligned interests act as a centralizing force and should be modeled as such. VBE formalizes this notion by measuring the similarity of participants' utility functions across a set of voting rounds. Unlike prior, ad hoc definitions of decentralization, VBE derives from first principles: We introduce a simple (yet powerful) reinforcement learning-based conceptual model for voting, that in turn implies VBE. We first show VBE's utility as a theoretical tool. We prove a number of results about the (de)centralizing effects of vote delegation, proposal bundling, bribery, etc. that are overlooked in previous notions of DAO decentralization. Our results lead to practical suggestions for enhancing DAO decentralization. We also show how VBE can be used empirically by presenting measurement studies and VBE-based governance experiments. We make the tools we developed for these results available to the community in the form of open-source artifacts in order to facilitate future study of DAO decentralization.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linear Risk Sharing on Networks</title>
<link>https://arxiv.org/abs/2509.21411</link>
<guid>https://arxiv.org/abs/2509.21411</guid>
<content:encoded><![CDATA[
arXiv:2509.21411v1 Announce Type: cross 
Abstract: Over the past decade alternatives to traditional insurance and banking have grown in popularity. The desire to encourage local participation has lead products such as peer-to-peer insurance, reciprocal contracts, and decentralized finance platforms to increasingly rely on network structures to redistribute risk among participants. In this paper, we develop a comprehensive framework for linear risk sharing (LRS), where random losses are reallocated through nonnegative linear operators which can accommodate a wide range of networks. Building on the theory of stochastic and doubly stochastic matrices, we establish conditions under which constraints such as budget balance, fairness, and diversification are guaranteed. The convex order framework allows us to compare different allocations rigorously, highlighting variance reduction and majorization as natural consequences of doubly stochastic mixing. We then extend the analysis to network-based sharing, showing how their topology shapes risk outcomes in complete, star, ring, random, and scale-free graphs. A second layer of randomness, where the sharing matrix itself is random, is introduced via Erd\H{o}s--R\'enyi and preferential-attachment networks, connecting risk-sharing properties to degree distributions. Finally, we study convex combinations of identity and network-induced operators, capturing the trade-off between self-retention and diversification. Our results provide design principles for fair and efficient peer-to-peer insurance and network-based risk pooling, combining mathematical soundness with economic interpretability.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proving and Rewarding Client Diversity to Strengthen Resilience of Blockchain Networks</title>
<link>https://arxiv.org/abs/2411.18401</link>
<guid>https://arxiv.org/abs/2411.18401</guid>
<content:encoded><![CDATA[
arXiv:2411.18401v2 Announce Type: replace 
Abstract: Client diversity is a cornerstone of blockchain resilience, yet most networks suffer from a dangerously skewed distribution of client implementations. This monoculture exposes the network to very risky scenarios, such as massive financial losses in the event of a majority client failure. In this paper, we present a novel framework that combines verifiable execution and economic incentives to provably identify and reward the use of minority clients, thereby promoting a healthier, more robust ecosystem. Our approach leverages state-of-the-art verifiable computation (zkVMs and TEEs) to generate cryptographic proofs of client execution, which are then verified on-chain. We design and implement an end-to-end prototype of verifiable client diversity in the context of Ethereum, by modifying the popular Lighthouse client and by deploying our novel diversity-aware reward protocol. Through comprehensive experiments, we quantify the practicality of our approach, from overheads of proof production and verification to the effectiveness of the incentive mechanism. This work demonstrates, for the first time, a practical and economically viable path to encourage and ensure provable client diversity in blockchain networks. Our findings inform the design of future protocols that seek to maximize the resilience of decentralized systems
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond A Single AI Cluster: A Survey of Decentralized LLM Training</title>
<link>https://arxiv.org/abs/2503.11023</link>
<guid>https://arxiv.org/abs/2503.11023</guid>
<content:encoded><![CDATA[
arXiv:2503.11023v3 Announce Type: replace 
Abstract: The emergence of large language models (LLMs) has revolutionized AI development, yet the resource demands beyond a single cluster or even datacenter, limiting accessibility to well-resourced organizations. Decentralized training has emerged as a promising paradigm to leverage dispersed resources across clusters, datacenters and regions, offering the potential to democratize LLM development for broader communities. As the first comprehensive exploration of this emerging field, we present decentralized LLM training as a resource-driven paradigm and categorize existing efforts into community-driven and organizational approaches. We further clarify this through: (1) a comparison with related paradigms, (2) a characterization of decentralized resources, and (3) a taxonomy of recent advancements. We also provide up-to-date case studies and outline future directions to advance research in decentralized LLM training.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedOC: Multi-Server FL with Overlapping Client Relays in Wireless Edge Networks</title>
<link>https://arxiv.org/abs/2509.19398</link>
<guid>https://arxiv.org/abs/2509.19398</guid>
<content:encoded><![CDATA[
arXiv:2509.19398v1 Announce Type: new 
Abstract: Multi-server Federated Learning (FL) has emerged as a promising solution to mitigate communication bottlenecks of single-server FL. We focus on a typical multi-server FL architecture, where the regions covered by different edge servers (ESs) may overlap. A key observation of this architecture is that clients located in the overlapping areas can access edge models from multiple ESs. Building on this insight, we propose FedOC (Federated learning with Overlapping Clients), a novel framework designed to fully exploit the potential of these overlapping clients. In FedOC, overlapping clients could serve dual roles: (1) as Relay Overlapping Clients (ROCs), they forward edge models between neighboring ESs in real time to facilitate model sharing among different ESs; and (2) as Normal Overlapping Clients (NOCs), they dynamically select their initial model for local training based on the edge model delivery time, which enables indirect data fusion among different regions of ESs. The overall FedOC workflow proceeds as follows: in every round, each client trains local model based on the earliest received edge model and transmits to the respective ESs for model aggregation. Then each ES transmits the aggregated edge model to neighboring ESs through ROC relaying. Upon receiving the relayed models, each ES performs a second aggregation and subsequently broadcasts the updated model to covered clients. The existence of ROCs enables the model of each ES to be disseminated to the other ESs in a decentralized manner, which indirectly achieves intercell model and speeding up the training process, making it well-suited for latency-sensitive edge environments. Extensive experimental results show remarkable performance gains of our scheme compared to existing methods.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Sharding Advancements, Methodologies, and Adoption Potential in Hedera</title>
<link>https://arxiv.org/abs/2509.19478</link>
<guid>https://arxiv.org/abs/2509.19478</guid>
<content:encoded><![CDATA[
arXiv:2509.19478v1 Announce Type: new 
Abstract: Sharding has emerged as a critical solution to address the scalability challenges faced by blockchain networks, enabling them to achieve higher transaction throughput, reduced latency, and optimized resource usage. This paper investigates the advancements, methodologies, and adoption potential of sharding in the context of Hedera, a distributed ledger technology known for its unique Gossip about Gossip protocol and asynchronous Byzantine Fault Tolerance (ABFT). We explore various academic and industrial sharding techniques, emphasizing their benefits and trade-offs. Building on these insights, we propose a hybrid sharding solution for Hedera that partitions the network into local and global committees, facilitating efficient cross-shard transactions and ensuring robust security through dynamic reconfiguration. Our analysis highlights significant reductions in storage and communication overhead, improved scalability, and enhanced fault tolerance, demonstrating the feasibility and advantages of integrating sharding into Hedera's architecture.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Aggregation of Demand Flexibility</title>
<link>https://arxiv.org/abs/2509.19612</link>
<guid>https://arxiv.org/abs/2509.19612</guid>
<content:encoded><![CDATA[
arXiv:2509.19612v1 Announce Type: new 
Abstract: This paper proposes a federated framework for demand flexibility aggregation to support grid operations. Unlike existing geometric methods that rely on a static, pre-defined base set as the geometric template for aggregation, our framework establishes a true federated process by enabling the collaborative optimization of this base set without requiring the participants sharing sensitive data with the aggregator. Specifically, we first formulate the base set optimization problem as a bilevel program. Using optimal solution functions, we then reformulate the bilevel program into a single-level, unconstrained learning task. By exploiting the decomposable structure of the overall gradient, we further design a decentralized gradient-based algorithm to solve this learning task. The entire framework, encompassing base set optimization, aggregation, and disaggregation, operates by design without exchanging raw user data. Numerical results demonstrate that our proposed framework unlocks substantially more flexibility than the approaches with static base sets, thus providing a promising framework for efficient and privacy-enhanced approaches to coordinate demand flexibility at scale.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Governing Together: Toward Infrastructure for Community-Run Social Media</title>
<link>https://arxiv.org/abs/2509.19653</link>
<guid>https://arxiv.org/abs/2509.19653</guid>
<content:encoded><![CDATA[
arXiv:2509.19653v1 Announce Type: new 
Abstract: Decentralizing the governance of social computing systems to communities promises to empower them to make independent decisions, with nuance and in accordance with their values. Yet, communities do not govern in isolation. Many problems communities face are common, or move across their boundaries. We therefore propose designing for "inter-community governance:" mechanisms that support relationships and interactions between communities to coordinate on governance issues. Drawing from workshops with 24 individuals on decentralized, community-run social media, we present six challenges in designing for inter-community governance surfaced through ideas proposed in workshops. Together, these ideas come together as an ecosystem of resources, infrastructures, and tools that highlight three key principles for designing for inter-community governance: modularity, forkability, and polycentricity. We end with a discussion of how the ideas proposed in workshops might be implemented in future work aiming to support community governance in social computing systems broadly.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Orbital Stabilization and Time Synchronization of Unstable Periodic Motions in Underactuated Robots</title>
<link>https://arxiv.org/abs/2509.20082</link>
<guid>https://arxiv.org/abs/2509.20082</guid>
<content:encoded><![CDATA[
arXiv:2509.20082v1 Announce Type: new 
Abstract: This paper presents a control methodology for achieving orbital stabilization with simultaneous time synchronization of periodic trajectories in underactuated robotic systems. The proposed approach extends the classical transverse linearization framework to explicitly incorporate time-desynchronization dynamics. To stabilize the resulting extended transverse dynamics, we employ a combination of time-varying LQR and sliding-mode control. The theoretical results are validated experimentally through the implementation of both centralized and decentralized control strategies on a group of six Butterfly robots.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>chainScale: Secure Functionality-oriented Scalability for Decentralized Resource Markets</title>
<link>https://arxiv.org/abs/2509.20356</link>
<guid>https://arxiv.org/abs/2509.20356</guid>
<content:encoded><![CDATA[
arXiv:2509.20356v1 Announce Type: new 
Abstract: Decentralized resource markets are Web 3.0 applications that build open-access platforms for trading digital resources among users without any central management. They promise cost reduction, transparency, and flexible service provision. However, these markets usually have large workload that must be processed in a timely manner, leading to serious scalability problems. Despite the large amount of work on blockchain scalability, existing solutions are ineffective as they do not account for these markets' work models and traffic patterns.
  We introduce chainScale, a secure hybrid sidechain-sharding solution that aims to boost throughput of decentralized resource markets and reduce their latency and storage footprint. At its core, chainScale leverages dependent sidechains and functionality-oriented workload splitting to parallelize traffic processing by having each market module assigned to a sidechain. Different from sharding, chainScale does not incur any cross-sidechain transactions that tend to be costly. chainScale introduces several techniques, including hierarchical workload sharing that further sub-divides overloaded modules, and weighted miner assignment that assigns miners with vested interest in the system to critical modules' sidechains. Furthermore, chainScale employs sidechain syncing to maintain the mainchain as the single truth of system state, and pruning to discard stale records. Beside analyzing security, we build a proof-of-concept implementation for a distributed file storage market as a use case. Our experiments show that, compared to a single sidechain-based prior solution, chainScale boosts throughput by 4x and reduces confirmation latency by 5x. Also, they show that chainScale outperforms sharding by 2.5x in throughput and 3.5x in latency.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proof-of-Social-Capital: A Consensus Protocol Replacing Stake for Social Capital</title>
<link>https://arxiv.org/abs/2505.12144</link>
<guid>https://arxiv.org/abs/2505.12144</guid>
<content:encoded><![CDATA[
arXiv:2505.12144v2 Announce Type: replace 
Abstract: Consensus protocols used today in blockchains mostly rely on scarce resources such as computational power or financial stake, favoring wealthy individuals due to a high entry barrier. We propose Proof-of-Social-Capital (PoSC), a new consensus protocol fueled by social capital as a staking resource to ensure fairness and decentralization. Consensus nodes in our system do not require financial or computational resources that are expensive to acquire; instead, they require preexisting social media influence, distributing consensus power not according to wealth but social capital. Our approach integrates zkSNARK proofs, verifiable credentials with a uniqueness-enforcing mechanism to prevent Sybil attacks, and the incentive scheme that rewards engagement with social media content by followers. This work offers a new concept aligned with modern social media lifestyle applied in finance, providing a practical insight for the evolution of decentralized consensus protocols.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Learning Strategies for Estimation Error Minimization with Graph Neural Networks</title>
<link>https://arxiv.org/abs/2404.03227</link>
<guid>https://arxiv.org/abs/2404.03227</guid>
<content:encoded><![CDATA[
arXiv:2404.03227v3 Announce Type: replace-cross 
Abstract: We address the challenge of sampling and remote estimation for autoregressive Markovian processes in a multi-hop wireless network with statistically-identical agents. Agents cache the most recent samples from others and communicate over wireless collision channels governed by an underlying graph topology. Our goal is to minimize time-average estimation error and/or age of information with decentralized scalable sampling and transmission policies, considering both oblivious (where decision-making is independent of the physical processes) and non-oblivious policies (where decision-making depends on physical processes). We prove that in oblivious policies, minimizing estimation error is equivalent to minimizing the age of information. The complexity of the problem, especially the multi-dimensional action spaces and arbitrary network topologies, makes theoretical methods for finding optimal transmission policies intractable. We optimize the policies using a graphical multi-agent reinforcement learning framework, where each agent employs a permutation-equivariant graph neural network architecture. Theoretically, we prove that our proposed framework exhibits desirable transferability properties, allowing transmission policies trained on small- or moderate-size networks to be executed effectively on large-scale topologies. Numerical experiments demonstrate that (i) Our proposed framework outperforms state-of-the-art baselines; (ii) The trained policies are transferable to larger networks, and their performance gains increase with the number of agents; (iii) The training procedure withstands non-stationarity even if we utilize independent learning techniques; and, (iv) Recurrence is pivotal in both independent learning and centralized training and decentralized execution, and improves the resilience to non-stationarity in independent learning.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentor-V: Lightweight ML Training on Low-Power RISC-V Edge Devices</title>
<link>https://arxiv.org/abs/2509.18118</link>
<guid>https://arxiv.org/abs/2509.18118</guid>
<content:encoded><![CDATA[
arXiv:2509.18118v1 Announce Type: new 
Abstract: Modern IoT devices increasingly rely on machine learning solutions to process data locally. However, the lack of graphics processing units (GPUs) or dedicated accelerators on most platforms makes on-device training largely infeasible, often requiring cloud-based services to perform this task. This procedure often raises privacy-related concerns, and creates dependency on reliable and always-on connectivity. Federated Learning (FL) is a new trend that addresses these issues by enabling decentralized and collaborative training directly on devices, but it requires highly efficient optimization algorithms. L-SGD, a lightweight variant of stochastic gradient descent, has enabled neural network training on Arm Cortex-M Microcontroller Units (MCUs). This work extends L-SGD to RISC-V-based MCUs, an open and emerging architecture that still lacks robust support for on-device training. L-SGD was evaluated on both Arm and RISC-V platforms using 32-bit floating-point arithmetic, highlighting the performance impact of the absence of Floating-Point Units (FPUs) in RISC-V MCUs. To mitigate these limitations, we introduce an 8-bit quantized version of L-SGD for RISC-V, which achieves nearly 4x reduction in memory usage and a 2.2x speedup in training time, with negligible accuracy degradation.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anomaly Detection in Electric Vehicle Charging Stations Using Federated Learning</title>
<link>https://arxiv.org/abs/2509.18126</link>
<guid>https://arxiv.org/abs/2509.18126</guid>
<content:encoded><![CDATA[
arXiv:2509.18126v1 Announce Type: new 
Abstract: Federated Learning (FL) is a decentralized training framework widely used in IoT ecosystems that preserves privacy by keeping raw data local, making it ideal for IoT-enabled cyber-physical systems with sensing and communication like Smart Grids (SGs), Connected and Automated Vehicles (CAV), and Electric Vehicle Charging Stations (EVCS). With the rapid expansion of electric vehicle infrastructure, securing these IoT-based charging stations against cyber threats has become critical. Centralized Intrusion Detection Systems (IDS) raise privacy concerns due to sensitive network and user data, making FL a promising alternative. However, current FL-based IDS evaluations overlook practical challenges such as system heterogeneity and non-IID data. To address these challenges, we conducted experiments to evaluate the performance of federated learning for anomaly detection in EV charging stations under system and data heterogeneity. We used FedAvg and FedAvgM, widely studied optimization approaches, to analyze their effectiveness in anomaly detection. Under IID settings, FedAvg achieves superior performance to centralized models using the same neural network. However, performance degrades with non-IID data and system heterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous settings, showing better convergence and higher anomaly detection accuracy. Our results demonstrate that FL can handle heterogeneity in IoT-based EVCS without significant performance loss, with FedAvgM as a promising solution for robust, privacy-preserving EVCS security.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Visual Deepfake Detection: Can AI Predict and Prevent Fake Content Before It's Created?</title>
<link>https://arxiv.org/abs/2509.18461</link>
<guid>https://arxiv.org/abs/2509.18461</guid>
<content:encoded><![CDATA[
arXiv:2509.18461v1 Announce Type: new 
Abstract: Generative adversarial networks (GANs) and diffusion models have dramatically advanced deepfake technology, and its threats to digital security, media integrity, and public trust have increased rapidly. This research explored zero-shot deepfake detection, an emerging method even when the models have never seen a particular deepfake variation. In this work, we studied self-supervised learning, transformer-based zero-shot classifier, generative model fingerprinting, and meta-learning techniques that better adapt to the ever-evolving deepfake threat. In addition, we suggested AI-driven prevention strategies that mitigated the underlying generation pipeline of the deepfakes before they occurred. They consisted of adversarial perturbations for creating deepfake generators, digital watermarking for content authenticity verification, real-time AI monitoring for content creation pipelines, and blockchain-based content verification frameworks. Despite these advancements, zero-shot detection and prevention faced critical challenges such as adversarial attacks, scalability constraints, ethical dilemmas, and the absence of standardized evaluation benchmarks. These limitations were addressed by discussing future research directions on explainable AI for deepfake detection, multimodal fusion based on image, audio, and text analysis, quantum AI for enhanced security, and federated learning for privacy-preserving deepfake detection. This further highlighted the need for an integrated defense framework for digital authenticity that utilized zero-shot learning in combination with preventive deepfake mechanisms. Finally, we highlighted the important role of interdisciplinary collaboration between AI researchers, cybersecurity experts, and policymakers to create resilient defenses against the rising tide of deepfake attacks.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Agent Access (A\^3) Network: An Embodied, Communication-Aware Multi-Agent Framework for 6G Coverage</title>
<link>https://arxiv.org/abs/2509.18526</link>
<guid>https://arxiv.org/abs/2509.18526</guid>
<content:encoded><![CDATA[
arXiv:2509.18526v1 Announce Type: new 
Abstract: The vision of 6G communication demands autonomous and resilient networking in environments without fixed infrastructure. Yet most multi-agent reinforcement learning (MARL) approaches focus on isolated stages - exploration, relay formation, or access - under static deployments and centralized control, limiting adaptability. We propose the AI Agent Access (A\^3) Network, a unified, embodied intelligence-driven framework that transforms multi-agent networking into a dynamic, decentralized, and end-to-end system. Unlike prior schemes, the A\^3 Network integrates exploration, target user access, and backhaul maintenance within a single learning process, while supporting on-demand agent addition during runtime. Its decentralized policies ensure that even a single agent can operate independently with limited observations, while coordinated agents achieve scalable, communication-optimized coverage. By embedding link-level communication metrics into actor-critic learning, the A\^3 Network couples topology formation with robust decision-making. Numerical simulations demonstrate that the A\^3 Network not only balances exploration and communication efficiency but also delivers system-level adaptability absent in existing MARL frameworks, offering a new paradigm for 6G multi-agent networks.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining I2P Resilience: Effect of Centrality-based Attack</title>
<link>https://arxiv.org/abs/2509.18572</link>
<guid>https://arxiv.org/abs/2509.18572</guid>
<content:encoded><![CDATA[
arXiv:2509.18572v1 Announce Type: new 
Abstract: This study examines the robustness of I2P, a well-regarded anonymous and decentralized peer-to-peer network designed to ensure anonymity, confidentiality, and circumvention of censorship. Unlike its more widely researched counterpart, TOR, I2P's resilience has received less scholarly attention. Employing network analysis, this research evaluates I2P's susceptibility to adversarial percolation. By utilizing the degree centrality as a measure of nodes' influence in the network, the finding suggests the network is vulnerable to targeted disruptions. Before percolation, the network exhibited a density of 0.01065443 and an average path length of 6.842194. At the end of the percolation process, the density decreased by approximately 10%, and the average path length increased by 33%, indicating a decline in efficiency and connectivity. These results highlight that even decentralized networks, such as I2P, exhibit structural fragility under targeted attacks, emphasizing the need for improved design strategies to enhance resilience against adversarial disruptions.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smart Cellular Bricks for Decentralized Shape Classification and Damage Recovery</title>
<link>https://arxiv.org/abs/2509.18659</link>
<guid>https://arxiv.org/abs/2509.18659</guid>
<content:encoded><![CDATA[
arXiv:2509.18659v1 Announce Type: new 
Abstract: Biological systems possess remarkable capabilities for self-recognition and morphological regeneration, often relying solely on local interactions. Inspired by these decentralized processes, we present a novel system of physical 3D bricks--simple cubic units equipped with local communication, processing, and sensing--that are capable of inferring their global shape class and detecting structural damage. Leveraging Neural Cellular Automata (NCA), a learned, fully-distributed algorithm, our system enables each module to independently execute the same neural network without access to any global state or positioning information. We demonstrate the ability of collections of hundreds of these cellular bricks to accurately classify a variety of 3D shapes through purely local interactions. The approach shows strong robustness to out-of-distribution shape variations and high tolerance to communication faults and failed modules. In addition to shape inference, the same decentralized framework is extended to detect missing or damaged components, allowing the collective to localize structural disruptions and to guide a recovery process. This work provides a physical realization of large-scale, decentralized self-recognition and damage detection, advancing the potential of robust, adaptive, and bio-inspired modular systems.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generic Adversarial Smart Contract Detection with Semantics and Uncertainty-Aware LLM</title>
<link>https://arxiv.org/abs/2509.18934</link>
<guid>https://arxiv.org/abs/2509.18934</guid>
<content:encoded><![CDATA[
arXiv:2509.18934v1 Announce Type: new 
Abstract: Adversarial smart contracts, mostly on EVM-compatible chains like Ethereum and BSC, are deployed as EVM bytecode to exploit vulnerable smart contracts typically for financial gains. Detecting such malicious contracts at the time of deployment is an important proactive strategy preventing loss from victim contracts. It offers a better cost-benefit than detecting vulnerabilities on diverse potential victims. However, existing works are not generic with limited detection types and effectiveness due to imbalanced samples, while the emerging LLM technologies, which show its potentials in generalization, have two key problems impeding its application in this task: hard digestion of compiled-code inputs, especially those with task-specific logic, and hard assessment of LLMs' certainty in their binary answers, i.e., yes-or-no answers. Therefore, we propose a generic adversarial smart contracts detection framework FinDet, which leverages LLMs with two enhancements addressing above two problems. FinDet takes as input only the EVM-bytecode contracts and identifies adversarial ones among them with high balanced accuracy. The first enhancement extracts concise semantic intentions and high-level behavioral logic from the low-level bytecode inputs, unleashing the LLM reasoning capability restricted by the task input. The second enhancement probes and measures the LLM uncertainty to its multi-round answering to the same query, improving the LLM answering robustness for binary classifications required by the task output. Our comprehensive evaluation shows that FinDet achieves a BAC of 0.9223 and a TPR of 0.8950, significantly outperforming existing baselines. It remains robust under challenging conditions including unseen attack patterns, low-data settings, and feature obfuscation. FinDet detects all 5 public and 20+ unreported adversarial contracts in a 10-day real-world test, confirmed manually.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAPPO for Edge Server Monitoring</title>
<link>https://arxiv.org/abs/2509.19079</link>
<guid>https://arxiv.org/abs/2509.19079</guid>
<content:encoded><![CDATA[
arXiv:2509.19079v1 Announce Type: new 
Abstract: In this paper, we consider a goal-oriented communication problem for edge server monitoring, where jobs arrive intermittently at multiple dispatchers and must be assigned to shared edge servers with finite queues and time-varying availability. Accurate knowledge of server status is critical for sustaining high throughput, yet remains challenging under dynamic workloads and partial observability. To address this challenge, each dispatcher maintains server knowledge through two complementary mechanisms: (i) active status queries that provide instantaneous updates at a communication cost, and (ii) job execution feedback that reveals server conditions opportunistically. We formulate a cooperative multi-agent distributed decision-making problem in which dispatchers jointly optimize query scheduling to balance throughput against communication overhead. To solve this problem, we propose a Multi-Agent Proximal Policy Optimization (MAPPO)-based algorithm that leverages centralized training with decentralized execution (CTDE) to learn distributed query-and-dispatch policies under partial and stale observations. Numerical evaluations show that MAPPO achieves superior throughput-cost tradeoffs and significantly outperforms baseline strategies, achieving on average a 30% improvement over the closest baseline.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs as verification oracles for Solidity</title>
<link>https://arxiv.org/abs/2509.19153</link>
<guid>https://arxiv.org/abs/2509.19153</guid>
<content:encoded><![CDATA[
arXiv:2509.19153v1 Announce Type: new 
Abstract: Ensuring the correctness of smart contracts is critical, as even subtle flaws can lead to severe financial losses. While bug detection tools able to spot common vulnerability patterns can serve as a first line of defense, most real-world exploits and losses stem from errors in the contract business logic. Formal verification tools such as SolCMC and the Certora Prover address this challenge, but their impact remains limited by steep learning curves and restricted specification languages. Recent works have begun to explore the use of large language models (LLMs) for security-related tasks such as vulnerability detection and test generation. Yet, a fundamental question remains open: can LLMs serve as verification oracles, capable of reasoning about arbitrary contract-specific properties? In this paper, we provide the first systematic evaluation of GPT-5, a state-of-the-art reasoning LLM, in this role. We benchmark its performance on a large dataset of verification tasks, compare its outputs against those of established formal verification tools, and assess its practical effectiveness in real-world auditing scenarios. Our study combines quantitative metrics with qualitative analysis, and shows that recent reasoning-oriented LLMs can be surprisingly effective as verification oracles, suggesting a new frontier in the convergence of AI and formal methods for secure smart contract development and auditing.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A decentralized future for the open-science databases</title>
<link>https://arxiv.org/abs/2509.19206</link>
<guid>https://arxiv.org/abs/2509.19206</guid>
<content:encoded><![CDATA[
arXiv:2509.19206v1 Announce Type: new 
Abstract: Continuous and reliable access to curated biological data repositories is indispensable for accelerating rigorous scientific inquiry and fostering reproducible research. Centralized repositories, though widely used, are vulnerable to single points of failure arising from cyberattacks, technical faults, natural disasters, or funding and political uncertainties. This can lead to widespread data unavailability, data loss, integrity compromises, and substantial delays in critical research, ultimately impeding scientific progress. Centralizing essential scientific resources in a single geopolitical or institutional hub is inherently dangerous, as any disruption can paralyze diverse ongoing research. The rapid acceleration of data generation, combined with an increasingly volatile global landscape, necessitates a critical re-evaluation of the sustainability of centralized models. Implementing federated and decentralized architectures presents a compelling and future-oriented pathway to substantially strengthen the resilience of scientific data infrastructures, thereby mitigating vulnerabilities and ensuring the long-term integrity of data. Here, we examine the structural limitations of centralized repositories, evaluate federated and decentralized models, and propose a hybrid framework for resilient, FAIR, and sustainable scientific data stewardship. Such an approach offers a significant reduction in exposure to governance instability, infrastructural fragility, and funding volatility, and also fosters fairness and global accessibility. The future of open science depends on integrating these complementary approaches to establish a globally distributed, economically sustainable, and institutionally robust infrastructure that safeguards scientific data as a public good, further ensuring continued accessibility, interoperability, and preservation for generations to come.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stability and Generalization of Adversarial Diffusion Training</title>
<link>https://arxiv.org/abs/2509.19234</link>
<guid>https://arxiv.org/abs/2509.19234</guid>
<content:encoded><![CDATA[
arXiv:2509.19234v1 Announce Type: new 
Abstract: Algorithmic stability is an established tool for analyzing generalization. While adversarial training enhances model robustness, it often suffers from robust overfitting and an enlarged generalization gap. Although recent work has established the convergence of adversarial training in decentralized networks, its generalization properties remain unexplored. This work presents a stability-based generalization analysis of adversarial training under the diffusion strategy for convex losses. We derive a bound showing that the generalization error grows with both the adversarial perturbation strength and the number of training steps, a finding consistent with single-agent case but novel for decentralized settings. Numerical experiments on logistic regression validate these theoretical predictions.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Filtering amplitude dependence of correlation dynamics in complex systems: application to the cryptocurrency market</title>
<link>https://arxiv.org/abs/2509.18820</link>
<guid>https://arxiv.org/abs/2509.18820</guid>
<content:encoded><![CDATA[
arXiv:2509.18820v1 Announce Type: cross 
Abstract: Based on the cryptocurrency market dynamics, this study presents a general methodology for analyzing evolving correlation structures in complex systems using the $q$-dependent detrended cross-correlation coefficient \rho(q,s). By extending traditional metrics, this approach captures correlations at varying fluctuation amplitudes and time scales. The method employs $q$-dependent minimum spanning trees ($q$MSTs) to visualize evolving network structures. Using minute-by-minute exchange rate data for 140 cryptocurrencies on Binance (Jan 2021-Oct 2024), a rolling window analysis reveals significant shifts in $q$MSTs, notably around April 2022 during the Terra/Luna crash. Initially centralized around Bitcoin (BTC), the network later decentralized, with Ethereum (ETH) and others gaining prominence. Spectral analysis confirms BTC's declining dominance and increased diversification among assets. A key finding is that medium-scale fluctuations exhibit stronger correlations than large-scale ones, with $q$MSTs based on the latter being more decentralized. Properly exploiting such facts may offer the possibility of a more flexible optimal portfolio construction. Distance metrics highlight that major disruptions amplify correlation differences, leading to fully decentralized structures during crashes. These results demonstrate $q$MSTs' effectiveness in uncovering fluctuation-dependent correlations, with potential applications beyond finance, including biology, social and other complex systems.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Atomic Smart Contract Interoperability with High Efficiency via Cross-Chain Integrated Execution</title>
<link>https://arxiv.org/abs/2502.12820</link>
<guid>https://arxiv.org/abs/2502.12820</guid>
<content:encoded><![CDATA[
arXiv:2502.12820v2 Announce Type: replace 
Abstract: With the development of Ethereum, numerous blockchains compatible with Ethereum's execution environment (i.e., Ethereum Virtual Machine, EVM) have emerged. Developers can leverage smart contracts to run various complex decentralized applications on top of blockchains. However, the increasing number of EVM-compatible blockchains has introduced significant challenges in cross-chain interoperability, particularly in ensuring efficiency and atomicity for the whole cross-chain application. Existing solutions are either limited in guaranteeing overall atomicity for the cross-chain application, or inefficient due to the need for multiple rounds of cross-chain smart contract execution. To address this gap, we propose IntegrateX, an efficient cross-chain interoperability system that ensures the overall atomicity of cross-chain smart contract invocations. The core idea is to deploy the logic required for cross-chain execution onto a single blockchain, where it can be executed in an integrated manner. This allows cross-chain applications to perform all cross-chain logic efficiently within the same blockchain. IntegrateX consists of a cross-chain smart contract deployment protocol and a cross-chain smart contract integrated execution protocol. The former achieves efficient and secure cross-chain deployment by decoupling smart contract logic from state, and employing an off-chain cross-chain deployment mechanism combined with on-chain cross-chain verification. The latter ensures atomicity of cross-chain invocations through a 2PC-based mechanism, and enhances performance through transaction aggregation and fine-grained state lock. We implement a prototype of IntegrateX. Extensive experiments demonstrate that it reduces up to 61.2% latency compared to the state-of-the-art baseline while maintaining low gas consumption.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fully Decentralized Cooperative Multi-Agent Reinforcement Learning is A Context Modeling Problem</title>
<link>https://arxiv.org/abs/2509.15519</link>
<guid>https://arxiv.org/abs/2509.15519</guid>
<content:encoded><![CDATA[
arXiv:2509.15519v1 Announce Type: new 
Abstract: This paper studies fully decentralized cooperative multi-agent reinforcement learning, where each agent solely observes the states, its local actions, and the shared rewards. The inability to access other agents' actions often leads to non-stationarity during value function updates and relative overgeneralization during value function estimation, hindering effective cooperative policy learning. However, existing works fail to address both issues simultaneously, due to their inability to model the joint policy of other agents in a fully decentralized setting. To overcome this limitation, we propose a novel method named Dynamics-Aware Context (DAC), which formalizes the task, as locally perceived by each agent, as an Contextual Markov Decision Process, and further addresses both non-stationarity and relative overgeneralization through dynamics-aware context modeling. Specifically, DAC attributes the non-stationary local task dynamics of each agent to switches between unobserved contexts, each corresponding to a distinct joint policy. Then, DAC models the step-wise dynamics distribution using latent variables and refers to them as contexts. For each agent, DAC introduces a context-based value function to address the non-stationarity issue during value function update. For value function estimation, an optimistic marginal value is derived to promote the selection of cooperative actions, thereby addressing the relative overgeneralization issue. Experimentally, we evaluate DAC on various cooperative tasks (including matrix game, predator and prey, and SMAC), and its superior performance against multiple baselines validates its effectiveness.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonconvex Decentralized Stochastic Bilevel Optimization under Heavy-Tailed Noises</title>
<link>https://arxiv.org/abs/2509.15543</link>
<guid>https://arxiv.org/abs/2509.15543</guid>
<content:encoded><![CDATA[
arXiv:2509.15543v1 Announce Type: new 
Abstract: Existing decentralized stochastic optimization methods assume the lower-level loss function is strongly convex and the stochastic gradient noise has finite variance. These strong assumptions typically are not satisfied in real-world machine learning models. To address these limitations, we develop a novel decentralized stochastic bilevel optimization algorithm for the nonconvex bilevel optimization problem under heavy-tailed noises. Specifically, we develop a normalized stochastic variance-reduced bilevel gradient descent algorithm, which does not rely on any clipping operation. Moreover, we establish its convergence rate by innovatively bounding interdependent gradient sequences under heavy-tailed noises for nonconvex decentralized bilevel optimization problems. As far as we know, this is the first decentralized bilevel optimization algorithm with rigorous theoretical guarantees under heavy-tailed noises. The extensive experimental results confirm the effectiveness of our algorithm in handling heavy-tailed noises.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Angelfish: Consensus with Optimal Throughput and Latency Across the Leader-DAG Spectrum</title>
<link>https://arxiv.org/abs/2509.15847</link>
<guid>https://arxiv.org/abs/2509.15847</guid>
<content:encoded><![CDATA[
arXiv:2509.15847v1 Announce Type: new 
Abstract: To maximize performance, many modern blockchain systems rely on eventually-synchronous, Byzantine fault-tolerant (BFT) consensus protocols. Two protocol designs have emerged in this space: protocols that minimize latency using a leader that drives both data dissemination and consensus, and protocols that maximize throughput using a separate, asynchronous data dissemination layer. Recent protocols such as Partially-Synchronous Bullshark and Sailfish combine elements of both approaches by using a DAG to enable parallel data dissemination and a leader that paces DAG formation. This improves latency while achieving state-of-the-art throughput. Yet the latency of leader-based protocols is still better under moderate loads.
  We present Angelfish, a hybrid protocol that adapts smoothly across this design space, from leader-based to Sailfish-like DAG-based consensus. Angelfish lets a dynamically-adjusted subset of parties use best-effort broadcast to issue lightweight votes instead of reliably broadcasting costlier DAG vertices. This reduces communication, helps lagging nodes catch up, and lowers latency in practice compared to prior DAG-based protocols. Our empirical evaluation shows that Angelfish attains state-of-the-art peak throughput while matching the latency of leader-based protocols under moderate throughput, delivering the best of both worlds.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Swarm Oracle: Trustless Blockchain Agreements through Robot Swarms</title>
<link>https://arxiv.org/abs/2509.15956</link>
<guid>https://arxiv.org/abs/2509.15956</guid>
<content:encoded><![CDATA[
arXiv:2509.15956v1 Announce Type: new 
Abstract: Blockchain consensus, rooted in the principle ``don't trust, verify'', limits access to real-world data, which may be ambiguous or inaccessible to some participants. Oracles address this limitation by supplying data to blockchains, but existing solutions may reduce autonomy, transparency, or reintroduce the need for trust. We propose Swarm Oracle: a decentralized network of autonomous robots -- that is, a robot swarm -- that use onboard sensors and peer-to-peer communication to collectively verify real-world data and provide it to smart contracts on public blockchains. Swarm Oracle leverages the built-in decentralization, fault tolerance and mobility of robot swarms, which can flexibly adapt to meet information requests on-demand, even in remote locations. Unlike typical cooperative robot swarms, Swarm Oracle integrates robots from multiple stakeholders, protecting the system from single-party biases but also introducing potential adversarial behavior. To ensure the secure, trustless and global consensus required by blockchains, we employ a Byzantine fault-tolerant protocol that enables robots from different stakeholders to operate together, reaching social agreements of higher quality than the estimates of individual robots. Through extensive experiments using both real and simulated robots, we showcase how consensus on uncertain environmental information can be achieved, despite several types of attacks orchestrated by large proportions of the robots, and how a reputation system based on blockchain tokens lets Swarm Oracle autonomously recover from faults and attacks, a requirement for long-term operation.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Exclusive are Ethereum Transactions? Evidence from non-winning blocks</title>
<link>https://arxiv.org/abs/2509.16052</link>
<guid>https://arxiv.org/abs/2509.16052</guid>
<content:encoded><![CDATA[
arXiv:2509.16052v1 Announce Type: new 
Abstract: We analyze 15,097 blocks proposed for inclusion in Ethereum's blockchain over an 8-minute window on December 3, 2024, during which 38 blocks were added to the chain. We classify transactions as exclusive -- present only in blocks from a single builder -- or private -- absent from the public mempool but included in blocks from multiple builders. We find that exclusive transactions account for 84% of the total fees paid by transactions in winning blocks. Furthermore, we show that exclusivity cannot be fully explained by exclusive relationships between senders and builders: about 7% of all exclusive transactions included on-chain, by value, come from senders who route exclusively to a single builder. Analyzing transaction logs shows that some exclusive transactions are duplicates or variations of the same strategy, but even accounting for that, the share of the total fees paid by transactions in winning blocks is at least 77.2%. Taken together, our findings highlight that exclusive transactions are the dominant source of builder revenues.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Quadruped Cooperative Object Transport: Learning Decentralized Pinch-Lift-Move</title>
<link>https://arxiv.org/abs/2509.14342</link>
<guid>https://arxiv.org/abs/2509.14342</guid>
<content:encoded><![CDATA[
arXiv:2509.14342v1 Announce Type: new 
Abstract: We study decentralized cooperative transport using teams of N-quadruped robots with arm that must pinch, lift, and move ungraspable objects through physical contact alone. Unlike prior work that relies on rigid mechanical coupling between robots and objects, we address the more challenging setting where mechanically independent robots must coordinate through contact forces alone without any communication or centralized control. To this end, we employ a hierarchical policy architecture that separates base locomotion from arm control, and propose a constellation reward formulation that unifies position and orientation tracking to enforce rigid contact behavior. The key insight is encouraging robots to behave as if rigidly connected to the object through careful reward design and training curriculum rather than explicit mechanical constraints. Our approach enables coordination through shared policy parameters and implicit synchronization cues - scaling to arbitrary team sizes without retraining. We show extensive simulation experiments to demonstrate robust transport across 2-10 robots on diverse object geometries and masses, along with sim2real transfer results on lightweight objects.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks</title>
<link>https://arxiv.org/abs/2509.14380</link>
<guid>https://arxiv.org/abs/2509.14380</guid>
<content:encoded><![CDATA[
arXiv:2509.14380v1 Announce Type: new 
Abstract: Multi-Agent Reinforcement Learning (MARL) provides a powerful framework for learning coordination in multi-agent systems. However, applying MARL to robotics still remains challenging due to high-dimensional continuous joint action spaces, complex reward design, and non-stationary transitions inherent to decentralized settings. On the other hand, humans learn complex coordination through staged curricula, where long-horizon behaviors are progressively built upon simpler skills. Motivated by this, we propose CRAFT: Coaching Reinforcement learning Autonomously using Foundation models for multi-robot coordination Tasks, a framework that leverages the reasoning capabilities of foundation models to act as a "coach" for multi-robot coordination. CRAFT automatically decomposes long-horizon coordination tasks into sequences of subtasks using the planning capability of Large Language Models (LLMs). In what follows, CRAFT trains each subtask using reward functions generated by LLM, and refines them through a Vision Language Model (VLM)-guided reward-refinement loop. We evaluate CRAFT on multi-quadruped navigation and bimanual manipulation tasks, demonstrating its capability to learn complex coordination behaviors. In addition, we validate the multi-quadruped navigation policy in real hardware experiments.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry-Aware Decentralized Sinkhorn for Wasserstein Barycenters</title>
<link>https://arxiv.org/abs/2509.14521</link>
<guid>https://arxiv.org/abs/2509.14521</guid>
<content:encoded><![CDATA[
arXiv:2509.14521v1 Announce Type: new 
Abstract: Distributed systems require fusing heterogeneous local probability distributions into a global summary over sparse and unreliable communication networks. Traditional consensus algorithms, which average distributions in Euclidean space, ignore their inherent geometric structure, leading to misleading results. Wasserstein barycenters offer a geometry-aware alternative by minimizing optimal transport costs, but their entropic approximations via the Sinkhorn algorithm typically require centralized coordination. This paper proposes a fully decentralized Sinkhorn algorithm that reformulates the centralized geometric mean as an arithmetic average in the log-domain, enabling approximation through local gossip protocols. Agents exchange log-messages with neighbors, interleaving consensus phases with local updates to mimic centralized iterations without a coordinator. To optimize bandwidth, we integrate event-triggered transmissions and b-bit quantization, providing tunable trade-offs between accuracy and communication while accommodating asynchrony and packet loss. Under mild assumptions, we prove convergence to a neighborhood of the centralized entropic barycenter, with bias linearly dependent on consensus tolerance, trigger threshold, and quantization error. Complexity scales near-linearly with network size. Simulations confirm near-centralized accuracy with significantly fewer messages, across various topologies and conditions.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Software-Defined Radio Testbed for Distributed LiDAR Point Cloud Sharing with IEEE 802.11p in V2V Networks</title>
<link>https://arxiv.org/abs/2509.14523</link>
<guid>https://arxiv.org/abs/2509.14523</guid>
<content:encoded><![CDATA[
arXiv:2509.14523v1 Announce Type: new 
Abstract: We present a Software Defined Radio (SDR)-based IEEE 802.11p testbed for distributed Vehicle-to-Vehicle (V2V) communication. The platform bridges the gap between network simulation and deployment by providing a modular codebase configured for cost-effective ADALM-Pluto SDRs. Any device capable of running a Docker with ROS, executing Matlab and interface with a Pluto via USB can act as a communication node. To demonstrate collaborative sensing, we share LiDAR point clouds between nodes and fuse them into a collective perception environment. We evaluated a theoretical model for leveraging decentralized storage systems (IPFS and Filecoin), analyzing constraints such as node storage convergence, latency, and scalability. In addition, we provide a channel quality study.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEED: A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.14680</link>
<guid>https://arxiv.org/abs/2509.14680</guid>
<content:encoded><![CDATA[
arXiv:2509.14680v1 Announce Type: new 
Abstract: Multi-agent reinforcement learning (MARL) holds substantial promise for intelligent decision-making in complex environments. However, it suffers from a coordination and scalability bottleneck as the number of agents increases. To address these issues, we propose the LLM-empowered expert demonstrations framework for multi-agent reinforcement learning (LEED). LEED consists of two components: a demonstration generation (DG) module and a policy optimization (PO) module. Specifically, the DG module leverages large language models to generate instructions for interacting with the environment, thereby producing high-quality demonstrations. The PO module adopts a decentralized training paradigm, where each agent utilizes the generated demonstrations to construct an expert policy loss, which is then integrated with its own policy loss. This enables each agent to effectively personalize and optimize its local policy based on both expert knowledge and individual experience. Experimental results show that LEED achieves superior sample efficiency, time efficiency, and robust scalability compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Federated Learning for Social Network with Mobility</title>
<link>https://arxiv.org/abs/2509.14938</link>
<guid>https://arxiv.org/abs/2509.14938</guid>
<content:encoded><![CDATA[
arXiv:2509.14938v1 Announce Type: new 
Abstract: Federated Learning (FL) offers a decentralized solution that allows collaborative local model training and global aggregation, thereby protecting data privacy. In conventional FL frameworks, data privacy is typically preserved under the assumption that local data remains absolutely private, whereas the mobility of clients is frequently neglected in explicit modeling. In this paper, we propose a hierarchical federated learning framework based on the social network with mobility namely HFL-SNM that considers both data sharing among clients and their mobility patterns. Under the constraints of limited resources, we formulate a joint optimization problem of resource allocation and client scheduling, which objective is to minimize the energy consumption of clients during the FL process. In social network, we introduce the concepts of Effective Data Coverage Rate and Redundant Data Coverage Rate. We analyze the impact of effective data and redundant data on the model performance through preliminary experiments. We decouple the optimization problem into multiple sub-problems, analyze them based on preliminary experimental results, and propose Dynamic Optimization in Social Network with Mobility (DO-SNM) algorithm. Experimental results demonstrate that our algorithm achieves superior model performance while significantly reducing energy consumption, compared to traditional baseline algorithms.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blockchain-Enabled Explainable AI for Trusted Healthcare Systems</title>
<link>https://arxiv.org/abs/2509.14987</link>
<guid>https://arxiv.org/abs/2509.14987</guid>
<content:encoded><![CDATA[
arXiv:2509.14987v1 Announce Type: new 
Abstract: This paper introduces a Blockchain-Integrated Explainable AI Framework (BXHF) for healthcare systems to tackle two essential challenges confronting health information networks: safe data exchange and comprehensible AI-driven clinical decision-making. Our architecture incorporates blockchain, ensuring patient records are immutable, auditable, and tamper-proof, alongside Explainable AI (XAI) methodologies that yield transparent and clinically relevant model predictions. By incorporating security assurances and interpretability requirements into a unified optimization pipeline, BXHF ensures both data-level trust (by verified and encrypted record sharing) and decision-level trust (with auditable and clinically aligned explanations). Its hybrid edge-cloud architecture allows for federated computation across different institutions, enabling collaborative analytics while protecting patient privacy. We demonstrate the framework's applicability through use cases such as cross-border clinical research networks, uncommon illness detection and high-risk intervention decision support. By ensuring transparency, auditability, and regulatory compliance, BXHF improves the credibility, uptake, and effectiveness of AI in healthcare, laying the groundwork for safer and more reliable clinical decision-making.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Hypergraph Learning with Local Differential Privacy: Toward Privacy-Aware Hypergraph Structure Completion</title>
<link>https://arxiv.org/abs/2408.05160</link>
<guid>https://arxiv.org/abs/2408.05160</guid>
<content:encoded><![CDATA[
arXiv:2408.05160v3 Announce Type: replace 
Abstract: The rapid growth of graph-structured data necessitates partitioning and distributed storage across decentralized systems, driving the emergence of federated graph learning to collaboratively train Graph Neural Networks (GNNs) without compromising privacy. However, current methods exhibit limited performance when handling hypergraphs, which inherently represent complex high-order relationships beyond pairwise connections. Partitioning hypergraph structures across federated subsystems amplifies structural complexity, hindering high-order information mining and compromising local information integrity. To bridge the gap between hypergraph learning and federated systems, we develop FedHGL, a first-of-its-kind framework for federated hypergraph learning on disjoint and privacy-constrained hypergraph partitions. Beyond collaboratively training a comprehensive hypergraph neural network across multiple clients, FedHGL introduces a pre-propagation hyperedge completion mechanism to preserve high-order structural integrity within each client. This procedure leverages the federated central server to perform cross-client hypergraph convolution without exposing internal topological information, effectively mitigating the high-order information loss induced by subgraph partitioning. Furthermore, by incorporating two kinds of local differential privacy (LDP) mechanisms, we provide formal privacy guarantees for this process, ensuring that sensitive node features remain protected against inference attacks from potentially malicious servers or clients. Experimental results on seven real-world datasets confirm the effectiveness of our approach and demonstrate its performance advantages over traditional federated graph learning methods.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication-Efficient and Privacy-Adaptable Mechanism for Federated Learning</title>
<link>https://arxiv.org/abs/2501.12046</link>
<guid>https://arxiv.org/abs/2501.12046</guid>
<content:encoded><![CDATA[
arXiv:2501.12046v2 Announce Type: replace 
Abstract: Training machine learning models on decentralized private data via federated learning (FL) poses two key challenges: communication efficiency and privacy protection. In this work, we address these challenges within the trusted aggregator model by introducing a novel approach called the Communication-Efficient and Privacy-Adaptable Mechanism (CEPAM), achieving both objectives simultaneously. In particular, CEPAM leverages the rejection-sampled universal quantizer (RSUQ), a construction of randomized vector quantizer whose resulting distortion is equivalent to a prescribed noise, such as Gaussian or Laplace noise, enabling joint differential privacy and compression. Our CEPAM provides the additional benefit of privacy adaptability, allowing clients and the server to customize privacy protection based on required accuracy and protection. We theoretically analyze the privacy guarantee of CEPAM and investigate the trade-offs among user privacy and accuracy of CEPAM through experimental evaluations. Moreover, we assess CEPAM's utility performance using MNIST dataset, demonstrating that CEPAM surpasses baseline models in terms of learning accuracy.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.04317</link>
<guid>https://arxiv.org/abs/2505.04317</guid>
<content:encoded><![CDATA[
arXiv:2505.04317v4 Announce Type: replace 
Abstract: In this paper, we tackle the problem of learning to play 3v3 multi-drone volleyball, a new embodied competitive task that requires both high-level strategic coordination and low-level agile control. The task is turn-based, multi-agent, and physically grounded, posing significant challenges due to its long-horizon dependencies, tight inter-agent coupling, and the underactuated dynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play (HCSP), a hierarchical reinforcement learning framework that separates centralized high-level strategic decision-making from decentralized low-level motion control. We design a three-stage population-based training pipeline to enable both strategy and skill to emerge from scratch without expert demonstrations: (I) training diverse low-level skills, (II) learning high-level strategy via self-play with fixed low-level skills, and (III) joint fine-tuning through co-self-play. Experiments show that HCSP achieves superior performance, outperforming non-hierarchical self-play and rule-based hierarchical baselines with an average 82.9% win rate and a 71.5% win rate against the two-stage variant. Moreover, co-self-play leads to emergent team behaviors such as role switching and coordinated formations, demonstrating the effectiveness of our hierarchical design and training scheme. The project page is at https://sites.google.com/view/hi-co-self-play.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Timestamp Manipulation: Timestamp-based Nakamoto-style Blockchains are Vulnerable</title>
<link>https://arxiv.org/abs/2505.05328</link>
<guid>https://arxiv.org/abs/2505.05328</guid>
<content:encoded><![CDATA[
arXiv:2505.05328v4 Announce Type: replace 
Abstract: Nakamoto consensus are the most widely adopted decentralized consensus mechanism in cryptocurrency systems. Since it was proposed in 2008, many studies have focused on analyzing its security. Most of them focus on maximizing the profit of the adversary. Examples include the selfish mining attack [FC '14] and the recent riskless uncle maker (RUM) attack [CCS '23]. In this work, we introduce the Staircase-Unrestricted Uncle Maker (SUUM), the first block withholding attack targeting the timestamp-based Nakamoto-style blockchain. Through block withholding, timestamp manipulation, and difficulty risk control, SUUM adversaries are capable of launching persistent attacks with zero cost and minimal difficulty risk characteristics, indefinitely exploiting rewards from honest participants. This creates a self-reinforcing cycle that threatens the security of blockchains. We conduct a comprehensive and systematic evaluation of SUUM, including the attack conditions, its impact on blockchains, and the difficulty risks. Finally, we further discuss four feasible mitigation measures against SUUM.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SwarmSearch: Decentralized Search Engine with Self-Funding Economy</title>
<link>https://arxiv.org/abs/2505.07452</link>
<guid>https://arxiv.org/abs/2505.07452</guid>
<content:encoded><![CDATA[
arXiv:2505.07452v2 Announce Type: replace 
Abstract: Centralized search engines control what we see, read, believe, and vote. Consequently, they raise concerns over information control, censorship, and bias. Decentralized search engines offer a remedy to this problem, but their adoption has been hindered by their inferior quality and lack of a self-sustaining economic framework. We present SwarmSearch, a fully decentralized, AI-powered search engine with a self-funding architecture. Our system is designed for deployment within the decentralized file-sharing software Tribler. SwarmSearch integrates volunteer-based with profit-driven mechanisms to foster an implicit marketplace for resources. Employing the state-of-the-art of AI-based retrieval and relevance ranking, we also aim to close the quality gap between decentralized search and centralized alternatives. Our system demonstrates high retrieval accuracy while showing robustness in the presence of 50% adversarial nodes.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trustless Autonomy: Understanding Motivations, Benefits, and Governance Dilemmas in Self-Sovereign Decentralized AI Agents</title>
<link>https://arxiv.org/abs/2505.09757</link>
<guid>https://arxiv.org/abs/2505.09757</guid>
<content:encoded><![CDATA[
arXiv:2505.09757v2 Announce Type: replace 
Abstract: The recent trend of self-sovereign Decentralized AI Agents (DeAgents) combines Large Language Model (LLM)-based AI agents with decentralization technologies such as blockchain smart contracts and trusted execution environments (TEEs). These tamper-resistant trustless substrates allow agents to achieve self-sovereignty through ownership of cryptowallet private keys and control of digital assets and social media accounts. DeAgents eliminate centralized control and reduce human intervention, addressing key trust concerns inherent in centralized AI systems. This contributes to social computing by enabling new human cooperative paradigm "intelligence as commons." However, given ongoing challenges in LLM reliability such as hallucinations, this creates paradoxical tension between trustlessness and unreliable autonomy. This study addresses this empirical research gap through interviews with DeAgents stakeholders-experts, founders, and developers-to examine their motivations, benefits, and governance dilemmas. The findings will guide future DeAgents system and protocol design and inform discussions about governance in sociotechnical AI systems in the future agentic web.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vehicle-to-Grid Integration: Ensuring Grid Stability, Strengthening Cybersecurity, and Advancing Energy Market Dynamics</title>
<link>https://arxiv.org/abs/2509.13393</link>
<guid>https://arxiv.org/abs/2509.13393</guid>
<content:encoded><![CDATA[
arXiv:2509.13393v1 Announce Type: new 
Abstract: The increasing adoption of electric vehicles has spurred significant interest in Vehicle-to-Grid technology as a transformative approach to modern energy systems. This paper presents a systematic review of V2G systems, focusing on their integration challenges and potential solutions. First, the current state of V2G development is examined, highlighting its growing importance in mitigating peak demand, enhancing voltage and frequency regulation, and reinforcing grid resilience. The study underscores the pivotal role of artificial intelligence and machine learning in optimizing energy management, load forecasting, and real-time grid control. A critical analysis of cybersecurity risks reveals heightened vulnerabilities stemming from V2G's dependence on interconnected networks and real-time data exchange, prompting an exploration of advanced mitigation strategies, including federated learning, blockchain, and quantum-resistant cryptography. Furthermore, the paper reviews economic and market aspects, including business models (V2G as an aggregator or due to self-consumption), regulation (as flexibility service provider) and factors influencing user acceptance shaping V2G adoption. Data from global case studies and pilot programs offer a snapshot of how V2G has been implemented at different paces across regions. Finally, the study suggests a multi-layered framework that incorporates grid stability resilience, cybersecurity resiliency, and energy market dynamics and provides strategic recommendations to enable scalable, secure, and economically viable V2G deployment.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Secure UAV-assisted Federated Learning: A Digital Twin-Driven Approach with Zero-Knowledge Proofs</title>
<link>https://arxiv.org/abs/2509.13634</link>
<guid>https://arxiv.org/abs/2509.13634</guid>
<content:encoded><![CDATA[
arXiv:2509.13634v1 Announce Type: new 
Abstract: Federated learning (FL) has gained popularity as a privacy-preserving method of training machine learning models on decentralized networks. However to ensure reliable operation of UAV-assisted FL systems, issues like as excessive energy consumption, communication inefficiencies, and security vulnerabilities must be solved. This paper proposes an innovative framework that integrates Digital Twin (DT) technology and Zero-Knowledge Federated Learning (zkFed) to tackle these challenges. UAVs act as mobile base stations, allowing scattered devices to train FL models locally and upload model updates for aggregation. By incorporating DT technology, our approach enables real-time system monitoring and predictive maintenance, improving UAV network efficiency. Additionally, Zero-Knowledge Proofs (ZKPs) strengthen security by allowing model verification without exposing sensitive data. To optimize energy efficiency and resource management, we introduce a dynamic allocation strategy that adjusts UAV flight paths, transmission power, and processing rates based on network conditions. Using block coordinate descent and convex optimization techniques, our method significantly reduces system energy consumption by up to 29.6% compared to conventional FL approaches. Simulation results demonstrate improved learning performance, security, and scalability, positioning this framework as a promising solution for next-generation UAV-based intelligent networks.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dissipativity-Based Data-Driven Decentralized Control of Interconnected Systems</title>
<link>https://arxiv.org/abs/2509.14047</link>
<guid>https://arxiv.org/abs/2509.14047</guid>
<content:encoded><![CDATA[
arXiv:2509.14047v1 Announce Type: new 
Abstract: We propose data-driven decentralized control algorithms for stabilizing interconnected systems. We first derive a data-driven condition to synthesize a local controller that ensures the dissipativity of the local subsystems. Then, we propose data-driven decentralized stability conditions for the global system based on the dissipativity of each local system. Since both conditions take the form of linear matrix inequalities and are based on dissipativity theory, this yields a unified pipeline, resulting in a data-driven decentralized control algorithm. As a special case, we also consider stabilizing systems interconnected through diffusive coupling and propose a control algorithm. We validate the effectiveness and the scalability of the proposed control algorithms in numerical examples in the context of microgrids.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrazyMARL: Decentralized Direct Motor Control Policies for Cooperative Aerial Transport of Cable-Suspended Payloads</title>
<link>https://arxiv.org/abs/2509.14126</link>
<guid>https://arxiv.org/abs/2509.14126</guid>
<content:encoded><![CDATA[
arXiv:2509.14126v1 Announce Type: new 
Abstract: Collaborative transportation of cable-suspended payloads by teams of Unmanned Aerial Vehicles (UAVs) has the potential to enhance payload capacity, adapt to different payload shapes, and provide built-in compliance, making it attractive for applications ranging from disaster relief to precision logistics. However, multi-UAV coordination under disturbances, nonlinear payload dynamics, and slack--taut cable modes remains a challenging control problem. To our knowledge, no prior work has addressed these cable mode transitions in the multi-UAV context, instead relying on simplifying rigid-link assumptions. We propose CrazyMARL, a decentralized Reinforcement Learning (RL) framework for multi-UAV cable-suspended payload transport. Simulation results demonstrate that the learned policies can outperform classical decentralized controllers in terms of disturbance rejection and tracking precision, achieving an 80% recovery rate from harsh conditions compared to 44% for the baseline method. We also achieve successful zero-shot sim-to-real transfer and demonstrate that our policies are highly robust under harsh conditions, including wind, random external disturbances, and transitions between slack and taut cable dynamics. This work paves the way for autonomous, resilient UAV teams capable of executing complex payload missions in unstructured environments.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Complete Decentralization of Linear Quadratic Gaussian Control for the Discrete Wave Equation</title>
<link>https://arxiv.org/abs/2509.13446</link>
<guid>https://arxiv.org/abs/2509.13446</guid>
<content:encoded><![CDATA[
arXiv:2509.13446v1 Announce Type: cross 
Abstract: The linear quadratic Gaussian (LQG) control problem for the linear wave equation on the unit circle with fully distributed actuation and partial state measurements is considered. An analytical solution to a spatial discretization of the problem is obtained. The main result of this work illustrates that for specific parameter values, the optimal LQG policy is completely decentralized, meaning only a measurement at spatial location $i$ is needed to compute an optimal control signal to actuate at this location. The relationship between performance and decentralization as a function of parameters is explored. Conditions for complete decentralization are related to metrics of kinetic and potential energy quantities and control effort.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Networks for Next-Generation-IoT: Recent Advances and Open Challenges</title>
<link>https://arxiv.org/abs/2412.20634</link>
<guid>https://arxiv.org/abs/2412.20634</guid>
<content:encoded><![CDATA[
arXiv:2412.20634v3 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) have emerged as a powerful framework for modeling complex interconnected systems, hence making them particularly well-suited to address the growing challenges of next-generation Internet of Things (NG-IoT) networks. Existing studies remain fragmented, and there is a lack of comprehensive guidance on how GNNs can be systematically applied to NG-IoT systems. As NG-IoT systems evolve toward 6G, they incorporate diverse technologies. These advances promise unprecedented connectivity, sensing, and automation but also introduce significant complexity, requiring new approaches for scalable learning, dynamic optimization, and secure, decentralized decision-making. This survey provides a comprehensive and forward-looking exploration of how GNNs can empower NG-IoT environments. We commence by exploring the fundamental paradigms of GNNs and articulating the motivation for their use in NG-IoT networks. Besides, we intrinsically connect GNNs with the family of low-density parity-check codes, modeling the NG-IoT as dynamic constrained graphs. We highlight the distinct roles of node-, edge-, and graph-level tasks in tackling key challenges and demonstrate the GNNs' ability to overcome the limitations of traditional optimization. We examine the application of GNNs across core NG-enabling technologies and their integration with distributed frameworks to support privacy-preservation and distributed intelligence. We then delve into the challenges posed by adversarial attacks, offering insights into defense mechanisms. Lastly, we examine how GNNs can be integrated with emerging technologies. Our findings highlight the transformative potential of GNNs in improving efficiency, scalability, and security. Finally, we summarize the key lessons learned and outline promising future research directions, along with a set of design guidelines tailored for NG-IoT applications.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Commit-Reveal$^2$: Securing Randomness Beacons with Randomized Reveal Order in Smart Contracts</title>
<link>https://arxiv.org/abs/2504.03936</link>
<guid>https://arxiv.org/abs/2504.03936</guid>
<content:encoded><![CDATA[
arXiv:2504.03936v2 Announce Type: replace 
Abstract: Simple commit-reveal beacons are vulnerable to last-revealer strategies, and existing descriptions often leave accountability and recovery mechanisms unspecified for practical deployments. We present Commit-Reveal$^2$, a layered design for blockchain deployments that cryptographically randomizes the final reveal order, together with a concrete accountability and fallback mechanism that we implement as smart-contract logic. The protocol is architected as a hybrid system, where routine coordination runs off chain for efficiency and the blockchain acts as the trust anchor for commitments and the final arbiter for disputes. Our implementation covers leader coordination, on-chain verification, slashing for non-cooperation, and an explicit on-chain recovery path that maintains progress when off-chain coordination fails. We formally define two security goals for distributed randomness beacons, unpredictability and bit-wise bias resistance, and we show that Commit-Reveal$^2$ meets these notions under standard hash assumptions in the random-oracle model. In measurements with small to moderate operator sets, the hybrid design reduces on-chain gas by more than 80% compared to a fully on-chain baseline. We release a publicly verifiable prototype and evaluation artifacts to support replication and adoption in blockchain applications.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedDiverse: Tackling Data Heterogeneity in Federated Learning with Diversity-Driven Client Selection</title>
<link>https://arxiv.org/abs/2504.11216</link>
<guid>https://arxiv.org/abs/2504.11216</guid>
<content:encoded><![CDATA[
arXiv:2504.11216v2 Announce Type: replace 
Abstract: Federated Learning (FL) enables decentralized training of machine learning models on distributed data while preserving privacy. However, in real-world FL settings, client data is often non-identically distributed and imbalanced, resulting in statistical data heterogeneity which impacts the generalization capabilities of the server's model across clients, slows convergence and reduces performance. In this paper, we address this challenge by proposing first a characterization of statistical data heterogeneity by means of 6 metrics of global and client attribute imbalance, class imbalance, and spurious correlations. Next, we create and share 7 computer vision datasets for binary and multiclass image classification tasks in Federated Learning that cover a broad range of statistical data heterogeneity and hence simulate real-world situations. Finally, we propose FEDDIVERSE, a novel client selection algorithm in FL which is designed to manage and leverage data heterogeneity across clients by promoting collaboration between clients with complementary data distributions. Experiments on the seven proposed FL datasets demonstrate FEDDIVERSE's effectiveness in enhancing the performance and robustness of a variety of FL methods while having low communication and computational overhead.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Benchmark of Federated Learning with Kolmogorov-Arnold Networks for Medical Imaging</title>
<link>https://arxiv.org/abs/2504.19639</link>
<guid>https://arxiv.org/abs/2504.19639</guid>
<content:encoded><![CDATA[
arXiv:2504.19639v2 Announce Type: replace 
Abstract: Federated Learning (FL) enables model training across decentralized devices without sharing raw data, thereby preserving privacy in sensitive domains like healthcare. In this paper, we evaluate Kolmogorov-Arnold Networks (KAN) architectures against traditional MLP across six state-of-the-art FL algorithms on a blood cell classification dataset. Notably, our experiments demonstrate that KAN can effectively replace MLP in federated environments, achieving superior performance with simpler architectures. Furthermore, we analyze the impact of key hyperparameters-grid size and network architecture-on KAN performance under varying degrees of Non-IID data distribution. In addition, our ablation studies reveal that optimizing KAN width while maintaining minimal depth yields the best performance in federated settings. As a result, these findings establish KAN as a promising alternative for privacy-preserving medical imaging applications in distributed healthcare. To the best of our knowledge, this is the first comprehensive benchmark of KAN in FL settings for medical imaging task.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Up Data Parallelism in Decentralized Deep Learning</title>
<link>https://arxiv.org/abs/2509.12213</link>
<guid>https://arxiv.org/abs/2509.12213</guid>
<content:encoded><![CDATA[
arXiv:2509.12213v1 Announce Type: new 
Abstract: Although it has been extensively explored in theory, decentralized learning is not yet green-lighted for production use, largely due to a lack of stability, scalability, and generality in large scale DNN training. To shed light on the production use of decentralized learning, this work studies decentralized data parallel training at scale. To this end, we introduce a benchmarking framework, namely DBench, to host both centralized and decentralized DNN training. Building upon DBench, we introduce a benchmarking methodology to uncover the correlations between model accuracy and the variances of parameter tensors by varying communication graphs and training scales. Based on the benchmarking results, we observe that, (1) Similar to centralized learning, decentralized data parallel training also presents the issues of scalability and generality when the training scales up; (2) The model accuracy of decentralized learning is correlated to the number of connections in a communication graph; (3) The model accuracy of decentralized learning is surprisingly sensitive to the variance of parameter tensors across model replicas. Built upon the observations, we propose Ada, a decentralized adaptive approach that performs large scale DNN training following a decentralized SGD method and adapting the communication graph in use dynamically throughout training iterations. We apply Ada on large scale training and observe that Ada can obtain the best convergence rates consistently in decentralized DNN training, and delivers equally or comparably good model accuracy for all sample applications as centralized learning does, even when training ResNet50 for ImageNet-1K on the scale of 1008 GPUs.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ratio1 -- AI meta-OS</title>
<link>https://arxiv.org/abs/2509.12223</link>
<guid>https://arxiv.org/abs/2509.12223</guid>
<content:encoded><![CDATA[
arXiv:2509.12223v1 Announce Type: new 
Abstract: We propose the Ratio1 AI meta-operating system (meta-OS), a decentralized MLOps protocol that unifies AI model development, deployment, and inference across heterogeneous edge devices. Its key innovation is an integrated blockchain-based framework that transforms idle computing resources (laptops, smartphones, cloud VMs) into a trustless global supercomputer. The architecture includes novel components: a decentralized authentication layer (dAuth), an in-memory state database (CSTORE), a distributed storage system (R1FS), homomorphic encrypted federated learning (EDIL), decentralized container orchestration (Deeploy) and an oracle network (OracleSync), which collectively ensure secure, resilient execution of AI pipelines and other container based apps at scale. The protocol enforces a formal circular token-economic model combining Proof-of-Availability (PoA) and Proof-of-AI (PoAI) consensus. Compared to centralized heterogeneous cloud MLOps and existing decentralized compute platforms, which often lack integrated AI toolchains or trusted Ratio1 node operators (R1OP) mechanics, Ratio1's holistic design lowers barriers for AI deployment and improves cost-efficiency. We provide mathematical formulations of its secure licensing and reward protocols, and include descriptive information for the system architecture and protocol flow. We argue that our proposed fully functional ecosystem proposes and demonstrates significant improvements in accessibility, scalability, and security over existing alternatives.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Private Markovian Equilibrium in Stackelberg Markov Games for Smart Grid Demand Response</title>
<link>https://arxiv.org/abs/2509.12225</link>
<guid>https://arxiv.org/abs/2509.12225</guid>
<content:encoded><![CDATA[
arXiv:2509.12225v1 Announce Type: new 
Abstract: The increasing integration of renewable energy introduces a great challenge to the supply and demand balance of the power grid. To address this challenge, this paper formulates a Stackelberg Markov game (SMG) between an aggregator and multiple users, where the aggregator sets electricity prices and users make demand and storage decisions. Considering that users' storage levels are private information, we introduce private states and propose the new concepts of private Markovian strategies (PMS) and private Markovian equilibrium (PME). We establish the existence of a pure PME in the lower-level Markov game and prove that it can be computed in polynomial time. Notably, computing equilibrium in general Markov games is hard, and polynomial-time algorithms are rarely available. Based on these theoretical results, we develop a scalable solution framework combining centralized and decentralized algorithms for the lower-level PME computation with upper-level pricing optimization. Numerical simulations with up to 50 users based on real data validate the effectiveness and scalability of the proposed methods, whereas prior studies typically consider no more than 5 users.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Storage places in diplomatic texts (7th-13th centuries). Lexical, semantic, and digital investigation</title>
<link>https://arxiv.org/abs/2509.12230</link>
<guid>https://arxiv.org/abs/2509.12230</guid>
<content:encoded><![CDATA[
arXiv:2509.12230v1 Announce Type: new 
Abstract: This study examines the evolution of references to grain storage structures in medieval European charters, based on a quantitative and semantic analysis of the digitized CEMA (Cartae Europae Medii Aevi) corpus comprising more than 225,000 documents. The author applies text mining and distributional analysis methods to a lexicon of some forty terms designating storage locations (grangia, horreum, granarium, granica, etc.), cross-referencing these data with references to grain and analyzing their semantic contexts over the long term. The analysis reveals a paradigm shift between the early Middle Ages (decentralized, loosely regulated storage) and the 12th-13th centuries (centralization of storage by the ruling classes). Granaries became instruments of spatial polarization and social control, contributing to the accentuation of social domination in medieval Europe. This evolution was accompanied by a new conceptualization of storage, both material and spiritual.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Collapse-Inspired Multi-Label Federated Learning under Label-Distribution Skew</title>
<link>https://arxiv.org/abs/2509.12544</link>
<guid>https://arxiv.org/abs/2509.12544</guid>
<content:encoded><![CDATA[
arXiv:2509.12544v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative model training across distributed clients while preserving data privacy. However, the performance of deep learning often deteriorates in FL due to decentralized and heterogeneous data. This challenge is further amplified in multi-label scenarios, where data exhibit complex characteristics such as label co-occurrence, inter-label dependency, and discrepancies between local and global label relationships. While most existing FL research primarily focuses on single-label classification, many real-world applications, particularly in domains such as medical imaging, often involve multi-label settings. In this paper, we address this important yet underexplored scenario in FL, where clients hold multi-label data with skewed label distributions. Neural Collapse (NC) describes a geometric structure in the latent feature space where features of each class collapse to their class mean with vanishing intra-class variance, and the class means form a maximally separated configuration. Motivated by this theory, we propose a method to align feature distributions across clients and to learn high-quality, well-clustered representations. To make the NC-structure applicable to multi-label settings, where image-level features may contain multiple semantic concepts, we introduce a feature disentanglement module that extracts semantically specific features. The clustering of these disentangled class-wise features is guided by a predefined shared NC structure, which mitigates potential conflicts between client models due to diverse local data distributions. In addition, we design regularisation losses to encourage compact clustering in the latent feature space. Experiments conducted on four benchmark datasets across eight diverse settings demonstrate that our approach outperforms existing methods, validating its effectiveness in this challenging FL scenario.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asymmetric Grid Quorum Systems for Heterogeneous Processes</title>
<link>https://arxiv.org/abs/2509.12942</link>
<guid>https://arxiv.org/abs/2509.12942</guid>
<content:encoded><![CDATA[
arXiv:2509.12942v1 Announce Type: new 
Abstract: Quorum systems are a common way to formalize failure assumptions in distributed systems. Traditionally, these assumptions are shared by all involved processes. More recently, systems have emerged which allow processes some freedom in choosing their own, subjective or asymmetric, failure assumptions. For such a system to work, individual processes' assumptions must be compatible. However, this leads to a Catch-22-style scenario: How can processes collaborate to agree on compatible failure assumptions when they have no compatible failure assumptions to start with?
  We introduce asymmetric grid quorum systems that allow a group of processes to specify heterogeneous trust assumptions independently of each other and without coordination. They are based on qualitative attributes describing how the processes differ. Each process may select a quorum system from this class that aligns best with its subjective view. The available choices are designed to be compatible by definition, thereby breaking the cycling dependency. Asymmetric grid quorum systems have many applications that range from cloud platforms to blockchain networks.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>xRWA: A Cross-Chain Framework for Interoperability of Real-World Assets</title>
<link>https://arxiv.org/abs/2509.12957</link>
<guid>https://arxiv.org/abs/2509.12957</guid>
<content:encoded><![CDATA[
arXiv:2509.12957v1 Announce Type: new 
Abstract: Real-World Assets (RWAs) have recently attracted increasing attention as a means of bridging traditional financial instruments with decentralized infrastructures. By representing assets such as bonds, commodities, and real estate on blockchains, RWAs can enhance liquidity, broaden accessibility, and extend the scope of decentralized finance. Industry forecasts further suggest rapid growth of tokenized RWAs in the coming years, underscoring their potential role in the evolution of digital financial markets. However, when deployed across multiple blockchains, RWAs face challenges such as repeated authentication on different chains and inefficiency caused by multi-step settlement protocols. To address these issues, we present a cross-chain framework for RWAs that emphasizes identity management, authentication, and interaction. The framework integrates Decentralized Identifiers and Verifiable Credentials with customized attributes to support decentralized identification, and incorporates an authentication protocol based on Simplified Payment Verification to avoid redundant verification across chains. Furthermore, we design a cross-chain channel that enables the settlement of RWAs without requiring channel closure, thereby improving operational efficiency. We implement the framework and evaluate it through simulations, which confirm its feasibility and demonstrate improvements in efficiency for RWAs in cross-chain settings.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Validating Solidity Code Defects using Symbolic and Concrete Execution powered by Large Language Models</title>
<link>https://arxiv.org/abs/2509.13023</link>
<guid>https://arxiv.org/abs/2509.13023</guid>
<content:encoded><![CDATA[
arXiv:2509.13023v1 Announce Type: new 
Abstract: The high rate of false alarms from static analysis tools and Large Language Models (LLMs) complicates vulnerability detection in Solidity Smart Contracts, demanding methods that can formally or empirically prove the presence of defects. This paper introduces a novel detection pipeline that integrates custom Slither-based detectors, LLMs, Kontrol, and Forge. Our approach is designed to reliably detect defects and generate proofs.  We currently perform experiments with promising results for seven types of critical defects. We demonstrate the pipeline's efficacy by presenting our findings for three vulnerabilities -- Reentrancy, Complex Fallback, and Faulty Access Control Policies -- that are challenging for current verification solutions, which often generate false alarms or fail to detect them entirely. We highlight the potential of either symbolic or concrete execution in correctly classifying such code faults. By chaining these instruments, our method effectively validates true positives, significantly reducing the manual verification burden. Although we identify potential limitations, such as the inconsistency and the cost of LLMs, our findings establish a robust framework for combining heuristic analysis with formal verification to achieve more reliable and automated smart contract auditing.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Internet of Intelligent Things Framework for Decentralized Heterogeneous Platforms</title>
<link>https://arxiv.org/abs/2509.10507</link>
<guid>https://arxiv.org/abs/2509.10507</guid>
<content:encoded><![CDATA[
arXiv:2509.10507v1 Announce Type: new 
Abstract: Internet of Intelligent Things (IoIT), an emerging field, combines the utility of Internet of Things (IoT) devices with the innovation of embedded AI algorithms. However, it does not come without challenges, and struggles regarding available computing resources, energy supply, and storage limitations. In particular, many impediments to IoIT are linked to the energy-efficient deployment of machine learning (ML)/deep learning (DL) models in embedded devices. Research has been conducted to design energy-efficient IoIT platforms, but these papers often focus on centralized systems, in which some central entity processes all the data and coordinates actions. This can be problematic, e.g., serve as bottleneck or lead to security concerns. In a decentralized system, nodes/devices would self-organize and make their own decisions. Therefore, to address such issues, we propose a heterogeneous, decentralized sensing and monitoring IoIT peer-to-peer mesh network system model. Nodes in the network will coordinate towards several optimization goals: reliability, energy efficiency, and latency. The system employs federated learning to train nodes in a distributed manner, metaheuristics to optimize task allocation and routing paths, and multi-objective optimization to balance conflicting performance goals.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Identity Management on Ripple: A Conceptual Framework for High-Speed, Low-Cost Identity Transactions in Attestation-Based Attribute-Based Identity</title>
<link>https://arxiv.org/abs/2509.10545</link>
<guid>https://arxiv.org/abs/2509.10545</guid>
<content:encoded><![CDATA[
arXiv:2509.10545v1 Announce Type: new 
Abstract: Recent years have seen many industrial implementations and much scholastic research, i.e., prototypes and theoretical frameworks, in Decentralized Identity Management Systems (DIDMS). It is safe to say that Attestation-Based Attribute-Based Decentralized IDM (ABABDIDM) has not received anywhere near the same level of attention in the literature as general Attribute-Based DIDMs (ABDIDM), i.e, decentralized Attribute-Based Access Control (ABAC). The use of decentralization, i.e., DIDM, is to improve upon the security and privacy-related issues of centralized Identity Management Systems (IDM) and Attribute-Based IDMs (ABIDM). And blockchain is the framework used for decentralization in all these schemes. Many DIDMs - even ABDIDMs - have been defined on popular blockchains such as Hyperledger, Ethereum, and Bitcoin. However, despite the characteristics of Ripple that makes it appealing for an ABIDM, there is a lack of research to develop an Identity Management System (IDMS) on Ripple in literature. We have attempted to conceptualize an ABABDIDM on Ripple.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZapGPT: Free-form Language Prompting for Simulated Cellular Control</title>
<link>https://arxiv.org/abs/2509.10660</link>
<guid>https://arxiv.org/abs/2509.10660</guid>
<content:encoded><![CDATA[
arXiv:2509.10660v1 Announce Type: new 
Abstract: Human language is one of the most expressive tools for conveying intent, yet most artificial or biological systems lack mechanisms to interpret or respond meaningfully to it. Bridging this gap could enable more natural forms of control over complex, decentralized systems. In AI and artificial life, recent work explores how language can specify high-level goals, but most systems still depend on engineered rewards, task-specific supervision, or rigid command sets, limiting generalization to novel instructions. Similar constraints apply in synthetic biology and bioengineering, where the locus of control is often genomic rather than environmental perturbation.
  A key open question is whether artificial or biological collectives can be guided by free-form natural language alone, without task-specific tuning or carefully designed evaluation metrics. We provide one possible answer here by showing, for the first time, that simple agents' collective behavior can be guided by free-form language prompts: one AI model transforms an imperative prompt into an intervention that is applied to simulated cells; a second AI model scores how well the prompt describes the resulting cellular dynamics; and the former AI model is evolved to improve the scores generated by the latter.
  Unlike previous work, our method does not require engineered fitness functions or domain-specific prompt design. We show that the evolved system generalizes to unseen prompts without retraining. By treating natural language as a control layer, the system suggests a future in which spoken or written prompts could direct computational, robotic, or biological systems to desired behaviors. This work provides a concrete step toward this vision of AI-biology partnerships, in which language replaces mathematical objective functions, fixed rules, and domain-specific programming.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Experimental Validation of Decentralized Affine Transformation</title>
<link>https://arxiv.org/abs/2509.10791</link>
<guid>https://arxiv.org/abs/2509.10791</guid>
<content:encoded><![CDATA[
arXiv:2509.10791v1 Announce Type: new 
Abstract: This paper presents an experimental validation of decentralized affine transformation (AT) in multi-agent systems using teams of mini-quadcopters. The AT framework enables an agent team to safely navigate constrained, obstacle-rich environments while allowing aggressive changes in inter-agent distances, which are formally characterized through the decomposition of the AT transformation matrix. Without loss of generality, we focus on two-dimensional AT, formulated as a decentralized leader-follower problem. In this formulation, three leader quadcopters are positioned at the vertices of a triangle, while all follower quadcopters remain within the triangle. The leaders know the desired trajectories prescribed by the AT, whereas the followers do not. Instead, the followers infer their trajectories through local communication governed by fixed communication weights determined by the initial spatial configuration of the team. Experimental results validate the asymptotic convergence of decentralized AT and demonstrate its capability to safely guide multi-agent teams through obstacle-laden environments.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arguzz: Testing zkVMs for Soundness and Completeness Bugs</title>
<link>https://arxiv.org/abs/2509.10819</link>
<guid>https://arxiv.org/abs/2509.10819</guid>
<content:encoded><![CDATA[
arXiv:2509.10819v1 Announce Type: new 
Abstract: Zero-knowledge virtual machines (zkVMs) are increasingly deployed in decentralized applications and blockchain rollups since they enable verifiable off-chain computation. These VMs execute general-purpose programs, frequently written in Rust, and produce succinct cryptographic proofs. However, zkVMs are complex, and bugs in their constraint systems or execution logic can cause critical soundness (accepting invalid executions) or completeness (rejecting valid ones) issues.
  We present Arguzz, the first automated tool for testing zkVMs for soundness and completeness bugs. To detect such bugs, Arguzz combines a novel variant of metamorphic testing with fault injection. In particular, it generates semantically equivalent program pairs, merges them into a single Rust program with a known output, and runs it inside a zkVM. By injecting faults into the VM, Arguzz mimics malicious or buggy provers to uncover overly weak constraints.
  We used Arguzz to test six real-world zkVMs (RISC Zero, Nexus, Jolt, SP1, OpenVM, and Pico) and found eleven bugs in three of them. One RISC Zero bug resulted in a $50,000 bounty, despite prior audits, demonstrating the critical need for systematic testing of zkVMs.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Paradigm Shift to Audit Rift: Exploring Vulnerabilities and Audit Tips for TON Smart Contracts</title>
<link>https://arxiv.org/abs/2509.10823</link>
<guid>https://arxiv.org/abs/2509.10823</guid>
<content:encoded><![CDATA[
arXiv:2509.10823v1 Announce Type: new 
Abstract: The Open Network (TON) is a high-performance blockchain platform designed for scalability and efficiency, leveraging an asynchronous execution model and a multi-layered architecture. While TON's design offers significant advantages, it also introduces unique challenges for smart contract development and security. This paper introduces a comprehensive audit checklist for TON smart contracts, based on an analysis of 34 professional audit reports containing 233 real-world vulnerabilities. The checklist addresses TON-specific challenges, such as asynchronous message handling, and provides actionable insights for developers and auditors. We also present detailed case studies of vulnerabilities in TON smart contracts, highlighting their implications and offering lessons learned. By adopting this checklist, developers and auditors can systematically identify and mitigate vulnerabilities, enhancing the security and reliability of TON-based projects. Our work bridges the gap between Ethereum's mature audit methodologies and the emerging needs of the TON ecosystem, fostering a more secure and robust blockchain environment.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design of scalable orthogonal digital encoding architecture for large-area flexible tactile sensing in robotics</title>
<link>https://arxiv.org/abs/2509.10888</link>
<guid>https://arxiv.org/abs/2509.10888</guid>
<content:encoded><![CDATA[
arXiv:2509.10888v1 Announce Type: new 
Abstract: Human-like embodied tactile perception is crucial for the next-generation intelligent robotics. Achieving large-area, full-body soft coverage with high sensitivity and rapid response, akin to human skin, remains a formidable challenge due to critical bottlenecks in encoding efficiency and wiring complexity in existing flexible tactile sensors, thus significantly hinder the scalability and real-time performance required for human skin-level tactile perception. Herein, we present a new architecture employing code division multiple access-inspired orthogonal digital encoding to overcome these challenges. Our decentralized encoding strategy transforms conventional serial signal transmission by enabling parallel superposition of energy-orthogonal base codes from distributed sensing nodes, drastically reducing wiring requirements and increasing data throughput. We implemented and validated this strategy with off-the-shelf 16-node sensing array to reconstruct the pressure distribution, achieving a temporal resolution of 12.8 ms using only a single transmission wire. Crucially, the architecture can maintain sub-20ms latency across orders-of-magnitude variations in node number (to thousands of nodes). By fundamentally redefining signal encoding paradigms in soft electronics, this work opens new frontiers in developing scalable embodied intelligent systems with human-like sensory capabilities.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Framework for Formal Modelling of Metaverse Applications Using Hierarchical Colored Petri Nets</title>
<link>https://arxiv.org/abs/2509.10936</link>
<guid>https://arxiv.org/abs/2509.10936</guid>
<content:encoded><![CDATA[
arXiv:2509.10936v1 Announce Type: new 
Abstract: The Metaverse emerges by integrating highly-distributed, complex, and interconnecting technologies. These technologies need to be formally verified and evaluated through formal modelling before executing them in real-world applications, in order to avoid negative impacts on the real world due to failure of the Metaverse technologies. However, the formal modelling of Metaverse technologies is challenging due to its highly complex nature. Therefore, a comprehensive formal verification of the Metaverse technologies is needed for its realization in multiple potential areas. In this study, a framework is proposed for the formal modelling of Metaverse technologies, which allows holistic insights for all applications of Metaverse technologies. By utilizing the proposed framework, Metaverse applications of any complexity can be modeled. The working of the proposed framework is illustrated by modelling a case study of an Air Traffic Control system. In the proposed framework, we utilize hierarchical colored Petri nets for formal modelling of behavior of the air traffic control system. The correctness of air traffic control system properties, such as liveness, reachability, and boundedness, is verified in the proposed framework. The results of the case study reveal that the proposed framework can be used as a template for mathematical verification of challenging and complex Metaverse applications. The results also show that formal modelling provides an effective tool for identifying flaws in the early phases of the design of Metaverse applications. The implication of using formal verification is that it can increase confidence about the correctness of the Metaverse applications.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Range-Based Sharding (RBS) Protocol for Scalable Enterprise Blockchain</title>
<link>https://arxiv.org/abs/2509.11006</link>
<guid>https://arxiv.org/abs/2509.11006</guid>
<content:encoded><![CDATA[
arXiv:2509.11006v1 Announce Type: new 
Abstract: Blockchain technology offers decentralization and security but struggles with scalability, particularly in enterprise settings where efficiency and controlled access are paramount. Sharding is a promising solution for private blockchains, yet existing approaches face challenges in coordinating shards, ensuring fault tolerance with limited nodes, and minimizing the high overhead of consensus mechanisms like PBFT. This paper proposes the Range-Based Sharding (RBS) Protocol, a novel sharding mechanism tailored for enterprise blockchains, implemented on Quorum. Unlike traditional sharding models such as OmniLedger and non-sharding Corda framework, RBS employs a commit-reveal scheme for secure and unbiased shard allocation, ensuring fair validator distribution while reducing cross-shard transaction delays. Our approach enhances scalability by balancing computational loads across shards, reducing consensus overhead, and improving parallel transaction execution. Experimental evaluations demonstrate that RBS achieves significantly higher throughput and lower latency compared to existing enterprise sharding frameworks, making it a viable and efficient solution for largescale blockchain deployments.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General Decentralized Stochastic Optimal Control via Change of Measure: Applications to the Witsenhausen Counterexample</title>
<link>https://arxiv.org/abs/2509.11013</link>
<guid>https://arxiv.org/abs/2509.11013</guid>
<content:encoded><![CDATA[
arXiv:2509.11013v1 Announce Type: new 
Abstract: In this paper we present global and person-by-person (PbP) optimality conditions for general decentralized stochastic dynamic optimal control problems, using a discrete-time version of Girsanov's change of measure. The PbP optimality conditions are applied to the Witsenhausen counterexample to show that the two strategies satisfy two coupled nonlinear integral equations. Further, we prove a fixed point theorem in a function space, establishing existence and uniqueness of solutions to the integral equations. We also provide numerical solutions of the two integral equations using the Gauss Hermite Quadrature scheme, and include a detail comparison to other numerical methods of the literature. The numerical solutions confirm Witsehausen's observation that, for certain choices of parameters, linear or affine strategies are optimal, while for other choices of parameters nonlinear strategies outperformed affine strategies.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Username Suggestion and Multimodal Gender Detection in Online Platforms: Introducing the PNGT-26K Dataset</title>
<link>https://arxiv.org/abs/2509.11136</link>
<guid>https://arxiv.org/abs/2509.11136</guid>
<content:encoded><![CDATA[
arXiv:2509.11136v1 Announce Type: new 
Abstract: Persian names present unique challenges for natural language processing applications, particularly in gender detection and digital identity creation, due to transliteration inconsistencies and cultural-specific naming patterns. Existing tools exhibit significant performance degradation on Persian names, while the scarcity of comprehensive datasets further compounds these limitations. To address these challenges, the present research introduces PNGT-26K, a comprehensive dataset of Persian names, their commonly associated gender, and their English transliteration, consisting of approximately 26,000 tuples. As a demonstration of how this resource can be utilized, we also introduce two frameworks, namely Open Gender Detection and Nominalist. Open Gender Detection is a production-grade, ready-to-use framework for using existing data from a user, such as profile photo and name, to give a probabilistic guess about the person's gender. Nominalist, the second framework introduced by this paper, utilizes agentic AI to help users choose a username for their social media accounts on any platform. It can be easily integrated into any website to provide a better user experience. The PNGT-26K dataset, Nominalist and Open Gender Detection frameworks are publicly available on Github.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Recommender System with Data Valuation for E-commerce Platform</title>
<link>https://arxiv.org/abs/2509.11196</link>
<guid>https://arxiv.org/abs/2509.11196</guid>
<content:encoded><![CDATA[
arXiv:2509.11196v1 Announce Type: new 
Abstract: Federated Learning (FL) is gaining prominence in machine learning as privacy concerns grow. This paradigm allows each client (e.g., an individual online store) to train a recommendation model locally while sharing only model updates, without exposing the raw interaction logs to a central server, thereby preserving privacy in a decentralized environment. Nonetheless, most existing FL-based recommender systems still rely solely on each client's private data, despite the abundance of publicly available datasets that could be leveraged to enrich local training; this potential remains largely underexplored. To this end, we consider a realistic scenario wherein a large shopping platform collaborates with multiple small online stores to build a global recommender system. The platform possesses global data, such as shareable user and item lists, while each store holds a portion of interaction data privately (or locally). Although integrating global data can help mitigate the limitations of sparse and biased clients' local data, it also introduces additional challenges: simply combining all global interactions can amplify noise and irrelevant patterns, worsening personalization and increasing computational costs. To address these challenges, we propose FedGDVE, which selectively augments each client's local graph with semantically aligned samples from the global dataset. FedGDVE employs: (i) a pre-trained graph encoder to extract global structural features, (ii) a local valid predictor to assess client-specific relevance, (iii) a reinforcement-learning-based probability estimator to filter and sample only the most pertinent global interactions. FedGDVE improves performance by up to 34.86% on recognized benchmarks in FL environments.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Web-Based Environment for the Specification and Generation of Smart Legal Contracts</title>
<link>https://arxiv.org/abs/2509.11258</link>
<guid>https://arxiv.org/abs/2509.11258</guid>
<content:encoded><![CDATA[
arXiv:2509.11258v1 Announce Type: new 
Abstract: Monitoring the compliance of contract performance against legal obligations is important in order to detect violations, ideally, as soon as they occur. Such monitoring can nowadays be achieved through the use of smart contracts, which provide protection against tampering as well as some level of automation in handling violations. However, there exists a large gap between natural language contracts and smart contract implementations. This paper introduces a Web-based environment that partly fills that gap by supporting the user-assisted refinement of Symboleo specifications corresponding to legal contract templates, followed by the automated generation of monitoring smart contracts deployable on the Hyperledger Fabric platform. This environment, illustrated using a sample contract from the transactive energy domain, shows much potential in accelerating the development of smart contracts in a legal compliance context.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Incentive-Compatible Reward Sharing Mechanism for Mitigating Mirroring Attacks in Decentralized Data-Feed Systems</title>
<link>https://arxiv.org/abs/2509.11294</link>
<guid>https://arxiv.org/abs/2509.11294</guid>
<content:encoded><![CDATA[
arXiv:2509.11294v1 Announce Type: new 
Abstract: Decentralized data-feed systems enable blockchain-based smart contracts to access off-chain information by aggregating values from multiple oracles. To improve accuracy, these systems typically use an aggregation function, such as majority voting, to consolidate the inputs they receive from oracles and make a decision. Depending on the final decision and the values reported by the oracles, the participating oracles are compensated through shared rewards. However, such incentive mechanisms are vulnerable to mirroring attacks, where a single user controls multiple oracles to bias the decision of the aggregation function and maximize rewards. This paper analyzes the impact of mirroring attacks on the reliability and dependability of majority voting-based data-feed systems. We demonstrate how existing incentive mechanisms can unintentionally encourage rational users to implement such attacks. To address this, we propose a new incentive mechanism that discourages Sybil behavior. We prove that the proposed mechanism leads to a Nash Equilibrium in which each user operates only one oracle. Finally, we discuss the practical implementation of the proposed incentive mechanism and provide numerical examples to demonstrate its effectiveness.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Escaping Efficiency of Distributed Adversarial Training Algorithms</title>
<link>https://arxiv.org/abs/2509.11337</link>
<guid>https://arxiv.org/abs/2509.11337</guid>
<content:encoded><![CDATA[
arXiv:2509.11337v1 Announce Type: new 
Abstract: Adversarial training has been widely studied in recent years due to its role in improving model robustness against adversarial attacks. This paper focuses on comparing different distributed adversarial training algorithms--including centralized and decentralized strategies--within multi-agent learning environments. Previous studies have highlighted the importance of model flatness in determining robustness. To this end, we develop a general theoretical framework to study the escaping efficiency of these algorithms from local minima, which is closely related to the flatness of the resulting models. We show that when the perturbation bound is sufficiently small (i.e., when the attack strength is relatively mild) and a large batch size is used, decentralized adversarial training algorithms--including consensus and diffusion--are guaranteed to escape faster from local minima than the centralized strategy, thereby favoring flatter minima. However, as the perturbation bound increases, this trend may no longer hold. In the simulation results, we illustrate our theoretical findings and systematically compare the performance of models obtained through decentralized and centralized adversarial training algorithms. The results highlight the potential of decentralized strategies to enhance the robustness of models in distributed settings.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Edge Learning for Predictive Maintenance in 6G Small Cell Networks</title>
<link>https://arxiv.org/abs/2509.11421</link>
<guid>https://arxiv.org/abs/2509.11421</guid>
<content:encoded><![CDATA[
arXiv:2509.11421v1 Announce Type: new 
Abstract: The rollout of 6G networks introduces unprecedented demands for autonomy, reliability, and scalability. However, the transmission of sensitive telemetry data to central servers raises concerns about privacy and bandwidth. To address this, we propose a federated edge learning framework for predictive maintenance in 6G small cell networks. The system adopts a Knowledge Defined Networking (KDN) architecture in Data, Knowledge, and Control Planes to support decentralized intelligence, telemetry-driven training, and coordinated policy enforcement. In the proposed model, each base station independently trains a failure prediction model using local telemetry metrics, including SINR, jitter, delay, and transport block size, without sharing raw data. A threshold-based multi-label encoding scheme enables the detection of concurrent fault conditions. We then conduct a comparative analysis of centralized and federated training strategies to evaluate their performance in this context. A realistic simulation environment is implemented using the ns-3 mmWave module, incorporating hybrid user placement and base station fault injection across various deployment scenarios. The learning pipeline is orchestrated via the Flower framework, and model aggregation is performed using the Federated Averaging (FedAvg) algorithm. Experimental results demonstrate that the federated model achieves performance comparable to centralized training in terms of accuracy and per-label precision, while preserving privacy and reducing communication overhead.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media</title>
<link>https://arxiv.org/abs/2509.11444</link>
<guid>https://arxiv.org/abs/2509.11444</guid>
<content:encoded><![CDATA[
arXiv:2509.11444v1 Announce Type: new 
Abstract: The emergence of decentralized social media platforms presents new opportunities and challenges for real-time analysis of public discourse. This study introduces CognitiveSky, an open-source and scalable framework designed for sentiment, emotion, and narrative analysis on Bluesky, a federated Twitter or X.com alternative. By ingesting data through Bluesky's Application Programming Interface (API), CognitiveSky applies transformer-based models to annotate large-scale user-generated content and produces structured and analyzable outputs. These summaries drive a dynamic dashboard that visualizes evolving patterns in emotion, activity, and conversation topics. Built entirely on free-tier infrastructure, CognitiveSky achieves both low operational cost and high accessibility. While demonstrated here for monitoring mental health discourse, its modular design enables applications across domains such as disinformation detection, crisis response, and civic sentiment analysis. By bridging large language models with decentralized networks, CognitiveSky offers a transparent, extensible tool for computational social science in an era of shifting digital ecosystems.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Partitioning techniques for non-centralized predictive control: A systematic review and novel theoretical insights</title>
<link>https://arxiv.org/abs/2509.11470</link>
<guid>https://arxiv.org/abs/2509.11470</guid>
<content:encoded><![CDATA[
arXiv:2509.11470v1 Announce Type: new 
Abstract: The partitioning problem is of central relevance for designing and implementing non-centralized Model Predictive Control (MPC) strategies for large-scale systems. These control approaches include decentralized MPC, distributed MPC, hierarchical MPC, and coalitional MPC. Partitioning a system for the application of non-centralized MPC consists of finding the best definition of the subsystems, and their allocation into groups for the definition of local controllers, to maximize the relevant performance indicators. The present survey proposes a novel systematization of the partitioning approaches in the literature in five main classes: optimization-based, algorithmic, community-detection-based, game-theoretic-oriented, and heuristic approaches. A unified graph-theoretical formalism, a mathematical re-formulation of the problem in terms of mixed-integer programming, the novel concepts of predictive partitioning and multi-topological representations, and a methodological formulation of quality metrics are developed to support the classification and further developments of the field. We analyze the different classes of partitioning techniques, and we present an overview of their strengths and limitations, which include a technical discussion about the different approaches. Representative case studies are discussed to illustrate the application of partitioning techniques for non-centralized MPC in various sectors, including power systems, water networks, wind farms, chemical processes, transportation systems, communication networks, industrial automation, smart buildings, and cyber-physical systems. An outlook of future challenges completes the survey.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dstack: A Zero Trust Framework for Confidential Containers</title>
<link>https://arxiv.org/abs/2509.11555</link>
<guid>https://arxiv.org/abs/2509.11555</guid>
<content:encoded><![CDATA[
arXiv:2509.11555v1 Announce Type: new 
Abstract: Web3 applications require execution platforms that maintain confidentiality and integrity without relying on centralized trust authorities. While Trusted Execution Environments (TEEs) offer promising capabilities for confidential computing, current implementations face significant limitations when applied to Web3 contexts, particularly in security reliability, censorship resistance, and vendor independence.
  This paper presents dstack, a comprehensive framework that transforms raw TEE technology into a true Zero Trust platform. We introduce three key innovations: (1) Portable Confidential Containers that enable seamless workload migration across heterogeneous TEE environments while maintaining security guarantees, (2) Decentralized Code Management that leverages smart contracts for transparent governance of TEE applications, and (3) Verifiable Domain Management that ensures secure and verifiable application identity without centralized authorities.
  These innovations are implemented through three core components: dstack-OS, dstack-KMS, and dstack-Gateway. Together, they demonstrate how to achieve both the performance advantages of VM-level TEE solutions and the trustless guarantees required by Web3 applications. Our evaluation shows that dstack provides comprehensive security guarantees while maintaining practical usability for real-world applications.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Evaluation to Enhancement: Large Language Models for Zero-Knowledge Proof Code Generation</title>
<link>https://arxiv.org/abs/2509.11708</link>
<guid>https://arxiv.org/abs/2509.11708</guid>
<content:encoded><![CDATA[
arXiv:2509.11708v1 Announce Type: new 
Abstract: Zero-knowledge proofs (ZKPs) are increasingly deployed in domains such as privacy-preserving authentication, blockchain scalability, and secure finance. However, authoring ZK programs remains challenging: unlike mainstream programming, ZK development requires reasoning about finite field arithmetic, constraint systems, and gadgets, making it knowledge-intensive and error-prone. While large language models (LLMs) have demonstrated strong code generation capabilities in general-purpose languages, their effectiveness for ZK programming, where correctness hinges on both language mastery and gadget-level reasoning, remains unexplored. To address this gap, we propose \textsc{ZK-Eval}, a domain-specific evaluation pipeline that probes LLM capabilities at three levels: language knowledge, gadget competence, and end-to-end program generation. Our evaluation of four state-of-the-art LLMs reveals that models excel at surface-level syntax but struggle with gadget usage and semantic correctness, often yielding incorrect programs. Based on these insights, we introduce \textsc{ZK-Coder}, an agentic framework that augments LLMs with constraint sketching, guided retrieval, and interactive repair. Experiments on Circom and Noir show substantial gains, with success rates improving from 17.35\% to 83.38\% and from 32.21\% to 90.05\%, respectively. With \textsc{ZK-Eval} and \textsc{ZK-Coder}, we establish a foundation for systematically measuring and augmenting LLMs in ZK code generation to lower barriers for practitioners and advance trustworthy computation.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-Constrained Intelligent Adversaries for Automation Vulnerability Testing: A Multi-Robot Patrol Case Study</title>
<link>https://arxiv.org/abs/2509.11971</link>
<guid>https://arxiv.org/abs/2509.11971</guid>
<content:encoded><![CDATA[
arXiv:2509.11971v1 Announce Type: new 
Abstract: Simulating hostile attacks of physical autonomous systems can be a useful tool to examine their robustness to attack and inform vulnerability-aware design. In this work, we examine this through the lens of multi-robot patrol, by presenting a machine learning-based adversary model that observes robot patrol behavior in order to attempt to gain undetected access to a secure environment within a limited time duration. Such a model allows for evaluation of a patrol system against a realistic potential adversary, offering insight into future patrol strategy design. We show that our new model outperforms existing baselines, thus providing a more stringent test, and examine its performance against multiple leading decentralized multi-robot patrol strategies.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MusicSwarm: Biologically Inspired Intelligence for Music Composition</title>
<link>https://arxiv.org/abs/2509.11973</link>
<guid>https://arxiv.org/abs/2509.11973</guid>
<content:encoded><![CDATA[
arXiv:2509.11973v1 Announce Type: new 
Abstract: We show that coherent, long-form musical composition can emerge from a decentralized swarm of identical, frozen foundation models that coordinate via stigmergic, peer-to-peer signals, without any weight updates. We compare a centralized multi-agent system with a global critic to a fully decentralized swarm in which bar-wise agents sense and deposit harmonic, rhythmic, and structural cues, adapt short-term memory, and reach consensus. Across symbolic, audio, and graph-theoretic analyses, the swarm yields superior quality while delivering greater diversity and structural variety and leads across creativity metrics. The dynamics contract toward a stable configuration of complementary roles, and self-similarity networks reveal a small-world architecture with efficient long-range connectivity and specialized bridging motifs, clarifying how local novelties consolidate into global musical form. By shifting specialization from parameter updates to interaction rules, shared memory, and dynamic consensus, MusicSwarm provides a compute- and data-efficient route to long-horizon creative structure that is immediately transferable beyond music to collaborative writing, design, and scientific discovery.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When MoE Meets Blockchain: A Trustworthy Distributed Framework of Large Models</title>
<link>https://arxiv.org/abs/2509.12141</link>
<guid>https://arxiv.org/abs/2509.12141</guid>
<content:encoded><![CDATA[
arXiv:2509.12141v2 Announce Type: new 
Abstract: As an enabling architecture of Large Models (LMs), Mixture of Experts (MoE) has become prevalent thanks to its sparsely-gated mechanism, which lowers computational overhead while maintaining learning performance comparable to dense LMs. The essence of MoE lies in utilizing a group of neural networks (called experts) with each specializing in different types of tasks, along with a trainable gating network that selectively activates a subset of these experts to handle specific tasks. Traditional cloud-based MoE encounters challenges such as prolonged response latency, high bandwidth consumption, and data privacy leakage. To address these issues, researchers have proposed to deploy MoE over distributed edge networks. However, a key concern of distributed MoE frameworks is the lack of trust in data interactions among distributed experts without the surveillance of any trusted authority, and thereby prone to potential attacks such as data manipulation. In response to the security issues of traditional distributed MoE, we propose a blockchain-aided trustworthy MoE (B-MoE) framework that consists of three layers: the edge layer, the blockchain layer, and the storage layer. In this framework, the edge layer employs the activated experts downloaded from the storage layer to process the learning tasks, while the blockchain layer functions as a decentralized trustworthy network to trace, verify, and record the computational results of the experts from the edge layer. The experimental results demonstrate that B-MoE is more robust to data manipulation attacks than traditional distributed MoE during both the training and inference processes.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V-ZOR: Enabling Verifiable Cross-Blockchain Communication via Quantum-Driven ZKP Oracle Relays</title>
<link>https://arxiv.org/abs/2509.10996</link>
<guid>https://arxiv.org/abs/2509.10996</guid>
<content:encoded><![CDATA[
arXiv:2509.10996v1 Announce Type: cross 
Abstract: Cross-chain bridges and oracle DAOs represent some of the most vulnerable components of decentralized systems, with more than $2.8 billion lost due to trust failures, opaque validation behavior, and weak incentives. Current oracle designs are based on multisigs, optimistic assumptions, or centralized aggregation, exposing them to attacks and delays. Moreover, predictable committee selection enables manipulation, which threatens data integrity across chains. We propose V-ZOR, a verifiable oracle relay that integrates zero-knowledge proofs, quantum-grade randomness, and cross-chain restaking to mitigate these risks. Each oracle packet includes a Halo 2 proof verifying that the reported data was correctly aggregated using a deterministic median. To prevent committee manipulation, VZOR reseeds its VRF using auditable quantum entropy, ensuring unpredictable and secure selection of reporters. Reporters stake once on a shared restaking hub; any connected chain can submit a fraud proof to trigger slashing, removing the need for multisigs or optimistic assumptions. A prototype in Sepolia and Scroll achieves sub-300k gas verification, one-block latency, and a 10x increase in collusion cost. V-ZOR demonstrates that combining ZK attestation with quantum-randomized restaking enables a trust-minimized, high-performance oracle layer for cross-chain DeFi.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Model-based Control Strategies for a Quadruple Tank System: Decentralized PID, LMPC, and NMPC</title>
<link>https://arxiv.org/abs/2509.11235</link>
<guid>https://arxiv.org/abs/2509.11235</guid>
<content:encoded><![CDATA[
arXiv:2509.11235v1 Announce Type: cross 
Abstract: This paper compares the performance of a decentralized proportional-integral-derivative (PID) controller, a linear model predictive controller (LMPC), and a nonlinear model predictive controller (NMPC) applied to a quadruple tank system (QTS). We present experimental data from a physical setup of the QTS as well as simulation results. The QTS is modeled as a stochastic nonlinear continuous-discrete-time system, with parameters estimated using a maximum-likelihood prediction-error-method (ML-PEM). The NMPC applies the stochastic nonlinear continuous-discrete-time model, while the LMPC uses a linearized version of the same model. We tune the decentralized PID controller using the simple internal model control (SIMC) rules. The SIMC rules require transfer functions of the process, and we obtain these from the linearized model. We compare the controller performances based on systematic tests using both the physical setup and the simulated QTS. We measure the performance in terms of tracking errors and rate of movement in the manipulated variables. The LMPC and the NMPC perform better than the decentralized PID control system for tracking pre-announced time-varying setpoints. For disturbance rejection, the MPCs perform only slightly better than the decentralized PID controller. The primary advantage of the MPCs is their ability to use the information of future setpoints. We demonstrate this by providing simulation results of the MPCs with and without such information. Finally, the NMPC achieves slightly improved tracking errors compared to the LMPC but at the expense of having a higher input rate of movement.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collusion-Resilience in Transaction Fee Mechanism Design</title>
<link>https://arxiv.org/abs/2402.09321</link>
<guid>https://arxiv.org/abs/2402.09321</guid>
<content:encoded><![CDATA[
arXiv:2402.09321v3 Announce Type: replace 
Abstract: Users bid in a transaction fee mechanism (TFM) to get their transactions included and confirmed by a blockchain protocol. Roughgarden (EC'21) initiated the formal treatment of TFMs and proposed three requirements: user incentive compatibility (UIC), miner incentive compatibility (MIC), and a form of collusion-resilience called OCA-proofness. Ethereum's EIP-1559 mechanism satisfies all three properties simultaneously when there is no contention between transactions, but loses the UIC property when there are too many eligible transactions to fit in a single block. Chung and Shi (SODA'23) considered an alternative notion of collusion-resilience, called $c$-side-contract-proofness ($c$-SCP), and showed that, when there is contention between transactions, no TFM can satisfy UIC, MIC, and $c$-SCP for any $c\geq 1$. OCA-proofness asserts that the users and a miner should not be able to "steal from the protocol." On the other hand, the $c$-SCP condition requires that a coalition of a miner and a subset of users should not be able to profit through strategic deviations (whether at the expense of the protocol or of the users outside the coalition).
  Our main result is the first proof that, when there is contention between transactions, no (possibly randomized) TFM in which users are expected to bid truthfully satisfies UIC, MIC, and OCA-proofness.This result resolves the main open question in Roughgarden (EC'21). We also suggest several relaxations of the basic model that allow our impossibility result to be circumvented.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanism Design for Automated Market Makers</title>
<link>https://arxiv.org/abs/2402.09357</link>
<guid>https://arxiv.org/abs/2402.09357</guid>
<content:encoded><![CDATA[
arXiv:2402.09357v3 Announce Type: replace 
Abstract: Blockchains have popularized automated market makers (AMMs). An AMM exchange is an application running on a blockchain which maintains a pool of crypto-assets and automatically trades assets with users governed by some pricing function that prices the assets based on their relative demand/supply. AMMs have created an important challenge commonly known as the Miner Extractable Value (MEV). In particular, the miners who control the contents and ordering of transactions in a block can extract value by front-running and back-running users' transactions, leading to arbitrage opportunities that guarantee them risk-free returns.
  In this paper, we consider how to design AMM mechanisms that eliminate MEV opportunities. Specifically, we propose a new AMM mechanism that processes all transactions contained within a block in a batch. We show that our new mechanism satisfies two tiers of guarantees. First, for legacy blockchains where each block is proposed by a single (possibly rotating) miner, we prove that our mechanism satisfies arbitrage resilience, i.e., a miner cannot gain risk-free profit. Moreover, we also guarantee fair treatment among all transactions within the same block, such that the miner is unable to sell off favorable positions in the block to users or arbitragers. Second, for blockchains where the block proposal process is decentralized and offers sequencing-fairness, we prove a stronger notion called incentive compatibility -- roughly speaking, we guarantee that any individual user's best response is to follow the honest strategy.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Formal Foundation for Blockchain Rollups</title>
<link>https://arxiv.org/abs/2406.16219</link>
<guid>https://arxiv.org/abs/2406.16219</guid>
<content:encoded><![CDATA[
arXiv:2406.16219v3 Announce Type: replace 
Abstract: Blockchains like Bitcoin and Ethereum have revolutionized digital transactions, yet scalability issues persist. Layer 2 solutions, such as validity proof Rollups (ZK-Rollups), aim to address these challenges by processing transactions off-chain and validating them on the main chain. However, concerns remain about security and censorship resistance, particularly regarding centralized control in Layer 2 and inadequate mechanisms for enforcing these properties through Layer 1 smart contracts. In their current form, L2s are susceptible to multisig attacks that can lead to total user funds loss. This work presents a formal analysis using the Alloy specification language to examine and design key Layer 2 functionalities, including forced transaction queues, safe blacklisting, and upgradeability. Through this analysis, we identify pitfalls in existing designs and introduce an enhanced model that has been model-checked to be correct. Finally, we propose a complete end-to-end methodology to analyze rollups' security and censorship resistance based on manually translating Alloy properties to property-based testing invariants, setting new standards.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Mean Field Social Control: A Unified Reinforcement Learning Framework</title>
<link>https://arxiv.org/abs/2502.20029</link>
<guid>https://arxiv.org/abs/2502.20029</guid>
<content:encoded><![CDATA[
arXiv:2502.20029v2 Announce Type: replace 
Abstract: This paper studies linear quadratic Gaussian robust mean field social control problems in the presence of multiplicative noise. We aim to compute asymptotic decentralized strategies without requiring full prior knowledge of agents' dynamics. The primary challenges lie in solving an indefinite stochastic algebraic Riccati equation for feedback gains, and an indefinite algebraic Riccati equation for feedforward gains. To overcome these challenges, we first propose a unified dual-loop iterative framework that handles both indefinite Riccati-type equations, and provide rigorous convergence proofs for both the outer-loop and inner-loop iterations. Secondly, considering the potential biases arising in the iterative processes due to estimation and modeling errors, we verify the robustness of the proposed algorithm using the small-disturbance input-to-state stability technique. Convergence to a neighborhood of the optimal solution is thus ensured, even in the existence of disturbances. Finally, to relax the limitation of requiring precise knowledge of agents' dynamics, we employ the integral reinforcement learning technique to develop a data-driven method within the dual-loop iterative framework. A numerical example is provided to demonstrate the effectiveness of the proposed algorithm.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Implicit Barrier from Utility Maximization: Lightweight Interior-Point Methods for Market Equilibrium</title>
<link>https://arxiv.org/abs/2508.04822</link>
<guid>https://arxiv.org/abs/2508.04822</guid>
<content:encoded><![CDATA[
arXiv:2508.04822v2 Announce Type: replace-cross 
Abstract: We study the computation of the market equilibrium in Fisher exchange markets with divisible goods and players endowed with heterogeneous utilities. In particular, we consider the decentralized polynomial-time interior-point strategies that update \emph{only} the prices, mirroring the t\^atonnement process. The key ingredient is the \emph{implicit barrier} inherent from utility maximization, which induces unbounded demand when the goods are almost free of charge. Focusing on a ubiquitous class of utilities, we formalize this observation. A companion result suggests that no additional effort is required for computing high-order derivatives; all the necessary information is readily available when collecting the best responses. To tackle the Newton systems in the interior-point methods, we present an explicitly invertible approximation of the Hessian operator with high probability guarantees, and a scaling matrix that minimizes the condition number of the linear system. Building on these tools, we design two inexact lightweight interior-point methods. One such method has $\cO(\log(\tfrac{1}{\epsilon}))$ complexity rate. Under mild conditions, the other method achieves a non-asymptotic superlinear convergence rate. Preliminary experiments are presented to justify the capability of the proposed methods for large-scale problems. Extensions of our approach are also discussed.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Setchain Algorithms for Blockchain Scalability</title>
<link>https://arxiv.org/abs/2509.09795</link>
<guid>https://arxiv.org/abs/2509.09795</guid>
<content:encoded><![CDATA[
arXiv:2509.09795v1 Announce Type: new 
Abstract: Setchain has been proposed to increase blockchain scalability by relaxing the strict total order requirement among transactions. Setchain organizes elements into a sequence of sets, referred to as epochs, so that elements within each epoch are unordered. In this paper, we propose and evaluate three distinct Setchain algorithms, that leverage an underlying block-based ledger. Vanilla is a basic implementation that serves as a reference point. Compresschain aggregates elements into batches, and compresses these batches before appending them as epochs in the ledger. Hashchain converts batches into fixed-length hashes which are appended as epochs in the ledger. This requires Hashchain to use a distributed service to obtain the batch contents from its hash. To allow light clients to safely interact with only one server, the proposed algorithms maintain, as part of the Setchain, proofs for the epochs. An epoch-proof is the hash of the epoch, cryptographically signed by a server. A client can verify the correctness of an epoch with $f+1$ epoch-proofs (where $f$ is the maximum number of Byzantine servers assumed). All three Setchain algorithms are implemented on top of the CometBFT blockchain application platform. We conducted performance evaluations across various configurations, using clusters of four, seven, and ten servers. Our results show that the Setchain algorithms reach orders of magnitude higher throughput than the underlying blockchain, and achieve finality with latency below 4 seconds.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ordered Consensus with Equal Opportunity</title>
<link>https://arxiv.org/abs/2509.09868</link>
<guid>https://arxiv.org/abs/2509.09868</guid>
<content:encoded><![CDATA[
arXiv:2509.09868v1 Announce Type: new 
Abstract: The specification of state machine replication (SMR) has no requirement on the final total order of commands. In blockchains based on SMR, however, order matters, since different orders could provide their clients with different financial rewards. Ordered consensus augments the specification of SMR to include specific guarantees on such order, with a focus on limiting the influence of Byzantine nodes. Real-world ordering manipulations, however, can and do happen even without Byzantine replicas, typically because of factors, such as faster networks or closer proximity to the blockchain infrastructure, that give some clients an unfair advantage. To address this challenge, this paper proceeds to extend ordered consensus by requiring it to also support equal opportunity, a concrete notion of fairness, widely adopted in social sciences. Informally, equal opportunity requires that two candidates who, according to a set of criteria deemed to be relevant, are equally qualified for a position (in our case, a specific slot in the SMR total order), should have an equal chance of landing it. We show how randomness can be leveraged to keep bias in check, and, to this end, introduce the secret random oracle (SRO), a system component that generates randomness in a fault-tolerant manner. We describe two SRO designs based, respectively, on trusted hardware and threshold verifiable random functions, and instantiate them in Bercow, a new ordered consensus protocol that, by approximating equal opportunity up to within a configurable factor, can effectively mitigate well-known ordering attacks in SMR-based blockchains.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Predictions in Power System Voltage Control: An Adaptive Approach</title>
<link>https://arxiv.org/abs/2509.09937</link>
<guid>https://arxiv.org/abs/2509.09937</guid>
<content:encoded><![CDATA[
arXiv:2509.09937v1 Announce Type: new 
Abstract: High variability of solar PV and sudden changes in load (e.g., electric vehicles and storage) can lead to large voltage fluctuations in the distribution system. In recent years, a number of controllers have been designed to optimize voltage control. These controllers, however, almost always assume that the net load in the system remains constant over a sufficiently long time, such that the control actions converge before the load changes again. Given the intermittent and uncertain nature of renewable resources, it is becoming important to explicitly consider net load that is time-varying.
  This paper proposes an adaptive approach to voltage control in power systems with significant time-varying net load. We leverage advances in short-term load forecasting, where the net load in the system can be partially predicted using local measurements. We integrate these predictions into the design of adaptive controllers, and prove that the overall control architecture achieves input-to-state stability in a decentralized manner. We optimize the control policy through reinforcement learning. Case studies are conducted using time-varying load data from a real-world distribution system.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SmartCoder-R1: Towards Secure and Explainable Smart Contract Generation with Security-Aware Group Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2509.09942</link>
<guid>https://arxiv.org/abs/2509.09942</guid>
<content:encoded><![CDATA[
arXiv:2509.09942v1 Announce Type: new 
Abstract: Smart contracts automate the management of high-value assets, where vulnerabilities can lead to catastrophic financial losses. This challenge is amplified in Large Language Models (LLMs) by two interconnected failures: they operate as unauditable "black boxes" lacking a transparent reasoning process, and consequently, generate code riddled with critical security vulnerabilities. To address both issues, we propose SmartCoder-R1 (based on Qwen2.5-Coder-7B), a novel framework for secure and explainable smart contract generation. It begins with Continual Pre-training (CPT) to specialize the model. We then apply Long Chain-of-Thought Supervised Fine-Tuning (L-CoT SFT) on 7,998 expert-validated reasoning-and-code samples to train the model to emulate human security analysis. Finally, to directly mitigate vulnerabilities, we employ Security-Aware Group Relative Policy Optimization (S-GRPO), a reinforcement learning phase that refines the generation policy by optimizing a weighted reward signal for compilation success, security compliance, and format correctness. Evaluated against 17 baselines on a benchmark of 756 real-world functions, SmartCoder-R1 establishes a new state of the art, achieving top performance across five key metrics: a ComPass of 87.70%, a VulRate of 8.60%, a SafeAval of 80.16%, a FuncRate of 53.84%, and a FullRate of 50.53%. This FullRate marks a 45.79% relative improvement over the strongest baseline, DeepSeek-R1. Crucially, its generated reasoning also excels in human evaluations, achieving high-quality ratings for Functionality (82.7%), Security (85.3%), and Clarity (90.7%).
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedRP: A Communication-Efficient Approach for Differentially Private Federated Learning Using Random Projection</title>
<link>https://arxiv.org/abs/2509.10041</link>
<guid>https://arxiv.org/abs/2509.10041</guid>
<content:encoded><![CDATA[
arXiv:2509.10041v1 Announce Type: new 
Abstract: Federated learning (FL) offers an innovative paradigm for collaborative model training across decentralized devices, such as smartphones, balancing enhanced predictive performance with the protection of user privacy in sensitive areas like Internet of Things (IoT) and medical data analysis. Despite its advantages, FL encounters significant challenges related to user privacy protection against potential attacks and the management of communication costs. This paper introduces a novel federated learning algorithm called FedRP, which integrates random projection techniques with the Alternating Direction Method of Multipliers (ADMM) optimization framework. This approach enhances privacy by employing random projection to reduce the dimensionality of model parameters prior to their transmission to a central server, reducing the communication cost. The proposed algorithm offers a strong $(\epsilon, \delta)$-differential privacy guarantee, demonstrating resilience against data reconstruction attacks. Experimental results reveal that FedRP not only maintains high model accuracy but also outperforms existing methods, including conventional differential privacy approaches and FedADMM, in terms of both privacy preservation and communication efficiency.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cost-Free Personalization via Information-Geometric Projection in Bayesian Federated Learning</title>
<link>https://arxiv.org/abs/2509.10132</link>
<guid>https://arxiv.org/abs/2509.10132</guid>
<content:encoded><![CDATA[
arXiv:2509.10132v1 Announce Type: new 
Abstract: Bayesian Federated Learning (BFL) combines uncertainty modeling with decentralized training, enabling the development of personalized and reliable models under data heterogeneity and privacy constraints. Existing approaches typically rely on Markov Chain Monte Carlo (MCMC) sampling or variational inference, often incorporating personalization mechanisms to better adapt to local data distributions. In this work, we propose an information-geometric projection framework for personalization in parametric BFL. By projecting the global model onto a neighborhood of the user's local model, our method enables a tunable trade-off between global generalization and local specialization. Under mild assumptions, we show that this projection step is equivalent to computing a barycenter on the statistical manifold, allowing us to derive closed-form solutions and achieve cost-free personalization. We apply the proposed approach to a variational learning setup using the Improved Variational Online Newton (IVON) optimizer and extend its application to general aggregation schemes in BFL. Empirical evaluations under heterogeneous data distributions confirm that our method effectively balances global and local performance with minimal computational overhead.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks</title>
<link>https://arxiv.org/abs/2509.10163</link>
<guid>https://arxiv.org/abs/2509.10163</guid>
<content:encoded><![CDATA[
arXiv:2509.10163v1 Announce Type: new 
Abstract: As sixth-generation (6G) networks move toward ultra-dense, intelligent edge environments, efficient resource management under stringent privacy, mobility, and energy constraints becomes critical. This paper introduces a novel Federated Multi-Agent Reinforcement Learning (Fed-MARL) framework that incorporates cross-layer orchestration of both the MAC layer and application layer for energy-efficient, privacy-preserving, and real-time resource management across heterogeneous edge devices. Each agent uses a Deep Recurrent Q-Network (DRQN) to learn decentralized policies for task offloading, spectrum access, and CPU energy adaptation based on local observations (e.g., queue length, energy, CPU usage, and mobility). To protect privacy, we introduce a secure aggregation protocol based on elliptic curve Diffie Hellman key exchange, which ensures accurate model updates without exposing raw data to semi-honest adversaries. We formulate the resource management problem as a partially observable multi-agent Markov decision process (POMMDP) with a multi-objective reward function that jointly optimizes latency, energy efficiency, spectral efficiency, fairness, and reliability under 6G-specific service requirements such as URLLC, eMBB, and mMTC. Simulation results demonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines in task success rate, latency, energy efficiency, and fairness, while ensuring robust privacy protection and scalability in dynamic, resource-constrained 6G edge networks.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XBOF: A Cost-Efficient CXL JBOF with Inter-SSD Compute Resource Sharing</title>
<link>https://arxiv.org/abs/2509.10251</link>
<guid>https://arxiv.org/abs/2509.10251</guid>
<content:encoded><![CDATA[
arXiv:2509.10251v1 Announce Type: new 
Abstract: Enterprise SSDs integrate numerous computing resources (e.g., ARM processor and onboard DRAM) to satisfy the ever-increasing performance requirements of I/O bursts. While these resources substantially elevate the monetary costs of SSDs, the sporadic nature of I/O bursts causes severe SSD resource underutilization in just a bunch of flash (JBOF) level. Tackling this challenge, we propose XBOF, a cost-efficient JBOF design, which only reserves moderate computing resources in SSDs at low monetary cost, while achieving demanded I/O performance through efficient inter-SSD resource sharing. Specifically, XBOF first disaggregates SSD architecture into multiple disjoint parts based on their functionality, enabling fine-grained SSD internal resource management. XBOF then employs a decentralized scheme to manage these disaggregated resources and harvests the computing resources of idle SSDs to assist busy SSDs in handling I/O bursts. This idea is facilitated by the cache-coherent capability of Compute eXpress Link (CXL), with which the busy SSDs can directly utilize the harvested computing resources to accelerate metadata processing. The evaluation results show that XBOF improves SSD resource utilization by 50.4% and saves 19.0% monetary costs with a negligible performance loss, compared to existing JBOF designs.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExDoS: Expert-Guided Dual-Focus Cross-Modal Distillation for Smart Contract Vulnerability Detection</title>
<link>https://arxiv.org/abs/2509.10252</link>
<guid>https://arxiv.org/abs/2509.10252</guid>
<content:encoded><![CDATA[
arXiv:2509.10252v1 Announce Type: new 
Abstract: The success of smart contracts has made them a target for attacks, but their closed-source nature often forces vulnerability detection to work on bytecode, which is inherently more challenging than source-code-based analysis. While recent studies try to align source and bytecode embeddings during training to transfer knowledge, current methods rely on graph-level alignment that obscures fine-grained structural and semantic correlations between the two modalities. Moreover, the absence of precise vulnerability patterns and granular annotations in bytecode leads to depriving the model of crucial supervisory signals for learning discriminant features. We propose ExDoS to transfer rich semantic knowledge from source code to bytecode, effectively supplementing the source code prior in practical settings. Specifically, we construct semantic graphs from source code and control-flow graphs from bytecode. To address obscured local signals in graph-level contract embeddings, we propose a Dual-Attention Graph Network introducing a novel node attention aggregation module to enhance local pattern capture in graph embeddings. Furthermore, by summarizing existing source code vulnerability patterns and designing a corresponding set of bytecode-level patterns for each, we construct the first dataset of vulnerability pattern annotations aligned with source code definitions to facilitate fine-grained cross-modal alignment and the capture of function-level vulnerability signals. Finally, we propose a dual-focus objective for our cross-modal distillation framework, comprising: a Global Semantic Distillation Loss for transferring graph-level knowledge and a Local Semantic Distillation Loss enabling expert-guided, fine-grained vulnerability-specific distillation. Experiments on real-world contracts demonstrate that our method achieves consistent F1-score improvements (3\%--6\%) over strong baselines.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proof of AutoML: SDN based Secure Energy Trading with Blockchain in Disaster Case</title>
<link>https://arxiv.org/abs/2509.10291</link>
<guid>https://arxiv.org/abs/2509.10291</guid>
<content:encoded><![CDATA[
arXiv:2509.10291v1 Announce Type: new 
Abstract: In disaster scenarios where conventional energy infrastructure is compromised, secure and traceable energy trading between solar-powered households and mobile charging units becomes a necessity. To ensure the integrity of such transactions over a blockchain network, robust and unpredictable nonce generation is vital. This study proposes an SDN-enabled architecture where machine learning regressors are leveraged not for their accuracy, but for their potential to generate randomized values suitable as nonce candidates. Therefore, it is newly called Proof of AutoML. Here, SDN allows flexible control over data flows and energy routing policies even in fragmented or degraded networks, ensuring adaptive response during emergencies. Using a 9000-sample dataset, we evaluate five AutoML-selected regression models - Gradient Boosting, LightGBM, Random Forest, Extra Trees, and K-Nearest Neighbors - not by their prediction accuracy, but by their ability to produce diverse and non-deterministic outputs across shuffled data inputs. Randomness analysis reveals that Random Forest and Extra Trees regressors exhibit complete dependency on randomness, whereas Gradient Boosting, K-Nearest Neighbors and LightGBM show strong but slightly lower randomness scores (97.6%, 98.8% and 99.9%, respectively). These findings highlight that certain machine learning models, particularly tree-based ensembles, may serve as effective and lightweight nonce generators within blockchain-secured, SDN-based energy trading infrastructures resilient to disaster conditions.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bitcoin Cross-Chain Bridge: A Taxonomy and Its Promise in Artificial Intelligence of Things</title>
<link>https://arxiv.org/abs/2509.10413</link>
<guid>https://arxiv.org/abs/2509.10413</guid>
<content:encoded><![CDATA[
arXiv:2509.10413v1 Announce Type: new 
Abstract: Bitcoin's limited scripting capabilities and lack of native interoperability mechanisms have constrained its integration into the broader blockchain ecosystem, especially decentralized finance (DeFi) and multi-chain applications. This paper presents a comprehensive taxonomy of Bitcoin cross-chain bridge protocols, systematically analyzing their trust assumptions, performance characteristics, and applicability to the Artificial Intelligence of Things (AIoT) scenarios. We categorize bridge designs into three main types: naive token swapping, pegged-asset bridges, and arbitrary-message bridges. Each category is evaluated across key metrics such as trust model, latency, capital efficiency, and DeFi composability. Emerging innovations like BitVM and recursive sidechains are highlighted for their potential to enable secure, scalable, and programmable Bitcoin interoperability. Furthermore, we explore practical use cases of cross-chain bridges in AIoT applications, including decentralized energy trading, healthcare data integration, and supply chain automation. This taxonomy provides a foundational framework for researchers and practitioners seeking to design secure and efficient cross-chain infrastructures in AIoT systems.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentially Private Decentralized Dataset Synthesis Through Randomized Mixing with Correlated Noise</title>
<link>https://arxiv.org/abs/2509.10385</link>
<guid>https://arxiv.org/abs/2509.10385</guid>
<content:encoded><![CDATA[
arXiv:2509.10385v1 Announce Type: cross 
Abstract: In this work, we explore differentially private synthetic data generation in a decentralized-data setting by building on the recently proposed Differentially Private Class-Centric Data Aggregation (DP-CDA). DP-CDA synthesizes data in a centralized setting by mixing multiple randomly-selected samples from the same class and injecting carefully calibrated Gaussian noise, ensuring ({\epsilon}, {\delta})-differential privacy. When deployed in a decentralized or federated setting, where each client holds only a small partition of the data, DP-CDA faces new challenges. The limited sample size per client increases the sensitivity of local computations, requiring higher noise injection to maintain the differential privacy guarantee. This, in turn, leads to a noticeable degradation in the utility compared to the centralized setting. To mitigate this issue, we integrate the Correlation-Assisted Private Estimation (CAPE) protocol into the federated DP-CDA framework and propose CAPE Assisted Federated DP-CDA algorithm. CAPE enables limited collaboration among the clients by allowing them to generate jointly distributed (anti-correlated) noise that cancels out in aggregate, while preserving privacy at the individual level. This technique significantly improves the privacy-utility trade-off in the federated setting. Extensive experiments on MNIST and FashionMNIST datasets demonstrate that the proposed CAPE Assisted Federated DP-CDA approach can achieve utility comparable to its centralized counterpart under some parameter regime, while maintaining rigorous differential privacy guarantees.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Group Fairness in Federated Learning: Challenges, Taxonomy of Solutions and Directions for Future Research</title>
<link>https://arxiv.org/abs/2410.03855</link>
<guid>https://arxiv.org/abs/2410.03855</guid>
<content:encoded><![CDATA[
arXiv:2410.03855v2 Announce Type: replace 
Abstract: Group fairness in machine learning is an important area of research focused on achieving equitable outcomes across different groups defined by sensitive attributes such as race or gender. Federated Learning, a decentralized approach to training machine learning models across multiple clients, amplifies the need for fairness methodologies due to its inherent heterogeneous data distributions that can exacerbate biases. The intersection of Federated Learning and group fairness has attracted significant interest, with 48 research works specifically dedicated to addressing this issue. However, no comprehensive survey has specifically focused on group fairness in Federated Learning. In this work, we analyze the key challenges of this topic, propose practices for its identification and benchmarking, and create a novel taxonomy based on criteria such as data partitioning, location, and strategy. Furthermore, we analyze broader concerns, review how different approaches handle the complexities of various sensitive attributes, examine common datasets and applications, and discuss the ethical, legal, and policy implications of group fairness in FL. We conclude by highlighting key areas for future research, emphasizing the need for more methods to address the complexities of achieving group fairness in federated systems.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Fair Exchange with Advertising</title>
<link>https://arxiv.org/abs/2503.10411</link>
<guid>https://arxiv.org/abs/2503.10411</guid>
<content:encoded><![CDATA[
arXiv:2503.10411v2 Announce Type: replace 
Abstract: Before a fair exchange takes place, there is typically an advertisement phase with the goal of increasing the appeal of possessing a digital asset while keeping it sufficiently hidden. Advertisement phases are implicit in mainstream definitions, and therefore are not explicitly integrated within fair-exchange protocols. In this work we give an explicit definition for such a fair exchange in a setting where parties communicate via broadcast messages only (i.e., no point-to-point connection between seller and buyer is needed). Next, we construct a fair-exchange protocol satisfying our new definition using zk-SNARKs and relying on mainstream decentralized platforms (i.e., a blockchain with smart contracts like Ethereum and a decentralized storage system like IPFS). Experimental results confirm the practical relevance of our decentralized approach, paving the road towards building decentralized marketplaces where users can, even anonymously, and without direct off-chain communications, effectively advertise and exchange their digital assets as part of a system of enhanced NFTs.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Local Voltage Control for Active Distribution Networks</title>
<link>https://arxiv.org/abs/2509.09048</link>
<guid>https://arxiv.org/abs/2509.09048</guid>
<content:encoded><![CDATA[
arXiv:2509.09048v1 Announce Type: new 
Abstract: Distribution networks face challenges from the increasing deployment of Distributed Energy Resources (DERs) and the emergence of bidirectional power flows. We propose a decentralized Volt/VAr control method based on a saddle-point reformulation and consensus+innovation (C+I) updates. Each agent at a controllable bus computes and enforces its own set-points using only neighbor communication. Our method embeds passive buses directly, preserves network physics through a linearized Jacobian model, and avoids any supervisory nodes. Simulation results on a modified CIGRE low-voltage network show voltage stability improvement within operational limits, indicating the viability of a fully decentralized (edge-based) Volt/VAr control solution.
]]></content:encoded>
<pubDate>Sat, 13 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scoping Review of Machine Learning Applications in Power System Protection and Disturbance Management</title>
<link>https://arxiv.org/abs/2509.09053</link>
<guid>https://arxiv.org/abs/2509.09053</guid>
<content:encoded><![CDATA[
arXiv:2509.09053v1 Announce Type: new 
Abstract: The integration of renewable and distributed energy resources reshapes modern power systems, challenging conventional protection schemes. This scoping review synthesizes recent literature on machine learning (ML) applications in power system protection and disturbance management, following the PRISMA for Scoping Reviews framework. Based on over 100 publications, three key objectives are addressed: (i) assessing the scope of ML research in protection tasks; (ii) evaluating ML performance across diverse operational scenarios; and (iii) identifying methods suitable for evolving grid conditions. ML models often demonstrate high accuracy on simulated datasets; however, their performance under real-world conditions remains insufficiently validated. The existing literature is fragmented, with inconsistencies in methodological rigor, dataset quality, and evaluation metrics. This lack of standardization hampers the comparability of results and limits the generalizability of findings. To address these challenges, this review introduces a ML-oriented taxonomy for protection tasks, resolves key terminological inconsistencies, and advocates for standardized reporting practices. It further provides guidelines for comprehensive dataset documentation, methodological transparency, and consistent evaluation protocols, aiming to improve reproducibility and enhance the practical relevance of research outcomes. Critical gaps remain, including the scarcity of real-world validation, insufficient robustness testing, and limited consideration of deployment feasibility. Future research should prioritize public benchmark datasets, realistic validation methods, and advanced ML architectures. These steps are essential to move ML-based protection from theoretical promise to practical deployment in increasingly dynamic and decentralized power systems.
]]></content:encoded>
<pubDate>Sat, 13 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persuasion Gains and Losses from Peer Communication</title>
<link>https://arxiv.org/abs/2509.09099</link>
<guid>https://arxiv.org/abs/2509.09099</guid>
<content:encoded><![CDATA[
arXiv:2509.09099v1 Announce Type: new 
Abstract: We study a Bayesian persuasion setting in which a sender wants to persuade a critical mass of receivers by revealing partial information about the state to them. The homogeneous binary-action receivers are located on a communication network, and each observes the private messages sent to them and their immediate neighbors. We examine how the sender's expected utility varies with increased communication among receivers. We show that for general families of networks, extending the network can strictly benefit the sender. Thus, the sender's gain from persuasion is not monotonic in network density. Moreover, many network extensions can achieve the upper bound on the sender's expected utility among all networks, which corresponds to the payoff in an empty network. This is the case in networks reflecting a clear informational hierarchy (e.g., in global corporations), as well as in decentralized networks in which information originates from multiple sources (e.g., influencers in social media). Finally, we show that a slight modification to the structure of some of these networks precludes the possibility of such beneficial extensions. Overall, our results caution against presuming that more communication necessarily leads to better collective outcomes.
]]></content:encoded>
<pubDate>Sat, 13 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Regulatory Multi-Agent Collaboration: Architecture, Challenges, and Solutions</title>
<link>https://arxiv.org/abs/2509.09215</link>
<guid>https://arxiv.org/abs/2509.09215</guid>
<content:encoded><![CDATA[
arXiv:2509.09215v1 Announce Type: new 
Abstract: Large language models (LLMs)-empowered autonomous agents are transforming both digital and physical environments by enabling adaptive, multi-agent collaboration. While these agents offer significant opportunities across domains such as finance, healthcare, and smart manufacturing, their unpredictable behaviors and heterogeneous capabilities pose substantial governance and accountability challenges. In this paper, we propose a blockchain-enabled layered architecture for regulatory agent collaboration, comprising an agent layer, a blockchain data layer, and a regulatory application layer. Within this framework, we design three key modules: (i) an agent behavior tracing and arbitration module for automated accountability, (ii) a dynamic reputation evaluation module for trust assessment in collaborative scenarios, and (iii) a malicious behavior forecasting module for early detection of adversarial activities. Our approach establishes a systematic foundation for trustworthy, resilient, and scalable regulatory mechanisms in large-scale agent ecosystems. Finally, we discuss the future research directions for blockchain-enabled regulatory frameworks in multi-agent systems.
]]></content:encoded>
<pubDate>Sat, 13 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voltage Synchronization and Proportional Current Sharing of Grid-Forming Inverters</title>
<link>https://arxiv.org/abs/2509.09277</link>
<guid>https://arxiv.org/abs/2509.09277</guid>
<content:encoded><![CDATA[
arXiv:2509.09277v1 Announce Type: new 
Abstract: Most previously proposed controllers are analyzed in the small-signal/quasi-steady regime rather than large-signal or transient stability for grid-forming inverters (GFMI). Additionally, methods that presume system-wide data--global measurements and complete grid-model knowledge--are challenging to realize in practice and unsuitable for large-scale operation. Moreover, proportional current sharing is rarely embedded into them. The whole system is a high-order, nonlinear differential system, making analysis intractable without principled simplifications. Hence, contraction stability analysis in GFMI is proposed to guarantee the large-signal stability. Furthermore, a contraction-based controller is proposed to synchronize GFMI. Additionally, this paper proposes integrating an auxiliary virtual-impedance layer into the contraction-based controller to achieve proportional current sharing, while the GFMI retains global stability and voltage synchronization. A dispatchable virtual oscillator control (dVOC), also known as the Andronov--Hopf oscillator (AHO) is used to validate the proposed contraction stability analysis and contraction-based controller with virtual-impedance. It is proved that the complex multi-converter system can achieve output-feedback contraction under large-signal operation. Therefore, without requiring system-wide data, the proposed method offers voltage synchronization, decentralized stability conditions for the transient stability of AHO and proportional current sharing, beyond prior small-signal, quasi-steady analysis.
]]></content:encoded>
<pubDate>Sat, 13 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning-Based Data-Assisted Port-Hamiltonian Control for Free-Floating Space Manipulators</title>
<link>https://arxiv.org/abs/2509.09563</link>
<guid>https://arxiv.org/abs/2509.09563</guid>
<content:encoded><![CDATA[
arXiv:2509.09563v1 Announce Type: new 
Abstract: A generic data-assisted control architecture within the port-Hamiltonian framework is proposed, introducing a physically meaningful observable that links conservative dynamics to all actuation, dissipation, and disturbance channels. A robust, model-based controller combined with a high-gain decentralized integrator establishes large robustness margins and strict time-scale separation, ensuring that subsequent learning cannot destabilize the primary dynamics. Learning, selected for its generalizability, is then applied to capture complex, unmodeled effects, despite inherent delay and transient error during adaptation. Formal Lyapunov analysis with explicit stability bounds guarantees convergence under bounded learning errors. The structured design confines learning to the simplest part of the dynamics, enhancing data efficiency while preserving physical interpretability. The approach is generic, with a free-floating space manipulator orientation control task, including integrated null-space collision avoidance, serving as a case study to demonstrate robust tracking performance and applicability to broader robotic domains.
]]></content:encoded>
<pubDate>Sat, 13 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-in-the-loop Learning Through Decentralized Communication Mechanisms</title>
<link>https://arxiv.org/abs/2509.09574</link>
<guid>https://arxiv.org/abs/2509.09574</guid>
<content:encoded><![CDATA[
arXiv:2509.09574v1 Announce Type: new 
Abstract: Information sharing platforms like TripAdvisor and Waze involve human agents as both information producers and consumers. All these platforms operate in a centralized way to collect agents' latest observations of new options (e.g., restaurants, hotels, travel routes) and share such information with all in real time. However, after hearing the central platforms' live updates, many human agents are found selfish and unwilling to further explore unknown options for the benefit of others in the long run. To regulate the human-in-the-loop learning (HILL) game against selfish agents' free-riding, this paper proposes a paradigm shift from centralized to decentralized way of operation that forces agents' local explorations through restricting information sharing. When game theory meets distributed learning, we formulate our decentralized communication mechanism's design as a new multi-agent Markov decision process (MA-MDP), and derive its analytical condition to outperform today's centralized operation. As the optimal decentralized communication mechanism in MA-MDP is NP-hard to solve, we present an asymptotically optimal algorithm with linear complexity to determine the mechanism's timing of intermittent information sharing. Then we turn to non-myopic agents who may revert to even over-explore, and adapt our mechanism design to work. Simulation experiments using real-world dataset demonstrate the effectiveness of our decentralized mechanisms for various scenarios.
]]></content:encoded>
<pubDate>Sat, 13 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>I Know Who Clones Your Code: Interpretable Smart Contract Similarity Detection</title>
<link>https://arxiv.org/abs/2509.09630</link>
<guid>https://arxiv.org/abs/2509.09630</guid>
<content:encoded><![CDATA[
arXiv:2509.09630v1 Announce Type: new 
Abstract: Widespread reuse of open-source code in smart contract development boosts programming efficiency but significantly amplifies bug propagation across contracts, while dedicated methods for detecting similar smart contract functions remain very limited. Conventional abstract-syntax-tree (AST) based methods for smart contract similarity detection face challenges in handling intricate tree structures, which impedes detailed semantic comparison of code. Recent deep-learning based approaches tend to overlook code syntax and detection interpretability, resulting in suboptimal performance.
  To fill this research gap, we introduce SmartDetector, a novel approach for computing similarity between smart contract functions, explainable at the fine-grained statement level. Technically, SmartDetector decomposes the AST of a smart contract function into a series of smaller statement trees, each reflecting a structural element of the source code. Then, SmartDetector uses a classifier to compute the similarity score of two functions by comparing each pair of their statement trees. To address the infinite hyperparameter space of the classifier, we mathematically derive a cosine-wise diffusion process to efficiently search optimal hyperparameters. Extensive experiments conducted on three large real-world datasets demonstrate that SmartDetector outperforms current state-of-the-art methods by an average improvement of 14.01% in F1-score, achieving an overall average F1-score of 95.88%.
]]></content:encoded>
<pubDate>Sat, 13 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accountable Liveness</title>
<link>https://arxiv.org/abs/2504.12218</link>
<guid>https://arxiv.org/abs/2504.12218</guid>
<content:encoded><![CDATA[
arXiv:2504.12218v2 Announce Type: replace 
Abstract: Safety and liveness are the two classical security properties of consensus protocols. Recent works have strengthened safety with accountability: should any safety violation occur, a sizable fraction of adversary nodes can be proven to be protocol violators. This paper studies to what extent analogous accountability guarantees are achievable for liveness. To reveal the full complexity of this question, we introduce an interpolation between the classical synchronous and partially-synchronous models that we call the $x$-partially-synchronous network model in which, intuitively, at most an $x$ fraction of the time steps in any sufficiently long interval are asynchronous (and, as with a partially-synchronous network, all time steps are synchronous following the passage of an unknown "global stablization time"). We prove a precise characterization of the parameter regime in which accountable liveness is achievable: if and only if $x < 1/2$ and $f < n/2$, where $n$ denotes the number of nodes and $f$ the number of nodes controlled by an adversary. We further refine the problem statement and our analysis by parameterizing by the number of violating nodes identified following a liveness violation, and provide evidence that the guarantees achieved by our protocol are near-optimal (as a function of $x$ and $f$). Our results provide rigorous foundations for liveness-accountability heuristics such as the "inactivity leaks" employed in Ethereum.
]]></content:encoded>
<pubDate>Sat, 13 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization Methods and Software for Federated Learning</title>
<link>https://arxiv.org/abs/2509.08120</link>
<guid>https://arxiv.org/abs/2509.08120</guid>
<content:encoded><![CDATA[
arXiv:2509.08120v1 Announce Type: new 
Abstract: Federated Learning (FL) is a novel, multidisciplinary Machine Learning paradigm where multiple clients, such as mobile devices, collaborate to solve machine learning problems. Initially introduced in Kone{\v{c}}n{\'y} et al. (2016a,b); McMahan et al. (2017), FL has gained further attention through its inclusion in the National AI Research and Development Strategic Plan (2023 Update) of the United States (Science and on Artificial Intelligence, 2023). The FL training process is inherently decentralized and often takes place in less controlled settings compared to data centers, posing unique challenges distinct from those in fully controlled environments. In this thesis, we identify five key challenges in Federated Learning and propose novel approaches to address them. These challenges arise from the heterogeneity of data and devices, communication issues, and privacy concerns for clients in FL training. Moreover, even well-established theoretical advances in FL require diverse forms of practical implementation to enhance their real-world applicability. Our contributions advance FL algorithms and systems, bridging theoretical advancements and practical implementations. More broadly, our work serves as a guide for researchers navigating the complexities of translating theoretical methods into efficient real-world implementations and software. Additionally, it offers insights into the reverse process of adapting practical implementation aspects back into theoretical algorithm design. This reverse process is particularly intriguing, as the practical perspective compels us to examine the underlying mechanics and flexibilities of algorithms more deeply, often uncovering new dimensions of the algorithms under study.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EnergyNet Explained: Internetification of Energy Distribution</title>
<link>https://arxiv.org/abs/2509.08152</link>
<guid>https://arxiv.org/abs/2509.08152</guid>
<content:encoded><![CDATA[
arXiv:2509.08152v1 Announce Type: new 
Abstract: In developing EnergyNet we have leveraged and are extending lessons from telecom's shift from a centralized, circuit-switched phone system to decentralized, packet-switched data networks. EnergyNet utilizes 1) an Energy Router that enforces galvanic separation and utilizes software-controlled energy flows over a DC backplane, 2) Energy Local and Wide Area Networks (ELAN/EWAN) based on DC microgrids that interconnect through an open Energy Protocol (EP), and 3) a control plane comprised of the Energy Router Operating System (EROS) and EP Server which is managed at operator scale through an Energy Network Management System (ENMS). We distinguish the architectural contribution (Tier-1 including components, interfaces, and operating model) from expected outcomes contingent on adoption (Tier-2). The latter includes local-first autonomy with global interoperability, near-real-time operation with local buffering, removal of EV-charging bottlenecks, freed grid capacity for data centers and industrial electrification, as well as a trend toward low, predictable, fixed-cost clean energy. Evidence from early municipal demonstrators illustrates feasibility and migration paths. The contribution is a coherent, open, and testable blueprint for software-defined, decentralized energy distribution, aligning power-systems engineering with networking principles and offering a practical route from legacy, synchronous grids to resilient, digitally routed energy distribution systems.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategies for Improving Communication Efficiency in Distributed and Federated Learning: Compression, Local Training, and Personalization</title>
<link>https://arxiv.org/abs/2509.08233</link>
<guid>https://arxiv.org/abs/2509.08233</guid>
<content:encoded><![CDATA[
arXiv:2509.08233v1 Announce Type: new 
Abstract: Distributed and federated learning are essential paradigms for training models across decentralized data sources while preserving privacy, yet communication overhead remains a major bottleneck. This dissertation explores strategies to improve communication efficiency, focusing on model compression, local training, and personalization. We establish a unified framework for biased and unbiased compression operators with convergence guarantees, then propose adaptive local training strategies that incorporate personalization to accelerate convergence and mitigate client drift. In particular, Scafflix balances global and personalized objectives, achieving superior performance under both IID and non-IID settings. We further introduce privacy-preserving pruning frameworks that optimize sparsity while minimizing communication costs, with Cohort-Squeeze leveraging hierarchical aggregation to reduce cross-device overhead. Finally, SymWanda, a symmetric post-training pruning method, enhances robustness under high sparsity and maintains accuracy without retraining. Extensive experiments on benchmarks and large-scale language models demonstrate favorable trade-offs among accuracy, convergence, and communication, offering theoretical and practical insights for scalable, efficient distributed learning.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Formal Math on the Blockchain: An Explorer for Proofgold</title>
<link>https://arxiv.org/abs/2509.08267</link>
<guid>https://arxiv.org/abs/2509.08267</guid>
<content:encoded><![CDATA[
arXiv:2509.08267v1 Announce Type: new 
Abstract: Proofgold is a blockchain that supports formalized mathematics alongside standard cryptocurrency functionality. It incorporates logical constructs into the blockchain, including declarations of formal theories, definitions, propositions and proofs. It also supports placing and collecting bounties on proving these propositions, incentivizing the development of the formal libraries contained in Proofgold. In this paper, we present a web-based blockchain explorer for Proofgold. The system exposes not only the usual transactional data but also the formal mathematical components embedded in the chain and allows some interaction with them. The explorer allows users to inspect blocks, transactions, and addresses, as well as formal objects: theories, definitions, theorems and their proofs. We also support the submission of transactions to the blockchain using our interface. We describe the system architecture and its integration with the Proofgold Lava software, highlighting how the explorer supports navigation of formal content and facilitates mathematical knowledge management in a decentralized setting, as well as a number of formalizations in category theory done in the system.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Payment Channels with Proofs</title>
<link>https://arxiv.org/abs/2509.08268</link>
<guid>https://arxiv.org/abs/2509.08268</guid>
<content:encoded><![CDATA[
arXiv:2509.08268v1 Announce Type: new 
Abstract: The fundamental building blocks of the Bitcoin lightning network are bidirectional payment channels. We describe an extension of payment channels in the Proofgold network which allow the two parties to bet on whether a proposition will be proven by a certain time. These provide the foundation for a Proofgold lightning network that would allow parties to request proofs (by betting there will be no proof by a certain time) and other parties to provide proofs (and be rewarded by betting there will be a proof). The bets may also provide a way to approximate the probability that a certain proposition is provable (in the given amount of time). We describe the implementation of payment channels supporting proofs in Proofgold and discuss a potential lightning network that could be built as a result. One application of such lightning network would be a large decentralized infrastructure for fast collaborative formalization projects.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming DNSSEC Islands of Security: A TLS and IP-Based Certificate Solution</title>
<link>https://arxiv.org/abs/2509.08364</link>
<guid>https://arxiv.org/abs/2509.08364</guid>
<content:encoded><![CDATA[
arXiv:2509.08364v1 Announce Type: new 
Abstract: The Domain Name System (DNS) serves as the backbone of the Internet, primarily translating domain names to IP addresses. Over time, various enhancements have been introduced to strengthen the integrity of DNS. Among these, DNSSEC stands out as a leading cryptographic solution. It protects against attacks (such as DNS spoofing) by establishing a chain of trust throughout the DNS nameserver hierarchy. However, DNSSEC's effectiveness is compromised when there is a break in this chain, resulting in "Islands of Security", where domains can authenticate locally but not across hierarchical levels, leading to a loss of trust and validation between them. Leading approaches to addressing these issues were centralized, with a single authority maintaining some kind of bulletin board. This approach requires significantly more infrastructure and places excessive trust in the entity responsible for managing it properly. In this paper, we propose a decentralized approach to addressing gaps in DNSSEC's chain of trust, commonly referred to as "Islands of Security". We leverage TLS and IP-based certificates to enable end-to-end authentication between hierarchical levels, eliminating the need for uniform DNSSEC deployment across every level of the DNS hierarchy. This approach enhances the overall integrity of DNSSEC, while reducing dependence on registrars for maintaining signature records to verify the child nameserver's authenticity. By offering a more flexible and efficient solution, our method strengthens DNS security and streamlines deployment across diverse environments.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIoT-Driven Comparison of Open Blockchain Platforms</title>
<link>https://arxiv.org/abs/2509.08399</link>
<guid>https://arxiv.org/abs/2509.08399</guid>
<content:encoded><![CDATA[
arXiv:2509.08399v1 Announce Type: new 
Abstract: Being propelled by the fourth industrial revolution (Industry 4.0), IoT devices and solutions are well adopted everywhere, ranging from home applications to industrial use, crossing through transportation, healthcare, energy, and so on. This wide use of IoT has not gone unnoticed, hackers are tracking the weakness of such a technology and threatening them continuously. Their security at various levels has become an important concern of professionals and researchers. This issue takes more risk, especially with the IoT variants, IIoT (Industrial IoT) and MIoT (Medical IoT). Many existing security solutions are adapted and proposed for addressing IoT security. In this paper, we are interested in exploring blockchain technology and we make a comparison of three free Blockchain platforms towards their applicability for MIoT context, namely Ethereum, Hyperledger Fabric and Corda. In general, Blockchain technology provides a decentralized, autonomous, trustless, and distributed environment. It is challenging to find a Blockchain platform that fits the MIoT context and performs well in terms of security. The retained platform should be deployed smartly to avoid its practical drawbacks related to energy-consuming and excessive computing.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Blockchain and Proxy Re-Encryption to secure Medical IoT Records</title>
<link>https://arxiv.org/abs/2509.08402</link>
<guid>https://arxiv.org/abs/2509.08402</guid>
<content:encoded><![CDATA[
arXiv:2509.08402v1 Announce Type: new 
Abstract: The integration of the Internet of Things (IoT) in healthcare has revolutionized patient monitoring and data collection, allowing real-time tracking of vital signs, remote diagnostics, and automated medical responses. However, the transmission and storage of sensitive medical data introduce significant security and privacy challenges. To address these concerns, blockchain technology provides a decentralized and immutable ledger that ensures data integrity, , and transparency. Unlike public blockchains, private blockchains are permissioned; the access is granted only to authorized participants; they are more suitable for handling confidential healthcare data. Although blockchain ensures security and trust, it lacks built-in mechanisms to support flexible and controlled data sharing; This is where Proxy Re-Encryption (PRE) comes into play. PRE is a cryptographic technique that allows encrypted data to be re-encrypted for a new recipient without exposing it to intermediaries. We propose an architecture integrating private blockchain and PRE to enable secure, traceable, and privacy-preserving data sharing in IoT-based healthcare systems. Blockchain guarantees tamper proof record-keeping, while PRE enables fine-grained access control, allowing medical professionals to securely share patient data without compromising confidentiality. This combination creates a robust security framework that enhances trust and efficiency in digital healthcare ecosystems.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DSFL: A Dual-Server Byzantine-Resilient Federated Learning Framework via Group-Based Secure Aggregation</title>
<link>https://arxiv.org/abs/2509.08449</link>
<guid>https://arxiv.org/abs/2509.08449</guid>
<content:encoded><![CDATA[
arXiv:2509.08449v1 Announce Type: new 
Abstract: Federated Learning (FL) enables decentralized model training without sharing raw data, offering strong privacy guarantees. However, existing FL protocols struggle to defend against Byzantine participants, maintain model utility under non-independent and identically distributed (non-IID) data, and remain lightweight for edge devices. Prior work either assumes trusted hardware, uses expensive cryptographic tools, or fails to address privacy and robustness simultaneously. We propose DSFL, a Dual-Server Byzantine-Resilient Federated Learning framework that addresses these limitations using a group-based secure aggregation approach. Unlike LSFL, which assumes non-colluding semi-honest servers, DSFL removes this dependency by revealing a key vulnerability: privacy leakage through client-server collusion. DSFL introduces three key innovations: (1) a dual-server secure aggregation protocol that protects updates without encryption or key exchange, (2) a group-wise credit-based filtering mechanism to isolate Byzantine clients based on deviation scores, and (3) a dynamic reward-penalty system for enforcing fair participation. DSFL is evaluated on MNIST, CIFAR-10, and CIFAR-100 under up to 30 percent Byzantine participants in both IID and non-IID settings. It consistently outperforms existing baselines, including LSFL, homomorphic encryption methods, and differential privacy approaches. For example, DSFL achieves 97.15 percent accuracy on CIFAR-10 and 68.60 percent on CIFAR-100, while FedAvg drops to 9.39 percent under similar threats. DSFL remains lightweight, requiring only 55.9 ms runtime and 1088 KB communication per round.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Motion-Based User Identification across XR and Metaverse Applications by Deep Classification and Similarity Learning</title>
<link>https://arxiv.org/abs/2509.08539</link>
<guid>https://arxiv.org/abs/2509.08539</guid>
<content:encoded><![CDATA[
arXiv:2509.08539v1 Announce Type: new 
Abstract: This paper examines the generalization capacity of two state-of-the-art classification and similarity learning models in reliably identifying users based on their motions in various Extended Reality (XR) applications. We developed a novel dataset containing a wide range of motion data from 49 users in five different XR applications: four XR games with distinct tasks and action patterns, and an additional social XR application with no predefined task sets. The dataset is used to evaluate the performance and, in particular, the generalization capacity of the two models across applications. Our results indicate that while the models can accurately identify individuals within the same application, their ability to identify users across different XR applications remains limited. Overall, our results provide insight into current models generalization capabilities and suitability as biometric methods for user verification and identification. The results also serve as a much-needed risk assessment of hazardous and unwanted user identification in XR and Metaverse applications. Our cross-application XR motion dataset and code are made available to the public to encourage similar research on the generalization of motion-based user identification in typical Metaverse application use cases.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sharing is Caring: Efficient LM Post-Training with Collective RL Experience Sharing</title>
<link>https://arxiv.org/abs/2509.08721</link>
<guid>https://arxiv.org/abs/2509.08721</guid>
<content:encoded><![CDATA[
arXiv:2509.08721v1 Announce Type: new 
Abstract: Post-training language models (LMs) with reinforcement learning (RL) can enhance their complex reasoning capabilities without supervised fine-tuning, as demonstrated by DeepSeek-R1-Zero. However, effectively utilizing RL for LMs requires significant parallelization to scale-up inference, which introduces non-trivial technical challenges (e.g. latency, memory, and reliability) alongside ever-growing financial costs. We present Swarm sAmpling Policy Optimization (SAPO), a fully decentralized and asynchronous RL post-training algorithm. SAPO is designed for decentralized networks of heterogenous compute nodes, where each node manages its own policy model(s) while "sharing" rollouts with others in the network; no explicit assumptions about latency, model homogeneity, or hardware are required and nodes can operate in silo if desired. As a result, the algorithm avoids common bottlenecks in scaling RL post-training while also allowing (and even encouraging) new possibilities. By sampling rollouts "shared" across the network, it enables "Aha moments" to propagate, thereby bootstrapping the learning process. In this paper we show SAPO achieved cumulative reward gains of up to 94% in controlled experiments. We also share insights from tests on a network with thousands of nodes contributed by Gensyn community members running the algorithm on diverse hardware and models during an open-source demo.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SilentLedger: Privacy-Preserving Auditing for Blockchains with Complete Non-Interactivity</title>
<link>https://arxiv.org/abs/2509.08722</link>
<guid>https://arxiv.org/abs/2509.08722</guid>
<content:encoded><![CDATA[
arXiv:2509.08722v1 Announce Type: new 
Abstract: Privacy-preserving blockchain systems are essential for protecting transaction data, yet they must also provide auditability that enables auditors to recover participant identities and transaction amounts when warranted. Existing designs often compromise the independence of auditing and transactions, introducing extra interactions that undermine usability and scalability. Moreover, many auditable solutions depend on auditors serving as validators or recording nodes, which introduces risks to both data security and system reliability.
  To overcome these challenges, we propose SilentLedger, a privacy-preserving transaction system with auditing and complete non-interactivity. To support public verification of authorization, we introduce a renewable anonymous certificate scheme with formal semantics and a rigorous security model. SilentLedger further employs traceable transaction mechanisms constructed from established cryptographic primitives, enabling users to transact without interaction while allowing auditors to audit solely from on-chain data. We formally prove security properties including authenticity, anonymity, confidentiality, and soundness, provide a concrete instantiation, and evaluate performance under a standard 2-2 transaction model. Our implementation and benchmarks demonstrate that SilentLedger achieves superior performance compared with state-of-the-art solutions.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Stochastic Nonconvex Optimization under the Relaxed Smoothness</title>
<link>https://arxiv.org/abs/2509.08726</link>
<guid>https://arxiv.org/abs/2509.08726</guid>
<content:encoded><![CDATA[
arXiv:2509.08726v1 Announce Type: cross 
Abstract: This paper studies decentralized optimization problem $f(\mathbf{x})=\frac{1}{m}\sum_{i=1}^m f_i(\mathbf{x})$, where each local function has the form of $f_i(\mathbf{x}) = {\mathbb E}\left[F(\mathbf{x};{\xi}_i)\right]$ which is $(L_0,L_1)$-smooth but possibly nonconvex and the random variable ${\xi}_i$ follows distribution ${\mathcal D}_i$. We propose a novel algorithm called decentralized normalized stochastic gradient descent (DNSGD), which can achieve the $\epsilon$-stationary point on each local agent. We present a new framework for analyzing decentralized first-order methods in the relaxed smooth setting, based on the Lyapunov function related to the product of the gradient norm and the consensus error. The analysis shows upper bounds on sample complexity of ${\mathcal O}(m^{-1}(L_f\sigma^2\Delta_f\epsilon^{-4} + \sigma^2\epsilon^{-2} + L_f^{-2}L_1^3\sigma^2\Delta_f\epsilon^{-1} + L_f^{-2}L_1^2\sigma^2))$ per agent and communication complexity of $\tilde{\mathcal O}((L_f\epsilon^{-2} + L_1\epsilon^{-1})\gamma^{-1/2}\Delta_f)$, where $L_f=L_0 +L_1\zeta$, $\sigma^2$ is the variance of the stochastic gradient, $\Delta_f$ is the initial optimal function value gap, $\gamma$ is the spectral gap of the network, and $\zeta$ is the degree of the gradient dissimilarity. In the special case of $L_1=0$, the above results (nearly) match the lower bounds on decentralized nonconvex optimization in the standard smooth setting. We also conduct numerical experiments to show the empirical superiority of our method.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRESCO: Fast and Reliable Edge Offloading with Reputation-based Hybrid Smart Contracts</title>
<link>https://arxiv.org/abs/2410.06715</link>
<guid>https://arxiv.org/abs/2410.06715</guid>
<content:encoded><![CDATA[
arXiv:2410.06715v3 Announce Type: replace 
Abstract: Mobile devices offload latency-sensitive application tasks to edge servers to satisfy applications' Quality of Service (QoS) deadlines. Consequently, ensuring reliable offloading without QoS violations is challenging in distributed and unreliable edge environments. However, current edge offloading solutions are either centralized or do not adequately address challenges in distributed environments. We propose FRESCO, a fast and reliable edge offloading framework that utilizes a blockchain-based reputation system, which enhances the reliability of offloading in the distributed edge. The distributed reputation system tracks the historical performance of edge servers, while blockchain through a consensus mechanism ensures that sensitive reputation information is secured against tampering. However, blockchain consensus typically has high latency, and therefore we employ a Hybrid Smart Contract (HSC) that automatically computes and stores reputation securely on-chain (i.e., on the blockchain) while allowing fast offloading decisions off-chain (i.e., outside of blockchain). The offloading decision engine uses a reputation score to derive fast offloading decisions, which are based on Satisfiability Modulo Theory (SMT). The SMT models edge resource constraints, and QoS deadlines, and can formally guarantee a feasible solution that is valuable for latency-sensitive applications that require high reliability. With a combination of on-chain HSC reputation state management and an off-chain SMT decision engine, FRESCO offloads tasks to reliable servers without being hindered by blockchain consensus. We evaluate FRESCO against real availability traces and simulated applications. FRESCO reduces response time by up to 7.86 times and saves energy by up to 5.4% compared to all baselines while minimizing QoS violations to 0.4% and achieving an average decision time of 5.05 milliseconds.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FediLoRA: Heterogeneous LoRA for Federated Multimodal Fine-tuning under Missing Modalities</title>
<link>https://arxiv.org/abs/2509.06984</link>
<guid>https://arxiv.org/abs/2509.06984</guid>
<content:encoded><![CDATA[
arXiv:2509.06984v1 Announce Type: new 
Abstract: Foundation models have demonstrated remarkable performance across a wide range of tasks, yet their large parameter sizes pose challenges for practical deployment, especially in decentralized environments. Parameter-efficient fine-tuning (PEFT), such as Low-Rank Adaptation (LoRA), reduces local computing and memory overhead, making it attractive for federated learning. However, existing federated LoRA methods typically assume uniform rank configurations and unimodal inputs, overlooking two key real-world challenges: (1) heterogeneous client resources have different LoRA ranks, and (2) multimodal data settings with potentially missing modalities. In this work, we propose FediLoRA, a simple yet effective framework for federated multimodal fine-tuning under heterogeneous LoRA ranks and missing modalities. FediLoRA introduces a dimension-wise aggregation strategy that reweights LoRA updates without information dilution during aggregation. It also includes a lightweight layer-wise model editing method that selectively incorporates global parameters to repair local components which improves both client and global model performances. Experimental results on three multimodal benchmark datasets demonstrate that FediLoRA achieves superior performance over competitive baselines in both global and personalized settings, particularly in the presence of modality incompleteness.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoK: Security and Privacy of AI Agents for Blockchain</title>
<link>https://arxiv.org/abs/2509.07131</link>
<guid>https://arxiv.org/abs/2509.07131</guid>
<content:encoded><![CDATA[
arXiv:2509.07131v1 Announce Type: new 
Abstract: Blockchain and smart contracts have garnered significant interest in recent years as the foundation of a decentralized, trustless digital ecosystem, thereby eliminating the need for traditional centralized authorities. Despite their central role in powering Web3, their complexity still presents significant barriers for non-expert users. To bridge this gap, Artificial Intelligence (AI)-based agents have emerged as valuable tools for interacting with blockchain environments, supporting a range of tasks, from analyzing on-chain data and optimizing transaction strategies to detecting vulnerabilities within smart contracts. While interest in applying AI to blockchain is growing, the literature still lacks a comprehensive survey that focuses specifically on the intersection with AI agents. Most of the related work only provides general considerations, without focusing on any specific domain. This paper addresses this gap by presenting the first Systematization of Knowledge dedicated to AI-driven systems for blockchain, with a special focus on their security and privacy dimensions, shedding light on their applications, limitations, and future research directions.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Evolutionary Framework for Safe, Efficient, and Cooperative Autonomous Vehicle Interactions</title>
<link>https://arxiv.org/abs/2509.07411</link>
<guid>https://arxiv.org/abs/2509.07411</guid>
<content:encoded><![CDATA[
arXiv:2509.07411v1 Announce Type: new 
Abstract: Modern transportation systems face significant challenges in ensuring road safety, given serious injuries caused by road accidents. The rapid growth of autonomous vehicles (AVs) has prompted new traffic designs that aim to optimize interactions among AVs. However, effective interactions between AVs remains challenging due to the absence of centralized control. Besides, there is a need for balancing multiple factors, including passenger demands and overall traffic efficiency. Traditional rule-based, optimization-based, and game-theoretic approaches each have limitations in addressing these challenges. Rule-based methods struggle with adaptability and generalization in complex scenarios, while optimization-based methods often require high computational resources. Game-theoretic approaches, such as Stackelberg and Nash games, suffer from limited adaptability and potential inefficiencies in cooperative settings. This paper proposes an Evolutionary Game Theory (EGT)-based framework for AV interactions that overcomes these limitations by utilizing a decentralized and adaptive strategy evolution mechanism. A causal evaluation module (CEGT) is introduced to optimize the evolutionary rate, balancing mutation and evolution by learning from historical interactions. Simulation results demonstrate the proposed CEGT outperforms EGT and popular benchmark games in terms of lower collision rates, improved safety distances, higher speeds, and overall better performance compared to Nash and Stackelberg games across diverse scenarios and parameter settings.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dependency-Aware Execution Mechanism in Hyperledger Fabric Architecture</title>
<link>https://arxiv.org/abs/2509.07425</link>
<guid>https://arxiv.org/abs/2509.07425</guid>
<content:encoded><![CDATA[
arXiv:2509.07425v1 Announce Type: new 
Abstract: Hyperledger Fabric is a leading permissioned blockchain framework for enterprise use, known for its modular design and privacy features. While it strongly supports configurable consensus and access control, Fabric can face challenges in achieving high transaction throughput and low rejection rates under heavy workloads. These performance limitations are often attributed to endorsement, ordering, and validation bottlenecks. Further, optimistic concurrency control and deferred validation in Fabric may lead to resource inefficiencies and contention, as conflicting transactions are identified only during the commit phase. To address these challenges, we propose a dependency-aware execution model for Hyperledger Fabric. Our approach includes: (a) a dependency flagging system during endorsement, marking transactions as independent or dependent using a hashmap; (b) an optimized block construction in the ordering service that prioritizes independent transactions; (c) the incorporation of a Directed Acyclic Graph (DAG) within each block to represent dependencies; and (d) parallel execution of independent transactions at the committer, with dependent transactions processed according to DAG order. Incorporated in Hyperledger Fabric v2.5, our framework was tested on workloads with varying dependency levels and system loads. Results show up to 40% higher throughput and significantly reduced rejection rates in high-contention scenarios. This demonstrates that dependency-aware scheduling and DAG-based execution can substantially enhance Fabric's scalability while remaining compatible with its existing consensus and smart contract layers.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DREAMS: Decentralized Resource Allocation and Service Management across the Compute Continuum Using Service Affinity</title>
<link>https://arxiv.org/abs/2509.07497</link>
<guid>https://arxiv.org/abs/2509.07497</guid>
<content:encoded><![CDATA[
arXiv:2509.07497v1 Announce Type: new 
Abstract: Modern manufacturing systems require adaptive computing infrastructures that can respond to highly dynamic workloads and increasingly customized production demands. The compute continuum emerges as a promising solution, enabling flexible deployment of microservices across distributed, heterogeneous domains. However, this paradigm also requires a novel approach to resource allocation and service placement, as traditional centralized solutions struggle to scale effectively, suffer from latency bottlenecks, and introduce single points of failure. In this paper, we present DREAMS, a decentralized framework that optimizes microservice placement decisions collaboratively across different computational domains. At its core, DREAMS introduces agents that operate autonomously within each domain while coordinating globally through a Raft-based consensus algorithm and cost-benefit voting. This decentralized architecture enables responsive, privacy-preserving, and fault-tolerant coordination, making it particularly suitable given the growing prevalence of multi-stakeholder scenarios across the compute continuum. In particular, within modern manufacturing environments, DREAMS achieves globally optimized service placements while maintaining high fault tolerance. Further evaluations demonstrate that key coordination operations, such as Local Domain Manager (LDM) registration and migration voting, scale sub-linearly with the number of domains, confirming the efficiency and scalability of our proposal.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bringing Multi-Modal Multi-Task Federated Foundation Models to Education Domain: Prospects and Challenges</title>
<link>https://arxiv.org/abs/2509.07946</link>
<guid>https://arxiv.org/abs/2509.07946</guid>
<content:encoded><![CDATA[
arXiv:2509.07946v1 Announce Type: new 
Abstract: Multi-modal multi-task (M3T) foundation models (FMs) have recently shown transformative potential in artificial intelligence, with emerging applications in education. However, their deployment in real-world educational settings is hindered by privacy regulations, data silos, and limited domain-specific data availability. We introduce M3T Federated Foundation Models (FedFMs) for education: a paradigm that integrates federated learning (FL) with M3T FMs to enable collaborative, privacy-preserving training across decentralized institutions while accommodating diverse modalities and tasks. Subsequently, this position paper aims to unveil M3T FedFMs as a promising yet underexplored approach to the education community, explore its potentials, and reveal its related future research directions. We outline how M3T FedFMs can advance three critical pillars of next-generation intelligent education systems: (i) privacy preservation, by keeping sensitive multi-modal student and institutional data local; (ii) personalization, through modular architectures enabling tailored models for students, instructors, and institutions; and (iii) equity and inclusivity, by facilitating participation from underrepresented and resource-constrained entities. We finally identify various open research challenges, including studying of (i) inter-institution heterogeneous privacy regulations, (ii) the non-uniformity of data modalities' characteristics, (iii) the unlearning approaches for M3T FedFMs, (iv) the continual learning frameworks for M3T FedFMs, and (v) M3T FedFM model interpretability, which must be collectively addressed for practical deployment.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asynchronous Gossip Algorithms for Rank-Based Statistical Methods</title>
<link>https://arxiv.org/abs/2509.07543</link>
<guid>https://arxiv.org/abs/2509.07543</guid>
<content:encoded><![CDATA[
arXiv:2509.07543v1 Announce Type: cross 
Abstract: As decentralized AI and edge intelligence become increasingly prevalent, ensuring robustness and trustworthiness in such distributed settings has become a critical issue-especially in the presence of corrupted or adversarial data. Traditional decentralized algorithms are vulnerable to data contamination as they typically rely on simple statistics (e.g., means or sum), motivating the need for more robust statistics. In line with recent work on decentralized estimation of trimmed means and ranks, we develop gossip algorithms for computing a broad class of rank-based statistics, including L-statistics and rank statistics-both known for their robustness to outliers. We apply our method to perform robust distributed two-sample hypothesis testing, introducing the first gossip algorithm for Wilcoxon rank-sum tests. We provide rigorous convergence guarantees, including the first convergence rate bound for asynchronous gossip-based rank estimation. We empirically validate our theoretical results through experiments on diverse network topologies.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Online Riemannian Optimization Beyond Hadamard Manifolds</title>
<link>https://arxiv.org/abs/2509.07779</link>
<guid>https://arxiv.org/abs/2509.07779</guid>
<content:encoded><![CDATA[
arXiv:2509.07779v1 Announce Type: cross 
Abstract: We study decentralized online Riemannian optimization over manifolds with possibly positive curvature, going beyond the Hadamard manifold setting. Decentralized optimization techniques rely on a consensus step that is well understood in Euclidean spaces because of their linearity. However, in positively curved Riemannian spaces, a main technical challenge is that geodesic distances may not induce a globally convex structure. In this work, we first analyze a curvature-aware Riemannian consensus step that enables a linear convergence beyond Hadamard manifolds. Building on this step, we establish a $O(\sqrt{T})$ regret bound for the decentralized online Riemannian gradient descent algorithm. Then, we investigate the two-point bandit feedback setup, where we employ computationally efficient gradient estimators using smoothing techniques, and we demonstrate the same $O(\sqrt{T})$ regret bound through the subconvexity analysis of smoothed objectives.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Based Calculation Method of Mining Fairness in Blockchain</title>
<link>https://arxiv.org/abs/2406.00595</link>
<guid>https://arxiv.org/abs/2406.00595</guid>
<content:encoded><![CDATA[
arXiv:2406.00595v3 Announce Type: replace 
Abstract: Mining fairness in blockchain refers to equality between the computational resources invested in mining and the block rewards received. There exists a dilemma wherein increasing the transaction processing capacity of a blockchain compromises mining fairness, thereby undermining its decentralization. This dilemma remains unresolved despite methods such as the greedy heaviest observed subtree (GHOST) protocol, indicating that mining fairness is an inherent bottleneck in the transaction processing capacity of the blockchain system. However, despite its significance, existing analyses neglect the impact of blockchain forks, resulting in imprecise evaluations and limited insights. To address this issue, we propose a method for calculating mining fairness that explicitly captures the influence of forks. First, we approximate a complex blockchain network using a simple mathematical model, assuming that no more than two blocks are generated per round. Within this model, we quantitatively determine local mining fairness and derive several measures of global mining fairness based on local mining fairness. Subsequently, we validated by blockchain network simulations that our calculation method computes mining fairness in networks much more accurately than existing methods. The proposed method facilitates a rigorous evaluation of trade-offs between scalability and decentralization by offering a clear, quantitative framework for measuring and comparing reward distribution among miners. Consequently, it is expected to provide valuable insights for future mining fairness research and the design of next-generation blockchain systems.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AGORA: Open More and Trust Less in Binary Verification Service</title>
<link>https://arxiv.org/abs/2407.15062</link>
<guid>https://arxiv.org/abs/2407.15062</guid>
<content:encoded><![CDATA[
arXiv:2407.15062v2 Announce Type: replace 
Abstract: Binary verification plays a pivotal role in software security, yet building a verification service that is both open and trustworthy poses a formidable challenge. In this paper, we introduce a novel binary verification service, AGORA, scrupulously designed to overcome the challenge. At the heart of this approach lies a strategic insight: certain tasks can be delegated to untrusted entities, while the corresponding validators are securely housed within the trusted computing base (TCB). AGORA can validate untrusted assertions generated for versatile policies. Through a novel blockchain-based bounty task manager, it also utilizes crowdsourcing to remove trust in theorem provers. These synergistic techniques successfully ameliorate the TCB size burden associated with two procedures: binary analysis and theorem proving. The design of AGORA allows untrusted parties to participate in these complex processes. Moreover, based on running the optimized TCB within trusted execution environments and recording the verification process on a blockchain, the public can audit the correctness of verification results. By implementing verification workflows for software-based fault isolation policy and side-channel mitigation, our evaluation demonstrates the efficacy of AGORA.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Knowledge Proofs in Sublinear Space</title>
<link>https://arxiv.org/abs/2509.05326</link>
<guid>https://arxiv.org/abs/2509.05326</guid>
<content:encoded><![CDATA[
arXiv:2509.05326v1 Announce Type: new 
Abstract: Modern zero-knowledge proof (ZKP) systems, essential for privacy and verifiable computation, suffer from a fundamental limitation: the prover typically uses memory that scales linearly with the computation's trace length T, making them impractical for resource-constrained devices and prohibitively expensive for large-scale tasks. This paper overcomes this barrier by constructing, to our knowledge, the first sublinear-space ZKP prover. Our core contribution is an equivalence that reframes proof generation as an instance of the classic Tree Evaluation problem. Leveraging a recent space-efficient tree-evaluation algorithm, we design a streaming prover that assembles the proof without ever materializing the full execution trace. The approach reduces prover memory from linear in T to O(sqrt(T)) (up to O(log T) lower-order terms) while preserving proof size, verifier time, and the transcript/security guarantees of the underlying system. This enables a shift from specialized, server-bound proving to on-device proving, opening applications in decentralized systems, on-device machine learning, and privacy-preserving technologies.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plantbot: Integrating Plant and Robot through LLM Modular Agent Networks</title>
<link>https://arxiv.org/abs/2509.05338</link>
<guid>https://arxiv.org/abs/2509.05338</guid>
<content:encoded><![CDATA[
arXiv:2509.05338v1 Announce Type: new 
Abstract: We introduce Plantbot, a hybrid lifeform that connects a living plant with a mobile robot through a network of large language model (LLM) modules. Each module - responsible for sensing, vision, dialogue, or action - operates asynchronously and communicates via natural language, enabling seamless interaction across biological and artificial domains. This architecture leverages the capacity of LLMs to serve as hybrid interfaces, where natural language functions as a universal protocol, translating multimodal data (soil moisture, temperature, visual context) into linguistic messages that coordinate system behaviors. The integrated network transforms plant states into robotic actions, installing normativity essential for agency within the sensor-motor loop. By combining biological and robotic elements through LLM-mediated communication, Plantbot behaves as an embodied, adaptive agent capable of responding autonomously to environmental conditions. This approach suggests possibilities for a new model of artificial life, where decentralized, LLM modules coordination enable novel interactions between biological and artificial systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEASONED: Semantic-Enhanced Self-Counterfactual Explainable Detection of Adversarial Exploiter Contracts</title>
<link>https://arxiv.org/abs/2509.05681</link>
<guid>https://arxiv.org/abs/2509.05681</guid>
<content:encoded><![CDATA[
arXiv:2509.05681v1 Announce Type: new 
Abstract: Decentralized Finance (DeFi) attacks have resulted in significant losses, often orchestrated through Adversarial Exploiter Contracts (AECs) that exploit vulnerabilities in victim smart contracts. To proactively identify such threats, this paper targets the explainable detection of AECs.
  Existing detection methods struggle to capture semantic dependencies and lack interpretability, limiting their effectiveness and leaving critical knowledge gaps in AEC analysis. To address these challenges, we introduce SEASONED, an effective, self-explanatory, and robust framework for AEC detection.
  SEASONED extracts semantic information from contract bytecode to construct a semantic relation graph (SRG), and employs a self-counterfactual explainable detector (SCFED) to classify SRGs and generate explanations that highlight the core attack logic. SCFED further enhances robustness, generalizability, and data efficiency by extracting representative information from these explanations. Both theoretical analysis and experimental results demonstrate the effectiveness of SEASONED, which showcases outstanding detection performance, robustness, generalizability, and data efficiency learning ability. To support further research, we also release a new dataset of 359 AECs.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Larger-scale Nakamoto-style Blockchains Offer Better Security</title>
<link>https://arxiv.org/abs/2509.05708</link>
<guid>https://arxiv.org/abs/2509.05708</guid>
<content:encoded><![CDATA[
arXiv:2509.05708v1 Announce Type: new 
Abstract: Traditional security models for Nakamoto-style blockchains overestimate adversarial coordination by assuming instantaneous synchronization among malicious nodes, neglecting the critical impact of internal communication delays on security. This paper introduces a dual-delay framework to revisit security analysis, addressing this oversight through two key innovations. First, the static delay model quantifies how adversarial communication delays (\(\Delta_a\)) constrain the effective growth rate of private chains, derived via an M/D/1 queuing model as \(\lambda_{eff} = \lambda_a / (1 + \lambda_a \Delta_a)\). This model reveals that the security threshold (\(\beta^*\)), the maximum adversarial power the system tolerates, increases with \(\Delta_a\), even exceeding the classic 51\% boundary when \(\Delta_a \textgreater \Delta\) (honest nodes' delay), breaking the long-standing 50\% assumption. Second, the dynamic delay model integrates probabilistic corruption and scale-dependent delays to characterize the total adversarial delay window (\(\Delta_{total} = \Delta(n) e^{-k\beta} + c \log(1 + \beta n)\)), where \(\Delta(n) \in \Theta(\log n)\) captures honest nodes' logarithmic delay growth. Asymptotic analysis shows adversarial power decays linearly with network scale, ensuring the probability of \(\beta \leq \beta^*\) approaches 1 as \(n \to \infty\). By exposing the interplay between network scale, communication delays, and power dilution, we provide a theoretical foundation for optimizing consensus protocols and assessing robustness in large-scale Nakamoto-style blockchains.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Robust Aggregation in Decentralized Gradient Marketplaces</title>
<link>https://arxiv.org/abs/2509.05833</link>
<guid>https://arxiv.org/abs/2509.05833</guid>
<content:encoded><![CDATA[
arXiv:2509.05833v1 Announce Type: new 
Abstract: The rise of distributed and privacy-preserving machine learning has sparked interest in decentralized gradient marketplaces, where participants trade intermediate artifacts like gradients. However, existing Federated Learning (FL) benchmarks overlook critical economic and systemic factors unique to such marketplaces-cost-effectiveness, fairness to sellers, and market stability-especially when a buyer relies on a private baseline dataset for evaluation.
  We introduce a comprehensive benchmark framework to holistically evaluate robust gradient aggregation methods within these buyer-baseline-reliant marketplaces. Our contributions include: (1) a simulation environment modeling marketplace dynamics with a variable buyer baseline and diverse seller distributions; (2) an evaluation methodology augmenting standard FL metrics with marketplace-centric dimensions such as Economic Efficiency, Fairness, and Selection Dynamics; (3) an in-depth empirical analysis of the existing Distributed Gradient Marketplace framework, MartFL, including the integration and comparative evaluation of adapted FLTrust and SkyMask as alternative aggregation strategies within it. This benchmark spans diverse datasets, local attacks, and Sybil attacks targeting the marketplace selection process; and (4) actionable insights into the trade-offs between model performance, robustness, cost, fairness, and stability.
  This benchmark equips the community with essential tools and empirical evidence to evaluate and design more robust, equitable, and economically viable decentralized gradient marketplaces.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dataset Ownership in the Era of Large Language Models</title>
<link>https://arxiv.org/abs/2509.05921</link>
<guid>https://arxiv.org/abs/2509.05921</guid>
<content:encoded><![CDATA[
arXiv:2509.05921v1 Announce Type: new 
Abstract: As datasets become critical assets in modern machine learning systems, ensuring robust copyright protection has emerged as an urgent challenge. Traditional legal mechanisms often fail to address the technical complexities of digital data replication and unauthorized use, particularly in opaque or decentralized environments. This survey provides a comprehensive review of technical approaches for dataset copyright protection, systematically categorizing them into three main classes: non-intrusive methods, which detect unauthorized use without modifying data; minimally-intrusive methods, which embed lightweight, reversible changes to enable ownership verification; and maximally-intrusive methods, which apply aggressive data alterations, such as reversible adversarial examples, to enforce usage restrictions. We synthesize key techniques, analyze their strengths and limitations, and highlight open research challenges. This work offers an organized perspective on the current landscape and suggests future directions for developing unified, scalable, and ethically sound solutions to protect datasets in increasingly complex machine learning ecosystems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VehiclePassport: A GAIA-X-Aligned, Blockchain-Anchored Privacy-Preserving, Zero-Knowledge Digital Passport for Smart Vehicles</title>
<link>https://arxiv.org/abs/2509.06133</link>
<guid>https://arxiv.org/abs/2509.06133</guid>
<content:encoded><![CDATA[
arXiv:2509.06133v1 Announce Type: new 
Abstract: Modern vehicles accumulate fragmented lifecycle records across OEMs, owners, and service centers that are difficult to verify and prone to fraud. We propose VehiclePassport, a GAIA-X-aligned digital passport anchored on blockchain with zero-knowledge proofs (ZKPs) for privacy-preserving verification. VehiclePassport immutably commits to manufacturing, telemetry, and service events while enabling selective disclosure via short-lived JWTs and Groth16 proofs. Our open-source reference stack anchors hashes on Polygon zkEVM at <$0.02 per event, validates proofs in <10 ms, and scales to millions of vehicles. This architecture eliminates paper-based KYC, ensures GDPR-compliant traceability, and establishes a trustless foundation for insurance, resale, and regulatory applications in global mobility data markets.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Secure Sequencer and Data Availability Committee for Rollups (Extended Version)</title>
<link>https://arxiv.org/abs/2509.06614</link>
<guid>https://arxiv.org/abs/2509.06614</guid>
<content:encoded><![CDATA[
arXiv:2509.06614v1 Announce Type: new 
Abstract: Blockchains face a scalability limitation, partly due to the throughput limitations of consensus protocols, especially when aiming to obtain a high degree of decentralization. Layer 2 Rollups (L2s) are a faster alternative to conventional blockchains. L2s perform most computations offchain using minimally blockchains (L1) under-the-hood to guarantee correctness. A sequencer is a service that receives offchain L2 transaction requests, batches these transactions, and commits compressed or hashed batches to L1. Using hashing needs less L1 space, which is beneficial for gas cost, but requires a data availability committee (DAC) service to translate hashes into their corresponding batches of transaction requests. The behavior of sequencers and DACs influence the evolution of the L2 blockchain, presenting a potential security threat and delaying L2 adoption. We propose in this paper fraud-proof mechanisms, arbitrated by L1 contracts, to detect and generate evidence of dishonest behavior of the sequencer and DAC. We study how these fraud-proofs limit the power of adversaries that control different number of sequencer and DACs members, and provide incentives for their honest behavior. We designed these fraud-proof mechanisms as two player games. Unlike the generic fraud-proofs in current L2s (designed to guarantee the correct execution of transactions), our fraud-proofs are over pred-etermined algorithms that verify the properties that determine the correctness of the DAC. Arbitrating over concrete algorithms makes our fraud-proofs more efficient, easier to understand, and simpler to prove correct. We provide as an artifact a mechanization in LEAN4 of our fraud-proof games, including (1) the verified strategies that honest players should play to win all games as well as (2) mechanisms to detect dishonest claims.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mangrove: Fast and Parallelizable State Replication for Blockchains</title>
<link>https://arxiv.org/abs/2509.06616</link>
<guid>https://arxiv.org/abs/2509.06616</guid>
<content:encoded><![CDATA[
arXiv:2509.06616v1 Announce Type: new 
Abstract: Mangrove is a novel scaling approach to building blockchains with parallel smart contract support. Unlike in monolithic blockchains, where a single consensus mechanism determines a strict total order over all transactions, Mangrove uses separate consensus instances per smart contract, without a global order. To allow multiple instances to run in parallel while ensuring that no conflicting transactions are committed, we propose a mechanism called Parallel Optimistic Agreement. Additionally, for simple transactions, we leverage a lightweight Byzantine Reliable Broadcast primitive to reduce latency. Mangrove is optimized for performance under optimistic conditions, where there is no misbehavior and the network is synchronous. Under these conditions, our protocol can achieve a latency of 2 communication steps between creating and executing a transaction.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network-level Censorship Attacks in the InterPlanetary File System</title>
<link>https://arxiv.org/abs/2509.06626</link>
<guid>https://arxiv.org/abs/2509.06626</guid>
<content:encoded><![CDATA[
arXiv:2509.06626v1 Announce Type: new 
Abstract: The InterPlanetary File System (IPFS) has been successfully established as the de facto standard for decentralized data storage in the emerging Web3. Despite its decentralized nature, IPFS nodes, as well as IPFS content providers, have converged to centralization in large public clouds. Centralization introduces BGP routing-based attacks, such as passive interception and BGP hijacking, as potential threats. Although this attack vector has been investigated for many other Web3 protocols, such as Bitcoin and Ethereum, to the best of our knowledge, it has not been analyzed for the IPFS network. In our work, we bridge this gap and demonstrate that BGP routing attacks can be effectively leveraged to censor content in IPFS. For the analysis, we collected 3,000 content blocks called CIDs and conducted a simulation of BGP hijacking and passive interception against them. We find that a single malicious AS can censor 75% of the IPFS content for more than 57% of all requester nodes. Furthermore, we show that even with a small set of only 62 hijacked prefixes, 70% of the full attack effectiveness can already be reached. We further propose and validate countermeasures based on global collaborative content replication among all nodes in the IPFS network, together with additional robust backup content provider nodes that are well-hardened against BGP hijacking. We hope this work raises awareness about the threat BGP routing-based attacks pose to IPFS and triggers further efforts to harden the live IPFS network against them.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hypergraph Approach to Distributed Broadcast</title>
<link>https://arxiv.org/abs/2404.16376</link>
<guid>https://arxiv.org/abs/2404.16376</guid>
<content:encoded><![CDATA[
arXiv:2404.16376v3 Announce Type: replace 
Abstract: This paper explores the distributed broadcast problem within the context of network communications, a critical challenge in decentralized information dissemination. We put forth a novel hypergraph-based approach to address this issue, focusing on minimizing the number of broadcasts to ensure comprehensive data sharing among all network users. The key contributions of this work include the establishment of a general lower bound for the problem using the min-cut capacity of hypergraphs, and a distributed broadcast for quasi-trees (DBQT) algorithm tailored for the unique structure of quasi-trees, which is proven to be optimal. This paper advances both network communication strategies and hypergraph theory, with implications for a wide range of real-world applications, from vehicular and sensor networks to distributed storage systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Architecture Built for Federated Learning: Addressing Data Heterogeneity through Adaptive Normalization-Free Feature Recalibration</title>
<link>https://arxiv.org/abs/2410.02006</link>
<guid>https://arxiv.org/abs/2410.02006</guid>
<content:encoded><![CDATA[
arXiv:2410.02006v2 Announce Type: replace 
Abstract: Federated learning is a decentralized collaborative training paradigm preserving stakeholders' data ownership while improving performance and generalization. However, statistical heterogeneity among client datasets degrades system performance. To address this issue, we propose Adaptive Normalization-free Feature Recalibration (ANFR), a model architecture-level approach that combines weight standardization and channel attention to combat heterogeneous data in FL. ANFR leverages weight standardization to avoid mismatched client statistics and inconsistent averaging, ensuring robustness under heterogeneity, and channel attention to produce learnable scaling factors for feature maps, suppressing inconsistencies across clients due to heterogeneity. We demonstrate that combining these techniques boosts model performance beyond their individual contributions, by improving class selectivity and channel attention weight distribution. ANFR works with any aggregation method, supports both global and personalized FL, and adds minimal overhead. Furthermore, when training with differential privacy, ANFR achieves an appealing balance between privacy and utility, enabling strong privacy guarantees without sacrificing performance. By integrating weight standardization and channel attention in the backbone model, ANFR offers a novel and versatile approach to the challenge of statistical heterogeneity. Extensive experiments show ANFR consistently outperforms established baselines across various aggregation methods, datasets, and heterogeneity conditions. Code is provided at https://github.com/siomvas/ANFR.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XChainWatcher: Monitoring and Identifying Attacks in Cross-Chain Bridges</title>
<link>https://arxiv.org/abs/2410.02029</link>
<guid>https://arxiv.org/abs/2410.02029</guid>
<content:encoded><![CDATA[
arXiv:2410.02029v3 Announce Type: replace 
Abstract: Cross-chain bridges are a type of middleware for blockchain interoperability that supports the transfer of assets and data across blockchains. However, several of these bridges have vulnerabilities that have caused 3.2 billion dollars in losses since May 2021. Some studies have revealed the existence of these vulnerabilities, but there is little quantitative research available, and there are no safeguard mechanisms to protect bridges from such attacks. Furthermore, no studies are available on the practices of cross-chain bridges that can cause financial losses. We propose \toolName~(Cross-Chain Watcher), a modular and extensible logic-driven anomaly detector for cross-chain bridges. It operates in three main phases: (1) decoding events and transactions from multiple blockchains, (2) building logic relations from the extracted data, and (3) evaluating these relations against a set of detection rules. Using \toolName, we analyze data from two previously attacked bridges: the Ronin and Nomad bridges. \toolName~was able to successfully identify the transactions that led to losses of \$611M and \$190M (USD) and surpassed the results obtained by a reputable security firm in the latter. We not only uncover successful attacks, but also reveal other anomalies, such as 37 cross-chain transactions (\CCTX) that these bridges should not have accepted, failed attempts to exploit Nomad, over \$7.8M worth of tokens locked on one chain but never released on Ethereum, and \$200K lost by users due to inadequate interaction with bridges. We provide the first open dataset of 81,000 \CCTXS~across three blockchains, capturing more than \$4.2B in token transfers.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Six Million (Suspected) Fake Stars in GitHub: A Growing Spiral of Popularity Contests, Spams, and Malware</title>
<link>https://arxiv.org/abs/2412.13459</link>
<guid>https://arxiv.org/abs/2412.13459</guid>
<content:encoded><![CDATA[
arXiv:2412.13459v2 Announce Type: replace 
Abstract: GitHub, the de facto platform for open-source software development, provides a set of social-media-like features to signal high-quality repositories. Among them, the star count is the most widely used popularity signal, but it is also at risk of being artificially inflated (i.e., faked), decreasing its value as a decision-making signal and posing a security risk to all GitHub users. In this paper, we present a systematic, global, and longitudinal measurement study of fake stars in GitHub. To this end, we build StarScout, a scalable tool able to detect anomalous starring behaviors across all GitHub metadata between 2019 and 2024. Analyzing the data collected using StarScout, we find that: (1) fake-star-related activities have rapidly surged in 2024; (2) the accounts and repositories in fake star campaigns have highly trivial activity patterns; (3) the majority of fake stars are used to promote short-lived phishing malware repositories; the remaining ones are mostly used to promote AI/LLM, blockchain, tool/application, and tutorial/demo repositories; (4) while repositories may have acquired fake stars for growth hacking, fake stars only have a promotion effect in the short term (i.e., less than two months) and become a liability in the long term. Our study has implications for platform moderators, open-source practitioners, and supply chain security researchers.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Insecurity Through Obscurity: Veiled Vulnerabilities in Closed-Source Contracts</title>
<link>https://arxiv.org/abs/2504.13398</link>
<guid>https://arxiv.org/abs/2504.13398</guid>
<content:encoded><![CDATA[
arXiv:2504.13398v3 Announce Type: replace 
Abstract: Most blockchains cannot hide the binary code of programs (i.e., smart contracts) running on them. To conceal proprietary business logic and to potentially deter attacks, many smart contracts are closed-source and employ layers of obfuscation. However, we demonstrate that such obfuscation can obscure critical vulnerabilities rather than enhance security, a phenomenon we term insecurity through obscurity. To systematically analyze these risks on a large scale, we present SKANF, a novel EVM bytecode analysis tool tailored for closed-source and obfuscated contracts. SKANF combines control-flow deobfuscation, symbolic execution, and concolic execution based on historical transactions to identify and exploit asset management vulnerabilities. Our evaluation on real-world Maximal Extractable Value (MEV) bots reveals that SKANF detects vulnerabilities in 1,030 contracts and successfully generates exploits for 394 of them, with potential losses of \$10.6M. Additionally, we uncover 104 real-world MEV bot attacks that collectively resulted in \$2.76M in losses.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Latent Fusion of ECG Leads for Early Assessment of Pulmonary Hypertension</title>
<link>https://arxiv.org/abs/2503.13470</link>
<guid>https://arxiv.org/abs/2503.13470</guid>
<content:encoded><![CDATA[
arXiv:2503.13470v2 Announce Type: replace-cross 
Abstract: Recent advancements in early assessment of pulmonary hypertension (PH) primarily focus on applying machine learning methods to centralized diagnostic modalities, such as 12-lead electrocardiogram (12L-ECG). Despite their potential, these approaches fall short in decentralized clinical settings, e.g., point-of-care and general practice, where handheld 6-lead ECG (6L-ECG) can offer an alternative but is limited by the scarcity of labeled data for developing reliable models. To address this, we propose a lead-specific electrocardiogram multimodal variational autoencoder (\textsc{LS-EMVAE}), which incorporates a hierarchical modality expert (HiME) fusion mechanism and a latent representation alignment loss. HiME combines mixture-of-experts and product-of-experts to enable flexible, adaptive latent fusion, while the alignment loss improves coherence among lead-specific and shared representations. To alleviate data scarcity and enhance representation learning, we adopt a transfer learning strategy: the model is first pre-trained on a large unlabeled 12L-ECG dataset and then fine-tuned on smaller task-specific labeled 6L-ECG datasets. We validate \textsc{LS-EMVAE} across two retrospective cohorts in a 6L-ECG setting: 892 subjects from the ASPIRE registry for (1) PH detection and (2) phenotyping pre-/post-capillary PH, and 16,416 subjects from UK Biobank for (3) predicting elevated pulmonary atrial wedge pressure, where it consistently outperforms unimodal and multimodal baseline methods and demonstrates strong generalizability and interpretability. The code is available at https://github.com/Shef-AIRE/LS-EMVAE.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network-Aware Differential Privacy</title>
<link>https://arxiv.org/abs/2509.04710</link>
<guid>https://arxiv.org/abs/2509.04710</guid>
<content:encoded><![CDATA[
arXiv:2509.04710v1 Announce Type: new 
Abstract: Differential privacy (DP) is a privacy-enhancement technology (PET) that receives prominent attention from the academia, industry, and government. One main development over the past decade has been the decentralization of DP, including local DP and shuffle DP. Despite that decentralized DP heavily relies on network communications for data collection,we found that: 1) no systematic study has surveyed the research opportunities at the intersection of networking and DP; 2) nor have there been significant efforts to develop DP mechanisms that are explicitly tailored for network environments. In this paper, we seek to address this gap by initiating a new direction of network-aware DP. We identified two focus areas where the network research can offer substantive contributions to the design and deployment of DP, related to network security and topology. Through this work, we hope to encourage more research that adapt/optimize DP's deployment in various network environments.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcing Secure Live Migration through Verifiable State Management</title>
<link>https://arxiv.org/abs/2509.05150</link>
<guid>https://arxiv.org/abs/2509.05150</guid>
<content:encoded><![CDATA[
arXiv:2509.05150v1 Announce Type: new 
Abstract: Live migration of applications is a fundamental capability for enabling resilient computing in modern distributed systems. However, extending this functionality to trusted applications (TA) -- executing within Trusted Execution Environments (TEEs) -- introduces unique challenges such as secure state preservation, integrity verification, replay and rollback prevention, and mitigation of unauthorized cloning of TAs. We present TALOS, a lightweight framework for verifiable state management and trustworthy application migration. While our implementation is prototyped and evaluated using Intel SGX with the Gramine LibOS and RISC-V Keystone (evidencing the framework's portability across diverse TEEs), its design is agnostic to the underlying TEE architecture. Such agility is a necessity in today's network service mesh (collaborative computing across the continuum) where application workloads must be managed across domain boundaries in a harmonized fashion. TALOS is built around the principle of minimizing trust assumptions: TAs are treated as untrusted until explicitly verified, and the migration process does not rely on a trusted third party. To ensure both the integrity and secure launch of the migrated application, TALOS integrates memory introspection and control-flow graph extraction, enabling robust verification of state continuity and execution flow. Thereby achieving strong security guarantees while maintaining efficiency, making it suitable for decentralized settings.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Evaluating the Poisoning Robustness of Federated Learning under Local Differential Privacy</title>
<link>https://arxiv.org/abs/2509.05265</link>
<guid>https://arxiv.org/abs/2509.05265</guid>
<content:encoded><![CDATA[
arXiv:2509.05265v1 Announce Type: new 
Abstract: Federated learning (FL) combined with local differential privacy (LDP) enables privacy-preserving model training across decentralized data sources. However, the decentralized data-management paradigm leaves LDPFL vulnerable to participants with malicious intent. The robustness of LDPFL protocols, particularly against model poisoning attacks (MPA), where adversaries inject malicious updates to disrupt global model convergence, remains insufficiently studied. In this paper, we propose a novel and extensible model poisoning attack framework tailored for LDPFL settings. Our approach is driven by the objective of maximizing the global training loss while adhering to local privacy constraints. To counter robust aggregation mechanisms such as Multi-Krum and trimmed mean, we develop adaptive attacks that embed carefully crafted constraints into a reverse training process, enabling evasion of these defenses. We evaluate our framework across three representative LDPFL protocols, three benchmark datasets, and two types of deep neural networks. Additionally, we investigate the influence of data heterogeneity and privacy budgets on attack effectiveness. Experimental results demonstrate that our adaptive attacks can significantly degrade the performance of the global model, revealing critical vulnerabilities and highlighting the need for more robust LDPFL defense strategies against MPA. Our code is available at https://github.com/ZiJW/LDPFL-Attack
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to accelerate distributed ADMM using graph neural networks</title>
<link>https://arxiv.org/abs/2509.05288</link>
<guid>https://arxiv.org/abs/2509.05288</guid>
<content:encoded><![CDATA[
arXiv:2509.05288v1 Announce Type: new 
Abstract: Distributed optimization is fundamental in large-scale machine learning and control applications. Among existing methods, the Alternating Direction Method of Multipliers (ADMM) has gained popularity due to its strong convergence guarantees and suitability for decentralized computation. However, ADMM often suffers from slow convergence and sensitivity to hyperparameter choices. In this work, we show that distributed ADMM iterations can be naturally represented within the message-passing framework of graph neural networks (GNNs). Building on this connection, we propose to learn adaptive step sizes and communication weights by a graph neural network that predicts the hyperparameters based on the iterates. By unrolling ADMM for a fixed number of iterations, we train the network parameters end-to-end to minimize the final iterates error for a given problem class, while preserving the algorithm's convergence properties. Numerical experiments demonstrate that our learned variant consistently improves convergence speed and solution quality compared to standard ADMM. The code is available at https://github.com/paulhausner/learning-distributed-admm.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-based Decentralized Task Allocation for Multi-Robot Target Localization</title>
<link>https://arxiv.org/abs/2309.08896</link>
<guid>https://arxiv.org/abs/2309.08896</guid>
<content:encoded><![CDATA[
arXiv:2309.08896v2 Announce Type: replace 
Abstract: We introduce a new graph neural operator-based approach for task allocation in a system of heterogeneous robots composed of Unmanned Ground Vehicles (UGVs) and Unmanned Aerial Vehicles (UAVs). The proposed model, \texttt{\method}, or \textbf{G}raph \textbf{A}ttention \textbf{T}ask \textbf{A}llocato\textbf{R} aggregates information from neighbors in the multi-robot system, with the aim of achieving globally optimal target localization. Being decentralized, our method is highly robust and adaptable to situations where the number of robots and the number of tasks may change over time. We also propose a heterogeneity-aware preprocessing technique to model the heterogeneity of the system. The experimental results demonstrate the effectiveness and scalability of the proposed approach in a range of simulated scenarios generated by varying the number of UGVs and UAVs and the number and location of the targets. We show that a single model can handle a heterogeneous robot team with a number of robots ranging between 2 and 12 while outperforming the baseline architectures.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rolling in the Shadows: Analyzing the Extraction of MEV Across Layer-2 Rollups</title>
<link>https://arxiv.org/abs/2405.00138</link>
<guid>https://arxiv.org/abs/2405.00138</guid>
<content:encoded><![CDATA[
arXiv:2405.00138v3 Announce Type: replace 
Abstract: The emergence of decentralized finance has transformed asset trading on the blockchain, making traditional financial instruments more accessible while also introducing a series of exploitative economic practices known as Maximal Extractable Value (MEV). Concurrently, decentralized finance has embraced rollup-based Layer-2 solutions to facilitate asset trading at reduced transaction costs compared to Layer-1 solutions such as Ethereum. However, rollups lack a public mempool like Ethereum, making the extraction of MEV more challenging. In this paper, we investigate the prevalence and impact of MEV on Ethereum and prominent rollups such as Arbitrum, Optimism, and zkSync over a nearly three-year period. Our analysis encompasses various metrics including volume, profits, costs, competition, and response time to MEV opportunities. We discover that MEV is widespread on rollups, with trading volume comparable to Ethereum. We also find that, although MEV costs are lower on rollups, profits are also significantly lower compared to Ethereum. Additionally, we examine the prevalence of sandwich attacks on rollups. While our findings did not detect any sandwiching activity on popular rollups, we did identify the potential for cross-layer sandwich attacks facilitated by transactions that are sent across rollups and Ethereum. Consequently, we propose and evaluate the feasibility of three novel attacks that exploit cross-layer transactions, revealing that attackers could have already earned approximately 2 million USD through cross-layer sandwich attacks.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Organizing Aerial Swarm Robotics for Resilient Load Transportation : A Table-Mechanics-Inspired Approach</title>
<link>https://arxiv.org/abs/2509.03563</link>
<guid>https://arxiv.org/abs/2509.03563</guid>
<content:encoded><![CDATA[
arXiv:2509.03563v1 Announce Type: new 
Abstract: In comparison with existing approaches, which struggle with scalability, communication dependency, and robustness against dynamic failures, cooperative aerial transportation via robot swarms holds transformative potential for logistics and disaster response. Here, we present a physics-inspired cooperative transportation approach for flying robot swarms that imitates the dissipative mechanics of table-leg load distribution. By developing a decentralized dissipative force model, our approach enables autonomous formation stabilization and adaptive load allocation without the requirement of explicit communication. Based on local neighbor robots and the suspended payload, each robot dynamically adjusts its position. This is similar to energy-dissipating table leg reactions. The stability of the resultant control system is rigorously proved. Simulations demonstrate that the tracking errors of the proposed approach are 20%, 68%, 55.5%, and 21.9% of existing approaches under the cases of capability variation, cable uncertainty, limited vision, and payload variation, respectively. In real-world experiments with six flying robots, the cooperative aerial transportation system achieved a 94% success rate under single-robot failure, disconnection events, 25% payload variation, and 40% cable length uncertainty, demonstrating strong robustness under outdoor winds up to Beaufort scale 4. Overall, this physics-inspired approach bridges swarm intelligence and mechanical stability principles, offering a scalable framework for heterogeneous aerial systems to collectively handle complex transportation tasks in communication-constrained environments.
]]></content:encoded>
<pubDate>Sat, 06 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-decentralized Federated Time Series Prediction with Client Availability Budgets</title>
<link>https://arxiv.org/abs/2509.03660</link>
<guid>https://arxiv.org/abs/2509.03660</guid>
<content:encoded><![CDATA[
arXiv:2509.03660v1 Announce Type: new 
Abstract: Federated learning (FL) effectively promotes collaborative training among distributed clients with privacy considerations in the Internet of Things (IoT) scenarios. Despite of data heterogeneity, FL clients may also be constrained by limited energy and availability budgets. Therefore, effective selection of clients participating in training is of vital importance for the convergence of the global model and the balance of client contributions. In this paper, we discuss the performance impact of client availability with time-series data on federated learning. We set up three different scenarios that affect the availability of time-series data and propose FedDeCAB, a novel, semi-decentralized client selection method applying probabilistic rankings of available clients. When a client is disconnected from the server, FedDeCAB allows obtaining partial model parameters from the nearest neighbor clients for joint optimization, improving the performance of offline models and reducing communication overhead. Experiments based on real-world large-scale taxi and vessel trajectory datasets show that FedDeCAB is effective under highly heterogeneous data distribution, limited communication budget, and dynamic client offline or rejoining.
]]></content:encoded>
<pubDate>Sat, 06 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Safety-Critical Control of Resilient DC Microgrids with Large-Signal Stability Guarantees</title>
<link>https://arxiv.org/abs/2509.03789</link>
<guid>https://arxiv.org/abs/2509.03789</guid>
<content:encoded><![CDATA[
arXiv:2509.03789v1 Announce Type: new 
Abstract: The increasing penetration of distributed energy resources and power-electronics interfaces in DC microgrids, coupled with rising cyber threats, necessitates primary controllers that are provably safe, cyber-resilient, and practical. The increasing penetration of distributed energy resources and power-electronics interfaces in DC microgrids, coupled with rising cyber threats, necessitates primary controllers that are provably safe, cyber-resilient, and practical. Conventional droop-based methods remain prevalent due to their simplicity, yet their design is largely empirical and conservative, lacking rigorous guarantees. Advanced strategies improve certain aspects, but often sacrifice scalability, robustness, or formal safety. In this work, we propose a Distributed Safety-Critical Controller (DSCC) that systematically integrates global stabilization with formal safety guarantees in a fully decentralized manner. Leveraging control barrier functions and the port-Hamiltonian system theory, the DSCC achieves scalable safe stabilization while preserving real-time implementability. High-fidelity switched-circuit simulations validate the controller's advantages under various contingencies. This framework paves the way for resilient, safety-critical, and scalable control in next-generation DC microgrids.
]]></content:encoded>
<pubDate>Sat, 06 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.03817</link>
<guid>https://arxiv.org/abs/2509.03817</guid>
<content:encoded><![CDATA[
arXiv:2509.03817v1 Announce Type: new 
Abstract: Multi-agent systems of large language models (LLMs) show promise for complex reasoning, but their effectiveness is often limited by fixed collaboration protocols. These frameworks typically focus on macro-level orchestration while overlooking agents' internal deliberative capabilities. This critical meta-cognitive blindspot treats agents as passive executors unable to adapt their strategy based on internal cognitive states like uncertainty or confidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where agents learn a decentralized policy over a set of high-level meta-cognitive actions: Persist, Refine, and Concede. To overcome the instability of traditional policy gradients in this setting, we develop SoftRankPO, a novel reinforcement learning algorithm. SoftRankPO stabilizes training by shaping advantages based on the rank of rewards mapped through smooth normal quantiles, making the learning process robust to reward variance. Experiments show that MPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across five mathematical and general reasoning benchmarks compared to six state-of-the-art heuristic and learning-based multi-agent reasoning algorithms. Our work presents a paradigm for learning adaptive, meta-cognitive policies for multi-agent LLM systems, shifting the focus from designing fixed protocols to learning dynamic, deliberative strategies.
]]></content:encoded>
<pubDate>Sat, 06 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KGBERT4Eth: A Feature-Complete Transformer Powered by Knowledge Graph for Multi-Task Ethereum Fraud Detection</title>
<link>https://arxiv.org/abs/2509.03860</link>
<guid>https://arxiv.org/abs/2509.03860</guid>
<content:encoded><![CDATA[
arXiv:2509.03860v1 Announce Type: new 
Abstract: Ethereum's rapid ecosystem expansion and transaction anonymity have triggered a surge in malicious activity. Detection mechanisms currently bifurcate into three technical strands: expert-defined features, graph embeddings, and sequential transaction patterns, collectively spanning the complete feature sets of Ethereum's native data layer. Yet the absence of cross-paradigm integration mechanisms forces practitioners to choose between sacrificing sequential context awareness, structured fund-flow patterns, or human-curated feature insights in their solutions. To bridge this gap, we propose KGBERT4Eth, a feature-complete pre-training encoder that synergistically combines two key components: (1) a Transaction Semantic Extractor, where we train an enhanced Transaction Language Model (TLM) to learn contextual semantic representations from conceptualized transaction records, and (2) a Transaction Knowledge Graph (TKG) that incorporates expert-curated domain knowledge into graph node embeddings to capture fund flow patterns and human-curated feature insights. We jointly optimize pre-training objectives for both components to fuse these complementary features, generating feature-complete embeddings. To emphasize rare anomalous transactions, we design a biased masking prediction task for TLM to focus on statistical outliers, while the Transaction TKG employs link prediction to learn latent transaction relationships and aggregate knowledge. Furthermore, we propose a mask-invariant attention coordination module to ensure stable dynamic information exchange between TLM and TKG during pre-training. KGBERT4Eth significantly outperforms state-of-the-art baselines in both phishing account detection and de-anonymization tasks, achieving absolute F1-score improvements of 8-16% on three phishing detection benchmarks and 6-26% on four de-anonymization datasets.
]]></content:encoded>
<pubDate>Sat, 06 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LMAE4Eth: Generalizable and Robust Ethereum Fraud Detection by Exploring Transaction Semantics and Masked Graph Embedding</title>
<link>https://arxiv.org/abs/2509.03939</link>
<guid>https://arxiv.org/abs/2509.03939</guid>
<content:encoded><![CDATA[
arXiv:2509.03939v1 Announce Type: new 
Abstract: Current Ethereum fraud detection methods rely on context-independent, numerical transaction sequences, failing to capture semantic of account transactions. Furthermore, the pervasive homogeneity in Ethereum transaction records renders it challenging to learn discriminative account embeddings. Moreover, current self-supervised graph learning methods primarily learn node representations through graph reconstruction, resulting in suboptimal performance for node-level tasks like fraud account detection, while these methods also encounter scalability challenges. To tackle these challenges, we propose LMAE4Eth, a multi-view learning framework that fuses transaction semantics, masked graph embedding, and expert knowledge. We first propose a transaction-token contrastive language model (TxCLM) that transforms context-independent numerical transaction records into logically cohesive linguistic representations. To clearly characterize the semantic differences between accounts, we also use a token-aware contrastive learning pre-training objective together with the masked transaction model pre-training objective, learns high-expressive account representations. We then propose a masked account graph autoencoder (MAGAE) using generative self-supervised learning, which achieves superior node-level account detection by focusing on reconstructing account node features. To enable MAGAE to scale for large-scale training, we propose to integrate layer-neighbor sampling into the graph, which reduces the number of sampled vertices by several times without compromising training quality. Finally, using a cross-attention fusion network, we unify the embeddings of TxCLM and MAGAE to leverage the benefits of both. We evaluate our method against 21 baseline approaches on three datasets. Experimental results show that our method outperforms the best baseline by over 10% in F1-score on two of the datasets.
]]></content:encoded>
<pubDate>Sat, 06 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trustworthy Second-hand Marketplace for Built Environment</title>
<link>https://arxiv.org/abs/2509.04085</link>
<guid>https://arxiv.org/abs/2509.04085</guid>
<content:encoded><![CDATA[
arXiv:2509.04085v1 Announce Type: new 
Abstract: The construction industry faces significant challenges regarding material waste and sustainable practices, necessitating innovative solutions that integrate automation, traceability, and decentralised decision-making to enable efficient material reuse. This paper presents a blockchain-enabled digital marketplace for sustainable construction material reuse, ensuring transparency and traceability using InterPlanetary File System (IPFS). The proposed framework enhances trust and accountability in material exchange, addressing key challenges in industrial automation and circular supply chains. A framework has been developed to demonstrate the operational processes of the marketplace, illustrating its practical application and effectiveness. Our contributions show how the marketplace can facilitate the efficient and trustworthy exchange of reusable materials, representing a substantial step towards more sustainable construction practices.
]]></content:encoded>
<pubDate>Sat, 06 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Cryptocurrency Transaction Analysis: A Bitcoin Case Study</title>
<link>https://arxiv.org/abs/2501.18158</link>
<guid>https://arxiv.org/abs/2501.18158</guid>
<content:encoded><![CDATA[
arXiv:2501.18158v3 Announce Type: replace 
Abstract: Cryptocurrencies are widely used, yet current methods for analyzing transactions often rely on opaque, black-box models. While these models may achieve high performance, their outputs are usually difficult to interpret and adapt, making it challenging to capture nuanced behavioral patterns. Large language models (LLMs) have the potential to address these gaps, but their capabilities in this area remain largely unexplored, particularly in cybercrime detection. In this paper, we test this hypothesis by applying LLMs to real-world cryptocurrency transaction graphs, with a focus on Bitcoin, one of the most studied and widely adopted blockchain networks. We introduce a three-tiered framework to assess LLM capabilities: foundational metrics, characteristic overview, and contextual interpretation. This includes a new, human-readable graph representation format, LLM4TG, and a connectivity-enhanced transaction graph sampling algorithm, CETraS. Together, they significantly reduce token requirements, transforming the analysis of multiple moderately large-scale transaction graphs with LLMs from nearly impossible to feasible under strict token limits. Experimental results demonstrate that LLMs have outstanding performance on foundational metrics and characteristic overview, where the accuracy of recognizing most basic information at the node level exceeds 98.50% and the proportion of obtaining meaningful characteristics reaches 95.00%. Regarding contextual interpretation, LLMs also demonstrate strong performance in classification tasks, even with very limited labeled data, where top-3 accuracy reaches 72.43% with explanations. While the explanations are not always fully accurate, they highlight the strong potential of LLMs in this domain. At the same time, several limitations persist, which we discuss along with directions for future research.
]]></content:encoded>
<pubDate>Sat, 06 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Elastic Restaking Networks</title>
<link>https://arxiv.org/abs/2503.00170</link>
<guid>https://arxiv.org/abs/2503.00170</guid>
<content:encoded><![CDATA[
arXiv:2503.00170v4 Announce Type: replace 
Abstract: Many blockchain-based decentralized services require their validators (operators) to deposit stake (collateral), which is forfeited (slashed) if they misbehave. Restaking networks let validators secure multiple services by reusing stake. These networks have quickly gained traction, leveraging over \$20 billion in stake. However, restaking introduces a new attack vector where validators can coordinate to misbehave across multiple services simultaneously, extracting digital assets while forfeiting their stake only once.
  Previous work focused either on preventing coordinated misbehavior or on protecting services if all other services are Byzantine and might unjustly cause slashing due to bugs or malice. The first model overlooks how a single Byzantine service can collapse the network, while the second ignores shared-stake benefits.
  To bridge the gap, we analyze the system as a strategic game of coordinated misbehavior, when a given fraction of the services are Byzantine. We introduce elastic restaking networks, where validators can allocate portions of their stake that may cumulatively exceed their total stake, and when allocations are lost, the remaining stake stretches to cover remaining allocations. We show that elastic networks exhibit superior robustness compared to previous approaches, and demonstrate a synergistic effect where an elastic restaking network enhances its blockchain's security, contrary to community concerns of an opposite effect in existing networks. We then design incentives for tuning validators' allocations.
  Our elastic restaking system and incentive design have immediate practical implications for deployed restaking networks.
]]></content:encoded>
<pubDate>Sat, 06 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Variable Modeling in Multi-Agent Reinforcement Learning via Expectation-Maximization for UAV-Based Wildlife Protection</title>
<link>https://arxiv.org/abs/2509.02579</link>
<guid>https://arxiv.org/abs/2509.02579</guid>
<content:encoded><![CDATA[
arXiv:2509.02579v1 Announce Type: new 
Abstract: Protecting endangered wildlife from illegal poaching presents a critical challenge, particularly in vast and partially observable environments where real-time response is essential. This paper introduces a novel Expectation-Maximization (EM) based latent variable modeling approach in the context of Multi-Agent Reinforcement Learning (MARL) for Unmanned Aerial Vehicle (UAV) coordination in wildlife protection. By modeling hidden environmental factors and inter-agent dynamics through latent variables, our method enhances exploration and coordination under uncertainty.We implement and evaluate our EM-MARL framework using a custom simulation involving 10 UAVs tasked with patrolling protected habitats of the endangered Iranian leopard. Extensive experimental results demonstrate superior performance in detection accuracy, adaptability, and policy convergence when compared to standard algorithms such as Proximal Policy Optimization (PPO) and Deep Deterministic Policy Gradient (DDPG). Our findings underscore the potential of combining EM inference with MARL to improve decentralized decisionmaking in complex, high-stakes conservation scenarios. The full implementation, simulation environment, and training scripts are publicly available on GitHub.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision: An Extensible Methodology for Formal Software Verification in Microservice Systems</title>
<link>https://arxiv.org/abs/2509.02860</link>
<guid>https://arxiv.org/abs/2509.02860</guid>
<content:encoded><![CDATA[
arXiv:2509.02860v1 Announce Type: new 
Abstract: Microservice systems are becoming increasingly adopted due to their scalability, decentralized development, and support for continuous integration and delivery (CI/CD). However, this decentralized development by separate teams and continuous evolution can introduce miscommunication and incompatible implementations, undermining system maintainability and reliability across aspects from security policy to system architecture. We propose a novel methodology that statically reconstructs microservice source code into a formal system model. From this model, a Satisfiability Modulo Theories (SMT) constraint set can be derived, enabling formal verification. Our methodology is extensible, supporting software verification across multiple cross-cutting concerns. We focus on applying the methodology to verify the system architecture concern, presenting formal reasoning to validate the methodology's correctness and applicability for this concern. Additional concerns such as security policy implementation are considered. Future directions are established to extend and evaluate the methodology.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Power Grid Control with Graph-Based Distributed Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.02861</link>
<guid>https://arxiv.org/abs/2509.02861</guid>
<content:encoded><![CDATA[
arXiv:2509.02861v1 Announce Type: new 
Abstract: The necessary integration of renewable energy sources, combined with the expanding scale of power networks, presents significant challenges in controlling modern power grids. Traditional control systems, which are human and optimization-based, struggle to adapt and to scale in such an evolving context, motivating the exploration of more dynamic and distributed control strategies. This work advances a graph-based distributed reinforcement learning framework for real-time, scalable grid management. The proposed architecture consists of a network of distributed low-level agents acting on individual power lines and coordinated by a high-level manager agent. A Graph Neural Network (GNN) is employed to encode the network's topological information within the single low-level agent's observation. To accelerate convergence and enhance learning stability, the framework integrates imitation learning and potential-based reward shaping. In contrast to conventional decentralized approaches that decompose only the action space while relying on global observations, this method also decomposes the observation space. Each low-level agent acts based on a structured and informative local view of the environment constructed through the GNN. Experiments on the Grid2Op simulation environment show the effectiveness of the approach, which consistently outperforms the standard baseline commonly adopted in the field. Additionally, the proposed model proves to be much more computationally efficient than the simulation-based Expert method.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TraceLLM: Security Diagnosis Through Traces and Smart Contracts in Ethereum</title>
<link>https://arxiv.org/abs/2509.03037</link>
<guid>https://arxiv.org/abs/2509.03037</guid>
<content:encoded><![CDATA[
arXiv:2509.03037v1 Announce Type: new 
Abstract: Ethereum smart contracts hold tens of billions of USD in DeFi and NFTs, yet comprehensive security analysis remains difficult due to unverified code, proxy-based architectures, and the reliance on manual inspection of complex execution traces. Existing approaches fall into two main categories: anomaly transaction detection, which flags suspicious transactions but offers limited insight into specific attack strategies hidden in execution traces inside transactions, and code vulnerability detection, which cannot analyze unverified contracts and struggles to show how identified flaws are exploited in real incidents. As a result, analysts must still manually align transaction traces with contract code to reconstruct attack scenarios and conduct forensics. To address this gap, TraceLLM is proposed as a framework that leverages LLMs to integrate execution trace-level detection with decompiled contract code. We introduce a new anomaly execution path identification algorithm and an LLM-refined decompile tool to identify vulnerable functions and provide explicit attack paths to LLM. TraceLLM establishes the first benchmark for joint trace and contract code-driven security analysis. For comparison, proxy baselines are created by jointly transmitting the results of three representative code analysis along with raw traces to LLM. TraceLLM identifies attacker and victim addresses with 85.19\% precision and produces automated reports with 70.37\% factual precision across 27 cases with ground truth expert reports, achieving 25.93\% higher accuracy than the best baseline. Moreover, across 148 real-world Ethereum incidents, TraceLLM automatically generates reports with 66.22\% expert-verified accuracy, demonstrating strong generalizability.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralised self-organisation of pivoting cube ensembles using geometric deep learning</title>
<link>https://arxiv.org/abs/2509.03140</link>
<guid>https://arxiv.org/abs/2509.03140</guid>
<content:encoded><![CDATA[
arXiv:2509.03140v1 Announce Type: new 
Abstract: We present a decentralized model for autonomous reconfiguration of homogeneous pivoting cube modular robots in two dimensions. Each cube in the ensemble is controlled by a neural network that only gains information from other cubes in its local neighborhood, trained using reinforcement learning. Furthermore, using geometric deep learning, we include the grid symmetries of the cube ensemble in the neural network architecture. We find that even the most localized versions succeed in reconfiguring to the target shape, although reconfiguration happens faster the more information about the whole ensemble is available to individual cubes. Near-optimal reconfiguration is achieved with only nearest neighbor interactions by using multiple information passing between cubes, allowing them to accumulate more global information about the ensemble. Compared to standard neural network architectures, using geometric deep learning approaches provided only minor benefits. Overall, we successfully demonstrate mostly local control of a modular self-assembling system, which is transferable to other space-relevant systems with different action spaces, such as sliding cube modular robots and CubeSat swarms.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyPV-LEAD: Proactive Early-Warning of Cryptocurrency Anomalies through Data-Driven Structural-Temporal Modeling</title>
<link>https://arxiv.org/abs/2509.03260</link>
<guid>https://arxiv.org/abs/2509.03260</guid>
<content:encoded><![CDATA[
arXiv:2509.03260v1 Announce Type: new 
Abstract: Abnormal cryptocurrency transactions - such as mixing services, fraudulent transfers, and pump-and-dump operations -- pose escalating risks to financial integrity but remain notoriously difficult to detect due to class imbalance, temporal volatility, and complex network dependencies. Existing approaches are predominantly model-centric and post hoc, flagging anomalies only after they occur and thus offering limited preventive value. This paper introduces HyPV-LEAD (Hyperbolic Peak-Valley Lead-time Enabled Anomaly Detection), a data-driven early-warning framework that explicitly incorporates lead time into anomaly detection. Unlike prior methods, HyPV-LEAD integrates three innovations: (1) window-horizon modeling to guarantee actionable lead-time alerts, (2) Peak-Valley (PV) sampling to mitigate class imbalance while preserving temporal continuity, and (3) hyperbolic embedding to capture the hierarchical and scale-free properties of blockchain transaction networks. Empirical evaluation on large-scale Bitcoin transaction data demonstrates that HyPV-LEAD consistently outperforms state-of-the-art baselines, achieving a PR-AUC of 0.9624 with significant gains in precision and recall. Ablation studies further confirm that each component - PV sampling, hyperbolic embedding, and structural-temporal modeling - provides complementary benefits, with the full framework delivering the highest performance. By shifting anomaly detection from reactive classification to proactive early-warning, HyPV-LEAD establishes a robust foundation for real-time risk management, anti-money laundering (AML) compliance, and financial security in dynamic blockchain environments.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tuning Block Size for Workload Optimization in Consortium Blockchain Networks</title>
<link>https://arxiv.org/abs/2509.03367</link>
<guid>https://arxiv.org/abs/2509.03367</guid>
<content:encoded><![CDATA[
arXiv:2509.03367v1 Announce Type: new 
Abstract: Determining the optimal block size is crucial for achieving high throughput in blockchain systems. Many studies have focused on tuning various components, such as databases, network bandwidth, and consensus mechanisms. However, the impact of block size on system performance remains a topic of debate, often resulting in divergent views and even leading to new forks in blockchain networks. This research proposes a mathematical model to maximize performance by determining the ideal block size for Hyperledger Fabric, a prominent consortium blockchain. By leveraging machine learning and solving the model with a genetic algorithm, the proposed approach assesses how factors such as block size, transaction size, and network capacity influence the block processing time. The integration of an optimization solver enables precise adjustments to block size configuration before deployment, ensuring improved performance from the outset. This systematic approach aims to balance block processing efficiency, network latency, and system throughput, offering a robust solution to improve blockchain performance across diverse business contexts.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MorphAgent: Empowering Agents through Self-Evolving Profiles and Decentralized Collaboration</title>
<link>https://arxiv.org/abs/2410.15048</link>
<guid>https://arxiv.org/abs/2410.15048</guid>
<content:encoded><![CDATA[
arXiv:2410.15048v2 Announce Type: replace 
Abstract: Large Language Model (LLM) based multi-agent systems (MAS) have shown promise in tackling complex tasks, but often rely on predefined roles and centralized coordination, limiting their adaptability to evolving challenges. This paper introduces MorphAgent, a novel Autonomous, Self-Organizing, and Self-Adaptive Multi-Agent System for decentralized agent collaboration that enables agents to dynamically evolve their roles and capabilities. Our approach employs self-evolving agent profiles, optimized through three key metrics, guiding agents in refining their individual expertise while maintaining complementary team dynamics. MorphAgent implements a two-phase process: a Profile Update phase for profile optimization, followed by a Task Execution phase where agents continuously adapt their roles based on task feedback. Our experimental results show that MorphAgent outperforms existing frameworks in terms of task performance and adaptability to changing requirements, paving the way for more robust and versatile multi-agent collaborative systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedP$^2$EFT: Federated Learning to Personalize PEFT for Multilingual LLMs</title>
<link>https://arxiv.org/abs/2502.04387</link>
<guid>https://arxiv.org/abs/2502.04387</guid>
<content:encoded><![CDATA[
arXiv:2502.04387v2 Announce Type: replace 
Abstract: Federated learning (FL) has enabled the training of multilingual large language models (LLMs) on diverse and decentralized multilingual data, especially on low-resource languages. To improve client-specific performance, personalization via the use of parameter-efficient fine-tuning (PEFT) modules such as LoRA is common. This involves a personalization strategy (PS), such as the design of the PEFT adapter structures (e.g., in which layers to add LoRAs and what ranks) and choice of hyperparameters (e.g., learning rates) for fine-tuning. Instead of manual PS configuration, we propose FedP$^2$EFT, a federated learning-to-personalize method for multilingual LLMs in cross-device FL settings. Unlike most existing PEFT structure selection methods, which are prone to overfitting low-data regimes, FedP$^2$EFT collaboratively learns the optimal personalized PEFT structure for each client via Bayesian sparse rank selection. Evaluations on both simulated and real-world multilingual FL benchmarks demonstrate that FedP$^2$EFT largely outperforms existing personalized fine-tuning methods, while complementing other existing FL methods. Code is available at https://github.com/SamsungLabs/fedp2eft.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation</title>
<link>https://arxiv.org/abs/2506.07940</link>
<guid>https://arxiv.org/abs/2506.07940</guid>
<content:encoded><![CDATA[
arXiv:2506.07940v2 Announce Type: replace 
Abstract: Current AutoML platforms leave substantial performance untapped. Testing 180 fine-tuning tasks across models from 70M to 70B parameters, we found that HuggingFace AutoTrain, TogetherAI, Databricks, and Google Cloud consistently produce suboptimal configurations. Gradients, built on the Bittensor network, attacks this problem through competition. Independent miners race to find optimal hyperparameters, earning rewards proportional to their models' performance. This tournament drives exploration of configuration spaces that single-strategy methods never examine. In our experiments, Gradients achieved a 100\% win rate against TogetherAI, Databricks, and Google Cloud, and beat HuggingFace AutoTrain in 82.8\% of experiments. Mean improvements reached 42.1\% against commercial platforms. Retrieval-augmented generation tasks saw 30-40\% gains; diffusion models improved 23.4\% on person-specific generation. When miners compete for rewards, they develop optimization strategies that centralized approaches overlook. These findings demonstrate that decentralized systems with economic incentives can systematically outperform traditional AutoML, suggesting market dynamics may be key to achieving superior fine-tuning results. Code is available at https://github.com/rayonlabs/G.O.D.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Agentic AI on Particle Accelerators</title>
<link>https://arxiv.org/abs/2409.06336</link>
<guid>https://arxiv.org/abs/2409.06336</guid>
<content:encoded><![CDATA[
arXiv:2409.06336v4 Announce Type: replace-cross 
Abstract: As particle accelerators grow in complexity, traditional control methods face increasing challenges in achieving optimal performance. This paper envisions a paradigm shift: a decentralized multi-agent framework for accelerator control, powered by Large Language Models (LLMs) and distributed among autonomous agents. We present a proposition of a self-improving decentralized system where intelligent agents handle high-level tasks and communication and each agent is specialized to control individual accelerator components.
  This approach raises some questions: What are the future applications of AI in particle accelerators? How can we implement an autonomous complex system such as a particle accelerator where agents gradually improve through experience and human feedback? What are the implications of integrating a human-in-the-loop component for labeling operational data and providing expert guidance? We show three examples, where we demonstrate the viability of such architecture.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BPI: A Novel Efficient and Reliable Search Structure for Hybrid Storage Blockchain</title>
<link>https://arxiv.org/abs/2509.00480</link>
<guid>https://arxiv.org/abs/2509.00480</guid>
<content:encoded><![CDATA[
arXiv:2509.00480v1 Announce Type: new 
Abstract: Hybrid storage solutions have emerged as potent strategies to alleviate the data storage bottlenecks prevalent in blockchain systems. These solutions harness off-chain Storage Services Providers (SPs) in conjunction with Authenticated Data Structures (ADS) to ensure data integrity and accuracy. Despite these advancements, the reliance on centralized SPs raises concerns about query correctness. Although ADS can verify the existence of individual query results, they fall short of preventing SPs from omitting valid results.
  In this paper, we delineate the fundamental distinctions between data search in blockchains and traditional database systems. Drawing upon these insights, we introduce BPI, a lightweight framework that enables efficient keyword queries and maintenance with low overhead. We propose "Articulated Search", a query pattern specifically designed for blockchain environments that enhances search efficiency while significantly reducing costs during data user updates. Furthermore, BPI employs a suite of validation models to ensure the inclusion of all valid content in search results while maintaining low overhead.
  Extensive experimental evaluations demonstrate that the BPI framework achieves outstanding scalability and performance in keyword searches within blockchain, surpassing EthMB+ and state of the art search databases commonly used in mainstream hybrid storage blockchains (HSB).
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLEET: A Federated Learning Emulation and Evaluation Testbed for Holistic Research</title>
<link>https://arxiv.org/abs/2509.00621</link>
<guid>https://arxiv.org/abs/2509.00621</guid>
<content:encoded><![CDATA[
arXiv:2509.00621v1 Announce Type: new 
Abstract: Federated Learning (FL) presents a robust paradigm for privacy-preserving, decentralized machine learning. However, a significant gap persists between the theoretical design of FL algorithms and their practical performance, largely because existing evaluation tools often fail to model realistic operational conditions. Many testbeds oversimplify the critical dynamics among algorithmic efficiency, client-level heterogeneity, and continuously evolving network infrastructure. To address this challenge, we introduce the Federated Learning Emulation and Evaluation Testbed (FLEET). This comprehensive platform provides a scalable and configurable environment by integrating a versatile, framework-agnostic learning component with a high-fidelity network emulator. FLEET supports diverse machine learning frameworks, customizable real-world network topologies, and dynamic background traffic generation. The testbed collects holistic metrics that correlate algorithmic outcomes with detailed network statistics. By unifying the entire experiment configuration, FLEET enables researchers to systematically investigate how network constraints, such as limited bandwidth, high latency, and packet loss, affect the convergence and efficiency of FL algorithms. This work provides the research community with a robust tool to bridge the gap between algorithmic theory and real-world network conditions, promoting the holistic and reproducible evaluation of federated learning systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Decentralized Federated Multi-task Learning With Trustworthiness in Cyber-Physical Systems</title>
<link>https://arxiv.org/abs/2509.00992</link>
<guid>https://arxiv.org/abs/2509.00992</guid>
<content:encoded><![CDATA[
arXiv:2509.00992v1 Announce Type: new 
Abstract: Multi-task learning is an effective way to address the challenge of model personalization caused by high data heterogeneity in federated learning. However, extending multi-task learning to the online decentralized federated learning setting is yet to be explored. The online decentralized federated learning setting considers many real-world applications of federated learning, such as autonomous systems, where clients communicate peer-to-peer and the data distribution of each client is time-varying. A more serious problem in real-world applications of federated learning is the presence of Byzantine clients. Byzantine-resilient approaches used in federated learning work only when the number of Byzantine clients is less than one-half the total number of clients. Yet, it is difficult to put a limit on the number of Byzantine clients within a system in reality. However, recent work in robotics shows that it is possible to exploit cyber-physical properties of a system to predict clients' behavior and assign a trust probability to received signals. This can help to achieve resiliency in the presence of a dominating number of Byzantine clients. Therefore, in this paper, we develop an online decentralized federated multi-task learning algorithm to provide model personalization and resiliency when the number of Byzantine clients dominates the number of honest clients. Our proposed algorithm leverages cyber-physical properties, such as the received signal strength in wireless systems or side information, to assign a trust probability to local models received from neighbors in each iteration. Our simulation results show that the proposed algorithm performs close to a Byzantine-free setting.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Rug Pulls in Decentralized Exchanges: Machine Learning Evidence from the TON Blockchain</title>
<link>https://arxiv.org/abs/2509.01168</link>
<guid>https://arxiv.org/abs/2509.01168</guid>
<content:encoded><![CDATA[
arXiv:2509.01168v1 Announce Type: new 
Abstract: This paper presents a machine learning framework for the early detection of rug pull scams on decentralized exchanges (DEXs) within The Open Network (TON) blockchain. TON's unique architecture, characterized by asynchronous execution and a massive web2 user base from Telegram, presents a novel and critical environment for fraud analysis. We conduct a comprehensive study on the two largest TON DEXs, Ston.Fi and DeDust, fusing data from both platforms to train our models. A key contribution is the implementation and comparative analysis of two distinct rug pull definitions--TVL-based (a catastrophic liquidity withdrawal) and idle-based (a sudden cessation of all trading activity)--within a single, unified study. We demonstrate that Gradient Boosting models can effectively identify rug pulls within the first five minutes of trading, with the TVL-based method achieving superior AUC (up to 0.891) while the idle-based method excels at recall. Our analysis reveals that while feature sets are consistent across exchanges, their underlying distributions differ significantly, challenging straightforward data fusion and highlighting the need for robust, platform-aware models. This work provides a crucial early-warning mechanism for investors and enhances the security infrastructure of the rapidly growing TON DeFi ecosystem.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Reinforcement Learning for Task Offloading in Wireless Edge Networks</title>
<link>https://arxiv.org/abs/2509.01257</link>
<guid>https://arxiv.org/abs/2509.01257</guid>
<content:encoded><![CDATA[
arXiv:2509.01257v1 Announce Type: new 
Abstract: In edge computing systems, autonomous agents must make fast local decisions while competing for shared resources. Existing MARL methods often resume to centralized critics or frequent communication, which fail under limited observability and communication constraints. We propose a decentralized framework in which each agent solves a constrained Markov decision process (CMDP), coordinating implicitly through a shared constraint vector. For the specific case of offloading, e.g., constraints prevent overloading shared server resources. Coordination constraints are updated infrequently and act as a lightweight coordination mechanism. They enable agents to align with global resource usage objectives but require little direct communication. Using safe reinforcement learning, agents learn policies that meet both local and global goals. We establish theoretical guarantees under mild assumptions and validate our approach experimentally, showing improved performance over centralized and independent baselines, especially in large-scale settings.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiFeChain: Lightweight Blockchain for Secure and Efficient Federated Lifelong Learning in IoT</title>
<link>https://arxiv.org/abs/2509.01434</link>
<guid>https://arxiv.org/abs/2509.01434</guid>
<content:encoded><![CDATA[
arXiv:2509.01434v1 Announce Type: new 
Abstract: The expansion of Internet of Things (IoT) devices constantly generates heterogeneous data streams, driving demand for continuous, decentralized intelligence. Federated Lifelong Learning (FLL) provides an ideal solution by incorporating federated and lifelong learning to overcome catastrophic forgetting. The extended lifecycle of FLL in IoT systems increases their vulnerability to persistent attacks, and these risks may be obscured by performance degradation caused by spatial-temporal data heterogeneity. Moreover, this problem is exacerbated by the standard single-server architecture, as its single point of failure makes it difficult to maintain a reliable audit trail for long-term threats. Blockchain provides a tamper-proof foundation for trustworthy FLL systems. Nevertheless, directly applying blockchain to FLL significantly increases computational and retrieval costs with the expansion of the knowledge base, slowing down the training on IoT devices. To address these challenges, we propose LiFeChain, a lightweight blockchain for secure and efficient federated lifelong learning by providing a tamper-resistant ledger with minimal on-chain disclosure and bidirectional verification. To the best of our knowledge, LiFeChain is the first blockchain tailored for FLL. LiFeChain incorporates two complementary mechanisms: the proof-of-model-correlation (PoMC) consensus on the server, which couples learning and unlearning mechanisms to mitigate negative transfer, and segmented zero-knowledge arbitration (Seg-ZA) on the client, which detects and arbitrates abnormal committee behavior without compromising privacy. LiFeChain is designed as a plug-and-play component that can be seamlessly integrated into existing FLL algorithms. Experimental results demonstrate that LiFeChain not only enhances model performance against two long-term attacks but also sustains high efficiency and scalability.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Technologies in Practical Demand Response: An Informational Requirement-based Roadmap</title>
<link>https://arxiv.org/abs/2509.01459</link>
<guid>https://arxiv.org/abs/2509.01459</guid>
<content:encoded><![CDATA[
arXiv:2509.01459v1 Announce Type: new 
Abstract: The future grid will be highly complex and decentralized, requiring sophisticated coordination across numerous human and software agents that manage distributed resources such as Demand Response (DR). Realizing this vision demands significant advances in semantic interoperability, which enables scalable and cost-effective automation across heterogeneous systems. While semantic technologies have progressed in commercial building and DR domains, current ontologies have two critical limitations: they are often developed without a formal framework that reflects real-world DR requirements, and proposals for integrating general and application-specific ontologies remain mostly conceptual, lacking formalization or empirical validation.
  In this paper, we address these gaps by applying a formal ontology evaluation/development approach to define the informational requirements (IRs) necessary for semantic interoperability in the area of incentive-based DR for commercial buildings. We identify the IRs associated with each stage of the wholesale incentive-based DR process, focusing on the perspective of building owners. Using these IRs, we evaluate how well existing ontologies (Brick, DELTA, and EFOnt) support the operational needs of DR participation. Our findings reveal substantial misalignments between current ontologies and practical DR requirements. Based on our assessments, we propose a roadmap of necessary extensions and integrations for these ontologies. This work ultimately aims to enhance the interoperability of today's and future smart grid, thereby facilitating scalable integration of DR systems into the grid's complex operational framework.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Foundation Models in Harsh Wireless Environments: Prospects, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2509.01957</link>
<guid>https://arxiv.org/abs/2509.01957</guid>
<content:encoded><![CDATA[
arXiv:2509.01957v1 Announce Type: new 
Abstract: Foundation models (FMs) have shown remarkable capabilities in generalized intelligence, multimodal understanding, and adaptive learning across a wide range of domains. However, their deployment in harsh or austere environments -- characterized by intermittent connectivity, limited computation, noisy data, and dynamically changing network topologies -- remains an open challenge. Existing distributed learning methods such as federated learning (FL) struggle to adapt in such settings due to their reliance on stable infrastructure, synchronized updates, and resource-intensive training. In this work, we explore the potential of Federated Foundation Models (FFMs) as a promising paradigm to address these limitations. By integrating the scalability and generalization power of FMs with novel decentralized, communication-aware FL frameworks, we aim to enable robust, energy-efficient, and adaptive intelligence in extreme and adversarial conditions. We present a detailed breakdown of system-level constraints in harsh environments, and discuss the open research challenges in communication design, model robustness, and energy-efficient personalization for these unique settings.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fault-Tolerant Decentralized Distributed Asynchronous Federated Learning with Adaptive Termination Detection</title>
<link>https://arxiv.org/abs/2509.02186</link>
<guid>https://arxiv.org/abs/2509.02186</guid>
<content:encoded><![CDATA[
arXiv:2509.02186v1 Announce Type: new 
Abstract: Federated Learning (FL) facilitates collaborative model training across distributed clients while ensuring data privacy. Traditionally, FL relies on a centralized server to coordinate learning, which creates bottlenecks and a single point of failure. Decentralized FL architectures eliminate the need for a central server and can operate in either synchronous or asynchronous modes. Synchronous FL requires all clients to compute updates and wait for one another before aggregation, guaranteeing consistency but often suffering from delays due to slower participants. Asynchronous FL addresses this by allowing clients to update independently, offering better scalability and responsiveness in heterogeneous environments.
  Our research develops an asynchronous decentralized FL approach in two progressive phases. (a) In Phase 1, we develop an asynchronous FL framework that enables clients to learn and update independently, removing the need for strict synchronization. (b) In Phase 2, we extend this framework with fault tolerance mechanisms to handle client failures and message drops, ensuring robust performance even under unpredictable conditions. As a central contribution, we propose Client-Confident Convergence and Client-Responsive Termination novel techniques that provide each client with the ability to autonomously determine appropriate termination points. These methods ensure that all active clients conclude meaningfully and efficiently, maintaining reliable convergence despite the challenges of asynchronous communication and faults.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VariAntNet: Learning Decentralized Control of Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2509.02271</link>
<guid>https://arxiv.org/abs/2509.02271</guid>
<content:encoded><![CDATA[
arXiv:2509.02271v1 Announce Type: new 
Abstract: A simple multi-agent system can be effectively utilized in disaster response applications, such as firefighting. Such a swarm is required to operate in complex environments with limited local sensing and no reliable inter-agent communication or centralized control. These simple robotic agents, also known as Ant Robots, are defined as anonymous agents that possess limited sensing capabilities, lack a shared coordinate system, and do not communicate explicitly with one another. A key challenge for simple swarms lies in maintaining cohesion and avoiding fragmentation despite limited-range sensing. Recent advances in machine learning offer effective solutions to some of the classical decentralized control challenges. We propose VariAntNet, a deep learning-based decentralized control model designed to facilitate agent swarming and collaborative task execution. VariAntNet includes geometric features extraction from unordered, variable-sized local observations. It incorporates a neural network architecture trained with a novel, differentiable, multi-objective, mathematically justified loss function that promotes swarm cohesiveness by utilizing the properties of the visibility graph Laplacian matrix. VariAntNet is demonstrated on the fundamental multi-agent gathering task, where agents with bearing-only and limited-range sensing must gather at some location. VariAntNet significantly outperforms an existing analytical solution, achieving more than double the convergence rate while maintaining high swarm connectivity across varying swarm sizes. While the analytical solution guarantees cohesion, it is often too slow in practice. In time-critical scenarios, such as emergency response operations where lives are at risk, slower analytical methods are impractical and justify the loss of some agents within the swarm. This paper presents and analyzes this trade-off in detail.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Near-Optimal Stability for Distributed Transaction Processing in Blockchain Sharding</title>
<link>https://arxiv.org/abs/2509.02421</link>
<guid>https://arxiv.org/abs/2509.02421</guid>
<content:encoded><![CDATA[
arXiv:2509.02421v1 Announce Type: new 
Abstract: In blockchain sharding, $n$ processing nodes are divided into $s$ shards, and each shard processes transactions in parallel. A key challenge in such a system is to ensure system stability for any ``tractable'' pattern of generated transactions; this is modeled by an adversary generating transactions with a certain rate of at most $\rho$ and burstiness $b$. This model captures worst-case scenarios and even some attacks on transactions' processing, e.g., DoS. A stable system ensures bounded transaction queue sizes and bounded transaction latency. It is known that the absolute upper bound on the maximum injection rate for which any scheduler could guarantee bounded queues and latency of transactions is $\max\left\{ \frac{2}{k+1}, \frac{2}{ \left\lfloor\sqrt{2s}\right\rfloor}\right\}$, where $k$ is the maximum number of shards that each transaction accesses. Here, we first provide a single leader scheduler that guarantees stability under injection rate $\rho \leq \max\left\{ \frac{1}{16k}, \frac{1}{16\lceil \sqrt{s} \rceil}\right\}$. Moreover, we also give a distributed scheduler with multiple leaders that guarantees stability under injection rate $\rho \leq \frac{1}{16c_1 \log D \log s}\max\left\{ \frac{1}{k}, \frac{1}{\lceil \sqrt{s} \rceil} \right\}$, where $c_1$ is some positive constant and $D$ is the diameter of shard graph $G_s$. This bound is within a poly-log factor from the optimal injection rate, and significantly improves the best previous known result for the distributed setting by Adhikari et al., SPAA 2024.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Transformers with Centralized Aggregation are Sample-Efficient Multi-Agent World Models</title>
<link>https://arxiv.org/abs/2406.15836</link>
<guid>https://arxiv.org/abs/2406.15836</guid>
<content:encoded><![CDATA[
arXiv:2406.15836v2 Announce Type: replace 
Abstract: Learning a world model for model-free Reinforcement Learning (RL) agents can significantly improve the sample efficiency by learning policies in imagination. However, building a world model for Multi-Agent RL (MARL) can be particularly challenging due to the scalability issue in a centralized architecture arising from a large number of agents, and also the non-stationarity issue in a decentralized architecture stemming from the inter-dependency among agents. To address both challenges, we propose a novel world model for MARL that learns decentralized local dynamics for scalability, combined with a centralized representation aggregation from all agents. We cast the dynamics learning as an auto-regressive sequence modeling problem over discrete tokens by leveraging the expressive Transformer architecture, in order to model complex local dynamics across different agents and provide accurate and consistent long-term imaginations. As the first pioneering Transformer-based world model for multi-agent systems, we introduce a Perceiver Transformer as an effective solution to enable centralized representation aggregation within this context. Results on Starcraft Multi-Agent Challenge (SMAC) show that it outperforms strong model-free approaches and existing model-based methods in both sample efficiency and overall performance.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nav-SCOPE: Swarm Robot Cooperative Perception and Coordinated Navigation</title>
<link>https://arxiv.org/abs/2409.10049</link>
<guid>https://arxiv.org/abs/2409.10049</guid>
<content:encoded><![CDATA[
arXiv:2409.10049v3 Announce Type: replace 
Abstract: This paper proposes a lightweight systematic solution for multi-robot coordinated navigation with decentralized cooperative perception. An information flow is first created to facilitate real-time observation sharing over unreliable ad-hoc networks. Then, the environmental uncertainties of each robot are reduced by interaction fields that deliver complementary information. Finally, path optimization is achieved, enabling self-organized coordination with effective convergence, divergence, and collision avoidance. Our method is fully interpretable and ready for deployment without gaps. Comprehensive simulations and real-world experiments demonstrate reduced path redundancy, robust performance across various tasks, and minimal demands on computation and communication.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hierarchical Gradient Tracking Algorithm for Mitigating Subnet-Drift in Fog Learning Networks</title>
<link>https://arxiv.org/abs/2409.17430</link>
<guid>https://arxiv.org/abs/2409.17430</guid>
<content:encoded><![CDATA[
arXiv:2409.17430v2 Announce Type: replace 
Abstract: Federated learning (FL) encounters scalability challenges when implemented over fog networks that do not follow FL's conventional star topology architecture. Semi-decentralized FL (SD-FL) has proposed a solution for device-to-device (D2D) enabled networks that divides model cooperation into two stages: at the lower stage, D2D communications is employed for local model aggregations within subnetworks (subnets), while the upper stage handles device-server (DS) communications for global model aggregations. However, existing SD-FL schemes are based on gradient diversity assumptions that become performance bottlenecks as data distributions become more heterogeneous. In this work, we develop semi-decentralized gradient tracking (SD-GT), the first SD-FL methodology that removes the need for such assumptions by incorporating tracking terms into device updates for each communication layer. Our analytical characterization of SD-GT reveals upper bounds on convergence for non-convex, convex, and strongly-convex problems. We show how the bounds enable the development of an optimization algorithm that navigates the performance-efficiency trade-off by tuning subnet sampling rate and D2D rounds for each global training interval. Our subsequent numerical evaluations demonstrate that SD-GT obtains substantial improvements in trained model quality and communication cost relative to baselines in SD-FL and gradient tracking on several datasets.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedSPD: A Soft-clustering Approach for Personalized Decentralized Federated Learning</title>
<link>https://arxiv.org/abs/2410.18862</link>
<guid>https://arxiv.org/abs/2410.18862</guid>
<content:encoded><![CDATA[
arXiv:2410.18862v2 Announce Type: replace 
Abstract: Federated learning has recently gained popularity as a framework for distributed clients to collaboratively train a machine learning model using local data. While traditional federated learning relies on a central server for model aggregation, recent advancements adopt a decentralized framework, enabling direct model exchange between clients and eliminating the single point of failure. However, existing decentralized frameworks often assume all clients train a shared model. Personalizing each client's model can enhance performance, especially with heterogeneous client data distributions. We propose FedSPD, an efficient personalized federated learning algorithm for the decentralized setting, and show that it learns accurate models even in low-connectivity networks. To provide theoretical guarantees on convergence, we introduce a clustering-based framework that enables consensus on models for distinct data clusters while personalizing to unique mixtures of these clusters at different clients. This flexibility, allowing selective model updates based on data distribution, substantially reduces communication costs compared to prior work on personalized federated learning in decentralized settings. Experimental results on real-world datasets show that FedSPD outperforms multiple decentralized variants of personalized federated learning algorithms, especially in scenarios with low-connectivity networks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interaction-Aware Vulnerability Detection in Smart Contract Bytecodes</title>
<link>https://arxiv.org/abs/2410.20712</link>
<guid>https://arxiv.org/abs/2410.20712</guid>
<content:encoded><![CDATA[
arXiv:2410.20712v2 Announce Type: replace 
Abstract: The detection of vulnerabilities in smart contracts remains a significant challenge. While numerous tools are available for analyzing smart contracts in source code, only about 1.79% of smart contracts on Ethereum are open-source. For existing tools that target bytecodes, most of them only consider the semantic logic context and disregard function interface information in the bytecodes. In this paper, we propose COBRA, a novel framework that integrates semantic context and function interfaces to detect vulnerabilities in bytecodes of the smart contract. To our best knowledge, COBRA is the first framework that combines these two features. Moreover, to infer the function signatures that are not present in signature databases, we propose SRIF, automatically learn the rules of function signatures from the smart contract bytecodes. The bytecodes associated with the function signatures are collected by constructing a control flow graph (CFG) for the SRIF training. We optimize the semantic context using the operation code in the static single assignment (SSA) format. Finally, we integrate the context and function interface representations in the latent space as the contract feature embedding. The contract features in the hidden space are decoded for vulnerability classifications with a decoder and attention module. Experimental results demonstrate that SRIF can achieve 94.76% F1-score for function signature inference. Furthermore, when the ground truth ABI exists, COBRA achieves 93.45% F1-score for vulnerability classification. In the absence of ABI, the inferred function feature fills the encoder, and the system accomplishes an 89.46% recall rate.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Trust-Centric Approach To Quantifying Maturity and Security in Internet Voting Protocols</title>
<link>https://arxiv.org/abs/2412.10611</link>
<guid>https://arxiv.org/abs/2412.10611</guid>
<content:encoded><![CDATA[
arXiv:2412.10611v2 Announce Type: replace 
Abstract: Voting is a cornerstone of collective participatory decision-making in contexts ranging from political elections to decentralized autonomous organizations (DAOs). Despite the proliferation of internet voting protocols promising enhanced accessibility and efficiency, their evaluation and comparison are complicated by a lack of standardized criteria and unified definitions of security and maturity. Furthermore, socio-technical requirements by decision makers are not structurally taken into consideration when comparing internet voting systems. This paper addresses this gap by introducing a trust-centric maturity scoring framework to quantify the security and maturity of seventeen internet voting systems. A comprehensive trust model analysis is conducted for selected internet voting protocols, examining their security properties, trust assumptions, technical complexity, and practical usability. In this paper we propose the Internet Voting Maturity Framework (IVMF) which supports nuanced assessment that reflects real-world deployment concerns and aids decision-makers in selecting appropriate systems tailored to their specific use-case requirements. The framework is general enough to be applied to other systems, where the aspects of decentralization, trust, and security are crucial, such as digital identity, Ethereum layer-two scaling solutions, and federated data infrastructures. Its objective is to provide an extendable toolkit for policy makers and technology experts alike that normalizes technical and non-technical requirements on a univariate scale.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenTorrent: Scaling Large Language Model Serving with An Overlay Network</title>
<link>https://arxiv.org/abs/2504.20101</link>
<guid>https://arxiv.org/abs/2504.20101</guid>
<content:encoded><![CDATA[
arXiv:2504.20101v3 Announce Type: replace 
Abstract: While significant progress has been made in research and development on open-source and cost-efficient large-language models (LLMs), serving scalability remains a critical challenge, particularly for small organizations and individuals seeking to deploy and test their LLM innovations. Inspired by peer-to-peer networks that leverage decentralized overlay nodes to increase throughput and availability, we propose GenTorrent, an LLM serving overlay that harnesses computing resources from decentralized contributors. We identify four key research problems inherent to enabling such a decentralized infrastructure: 1) overlay network organization; 2) LLM communication privacy; 3) overlay forwarding for resource efficiency; and 4) verification of serving quality. This work presents the first systematic study of these fundamental problems in the context of decentralized LLM serving. Evaluation results from a prototype implemented on a set of decentralized nodes demonstrate that GenTorrent achieves a latency reduction of over 50% compared to the baseline design without overlay forwarding. Furthermore, the security features introduce minimal overhead to serving latency and throughput. We believe this work pioneers a new direction for democratizing and scaling future AI serving capabilities.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graded Transformers</title>
<link>https://arxiv.org/abs/2507.20108</link>
<guid>https://arxiv.org/abs/2507.20108</guid>
<content:encoded><![CDATA[
arXiv:2507.20108v2 Announce Type: replace 
Abstract: We introduce the Graded Transformer framework, a new class of sequence models that embeds algebraic inductive biases through grading transformations on vector spaces. Extending Graded Neural Networks (GNNs), we propose two architectures: the Linearly Graded Transformer (LGT) and the Exponentially Graded Transformer (EGT). These models apply parameterized scaling operators, governed by fixed or learnable grading tuples and in the case of EGT exponential factors, to encode hierarchical structure in attention and representation layers and to improve efficiency for structured data.
  We establish rigorous guarantees, including universal approximation theorems for continuous and Sobolev functions, reduced sample complexity via effective VC dimension bounds, Lipschitz continuity of graded operations, and robustness to perturbations. A graded loss ensures gradient stability and alignment with domain priors during optimization. By treating grades as differentiable parameters, the framework enables adaptive feature prioritization, overcoming limitations of fixed grades in earlier models.
  The Graded Transformer provides a mathematically principled approach to hierarchical learning and neuro-symbolic reasoning. Applications include algebraic geometry (moduli spaces and zeta functions), physics (multiscale systems), natural language processing (syntactic parsing), biological sequence analysis (variant prediction), robotics and autonomous systems (safety-critical prioritization), the automotive industry (certifiable AI for ADAS), and blockchain and financial cryptography (secure coding and structured prediction).
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Federated Averaging via Random Walk</title>
<link>https://arxiv.org/abs/2508.21286</link>
<guid>https://arxiv.org/abs/2508.21286</guid>
<content:encoded><![CDATA[
arXiv:2508.21286v1 Announce Type: new 
Abstract: Federated Learning (FL) is a communication-efficient distributed machine learning method that allows multiple devices to collaboratively train models without sharing raw data. FL can be categorized into centralized and decentralized paradigms. The centralized paradigm relies on a central server to aggregate local models, potentially resulting in single points of failure, communication bottlenecks, and exposure of model parameters. In contrast, the decentralized paradigm, which does not require a central server, provides improved robustness and privacy. The essence of federated learning lies in leveraging multiple local updates for efficient communication. However, this approach may result in slower convergence or even convergence to suboptimal models in the presence of heterogeneous and imbalanced data. To address this challenge, we study decentralized federated averaging via random walk (DFedRW), which replaces multiple local update steps on a single device with random walk updates. Traditional Federated Averaging (FedAvg) and its decentralized versions commonly ignore stragglers, which reduces the amount of training data and introduces sampling bias. Therefore, we allow DFedRW to aggregate partial random walk updates, ensuring that each computation contributes to the model update. To further improve communication efficiency, we also propose a quantized version of DFedRW. We demonstrate that (quantized) DFedRW achieves convergence upper bound of order $\mathcal{O}(\frac{1}{k^{1-q}})$ under convex conditions. Furthermore, we propose a sufficient condition that reveals when quantization balances communication and convergence. Numerical analysis indicates that our proposed algorithms outperform (decentralized) FedAvg in both convergence rate and accuracy, achieving a 38.3\% and 37.5\% increase in test accuracy under high levels of heterogeneities.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Tells All: Deanonymization of Blockchain RPC Users with Zero Transaction Fee (Extended Version)</title>
<link>https://arxiv.org/abs/2508.21440</link>
<guid>https://arxiv.org/abs/2508.21440</guid>
<content:encoded><![CDATA[
arXiv:2508.21440v1 Announce Type: new 
Abstract: Remote Procedure Call (RPC) services have become a primary gateway for users to access public blockchains. While they offer significant convenience, RPC services also introduce critical privacy challenges that remain insufficiently examined. Existing deanonymization attacks either do not apply to blockchain RPC users or incur costs like transaction fees assuming an active network eavesdropper. In this paper, we propose a novel deanonymization attack that can link an IP address of a RPC user to this user's blockchain pseudonym. Our analysis reveals a temporal correlation between the timestamps of transaction confirmations recorded on the public ledger and those of TCP packets sent by the victim when querying transaction status. We assume a strong passive adversary with access to network infrastructure, capable of monitoring traffic at network border routers or Internet exchange points. By monitoring network traffic and analyzing public ledgers, the attacker can link the IP address of the TCP packet to the pseudonym of the transaction initiator by exploiting the temporal correlation. This deanonymization attack incurs zero transaction fee. We mathematically model and analyze the attack method, perform large-scale measurements of blockchain ledgers, and conduct real-world attacks to validate the attack. Our attack achieves a high success rate of over 95% against normal RPC users on various blockchain networks, including Ethereum, Bitcoin and Solana.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Fine-tuning of SAM-Med3D for MRI-based Dementia Classification</title>
<link>https://arxiv.org/abs/2508.21458</link>
<guid>https://arxiv.org/abs/2508.21458</guid>
<content:encoded><![CDATA[
arXiv:2508.21458v1 Announce Type: new 
Abstract: While foundation models (FMs) offer strong potential for AI-based dementia diagnosis, their integration into federated learning (FL) systems remains underexplored. In this benchmarking study, we systematically evaluate the impact of key design choices: classification head architecture, fine-tuning strategy, and aggregation method, on the performance and efficiency of federated FM tuning using brain MRI data. Using a large multi-cohort dataset, we find that the architecture of the classification head substantially influences performance, freezing the FM encoder achieves comparable results to full fine-tuning, and advanced aggregation methods outperform standard federated averaging. Our results offer practical insights for deploying FMs in decentralized clinical settings and highlight trade-offs that should guide future method development.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unpacking Maximum Extractable Value on Polygon: A Study on Atomic Arbitrage</title>
<link>https://arxiv.org/abs/2508.21473</link>
<guid>https://arxiv.org/abs/2508.21473</guid>
<content:encoded><![CDATA[
arXiv:2508.21473v1 Announce Type: new 
Abstract: The evolution of blockchain technology, from its origins as a decentralized ledger for cryptocurrencies to its broader applications in areas like decentralized finance (DeFi), has significantly transformed financial ecosystems while introducing new challenges such as Maximum Extractable Value (MEV). This paper explores MEV on the Polygon blockchain, with a particular focus on Atomic Arbitrage (AA) transactions. We establish criteria for identifying AA transactions and analyze key factors such as searcher behavior, bidding dynamics, and token usage. Utilizing a dataset spanning 22 months and covering 23 million blocks, we examine MEV dynamics with a focus on Spam-based and Auction-based backrunning strategies. Our findings reveal that while Spam-based transactions are more prevalent, Auction-based transactions demonstrate greater profitability. Through detailed examples and analysis, we investigate the interactions between network architecture, transaction sequencing, and MEV extraction, offering comprehensive insights into the evolution and challenges of MEV in decentralized ecosystems. These results emphasize the need for robust transaction ordering mechanisms and highlight the implications of emerging MEV strategies for blockchain networks.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Decentralized IoT Onboarding for Smart Homes Using Consortium Blockchain</title>
<link>https://arxiv.org/abs/2508.21480</link>
<guid>https://arxiv.org/abs/2508.21480</guid>
<content:encoded><![CDATA[
arXiv:2508.21480v1 Announce Type: new 
Abstract: The increasing adoption of smart home devices and IoT-based security systems presents significant opportunities to enhance convenience, safety, and risk management for homeowners and service providers. However, secure onboarding-provisioning credentials and establishing trust with cloud platforms-remains a considerable challenge. Traditional onboarding methods often rely on centralized Public Key Infrastructure (PKI) models and manufacturer-controlled keys, which introduce security risks and limit the user's digital sovereignty. These limitations hinder the widespread deployment of scalable IoT solutions. This paper presents a novel onboarding framework that builds upon existing network-layer onboarding techniques and extends them to the application layer to address these challenges. By integrating consortium blockchain technology, we propose a decentralized onboarding mechanism that enhances transparency, security, and monitoring for smart home architectures. The architecture supports device registration, key revocation, access control management, and risk detection through event-driven alerts across dedicated blockchain channels and smart contracts. To evaluate the framework, we formally model the protocol using the Tamarin Prover under the Dolev-Yao adversary model. The analysis focuses on authentication, token integrity, key confidentiality, and resilience over public channels. A prototype implementation demonstrates the system's viability in smart home settings, with verification completing in 0.34 seconds, highlighting its scalability and suitability for constrained devices and diverse stakeholders. Additionally, performance evaluation shows that the blockchain-based approach effectively handles varying workloads, maintains high throughput and low latency, and supports near real-time IoT data processing.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Stream Pipeline Framework for Digital Payment Programming based on Smart Contracts</title>
<link>https://arxiv.org/abs/2508.21075</link>
<guid>https://arxiv.org/abs/2508.21075</guid>
<content:encoded><![CDATA[
arXiv:2508.21075v1 Announce Type: cross 
Abstract: Digital payments play a pivotal role in the burgeoning digital economy. Moving forward, the enhancement of digital payment systems necessitates programmability, going beyond just efficiency and convenience, to meet the evolving needs and complexities. Smart contract platforms like Central Bank Digital Currency (CBDC) networks and blockchains support programmable digital payments. However, the prevailing paradigm of programming payment logics involves coding smart contracts with programming languages, leading to high costs and significant security challenges. A novel and versatile method for payment programming on DLTs was presented in this paper - transforming digital currencies into token streams, then pipelining smart contracts to authorize, aggregate, lock, direct, and dispatch these streams efficiently from source to target accounts. By utilizing a small set of configurable templates, a few specialized smart contracts could be generated, and support most of payment logics through configuring and composing them. This approach could substantially reduce the cost of payment programming and enhance security, self-enforcement, adaptability, and controllability, thus hold the potential to become an essential component in the infrastructure of digital economy.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EconAgentic in DePIN Markets: A Large Language Model Approach to the Sharing Economy of Decentralized Physical Infrastructure</title>
<link>https://arxiv.org/abs/2508.21368</link>
<guid>https://arxiv.org/abs/2508.21368</guid>
<content:encoded><![CDATA[
arXiv:2508.21368v1 Announce Type: cross 
Abstract: The Decentralized Physical Infrastructure (DePIN) market is revolutionizing the sharing economy through token-based economics and smart contracts that govern decentralized operations. By 2024, DePIN projects have exceeded \$10 billion in market capitalization, underscoring their rapid growth. However, the unregulated nature of these markets, coupled with the autonomous deployment of AI agents in smart contracts, introduces risks such as inefficiencies and potential misalignment with human values. To address these concerns, we introduce EconAgentic, a Large Language Model (LLM)-powered framework designed to mitigate these challenges. Our research focuses on three key areas: 1) modeling the dynamic evolution of DePIN markets, 2) evaluating stakeholders' actions and their economic impacts, and 3) analyzing macroeconomic indicators to align market outcomes with societal goals. Through EconAgentic, we simulate how AI agents respond to token incentives, invest in infrastructure, and adapt to market conditions, comparing AI-driven decisions with human heuristic benchmarks. Our results show that EconAgentic provides valuable insights into the efficiency, inclusion, and stability of DePIN markets, contributing to both academic understanding and practical improvements in the design and governance of decentralized, tokenized economies.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mobile Apps Prioritizing Privacy, Efficiency and Equity: A Decentralized Approach to COVID-19 Vaccination Coordination</title>
<link>https://arxiv.org/abs/2102.09372</link>
<guid>https://arxiv.org/abs/2102.09372</guid>
<content:encoded><![CDATA[
arXiv:2102.09372v2 Announce Type: replace 
Abstract: In this early draft, we describe a decentralized, app-based approach to COVID-19 vaccine distribution that facilitates zero knowledge verification, dynamic vaccine scheduling, continuous symptoms reporting, access to aggregate analytics based on population trends and more. To ensure equity, our solution is developed to work with limited internet access as well. In addition, we describe the six critical functions that we believe last mile vaccination management platforms must perform, examine existing vaccine management systems, and present a model for privacy-focused, individual-centric solutions.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Domain Generalization with Style Sharing: Formal Model and Convergence Analysis</title>
<link>https://arxiv.org/abs/2504.06235</link>
<guid>https://arxiv.org/abs/2504.06235</guid>
<content:encoded><![CDATA[
arXiv:2504.06235v3 Announce Type: replace 
Abstract: Much of federated learning (FL) focuses on settings where local dataset statistics remain the same between training and testing. However, this assumption often does not hold in practice due to distribution shifts, motivating the development of domain generalization (DG) approaches that leverage source domain data to train models capable of generalizing to unseen target domains. In this paper, we are motivated by two major gaps in existing work on FL and DG: (1) the lack of formal mathematical analysis of DG objectives; and (2) DG research in FL being limited to the star-topology architecture. We develop Decentralized Federated Domain Generalization with Style Sharing ($\textit{StyleDDG}$), a decentralized DG algorithm which allows devices in a peer-to-peer network to achieve DG based on sharing style information inferred from their datasets. Additionally, we provide the first systematic approach to analyzing style-based DG training in decentralized networks. We cast existing centralized DG algorithms within our framework, and employ their formalisms to model $\textit{StyleDDG}$. We then obtain analytical conditions under which convergence of $\textit{StyleDDG}$ can be guaranteed. Through experiments on popular DG datasets, we demonstrate that $\textit{StyleDDG}$ can obtain significant improvements in accuracy across target domains with minimal communication overhead compared to baseline decentralized gradient methods.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Evolution of Intelligent Agents in Large-Scale Microservice Systems</title>
<link>https://arxiv.org/abs/2508.20508</link>
<guid>https://arxiv.org/abs/2508.20508</guid>
<content:encoded><![CDATA[
arXiv:2508.20508v1 Announce Type: new 
Abstract: This paper proposes an intelligent service optimization method based on a multi-agent collaborative evolution mechanism to address governance challenges in large-scale microservice architectures. These challenges include complex service dependencies, dynamic topology structures, and fluctuating workloads. The method models each service as an agent and introduces graph representation learning to construct a service dependency graph. This enables agents to perceive and embed structural changes within the system. Each agent learns its policy based on a Markov Decision Process. A centralized training and decentralized execution framework is used to integrate local autonomy with global coordination. To enhance overall system performance and adaptability, a game-driven policy optimization mechanism is designed. Through a selection-mutation process, agent strategy distributions are dynamically adjusted. This supports adaptive collaboration and behavioral evolution among services. Under this mechanism, the system can quickly respond and achieve stable policy convergence when facing scenarios such as sudden workload spikes, topology reconfigurations, or resource conflicts. To evaluate the effectiveness of the proposed method, experiments are conducted on a representative microservice simulation platform. Comparative analyses are performed against several advanced approaches, focusing on coordination efficiency, adaptability, and policy convergence performance. Experimental results show that the proposed method outperforms others in several key metrics. It significantly improves governance efficiency and operational stability in large-scale microservice systems. The method demonstrates strong practical value and engineering feasibility.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BridgeShield: Enhancing Security for Cross-chain Bridge Applications via Heterogeneous Graph Mining</title>
<link>https://arxiv.org/abs/2508.20517</link>
<guid>https://arxiv.org/abs/2508.20517</guid>
<content:encoded><![CDATA[
arXiv:2508.20517v1 Announce Type: new 
Abstract: Cross-chain bridges play a vital role in enabling blockchain interoperability. However, due to the inherent design flaws and the enormous value they hold, they have become prime targets for hacker attacks. Existing detection methods show progress yet remain limited, as they mainly address single-chain behaviors and fail to capture cross-chain semantics. To address this gap, we leverage heterogeneous graph attention networks, which are well-suited for modeling multi-typed entities and relations, to capture the complex execution semantics of cross-chain behaviors. We propose BridgeShield, a detection framework that jointly models the source chain, off-chain coordination, and destination chain within a unified heterogeneous graph representation. BridgeShield incorporates intra-meta-path attention to learn fine-grained dependencies within cross-chain paths and inter-meta-path attention to highlight discriminative cross-chain patterns, thereby enabling precise identification of attack behaviors. Extensive experiments on 51 real-world cross-chain attack events demonstrate that BridgeShield achieves an average F1-score of 92.58%, representing a 24.39% improvement over state-of-the-art baselines. These results validate the effectiveness of BridgeShield as a practical solution for securing cross-chain bridges and enhancing the resilience of multi-chain ecosystems.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Oriented Edge-Assisted Cross-System Design for Real-Time Human-Robot Interaction in Industrial Metaverse</title>
<link>https://arxiv.org/abs/2508.20664</link>
<guid>https://arxiv.org/abs/2508.20664</guid>
<content:encoded><![CDATA[
arXiv:2508.20664v1 Announce Type: new 
Abstract: Real-time human-device interaction in industrial Metaverse faces challenges such as high computational load, limited bandwidth, and strict latency. This paper proposes a task-oriented edge-assisted cross-system framework using digital twins (DTs) to enable responsive interactions. By predicting operator motions, the system supports: 1) proactive Metaverse rendering for visual feedback, and 2) preemptive control of remote devices. The DTs are decoupled into two virtual functions-visual display and robotic control-optimizing both performance and adaptability. To enhance generalizability, we introduce the Human-In-The-Loop Model-Agnostic Meta-Learning (HITL-MAML) algorithm, which dynamically adjusts prediction horizons. Evaluation on two tasks demonstrates the framework's effectiveness: in a Trajectory-Based Drawing Control task, it reduces weighted RMSE from 0.0712 m to 0.0101 m; in a real-time 3D scene representation task for nuclear decommissioning, it achieves a PSNR of 22.11, SSIM of 0.8729, and LPIPS of 0.1298. These results show the framework's capability to ensure spatial precision and visual fidelity in real-time, high-risk industrial environments.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Composable Life: Speculation for Decentralized AI Life</title>
<link>https://arxiv.org/abs/2508.20668</link>
<guid>https://arxiv.org/abs/2508.20668</guid>
<content:encoded><![CDATA[
arXiv:2508.20668v1 Announce Type: new 
Abstract: "Composable Life" is a hybrid project blending design fiction, experiential virtual reality, and scientific research. Through a multi-perspective, cross-media approach to speculative design, it reshapes our understanding of the digital future from AI's perspective. The project explores the hypothetical first suicide of an on-chain artificial life, examining the complex symbiotic relationship between humans, AI, and blockchain technology.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Forkless Blockchain Databases</title>
<link>https://arxiv.org/abs/2508.20686</link>
<guid>https://arxiv.org/abs/2508.20686</guid>
<content:encoded><![CDATA[
arXiv:2508.20686v1 Announce Type: new 
Abstract: Operating nodes in an L1 blockchain remains costly despite recent advances in blockchain technology. One of the most resource-intensive components of a node is the blockchain database, also known as StateDB, that manages balances, nonce, code, and the persistent storage of accounts/smart contracts. Although the blockchain industry has transitioned from forking to forkless chains due to improved consensus protocols, forkless blockchains still rely on legacy forking databases that are suboptimal for their purposes. In this paper, we propose a forkless blockchain database, showing a 100x improvement in storage and a 10x improvement in throughput compared to the geth-based Fantom Blockchain client.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoCoL: A Communication Efficient Decentralized Collaborative Method for Multi-Robot Systems</title>
<link>https://arxiv.org/abs/2508.20898</link>
<guid>https://arxiv.org/abs/2508.20898</guid>
<content:encoded><![CDATA[
arXiv:2508.20898v1 Announce Type: new 
Abstract: Collaborative learning enhances the performance and adaptability of multi-robot systems in complex tasks but faces significant challenges due to high communication overhead and data heterogeneity inherent in multi-robot tasks. To this end, we propose CoCoL, a Communication efficient decentralized Collaborative Learning method tailored for multi-robot systems with heterogeneous local datasets. Leveraging a mirror descent framework, CoCoL achieves remarkable communication efficiency with approximate Newton-type updates by capturing the similarity between objective functions of robots, and reduces computational costs through inexact sub-problem solutions. Furthermore, the integration of a gradient tracking scheme ensures its robustness against data heterogeneity. Experimental results on three representative multi robot collaborative learning tasks show the superiority of the proposed CoCoL in significantly reducing both the number of communication rounds and total bandwidth consumption while maintaining state-of-the-art accuracy. These benefits are particularly evident in challenging scenarios involving non-IID (non-independent and identically distributed) data distribution, streaming data, and time-varying network topologies.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Review and Layered Framework for Privacy-by-Design in Self-Sovereign Identity Systems</title>
<link>https://arxiv.org/abs/2502.02520</link>
<guid>https://arxiv.org/abs/2502.02520</guid>
<content:encoded><![CDATA[
arXiv:2502.02520v2 Announce Type: replace 
Abstract: The use of Self-Sovereign Identity (SSI) systems for digital identity management is gaining traction and interest. Countries such as Bhutan have already implemented an SSI infrastructure to manage the identity of their citizens. The EU, thanks to the revised eIDAS regulation, is opening the door for SSI vendors to develop SSI systems for the planned EU digital identity wallet. These developments, which fall within the sovereign domain, raise questions about individual privacy. The design of SSI systems is complex, often characterized by a large number of components and architectural choices because the current SSI communities differ on how to create identifiers, how to build and present credentials, and even how to design a user wallet. SSI stacks developed by different organizations provide different privacy features for different privacy needs. This paper performs a systematic mapping and review of SSI components and technologies into a novel four-layer privacy framework to address the design complexity of SSI systems. Based on this review, we provide an accompanying Design Assistance Dashboard (DAD). The DAD shows the interdependencies between SSI components in different layers, and maps these components to different privacy requirements and considerations, even providing a simple privacy class for each component.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Distributed Key Generation</title>
<link>https://arxiv.org/abs/2502.20835</link>
<guid>https://arxiv.org/abs/2502.20835</guid>
<content:encoded><![CDATA[
arXiv:2502.20835v2 Announce Type: replace 
Abstract: Distributed Key Generation (DKG) is vital to threshold-based cryptographic protocols such as threshold signatures, secure multiparty computation, and i-voting. Yet, standard $(n,t)$-DKG requires a known set of $n$ participants and a fixed threshold $t$, making it impractical for public or decentralized settings where membership and availability can change.
  We introduce Federated Distributed Key Generation (FDKG), which relaxes these constraints by allowing each participant to select its own guardian set, with a local threshold to reconstruct that participant's partial key. FDKG generalizes DKG and draws inspiration from Federated Byzantine Agreement, enabling dynamic trust delegation with minimal message complexity (two rounds). The protocol's liveness can tolerate adversary that controls up to $k - t + 1$ nodes in every guardian set. The paper presents a detailed protocol, a formal description of liveness, privacy, and integrity properties, and a simulation-based evaluation showcasing the efficacy of FDKG in mitigating node unreliability.
  In a setting of 100 parties, a 50% participation rate, 80% retention, and 40 guardians, the distribution phase incurred a total message size of 332.7 kB ($O(n\,k)$), and reconstruction phase 416.56 kB ($O(n\,k)$. Groth16 client-side proving took about 5 s in the distribution phase and ranged from 0.619 s up to 29.619 s in the reconstruction phase.
  Our work advances distributed cryptography by enabling flexible trust models for dynamic networks, with applications ranging from ad-hoc collaboration to blockchain governance.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated nnU-Net for Privacy-Preserving Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2503.02549</link>
<guid>https://arxiv.org/abs/2503.02549</guid>
<content:encoded><![CDATA[
arXiv:2503.02549v2 Announce Type: replace 
Abstract: The nnU-Net framework has played a crucial role in medical image segmentation and has become the gold standard in multitudes of applications targeting different diseases, organs, and modalities. However, so far it has been used primarily in a centralized approach where the collected data is stored in the same location where nnU-Net is trained. This centralized approach has various limitations, such as potential leakage of sensitive patient information and violation of patient privacy. Federated learning has emerged as a key approach for training segmentation models in a decentralized manner, enabling collaborative development while prioritising patient privacy. In this paper, we propose FednnU-Net, a plug-and-play, federated learning extension of the nnU-Net framework. To this end, we contribute two federated methodologies to unlock decentralized training of nnU-Net, namely, Federated Fingerprint Extraction (FFE) and Asymmetric Federated Averaging (AsymFedAvg). We conduct a comprehensive set of experiments demonstrating high and consistent performance of our methods for breast, cardiac and fetal segmentation based on a multi-modal collection of 6 datasets representing samples from 18 different institutions. To democratize research as well as real-world deployments of decentralized training in clinical centres, we publicly share our framework at https://github.com/faildeny/FednnUNet .
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>L2RW+: A Comprehensive Benchmark Towards Privacy-Preserved Visible-Infrared Person Re-Identification</title>
<link>https://arxiv.org/abs/2503.12232</link>
<guid>https://arxiv.org/abs/2503.12232</guid>
<content:encoded><![CDATA[
arXiv:2503.12232v2 Announce Type: replace 
Abstract: Visible-infrared person re-identification (VI-ReID) is a challenging task that aims to match pedestrian images captured under varying lighting conditions, which has drawn intensive research attention and achieved promising results. However, existing methods adopt the centralized training, ignoring the potential privacy concerns as the data is distributed across multiple devices or entities in reality. In this paper, we propose L2RW+, a benchmark that brings VI-ReID closer to real-world applications. The core rationale behind L2RW+ is that incorporating decentralized training into VI-ReID can address privacy concerns in scenarios with limited data-sharing constrains. Specifically, we design protocols and corresponding algorithms for different privacy sensitivity levels. In our new benchmark, we simulate the training under real-world data conditions that: 1) data from each camera is completely isolated, or 2) different data entities (e.g., data controllers of a certain region) can selectively share the data. In this way, we simulate scenarios with strict privacy restrictions, which is closer to real-world conditions. Comprehensive experiments show the feasibility and potential of decentralized VI-ReID training at both image and video levels. In particular, with increasing data scales, the performance gap between decentralized and centralized training decreases, especially in video-level VI-ReID. In unseen domains, decentralized training even achieves performance comparable to SOTA centralized methods. This work offers a novel research entry for deploying VI-ReID into real-world scenarios and can benefit the community. Code is available at: https://github.com/Joey623/L2RW.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMM-based DEX on the XRP Ledger</title>
<link>https://arxiv.org/abs/2312.13749</link>
<guid>https://arxiv.org/abs/2312.13749</guid>
<content:encoded><![CDATA[
arXiv:2312.13749v5 Announce Type: replace 
Abstract: Automated Market Maker (AMM)-based Decentralized Exchanges (DEXs) are crucial in Decentralized Finance (DeFi), but Ethereum implementations suffer from high transaction costs and price synchronization challenges. To address these limitations, we compare the XRP Ledger (XRPL)-AMM-Decentralized Exchange (DEX), a protocol-level implementation, against a Generic AMM-based DEX (G-AMM-DEX) on Ethereum, akin to Uniswap's V2 AMM implementation, through agent-based simulations using real market data and multiple volatility scenarios generated via Geometric Brownian Motion (GBM). Results demonstrate that the XRPL-AMM-DEX achieves superior price synchronization, reduced slippage, and improved returns due to XRPL's lower fees and shorter block times, with benefits amplifying during market volatility. The integrated Continuous Auction Mechanism (CAM) further mitigates impermanent loss by redistributing arbitrage value to Liquidity Providers (LPs). To the best of our knowledge, this study represents the first comparative analysis between protocol-level and smart contract AMM-based DEX implementations and the first agent-based simulation validating theoretical auction mechanisms for AMM-based DEXs.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Formal Modeling and Verification of the Algorand Consensus Protocol in CADP</title>
<link>https://arxiv.org/abs/2508.19452</link>
<guid>https://arxiv.org/abs/2508.19452</guid>
<content:encoded><![CDATA[
arXiv:2508.19452v1 Announce Type: new 
Abstract: Algorand is a scalable and secure permissionless blockchain that achieves proof-of-stake consensus via cryptographic self-sortition and binary Byzantine agreement. In this paper, we present a process algebraic model of the Algorand consensus protocol with the aim of enabling rigorous formal verification. Our model captures the behavior of participants with respect to the structured alternation of consensus steps toward a committee-based agreement by means of a probabilistic process calculus. We validate the correctness of the protocol in the absence of adversaries and then extend our model to capture the influence of coordinated malicious nodes that can force the commit of an empty block instead of the proposed one. The adversarial scenario is analyzed by using an equivalence-checking-based noninterference framework that we have implemented in the CADP verification toolkit. In addition to highlighting both the robustness and the limitations of the Algorand protocol under adversarial assumptions, this work illustrates the added value of using formal methods for the analysis of blockchain consensus algorithms.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Divide, Weight, and Route: Difficulty-Aware Optimization with Dynamic Expert Fusion for Long-tailed Recognition</title>
<link>https://arxiv.org/abs/2508.19630</link>
<guid>https://arxiv.org/abs/2508.19630</guid>
<content:encoded><![CDATA[
arXiv:2508.19630v1 Announce Type: new 
Abstract: Long-tailed visual recognition is challenging not only due to class imbalance but also because of varying classification difficulty across categories. Simply reweighting classes by frequency often overlooks those that are intrinsically hard to learn. To address this, we propose \textbf{DQRoute}, a modular framework that combines difficulty-aware optimization with dynamic expert collaboration. DQRoute first estimates class-wise difficulty based on prediction uncertainty and historical performance, and uses this signal to guide training with adaptive loss weighting. On the architectural side, DQRoute employs a mixture-of-experts design, where each expert specializes in a different region of the class distribution. At inference time, expert predictions are weighted by confidence scores derived from expert-specific OOD detectors, enabling input-adaptive routing without the need for a centralized router. All components are trained jointly in an end-to-end manner. Experiments on standard long-tailed benchmarks demonstrate that DQRoute significantly improves performance, particularly on rare and difficult classes, highlighting the benefit of integrating difficulty modeling with decentralized expert routing.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Secure Multi-LLM Agentic AI and Agentification for Edge General Intelligence by Zero-Trust: A Survey</title>
<link>https://arxiv.org/abs/2508.19870</link>
<guid>https://arxiv.org/abs/2508.19870</guid>
<content:encoded><![CDATA[
arXiv:2508.19870v1 Announce Type: new 
Abstract: Agentification serves as a critical enabler of Edge General Intelligence (EGI), transforming massive edge devices into cognitive agents through integrating Large Language Models (LLMs) and perception, reasoning, and acting modules. These agents collaborate across heterogeneous edge infrastructures, forming multi-LLM agentic AI systems that leverage collective intelligence and specialized capabilities to tackle complex, multi-step tasks. However, the collaborative nature of multi-LLM systems introduces critical security vulnerabilities, including insecure inter-LLM communications, expanded attack surfaces, and cross-domain data leakage that traditional perimeter-based security cannot adequately address. To this end, this survey introduces zero-trust security of multi-LLM in EGI, a paradigmatic shift following the ``never trust, always verify'' principle. We begin by systematically analyzing the security risks in multi-LLM systems within EGI contexts. Subsequently, we present the vision of a zero-trust multi-LLM framework in EGI. We then survey key technical progress to facilitate zero-trust multi-LLM systems in EGI. Particularly, we categorize zero-trust security mechanisms into model- and system-level approaches. The former and latter include strong identification, context-aware access control, etc., and proactive maintenance, blockchain-based management, etc., respectively. Finally, we identify critical research directions. This survey serves as the first systematic treatment of zero-trust applied to multi-LLM systems, providing both theoretical foundations and practical strategies.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence</title>
<link>https://arxiv.org/abs/2508.20019</link>
<guid>https://arxiv.org/abs/2508.20019</guid>
<content:encoded><![CDATA[
arXiv:2508.20019v1 Announce Type: new 
Abstract: Most existing Large Language Model (LLM)-based agent frameworks rely on centralized orchestration, incurring high deployment costs, rigid communication topologies, and limited adaptability. To address these challenges, we introduce Symphony, a decentralized multi-agent system which enables lightweight LLMs on consumer-grade GPUs to coordinate. Symphony introduces three key mechanisms: (1) a decentralized ledger that records capabilities, (2) a Beacon-selection protocol for dynamic task allocation, and (3) weighted result voting based on CoTs. This design forms a privacy-saving, scalable, and fault-tolerant orchestration with low overhead. Empirically, Symphony outperforms existing baselines on reasoning benchmarks, achieving substantial accuracy gains and demonstrating robustness across models of varying capacities.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smart Contract Intent Detection with Pre-trained Programming Language Model</title>
<link>https://arxiv.org/abs/2508.20086</link>
<guid>https://arxiv.org/abs/2508.20086</guid>
<content:encoded><![CDATA[
arXiv:2508.20086v1 Announce Type: new 
Abstract: Malicious intent in smart contract development can lead to substantial economic losses. SmartIntentNN is a deep learning model specifically designed to identify unsafe intents in smart contracts. This model integrates the Universal Sentence Encoder, a K-means clustering-based intent highlighting mechanism, and a Bidirectional Long Short-Term Memory network for multi-label classification, achieving an F1 of 0.8633 in distinguishing ten different intent categories. In this study, we present an upgraded version of this model, SmartIntentNN2 (Smart Contract Intent Neural Network V2). A significant enhancement in V2 is the incorporation of a BERT-based pre-trained language model, which has been trained on a dataset of 16,000 real smart contracts using a Masked Language Modeling objective. SmartIntentNN2 retains the BiLSTM-based multi-label classification network. With an improved F1 of 0.927, V2 demonstrates enhanced performance compared to its predecessor, establishing itself as the state-of-the-art model for smart contract intent detection.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DVM-SLAM: Decentralized Visual Monocular Simultaneous Localization and Mapping for Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2503.04126</link>
<guid>https://arxiv.org/abs/2503.04126</guid>
<content:encoded><![CDATA[
arXiv:2503.04126v2 Announce Type: replace 
Abstract: Cooperative Simultaneous Localization and Mapping (C-SLAM) enables multiple agents to work together in mapping unknown environments while simultaneously estimating their own positions. This approach enhances robustness, scalability, and accuracy by sharing information between agents, reducing drift, and enabling collective exploration of larger areas. In this paper, we present Decentralized Visual Monocular SLAM (DVM-SLAM), the first open-source decentralized monocular C-SLAM system. By only utilizing low-cost and light-weight monocular vision sensors, our system is well suited for small robots and micro aerial vehicles (MAVs). DVM-SLAM's real-world applicability is validated on physical robots with a custom collision avoidance framework, showcasing its potential in real-time multi-agent autonomous navigation scenarios. We also demonstrate comparable accuracy to state-of-the-art centralized monocular C-SLAM systems. We open-source our code and provide supplementary material online.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoboComm: A DID-based scalable and privacy-preserving Robot-to-Robot interaction over state channels</title>
<link>https://arxiv.org/abs/2504.09517</link>
<guid>https://arxiv.org/abs/2504.09517</guid>
<content:encoded><![CDATA[
arXiv:2504.09517v3 Announce Type: replace 
Abstract: In a multi robot system establishing trust amongst untrusted robots from different organisations while preserving a robot's privacy is a challenge. Recently decentralized technologies such as smart contract and blockchain are being explored for applications in robotics. However, the limited transaction processing and high maintenance cost hinder the widespread adoption of such approaches. Moreover, blockchain transactions be they on public or private permissioned blockchain are publically readable which further fails to preserve the confidentiality of the robot's data and privacy of the robot. In this work, we propose RoboComm a Decentralized Identity based approach for privacy-preserving interaction between robots. With DID a component of Self-Sovereign Identity; robots can authenticate each other independently without relying on any third-party service. Verifiable Credentials enable private data associated with a robot to be stored within the robot's hardware, unlike existing blockchain based approaches where the data has to be on the blockchain. We improve throughput by allowing message exchange over state channels. Being a blockchain backed solution RoboComm provides a trustworthy system without relying on a single party. Moreover, we implement our proposed approach to demonstrate the feasibility of our solution.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Model Privacy in Federated Learning with Random Masking and Quantization</title>
<link>https://arxiv.org/abs/2508.18911</link>
<guid>https://arxiv.org/abs/2508.18911</guid>
<content:encoded><![CDATA[
arXiv:2508.18911v2 Announce Type: new 
Abstract: The primary goal of traditional federated learning is to protect data privacy by enabling distributed edge devices to collaboratively train a shared global model while keeping raw data decentralized at local clients. The rise of large language models (LLMs) has introduced new challenges in distributed systems, as their substantial computational requirements and the need for specialized expertise raise critical concerns about protecting intellectual property (IP). This highlights the need for a federated learning approach that can safeguard both sensitive data and proprietary models. To tackle this challenge, we propose FedQSN, a federated learning approach that leverages random masking to obscure a subnetwork of model parameters and applies quantization to the remaining parameters. Consequently, the server transmits only a privacy-preserving proxy of the global model to clients during each communication round, thus enhancing the model's confidentiality. Experimental results across various models and tasks demonstrate that our approach not only maintains strong model performance in federated learning settings but also achieves enhanced protection of model parameters compared to baseline methods.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Federated Learning Framework for Risk-Based Adaptive Authentication</title>
<link>https://arxiv.org/abs/2508.18453</link>
<guid>https://arxiv.org/abs/2508.18453</guid>
<content:encoded><![CDATA[
arXiv:2508.18453v1 Announce Type: new 
Abstract: Balancing robust security with strong privacy guarantees is critical for Risk-Based Adaptive Authentication (RBA), particularly in decentralized settings. Federated Learning (FL) offers a promising solution by enabling collaborative risk assessment without centralizing user data. However, existing FL approaches struggle with Non-Independent and Identically Distributed (Non-IID) user features, resulting in biased, unstable, and poorly generalized global models. This paper introduces FL-RBA2, a novel Federated Learning framework for Risk-Based Adaptive Authentication that addresses Non-IID challenges through a mathematically grounded similarity transformation. By converting heterogeneous user features (including behavioral, biometric, contextual, interaction-based, and knowledge-based modalities) into IID similarity vectors, FL-RBA2 supports unbiased aggregation and personalized risk modeling across distributed clients. The framework mitigates cold-start limitations via clustering-based risk labeling, incorporates Differential Privacy (DP) to safeguard sensitive information, and employs Message Authentication Codes (MACs) to ensure model integrity and authenticity. Federated updates are securely aggregated into a global model, achieving strong balance between user privacy, scalability, and adaptive authentication robustness. Rigorous game-based security proofs in the Random Oracle Model formally establish privacy, correctness, and adaptive security guarantees. Extensive experiments on keystroke, mouse, and contextual datasets validate FL-RBA2's effectiveness in high-risk user detection and its resilience to model inversion and inference attacks, even under strong DP constraints.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Fairness Shaping with LLM-Guided Multi-Agent Reinforcement Learning for Peer-to-Peer Electricity Markets</title>
<link>https://arxiv.org/abs/2508.18610</link>
<guid>https://arxiv.org/abs/2508.18610</guid>
<content:encoded><![CDATA[
arXiv:2508.18610v1 Announce Type: new 
Abstract: Peer-to-peer (P2P) energy trading is becoming central to modern distribution systems as rooftop PV and home energy management systems become pervasive, yet most existing market and reinforcement learning designs emphasize efficiency or private profit and offer little real-time guidance to ensure equitable outcomes under uncertainty. To address this gap, a fairness-aware multiagent reinforcement learning framework, FairMarket-RL, is proposed in which a large language model (LLM) critic shapes bidding policies within a continuous double auction under partial observability and discrete price-quantity actions. After each trading slot, the LLM returns normalized fairness scores Fairness-to-Grid (FTG), Fairness-Between-Sellers (FBS), and Fairness-of-Pricing (FPP) that are integrated into the reward via ramped coefficients and tunable scaling, so that fairness guidance complements, rather than overwhelms, economic incentives. The environment models realistic residential load and PV profiles and enforce hard constraints on prices, physical feasibility, and policy-update stability. Across a progression of experiments from a small pilot to a larger simulated community and a mixed-asset real-world dataset, the framework shifts exchanges toward local P2P trades, lowers consumer costs relative to grid-only procurement, sustains strong fairness across participants, and preserves utility viability. Sensitivity analyses over solar availability and aggregate demand further indicate robust performance, suggesting a scalable, LLM-guided pathway to decentralized electricity markets that are economically efficient, socially equitable, and technically sound.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Edge General Intelligence with Agentic AI and Agentification: Concepts, Technologies, and Future Directions</title>
<link>https://arxiv.org/abs/2508.18725</link>
<guid>https://arxiv.org/abs/2508.18725</guid>
<content:encoded><![CDATA[
arXiv:2508.18725v1 Announce Type: new 
Abstract: The rapid expansion of sixth-generation (6G) wireless networks and the Internet of Things (IoT) has catalyzed the evolution from centralized cloud intelligence towards decentralized edge general intelligence. However, traditional edge intelligence methods, characterized by static models and limited cognitive autonomy, fail to address the dynamic, heterogeneous, and resource-constrained scenarios inherent to emerging edge networks. Agentic artificial intelligence (Agentic AI) emerges as a transformative solution, enabling edge systems to autonomously perceive multimodal environments, reason contextually, and adapt proactively through continuous perception-reasoning-action loops. In this context, the agentification of edge intelligence serves as a key paradigm shift, where distributed entities evolve into autonomous agents capable of collaboration and continual adaptation. This paper presents a comprehensive survey dedicated to Agentic AI and agentification frameworks tailored explicitly for edge general intelligence. First, we systematically introduce foundational concepts and clarify distinctions from traditional edge intelligence paradigms. Second, we analyze important enabling technologies, including compact model compression, energy-aware computing strategies, robust connectivity frameworks, and advanced knowledge representation and reasoning mechanisms. Third, we provide representative case studies demonstrating Agentic AI's capabilities in low-altitude economy networks, intent-driven networking, vehicular networks, and human-centric service provisioning, supported by numerical evaluations. Furthermore, we identify current research challenges, review emerging open-source platforms, and highlight promising future research directions to guide robust, scalable, and trustworthy Agentic AI deployments for next-generation edge environments.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLAegis: A Two-Layer Defense Framework for Federated Learning Against Poisoning Attacks</title>
<link>https://arxiv.org/abs/2508.18737</link>
<guid>https://arxiv.org/abs/2508.18737</guid>
<content:encoded><![CDATA[
arXiv:2508.18737v1 Announce Type: new 
Abstract: Federated Learning (FL) has become a powerful technique for training Machine Learning (ML) models in a decentralized manner, preserving the privacy of the training datasets involved. However, the decentralized nature of FL limits the visibility of the training process, relying heavily on the honesty of participating clients. This assumption opens the door to malicious third parties, known as Byzantine clients, which can poison the training process by submitting false model updates. Such malicious clients may engage in poisoning attacks, manipulating either the dataset or the model parameters to induce misclassification. In response, this study introduces FLAegis, a two-stage defensive framework designed to identify Byzantine clients and improve the robustness of FL systems. Our approach leverages symbolic time series transformation (SAX) to amplify the differences between benign and malicious models, and spectral clustering, which enables accurate detection of adversarial behavior. Furthermore, we incorporate a robust FFT-based aggregation function as a final layer to mitigate the impact of those Byzantine clients that manage to evade prior defenses. We rigorously evaluate our method against five poisoning attacks, ranging from simple label flipping to adaptive optimization-based strategies. Notably, our approach outperforms state-of-the-art defenses in both detection precision and final model accuracy, maintaining consistently high performance even under strong adversarial conditions.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Immutable Digital Recognition via Blockchain</title>
<link>https://arxiv.org/abs/2508.18750</link>
<guid>https://arxiv.org/abs/2508.18750</guid>
<content:encoded><![CDATA[
arXiv:2508.18750v1 Announce Type: new 
Abstract: The process integrates the decentralised management and centralised operation models, aligning them with the national policy directives. The developed solution enables the full utilisation of blockchain technology's advantages while also fostering community participation. Consequently, it establishes a secure, legal, reliable, and dynamic electronic certification system.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EnerSwap: Large-Scale, Privacy-First Automated Market Maker for V2G Energy Trading</title>
<link>https://arxiv.org/abs/2508.18942</link>
<guid>https://arxiv.org/abs/2508.18942</guid>
<content:encoded><![CDATA[
arXiv:2508.18942v1 Announce Type: new 
Abstract: With the rapid growth of Electric Vehicle (EV) technology, EVs are destined to shape the future of transportation. The large number of EVs facilitates the development of the emerging vehicle-to-grid (V2G) technology, which realizes bidirectional energy exchanges between EVs and the power grid. This has led to the setting up of electricity markets that are usually confined to a small geographical location, often with a small number of participants. Usually, these markets are manipulated by intermediaries responsible for collecting bids from prosumers, determining the market-clearing price, incorporating grid constraints, and accounting for network losses. While centralized models can be highly efficient, they grant excessive power to the intermediary by allowing them to gain exclusive access to prosumers \textquotesingle price preferences. This opens the door to potential market manipulation and raises significant privacy concerns for users, such as the location of energy providers. This lack of protection exposes users to potential risks, as untrustworthy servers and malicious adversaries can exploit this information to infer trading activities and real identities. This work proposes a secure, decentralized exchange market built on blockchain technology, utilizing a privacy-preserving Automated Market Maker (AMM) model to offer open and fair, and equal access to traders, and mitigates the most common trading-manipulation attacks. Additionally, it incorporates a scalable architecture based on geographical dynamic sharding, allowing for efficient resource allocation and improved performance as the market grows.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Collective Action with Multiple Collectives</title>
<link>https://arxiv.org/abs/2508.19149</link>
<guid>https://arxiv.org/abs/2508.19149</guid>
<content:encoded><![CDATA[
arXiv:2508.19149v1 Announce Type: new 
Abstract: As learning systems increasingly influence everyday decisions, user-side steering via Algorithmic Collective Action (ACA)-coordinated changes to shared data-offers a complement to regulator-side policy and firm-side model design. Although real-world actions have been traditionally decentralized and fragmented into multiple collectives despite sharing overarching objectives-with each collective differing in size, strategy, and actionable goals, most of the ACA literature focused on single collective settings. In this work, we present the first theoretical framework for ACA with multiple collectives acting on the same system. In particular, we focus on collective action in classification, studying how multiple collectives can plant signals, i.e., bias a classifier to learn an association between an altered version of the features and a chosen, possibly overlapping, set of target classes. We provide quantitative results about the role and the interplay of collectives' sizes and their alignment of goals. Our framework, by also complementing previous empirical results, opens a path for a holistic treatment of ACA with multiple collectives.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Lightweight Blockchain for Decentralized IoT</title>
<link>https://arxiv.org/abs/2508.19219</link>
<guid>https://arxiv.org/abs/2508.19219</guid>
<content:encoded><![CDATA[
arXiv:2508.19219v1 Announce Type: new 
Abstract: The Internet of Things (IoT) is applied in various fields, and the number of physical devices connected to the IoT is increasingly growing. There are significant challenges to the IoT's growth and development, mainly due to the centralized nature and large-scale IoT networks. The emphasis on the decentralization of IoT's architecture can overcome challenges to IoT's capabilities. A promising decentralized platform for IoT is blockchain. Owing to IoT devices' limited resources, traditional consensus algorithms such as PoW and PoS in the blockchain are computationally expensive. Therefore, the PoA consensus algorithm is proposed in the blockchain consensus network for IoT. The PoA selects the validator as Turn-based selection (TBS) that needs optimization and faces system reliability, energy consumption, latency, and low scalability. We propose an efficient, lightweight blockchain for decentralizing IoT architecture by using virtualization and clustering to increase productivity and scalability to address these issues. We also introduce a novel PoA based on the Weight-Based-Selection (WBS) method for validators to validate transactions and add them to the blockchain. By simulation, we evaluated the performance of our proposed WBS method as opposed to TBS. The results show reduced energy consumption, and response time, and increased throughput.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SkyTrust: Blockchain-Enhanced UAV Security for NTNs with Dynamic Trust and Energy-Aware Consensus</title>
<link>https://arxiv.org/abs/2508.18735</link>
<guid>https://arxiv.org/abs/2508.18735</guid>
<content:encoded><![CDATA[
arXiv:2508.18735v1 Announce Type: cross 
Abstract: Non-Terrestrial Networks (NTNs) based on Unmanned Aerial Vehicles (UAVs) as base stations are extremely susceptible to security attacks due to their distributed and dynamic nature, which makes them vulnerable to rogue nodes. In this paper, a new Dynamic Trust Score Adjustment Mechanism with Energy-Aware Consensus (DTSAM-EAC) is proposed to enhance security in UAV-based NTNs. The proposed framework integrates a permissioned Hyperledger Fabric blockchain with Federated Learning (FL) to support privacy-preserving trust evaluation. Trust ratings are updated continuously through weighted aggregation of past trust, present behavior, and energy contribution, thus making the system adaptive to changing network conditions. An energy-aware consensus mechanism prioritizes UAVs with greater available energy for block validation, ensuring efficient use of resources under resource-constrained environments. FL aggregation with trust-weighting further increases the resilience of the global trust model. Simulation results verify the designed framework achieves 94\% trust score prediction accuracy and 96\% rogue UAV detection rate while outperforming centralized and static baselines of trust-based solutions on privacy, energy efficiency, and reliability. It complies with 6G requirements in terms of distributed intelligence and sustainability and is an energy-efficient and scalable solution to secure NTNs.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blockchain Security Risk Assessment in Quantum Era, Migration Strategies and Proactive Defense</title>
<link>https://arxiv.org/abs/2501.11798</link>
<guid>https://arxiv.org/abs/2501.11798</guid>
<content:encoded><![CDATA[
arXiv:2501.11798v2 Announce Type: replace 
Abstract: The emergence of quantum computing presents a formidable challenge to the security of blockchain systems. Traditional cryptographic algorithms, foundational to digital signatures, message encryption, and hashing functions, become vulnerable to the immense computational power of quantum computers. This paper conducts a thorough risk assessment of transitioning to quantum-resistant blockchains, comprehensively analyzing potential threats targeting vital blockchain components: the network, mining pools, transaction verification mechanisms, smart contracts, and user wallets. By elucidating the intricate challenges and strategic considerations inherent in transitioning to quantum-resistant algorithms, the paper evaluates risks and highlights obstacles in securing blockchain components with quantum-resistant cryptography. It offers a hybrid migration strategy to facilitate a smooth transition from classical to quantum-resistant cryptography. The analysis extends to prominent blockchains such as Bitcoin, Ethereum, Ripple, Litecoin, and Zcash, assessing vulnerable components, potential impacts, and associated STRIDE threats, thereby identifying areas susceptible to quantum attacks. Beyond analysis, the paper provides actionable guidance for designing secure and resilient blockchain ecosystems in the quantum computing era. Recognizing the looming threat of quantum computers, this research advocates for a proactive transition to quantum-resistant blockchain networks. It proposes a tailored security blueprint that strategically fortifies each component against the evolving landscape of quantum-induced cyber threats. Emphasizing the critical need for blockchain stakeholders to adopt proactive measures and implement quantum-resistant solutions, the paper underscores the importance of embracing these insights to navigate the complexities of the quantum era with resilience and confidence.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>zkPHIRE: A Programmable Accelerator for ZKPs over HIgh-degRee, Expressive Gates</title>
<link>https://arxiv.org/abs/2508.16738</link>
<guid>https://arxiv.org/abs/2508.16738</guid>
<content:encoded><![CDATA[
arXiv:2508.16738v1 Announce Type: new 
Abstract: Zero-Knowledge Proofs (ZKPs) have emerged as powerful tools for secure and privacy-preserving computation. ZKPs enable one party to convince another of a statement's validity without revealing anything else. This capability has profound implications in many domains, including: machine learning, blockchain, image authentication, and electronic voting. Despite their potential, ZKPs have seen limited deployment because of their exceptionally high computational overhead, which manifests primarily during proof generation. To mitigate these overheads, a (growing) body of researchers has proposed hardware accelerators and GPU implementations for kernels and complete protocols. Prior art spans a wide variety of ZKP schemes that vary significantly in computational overhead, proof size, verifier cost, protocol setup, and trust. The latest, and widely used ZKP protocols are intentionally designed to balance these trade-offs. A particular challenge in modern ZKP systems is supporting complex, high-degree gates using the SumCheck protocol. We address this challenge with a novel programmable accelerator that efficiently handles arbitrary custom gates via SumCheck. Our accelerator achieves upwards of $1000\times$ geomean speedup over CPU-based SumChecks across a range of gate types. We integrate this unit into a full-system accelerator, zkPHIRE, which achieves $1486\times$ geomean speedup over CPU and $11.87\times$ speedup over the state-of-the-art at iso-area. zkPHIRE is the first accelerator to scale to problem sizes of $2^{30}$ nominal constraints while maintaining small proof sizes and programmability.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Decentralized Stability Condition for Power Systems Based on Projecting DW Shells</title>
<link>https://arxiv.org/abs/2508.17033</link>
<guid>https://arxiv.org/abs/2508.17033</guid>
<content:encoded><![CDATA[
arXiv:2508.17033v1 Announce Type: new 
Abstract: The development of decentralized stability conditions has gained considerable attention due to the need to analyze heterogeneous multi-converter power systems. A recent advance is the application of the small-phase theorem, which extends the passivity theory. However, it requires the transfer function matrix to be sectorial, which may not hold in some frequency range and will result in conservatism. This letter tackles this problem by leveraging the Davis-Wielandt (DW) shells for decentralized stability analysis. We develop a geometric decentralized stability condition that visually displays how heterogeneous converters interact with the power grid and enable modular system analysis.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-Quantum Blockchain: Challenges and Opportunities</title>
<link>https://arxiv.org/abs/2508.17071</link>
<guid>https://arxiv.org/abs/2508.17071</guid>
<content:encoded><![CDATA[
arXiv:2508.17071v1 Announce Type: new 
Abstract: Blockchain is a Distributed Ledger Technology (DLT) that offers numerous benefits including decentralization, transparency, efficiency, and reduced costs. Hence, blockchain has been included in many fields. Blockchain relies on cryptographic protocols (especially public-key cryptography and hash functions) to achieve many essential sub-routines. However, the increased progress of quantum computation and algorithms has threatened the security of many traditional cryptosystems. Therefore, this represents a serious risk for the existing blockchain technology. For example, SHA-256 and the Elliptic Curve Digital Signature Algorithm (ECDSA) cryptosystems can be compromised by Shor s and Grover s quantum algorithms in the foreseeable future. Post-Quantum Cryptography (PQC) is a basic solution for resisting these quantum attacks. Applying PQC to blockchains results in creating Post-Quantum Blockchains (PQB). Thus, this paper aims to review the threats imposed by quantum computers on classical blockchain technology and provide useful guidelines on PQB security to blockchain researchers. The paper focuses on the challenges and opportunities of future work direction in this field.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconciling Communication Compression and Byzantine-Robustness in Distributed Learning</title>
<link>https://arxiv.org/abs/2508.17129</link>
<guid>https://arxiv.org/abs/2508.17129</guid>
<content:encoded><![CDATA[
arXiv:2508.17129v1 Announce Type: new 
Abstract: Distributed learning (DL) enables scalable model training over decentralized data, but remains challenged by Byzantine faults and high communication costs. While both issues have been studied extensively in isolation, their interaction is less explored. Prior work shows that naively combining communication compression with Byzantine-robust aggregation degrades resilience to faulty nodes (or workers). The state-of-the-art algorithm, namely Byz-DASHA-PAGE [29], makes use of the momentum variance reduction scheme to mitigate the detrimental impact of compression noise on Byzantine-robustness. We propose a new algorithm, named RoSDHB, that integrates the classic Polyak's momentum with a new coordinated compression mechanism. We show that RoSDHB performs comparably to Byz-DASHA-PAGE under the standard (G, B)-gradient dissimilarity heterogeneity model, while it relies on fewer assumptions. In particular, we only assume Lipschitz smoothness of the average loss function of the honest workers, in contrast to [29]that additionally assumes a special smoothness of bounded global Hessian variance. Empirical results on benchmark image classification task show that RoSDHB achieves strong robustness with significant communication savings.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Literature Review of the Effect of Quantum Computing on Cryptocurrencies using Blockchain Technology</title>
<link>https://arxiv.org/abs/2508.17296</link>
<guid>https://arxiv.org/abs/2508.17296</guid>
<content:encoded><![CDATA[
arXiv:2508.17296v1 Announce Type: new 
Abstract: With the advent of quantum computing, cryptocurrencies that rely on blockchain technology face mounting cryptographic vulnerabilities. This paper presents a comprehensive literature review evaluating how quantum algorithms, specifically Shors and Grovers, could disrupt the foundational security mechanisms of cryptocurrencies. Shors algorithm poses a threat to public-key cryptographic schemes by enabling efficient factorization and discrete logarithm solving, thereby endangering digital signature systems. Grovers algorithm undermines hash-based functions, increasing the feasibility of fifty one percent attacks and hash collisions. By examining the internal mechanisms of major cryptocurrencies such as Bitcoin, Ethereum, Litecoin, Monero, and Zcash, this review identifies specific vulnerabilities in transaction and consensus processes. It further analyses the current hardware limitations of quantum systems and estimates when such attacks could become feasible. In anticipation, it investigates countermeasures including Post-Quantum Cryptography (PQC), Quantum Key Distribution (QKD), and protocol-level modifications such as memory-intensive proof-of-work algorithms and multi-signature schemes. The discussion integrates recent advancements in quantum error correction, hardware scalability, and NIST-standardized cryptographic algorithms. This review concludes that while quantum computers are not yet advanced enough to pose an immediate threat, proactive integration of quantum-resistant solutions is essential. The findings underscore the urgent need for cryptocurrencies to adopt post-quantum cryptographic standards to preserve the decentralized trust, integrity, and security that define blockchain-based digital cryptocurrencies.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaFed: Advancing Privacy, Performance, and Sustainability in Federated Metaverse Systems</title>
<link>https://arxiv.org/abs/2508.17341</link>
<guid>https://arxiv.org/abs/2508.17341</guid>
<content:encoded><![CDATA[
arXiv:2508.17341v1 Announce Type: new 
Abstract: The rapid expansion of immersive Metaverse applications introduces complex challenges at the intersection of performance, privacy, and environmental sustainability. Centralized architectures fall short in addressing these demands, often resulting in elevated energy consumption, latency, and privacy concerns. This paper proposes MetaFed, a decentralized federated learning (FL) framework that enables sustainable and intelligent resource orchestration for Metaverse environments. MetaFed integrates (i) multi-agent reinforcement learning for dynamic client selection, (ii) privacy-preserving FL using homomorphic encryption, and (iii) carbon-aware scheduling aligned with renewable energy availability. Evaluations on MNIST and CIFAR-10 using lightweight ResNet architectures demonstrate that MetaFed achieves up to 25\% reduction in carbon emissions compared to conventional approaches, while maintaining high accuracy and minimal communication overhead. These results highlight MetaFed as a scalable solution for building environmentally responsible and privacy-compliant Metaverse infrastructures.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Compiler Optimization Impacts on zkVM Performance</title>
<link>https://arxiv.org/abs/2508.17518</link>
<guid>https://arxiv.org/abs/2508.17518</guid>
<content:encoded><![CDATA[
arXiv:2508.17518v1 Announce Type: new 
Abstract: Zero-knowledge proofs (ZKPs) are the cornerstone of programmable cryptography. They enable (1) privacy-preserving and verifiable computation across blockchains, and (2) an expanding range of off-chain applications such as credential schemes. Zero-knowledge virtual machines (zkVMs) lower the barrier by turning ZKPs into a drop-in backend for standard compilation pipelines. This lets developers write proof-generating programs in conventional languages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits. However, these VMs inherit compiler infrastructures tuned for traditional architectures rather than for proof systems. In particular, standard compiler optimizations assume features that are absent in zkVMs, including cache locality, branch prediction, or instruction-level parallelism. Therefore, their impact on proof generation is questionable.
  We present the first systematic study of the impact of compiler optimizations on zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an unoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero and SP1). While standard LLVM optimization levels do improve zkVM performance (over 40\%), their impact is far smaller than on traditional CPUs, since their decisions rely on hardware features rather than proof constraints. Guided by a fine-grained pass-level analysis, we~\emph{slightly} refine a small set of LLVM passes to be zkVM-aware, improving zkVM execution time by up to 45\% (average +4.6\% on RISC Zero, +1\% on SP1) and achieving consistent proving-time gains. Our work highlights the potential of compiler-level optimizations for zkVM performance and opens new direction for zkVM-specific passes, backends, and superoptimizers.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group Expectation Policy Optimization for Stable Heterogeneous Reinforcement Learning in LLMs</title>
<link>https://arxiv.org/abs/2508.17850</link>
<guid>https://arxiv.org/abs/2508.17850</guid>
<content:encoded><![CDATA[
arXiv:2508.17850v1 Announce Type: new 
Abstract: As single-center computing approaches power constraints, decentralized training is becoming essential. Reinforcement Learning (RL) post-training enhances Large Language Models (LLMs) but faces challenges in heterogeneous distributed environments due to its tightly-coupled sampling-learning alternation. We propose HeteroRL, an asynchronous RL architecture that decouples rollout sampling from parameter learning, enabling robust deployment across geographically distributed nodes under network delays. We identify that latency-induced KL divergence causes importance sampling failure due to high variance. To address this, we propose Group Expectation Policy Optimization (GEPO), which reduces importance weight variance through a refined sampling mechanism. Theoretically, GEPO achieves exponential variance reduction. Experiments show it maintains superior stability over methods like GRPO, with less than 3% performance degradation under 1800-second delays, demonstrating strong potential for decentralized RL in heterogeneous networks.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real World Assets on-Chain Assistance Low-Altitude Computility Networks: Architecture, Methodology, and Challenges</title>
<link>https://arxiv.org/abs/2508.17911</link>
<guid>https://arxiv.org/abs/2508.17911</guid>
<content:encoded><![CDATA[
arXiv:2508.17911v1 Announce Type: new 
Abstract: Low-altitude airspace is becoming a new frontier for smart city services and commerce. Networks of drones, electric Vertical Takeoff and Landing (eVTOL) vehicles, and other aircraft, termed Low-Altitude Economic Networks (LAENets), promise to transform urban logistics, aerial sensing, and communication. A key challenge is how to efficiently share and trust the computing utility, termed computility, of these aerial devices. We propose treating the computing power on aircraft as tokenized Real-World Assets (RWAs) that can be traded and orchestrated via blockchain. By representing distributed edge computing resources as blockchain tokens, disparate devices can form Low-Altitude Computility Networks (LACNets), collaborative computing clusters in the sky. We first compare blockchain technologies, non-fungible tokens (NFTs), and RWA frameworks to clarify how physical hardware and its computational output can be tokenized as assets. Then, we present an architecture using blockchain to integrate aircraft fleets into a secure, interoperable computing network. Furthermore, a case study models an urban logistics LACNet of delivery drones and air-taxis. Simulation results indicate improvements in task latency, trust assurance, and resource efficiency when leveraging RWA-based coordination. Finally, we discuss future research directions, including AI-driven orchestration, edge AI offloading and collaborative computing, and cross-jurisdictional policy for tokenized assets.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoveScanner: Analysis of Security Risks of Move Smart Contracts</title>
<link>https://arxiv.org/abs/2508.17964</link>
<guid>https://arxiv.org/abs/2508.17964</guid>
<content:encoded><![CDATA[
arXiv:2508.17964v1 Announce Type: new 
Abstract: As blockchain technology continues to evolve, the security of smart contracts has increasingly drawn attention from both academia and industry. The Move language, with its unique resource model and linear type system, provides a solid foundation for the security of digital assets. However, smart contracts still face new security challenges due to developer programming errors and the potential risks associated with cross-module interactions. This paper systematically analyzes the limitations of existing security tools within the Move ecosystem and reveals their unique vulnerability patterns. To address these issues, it introduces MoveScanner, a static analysis tool based on a control flow graph and data flow analysis architecture. By incorporating cross-module call graph tracking, MoveScanner can effectively identify five key types of security vulnerabilities, including resource leaks, weak permission management, and arithmetic overflows. In terms of design, MoveScanner adheres to a modular principle, supports bytecode-level analysis and multi-chain adaptation, and introduces innovative resource trajectory tracking algorithms and capability matrix analysis methods, thereby significantly reducing the false positive rate. Empirical results show that MoveScanner achieved 88.2% detection accuracy in benchmark testing, filling the gap in security tools in the Move ecosystem. Furthermore, this paper identifies twelve new types of security risks based on the resource-oriented programming paradigm and provides a theoretical foundation and practical experience for the development of smart contract security mechanisms. Future work will focus on combining formal verification and dynamic analysis techniques to build a security protection framework covering the entire contract lifecycle
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative-Online-Learning-Enabled Distributionally Robust Motion Control for Multi-Robot Systems</title>
<link>https://arxiv.org/abs/2508.17173</link>
<guid>https://arxiv.org/abs/2508.17173</guid>
<content:encoded><![CDATA[
arXiv:2508.17173v1 Announce Type: cross 
Abstract: This paper develops a novel COllaborative-Online-Learning (COOL)-enabled motion control framework for multi-robot systems to avoid collision amid randomly moving obstacles whose motion distributions are partially observable through decentralized data streams. To address the notable challenge of data acquisition due to occlusion, a COOL approach based on the Dirichlet process mixture model is proposed to efficiently extract motion distribution information by exchanging among robots selected learning structures. By leveraging the fine-grained local-moment information learned through COOL, a data-stream-driven ambiguity set for obstacle motion is constructed. We then introduce a novel ambiguity set propagation method, which theoretically admits the derivation of the ambiguity sets for obstacle positions over the entire prediction horizon by utilizing obstacle current positions and the ambiguity set for obstacle motion. Additionally, we develop a compression scheme with its safety guarantee to automatically adjust the complexity and granularity of the ambiguity set by aggregating basic ambiguity sets that are close in a measure space, thereby striking an attractive trade-off between control performance and computation time. Then the probabilistic collision-free trajectories are generated through distributionally robust optimization problems. The distributionally robust obstacle avoidance constraints based on the compressed ambiguity set are equivalently reformulated by deriving separating hyperplanes through tractable semi-definite programming. Finally, we establish the probabilistic collision avoidance guarantee and the long-term tracking performance guarantee for the proposed framework. The numerical simulations are used to demonstrate the efficacy and superiority of the proposed approach compared with state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evasive Active Hypothesis Testing with Deep Neuroevolution: The Single- and Multi-Agent Cases</title>
<link>https://arxiv.org/abs/2403.10112</link>
<guid>https://arxiv.org/abs/2403.10112</guid>
<content:encoded><![CDATA[
arXiv:2403.10112v2 Announce Type: replace 
Abstract: Active hypothesis testing is a thoroughly studied problem that finds numerous applications in wireless communications and sensor networks. In this paper, we focus on one centralized and one decentralized problem of active hypothesis testing in the presence of an eavesdropper. For the centralized problem including a single legitimate agent, we present a new framework based on deep NeuroEvolution (NE), whereas, for the decentralized problem, we develop a novel NE-based method for solving collaborative multi-agent tasks, which, interestingly, maintains all computational benefits of our single-agent NE-based scheme. To further reduce the computational complexity of the latter scheme, a novel multi-agent joint NE and pruning framework is also designed. The superiority of the proposed NE-based evasive active hypothesis testing schemes over conventional active hypothesis testing policies, as well as learning-based methods, is validated through extensive numerical investigations in an example use case of anomaly detection over wireless sensor networks. It is demonstrated that the proposed joint optimization and pruning framework achieves nearly identical performance with its unpruned counterpart, while removing a very large percentage of redundant deep neural network weights.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PixRO: Pixel-Distributed Rotational Odometry with Gaussian Belief Propagation</title>
<link>https://arxiv.org/abs/2406.09726</link>
<guid>https://arxiv.org/abs/2406.09726</guid>
<content:encoded><![CDATA[
arXiv:2406.09726v2 Announce Type: replace 
Abstract: Images are the standard input for most computer vision algorithms. However, their processing often reduces to parallelizable operations applied locally and independently to individual pixels. Yet, many of these low-level raw pixel readings only provide redundant or noisy information for specific high-level tasks, leading to inefficiencies in both energy consumption during their transmission off-sensor and computational resources in their subsequent processing.
  As novel sensors featuring advanced in-pixel processing capabilities emerge, we envision a paradigm shift toward performing increasingly complex visual processing directly in-pixel, reducing computational overhead downstream. We advocate for synthesizing high-level cues at the pixel level, enabling their off-sensor transmission to directly support downstream tasks more effectively than raw pixel readings.
  This paper conceptualizes a novel photometric rotation estimation algorithm to be distributed at pixel level, where each pixel estimates the global motion of the camera by exchanging information with other pixels to achieve global consensus. We employ a probabilistic formulation and leverage Gaussian Belief Propagation (GBP) for decentralized inference using messaging-passing. The proposed proposed technique is evaluated on real-world public datasets and we offer a in-depth analysis of the practicality of applying GBP to distributed rotation estimation at pixel level.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Octopus Inspired Optimization (OIO): A Hierarchical Framework for Navigating Protein Fitness Landscapes</title>
<link>https://arxiv.org/abs/2410.07968</link>
<guid>https://arxiv.org/abs/2410.07968</guid>
<content:encoded><![CDATA[
arXiv:2410.07968v3 Announce Type: replace 
Abstract: Navigating the vast, rugged, and multi-modal fitness landscapes of protein sequences presents a formidable challenge for computational protein engineering, often trapping algorithms in suboptimal solutions. Existing methods struggle with the exploration-exploitation dilemma, failing to synergize global search with deep local refinement. To overcome this critical barrier, we introduce the Octopus Inspired Optimization (OIO), a novel hierarchical metaheuristic that mimics the octopus's unique neural architecture of centralized control and decentralized, parallel execution. OIO's "individual-tentacle-sucker" framework provides an intrinsic unification of global exploration and parallelized local exploitation, making it structurally ideal for complex combinatorial problems. We validated OIO's efficacy through a rigorous three-tiered experimental framework. On a real-world Green Fluorescent Protein (GFP) design benchmark, OIO surpassed a comprehensive suite of 15 competing metaheuristics, including 7 classic algorithms and 8 state-of-the-art methods from the past two years, delivering performance comparable only to a specialized local search algorithm. This success is explained by its fundamental strengths: OIO ranked first on the NK-Landscape benchmark, confirming its architectural suitability for protein-like fitness landscapes, and also ranked first on the gold-standard CEC2022 benchmark, demonstrating the raw power and efficiency of its optimization engine. OIO establishes a new, nature-inspired paradigm for protein engineering, offering a robust and powerful tool with significant potential for advancing therapeutic and enzymatic design.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HeteroTune: Efficient Federated Learning for Large Heterogeneous Models</title>
<link>https://arxiv.org/abs/2411.16796</link>
<guid>https://arxiv.org/abs/2411.16796</guid>
<content:encoded><![CDATA[
arXiv:2411.16796v2 Announce Type: replace 
Abstract: While large pre-trained models have achieved impressive performance across AI tasks, their deployment in privacy-sensitive and distributed environments remains challenging. Federated learning (FL) offers a viable solution by enabling decentralized fine-tuning without data sharing, but real-world applications face significant obstacles due to heterogeneous client resources in compute and memory. To address this, we propose HeteroTune, a novel federated fine-tuning paradigm for large, heterogeneous models operating under limited communication and computation budgets. The core of our method lies in a novel architecture, DeMA (Dense Mixture of Adapters), which enables flexible and efficient aggregation of heterogeneous models by preserving their full representational capacity while facilitating seamless cross-model knowledge fusion. We further introduce CMGA (Cross-Model Gradient Alignment), a lightweight yet effective mechanism that enhances training stability by harmonizing gradient directions across heterogeneous client models during aggregation, mitigating update conflicts and promoting more consistent convergence in federated settings. We provide both theoretical analysis and empirical evidence showing that HeteroTune achieves state-of-the-art performance and efficiency across diverse tasks and model architectures. For example, on LLaMA models, it reduces communication overhead by 99.5%, cuts peak memory usage by ~50%, and improves performance by 4.61%.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SolEval: Benchmarking Large Language Models for Repository-level Solidity Code Generation</title>
<link>https://arxiv.org/abs/2502.18793</link>
<guid>https://arxiv.org/abs/2502.18793</guid>
<content:encoded><![CDATA[
arXiv:2502.18793v2 Announce Type: replace 
Abstract: Large language models (LLMs) have transformed code generation. However, most existing approaches focus on mainstream languages such as Python and Java, neglecting the Solidity language, the predominant programming language for Ethereum smart contracts. Due to the lack of adequate benchmarks for Solidity, LLMs' ability to generate secure, cost-effective smart contracts remains unexplored. To fill this gap, we construct SolEval, the first repository-level benchmark designed for Solidity smart contract generation, to evaluate the performance of LLMs on Solidity. SolEval consists of 1,507 samples from 28 different repositories, covering 6 popular domains, providing LLMs with a comprehensive evaluation benchmark. Unlike the existing Solidity benchmark, SolEval not only includes complex function calls but also reflects the real-world complexity of the Ethereum ecosystem by incorporating Gas@k and Vul@k. We evaluate 16 LLMs on SolEval, and our results show that the best-performing LLM achieves only 26.29% Pass@10, highlighting substantial room for improvement in Solidity code generation by LLMs. Additionally, we conduct supervised fine-tuning (SFT) on Qwen-7B using SolEval, resulting in a significant performance improvement, with Pass@5 increasing from 16.67% to 58.33%, demonstrating the effectiveness of fine-tuning LLMs on our benchmark. We release our data and code at https://github.com/pzy2000/SolEval.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Federated Learning: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2508.15998</link>
<guid>https://arxiv.org/abs/2508.15998</guid>
<content:encoded><![CDATA[
arXiv:2508.15998v1 Announce Type: new 
Abstract: Quantum federated learning (QFL) is a combination of distributed quantum computing and federated machine learning, integrating the strengths of both to enable privacy-preserving decentralized learning with quantum-enhanced capabilities. It appears as a promising approach for addressing challenges in efficient and secure model training across distributed quantum systems. This paper presents a comprehensive survey on QFL, exploring its key concepts, fundamentals, applications, and emerging challenges in this rapidly developing field. Specifically, we begin with an introduction to the recent advancements of QFL, followed by discussion on its market opportunity and background knowledge. We then discuss the motivation behind the integration of quantum computing and federated learning, highlighting its working principle. Moreover, we review the fundamentals of QFL and its taxonomy. Particularly, we explore federation architecture, networking topology, communication schemes, optimization techniques, and security mechanisms within QFL frameworks. Furthermore, we investigate applications of QFL across several domains which include vehicular networks, healthcare networks, satellite networks, metaverse, and network security. Additionally, we analyze frameworks and platforms related to QFL, delving into its prototype implementations, and provide a detailed case study. Key insights and lessons learned from this review of QFL are also highlighted. We complete the survey by identifying current challenges and outlining potential avenues for future research in this rapidly advancing field.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Relay-Chain-Powered Ciphertext-Policy Attribute-Based Encryption in Intelligent Transportation Systems</title>
<link>https://arxiv.org/abs/2508.16189</link>
<guid>https://arxiv.org/abs/2508.16189</guid>
<content:encoded><![CDATA[
arXiv:2508.16189v1 Announce Type: new 
Abstract: The very high growth of Intelligent Transportation Systems (ITS) has generated an urgent requirement for secure, effective, and context-aware data sharing mechanisms, especially over heterogeneous and geographically dispersed settings. This work suggests a new architecture that combines a relay chain-driven encryption system with a modified Ciphertext-Policy Attribute-Based Encryption (CP-ABE) scheme to tackle the double impediment of dynamic access and low-latency communication. The model proposes a context-aware smart contract on a worldwide relay chain that checks against data properties, including event type, time, and geographical region, to specify the suitable level of encryption policy. From such relay-directed judgment, On-Board Units (OBUs) encrypt data end-to-end by utilising CP-ABE and store ciphertext inside localised regional blockchains, preventing dependence on symmetric encryption or off-chain storage. High-sensitivity events are secured with firm, multi-attribute access rules, whereas common updates use light policies to help reduce processing burdens. The crypto system also adds traceability and low-latency revocation, with global enforcement managed through the relay chain. This distributed, scalable model provides a proper balance between responsiveness in real time and security and is extremely apt for next-gen vehicular networks that function across multi-jurisdictional domains.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Beat Nakamoto in the Race</title>
<link>https://arxiv.org/abs/2508.16202</link>
<guid>https://arxiv.org/abs/2508.16202</guid>
<content:encoded><![CDATA[
arXiv:2508.16202v1 Announce Type: new 
Abstract: This paper studies proof-of-work Nakamoto consensus under bounded network delays, settling two long-standing questions in blockchain security: How can an adversary most effectively attack block safety under a given block confirmation latency? And what is the resulting probability of safety violation? A Markov decision process (MDP) framework is introduced to precise characterize the system state (including the tree and timings of all blocks mined), the adversary's potential actions, and the state transitions due to the adversarial action and the random block arrival processes. An optimal attack, called bait-and-switch, is proposed and proved to maximize the adversary's chance of violating block safety by "beating Nakamoto in the race". The exact probability of this violation is calculated for any confirmation depth using Markov chain analysis, offering fresh insights into the interplay of network delay, confirmation rules, and blockchain security.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Evolution of Federated Post-Training Large Language Models: A Model Accessibility View</title>
<link>https://arxiv.org/abs/2508.16261</link>
<guid>https://arxiv.org/abs/2508.16261</guid>
<content:encoded><![CDATA[
arXiv:2508.16261v1 Announce Type: new 
Abstract: Federated Learning (FL) enables training models across decentralized data silos while preserving client data privacy. Recent research has explored efficient methods for post-training large language models (LLMs) within FL to address computational and communication challenges. While existing approaches often rely on access to LLMs' internal information, which is frequently restricted in real-world scenarios, an inference-only paradigm (black-box FedLLM) has emerged to address these limitations. This paper presents a comprehensive survey on federated tuning for LLMs. We propose a taxonomy categorizing existing studies along two axes: model access-based and parameter efficiency-based optimization. We classify FedLLM approaches into white-box, gray-box, and black-box techniques, highlighting representative methods within each category. We review emerging research treating LLMs as black-box inference APIs and discuss promising directions and open challenges for future research.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Social Choice Analysis of Optimism's Retroactive Project Funding</title>
<link>https://arxiv.org/abs/2508.16285</link>
<guid>https://arxiv.org/abs/2508.16285</guid>
<content:encoded><![CDATA[
arXiv:2508.16285v1 Announce Type: new 
Abstract: The Optimism Retroactive Project Funding (RetroPGF) is a key initiative within the blockchain ecosystem that retroactively rewards projects deemed valuable to the Ethereum and Optimism communities. Managed by the Optimism Collective, a decentralized autonomous organization (DAO), RetroPGF represents a large-scale experiment in decentralized governance. Funding rewards are distributed in OP tokens, the native digital currency of the ecosystem. As of this writing, four funding rounds have been completed, collectively allocating over 100M dollars, with an additional 1.3B dollars reserved for future rounds. However, we identify significant shortcomings in the current allocation system, underscoring the need for improved governance mechanisms given the scale of funds involved.
  Leveraging computational social choice techniques and insights from multiagent systems, we propose improvements to the voting process by recommending the adoption of a utilitarian moving phantoms mechanism. This mechanism, originally introduced by Freeman et al. in 2019, is designed to enhance social welfare (using the L1 norm) while satisfying strategyproofness -- two key properties aligned with the application's governance requirements. Our analysis provides a formal framework for designing improved funding mechanisms for DAOs, contributing to the broader discourse on decentralized governance and public goods allocation.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrated Noise and Safety Management in UAM via A Unified Reinforcement Learning Framework</title>
<link>https://arxiv.org/abs/2508.16440</link>
<guid>https://arxiv.org/abs/2508.16440</guid>
<content:encoded><![CDATA[
arXiv:2508.16440v1 Announce Type: new 
Abstract: Urban Air Mobility (UAM) envisions the widespread use of small aerial vehicles to transform transportation in dense urban environments. However, UAM faces critical operational challenges, particularly the balance between minimizing noise exposure and maintaining safe separation in low-altitude urban airspace, two objectives that are often addressed separately. We propose a reinforcement learning (RL)-based air traffic management system that integrates both noise and safety considerations within a unified, decentralized framework. Under this scalable air traffic coordination solution, agents operate in a structured, multi-layered airspace and learn altitude adjustment policies to jointly manage noise impact and separation constraints. The system demonstrates strong performance across both objectives and reveals tradeoffs among separation, noise exposure, and energy efficiency under high traffic density. The findings highlight the potential of RL and multi-objective coordination strategies in enhancing the safety, quietness, and efficiency of UAM operations.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Swarming Without an Anchor (SWA): Robot Swarms Adapt Better to Localization Dropouts Then a Single Robot</title>
<link>https://arxiv.org/abs/2508.16460</link>
<guid>https://arxiv.org/abs/2508.16460</guid>
<content:encoded><![CDATA[
arXiv:2508.16460v1 Announce Type: new 
Abstract: In this paper, we present the Swarming Without an Anchor (SWA) approach to state estimation in swarms of Unmanned Aerial Vehicles (UAVs) experiencing ego-localization dropout, where individual agents are laterally stabilized using relative information only. We propose to fuse decentralized state estimation with robust mutual perception and onboard sensor data to maintain accurate state awareness despite intermittent localization failures. Thus, the relative information used to estimate the lateral state of UAVs enables the identification of the unambiguous state of UAVs with respect to the local constellation. The resulting behavior reaches velocity consensus, as this task can be referred to as the double integrator synchronization problem. All disturbances and performance degradations except a uniform translation drift of the swarm as a whole is attenuated which is enabling new opportunities in using tight cooperation for increasing reliability and resilience of multi-UAV systems. Simulations and real-world experiments validate the effectiveness of our approach, demonstrating its capability to sustain cohesive swarm behavior in challenging conditions of unreliable or unavailable primary localization.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair and efficient contribution valuation for vertical federated learning</title>
<link>https://arxiv.org/abs/2201.02658</link>
<guid>https://arxiv.org/abs/2201.02658</guid>
<content:encoded><![CDATA[
arXiv:2201.02658v2 Announce Type: replace 
Abstract: Federated learning is an emerging technology for training machine learning models across decentralized data sources without sharing data. Vertical federated learning, also known as feature-based federated learning, applies to scenarios where data sources have the same sample IDs but different feature sets. To ensure fairness among data owners, it is critical to objectively assess the contributions from different data sources and compensate the corresponding data owners accordingly. The Shapley value is a provably fair contribution valuation metric originating from cooperative game theory. However, its straight-forward computation requires extensively retraining a model on each potential combination of data sources, leading to prohibitively high communication and computation overheads due to multiple rounds of federated learning. To tackle this challenge, we propose a contribution valuation metric called vertical federated Shapley value (VerFedSV) based on the classic Shapley value. We show that VerFedSV not only satisfies many desirable properties of fairness but is also efficient to compute. Moreover, VerFedSV can be adapted to both synchronous and asynchronous vertical federated learning algorithms. Both theoretical analysis and extensive experimental results demonstrate the fairness, efficiency, adaptability, and effectiveness of VerFedSV.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIGMA: Sheaf-Informed Geometric Multi-Agent Pathfinding</title>
<link>https://arxiv.org/abs/2502.06440</link>
<guid>https://arxiv.org/abs/2502.06440</guid>
<content:encoded><![CDATA[
arXiv:2502.06440v2 Announce Type: replace 
Abstract: The Multi-Agent Path Finding (MAPF) problem aims to determine the shortest and collision-free paths for multiple agents in a known, potentially obstacle-ridden environment. It is the core challenge for robotic deployments in large-scale logistics and transportation. Decentralized learning-based approaches have shown great potential for addressing the MAPF problems, offering more reactive and scalable solutions. However, existing learning-based MAPF methods usually rely on agents making decisions based on a limited field of view (FOV), resulting in short-sighted policies and inefficient cooperation in complex scenarios. There, a critical challenge is to achieve consensus on potential movements between agents based on limited observations and communications. To tackle this challenge, we introduce a new framework that applies sheaf theory to decentralized deep reinforcement learning, enabling agents to learn geometric cross-dependencies between each other through local consensus and utilize them for tightly cooperative decision-making. In particular, sheaf theory provides a mathematical proof of conditions for achieving global consensus through local observation. Inspired by this, we incorporate a neural network to approximately model the consensus in latent space based on sheaf theory and train it through self-supervised learning. During the task, in addition to normal features for MAPF as in previous works, each agent distributedly reasons about a learned consensus feature, leading to efficient cooperation on pathfinding and collision avoidance. As a result, our proposed method demonstrates significant improvements over state-of-the-art learning-based MAPF planners, especially in relatively large and complex scenarios, demonstrating its superiority over baselines in various simulations and real-world robot experiments. The code is available at https://github.com/marmotlab/SIGMA
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Partially Decentralized Multi-Agent Q-Learning via Digital Cousins for Wireless Networks</title>
<link>https://arxiv.org/abs/2503.05970</link>
<guid>https://arxiv.org/abs/2503.05970</guid>
<content:encoded><![CDATA[
arXiv:2503.05970v3 Announce Type: replace 
Abstract: Q-learning is a widely used reinforcement learning (RL) algorithm for optimizing wireless networks, but faces challenges with large state-spaces. Recently proposed multi-environment mixed Q-learning (MEMQ) algorithm addresses these challenges by employing multiple Q-learning algorithms across multiple synthetically generated, distinct but structurally related environments, so-called digital cousins. In this paper, we propose a novel multi-agent MEMQ (M-MEMQ) for cooperative decentralized wireless networks with multiple networked transmitters (TXs) and base stations (BSs). TXs do not have access to global information (joint state and actions). The new concept of coordinated and uncoordinated states is introduced. In uncoordinated states, TXs act independently to minimize their individual costs and update local Q-functions. In coordinated states, TXs use a Bayesian approach to estimate the joint state and update the joint Q-functions. The cost of information-sharing scales linearly with the number of TXs and is independent of the joint state-action space size. Several theoretical guarantees, including deterministic and probabilistic convergence, bounds on estimation error variance, and the probability of misdetecting the joint states, are given. Numerical simulations show that M-MEMQ outperforms several decentralized and centralized training with decentralized execution (CTDE) multi-agent RL algorithms by achieving 60% lower average policy error (APE), 40% faster convergence, 45% reduced runtime complexity, and 40% less sample complexity. Furthermore, M-MEMQ achieves comparable APE with significantly lower complexity than centralized methods. Simulations validate the theoretical analyses.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedEFC: Federated Learning Using Enhanced Forward Correction Against Noisy Labels</title>
<link>https://arxiv.org/abs/2504.05615</link>
<guid>https://arxiv.org/abs/2504.05615</guid>
<content:encoded><![CDATA[
arXiv:2504.05615v2 Announce Type: replace 
Abstract: Federated Learning (FL) is a powerful framework for privacy-preserving distributed learning. It enables multiple clients to collaboratively train a global model without sharing raw data. However, handling noisy labels in FL remains a major challenge due to heterogeneous data distributions and communication constraints, which can severely degrade model performance. To address this issue, we propose FedEFC, a novel method designed to tackle the impact of noisy labels in FL. FedEFC mitigates this issue through two key techniques: (1) prestopping, which prevents overfitting to mislabeled data by dynamically halting training at an optimal point, and (2) loss correction, which adjusts model updates to account for label noise. In particular, we develop an effective loss correction tailored to the unique challenges of FL, including data heterogeneity and decentralized training. Furthermore, we provide a theoretical analysis, leveraging the composite proper loss property, to demonstrate that the FL objective function under noisy label distributions can be aligned with the clean label distribution. Extensive experimental results validate the effectiveness of our approach, showing that it consistently outperforms existing FL techniques in mitigating the impact of noisy labels, particularly under heterogeneous data settings (e.g., achieving up to 41.64% relative performance improvement over the existing loss correction method).
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Vision-Based Autonomous Aerial Wildlife Monitoring</title>
<link>https://arxiv.org/abs/2508.15038</link>
<guid>https://arxiv.org/abs/2508.15038</guid>
<content:encoded><![CDATA[
arXiv:2508.15038v1 Announce Type: new 
Abstract: Wildlife field operations demand efficient parallel deployment methods to identify and interact with specific individuals, enabling simultaneous collective behavioral analysis, and health and safety interventions. Previous robotics solutions approach the problem from the herd perspective, or are manually operated and limited in scale. We propose a decentralized vision-based multi-quadrotor system for wildlife monitoring that is scalable, low-bandwidth, and sensor-minimal (single onboard RGB camera). Our approach enables robust identification and tracking of large species in their natural habitat. We develop novel vision-based coordination and tracking algorithms designed for dynamic, unstructured environments without reliance on centralized communication or control. We validate our system through real-world experiments, demonstrating reliable deployment in diverse field conditions.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scalable Trie Building Algorithm for High-Throughput Phyloanalysis of Wafer-Scale Digital Evolution Experiments</title>
<link>https://arxiv.org/abs/2508.15074</link>
<guid>https://arxiv.org/abs/2508.15074</guid>
<content:encoded><![CDATA[
arXiv:2508.15074v1 Announce Type: new 
Abstract: Agent-based simulation platforms play a key role in enabling fast-to-run evolution experiments that can be precisely controlled and observed in detail. Availability of high-resolution snapshots of lineage ancestries from digital experiments, in particular, is key to investigations of evolvability and open-ended evolution, as well as in providing a validation testbed for bioinformatics method development. Ongoing advances in AI/ML hardware accelerator devices, such as the 850,000-processor Cerebras Wafer-Scale Engine (WSE), are poised to broaden the scope of evolutionary questions that can be investigated in silico. However, constraints in memory capacity and locality characteristic of these systems introduce difficulties in exhaustively tracking phylogenies at runtime. To overcome these challenges, recent work on hereditary stratigraphy algorithms has developed space-efficient genetic markers to facilitate fully decentralized estimation of relatedness among digital organisms. However, in existing work, compute time to reconstruct phylogenies from these genetic markers has proven a limiting factor in achieving large-scale phyloanalyses. Here, we detail an improved trie-building algorithm designed to produce reconstructions equivalent to existing approaches. For modestly-sized 10,000-tip trees, the proposed approach achieves a 300-fold speedup versus existing state-of-the-art. Finally, using 1 billion genome datasets drawn from WSE simulations encompassing 954 trillion replication events, we report a pair of large-scale phylogeny reconstruction trials, achieving end-to-end reconstruction times of 2.6 and 2.9 hours. In substantially improving reconstruction scaling and throughput, presented work establishes a key foundation to enable powerful high-throughput phyloanalysis techniques in large-scale digital evolution experiments.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning based on Self-Evolving Gaussian Clustering</title>
<link>https://arxiv.org/abs/2508.15393</link>
<guid>https://arxiv.org/abs/2508.15393</guid>
<content:encoded><![CDATA[
arXiv:2508.15393v1 Announce Type: new 
Abstract: In this study, we present an Evolving Fuzzy System within the context of Federated Learning, which adapts dynamically with the addition of new clusters and therefore does not require the number of clusters to be selected apriori. Unlike traditional methods, Federated Learning allows models to be trained locally on clients' devices, sharing only the model parameters with a central server instead of the data. Our method, implemented using PyTorch, was tested on clustering and classification tasks. The results show that our approach outperforms established classification methods on several well-known UCI datasets. While computationally intensive due to overlap condition calculations, the proposed method demonstrates significant advantages in decentralized data processing.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BadFU: Backdoor Federated Learning through Adversarial Machine Unlearning</title>
<link>https://arxiv.org/abs/2508.15541</link>
<guid>https://arxiv.org/abs/2508.15541</guid>
<content:encoded><![CDATA[
arXiv:2508.15541v1 Announce Type: new 
Abstract: Federated learning (FL) has been widely adopted as a decentralized training paradigm that enables multiple clients to collaboratively learn a shared model without exposing their local data. As concerns over data privacy and regulatory compliance grow, machine unlearning, which aims to remove the influence of specific data from trained models, has become increasingly important in the federated setting to meet legal, ethical, or user-driven demands. However, integrating unlearning into FL introduces new challenges and raises largely unexplored security risks. In particular, adversaries may exploit the unlearning process to compromise the integrity of the global model. In this paper, we present the first backdoor attack in the context of federated unlearning, demonstrating that an adversary can inject backdoors into the global model through seemingly legitimate unlearning requests. Specifically, we propose BadFU, an attack strategy where a malicious client uses both backdoor and camouflage samples to train the global model normally during the federated training process. Once the client requests unlearning of the camouflage samples, the global model transitions into a backdoored state. Extensive experiments under various FL frameworks and unlearning strategies validate the effectiveness of BadFU, revealing a critical vulnerability in current federated unlearning practices and underscoring the urgent need for more secure and robust federated unlearning mechanisms.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Detection of Adversarial Attacks in Multi-Agent Reinforcement Learning with Continuous Action Space</title>
<link>https://arxiv.org/abs/2508.15764</link>
<guid>https://arxiv.org/abs/2508.15764</guid>
<content:encoded><![CDATA[
arXiv:2508.15764v1 Announce Type: new 
Abstract: We address the problem of detecting adversarial attacks against cooperative multi-agent reinforcement learning with continuous action space. We propose a decentralized detector that relies solely on the local observations of the agents and makes use of a statistical characterization of the normal behavior of observable agents. The proposed detector utilizes deep neural networks to approximate the normal behavior of agents as parametric multivariate Gaussian distributions. Based on the predicted density functions, we define a normality score and provide a characterization of its mean and variance. This characterization allows us to employ a two-sided CUSUM procedure for detecting deviations of the normality score from its mean, serving as a detector of anomalous behavior in real-time. We evaluate our scheme on various multi-agent PettingZoo benchmarks against different state-of-the-art attack methods, and our results demonstrate the effectiveness of our method in detecting impactful adversarial attacks. Particularly, it outperforms the discrete counterpart by achieving AUC-ROC scores of over 0.95 against the most impactful attacks in all evaluated environments.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Modelling of Cryptocurrency Price Movements Using Discretisation-Aware Bayesian Networks</title>
<link>https://arxiv.org/abs/2303.16148</link>
<guid>https://arxiv.org/abs/2303.16148</guid>
<content:encoded><![CDATA[
arXiv:2303.16148v2 Announce Type: replace-cross 
Abstract: This study identifies the key factors influencing the price movements of major cryptocurrencies, Bitcoin, Binance Coin, Ethereum, Litecoin, Ripple, and Tether, using Bayesian networks (BNs). This study addresses two key challenges: modelling price movements in highly volatile cryptocurrency markets and enhancing predictive performance through discretisation-aware Bayesian Networks. It analyses both macro-financial indicators (gold, oil, MSCI, S and P 500, USDX) and social media signals (tweet volume) as potential price drivers. Moreover, since discretisation is a critical step in the effectiveness of BNs, we implement a structured procedure to build 54 BNs models by combining three discretisation methods (equal interval, equal quantile, and k-means) with several bin counts. These models are evaluated using four metrics, including balanced accuracy, F1 score, area under the ROC curve and a composite score. Results show that equal interval with two bins consistently yields the best predictive performance. We also provide deeper insights into each network's structure through inference, sensitivity, and influence strength analyses. These analyses reveal distinct price-driving patterns for each cryptocurrency, underscore the importance of coin-specific analysis, and demonstrate the value of BNs for interpretable causal modelling in volatile cryptocurrency markets.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Distributed Learned Hash Table</title>
<link>https://arxiv.org/abs/2508.14239</link>
<guid>https://arxiv.org/abs/2508.14239</guid>
<content:encoded><![CDATA[
arXiv:2508.14239v1 Announce Type: new 
Abstract: Distributed Hash Tables (DHTs) are pivotal in numerous high-impact key-value applications built on distributed networked systems, offering a decentralized architecture that avoids single points of failure and improves data availability. Despite their widespread utility, DHTs face substantial challenges in handling range queries, which are crucial for applications such as LLM serving, distributed storage, databases, content delivery networks, and blockchains. To address this limitation, we present LEAD, a novel system incorporating learned models within DHT structures to significantly optimize range query performance. LEAD utilizes a recursive machine learning model to map and retrieve data across a distributed system while preserving the inherent order of data. LEAD includes the designs to minimize range query latency and message cost while maintaining high scalability and resilience to network churn. Our comprehensive evaluations, conducted in both testbed implementation and simulations, demonstrate that LEAD achieves tremendous advantages in system efficiency compared to existing range query methods in large-scale distributed systems, reducing query latency and message cost by 80% to 90%+. Furthermore, LEAD exhibits remarkable scalability and robustness against system churn, providing a robust, scalable solution for efficient data retrieval in distributed key-value systems.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hierarchical Sharded Blockchain Balancing Performance and Availability</title>
<link>https://arxiv.org/abs/2508.14457</link>
<guid>https://arxiv.org/abs/2508.14457</guid>
<content:encoded><![CDATA[
arXiv:2508.14457v1 Announce Type: new 
Abstract: Blockchain networks offer decentralization, transparency, and immutability for managing critical data but encounter scalability problems as the number of network members and transaction issuers grows. Sharding is considered a promising solution to enhance blockchain scalability. However, most existing blockchain sharding techniques prioritize performance at the cost of availability (e.g., a failure in a few servers holding a shard leads to data unavailability). In this paper, we propose PyloChain, a hierarchical sharded blockchain that balances availability and performance. PyloChain consists of multiple lower-level local chains and one higher-level main chain. Each local chain speculatively executes local transactions to achieve high parallelism across multiple local chains. The main chain leverages a directed-acyclic-graph (DAG)-based mempool to guarantee local block availability and to enable efficient Byzantine Fault Tolerance (BFT) consensus to execute global (or cross-shard) transactions within a collocated sharding. PyloChain speculatively executes local transactions across multiple local chains to achieve high parallelism. In order to reduce the number of aborted local transactions, PyloChain applies a simple scheduling technique to handle global transactions in the main chain. PyloChain provides a fine-grained auditing mechanism to mitigate faulty higher-level members by externalizing main chain operations to lower-level local members. We implemented and evaluated PyloChain, demonstrating its performance scalability with 1.49x higher throughput and 2.63x faster latency compared to the state-of-the-art balanced hierarchical sharded blockchain.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Markov Chain-based Model of Blockchain Radio Access Networks</title>
<link>https://arxiv.org/abs/2508.14519</link>
<guid>https://arxiv.org/abs/2508.14519</guid>
<content:encoded><![CDATA[
arXiv:2508.14519v1 Announce Type: new 
Abstract: Security has always been a priority, for researchers, service providers and network operators when it comes to radio access networks (RAN). One wireless access approach that has captured attention is blockchain enabled RAN (B-RAN) due to its secure nature. This research introduces a framework that integrates blockchain technology into RAN while also addressing the limitations of state-of-the-art models. The proposed framework utilizes queuing and Markov chain theory to model the aspects of B-RAN. An extensive evaluation of the models performance is provided, including an analysis of timing factors and a focused assessment of its security aspects. The results demonstrate reduced latency and comparable security making the presented framework suitable for diverse application scenarios.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Payment Channel Network Liquidity with Topology Optimization and Transaction Selection</title>
<link>https://arxiv.org/abs/2508.14524</link>
<guid>https://arxiv.org/abs/2508.14524</guid>
<content:encoded><![CDATA[
arXiv:2508.14524v1 Announce Type: new 
Abstract: Payment channel networks (PCNs) are a promising technology that alleviates blockchain scalability by shifting the transaction load from the blockchain to the PCN. Nevertheless, the network topology has to be carefully designed to maximise the transaction throughput in PCNs. Additionally, users in PCNs also have to make optimal decisions on which transactions to forward and which to reject to prolong the lifetime of their channels. In this work, we consider an input sequence of transactions over $p$ parties. Each transaction consists of a transaction size, source, and target, and can be either accepted or rejected (entailing a cost). The goal is to design a PCN topology among the $p$ cooperating parties, along with the channel capacities, and then output a decision for each transaction in the sequence to minimise the cost of creating and augmenting channels, as well as the cost of rejecting transactions. Our main contribution is an $\mathcal{O}(p)$ approximation algorithm for the problem with $p$ parties. We further show that with some assumptions on the distribution of transactions, we can reduce the approximation ratio to $\mathcal{O}(\sqrt{p})$. We complement our theoretical analysis with an empirical study of our assumptions and approach in the context of the Lightning Network.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DOPA: Stealthy and Generalizable Backdoor Attacks from a Single Client under Challenging Federated Constraints</title>
<link>https://arxiv.org/abs/2508.14530</link>
<guid>https://arxiv.org/abs/2508.14530</guid>
<content:encoded><![CDATA[
arXiv:2508.14530v1 Announce Type: new 
Abstract: Federated Learning (FL) is increasingly adopted for privacy-preserving collaborative training, but its decentralized nature makes it particularly susceptible to backdoor attacks. Existing attack methods, however, often rely on idealized assumptions and fail to remain effective under real-world constraints, such as limited attacker control, non-IID data distributions, and the presence of diverse defense mechanisms. To address this gap, we propose DOPA (Divergent Optimization Path Attack), a novel framework that simulates heterogeneous local training dynamics and seeks consensus across divergent optimization trajectories to craft universally effective and stealthy backdoor triggers. By leveraging consistency signals across simulated paths to guide optimization, DOPA overcomes the challenge of heterogeneity-induced instability and achieves practical attack viability under stringent federated constraints. We validate DOPA on a comprehensive suite of 12 defense strategies, two model architectures (ResNet18/VGG16), two datasets (CIFAR-10/TinyImageNet), and both mild and extreme non-IID settings. Despite operating under a single-client, black-box, and sparsely participating threat model, DOPA consistently achieves high attack success, minimal accuracy degradation, low runtime, and long-term persistence. These results demonstrate a more practical attack paradigm, offering new perspectives for designing robust defense strategies in federated learning systems
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Vision-Based Coverage Optimization in Mobile Wireless Sensor Networks: A Multi-Agent Deep Reinforcement Learning Approach</title>
<link>https://arxiv.org/abs/2508.14676</link>
<guid>https://arxiv.org/abs/2508.14676</guid>
<content:encoded><![CDATA[
arXiv:2508.14676v1 Announce Type: new 
Abstract: Traditional Wireless Sensor Networks (WSNs) typically rely on pre-analysis of the target area, network size, and sensor coverage to determine initial deployment. This often results in significant overlap to ensure continued network operation despite sensor energy depletion. With the emergence of Mobile Wireless Sensor Networks (MWSNs), issues such as sensor failure and static coverage limitations can be more effectively addressed through mobility. This paper proposes a novel deployment strategy in which mobile sensors autonomously position themselves to maximize area coverage, eliminating the need for predefined policies. A live camera system, combined with deep reinforcement learning (DRL), monitors the network by detecting sensor LED indicators and evaluating real-time coverage. Rewards based on coverage efficiency and sensor movement are computed at each learning step and shared across the network through a Multi-Agent Reinforcement Learning (MARL) framework, enabling decentralized, cooperative sensor control. Key contributions include a vision-based, low-cost coverage evaluation method; a scalable MARL-DRL framework for autonomous deployment; and a self-reconfigurable system that adjusts sensor positioning in response to energy depletion. Compared to traditional distance-based localization, the proposed method achieves a 26.5% improvement in coverage, a 32% reduction in energy consumption, and a 22% decrease in redundancy, extending network lifetime by 45%. This approach significantly enhances adaptability, energy efficiency, and robustness in MWSNs, offering a practical deployment solution within the IoT framework.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Efficient Routing Algorithm for Wireless Sensor Networks: A Multi-Agent Reinforcement Learning Approach</title>
<link>https://arxiv.org/abs/2508.14679</link>
<guid>https://arxiv.org/abs/2508.14679</guid>
<content:encoded><![CDATA[
arXiv:2508.14679v1 Announce Type: new 
Abstract: Efficient energy management is essential in Wireless Sensor Networks (WSNs) to extend network lifetime and ensure reliable data transmission. This paper presents a novel method using reinforcement learning-based cluster-head selection and a hybrid multi-hop routing algorithm, which leverages Q-learning within a multi-agent system to dynamically adapt transmission paths based on the energy distribution across sensor nodes. Each sensor node is modeled as an autonomous agent that observes local state parameters, such as residual energy, distance to sink, hop count, and hotspot proximity, and selects routing actions that maximize long-term energy efficiency. After computing the optimal paths, each sensor aggregates sensed data and forwards it through intermediate nodes to a selected transmitter node, chosen based on the highest remaining State of Charge (SoC), thereby avoiding premature node depletion. To promote efficient learning, a carefully designed reward function incentivizes balanced load distribution, hotspot avoidance, and energy-aware forwarding while maintaining signal quality. The learning process occurs either in a decentralized manner or via a cloud-based controller that offloads computation in large-scale deployments. Moreover, the RL-driven routing decisions are fused with classical graph-based methods, Minimum Energy Routing Algorithm (MERA) and Minimum Spanning Tree (MST), to optimize energy consumption and load balancing. Simulations confirm that the proposed approach significantly improves node survival rate, reduces SoC variance, and enhances network resilience, making it a scalable and adaptive solution for energy-constrained WSNs in dynamic sensor deployments and IoT applications.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLAIR: Feedback Learning for Adaptive Information Retrieval</title>
<link>https://arxiv.org/abs/2508.13390</link>
<guid>https://arxiv.org/abs/2508.13390</guid>
<content:encoded><![CDATA[
arXiv:2508.13390v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have driven the adoption of copilots in complex technical scenarios, underscoring the growing need for specialized information retrieval solutions. In this paper, we introduce FLAIR, a lightweight, feedback learning framework that adapts copilot systems' retrieval strategies by integrating domain-specific expert feedback. FLAIR operates in two stages: an offline phase obtains indicators from (1) user feedback and (2) questions synthesized from documentation, storing these indicators in a decentralized manner. An online phase then employs a two-track ranking mechanism to combine raw similarity scores with the collected indicators. This iterative setup refines retrieval performance for any query. Extensive real-world evaluations of FLAIR demonstrate significant performance gains on both previously seen and unseen queries, surpassing state-of-the-art approaches. The system has been successfully integrated into Copilot DECO, serving thousands of users at Microsoft, demonstrating its scalability and effectiveness in operational environments.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Robot Navigation in Social Mini-Games: Definitions, Taxonomy, and Algorithms</title>
<link>https://arxiv.org/abs/2508.13459</link>
<guid>https://arxiv.org/abs/2508.13459</guid>
<content:encoded><![CDATA[
arXiv:2508.13459v1 Announce Type: new 
Abstract: The ``Last Mile Challenge'' has long been considered an important, yet unsolved, challenge for autonomous vehicles, public service robots, and delivery robots. A central issue in this challenge is the ability of robots to navigate constrained and cluttered environments (e.g., doorways, hallways, corridor intersections), often while competing for space with other robots and humans. We refer to these environments as ``Social Mini-Games'' (SMGs). SMGs are tightly coupled, high-agency interactions that arise within general multi-robot navigation (MRN) scenarios. They are identified through certain distinct characteristics and require specialized metrics to evaluate them. Traditional navigation approaches designed for MRN do not perform well in SMGs, which has led to focused research on dedicated SMG solvers (navigation methods specialized to navigate in SMGs), which has flourished in recent years. However, publications on SMG navigation research make different assumptions (on centralized versus decentralized, observability, communication, cooperation, etc.), and have different objective functions (safety versus liveness). These assumptions and objectives are sometimes implicitly assumed or described informally. This makes it difficult to establish appropriate baselines for comparison in research papers, as well as making it difficult for practitioners to find the papers relevant to their concrete application. Such ad-hoc representation of the field also presents a barrier to new researchers wanting to start research in this area. SMG navigation research requires its own taxonomy, definitions, and evaluation protocols to guide effective research moving forward. This survey is the first to catalog SMG solvers using a well-defined and unified taxonomy and to classify existing methods accordingly.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Scalar Selection in Elliptic Curve Cryptography Using Differential Evolution for Enhanced Security</title>
<link>https://arxiv.org/abs/2508.13520</link>
<guid>https://arxiv.org/abs/2508.13520</guid>
<content:encoded><![CDATA[
arXiv:2508.13520v1 Announce Type: new 
Abstract: Elliptic Curve Cryptography (ECC) is a fundamental component of modern public-key cryptosystems that enable efficient and secure digital signatures, key exchanges, and encryption. Its core operation, scalar multiplication, denoted as $k \cdot P$, where $P$ is a base point and $k$ is a private scalar, relies heavily on the secrecy and unpredictability of $k$. Conventionally, $k$ is selected using user input or pseudorandom number generators. However, in resource-constrained environments with weak entropy sources, these approaches may yield low-entropy or biased scalars, increasing susceptibility to side-channel and key recovery attacks. To mitigate these vulnerabilities, we introduce an optimization-driven scalar generation method that explicitly maximizes bit-level entropy. Our approach uses differential evolution (DE), a population-based metaheuristic algorithm, to search for scalars whose binary representations exhibit maximal entropy, defined by an even and statistically uniform distribution of ones and zeros. This reformulation of scalar selection as an entropy-optimization problem enhances resistance to entropy-based cryptanalytic techniques and improves overall unpredictability. Experimental results demonstrate that DE-optimized scalars achieve entropy significantly higher than conventionally generated scalars. The proposed method can be integrated into existing ECC-based protocols, offering a deterministic, tunable alternative to traditional randomness, ideal for applications in blockchain, secure messaging, IoT, and other resource-constrained environments.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Larger Model via One-Shot Federated Learning on Heterogeneous Client Models</title>
<link>https://arxiv.org/abs/2508.13625</link>
<guid>https://arxiv.org/abs/2508.13625</guid>
<content:encoded><![CDATA[
arXiv:2508.13625v1 Announce Type: new 
Abstract: Large models, renowned for superior performance, outperform smaller ones even without billion-parameter scales. While mobile network servers have ample computational resources to support larger models than client devices, privacy constraints prevent clients from directly sharing their raw data. Federated Learning (FL) enables decentralized clients to collaboratively train a shared model by exchanging model parameters instead of transmitting raw data. Yet, it requires a uniform model architecture and multiple communication rounds, which neglect resource heterogeneity, impose heavy computational demands on clients, and increase communication overhead. To address these challenges, we propose FedOL, to construct a larger and more comprehensive server model in one-shot settings (i.e., in a single communication round). Instead of model parameter sharing, FedOL employs knowledge distillation, where clients only exchange model prediction outputs on an unlabeled public dataset. This reduces communication overhead by transmitting compact predictions instead of full model weights and enables model customization by allowing heterogeneous model architectures. A key challenge in this setting is that client predictions may be biased due to skewed local data distributions, and the lack of ground-truth labels in the public dataset further complicates reliable learning. To mitigate these issues, FedOL introduces a specialized objective function that iteratively refines pseudo-labels and the server model, improving learning reliability. To complement this, FedOL incorporates a tailored pseudo-label generation and knowledge distillation strategy that effectively integrates diverse knowledge. Simulation results show that FedOL significantly outperforms existing baselines, offering a cost-effective solution for mobile networks where clients possess valuable private data but limited computational resources.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Vision-Language Retrieval of Educational Metaverse Content in Agriculture</title>
<link>https://arxiv.org/abs/2508.13713</link>
<guid>https://arxiv.org/abs/2508.13713</guid>
<content:encoded><![CDATA[
arXiv:2508.13713v1 Announce Type: new 
Abstract: Every day, a large amount of educational content is uploaded online across different areas, including agriculture and gardening. When these videos or materials are grouped meaningfully, they can make learning easier and more effective. One promising way to organize and enrich such content is through the Metaverse, which allows users to explore educational experiences in an interactive and immersive environment. However, searching for relevant Metaverse scenarios and finding those matching users' interests remains a challenging task. A first step in this direction has been done recently, but existing datasets are small and not sufficient for training advanced models. In this work, we make two main contributions: first, we introduce a new dataset containing 457 agricultural-themed virtual museums (AgriMuseums), each enriched with textual descriptions; and second, we propose a hierarchical vision-language model to represent and retrieve relevant AgriMuseums using natural language queries. In our experimental setting, the proposed method achieves up to about 62\% R@1 and 78\% MRR, confirming its effectiveness, and it also leads to improvements on existing benchmarks by up to 6\% R@1 and 11\% MRR. Moreover, an extensive evaluation validates our design choices. Code and dataset are available at https://github.com/aliabdari/Agricultural_Metaverse_Retrieval .
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BetaWeb: Towards a Blockchain-enabled Trustworthy Agentic Web</title>
<link>https://arxiv.org/abs/2508.13787</link>
<guid>https://arxiv.org/abs/2508.13787</guid>
<content:encoded><![CDATA[
arXiv:2508.13787v1 Announce Type: new 
Abstract: The rapid development of large language models (LLMs) has significantly propelled the development of artificial intelligence (AI) agents, which are increasingly evolving into diverse autonomous entities, advancing the LLM-based multi-agent systems (LaMAS). However, current agentic ecosystems remain fragmented and closed. Establishing an interconnected and scalable paradigm for Agentic AI has become a critical prerequisite. Although Agentic Web proposes an open architecture to break the ecosystem barriers, its implementation still faces core challenges such as privacy protection, data management, and value measurement. Existing centralized or semi-centralized paradigms suffer from inherent limitations, making them inadequate for supporting large-scale, heterogeneous, and cross-domain autonomous interactions. To address these challenges, this paper introduces the blockchain-enabled trustworthy Agentic Web (BetaWeb). By leveraging the inherent strengths of blockchain, BetaWeb not only offers a trustworthy and scalable infrastructure for LaMAS but also has the potential to advance the Web paradigm from Web3 (centered on data ownership) towards Web3.5, which emphasizes ownership of agent capabilities and the monetization of intelligence. Beyond a systematic examination of the BetaWeb framework, this paper presents a five-stage evolutionary roadmap, outlining the path of LaMAS from passive execution to advanced collaboration and autonomous governance. We also conduct a comparative analysis of existing products and discuss key challenges of BetaWeb from multiple perspectives. Ultimately, we argue that deep integration between blockchain and LaMAS can lay the foundation for a resilient, trustworthy, and sustainably incentivized digital ecosystem. A summary of the enabling technologies for each stage is available at https://github.com/MatZaharia/BetaWeb.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication-Efficient Federated Learning with Adaptive Number of Participants</title>
<link>https://arxiv.org/abs/2508.13803</link>
<guid>https://arxiv.org/abs/2508.13803</guid>
<content:encoded><![CDATA[
arXiv:2508.13803v1 Announce Type: new 
Abstract: Rapid scaling of deep learning models has enabled performance gains across domains, yet it introduced several challenges. Federated Learning (FL) has emerged as a promising framework to address these concerns by enabling decentralized training. Nevertheless, communication efficiency remains a key bottleneck in FL, particularly under heterogeneous and dynamic client participation. Existing methods, such as FedAvg and FedProx, or other approaches, including client selection strategies, attempt to mitigate communication costs. However, the problem of choosing the number of clients in a training round remains extremely underexplored. We introduce Intelligent Selection of Participants (ISP), an adaptive mechanism that dynamically determines the optimal number of clients per round to enhance communication efficiency without compromising model accuracy. We validate the effectiveness of ISP across diverse setups, including vision transformers, real-world ECG classification, and training with gradient compression. Our results show consistent communication savings of up to 30\% without losing the final quality. Applying ISP to different real-world ECG classification setups highlighted the selection of the number of clients as a separate task of federated learning.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming Unbalanced Training Workloads in Deep Learning with Partial Collective Operations</title>
<link>https://arxiv.org/abs/1908.04207</link>
<guid>https://arxiv.org/abs/1908.04207</guid>
<content:encoded><![CDATA[
arXiv:1908.04207v4 Announce Type: replace 
Abstract: Load imbalance pervasively exists in distributed deep learning training systems, either caused by the inherent imbalance in learned tasks or by the system itself. Traditional synchronous Stochastic Gradient Descent (SGD) achieves good accuracy for a wide variety of tasks, but relies on global synchronization to accumulate the gradients at every training step. In this paper, we propose eager-SGD, which relaxes the global synchronization for decentralized accumulation. To implement eager-SGD, we propose to use two partial collectives: solo and majority. With solo allreduce, the faster processes contribute their gradients eagerly without waiting for the slower processes, whereas with majority allreduce, at least half of the participants must contribute gradients before continuing, all without using a central parameter server. We theoretically prove the convergence of the algorithms and describe the partial collectives in detail. Experimental results on load-imbalanced environments (CIFAR-10, ImageNet, and UCF101 datasets) show that eager-SGD achieves 1.27x speedup over the state-of-the-art synchronous SGD, without losing accuracy.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking (Global) Barriers in Parallel Stochastic Optimization with Wait-Avoiding Group Averaging</title>
<link>https://arxiv.org/abs/2005.00124</link>
<guid>https://arxiv.org/abs/2005.00124</guid>
<content:encoded><![CDATA[
arXiv:2005.00124v4 Announce Type: replace 
Abstract: Deep learning at scale is dominated by communication time. Distributing samples across nodes usually yields the best performance, but poses scaling challenges due to global information dissemination and load imbalance across uneven sample lengths. State-of-the-art decentralized optimizers mitigate the problem, but require more iterations to achieve the same accuracy as their globally-communicating counterparts. We present Wait-Avoiding Group Model Averaging (WAGMA) SGD, a wait-avoiding stochastic optimizer that reduces global communication via subgroup weight exchange. The key insight is a combination of algorithmic changes to the averaging scheme and the use of a group allreduce operation. We prove the convergence of WAGMA-SGD, and empirically show that it retains convergence rates similar to Allreduce-SGD. For evaluation, we train ResNet-50 on ImageNet; Transformer for machine translation; and deep reinforcement learning for navigation at scale. Compared with state-of-the-art decentralized SGD variants, WAGMA-SGD significantly improves training throughput (e.g., 2.1x on 1,024 GPUs for reinforcement learning), and achieves the fastest time-to-solution (e.g., the highest score using the shortest training time for Transformer).
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reclaiming Constitutional Authority of Algorithmic Power</title>
<link>https://arxiv.org/abs/2508.11699</link>
<guid>https://arxiv.org/abs/2508.11699</guid>
<content:encoded><![CDATA[
arXiv:2508.11699v1 Announce Type: new 
Abstract: Whether and how to govern AI is no longer a question of technical regulation. It is a question of constitutional authority. Across jurisdictions, algorithmic systems now perform functions once reserved to public institutions: allocating welfare, determining legal status, mediating access to housing, employment, and healthcare. These are not merely administrative operations. They are acts of rule. Yet the dominant models of AI governance fail to confront this reality. The European approach centers on rights-based oversight, presenting its regulatory framework as a principled defense of human dignity. The American model relies on decentralized experimentation, treating fragmentation as a proxy for democratic legitimacy. Both, in different ways, evade the structural question: who authorizes algorithmic power, through what institutions, and on what terms. This Article offers an alternative. Drawing from early modern Reformed political thought, it reconstructs a constitutional framework grounded in covenantal authority and the right of lawful resistance. It argues that algorithmic governance must rest on three principles. First, that all public power must be lawfully delegated through participatory authorization. Second, that authority must be structured across representative communities with the standing to consent, contest, or refuse. Third, that individuals retain a constitutional right to resist systems that impose orthodoxy or erode the domain of conscience. These principles are then operationalized through doctrinal analysis of federalism, nondelegation, compelled speech, and structural accountability. On this view, the legitimacy of algorithmic governance turns not on procedural safeguards or policy design, but on whether it reflects a constitutional order in which power is authorized by the governed, constrained by law, and answerable to those it affects.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Centralized Permutation Equivariant Policy for Cooperative Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.11706</link>
<guid>https://arxiv.org/abs/2508.11706</guid>
<content:encoded><![CDATA[
arXiv:2508.11706v1 Announce Type: new 
Abstract: The Centralized Training with Decentralized Execution (CTDE) paradigm has gained significant attention in multi-agent reinforcement learning (MARL) and is the foundation of many recent algorithms. However, decentralized policies operate under partial observability and often yield suboptimal performance compared to centralized policies, while fully centralized approaches typically face scalability challenges as the number of agents increases.
  We propose Centralized Permutation Equivariant (CPE) learning, a centralized training and execution framework that employs a fully centralized policy to overcome these limitations. Our approach leverages a novel permutation equivariant architecture, Global-Local Permutation Equivariant (GLPE) networks, that is lightweight, scalable, and easy to implement. Experiments show that CPE integrates seamlessly with both value decomposition and actor-critic methods, substantially improving the performance of standard CTDE algorithms across cooperative benchmarks including MPE, SMAC, and RWARE, and matching the performance of state-of-the-art RWARE implementations.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Securing Sideways: Thwarting Lateral Movement by Implementing Active Directory Tiering</title>
<link>https://arxiv.org/abs/2508.11812</link>
<guid>https://arxiv.org/abs/2508.11812</guid>
<content:encoded><![CDATA[
arXiv:2508.11812v1 Announce Type: new 
Abstract: The advancement of computing equipment and the advances in services over the Internet has allowed corporations, higher education, and many other organizations to pursue the shared computing network environment. A requirement for shared computing environments is a centralized identity system to authenticate and authorize user access. An organization's digital identity plane is a prime target for cyber threat actors. When compromised, identities can be exploited to steal credentials, create unauthorized accounts, and manipulate permissions-enabling attackers to gain control of the network and undermine its confidentiality, availability, and integrity. Cybercrime losses reached a record of 16.6 B in the United States in 2024. For organizations using Microsoft software, Active Directory is the on-premises identity system of choice. In this article, we examine the challenge of security compromises in Active Directory (AD) environments and present effective strategies to prevent credential theft and limit lateral movement by threat actors. Our proposed approaches aim to confine the movement of compromised credentials, preventing significant privilege escalation and theft. We argue that through our illustration of real-world scenarios, tiering can halt lateral movement and advanced cyber-attacks, thus reducing ransom escalation. Our work bridges a gap in existing literature by combining technical guidelines with theoretical arguments in support of tiering, positioning it as a vital component of modern cybersecurity strategy even though it cannot function in isolation. As the hardware advances and the cloud sourced services along with AI is advancing with unprecedented speed, we think it is important for security experts and the business to work together and start designing and developing software and frameworks to classify devices automatically and accurately within the tiered structure.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Balancing Sparsity with Reliable Connectivity in Distributed Network Design with Random K-out Graphs</title>
<link>https://arxiv.org/abs/2508.11863</link>
<guid>https://arxiv.org/abs/2508.11863</guid>
<content:encoded><![CDATA[
arXiv:2508.11863v1 Announce Type: new 
Abstract: In several applications in distributed systems, an important design criterion is ensuring that the network is sparse, i.e., does not contain too many edges, while achieving reliable connectivity. Sparsity ensures communication overhead remains low, while reliable connectivity is tied to reliable communication and inference on decentralized data reservoirs and computational resources. A class of network models called random K-out graphs appear widely as a heuristic to balance connectivity and sparsity, especially in settings with limited trust, e.g., privacy-preserving aggregation of networked data in which networks are deployed. However, several questions remain regarding how to choose network parameters in response to different operational requirements, including the need to go beyond asymptotic results and the ability to model the stochastic and adversarial environments. To address this gap, we present theorems to inform the choice of network parameters that guarantee reliable connectivity in regimes where nodes can be finite or unreliable. We first derive upper and lower bounds for probability of connectivity in random K-out graphs when the number of nodes is finite. Next, we analyze the property of r-robustness, a stronger notion than connectivity that enables resilient consensus in the presence of malicious nodes. Finally, motivated by aggregation mechanisms based on pairwise masking, we model and analyze the impact of a subset of adversarial nodes, modeled as deletions, on connectivity and giant component size - metrics that are closely tied to privacy guarantees. Together, our results pave the way for end-to-end performance guarantees for a suite of algorithms for reliable inference on networks.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedUHD: Unsupervised Federated Learning using Hyperdimensional Computing</title>
<link>https://arxiv.org/abs/2508.12021</link>
<guid>https://arxiv.org/abs/2508.12021</guid>
<content:encoded><![CDATA[
arXiv:2508.12021v1 Announce Type: new 
Abstract: Unsupervised federated learning (UFL) has gained attention as a privacy-preserving, decentralized machine learning approach that eliminates the need for labor-intensive data labeling. However, UFL faces several challenges in practical applications: (1) non-independent and identically distributed (non-iid) data distribution across devices, (2) expensive computational and communication costs at the edge, and (3) vulnerability to communication noise. Previous UFL approaches have relied on deep neural networks (NN), which introduce substantial overhead in both computation and communication. In this paper, we propose FedUHD, the first UFL framework based on Hyperdimensional Computing (HDC). HDC is a brain-inspired computing scheme with lightweight training and inference operations, much smaller model size, and robustness to communication noise. FedUHD introduces two novel HDC-based designs to improve UFL performance. On the client side, a kNN-based cluster hypervector removal method addresses non-iid data samples by eliminating detrimental outliers. On the server side, a weighted HDC aggregation technique balances the non-iid data distribution across clients. Our experiments demonstrate that FedUHD achieves up to 173.6x and 612.7x better speedup and energy efficiency, respectively, in training, up to 271x lower communication cost, and 15.50% higher accuracy on average across diverse settings, along with superior robustness to various types of noise compared to state-of-the-art NN-based UFL approaches.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairness Regularization in Federated Learning</title>
<link>https://arxiv.org/abs/2508.12042</link>
<guid>https://arxiv.org/abs/2508.12042</guid>
<content:encoded><![CDATA[
arXiv:2508.12042v1 Announce Type: new 
Abstract: Federated Learning (FL) has emerged as a vital paradigm in modern machine learning that enables collaborative training across decentralized data sources without exchanging raw data. This approach not only addresses privacy concerns but also allows access to overall substantially larger and potentially more diverse datasets, without the need for centralized storage or hardware resources. However, heterogeneity in client data may cause certain clients to have disproportionate impacts on the global model, leading to disparities in the clients' performances. Fairness, therefore, becomes a crucial concern in FL and can be addressed in various ways. However, the effectiveness of existing fairness-aware methods, particularly in heterogeneous data settings, remains unclear, and the relationships between different approaches are not well understood. In this work, we focus on performance equitable fairness, which aims to minimize differences in performance across clients. We restrict our study to fairness-aware methods that explicitly regularize client losses, evaluating both existing and newly proposed approaches. We identify and theoretically explain connections between the investigated fairness methods, and empirically show that FairGrad (approximate) and FairGrad* (exact) (two variants of a gradient variance regularization method introduced here for performance equitable fairness) improve both fairness and overall model performance in heterogeneous data settings.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAPF-World: Action World Model for Multi-Agent Path Finding</title>
<link>https://arxiv.org/abs/2508.12087</link>
<guid>https://arxiv.org/abs/2508.12087</guid>
<content:encoded><![CDATA[
arXiv:2508.12087v1 Announce Type: new 
Abstract: Multi-agent path finding (MAPF) is the problem of planning conflict-free paths from the designated start locations to goal positions for multiple agents. It underlies a variety of real-world tasks, including multi-robot coordination, robot-assisted logistics, and social navigation. Recent decentralized learnable solvers have shown great promise for large-scale MAPF, especially when leveraging foundation models and large datasets. However, these agents are reactive policy models and exhibit limited modeling of environmental temporal dynamics and inter-agent dependencies, resulting in performance degradation in complex, long-term planning scenarios. To address these limitations, we propose MAPF-World, an autoregressive action world model for MAPF that unifies situation understanding and action generation, guiding decisions beyond immediate local observations. It improves situational awareness by explicitly modeling environmental dynamics, including spatial features and temporal dependencies, through future state and actions prediction. By incorporating these predicted futures, MAPF-World enables more informed, coordinated, and far-sighted decision-making, especially in complex multi-agent settings. Furthermore, we augment MAPF benchmarks by introducing an automatic map generator grounded in real-world scenarios, capturing practical map layouts for training and evaluating MAPF solvers. Extensive experiments demonstrate that MAPF-World outperforms state-of-the-art learnable solvers, showcasing superior zero-shot generalization to out-of-distribution cases. Notably, MAPF-World is trained with a 96.5% smaller model size and 92% reduced data.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ethereum Crypto Wallets under Address Poisoning: How Usable and Secure Are They?</title>
<link>https://arxiv.org/abs/2508.12107</link>
<guid>https://arxiv.org/abs/2508.12107</guid>
<content:encoded><![CDATA[
arXiv:2508.12107v1 Announce Type: new 
Abstract: Blockchain address poisoning is an emerging phishing attack that crafts "similar-looking" transfer records in the victim's transaction history, which aims to deceive victims and lure them into mistakenly transferring funds to the attacker. Recent works have shown that millions of Ethereum users were targeted and lost over 100 million US dollars.
  Ethereum crypto wallets, serving users in browsing transaction history and initiating transactions to transfer funds, play a central role in deploying countermeasures to mitigate the address poisoning attack. However, whether they have done so remains an open question. To fill the research void, in this paper, we design experiments to simulate address poisoning attacks and systematically evaluate the usability and security of 53 popular Ethereum crypto wallets. Our evaluation shows that there exist communication failures between 12 wallets and their transaction activity provider, which renders them unable to download the users' transaction history. Besides, our evaluation also shows that 16 wallets pose a high risk to their users due to displaying fake token phishing transfers. Moreover, our further analysis suggests that most wallets rely on transaction activity providers to filter out phishing transfers. However, their phishing detection capability varies. Finally, we found that only three wallets throw an explicit warning message when users attempt to transfer to the phishing address, implying a significant gap within the broader Ethereum crypto wallet community in protecting users from address poisoning attacks.
  Overall, our work shows that more efforts are needed by the Ethereum crypto wallet developer community to achieve the highest usability and security standard. Our bug reports have been acknowledged by the developer community, who are currently developing mitigation solutions.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Substituting Proof of Work in Blockchain with Training-Verified Collaborative Model Computation</title>
<link>https://arxiv.org/abs/2508.12138</link>
<guid>https://arxiv.org/abs/2508.12138</guid>
<content:encoded><![CDATA[
arXiv:2508.12138v1 Announce Type: new 
Abstract: Bitcoin's Proof of Work (PoW) mechanism, while central to achieving decentralized consensus, has long been criticized for excessive energy use and hardware inefficiencies \cite{devries2018bitcoin, truby2018decarbonizing}. This paper introduces a hybrid architecture that replaces Bitcoin's traditional PoW with a centralized, cloud-based collaborative training framework. In this model, miners contribute computing resources to train segments of horizontally scaled machine learning models on preprocessed datasets, ensuring privacy and generating meaningful outputs \cite{li2017securing}. A central server evaluates contributions using two metrics: number of parameters trained and reduction in model loss during each cycle. At the end of every cycle, a weighted lottery selects the winning miner, who receives a digitally signed certificate. This certificate serves as a verifiable substitute for PoW and grants the right to append a block to the blockchain \cite{nakamoto2008bitcoin}. By integrating digital signatures and SHA-256 hashing \cite{nist2015sha}, the system preserves blockchain integrity while redirecting energy toward productive computation. The proposed approach addresses the sustainability concerns of traditional mining by converting resource expenditure into socially valuable work, aligning security incentives with real-world computational progress.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Fomo3D to Lottery DAPP: Analysis of Ethereum-Based Gambling Applications</title>
<link>https://arxiv.org/abs/2508.12303</link>
<guid>https://arxiv.org/abs/2508.12303</guid>
<content:encoded><![CDATA[
arXiv:2508.12303v1 Announce Type: new 
Abstract: As blockchain technology advances, Ethereum based gambling decentralized applications (DApps) represent a new paradigm in online gambling. This paper examines the concepts, principles, implementation, and prospects of Ethereum based gambling DApps. First, we outline the concept and operational principles of gambling DApps. These DApps are blockchain based online lottery platforms. They utilize smart contracts to manage the entire lottery process, including issuance, betting, drawing, and prize distribution. Being decentralized, lottery DApps operate without central oversight, unlike traditional lotteries. This ensures fairness and eliminates control by any single entity. Automated smart contract execution further reduces management costs, increases profitability, and enhances game transparency and credibility. Next, we analyze an existing Ethereum based gambling DApp, detailing its technical principles, implementation, operational status, vulnerabilities, and potential solutions. We then elaborate on the implementation of lottery DApps. Smart contracts automate the entire lottery process including betting, drawing, and prize distribution. Although developing lottery DApps requires technical expertise, the expanding Ethereum ecosystem provides growing tools and frameworks, lowering development barriers. Finally, we discuss current limitations and prospects of lottery DApps. As blockchain technology and smart contracts evolve, lottery DApps are positioned to significantly transform the online lottery industry. Advantages like decentralization, automation, and transparency will likely drive broader future adoption.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Cross-Modal Style-Aware Prompt Generation</title>
<link>https://arxiv.org/abs/2508.12399</link>
<guid>https://arxiv.org/abs/2508.12399</guid>
<content:encoded><![CDATA[
arXiv:2508.12399v1 Announce Type: new 
Abstract: Prompt learning has propelled vision-language models like CLIP to excel in diverse tasks, making them ideal for federated learning due to computational efficiency. However, conventional approaches that rely solely on final-layer features miss out on rich multi-scale visual cues and domain-specific style variations in decentralized client data. To bridge this gap, we introduce FedCSAP (Federated Cross-Modal Style-Aware Prompt Generation). Our framework harnesses low, mid, and high-level features from CLIP's vision encoder alongside client-specific style indicators derived from batch-level statistics. By merging intricate visual details with textual context, FedCSAP produces robust, context-aware prompt tokens that are both distinct and non-redundant, thereby boosting generalization across seen and unseen classes. Operating within a federated learning paradigm, our approach ensures data privacy through local training and global aggregation, adeptly handling non-IID class distributions and diverse domain-specific styles. Comprehensive experiments on multiple image classification datasets confirm that FedCSAP outperforms existing federated prompt learning methods in both accuracy and overall generalization.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Oil Spill Response Through Liquid Neural Trajectory Modeling and Coordinated Marine Robotics</title>
<link>https://arxiv.org/abs/2508.12456</link>
<guid>https://arxiv.org/abs/2508.12456</guid>
<content:encoded><![CDATA[
arXiv:2508.12456v1 Announce Type: new 
Abstract: Marine oil spills pose grave environmental and economic risks, threatening marine ecosystems, coastlines, and dependent industries. Predicting and managing oil spill trajectories is highly complex, due to the interplay of physical, chemical, and environmental factors such as wind, currents, and temperature, which makes timely and effective response challenging. Accurate real-time trajectory forecasting and coordinated mitigation are vital for minimizing the impact of these disasters. This study introduces an integrated framework combining a multi-agent swarm robotics system built on the MOOS-IvP platform with Liquid Time-Constant Neural Networks (LTCNs). The proposed system fuses adaptive machine learning with autonomous marine robotics, enabling real-time prediction, dynamic tracking, and rapid response to evolving oil spills. By leveraging LTCNs--well-suited for modeling complex, time-dependent processes--the framework achieves real-time, high-accuracy forecasts of spill movement. Swarm intelligence enables decentralized, scalable, and resilient decision-making among robot agents, enhancing collective monitoring and containment efforts. Our approach was validated using data from the Deepwater Horizon spill, where the LTC-RK4 model achieved 0.96 spatial accuracy, surpassing LSTM approaches by 23%. The integration of advanced neural modeling with autonomous, coordinated robotics demonstrates substantial improvements in prediction precision, flexibility, and operational scalability. Ultimately, this research advances the state-of-the-art for sustainable, autonomous oil spill management and environmental protection by enhancing both trajectory prediction and response coordination.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Widening the Network Mitigates the Impact of Data Heterogeneity on FedAvg</title>
<link>https://arxiv.org/abs/2508.12576</link>
<guid>https://arxiv.org/abs/2508.12576</guid>
<content:encoded><![CDATA[
arXiv:2508.12576v1 Announce Type: new 
Abstract: Federated learning (FL) enables decentralized clients to train a model collaboratively without sharing local data. A key distinction between FL and centralized learning is that clients' data are non-independent and identically distributed, which poses significant challenges in training a global model that generalizes well across heterogeneous local data distributions. In this paper, we analyze the convergence of overparameterized FedAvg with gradient descent (GD). We prove that the impact of data heterogeneity diminishes as the width of neural networks increases, ultimately vanishing when the width approaches infinity. In the infinite-width regime, we further prove that both the global and local models in FedAvg behave as linear models, and that FedAvg achieves the same generalization performance as centralized learning with the same number of GD iterations. Extensive experiments validate our theoretical findings across various network architectures, loss functions, and optimization methods.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MPOCryptoML: Multi-Pattern based Off-Chain Crypto Money Laundering Detection</title>
<link>https://arxiv.org/abs/2508.12641</link>
<guid>https://arxiv.org/abs/2508.12641</guid>
<content:encoded><![CDATA[
arXiv:2508.12641v1 Announce Type: new 
Abstract: Recent advancements in money laundering detection have demonstrated the potential of using graph neural networks to capture laundering patterns accurately. However, existing models are not explicitly designed to detect the diverse patterns of off-chain cryptocurrency money laundering. Neglecting any laundering pattern introduces critical detection gaps, as each pattern reflects unique transactional structures that facilitate the obfuscation of illicit fund origins and movements. Failure to account for these patterns may result in under-detection or omission of specific laundering activities, diminishing model accuracy and allowing schemes to bypass detection. To address this gap, we propose the MPOCryptoML model to effectively detect multiple laundering patterns in cryptocurrency transactions. MPOCryptoML includes the development of a multi-source Personalized PageRank algorithm to identify random laundering patterns. Additionally, we introduce two novel algorithms by analyzing the timestamp and weight of transactions in high-volume financial networks to detect various money laundering structures, including fan-in, fan-out, bipartite, gather-scatter, and stack patterns. We further examine correlations between these patterns using a logistic regression model. An anomaly score function integrates results from each module to rank accounts by anomaly score, systematically identifying high-risk accounts. Extensive experiments on public datasets including Elliptic++, Ethereum fraud detection, and Wormhole transaction datasets validate the efficacy and efficiency of MPOCryptoML. Results show consistent performance gains, with improvements up to 9.13% in precision, up to 10.16% in recall, up to 7.63% in F1-score, and up to 10.19% in accuracy.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIT: Dimension Reduction View on Optimal NFT Rarity Meters</title>
<link>https://arxiv.org/abs/2508.12671</link>
<guid>https://arxiv.org/abs/2508.12671</guid>
<content:encoded><![CDATA[
arXiv:2508.12671v1 Announce Type: new 
Abstract: Non-fungible tokens (NFTs) have become a significant digital asset class, each uniquely representing virtual entities such as artworks. These tokens are stored in collections within smart contracts and are actively traded across platforms on Ethereum, Bitcoin, and Solana blockchains. The value of NFTs is closely tied to their distinctive characteristics that define rarity, leading to a growing interest in quantifying rarity within both industry and academia. While there are existing rarity meters for assessing NFT rarity, comparing them can be challenging without direct access to the underlying collection data. The Rating over all Rarities (ROAR) benchmark addresses this challenge by providing a standardized framework for evaluating NFT rarity. This paper explores a dimension reduction approach to rarity design, introducing new performance measures and meters, and evaluates them using the ROAR benchmark. Our contributions to the rarity meter design issue include developing an optimal rarity meter design using non-metric weighted multidimensional scaling, introducing Dissimilarity in Trades (DIT) as a performance measure inspired by dimension reduction techniques, and unveiling the non-interpretable rarity meter DIT, which demonstrates superior performance compared to existing methods.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Nomadic 6G Communication Networks: Implications on Architecture, Standardization, and Regulatory Aspects</title>
<link>https://arxiv.org/abs/2508.12710</link>
<guid>https://arxiv.org/abs/2508.12710</guid>
<content:encoded><![CDATA[
arXiv:2508.12710v1 Announce Type: new 
Abstract: The emergence of nomadic mobile communication networks for sixth-generation (6G) introduces a paradigm shift in how network infrastructure is conceptualized, deployed, and operated. Unlike traditional fixed systems, Nomadic Networks (NNs) consist of mobile and self-organizing nodes that provide radio infrastructure capabilities in motion. This paper explores the architectural implications of such systems, with a particular focus on the design and evolution of network interfaces. We analyze the requirements for inter-node communication, service discovery, and control delegation in dynamic environments. Furthermore, we examine the regulatory and licensing challenges that arise when infrastructure elements traverse jurisdictional boundaries. Based on current 6G visions and relevant research, we identify limitations in existing architectures and propose a set of interface principles tailored to nomadicity. By synthesizing findings from mobile, non-terrestrial, and organic network domains, this work contributes to the architectural foundation for future nomadic 6G communication systems and outlines directions for interface standardization in decentralized, mobile infrastructures.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Argos: A Decentralized Federated System for Detection of Traffic Signs in CAVs</title>
<link>https://arxiv.org/abs/2508.12712</link>
<guid>https://arxiv.org/abs/2508.12712</guid>
<content:encoded><![CDATA[
arXiv:2508.12712v1 Announce Type: new 
Abstract: Connected and automated vehicles generate vast amounts of sensor data daily, raising significant privacy and communication challenges for centralized machine learning approaches in perception tasks. This study presents a decentralized, federated learning framework tailored for traffic sign detection in vehicular networks to enable collaborative model training without sharing raw data. The framework partitioned traffic sign classes across vehicles for specialized local training using lightweight object detectors, aggregated model parameters via algorithms like FedProx, FedAdam and FedAVG in a simulated environment with the Flower framework, and evaluated multiple configurations including varying server rounds, local epochs, client participation fractions, and data distributions. Experiments demonstrated that increasing server rounds from 2 to 20 boosted accuracy from below 0.1 to over 0.8, moderate local epochs (8-10) provided optimal efficiency with accuracies around 0.67, higher client participation fractions enhanced generalization up to 0.83, FedProx outperformed other aggregators in handling heterogeneity, non-IID data distributions reduced performance compared to IID, and training duration primarily scaled with the number of rounds rather than aggregation strategy. We conclude that this federated approach may offer a scalable, privacy-preserving solution for real-world vehicular deployments, potentially guiding future integrations of robust aggregation and communication optimizations to advance intelligent transportation systems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedUNet: A Lightweight Additive U-Net Module for Federated Learning with Heterogeneous Models</title>
<link>https://arxiv.org/abs/2508.12740</link>
<guid>https://arxiv.org/abs/2508.12740</guid>
<content:encoded><![CDATA[
arXiv:2508.12740v1 Announce Type: new 
Abstract: Federated learning (FL) enables decentralized model training without sharing local data. However, most existing methods assume identical model architectures across clients, limiting their applicability in heterogeneous real-world environments. To address this, we propose FedUNet, a lightweight and architecture-agnostic FL framework that attaches a U-Net-inspired additive module to each client's backbone. By sharing only the compact bottleneck of the U-Net, FedUNet enables efficient knowledge transfer without structural alignment. The encoder-decoder design and skip connections in the U-Net help capture both low-level and high-level features, facilitating the extraction of clientinvariant representations. This enables cooperative learning between the backbone and the additive module with minimal communication cost. Experiment with VGG variants shows that FedUNet achieves 93.11% accuracy and 92.68% in compact form (i.e., a lightweight version of FedUNet) with only 0.89 MB low communication overhead.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tokenize Everything, But Can You Sell It? RWA Liquidity Challenges and the Road Ahead</title>
<link>https://arxiv.org/abs/2508.11651</link>
<guid>https://arxiv.org/abs/2508.11651</guid>
<content:encoded><![CDATA[
arXiv:2508.11651v1 Announce Type: cross 
Abstract: The tokenization of real-world assets (RWAs) promises to transform financial markets by enabling fractional ownership, global accessibility, and programmable settlement of traditionally illiquid assets such as real estate, private credit, and government bonds. While technical progress has been rapid, with over \$25 billion in tokenized RWAs brought on-chain as of 2025, liquidity remains a critical bottleneck. This paper investigates the gap between tokenization and tradability, drawing on recent academic research and market data from platforms such as RWA.xyz. We document that most RWA tokens exhibit low trading volumes, long holding periods, and limited investor participation, despite their potential for 24/7 global markets. Through case studies of tokenized real estate, private credit, and tokenized treasury funds, we present empirical liquidity observations that reveal low transfer activity, limited active address counts, and minimal secondary trading for most tokenized asset classes. Next, we categorize the structural barriers to liquidity, including regulatory gating, custodial concentration, whitelisting, valuation opacity, and lack of decentralized trading venues. Finally, we propose actionable pathways to improve liquidity, ranging from hybrid market structures and collateral-based liquidity to transparency enhancements and compliance innovation. Our findings contribute to the growing discourse on digital asset market microstructure and highlight that realizing the liquidity potential of RWAs requires coordinated progress across legal, technical, and institutional domains.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Robust Optimization for Swarms: A Distributed Perspective</title>
<link>https://arxiv.org/abs/2508.11799</link>
<guid>https://arxiv.org/abs/2508.11799</guid>
<content:encoded><![CDATA[
arXiv:2508.11799v1 Announce Type: cross 
Abstract: This article introduces a decentralized robust optimization framework for safe multi-agent control under uncertainty. Although stochastic noise has been the primary form of modeling uncertainty in such systems, these formulations might fall short in addressing uncertainties that are deterministic in nature or simply lack probabilistic data. To ensure safety under such scenarios, we employ the concept of robust constraints that must hold for all possible uncertainty realizations lying inside a bounded set. Nevertheless, standard robust optimization approaches become intractable due to the large number or non-convexity of the constraints involved in safe multi-agent control. To address this, we introduce novel robust reformulations that significantly reduce complexity without compromising safety. The applicability of the framework is further broadened to address both deterministic and stochastic uncertainties by incorporating robust chance constraints and distribution steering techniques. To achieve scalability, we derive a distributed approach based on the Alternating Direction Method of Multipliers (ADMM), supported by a convergence study that accounts for the underlying non-convexity. In addition, computational complexity bounds highlighting the efficiency of the proposed frameworks against standard approaches are presented. Finally, the robustness and scalability of the framework is demonstrated through extensive simulation results across diverse scenarios, including environments with nonconvex obstacles and up to 246 agents.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reputation-based PoS for the Restriction of Illicit Activities on Blockchain: Algorand Usecase</title>
<link>https://arxiv.org/abs/2112.11024</link>
<guid>https://arxiv.org/abs/2112.11024</guid>
<content:encoded><![CDATA[
arXiv:2112.11024v4 Announce Type: replace 
Abstract: In cryptocurrency-based permissionless blockchain networks, the decentralized structure enables any user to join and operate across different regions. The criminal entities exploit it by using cryptocurrency transactions on the blockchain to facilitate activities such as money laundering, gambling, and ransomware attacks. In recent times, different machine learning-based techniques can detect such criminal elements based on blockchain transaction data. However, there is no provision within the blockchain to deal with such elements. We propose a reputation-based methodology for response to the users detected carrying out the aforementioned illicit activities. We select Algorand blockchain to implement our methodology by incorporating it within the consensus protocol. The theoretical results obtained prove the restriction and exclusion of criminal elements through block proposal rejection and attenuation of the voting power as a validator for such entities. Further, we analyze the efficacy of our method and show that it puts no additional strain on the communication resources.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Near-Optimal Sparse Allreduce for Distributed Deep Learning</title>
<link>https://arxiv.org/abs/2201.07598</link>
<guid>https://arxiv.org/abs/2201.07598</guid>
<content:encoded><![CDATA[
arXiv:2201.07598v3 Announce Type: replace 
Abstract: Communication overhead is one of the major obstacles to train large deep learning models at scale. Gradient sparsification is a promising technique to reduce the communication volume. However, it is very challenging to obtain real performance improvement because of (1) the difficulty of achieving an scalable and efficient sparse allreduce algorithm and (2) the sparsification overhead. This paper proposes O$k$-Top$k$, a scheme for distributed training with sparse gradients. O$k$-Top$k$ integrates a novel sparse allreduce algorithm (less than 6$k$ communication volume which is asymptotically optimal) with the decentralized parallel Stochastic Gradient Descent (SGD) optimizer, and its convergence is proved. To reduce the sparsification overhead, O$k$-Top$k$ efficiently selects the top-$k$ gradient values according to an estimated threshold. Evaluations are conducted on the Piz Daint supercomputer with neural network models from different deep learning domains. Empirical results show that O$k$-Top$k$ achieves similar model accuracy to dense allreduce. Compared with the optimized dense and the state-of-the-art sparse allreduces, O$k$-Top$k$ is more scalable and significantly improves training throughput (e.g., 3.29x-12.95x improvement for BERT on 256 GPUs).
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>It Takes Two: A Peer-Prediction Solution for Blockchain Verifier's Dilemma</title>
<link>https://arxiv.org/abs/2406.01794</link>
<guid>https://arxiv.org/abs/2406.01794</guid>
<content:encoded><![CDATA[
arXiv:2406.01794v4 Announce Type: replace 
Abstract: The security of blockchain systems is fundamentally based on the decentralized consensus in which the majority of parties behave honestly, and the content verification process is essential to maintaining the robustness of blockchain systems. However, the phenomenon that a rational verifier may not have the incentive to honestly perform the costly verification, referred to as the Verifier's Dilemma, could incentivize lazy reporting and undermine the fundamental security of blockchain systems, particularly for verification-expensive decentralized AI applications.
  In this paper, we initiate the research with the development of a Byzantine-robust peer prediction framework towards the design of one-phase Bayesian truthful mechanisms for the decentralized verification games among multiple verifiers, incentivizing all verifiers to perform honest verification without access to the ground truth even in the presence of noisy observations, malicious players and inaccurate priors in the verification process, proposing the compactness criteria that ensures such robustness guarantees. With robust incentive guarantees and budget efficiency, our study provides a framework of incentive design for decentralized verification protocols that enhances the security and robustness of the blockchain, decentralized AI, and potentially other decentralized systems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Federated Learning for Semantic Datasets: Federated Scene Graph Generation</title>
<link>https://arxiv.org/abs/2412.10436</link>
<guid>https://arxiv.org/abs/2412.10436</guid>
<content:encoded><![CDATA[
arXiv:2412.10436v2 Announce Type: replace 
Abstract: Federated learning (FL) enables decentralized training while preserving data privacy, yet existing FL benchmarks address relatively simple classification tasks, where each sample is annotated with a one-hot label. However, little attention has been paid to demonstrating an FL benchmark that handles complicated semantics, where each sample encompasses diverse semantic information, such as relations between objects. Because the existing benchmarks are designed to distribute data in a narrow view of a single semantic, managing the complicated \textit{semantic heterogeneity} across clients when formalizing FL benchmarks is non-trivial. In this paper, we propose a benchmark process to establish an FL benchmark with controllable semantic heterogeneity across clients: two key steps are (i) data clustering with semantics and (ii) data distributing via controllable semantic heterogeneity across clients. As a proof of concept, we construct a federated PSG benchmark, demonstrating the efficacy of the existing PSG methods in an FL setting with controllable semantic heterogeneity of scene graphs. We also present the effectiveness of our benchmark by applying robust federated learning algorithms to data heterogeneity to show increased performance. To our knowledge, this is the first benchmark framework that enables federated learning and its evaluation for multi-semantic vision tasks under the controlled semantic heterogeneity. Our code is available at \textit{https://github.com/Seung-B/FL-PSG}.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blockchain Data Analytics: Review and Challenges</title>
<link>https://arxiv.org/abs/2503.09165</link>
<guid>https://arxiv.org/abs/2503.09165</guid>
<content:encoded><![CDATA[
arXiv:2503.09165v2 Announce Type: replace 
Abstract: The integration of blockchain technology with data analytics is essential for extracting insights in the cryptocurrency space. Although academic literature on blockchain data analytics is limited, various industry solutions have emerged to address these needs. This paper provides a comprehensive literature review, drawing from both academic research and industry applications. We classify blockchain analytics tools into categories such as block explorers, on-chain data providers, research platforms, and crypto market data providers. Additionally, we discuss the challenges associated with blockchain data analytics, including data accessibility, scalability, accuracy, and interoperability. Our findings emphasize the importance of bridging academic research and industry innovations to advance blockchain data analytics.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An overview of the efficiency and censorship-resistance guarantees of widely-used consensus protocols</title>
<link>https://arxiv.org/abs/2504.03588</link>
<guid>https://arxiv.org/abs/2504.03588</guid>
<content:encoded><![CDATA[
arXiv:2504.03588v2 Announce Type: replace 
Abstract: Censorship resistance with short-term inclusion guarantees is an important feature of decentralized systems, missing from many state-of-the-art and even deployed consensus protocols. In leader-based protocols the leader arbitrarily selects the transactions to be included in the new block, and so does a block builder in protocols such as Bitcoin and Ethereum.
  In a different line of work, since the redundancy of consensus for implementing distributed payments was formally proven, consensusless protocols have been described in theory and deployed in the real world. This has resulted in blockchains and payment systems that are more efficient, and at the same time avoid the centralized role of a leader or block builder.
  In this report we review existing consensus and consensusless protocols with regard to their censorship-resistance, efficiency, and other properties. Moreover, we present an approach for new constructions with these properties in mind, building on existing leader-based protocols.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DFed-SST: Building Semantic- and Structure-aware Topologies for Decentralized Federated Graph Learning</title>
<link>https://arxiv.org/abs/2508.11530</link>
<guid>https://arxiv.org/abs/2508.11530</guid>
<content:encoded><![CDATA[
arXiv:2508.11530v1 Announce Type: new 
Abstract: Decentralized Federated Learning (DFL) has emerged as a robust distributed paradigm that circumvents the single-point-of-failure and communication bottleneck risks of centralized architectures. However, a significant challenge arises as existing DFL optimization strategies, primarily designed for tasks such as computer vision, fail to address the unique topological information inherent in the local subgraph. Notably, while Federated Graph Learning (FGL) is tailored for graph data, it is predominantly implemented in a centralized server-client model, failing to leverage the benefits of decentralization.To bridge this gap, we propose DFed-SST, a decentralized federated graph learning framework with adaptive communication. The core of our method is a dual-topology adaptive communication mechanism that leverages the unique topological features of each client's local subgraph to dynamically construct and optimize the inter-client communication topology. This allows our framework to guide model aggregation efficiently in the face of heterogeneity. Extensive experiments on eight real-world datasets consistently demonstrate the superiority of DFed-SST, achieving 3.26% improvement in average accuracy over baseline methods.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Price of Cheaper Data: Measuring the Strategic Inefficiencies in the Post-EIP-4844 Ethereum Market</title>
<link>https://arxiv.org/abs/2411.03892</link>
<guid>https://arxiv.org/abs/2411.03892</guid>
<content:encoded><![CDATA[
arXiv:2411.03892v3 Announce Type: replace 
Abstract: High transaction fees on Ethereum have long hindered its scalability and user adoption. The recent Ethereum EIP-4844 upgrade aims to alleviate the scalability issue by introducing the blob, a new data structure for Layer-2 rollups. Instead of using expensive blockchain storage, blobs provide a cheaper, separate data layer with its own fee market, which drastically lowers data availability costs. This change, while lowering transaction costs, has created a new high-stakes economic game for block builders and rollups. However, the dynamics of this game remain poorly understood. In this paper, we conduct the first large-scale empirical analysis of the post-EIP-4844 ecosystem, leveraging a dataset of 319.5 million transactions, including 1.3 million blob-carrying type-3 transactions. Our analysis demonstrates clear evidence of a structural shift towards the utilization of cheap blobs over expensive transactions for Layer-2 data posting: while average block size grew 2.5 times, the space consumed by expensive transactions in the public mempool shrank from over 150 KB to just 30 KB. Yet, this scalability success masks widespread economic inefficiencies on both sides of the market. On the builder side, 29.48% of blocks containing blobs are constructed sub-optimally, earning less revenue than that could be achieved otherwise. On the rollup side, we identify that flawed submission strategies have led to $186.92$ ETH in direct losses and average inclusion delays of over 19 seconds. Moving beyond characterization, our work offers actionable solutions. For builders, we develop an optimal pricing model derived from a formal first-price auction framework, allowing builders to make provably profitable inclusion decisions. For rollups, we prove that batching multiple blobs into a single transaction is a dominant, utility-maximizing strategy.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized State Estimation and Opacity Verification Based on Partially Ordered Observation Sequences</title>
<link>https://arxiv.org/abs/2502.10367</link>
<guid>https://arxiv.org/abs/2502.10367</guid>
<content:encoded><![CDATA[
arXiv:2502.10367v3 Announce Type: replace 
Abstract: In this paper, we investigate state estimation and opacity verification problems within a decentralized observation architecture. Specifically, we consider a discrete event system whose behavior is recorded by a set of observation sites. These sites transmit the partially ordered sequences of observations that they record to a coordinator whenever a synchronization occurs. To properly analyze the system behavior from the coordinator's viewpoint, we first introduce the notion of a Complete Synchronizing Sequence structure (CSS structure), which concisely captures the state evolution of each system state upon different information provided by the observation sites. Based on the CSS structure, we then construct corresponding current-state and initial-state estimators for offline state estimation at the coordinator. When used to verify state-isolation properties under this decentralized architecture, the use of CSS structure demonstrates a significant reduction in complexity compared with existing approaches in the literature. In particular, we discuss how to verify initial-state opacity at the coordinator, as well as a novel opacity notion, namely current-state-at-synchronization opacity.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe Multi-Robotic Arm Interaction via 3D Convex Shapes</title>
<link>https://arxiv.org/abs/2503.11791</link>
<guid>https://arxiv.org/abs/2503.11791</guid>
<content:encoded><![CDATA[
arXiv:2503.11791v2 Announce Type: replace 
Abstract: Inter-robot collisions pose a significant safety risk when multiple robotic arms operate in close proximity. We present an online collision avoidance methodology leveraging 3D convex shape-based High-Order Control Barrier Functions (HOCBFs) to address this issue. While prior works focused on using Control Barrier Functions (CBFs) for human-robotic arm and single-arm collision avoidance, we explore the problem of collision avoidance between multiple robotic arms operating in a shared space. In our methodology, we utilize the proposed HOCBFs as centralized and decentralized safety filters. These safety filters are compatible with many nominal controllers and ensure safety without significantly restricting the robots' workspace. A key challenge in implementing these filters is the computational overhead caused by the large number of safety constraints and the computation of a Hessian matrix per constraint. We address this challenge by employing numerical differentiation methods to approximate computationally intensive terms. The effectiveness of our method is demonstrated through extensive simulation studies and real-world experiments with Franka Research 3 robotic arms. The project video is available at this link.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain</title>
<link>https://arxiv.org/abs/2508.09993</link>
<guid>https://arxiv.org/abs/2508.09993</guid>
<content:encoded><![CDATA[
arXiv:2508.09993v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in realworld applications, yet concerns about their fairness persist especially in highstakes domains like criminal justice, education, healthcare, and finance. This paper introduces transparent evaluation protocol for benchmarking the fairness of opensource LLMs using smart contracts on the Internet Computer Protocol (ICP) blockchain (Foundation, 2023). Our method ensures verifiable, immutable, and reproducible evaluations by executing onchain HTTP requests to hosted Hugging Face endpoints and storing datasets, prompts, and metrics directly onchain. We benchmark the Llama, DeepSeek, and Mistral models on the PISA dataset for academic performance prediction (OECD, 2018), a dataset suitable for fairness evaluation using statistical parity and equal opportunity metrics (Hardt et al., 2016). We also evaluate structured Context Association Metrics derived from the StereoSet dataset (Nadeem et al., 2020) to measure social bias in contextual associations. We further extend our analysis with a multilingual evaluation across English, Spanish, and Portuguese using the Kaleidoscope benchmark (Salazar et al., 2025), revealing cross-linguistic disparities. All code and results are open source, enabling community audits and longitudinal fairness tracking across model versions.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Robust Pipeline for Differentially Private Federated Learning on Imbalanced Clinical Data using SMOTETomek and FedProx</title>
<link>https://arxiv.org/abs/2508.10017</link>
<guid>https://arxiv.org/abs/2508.10017</guid>
<content:encoded><![CDATA[
arXiv:2508.10017v1 Announce Type: new 
Abstract: Federated Learning (FL) presents a groundbreaking approach for collaborative health research, allowing model training on decentralized data while safeguarding patient privacy. FL offers formal security guarantees when combined with Differential Privacy (DP). The integration of these technologies, however, introduces a significant trade-off between privacy and clinical utility, a challenge further complicated by the severe class imbalance often present in medical datasets. The research presented herein addresses these interconnected issues through a systematic, multi-stage analysis. An FL framework was implemented for cardiovascular risk prediction, where initial experiments showed that standard methods struggled with imbalanced data, resulting in a recall of zero. To overcome such a limitation, we first integrated the hybrid Synthetic Minority Over-sampling Technique with Tomek Links (SMOTETomek) at the client level, successfully developing a clinically useful model. Subsequently, the framework was optimized for non-IID data using a tuned FedProx algorithm. Our final results reveal a clear, non-linear trade-off between the privacy budget (epsilon) and model recall, with the optimized FedProx consistently out-performing standard FedAvg. An optimal operational region was identified on the privacy-utility frontier, where strong privacy guarantees (with epsilon 9.0) can be achieved while maintaining high clinical utility (recall greater than 77%). Ultimately, our study provides a practical methodological blueprint for creating effective, secure, and accurate diagnostic tools that can be applied to real-world, heterogeneous healthcare data.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FIDELIS: Blockchain-Enabled Protection Against Poisoning Attacks in Federated Learning</title>
<link>https://arxiv.org/abs/2508.10042</link>
<guid>https://arxiv.org/abs/2508.10042</guid>
<content:encoded><![CDATA[
arXiv:2508.10042v1 Announce Type: new 
Abstract: Federated learning enhances traditional deep learning by enabling the joint training of a model with the use of IoT device's private data. It ensures privacy for clients, but is susceptible to data poisoning attacks during training that degrade model performance and integrity. Current poisoning detection methods in federated learning lack a standardized detection method or take significant liberties with trust. In this paper, we present \Sys, a novel blockchain-enabled poison detection framework in federated learning. The framework decentralizes the role of the global server across participating clients. We introduce a judge model used to detect data poisoning in model updates. The judge model is produced by each client and verified to reach consensus on a single judge model. We implement our solution to show \Sys is robust against data poisoning attacks and the creation of our judge model is scalable.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NetMoniAI: An Agentic AI Framework for Network Security &amp; Monitoring</title>
<link>https://arxiv.org/abs/2508.10052</link>
<guid>https://arxiv.org/abs/2508.10052</guid>
<content:encoded><![CDATA[
arXiv:2508.10052v1 Announce Type: new 
Abstract: In this paper, we present NetMoniAI, an agentic AI framework for automatic network monitoring and security that integrates decentralized analysis with lightweight centralized coordination. The framework consists of two layers: autonomous micro-agents at each node perform local traffic analysis and anomaly detection. A central controller then aggregates insights across nodes to detect coordinated attacks and maintain system-wide situational awareness. We evaluated NetMoniAI on a local micro-testbed and through NS-3 simulations. Results confirm that the two-tier agentic-AI design scales under resource constraints, reduces redundancy, and improves response time without compromising accuracy. To facilitate broader adoption and reproducibility, the complete framework is available as open source. This enables researchers and practitioners to replicate, validate, and extend it across diverse network environments and threat scenarios. Github link: https://github.com/pzambare3/NetMoniAI
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Architecture for Distributed Digital Identities in the Physical World</title>
<link>https://arxiv.org/abs/2508.10185</link>
<guid>https://arxiv.org/abs/2508.10185</guid>
<content:encoded><![CDATA[
arXiv:2508.10185v1 Announce Type: new 
Abstract: Digital identities are increasingly important for mediating not only digital but also physical service transactions. Managing such identities through centralized providers can cause both availability and privacy concerns: single points of failure and control are ideal targets for global attacks on technical, organizational, or legal fronts. We design, analyze, and build a distributed digital identity architecture for physical world transactions in common scenarios like unlocking doors, public transport, or crossing country borders. This architecture combines (biometric and other) sensors, (established and upcoming) identity authorities, attribute verifiers, and a new core component we call the \emph{Personal Identity Agent (PIA)} that represents individuals with their identity attributes in the digital domain. All transactions are conducted in a completely decentralized manner, and the components for which we currently assume central coordination are optional and only used for assisting with service discovery and latency reduction. We present a first protocol between these parties and formally verify that it achieves relevant security properties based on a realistic threat model including strong global adversaries. A proof-of-concept implementation demonstrates practical feasibility of both architecture and initial protocol for applications that can tolerate end-to-end latencies in the range of a few seconds.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphFedMIG: Tackling Class Imbalance in Federated Graph Learning via Mutual Information-Guided Generation</title>
<link>https://arxiv.org/abs/2508.10471</link>
<guid>https://arxiv.org/abs/2508.10471</guid>
<content:encoded><![CDATA[
arXiv:2508.10471v1 Announce Type: new 
Abstract: Federated graph learning (FGL) enables multiple clients to collaboratively train powerful graph neural networks without sharing their private, decentralized graph data. Inherited from generic federated learning, FGL is critically challenged by statistical heterogeneity, where non-IID data distributions across clients can severely impair model performance. A particularly destructive form of this is class imbalance, which causes the global model to become biased towards majority classes and fail at identifying rare but critical events. This issue is exacerbated in FGL, as nodes from a minority class are often surrounded by biased neighborhood information, hindering the learning of expressive embeddings. To grapple with this challenge, we propose GraphFedMIG, a novel FGL framework that reframes the problem as a federated generative data augmentation task. GraphFedMIG employs a hierarchical generative adversarial network where each client trains a local generator to synthesize high-fidelity feature representations. To provide tailored supervision, clients are grouped into clusters, each sharing a dedicated discriminator. Crucially, the framework designs a mutual information-guided mechanism to steer the evolution of these client generators. By calculating each client's unique informational value, this mechanism corrects the local generator parameters, ensuring that subsequent rounds of mutual information-guided generation are focused on producing high-value, minority-class features. We conduct extensive experiments on four real-world datasets, and the results demonstrate the superiority of the proposed GraphFedMIG compared with other baselines.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlDBaran: Towards Blazingly Fast State Commitments for Blockchains</title>
<link>https://arxiv.org/abs/2508.10493</link>
<guid>https://arxiv.org/abs/2508.10493</guid>
<content:encoded><![CDATA[
arXiv:2508.10493v1 Announce Type: new 
Abstract: The fundamental basis for maintaining integrity within contemporary blockchain systems is provided by authenticated databases. Our analysis indicates that a significant portion of the approaches applied in this domain fail to sufficiently meet the stringent requirements of systems processing transactions at rates of multi-million TPS. AlDBaran signifies a substantial advancement in authenticated databases. By eliminating disk I/O operations from the critical path, implementing prefetching strategies, and refining the update mechanism of the Merkle tree, we have engineered an authenticated data structure capable of handling state updates efficiently at a network throughput of 50 Gbps. This throughput capacity significantly surpasses any empirically documented blockchain throughput, guaranteeing the ability of even the most high-throughput blockchains to generate state commitments effectively.
  AlDBaran provides support for historical state proofs, which facilitates a wide array of novel applications. For instance, the deployment of AlDBaran could enable blockchains that do not currently support state commitments to offer functionalities for light clients and/or implement rollups.
  When benchmarked against alternative authenticated data structure projects, AlDBaran exhibits superior performance and simplicity. In particular, AlDBaran achieves speeds of approximately 48 million updates per second using an identical machine configuration. This characteristic renders AlDBaran an attractive solution for resource-limited environments, as its historical data capabilities can be modularly isolated (and deactivated), which further enhances performance. On consumer-level portable hardware, it achieves approximately 8 million updates/s in an in-memory setting and 5 million updates/s with snapshots at sub-second intervals, illustrating compelling and cost-effective scalability.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Solidity Evolution Gaps: An LLM-Enhanced Approach for Smart Contract Compilation Error Resolution</title>
<link>https://arxiv.org/abs/2508.10517</link>
<guid>https://arxiv.org/abs/2508.10517</guid>
<content:encoded><![CDATA[
arXiv:2508.10517v1 Announce Type: new 
Abstract: Solidity, the dominant smart contract language for Ethereum, has rapidly evolved with frequent version updates to enhance security, functionality, and developer experience. However, these continual changes introduce significant challenges, particularly in compilation errors, code migration, and maintenance. Therefore, we conduct an empirical study to investigate the challenges in the Solidity version evolution and reveal that 81.68% of examined contracts encounter errors when compiled across different versions, with 86.92% of compilation errors.
  To mitigate these challenges, we conducted a systematic evaluation of large language models (LLMs) for resolving Solidity compilation errors during version migrations. Our empirical analysis across both open-source (LLaMA3, DeepSeek) and closed-source (GPT-4o, GPT-3.5-turbo) LLMs reveals that although these models exhibit error repair capabilities, their effectiveness diminishes significantly for semantic-level issues and shows strong dependency on prompt engineering strategies. This underscores the critical need for domain-specific adaptation in developing reliable LLM-based repair systems for smart contracts.
  Building upon these insights, we introduce SMCFIXER, a novel framework that systematically integrates expert knowledge retrieval with LLM-based repair mechanisms for Solidity compilation error resolution. The architecture comprises three core phases: (1) context-aware code slicing that extracts relevant error information; (2) expert knowledge retrieval from official documentation; and (3) iterative patch generation for Solidity migration. Experimental validation across Solidity version migrations demonstrates our approach's statistically significant 24.24% improvement over baseline GPT-4o on real-world datasets, achieving near-perfect 96.97% accuracy.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Robust Optimization Approach for Demand Response Participation of Fixed-Frequency Air Conditioners</title>
<link>https://arxiv.org/abs/2508.10679</link>
<guid>https://arxiv.org/abs/2508.10679</guid>
<content:encoded><![CDATA[
arXiv:2508.10679v1 Announce Type: new 
Abstract: With the continuous increase in the penetration of renewable energy in the emerging power systems, the pressure on system peak regulation has been significantly intensified. Against this backdrop, demand side resources particularly air conditioning loads have garnered considerable attention for their substantial regulation potential and fast response capabilities, making them promising candidates for providing auxiliary peak shaving services. This study focuses on fixed frequency air conditioners (FFACs) and proposes an optimization model and solution method for their participation in demand response (DR) programs. First, a probabilistic response model for FFACs is developed based on the Markov assumption. Second, by sampling this probabilistic model, the aggregate power consumption of an FFAC cluster under decentralized control is obtained. Subsequently, a robust optimization model is formulated to maximize the profit of an aggregator managing the FFAC cluster during DR events, taking into account the aggregated response power. The model explicitly considers temperature uncertainty to ensure user comfort in a robust sense. Finally, leveraging the structure of the proposed model, it is reformulated as a mixed-integer linear programming (MILP) problem and solved using a commercial optimization solver. Simulation results validate the effectiveness of the proposed model and solution approach.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Intelligent Infrastructure as a Foundation for Modern Science</title>
<link>https://arxiv.org/abs/2508.10051</link>
<guid>https://arxiv.org/abs/2508.10051</guid>
<content:encoded><![CDATA[
arXiv:2508.10051v1 Announce Type: cross 
Abstract: Infrastructure shapes societies and scientific discovery. Traditional scientific infrastructure, often static and fragmented, leads to issues like data silos, lack of interoperability and reproducibility, and unsustainable short-lived solutions. Our current technical inability and social reticence to connect and coordinate scientific research and engineering leads to inefficiencies and impedes progress. With AI technologies changing how we interact with the world around us, there is an opportunity to transform scientific processes. Neuroscience's exponential growth of multimodal and multiscale data, and urgent clinical relevance demand an infrastructure itself learns, coordinates, and improves. Using neuroscience as a stress test, this perspective argues for a paradigm shift: infrastructure must evolve into a dynamic, AI-aligned ecosystem to accelerate science. Building on several existing principles for data, collective benefit, and digital repositories, I recommend operational guidelines for implementing them to create this dynamic ecosystem, aiming to foster a decentralized, self-learning, and self-correcting system where humans and AI can collaborate seamlessly. Addressing the chronic underfunding of scientific infrastructure, acknowledging diverse contributions beyond publications, and coordinating global efforts are critical steps for this transformation. By prioritizing an intelligent infrastructure as a central scientific instrument for knowledge generation, we can overcome current limitations, accelerate discovery, ensure reproducibility and ethical practices, and ultimately translate neuroscientific understanding into tangible societal benefits, setting a blueprint for other scientific domains.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-preserving Blockchain-enabled Parametric Insurance via Remote Sensing and IoT</title>
<link>https://arxiv.org/abs/2305.08384</link>
<guid>https://arxiv.org/abs/2305.08384</guid>
<content:encoded><![CDATA[
arXiv:2305.08384v3 Announce Type: replace 
Abstract: Traditional Insurance, a popular approach of financial risk management, has suffered from the issues of high operational costs, opaqueness, inefficiency and a lack of trust. Recently, blockchain-enabled "parametric insurance" through authorized data sources (e.g., remote sensing and IoT) aims to overcome these issues by automating the underwriting and claim processes of insurance policies on a blockchain. However, the openness of blockchain platforms raises a concern of user privacy, as the private user data in insurance claims on a blockchain may be exposed to outsiders. In this paper, we propose a privacy-preserving parametric insurance framework based on succinct zero-knowledge proofs (zk-SNARKs), whereby an insuree submits a zero-knowledge proof (without revealing any private data) for the validity of an insurance claim and the authenticity of its data sources to a blockchain for transparent verification. Moreover, we extend the recent zk-SNARKs to support robust privacy protection for multiple heterogeneous data sources and improve its efficiency to cut the incurred gas cost by 80%. As a proof-of-concept, we implemented a working prototype of bushfire parametric insurance on real-world blockchain platform Ethereum, and present extensive empirical evaluations.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Parallel Execution of Blockchain Transactions Leveraging Conflict Specifications</title>
<link>https://arxiv.org/abs/2503.03203</link>
<guid>https://arxiv.org/abs/2503.03203</guid>
<content:encoded><![CDATA[
arXiv:2503.03203v2 Announce Type: replace 
Abstract: Parallel execution of smart contract transactions in large multicore architectures is critical for higher efficiency and improved throughput. The main bottleneck for maximizing the throughput of a node through parallel execution is transaction conflict resolution: when two transactions interact with the same data, like an account balance, their order matters. Imagine one transaction sends tokens from account A to account B, and another tries to send tokens from account B to account C. If the second transaction happens before the first one, the token balance in account B might be wrong, causing the entire system to break. Conflicts like these must be managed carefully, or you end up with an inconsistent, unusable blockchain state.
  Traditional software transactional memory (STM) has been identified as a possible abstraction for the concurrent execution of transactions within a block, with Block-STM pioneering its application for efficient blockchain transaction processing on multicore validator nodes. This paper presents a parallel execution methodology that leverages conflict specification information of the transactions for block transactional memory (BTM) algorithms. Our experimental analysis, conducted over synthetic transactional workloads and real-world blocks, demonstrates that BTMs leveraging conflict specifications outperform their plain counterparts on both EVM and MoveVM. Our proposed BTM implementations achieve up to 1.75x speedup over sequential execution and outperform the state-of-the-art Parallel-EVM (PEVM) execution by up to 1.33x across synthetic workloads.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedMP: Tackling Medical Feature Heterogeneity in Federated Learning from a Manifold Perspective</title>
<link>https://arxiv.org/abs/2508.09174</link>
<guid>https://arxiv.org/abs/2508.09174</guid>
<content:encoded><![CDATA[
arXiv:2508.09174v1 Announce Type: new 
Abstract: Federated learning (FL) is a decentralized machine learning paradigm in which multiple clients collaboratively train a shared model without sharing their local private data. However, real-world applications of FL frequently encounter challenges arising from the non-identically and independently distributed (non-IID) local datasets across participating clients, which is particularly pronounced in the field of medical imaging, where shifts in image feature distributions significantly hinder the global model's convergence and performance. To address this challenge, we propose FedMP, a novel method designed to enhance FL under non-IID scenarios. FedMP employs stochastic feature manifold completion to enrich the training space of individual client classifiers, and leverages class-prototypes to guide the alignment of feature manifolds across clients within semantically consistent subspaces, facilitating the construction of more distinct decision boundaries. We validate the effectiveness of FedMP on multiple medical imaging datasets, including those with real-world multi-center distributions, as well as on a multi-domain natural image dataset. The experimental results demonstrate that FedMP outperforms existing FL algorithms. Additionally, we analyze the impact of manifold dimensionality, communication efficiency, and privacy implications of feature exposure in our method.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Term Client Selection for Federated Learning with Non-IID Data: A Truthful Auction Approach</title>
<link>https://arxiv.org/abs/2508.09181</link>
<guid>https://arxiv.org/abs/2508.09181</guid>
<content:encoded><![CDATA[
arXiv:2508.09181v1 Announce Type: new 
Abstract: Federated learning (FL) provides a decentralized framework that enables universal model training through collaborative efforts on mobile nodes, such as smart vehicles in the Internet of Vehicles (IoV). Each smart vehicle acts as a mobile client, contributing to the process without uploading local data. This method leverages non-independent and identically distributed (non-IID) training data from different vehicles, influenced by various driving patterns and environmental conditions, which can significantly impact model convergence and accuracy. Although client selection can be a feasible solution for non-IID issues, it faces challenges related to selection metrics. Traditional metrics evaluate client data quality independently per round and require client selection after all clients complete local training, leading to resource wastage from unused training results. In the IoV context, where vehicles have limited connectivity and computational resources, information asymmetry in client selection risks clients submitting false information, potentially making the selection ineffective. To tackle these challenges, we propose a novel Long-term Client-Selection Federated Learning based on Truthful Auction (LCSFLA). This scheme maximizes social welfare with consideration of long-term data quality using a new assessment mechanism and energy costs, and the advised auction mechanism with a deposit requirement incentivizes client participation and ensures information truthfulness. We theoretically prove the incentive compatibility and individual rationality of the advised incentive mechanism. Experimental results on various datasets, including those from IoV scenarios, demonstrate its effectiveness in mitigating performance degradation caused by non-IID data.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Learning for Speeding Up Large Model Inference in Decentralized Environments</title>
<link>https://arxiv.org/abs/2508.09194</link>
<guid>https://arxiv.org/abs/2508.09194</guid>
<content:encoded><![CDATA[
arXiv:2508.09194v1 Announce Type: new 
Abstract: The deployment of large-scale models, such as large language models (LLMs), incurs substantial costs due to their computational demands. To mitigate these costs and address challenges related to scalability and data security, there is a growing shift towards decentralized systems for model deployment, where choosing efficient inference acceleration schemes become crucial to manage computational resources effectively and enhance system responsiveness. In this work, we address the challenge of selecting optimal acceleration methods in decentralized systems by introducing a meta-learning-based framework. This framework automates the selection process by learning from historical performance data of various acceleration techniques across different tasks. Unlike traditional methods that rely on random selection or expert intuition, our approach systematically identifies the best acceleration strategies based on the specific characteristics of each task. We demonstrate that our meta-learning framework not only streamlines the decision-making process but also consistently outperforms conventional methods in terms of efficiency and performance. Our results highlight the potential of inference acceleration in decentralized AI systems, offering a path towards more democratic and economically feasible artificial intelligence solutions.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blockchain Network Analysis using Quantum Inspired Graph Neural Networks &amp; Ensemble Models</title>
<link>https://arxiv.org/abs/2508.09237</link>
<guid>https://arxiv.org/abs/2508.09237</guid>
<content:encoded><![CDATA[
arXiv:2508.09237v1 Announce Type: new 
Abstract: In the rapidly evolving domain of financial technology, the detection of illicit transactions within blockchain networks remains a critical challenge, necessitating robust and innovative solutions. This work proposes a novel approach by combining Quantum Inspired Graph Neural Networks (QI-GNN) with flexibility of choice of an Ensemble Model using QBoost or a classic model such as Random Forrest Classifier. This system is tailored specifically for blockchain network analysis in anti-money laundering (AML) efforts. Our methodology to design this system incorporates a novel component, a Canonical Polyadic (CP) decomposition layer within the graph neural network framework, enhancing its capability to process and analyze complex data structures efficiently. Our technical approach has undergone rigorous evaluation against classical machine learning implementations, achieving an F2 score of 74.8% in detecting fraudulent transactions. These results highlight the potential of quantum-inspired techniques, supplemented by the structural advancements of the CP layer, to not only match but potentially exceed traditional methods in complex network analysis for financial security. The findings advocate for a broader adoption and further exploration of quantum-inspired algorithms within the financial sector to effectively combat fraud.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Weather Forecasting via Distributed Machine Learning and Blockchain-Based Model Validation</title>
<link>https://arxiv.org/abs/2508.09299</link>
<guid>https://arxiv.org/abs/2508.09299</guid>
<content:encoded><![CDATA[
arXiv:2508.09299v1 Announce Type: new 
Abstract: Weather forecasting plays a vital role in disaster preparedness, agriculture, and resource management, yet current centralized forecasting systems are increasingly strained by security vulnerabilities, limited scalability, and susceptibility to single points of failure. To address these challenges, we propose a decentralized weather forecasting framework that integrates Federated Learning (FL) with blockchain technology. FL enables collaborative model training without exposing sensitive local data; this approach enhances privacy and reduces data transfer overhead. Meanwhile, the Ethereum blockchain ensures transparent and dependable verification of model updates. To further enhance the system's security, we introduce a reputation-based voting mechanism that assesses the trustworthiness of submitted models while utilizing the Interplanetary File System (IPFS) for efficient off-chain storage. Experimental results demonstrate that our approach not only improves forecasting accuracy but also enhances system resilience and scalability, making it a viable candidate for deployment in real-world, security-critical environments.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Formal Methods to Data-Driven Safety Certificates of Unknown Large-Scale Networks</title>
<link>https://arxiv.org/abs/2508.09520</link>
<guid>https://arxiv.org/abs/2508.09520</guid>
<content:encoded><![CDATA[
arXiv:2508.09520v1 Announce Type: new 
Abstract: In this work, we propose a data-driven scheme within a compositional framework with noisy data to design robust safety controllers in a fully decentralized fashion for large-scale interconnected networks with unknown mathematical dynamics. Despite the network's high dimensionality and the inherent complexity of its unknown model, which make it intractable, our approach effectively addresses these challenges by (i) treating the network as a composition of smaller subsystems, and (ii) collecting noisy data from each subsystem's trajectory to design a control sub-barrier certificate (CSBC) and its corresponding local controller. To achieve this, our proposed scheme only requires a noise-corrupted single input-state trajectory from each unknown subsystem up to a specified time horizon, satisfying a certain rank condition. Subsequently, under a small-gain compositional reasoning, we compose those CSBC, derived from noisy data, and formulate a control barrier certificate (CBC) for the unknown network, ensuring its safety over an infinite time horizon, while providing correctness guarantees. We offer a data-dependent sum-of-squares (SOS) optimization program for computing CSBC alongside local controllers of subsystems. We illustrate that while the computational complexity of designing a CBC and its safety controller grows polynomially with network dimension using SOS optimization, our compositional data-driven approach significantly reduces it to a linear scale concerning the number of subsystems. We demonstrate the capability of our data-driven approach on multiple physical networks involving unknown models and a range of interconnection topologies.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Rank Scheduling for Energy-Constrained Multi-Task Federated Fine-Tuning in Edge-Assisted IoV Networks</title>
<link>https://arxiv.org/abs/2508.09532</link>
<guid>https://arxiv.org/abs/2508.09532</guid>
<content:encoded><![CDATA[
arXiv:2508.09532v1 Announce Type: new 
Abstract: Federated fine-tuning has emerged as a promising approach for adapting foundation models (FMs) to diverse downstream tasks in edge environments. In Internet of Vehicles (IoV) systems, enabling efficient and low-latency multi-task adaptation is particularly challenging due to client mobility, heterogeneous resources, and intermittent connectivity. This paper proposes a hierarchical federated fine-tuning framework that coordinates roadside units (RSUs) and vehicles to support resource-aware and mobility-resilient learning across dynamic IoV scenarios. Leveraging Low-Rank Adaptation (LoRA), we introduce a decentralized, energy-aware rank adaptation mechanism formulated as a constrained multi-armed bandit problem. A novel UCB-DUAL algorithm is developed to enable adaptive exploration under per-task energy budgets, achieving provable sublinear regret. To evaluate our method, we construct a large-scale IoV simulator based on real-world trajectories, capturing dynamic participation, RSU handoffs, and communication variability. Extensive experiments show that our approach achieves the best accuracy-efficiency trade-off among all baselines, reducing latency by over 24\% and improving average accuracy by more than 2.5\%.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedShard: Federated Unlearning with Efficiency Fairness and Performance Fairness</title>
<link>https://arxiv.org/abs/2508.09866</link>
<guid>https://arxiv.org/abs/2508.09866</guid>
<content:encoded><![CDATA[
arXiv:2508.09866v1 Announce Type: new 
Abstract: To protect clients' right to be forgotten in federated learning, federated unlearning aims to remove the data contribution of leaving clients from the global learned model. While current studies mainly focused on enhancing unlearning efficiency and effectiveness, the crucial aspects of efficiency fairness and performance fairness among decentralized clients during unlearning have remained largely unexplored. In this study, we introduce FedShard, the first federated unlearning algorithm designed to concurrently guarantee both efficiency fairness and performance fairness. FedShard adaptively addresses the challenges introduced by dilemmas among convergence, unlearning efficiency, and unlearning fairness. Furthermore, we propose two novel metrics to quantitatively assess the fairness of unlearning algorithms, which we prove to satisfy well-known properties in other existing fairness measurements. Our theoretical analysis and numerical evaluation validate FedShard's fairness in terms of both unlearning performance and efficiency. We demonstrate that FedShard mitigates unfairness risks such as cascaded leaving and poisoning attacks and realizes more balanced unlearning costs among clients. Experimental results indicate that FedShard accelerates the data unlearning process 1.3-6.2 times faster than retraining from scratch and 4.9 times faster than the state-of-the-art exact unlearning methods.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hummingbird: Fast, Flexible, and Fair Inter-Domain Bandwidth Reservations</title>
<link>https://arxiv.org/abs/2308.09959</link>
<guid>https://arxiv.org/abs/2308.09959</guid>
<content:encoded><![CDATA[
arXiv:2308.09959v4 Announce Type: replace 
Abstract: The current Internet lacks quality-of-service guarantees for real-time applications like video calls and gaming, cloud-based systems, financial transactions, telesurgery, and other remote applications that benefit from reliable communication. To address this problem, this paper introduces Hummingbird: a novel, lightweight bandwidth-reservation system that provides fine-grained inter-domain reservations for end hosts and introduces several improvements over previous designs.
  Hummingbird enables flexible and composable reservations with end-to-end guarantees, and addresses an often overlooked, but crucial, aspect of bandwidth reservation systems: incentivization of network providers. Hummingbird represents bandwidth reservations as tradeable assets which allows markets to emerge that ensure fair and efficient resource allocation and encourage deployment by remunerating providers. This incentivization is facilitated by decoupling reservations from network identities, which enables novel control-plane mechanisms and allows us to design a control plane based on smart contracts.
  Hummingbird also provides an efficient reservation data plane which streamlines the processing on routers and thus simplifies the implementation, deployment, and traffic policing while maintaining robust security properties.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Bio-Inspired Models under Different Learning Settings For Energy Efficiency in Network Traffic Prediction</title>
<link>https://arxiv.org/abs/2412.17565</link>
<guid>https://arxiv.org/abs/2412.17565</guid>
<content:encoded><![CDATA[
arXiv:2412.17565v2 Announce Type: replace 
Abstract: Cellular traffic forecasting is a critical task that enables network operators to efficiently allocate resources and address anomalies in rapidly evolving environments. The exponential growth of data collected from base stations poses significant challenges to processing and analysis. While machine learning (ML) algorithms have emerged as powerful tools for handling these large datasets and providing accurate predictions, their environmental impact, particularly in terms of energy consumption, is often overlooked in favor of their predictive capabilities. This study investigates the potential of two bio-inspired models: Spiking Neural Networks (SNNs) and Reservoir Computing through Echo State Networks (ESNs) for cellular traffic forecasting. The evaluation focuses on both their predictive performance and energy efficiency. These models are implemented in both centralized and federated settings to analyze their effectiveness and energy consumption in decentralized systems. Additionally, we compare bio-inspired models with traditional architectures, such as Convolutional Neural Networks (CNNs) and Multi-Layer Perceptrons (MLPs), to provide a comprehensive evaluation. Using data collected from three diverse locations in Barcelona, Spain, we examine the trade-offs between predictive accuracy and energy demands across these approaches. The results indicate that bio-inspired models, such as SNNs and ESNs, can achieve significant energy savings while maintaining predictive accuracy comparable to traditional architectures. Furthermore, federated implementations were tested to evaluate their energy efficiency in decentralized settings compared to centralized systems, particularly in combination with bio-inspired models. These findings offer valuable insights into the potential of bio-inspired models for sustainable and privacy-preserving cellular traffic forecasting.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Illusory Normativity of Rights-Based AI Regulation</title>
<link>https://arxiv.org/abs/2503.05784</link>
<guid>https://arxiv.org/abs/2503.05784</guid>
<content:encoded><![CDATA[
arXiv:2503.05784v2 Announce Type: replace 
Abstract: Whether and how to regulate AI is now a central question of governance. Across academic, policy, and international legal circles, the European Union is widely treated as the normative leader in this space. Its regulatory framework, anchored in the General Data Protection Regulation, the Digital Services and Markets Acts, and the AI Act, is often portrayed as a principled model grounded in fundamental rights. This Article challenges that assumption. We argue that the rights-based narrative surrounding EU AI regulation mischaracterizes the logic of its institutional design. While rights language pervades EU legal instruments, its function is managerial, not foundational. These rights operate as tools of administrative ordering, used to mitigate technological disruption, manage geopolitical risk, and preserve systemic balance, rather than as expressions of moral autonomy or democratic consent. Drawing on comparative institutional analysis, we situate EU AI governance within a longer tradition of legal ordering shaped by the need to coordinate power across fragmented jurisdictions. We contrast this approach with the American model, which reflects a different regulatory logic rooted in decentralized authority, sectoral pluralism, and a constitutional preference for innovation and individual autonomy. Through case studies in five key domains -- data privacy, cybersecurity, healthcare, labor, and disinformation -- we show that EU regulation is not meaningfully rights-driven, as is often claimed. It is instead structured around the containment of institutional risk. Our aim is not to endorse the American model but to reject the presumption that the EU approach reflects a normative ideal that other nations should uncritically adopt. The EU model is best understood as a historically contingent response to its own political conditions, not a template for others to blindly follow.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralization: A Qualitative Survey of Node Operators</title>
<link>https://arxiv.org/abs/2503.17246</link>
<guid>https://arxiv.org/abs/2503.17246</guid>
<content:encoded><![CDATA[
arXiv:2503.17246v4 Announce Type: replace 
Abstract: Decentralization is understood both by professionals in the blockchain industry and general users as a core design goal of permissionless ledgers. However, its meaning is far from universally agreed, and often it is easier to get opinions on what it is not, rather than what it is. In this paper, we solicit definitions of 'decentralization' and 'decentralization theatre' from blockchain node operators. Key to a definition is asking about effective decentralization strategies, as well as those that are ineffective, sometimes deliberately so. Malicious, deceptive, or incompetent strategies are commonly referred to by the term 'decentralization theatre.' Finally, we ask what is being decentralized. Via thematic analysis of interview transcripts, we find that most operators conceive decentralization as existing broadly on a technical and a governance axis. Isolating relevant variables, we collapse the categories to network topology and governance topology, or the structure of decision-making power. Our key finding is that `decentralization' alone does not affect ledger immutability or systemic robustness.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finite-Time Global Optimality Convergence in Deep Neural Actor-Critic Methods for Decentralized Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.18433</link>
<guid>https://arxiv.org/abs/2505.18433</guid>
<content:encoded><![CDATA[
arXiv:2505.18433v2 Announce Type: replace 
Abstract: Actor-critic methods for decentralized multi-agent reinforcement learning (MARL) facilitate collaborative optimal decision making without centralized coordination, thus enabling a wide range of applications in practice. To date, however, most theoretical convergence studies for existing actor-critic decentralized MARL methods are limited to the guarantee of a stationary solution under the linear function approximation. This leaves a significant gap between the highly successful use of deep neural actor-critic for decentralized MARL in practice and the current theoretical understanding. To bridge this gap, in this paper, we make the first attempt to develop a deep neural actor-critic method for decentralized MARL, where both the actor and critic components are inherently non-linear. We show that our proposed method enjoys a global optimality guarantee with a finite-time convergence rate of O(1/T), where T is the total iteration times. This marks the first global convergence result for deep neural actor-critic methods in the MARL literature. We also conduct extensive numerical experiments, which verify our theoretical results.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecast-Driven MPC for Decentralized Multi-Robot Collision Avoidance</title>
<link>https://arxiv.org/abs/2508.08264</link>
<guid>https://arxiv.org/abs/2508.08264</guid>
<content:encoded><![CDATA[
arXiv:2508.08264v1 Announce Type: new 
Abstract: The Iterative Forecast Planner (IFP) is a geometric planning approach that offers lightweight computations, scal- able, and reactive solutions for multi-robot path planning in decentralized, communication-free settings. However, it struggles in symmetric configurations, where mirrored interactions often lead to collisions and deadlocks. We introduce eIFP-MPC, an optimized and extended version of IFP that improves robustness and path consistency in dense, dynamic environments. The method refines threat prioritization using a time-to-collision heuristic, stabilizes path generation through cost-based via- point selection, and ensures dynamic feasibility by incorporating model predictive control (MPC) into the planning process. These enhancements are tightly integrated into the IFP to preserve its efficiency while improving its adaptability and stability. Ex- tensive simulations across symmetric and high-density scenarios show that eIFP-MPC significantly reduces oscillations, ensures collision-free motion, and improves trajectory efficiency. The results demonstrate that geometric planners can be strengthened through optimization, enabling robust performance at scale in complex multi-agent environments.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Heterogeneity-Aware and Energy-Efficient Topology Optimization for Decentralized Federated Learning in Edge Environment</title>
<link>https://arxiv.org/abs/2508.08278</link>
<guid>https://arxiv.org/abs/2508.08278</guid>
<content:encoded><![CDATA[
arXiv:2508.08278v1 Announce Type: new 
Abstract: Federated learning (FL) has emerged as a promising paradigm within edge computing (EC) systems, enabling numerous edge devices to collaboratively train artificial intelligence (AI) models while maintaining data privacy. To overcome the communication bottlenecks associated with centralized parameter servers, decentralized federated learning (DFL), which leverages peer-to-peer (P2P) communication, has been extensively explored in the research community. Although researchers design a variety of DFL approach to ensure model convergence, its iterative learning process inevitably incurs considerable cost along with the growth of model complexity and the number of participants. These costs are largely influenced by the dynamic changes of topology in each training round, particularly its sparsity and connectivity conditions. Furthermore, the inherent resources heterogeneity in the edge environments affects energy efficiency of learning process, while data heterogeneity degrades model performance. These factors pose significant challenges to the design of an effective DFL framework for EC systems. To this end, we propose Hat-DFed, a heterogeneity-aware and coset-effective decentralized federated learning (DFL) framework. In Hat-DFed, the topology construction is formulated as a dual optimization problem, which is then proven to be NP-hard, with the goal of maximizing model performance while minimizing cumulative energy consumption in complex edge environments. To solve this problem, we design a two-phase algorithm that dynamically constructs optimal communication topologies while unbiasedly estimating their impact on both model performance and energy cost. Additionally, the algorithm incorporates an importance-aware model aggregation mechanism to mitigate performance degradation caused by data heterogeneity.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Minimal Model for Emergent Collective Behaviors in Autonomous Robotic Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2508.08473</link>
<guid>https://arxiv.org/abs/2508.08473</guid>
<content:encoded><![CDATA[
arXiv:2508.08473v1 Announce Type: new 
Abstract: Collective behaviors such as swarming and flocking emerge from simple, decentralized interactions in biological systems. Existing models, such as Vicsek and Cucker-Smale, lack collision avoidance, whereas the Olfati-Saber model imposes rigid formations, limiting their applicability in swarm robotics. To address these limitations, this paper proposes a minimal yet expressive model that governs agent dynamics using relative positions, velocities, and local density, modulated by two tunable parameters: the spatial offset and kinetic offset. The model achieves spatially flexible, collision-free behaviors that reflect naturalistic group dynamics. Furthermore, we extend the framework to cognitive autonomous systems, enabling energy-aware phase transitions between swarming and flocking through adaptive control parameter tuning. This cognitively inspired approach offers a robust foundation for real-world applications in multi-robot systems, particularly autonomous aerial swarms.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Traffic Load-Aware Resource Management Strategy for Underwater Wireless Sensor Networks</title>
<link>https://arxiv.org/abs/2508.08555</link>
<guid>https://arxiv.org/abs/2508.08555</guid>
<content:encoded><![CDATA[
arXiv:2508.08555v1 Announce Type: new 
Abstract: Underwater Wireless Sensor Networks (UWSNs) represent a promising technology that enables diverse underwater applications through acoustic communication. However, it encounters significant challenges including harsh communication environments, limited energy supply, and restricted signal transmission. This paper aims to provide efficient and reliable communication in underwater networks with limited energy and communication resources by optimizing the scheduling of communication links and adjusting transmission parameters (e.g., transmit power and transmission rate). The efficient and reliable communication multi-objective optimization problem (ERCMOP) is formulated as a decentralized partially observable Markov decision process (Dec-POMDP). A Traffic Load-Aware Resource Management (TARM) strategy based on deep multi-agent reinforcement learning (MARL) is presented to address this problem. Specifically, a traffic load-aware mechanism that leverages the overhear information from neighboring nodes is designed to mitigate the disparity between partial observations and global states. Moreover, by incorporating a solution space optimization algorithm, the number of candidate solutions for the deep MARL-based decision-making model can be effectively reduced, thereby optimizing the computational complexity. Simulation results demonstrate the adaptability of TARM in various scenarios with different transmission demands and collision probabilities, while also validating the effectiveness of the proposed approach in supporting efficient and reliable communication in underwater networks with limited resources.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed optimization: designed for federated learning</title>
<link>https://arxiv.org/abs/2508.08606</link>
<guid>https://arxiv.org/abs/2508.08606</guid>
<content:encoded><![CDATA[
arXiv:2508.08606v1 Announce Type: new 
Abstract: Federated Learning (FL), as a distributed collaborative Machine Learning (ML) framework under privacy-preserving constraints, has garnered increasing research attention in cross-organizational data collaboration scenarios. This paper proposes a class of distributed optimization algorithms based on the augmented Lagrangian technique, designed to accommodate diverse communication topologies in both centralized and decentralized FL settings. Furthermore, we develop multiple termination criteria and parameter update mechanisms to enhance computational efficiency, accompanied by rigorous theoretical guarantees of convergence. By generalizing the augmented Lagrangian relaxation through the incorporation of proximal relaxation and quadratic approximation, our framework systematically recovers a broad of classical unconstrained optimization methods, including proximal algorithm, classic gradient descent, and stochastic gradient descent, among others. Notably, the convergence properties of these methods can be naturally derived within the proposed theoretical framework. Numerical experiments demonstrate that the proposed algorithm exhibits strong performance in large-scale settings with significant statistical heterogeneity across clients.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dual Framework for Optimized Data Storage and Retrieval using Lightweight Python Blockchain and Scalable Smart Contracts with IPFS</title>
<link>https://arxiv.org/abs/2508.08887</link>
<guid>https://arxiv.org/abs/2508.08887</guid>
<content:encoded><![CDATA[
arXiv:2508.08887v1 Announce Type: new 
Abstract: The exponential growth of IoT data demands efficient, secure, and scalable storage solutions on one hand, and efficient data migration and retrieval on the other hand are essential for the systems to be practical and acceptable for different applications. The traditional cloud-based models face latency, security, and high operational costs, while existing bi-directional data storage and retrieval-based IPFS models are not computationally efficient and incur high gas costs at the cost of a necessary blockchain deployment. To overcome the challenges of efficient data migration, we initially developed a 2-way data storage and retrieval system as well as a scalable framework that dynamically monitors and transfers device-generated data to IPFS, records the content identifier(CID) on a blockchain, and enables secure, real-time access via smart contracts. Experimental results demonstrate that the existing work achieved an average data upload time of 117.12 sec for a file size of 500 MB; our framework achieves a faster upload time of 7.63 sec, marking a 93.47% improvement. We further optimize the proposed framework to reduce the file upload time incurred from the smart contracts by introducing a blockchain-inspired, lightweight, and customizable Python framework that replicates the storage and retrieval functionalities of a traditional blockchain, where the file upload time is 4.2 sec, further optimized by 45% from our previous approach, thus demonstrating its efficiency, security and suitability for deploy ment in real-time and critical IoT applications and outperforming the existing IPFS-smart contract based solutions.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Redactable Blockchains: An Overview</title>
<link>https://arxiv.org/abs/2508.08898</link>
<guid>https://arxiv.org/abs/2508.08898</guid>
<content:encoded><![CDATA[
arXiv:2508.08898v1 Announce Type: new 
Abstract: Blockchains are widely recognized for their immutability, which provides robust guarantees of data integrity and transparency. However, this same feature poses significant challenges in real-world situations that require regulatory compliance, correction of erroneous data, or removal of sensitive information. Redactable blockchains address the limitations of traditional ones by enabling controlled, auditable modifications to blockchain data, primarily through cryptographic mechanisms such as chameleon hash functions and alternative redaction schemes. This report examines the motivations for introducing redactability, surveys the cryptographic primitives that enable secure edits, and analyzes competing approaches and their shortcomings. Special attention is paid to the practical deployment of redactable blockchains in private settings, with discussions of use cases in healthcare, finance, Internet of drones, and federated learning. Finally, the report outlines further challenges, also in connection with reversible computing, and the future potential of redactable blockchains in building law-compliant, trustworthy, and scalable digital infrastructures.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Inspired Legal Tech Environmental Integration for Emergency Pharmaceutical Logistics with Entropy Modulated Collapse and Multilevel Governance</title>
<link>https://arxiv.org/abs/2508.08286</link>
<guid>https://arxiv.org/abs/2508.08286</guid>
<content:encoded><![CDATA[
arXiv:2508.08286v1 Announce Type: cross 
Abstract: Emergency pharmaceutical logistics during rapid-onset disasters must balance timeliness, legal compliance, and environmental uncertainty. We present a hybrid framework that co-designs quantum-inspired decision dynamics, embedded legal constraints, and blockchain-verified environmental feedback. Candidate routes are modeled as a superposed state whose collapse is governed by entropy modulation-delaying commitment under ambiguity and accelerating resolution when coherent signals emerge. Legal statutes act as real-time projection operators shaping feasible choices, while environmental decoherence cues adjust confidence and path viability. The core engine is situated within a multilevel governance and mechanism design architecture, establishing clear roles, accountability channels, and audit trails. Large-scale simulations in wildfire scenarios demonstrate substantial gains over conventional baselines in latency, compliance, and robustness, while preserving interpretability and fairness adaptation. The resulting system offers a deployable, governance-aware infrastructure where law and physical risk jointly inform emergency routing decisions.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-participant externalities reshape the evolution of altruistic punishment</title>
<link>https://arxiv.org/abs/2508.08302</link>
<guid>https://arxiv.org/abs/2508.08302</guid>
<content:encoded><![CDATA[
arXiv:2508.08302v1 Announce Type: cross 
Abstract: While voluntary participation is a key mechanism that enables altruistic punishment to emerge, its explanatory power typically rests on the common assumption that non-participants have no impact on the public good. Yet, given the decentralized nature of voluntary participation, opting out does not necessarily preclude individuals from influencing the public good. Here, we revisit the role of voluntary participation by allowing non-participants to exert either positive or negative impacts on the public good. Using evolutionary analysis in a well-mixed finite population, we find that positive externalities from non-participants lower the synergy threshold required for altruistic punishment to dominate. In contrast, negative externalities raise this threshold, making altruistic punishment harder to sustain. Notably, when non-participants have positive impacts, altruistic punishment thrives only if non-participation is incentivized, whereas under negative impacts, it can persist even when non-participation is discouraged. Our findings reveal that efforts to promote altruistic punishment must account for the active role of non-participants, whose influence can make or break collective outcomes.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs</title>
<link>https://arxiv.org/abs/2508.06601</link>
<guid>https://arxiv.org/abs/2508.06601</guid>
<content:encoded><![CDATA[
arXiv:2508.06601v1 Announce Type: new 
Abstract: Open-weight AI systems offer unique benefits, including enhanced transparency, open research, and decentralized access. However, they are vulnerable to tampering attacks which can efficiently elicit harmful behaviors by modifying weights or activations. Currently, there is not yet a robust science of open-weight model risk management. Existing safety fine-tuning methods and other post-training techniques have struggled to make LLMs resistant to more than a few dozen steps of adversarial fine-tuning. In this paper, we investigate whether filtering text about dual-use topics from training data can prevent unwanted capabilities and serve as a more tamper-resistant safeguard. We introduce a multi-stage pipeline for scalable data filtering and show that it offers a tractable and effective method for minimizing biothreat proxy knowledge in LLMs. We pretrain multiple 6.9B-parameter models from scratch and find that they exhibit substantial resistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M tokens of biothreat-related text -- outperforming existing post-training baselines by over an order of magnitude -- with no observed degradation to unrelated capabilities. However, while filtered models lack internalized dangerous knowledge, we find that they can still leverage such information when it is provided in context (e.g., via search tool augmentation), demonstrating a need for a defense-in-depth approach. Overall, these findings help to establish pretraining data curation as a promising layer of defense for open-weight AI systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Secure and Decentralized Peer-to-Peer Energy Transactions using Blockchain Technology</title>
<link>https://arxiv.org/abs/2508.06728</link>
<guid>https://arxiv.org/abs/2508.06728</guid>
<content:encoded><![CDATA[
arXiv:2508.06728v1 Announce Type: new 
Abstract: This paper presents an optimal peer-to-peer (P2P) energy transaction mechanism leveraging decentralized blockchain technology to enable a secure and scalable retail electricity market for the increasing penetration of distributed energy resources (DERs). A decentralized bidding strategy is proposed to maximize individual profits while collectively enhancing social welfare. The market design and transaction processes are simulated using the Ethereum testnet, demonstrating the blockchain network's capability to ensure secure, transparent, and sustainable P2P energy trading among DER participants.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy Efficient Task Offloading in UAV-Enabled MEC Using a Fully Decentralized Deep Reinforcement Learning Approach</title>
<link>https://arxiv.org/abs/2508.06863</link>
<guid>https://arxiv.org/abs/2508.06863</guid>
<content:encoded><![CDATA[
arXiv:2508.06863v1 Announce Type: new 
Abstract: Unmanned aerial vehicles (UAVs) have been recently utilized in multi-access edge computing (MEC) as edge servers. It is desirable to design UAVs' trajectories and user to UAV assignments to ensure satisfactory service to the users and energy efficient operation simultaneously. The posed optimization problem is challenging to solve because: (i) The formulated problem is non-convex, (ii) Due to the mobility of ground users, their future positions and channel gains are not known in advance, (iii) Local UAVs' observations should be communicated to a central entity that solves the optimization problem. The (semi-) centralized processing leads to communication overhead, communication/processing bottlenecks, lack of flexibility and scalability, and loss of robustness to system failures. To simultaneously address all these limitations, we advocate a fully decentralized setup with no centralized entity. Each UAV obtains its local observation and then communicates with its immediate neighbors only. After sharing information with neighbors, each UAV determines its next position via a locally run deep reinforcement learning (DRL) algorithm. None of the UAVs need to know the global communication graph. Two main components of our proposed solution are (i) Graph attention layers (GAT), and (ii) Experience and parameter sharing proximal policy optimization (EPS-PPO). Our proposed approach eliminates all the limitations of semi-centralized MADRL methods such as MAPPO and MA deep deterministic policy gradient (MADDPG), while guaranteeing a better performance than independent local DRLs such as in IPPO. Numerical results reveal notable performance gains in several different criteria compared to the existing MADDPG algorithm, demonstrating the potential for offering a better performance, while utilizing local communications only.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consensus-based Decentralized Multi-agent Reinforcement Learning for Random Access Network Optimization</title>
<link>https://arxiv.org/abs/2508.07001</link>
<guid>https://arxiv.org/abs/2508.07001</guid>
<content:encoded><![CDATA[
arXiv:2508.07001v1 Announce Type: new 
Abstract: With wireless devices increasingly forming a unified smart network for seamless, user-friendly operations, random access (RA) medium access control (MAC) design is considered a key solution for handling unpredictable data traffic from multiple terminals. However, it remains challenging to design an effective RA-based MAC protocol to minimize collisions and ensure transmission fairness across the devices. While existing multi-agent reinforcement learning (MARL) approaches with centralized training and decentralized execution (CTDE) have been proposed to optimize RA performance, their reliance on centralized training and the significant overhead required for information collection can make real-world applications unrealistic. In this work, we adopt a fully decentralized MARL architecture, where policy learning does not rely on centralized tasks but leverages consensus-based information exchanges across devices. We design our MARL algorithm over an actor-critic (AC) network and propose exchanging only local rewards to minimize communication overhead. Furthermore, we provide a theoretical proof of global convergence for our approach. Numerical experiments show that our proposed MARL algorithm can significantly improve RA network performance compared to other baselines.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScamDetect: Towards a Robust, Agnostic Framework to Uncover Threats in Smart Contracts</title>
<link>https://arxiv.org/abs/2508.07094</link>
<guid>https://arxiv.org/abs/2508.07094</guid>
<content:encoded><![CDATA[
arXiv:2508.07094v1 Announce Type: new 
Abstract: Smart contracts have transformed decentralized finance by enabling programmable, trustless transactions. However, their widespread adoption and growing financial significance have attracted persistent and sophisticated threats, such as phishing campaigns and contract-level exploits. Traditional transaction-based threat detection methods often expose sensitive user data and interactions, raising privacy and security concerns. In response, static bytecode analysis has emerged as a proactive mitigation strategy, identifying malicious contracts before they execute harmful actions.Building on this approach, we introduced PhishingHook, the first machine-learning-based framework for detecting phishing activities in smart contracts via static bytecode and opcode analysis, achieving approximately 90% detection accuracy. Nevertheless, two pressing challenges remain: (1) the increasing use of sophisticated bytecode obfuscation techniques designed to evade static analysis, and (2) the heterogeneity of blockchain environments requiring platform-agnostic solutions.This paper presents a vision for ScamDetect (Smart Contract Agnostic Malware Detector), a robust, modular, and platform-agnostic framework for smart contract malware detection. Over the next 2.5 years, ScamDetect will evolve in two stages: first, by tackling obfuscated Ethereum Virtual Machine (EVM) bytecode through graph neural network (GNN) analysis of control flow graphs (CFGs), leveraging GNNs' ability to capture complex structural patterns beyond opcode sequences; and second, by generalizing detection capabilities to emerging runtimes such as WASM. ScamDetect aims to enable proactive, scalable security for the future of decentralized ecosystems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AerialDB: A Federated Peer-to-Peer Spatio-temporal Edge Datastore for Drone Fleets</title>
<link>https://arxiv.org/abs/2508.07124</link>
<guid>https://arxiv.org/abs/2508.07124</guid>
<content:encoded><![CDATA[
arXiv:2508.07124v1 Announce Type: new 
Abstract: Recent years have seen an unprecedented growth in research that leverages the newest computing paradigm of Internet of Drones, comprising a fleet of connected Unmanned Aerial Vehicles (UAVs) used for a wide range of tasks such as monitoring and analytics in highly mobile and changing environments characteristic of disaster regions. Given that the typical data (i.e., videos and images) collected by the fleet of UAVs deployed in such scenarios can be considerably larger than what the onboard computers can process, the UAVs need to offload their data in real-time to the edge and the cloud for further processing. To that end, we present the design of AerialDB - a lightweight decentralized data storage and query system that can store and process time series data on a multi-UAV system comprising: A) a fleet of hundreds of UAVs fitted with onboard computers, and B) ground-based edge servers connected through a cellular link. Leveraging lightweight techniques for content-based replica placement and indexing of shards, AerialDB has been optimized for efficient processing of different possible combinations of typical spatial and temporal queries performed by real-world disaster management applications. Using containerized deployment spanning up to 400 drones and 80 edges, we demonstrate that AerialDB is able to scale efficiently while providing near real-time performance with different realistic workloads. Further, AerialDB comprises a decentralized and locality-aware distributed execution engine which provides graceful degradation of performance upon edge failures with relatively low latency while processing large spatio-temporal data. AerialDB exhibits comparable insertion performance and 100 times improvement in query performance against state-of-the-art baseline. Moreover, it exhibits a 10 times and 100 times improvement with insertion and query workloads respectively over the cloud baseline.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Competition Helps: Achieving Optimal Traffic Flow with Multiple Autonomous Planners</title>
<link>https://arxiv.org/abs/2508.07145</link>
<guid>https://arxiv.org/abs/2508.07145</guid>
<content:encoded><![CDATA[
arXiv:2508.07145v1 Announce Type: new 
Abstract: The inefficiency of selfish routing in congested networks is a classical problem in algorithmic game theory, often captured by the Price of Anarchy (i.e., the ratio between the social cost of decentralized decisions and that of a centrally optimized solution.) With the advent of autonomous vehicles, capable of receiving and executing centrally assigned routes, it is natural to ask whether their deployment can eliminate this inefficiency. At first glance, a central authority could simply compute an optimal traffic assignment and instruct each vehicle to follow its assigned path. However, this vision overlooks critical challenges: routes must be individually rational (no vehicle has an incentive to deviate), and in practice, multiple planning agents (e.g., different companies) may coexist and compete. Surprisingly, we show that such competition is not merely an obstacle but a necessary ingredient for achieving optimal outcomes. In this work, we design a routing mechanism that embraces competition and converges to an optimal assignment, starting from the classical Pigou network as a foundational case.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding NFTs from EIP Standards</title>
<link>https://arxiv.org/abs/2508.07190</link>
<guid>https://arxiv.org/abs/2508.07190</guid>
<content:encoded><![CDATA[
arXiv:2508.07190v1 Announce Type: new 
Abstract: We argue that the technical foundations of non-fungible tokens (NFTs) remain inadequately understood. Prior research has focused on market dynamics, user behavior, and isolated security incidents, yet systematic analysis of the standards underpinning NFT functionality is largely absent.
  We present the first study of NFTs through the lens of Ethereum Improvement Proposals (EIPs). We conduct a large-scale empirical analysis of 191 NFT-related EIPs and 10K+ Ethereum Magicians discussions (as of July, 2025). We integrate multi-dimensional analyses including the automated parsing of Solidity interfaces, graph-based modeling of inheritance structures, contributor profiling, and mining of community discussion data. We distinguish foundational from emerging standards, expose poor cross-version interoperability, and show that growing functional complexity heightens security risks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Efficiency of Dynamic Transaction Scheduling in Blockchain Sharding</title>
<link>https://arxiv.org/abs/2508.07472</link>
<guid>https://arxiv.org/abs/2508.07472</guid>
<content:encoded><![CDATA[
arXiv:2508.07472v1 Announce Type: new 
Abstract: Sharding is a technique to speed up transaction processing in blockchains, where the $n$ processing nodes in the blockchain are divided into $s$ disjoint groups (shards) that can process transactions in parallel. We study dynamic scheduling problems on a shard graph $G_s$ where transactions arrive online over time and are not known in advance. Each transaction may access at most $k$ shards, and we denote by $d$ the worst distance between a transaction and its accessing (destination) shards (the parameter $d$ is unknown to the shards). To handle different values of $d$, we assume a locality sensitive decomposition of $G_s$ into clusters of shards, where every cluster has a leader shard that schedules transactions for the cluster. We first examine the simpler case of the stateless model, where leaders are not aware of the current state of the transaction accounts, and we prove a $O(d \log^2 s \cdot \min\{k, \sqrt{s}\})$ competitive ratio for latency. We then consider the stateful model, where leader shards gather the current state of accounts, and we prove a $O(\log s\cdot \min\{k, \sqrt{s}\}+\log^2 s)$ competitive ratio for latency. Each leader calculates the schedule in polynomial time for each transaction that it processes. We show that for any $\epsilon > 0$, approximating the optimal schedule within a $(\min\{k, \sqrt{s}\})^{1 -\epsilon}$ factor is NP-hard. Hence, our bound for the stateful model is within a poly-log factor from the best possibly achievable. To the best of our knowledge, this is the first work to establish provably efficient dynamic scheduling algorithms for blockchain sharding systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Privacy in Decentralized Min-Max Optimization: A Differentially Private Approach</title>
<link>https://arxiv.org/abs/2508.07505</link>
<guid>https://arxiv.org/abs/2508.07505</guid>
<content:encoded><![CDATA[
arXiv:2508.07505v1 Announce Type: new 
Abstract: Decentralized min-max optimization allows multi-agent systems to collaboratively solve global min-max optimization problems by facilitating the exchange of model updates among neighboring agents, eliminating the need for a central server. However, sharing model updates in such systems carry a risk of exposing sensitive data to inference attacks, raising significant privacy concerns. To mitigate these privacy risks, differential privacy (DP) has become a widely adopted technique for safeguarding individual data. Despite its advantages, implementing DP in decentralized min-max optimization poses challenges, as the added noise can hinder convergence, particularly in non-convex scenarios with complex agent interactions in min-max optimization problems. In this work, we propose an algorithm called DPMixSGD (Differential Private Minmax Hybrid Stochastic Gradient Descent), a novel privacy-preserving algorithm specifically designed for non-convex decentralized min-max optimization. Our method builds on the state-of-the-art STORM-based algorithm, one of the fastest decentralized min-max solutions. We rigorously prove that the noise added to local gradients does not significantly compromise convergence performance, and we provide theoretical bounds to ensure privacy guarantees. To validate our theoretical findings, we conduct extensive experiments across various tasks and models, demonstrating the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Scheduling and Resource Allocation in mmWave IAB Networks Using Deep RL</title>
<link>https://arxiv.org/abs/2508.07604</link>
<guid>https://arxiv.org/abs/2508.07604</guid>
<content:encoded><![CDATA[
arXiv:2508.07604v1 Announce Type: new 
Abstract: Integrated Access and Backhaul (IAB) is critical for dense 5G and beyond deployments, especially in mmWave bands where fiber backhaul is infeasible. We propose a novel Deep Reinforcement Learning (DRL) framework for joint link scheduling and resource slicing in dynamic, interference-prone IAB networks. Our method integrates a greedy Double Deep Q-Network (DDQN) scheduler to activate access and backhaul links based on traffic and topology, with a multi-agent DDQN allocator for bandwidth and antenna assignment across network slices. This decentralized approach respects strict antenna constraints and supports concurrent scheduling across heterogeneous links. Evaluations across 96 dynamic topologies show 99.84 percent scheduling accuracy and 20.90 percent throughput improvement over baselines. The framework's efficient operation and adaptability make it suitable for dynamic and resource-constrained deployments, where fast link scheduling and autonomous backhaul coordination are vital.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Hop Privacy Propagation for Differentially Private Federated Learning in Social Networks</title>
<link>https://arxiv.org/abs/2508.07676</link>
<guid>https://arxiv.org/abs/2508.07676</guid>
<content:encoded><![CDATA[
arXiv:2508.07676v1 Announce Type: new 
Abstract: Federated learning (FL) enables collaborative model training across decentralized clients without sharing local data, thereby enhancing privacy and facilitating collaboration among clients connected via social networks. However, these social connections introduce privacy externalities: a client's privacy loss depends not only on its privacy protection strategy but also on the privacy decisions of others, propagated through the network via multi-hop interactions. In this work, we propose a socially-aware privacy-preserving FL mechanism that systematically quantifies indirect privacy leakage through a multi-hop propagation model. We formulate the server-client interaction as a two-stage Stackelberg game, where the server, as the leader, optimizes incentive policies, and clients, as followers, strategically select their privacy budgets, which determine their privacy-preserving levels by controlling the magnitude of added noise. To mitigate information asymmetry in networked privacy estimation, we introduce a mean-field estimator to approximate the average external privacy risk. We theoretically prove the existence and convergence of the fixed point of the mean-field estimator and derive closed-form expressions for the Stackelberg Nash Equilibrium. Despite being designed from a client-centric incentive perspective, our mechanism achieves approximately-optimal social welfare, as revealed by Price of Anarchy (PoA) analysis. Experiments on diverse datasets demonstrate that our approach significantly improves client utilities and reduces server costs while maintaining model performance, outperforming both Social-Agnostic (SA) baselines and methods that account for social externalities.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IPBA: Imperceptible Perturbation Backdoor Attack in Federated Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2508.08031</link>
<guid>https://arxiv.org/abs/2508.08031</guid>
<content:encoded><![CDATA[
arXiv:2508.08031v1 Announce Type: new 
Abstract: Federated self-supervised learning (FSSL) combines the advantages of decentralized modeling and unlabeled representation learning, serving as a cutting-edge paradigm with strong potential for scalability and privacy preservation. Although FSSL has garnered increasing attention, research indicates that it remains vulnerable to backdoor attacks. Existing methods generally rely on visually obvious triggers, which makes it difficult to meet the requirements for stealth and practicality in real-world deployment. In this paper, we propose an imperceptible and effective backdoor attack method against FSSL, called IPBA. Our empirical study reveals that existing imperceptible triggers face a series of challenges in FSSL, particularly limited transferability, feature entanglement with augmented samples, and out-of-distribution properties. These issues collectively undermine the effectiveness and stealthiness of traditional backdoor attacks in FSSL. To overcome these challenges, IPBA decouples the feature distributions of backdoor and augmented samples, and introduces Sliced-Wasserstein distance to mitigate the out-of-distribution properties of backdoor samples, thereby optimizing the trigger generation process. Our experimental results on several FSSL scenarios and datasets show that IPBA significantly outperforms existing backdoor attack methods in performance and exhibits strong robustness under various defense mechanisms.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fed-TGAN: Federated Learning Framework for Synthesizing Tabular Data</title>
<link>https://arxiv.org/abs/2108.07927</link>
<guid>https://arxiv.org/abs/2108.07927</guid>
<content:encoded><![CDATA[
arXiv:2108.07927v2 Announce Type: replace 
Abstract: Generative Adversarial Networks (GANs) are typically trained to synthesize data, from images and more recently tabular data, under the assumption of directly accessible training data. Recently, federated learning (FL) is an emerging paradigm that features decentralized learning on client's local data with a privacy-preserving capability. And, while learning GANs to synthesize images on FL systems has just been demonstrated, it is unknown if GANs for tabular data can be learned from decentralized data sources. Moreover, it remains unclear which distributed architecture suits them best. Different from image GANs, state-of-the-art tabular GANs require prior knowledge on the data distribution of each (discrete and continuous) column to agree on a common encoding -- risking privacy guarantees. In this paper, we propose Fed-TGAN, the first Federated learning framework for Tabular GANs. To effectively learn a complex tabular GAN on non-identical participants, Fed-TGAN designs two novel features: (i) a privacy-preserving multi-source feature encoding for model initialization; and (ii) table similarity aware weighting strategies to aggregate local models for countering data skew. We extensively evaluate the proposed Fed-TGAN against variants of decentralized learning architectures on four widely used datasets. Results show that Fed-TGAN accelerates training time per epoch up to 200% compared to the alternative architectures, for both IID and Non-IID data. Overall, Fed-TGAN not only stabilizes the training loss, but also achieves better similarity between generated and original data. Our code is released at https://github.com/zhao-zilong/Fed-TGAN.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Communication Consistent Approach to Signal Temporal Logic Task Decomposition in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2410.12563</link>
<guid>https://arxiv.org/abs/2410.12563</guid>
<content:encoded><![CDATA[
arXiv:2410.12563v2 Announce Type: replace 
Abstract: We consider the problem of decomposing a global task assigned to a multi-agent system, expressed as a formula within a fragment of Signal Temporal Logic (STL), under range-limited communication. Given a global task expressed as a conjunction of local tasks defined over the individual and relative states of agents in the system, we propose representing task dependencies among agents as edges of a suitably defined task graph. At the same time, range-limited communication naturally induces the definition of a communication graph that defines which agents have access to each other's states. Within these settings, inconsistencies arise when a task dependency between a pair of agents is not supported by a corresponding communication link due to the limited communication range. As a result, state feedback control laws previously derived to achieve the tasks' satisfaction can not be leveraged. We propose a task decomposition mechanism to distribute tasks assigned to pairs of non-communicating agents in the system as conjunctions of tasks defined over the relative states of communicating agents, thus enforcing consistency between task and communication graphs. Assuming the super-level sets of the predicate functions composing the STL tasks are bounded polytopes, our task decomposition mechanism can be cast as a parameter optimization problem and solved via state-of-the-art decentralized convex optimization algorithms. To guarantee the soundness of our approach, we present various conditions under which the tasks defined in the applied STL fragment are unsatisfiable, and we show sufficient conditions such that our decomposition approach yields satisfiable global tasks after decomposition.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Model Empowered Metaverse: State-of-the-Art, Challenges and Opportunities</title>
<link>https://arxiv.org/abs/2502.10397</link>
<guid>https://arxiv.org/abs/2502.10397</guid>
<content:encoded><![CDATA[
arXiv:2502.10397v2 Announce Type: replace 
Abstract: The Metaverse represents a transformative shift beyond traditional mobile Internet, creating an immersive, persistent digital ecosystem where users can interact, socialize, and work within 3D virtual environments. Powered by large models such as ChatGPT and Sora, the Metaverse benefits from precise large-scale real-world modeling, automated multimodal content generation, realistic avatars, and seamless natural language understanding, which enhance user engagement and enable more personalized, intuitive interactions. However, challenges remain, including limited scalability, constrained responsiveness, and low adaptability in dynamic environments. This paper investigates the integration of large models within the Metaverse, examining their roles in enhancing user interaction, perception, content creation, and service quality. To address existing challenges, we propose a generative AI-based framework for optimizing Metaverse rendering. This framework includes a cloud-edge-end collaborative model to allocate rendering tasks with minimal latency, a mobility-aware pre-rendering mechanism that dynamically adjusts to user movement, and a diffusion model-based adaptive rendering strategy to fine-tune visual details. Experimental results demonstrate the effectiveness of our approach in enhancing rendering efficiency and reducing rendering overheads, advancing large model deployment for a more responsive and immersive Metaverse.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum advantage in decentralized control of POMDPs: A control-theoretic view of the Mermin-Peres square</title>
<link>https://arxiv.org/abs/2501.16690</link>
<guid>https://arxiv.org/abs/2501.16690</guid>
<content:encoded><![CDATA[
arXiv:2501.16690v2 Announce Type: replace-cross 
Abstract: Consider a decentralized partially-observed Markov decision problem (POMDP) with multiple cooperative agents aiming to maximize a long-term-average reward criterion. We observe that the availability, at a fixed rate, of entangled states of a product quantum system between the agents, where each agent has access to one of the component systems, can result in strictly improved performance even compared to the scenario where common randomness is provided to the agents, i.e. there is a quantum advantage in decentralized control. This observation comes from a simple reinterpretation of the conclusions of the well-known Mermin-Peres square, which underpins the Mermin-Peres game. While quantum advantage has been demonstrated earlier in one-shot team problems of this kind, it is notable that there are examples where there is a quantum advantage for the one-shot criterion but it disappears in the dynamical scenario. The presence of a quantum advantage in dynamical scenarios is thus seen to be a novel finding relative to the current state of knowledge about the achievable performance in decentralized control problems.
  This paper is dedicated to the memory of Pravin P. Varaiya.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blockchain-Based Decentralized Domain Name System</title>
<link>https://arxiv.org/abs/2508.05655</link>
<guid>https://arxiv.org/abs/2508.05655</guid>
<content:encoded><![CDATA[
arXiv:2508.05655v1 Announce Type: new 
Abstract: The current Domain Name System (DNS) infrastructure faces critical vulnerabilities including poisoning attacks, censorship mechanisms, and centralized points of failure that compromise internet freedom and security. Recent incidents such as DNS poisoning attacks on ISP customers highlight the urgent need for resilient alternatives. This paper presents a novel blockchain-based Decentralized Domain Name System (DDNS). We designed a specialized Proof-of-Work blockchain to maximize support for DNS-related protocols and achieve node decentralization. The system integrates our blockchain with IPFS for distributed storage, implements cryptographic primitives for end-to-end trust signatures, and achieves Never Trust, Always Verify zero-trust verification. Our implementation achieves 15-second domain record propagation times, supports 20 standard DNS record types, and provides perpetual free .ddns domains. The system has been deployed across distributed infrastructure in San Jose, Los Angeles, and Orange County, demonstrating practical scalability and resistance to traditional DNS manipulation techniques. Performance evaluation shows the system can handle up to Max Theor. TPS 1,111.1 tx/s (minimal transactions) and Max Theor. TPS 266.7 tx/s (regular transactions) for domain operations while maintaining sub-second query resolution through intelligent caching mechanisms.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Secure and Scalable Blockchain Voting: A Comparative Framework and the Role of Large Language Models</title>
<link>https://arxiv.org/abs/2508.05865</link>
<guid>https://arxiv.org/abs/2508.05865</guid>
<content:encoded><![CDATA[
arXiv:2508.05865v1 Announce Type: new 
Abstract: Blockchain technology offers a promising foundation for modernizing E-Voting systems by enhancing transparency, decentralization, and security. Yet, real-world adoption remains limited due to persistent challenges such as scalability constraints, high computational demands, and complex privacy requirements. This paper presents a comparative framework for analyzing blockchain-based E-Voting architectures, consensus mechanisms, and cryptographic protocols. We examine the limitations of prevalent models like Proof of Work, Proof of Stake, and Delegated Proof of Stake, and propose optimization strategies that include hybrid consensus, lightweight cryptography, and decentralized identity management. Additionally, we explore the novel role of Large Language Models (LLMs) in smart contract generation, anomaly detection, and user interaction. Our findings offer a foundation for designing secure, scalable, and intelligent blockchain-based E-Voting systems suitable for national-scale deployment. This work lays the groundwork for building an end-to-end blockchain E-Voting prototype enhanced by LLM-guided smart contract generation and validation, supported by a systematic framework and simulation-based analysis.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Overlapping Coalition Game Approach for Collaborative Block Mining and Edge Task Offloading in MEC-assisted Blockchain Networks</title>
<link>https://arxiv.org/abs/2508.06031</link>
<guid>https://arxiv.org/abs/2508.06031</guid>
<content:encoded><![CDATA[
arXiv:2508.06031v1 Announce Type: new 
Abstract: Mobile edge computing (MEC) is a promising technology that enhances the efficiency of mobile blockchain networks, by enabling miners, often acted by mobile users (MUs) with limited computing resources, to offload resource-intensive mining tasks to nearby edge computing servers. Collaborative block mining can further boost mining efficiency by allowing multiple miners to form coalitions, pooling their computing resources and transaction data together to mine new blocks collaboratively. Therefore, an MEC-assisted collaborative blockchain network can leverage the strengths of both technologies, offering improved efficiency, security, and scalability for blockchain systems. While existing research in this area has mainly focused on the single-coalition collaboration mode, where each miner can only join one coalition, this work explores a more comprehensive multi-coalition collaboration mode, which allows each miner to join multiple coalitions. To analyze the behavior of miners and the edge computing service provider (ECP) in this scenario, we propose a novel two-stage Stackelberg game. In Stage I, the ECP, as the leader, determines the prices of computing resources for all MUs. In Stage II, each MU decides the coalitions to join, resulting in an overlapping coalition formation (OCF) game; Subsequently, each coalition decides how many edge computing resources to purchase from the ECP, leading to an edge resource competition (ERC) game. We derive the closed-form Nash equilibrium for the ERC game, based on which we further propose an OCF-based alternating algorithm to achieve a stable coalition structure for the OCF game and develop a near-optimal pricing strategy for the ECP's resource pricing problem.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Game-Theoretic Foundation for Bitcoin's Price: A Security-Utility Equilibrium</title>
<link>https://arxiv.org/abs/2508.06071</link>
<guid>https://arxiv.org/abs/2508.06071</guid>
<content:encoded><![CDATA[
arXiv:2508.06071v1 Announce Type: new 
Abstract: This paper introduces a structural game-theoretic model to value decentralized digital assets like Bitcoin. Instead of relying on speculative beliefs, it frames the asset's price within a Rational-Expectations Security-Utility Nash Equilibrium (RESUNE). This equilibrium is a fixed point where the market-clearing price dictates the hash rate through a free-entry mining model, which in turn endogenously sets the network's security. The security, defined as one minus the probability of a 51% attack, is determined via a global games model of attacker coordination, providing a unique and continuous security function. We prove the existence of a RESUNE and offer conditions for its uniqueness and stability. The model predicts that the stabilizing direct effect of price on demand must outweigh the potentially destabilizing feedback from price to security. The framework generates testable predictions, such as a protocol halving causing a contraction in both hash rate and price. A structural Vector Autoregression (VAR) model is proposed to test this mechanism. The model decomposes Bitcoin's value into transactional utility, security, and speculative components and explains the observed unidirectional causality from price to hash rate.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Inconsistent State Update Vulnerabilities in Smart Contracts</title>
<link>https://arxiv.org/abs/2508.06192</link>
<guid>https://arxiv.org/abs/2508.06192</guid>
<content:encoded><![CDATA[
arXiv:2508.06192v1 Announce Type: new 
Abstract: Smart contracts enable contract terms to be automatically executed and verified on the blockchain, and recent years have witnessed numerous applications of them in areas such as financial institutions and supply chains. The execution logic of a smart contract is closely related to the contract state, and thus the correct and safe execution of the contract depends heavily on the precise control and update of the contract state. However, the contract state update process can have issues. In particular, inconsistent state update issues can arise for reasons such as unsynchronized modifications. Inconsistent state update bugs have been exploited by attackers many times, but existing detection tools still have difficulty in effectively identifying them. This paper conducts the first large-scale empirical study about inconsistent state update vulnerabilities (that is, inconsistent state update bugs that are exploitable) in smart contracts, aiming to shed light for developers, researchers, tool builders, and language or library designers in order to avoid inconsistent state update vulnerabilities. We systematically investigate 116 inconsistent state update vulnerabilities in 352 real-world smart contract projects, summarizing their root causes, fix strategies, and exploitation methods. Our study provides 11 original and important findings, and we also give the implications of our findings. To illustrate the potential benefits of our research, we also develop a proof-of-concept checker based on one of our findings. The checker effectively detects issues in 64 popular GitHub projects, and 19 project owners have confirmed the detected issues at the time of writing. The result demonstrates the usefulness and importance of our findings for avoiding inconsistent state update vulnerabilities in smart contracts.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedX: Explanation-Guided Pruning for Communication-Efficient Federated Learning in Remote Sensing</title>
<link>https://arxiv.org/abs/2508.06256</link>
<guid>https://arxiv.org/abs/2508.06256</guid>
<content:encoded><![CDATA[
arXiv:2508.06256v1 Announce Type: new 
Abstract: Federated learning (FL) enables the collaborative training of deep neural networks across decentralized data archives (i.e., clients), where each client stores data locally and only shares model updates with a central server. This makes FL a suitable learning paradigm for remote sensing (RS) image classification tasks, where data centralization may be restricted due to legal and privacy constraints. However, a key challenge in applying FL to RS tasks is the communication overhead caused by the frequent exchange of large model updates between clients and the central server. To address this issue, in this paper we propose a novel strategy (denoted as FedX) that uses explanation-guided pruning to reduce communication overhead by minimizing the size of the transmitted models without compromising performance. FedX leverages backpropagation-based explanation methods to estimate the task-specific importance of model components and prunes the least relevant ones at the central server. The resulting sparse global model is then sent to clients, substantially reducing communication overhead. We evaluate FedX on multi-label scene classification using the BigEarthNet-S2 dataset and single-label scene classification using the EuroSAT dataset. Experimental results show the success of FedX in significantly reducing the number of shared model parameters while enhancing the generalization capability of the global model, compared to both unpruned model and state-of-the-art pruning methods. The code of FedX will be available at https://git.tu-berlin.de/rsim/FedX.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Welfare in Battery Charging Games</title>
<link>https://arxiv.org/abs/2508.06320</link>
<guid>https://arxiv.org/abs/2508.06320</guid>
<content:encoded><![CDATA[
arXiv:2508.06320v1 Announce Type: new 
Abstract: The recent rise of renewable energy produced by many decentralized sources yields interesting market design challenges for electrical grids. Balancing supply and demand in such networks is both a temporal and spatial challenge due to capacity constraints. The recent surge in the number of household-owned batteries, especially in regions with rooftop solar adoption, offers mitigation potential but often acts misaligned with grid-level objectives. In fact, the decision to charge or discharge a household-owned battery is a strategic choice by each battery owner governed by selfish incentives. This calls for an analysis from a game-theoretic point of view.
  We initiate this timely research direction by considering a game-theoretic setting where selfish agents strategically charge or discharge their batteries to increase their profit. In particular, we study a Stackelberg-like market model where a third party introduces price incentives, aiming to optimize renewable energy utilization while preserving grid feasibility. For this, we study the existence and the quality of equilibria under various pricing strategies. We find that the existence of equilibria crucially depends on the chosen pricing and that the obtained social welfare varies widely. This calls for more sophisticated market models and pricing mechanisms and opens up a rich field for future research in Algorithmic Game Theory on incentives in renewable energy networks.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blockchain-Enabled Federated Learning</title>
<link>https://arxiv.org/abs/2508.06406</link>
<guid>https://arxiv.org/abs/2508.06406</guid>
<content:encoded><![CDATA[
arXiv:2508.06406v1 Announce Type: new 
Abstract: Blockchain-enabled federated learning (BCFL) addresses fundamental challenges of trust, privacy, and coordination in collaborative AI systems. This chapter provides comprehensive architectural analysis of BCFL systems through a systematic four-dimensional taxonomy examining coordination structures, consensus mechanisms, storage architectures, and trust models. We analyze design patterns from blockchain-verified centralized coordination to fully decentralized peer-to-peer networks, evaluating trade-offs in scalability, security, and performance. Through detailed examination of consensus mechanisms designed for federated learning contexts, including Proof of Quality and Proof of Federated Learning, we demonstrate how computational work can be repurposed from arbitrary cryptographic puzzles to productive machine learning tasks. The chapter addresses critical storage challenges by examining multi-tier architectures that balance blockchain's transaction constraints with neural networks' large parameter requirements while maintaining cryptographic integrity. A technical case study of the TrustMesh framework illustrates practical implementation considerations in BCFL systems through distributed image classification training, demonstrating effective collaborative learning across IoT devices with highly non-IID data distributions while maintaining complete transparency and fault tolerance. Analysis of real-world deployments across healthcare consortiums, financial services, and IoT security applications validates the practical viability of BCFL systems, achieving performance comparable to centralized approaches while providing enhanced security guarantees and enabling new models of trustless collaborative intelligence.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI and the Future of the Digital Commons: Five Open Questions and Knowledge Gaps</title>
<link>https://arxiv.org/abs/2508.06470</link>
<guid>https://arxiv.org/abs/2508.06470</guid>
<content:encoded><![CDATA[
arXiv:2508.06470v1 Announce Type: new 
Abstract: The rapid advancement of Generative AI (GenAI) relies heavily on the digital commons, a vast collection of free and open online content that is created, shared, and maintained by communities. However, this relationship is becoming increasingly strained due to financial burdens, decreased contributions, and misalignment between AI models and community norms. As we move deeper into the GenAI era, it is essential to examine the interdependent relationship between GenAI, the long-term sustainability of the digital commons, and the equity of current AI development practices. We highlight five critical questions that require urgent attention: 1. How can we prevent the digital commons from being threatened by undersupply as individuals cease contributing to the commons and turn to Generative AI for information? 2. How can we mitigate the risk of the open web closing due to restrictions on access to curb AI crawlers? 3. How can technical standards and legal frameworks be updated to reflect the evolving needs of organizations hosting common content? 4. What are the effects of increased synthetic content in open knowledge databases, and how can we ensure their integrity? 5. How can we account for and distribute the infrastructural and environmental costs of providing data for AI training? We emphasize the need for more responsible practices in AI development, recognizing the digital commons not only as content but as a collaborative and decentralized form of knowledge governance, which relies on the practice of "commoning" - making, maintaining, and protecting shared and open resources. Ultimately, our goal is to stimulate discussion and research on the intersection of Generative AI and the digital commons, with the aim of developing an "AI commons" and public infrastructures for AI development that support the long-term health of the digital commons.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random Walk Learning and the Pac-Man Attack</title>
<link>https://arxiv.org/abs/2508.05663</link>
<guid>https://arxiv.org/abs/2508.05663</guid>
<content:encoded><![CDATA[
arXiv:2508.05663v1 Announce Type: cross 
Abstract: Random walk (RW)-based algorithms have long been popular in distributed systems due to low overheads and scalability, with recent growing applications in decentralized learning. However, their reliance on local interactions makes them inherently vulnerable to malicious behavior. In this work, we investigate an adversarial threat that we term the ``Pac-Man'' attack, in which a malicious node probabilistically terminates any RW that visits it. This stealthy behavior gradually eliminates active RWs from the network, effectively halting the learning process without triggering failure alarms. To counter this threat, we propose the Average Crossing (AC) algorithm--a fully decentralized mechanism for duplicating RWs to prevent RW extinction in the presence of Pac-Man. Our theoretical analysis establishes that (i) the RW population remains almost surely bounded under AC and (ii) RW-based stochastic gradient descent remains convergent under AC, even in the presence of Pac-Man, with a quantifiable deviation from the true optimum. Our extensive empirical results on both synthetic and real-world datasets corroborate our theoretical findings. Furthermore, they uncover a phase transition in the extinction probability as a function of the duplication threshold. We offer theoretical insights by analyzing a simplified variant of the AC, which sheds light on the observed phase transition.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Optimization via RC-ALADIN with Efficient Quantized Communication</title>
<link>https://arxiv.org/abs/2508.06197</link>
<guid>https://arxiv.org/abs/2508.06197</guid>
<content:encoded><![CDATA[
arXiv:2508.06197v1 Announce Type: cross 
Abstract: In this paper, we investigate the problem of decentralized consensus optimization over directed graphs with limited communication bandwidth. We introduce a novel decentralized optimization algorithm that combines the Reduced Consensus Augmented Lagrangian Alternating Direction Inexact Newton (RC-ALADIN) method with a finite time quantized coordination protocol, enabling quantized information exchange among nodes. Assuming the nodes' local objective functions are $\mu$-strongly convex and simply smooth, we establish global convergence at a linear rate to a neighborhood of the optimal solution, with the neighborhood size determined by the quantization level. Additionally, we show that the same convergence result also holds for the case where the local objective functions are convex and $L$-smooth. Numerical experiments demonstrate that our proposed algorithm compares favorably against algorithms in the current literature while exhibiting communication efficient operation.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EVA-S2PLoR: Decentralized Secure 2-party Logistic Regression with A Subtly Hadamard Product Protocol (Full Version)</title>
<link>https://arxiv.org/abs/2501.05223</link>
<guid>https://arxiv.org/abs/2501.05223</guid>
<content:encoded><![CDATA[
arXiv:2501.05223v3 Announce Type: replace 
Abstract: The implementation of accurate nonlinear operators (e.g., sigmoid function) on heterogeneous datasets is a key challenge in privacy-preserving machine learning (PPML). Most existing frameworks approximate it through linear operations, which not only result in significant precision loss but also introduce substantial computational overhead. This paper proposes an efficient, verifiable, and accurate security 2-party logistic regression framework (EVA-S2PLoR), which achieves accurate nonlinear function computation through a subtly secure hadamard product protocol and its derived protocols. All protocols are based on a practical semi-honest security model, which is designed for decentralized privacy-preserving application scenarios that balance efficiency, precision, and security. High efficiency and precision are guaranteed by the asynchronous computation flow on floating point numbers and the few number of fixed communication rounds in the hadamard product protocol, where robust anomaly detection is promised by dimension transformation and Monte Carlo methods. EVA-S2PLoR outperforms many advanced frameworks in terms of precision, improving the performance of the sigmoid function by about 10 orders of magnitude compared to most frameworks. Moreover, EVA-S2PLoR delivers the best overall performance in secure logistic regression experiments with training time reduced by over 47.6% under WAN settings and a classification accuracy difference of only about 0.5% compared to the plaintext model.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transaction Fee Market Design for Parallel Execution</title>
<link>https://arxiv.org/abs/2502.11964</link>
<guid>https://arxiv.org/abs/2502.11964</guid>
<content:encoded><![CDATA[
arXiv:2502.11964v2 Announce Type: replace 
Abstract: Given the low throughput of blockchains like Bitcoin and Ethereum, scalability - the ability to process an increasing number of transactions - has become a central focus of blockchain research. One promising approach is the parallelization of transaction execution across multiple threads. However, achieving efficient parallelization requires a redesign of the incentive structure within the fee market. Currently, the fee market does not differentiate between transactions that access multiple high-demand storage keys (i.e., unique identifiers for individual data entries) versus a single low-demand one, as long as they require the same computational effort. Addressing this discrepancy is crucial for enabling more effective parallel execution.
  In this work, we aim to bridge the gap between the current fee market and the need for parallel execution by exploring alternative fee market designs. To this end, we propose a framework consisting of two key components: a Gas Computation Mechanism (GCM), which quantifies the load a transaction places on the network in terms of parallelization and computation, measured in units of gas, and a Transaction Fee Mechanism (TFM), which assigns a price to each unit of gas. We additionally introduce a set of desirable properties for a GCM, propose several candidate mechanisms, and evaluate them against these criteria. Our analysis highlights two strong candidates: the weighted area GCM, which integrates smoothly with existing TFMs such as EIP-1559 and satisfies a broad subset of the outlined properties, and the time-proportional makespan GCM, which assigns gas costs based on the context of the entire block's schedule and, through this dependence on the overall execution outcome, captures the dynamics of parallel execution more accurately.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairDAG: Consensus Fairness over Multi-Proposer Causal Design</title>
<link>https://arxiv.org/abs/2504.02194</link>
<guid>https://arxiv.org/abs/2504.02194</guid>
<content:encoded><![CDATA[
arXiv:2504.02194v2 Announce Type: replace 
Abstract: The rise of cryptocurrencies like Bitcoin and Ethereum has driven interest in blockchain database technology, with smart contracts enabling the growth of decentralized finance (DeFi). However, research has shown that adversaries exploit transaction ordering to extract profits through attacks like front-running, sandwich attacks, and liquidation manipulation. This issue affects blockchain databases in which block proposers have full control over transaction ordering. To address this, a more fair approach to transaction ordering is essential.
  Existing fairness protocols, such as Pompe and Themis, operate on leader-based consensus protocols, which not only suffer from low throughput, but also allow adversaries to manipulate transaction ordering. To address these limitations, we propose FairDAG-AB and FairDAG-RL that run fairness protocols on top of DAG-based consensus protocols, which improve protocol performance in both throughput and fairness quality, leveraging the multi-proposer design and validity of DAG-based consensus protocols.
  We conducted a comprehensive analytical and experimental evaluation of our protocols. The results show that FairDAG-AB and FairDAG-RL outperform the prior fairness protocols in both throughput and fairness quality.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agency, Affordances, and Enculturation of Augmentation Technologies</title>
<link>https://arxiv.org/abs/2508.04725</link>
<guid>https://arxiv.org/abs/2508.04725</guid>
<content:encoded><![CDATA[
arXiv:2508.04725v1 Announce Type: new 
Abstract: Augmentation technologies are undergoing a process of enculturation due to many factors, one being the rise of artificial intelligence (AI), or what the World Intellectual Property Organization (WIPO) terms the AI wave or AI boom. Chapter 3 focuses critical attention on the hyped assumption that sophisticated, emergent, and embodied augmentation technologies will improve lives, literacy, cultures, arts, economies, and social contexts. The chapter begins by discussing the problem of ambiguity with AI terminology, which it aids with a description of the WIPO Categorization of AI Technologies Scheme. It then draws on media and communication studies to explore concepts such as agents, agency, power, and agentive relationships between humans and robots. The chapter focuses on the development of non-human agents in industry as a critical factor in the rise of augmentation technologies. It looks at how marketing communication enculturates future users to adopt and adapt to the technology. Scholars are charting the significant ways that people are drawn further into commercial digital landscapes, such as the Metaverse concept, in post-internet society. It concludes by examining recent claims concerning the Metaverse and augmented reality.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OPTIMUMP2P: Fast and Reliable Gossiping in P2P Networks</title>
<link>https://arxiv.org/abs/2508.04833</link>
<guid>https://arxiv.org/abs/2508.04833</guid>
<content:encoded><![CDATA[
arXiv:2508.04833v1 Announce Type: new 
Abstract: Gossip algorithms are pivotal in the dissemination of information within decentralized systems. Consequently, numerous gossip libraries have been developed and widely utilized especially in blockchain protocols for the propagation of blocks and transactions. A well-established library is libp2p, which provides two gossip algorithms: floodsup and gossibsup. These algorithms enable the delivery of published messages to a set of peers. In this work we aim to enhance the performance and reliability of libp2p by introducing OPTIMUMP2P, a novel gossip algorithm that leverages the capabilities of Random Linear Network Coding (RLNC) to expedite the dissemination of information in a peer-to-peer (P2P) network while ensuring reliable delivery, even in the presence of malicious actors capable of corrupting the transmitted data. Preliminary research from the Ethereum Foundation has demonstrated the use of RLNC in the significant improvement in the block propagation time [14]. Here we present extensive evaluation results both in simulation and real-world environments that demonstrate the performance gains of OPTIMUMP2P over the Gossipsub protocol.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graffiti: Enabling an Ecosystem of Personalized and Interoperable Social Applications</title>
<link>https://arxiv.org/abs/2508.04889</link>
<guid>https://arxiv.org/abs/2508.04889</guid>
<content:encoded><![CDATA[
arXiv:2508.04889v1 Announce Type: new 
Abstract: Most social applications, from Twitter to Wikipedia, have rigid one-size-fits-all designs, but building new social applications is both technically challenging and results in applications that are siloed away from existing communities. We present Graffiti, a system that can be used to build a wide variety of personalized social applications with relative ease that also interoperate with each other. People can freely move between a plurality of designs -- each with its own aesthetic, feature set, and moderation -- all without losing their friends or data.
  Our concept of total reification makes it possible for seemingly contradictory designs, including conflicting moderation rules, to interoperate. Conversely, our concept of channels prevents interoperation from occurring by accident, avoiding context collapse.
  Graffiti applications interact through a minimal client-side API, which we show admits at least two decentralized implementations. Above the API, we built a Vue.js plugin, which we use to develop applications similar to Twitter, Messenger, and Wikipedia using only client-side code. Our case studies explore how these and other novel applications interoperate, as well as the broader ecosystem that Graffiti enables.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compressed Decentralized Momentum Stochastic Gradient Methods for Nonconvex Optimization</title>
<link>https://arxiv.org/abs/2508.04950</link>
<guid>https://arxiv.org/abs/2508.04950</guid>
<content:encoded><![CDATA[
arXiv:2508.04950v1 Announce Type: new 
Abstract: In this paper, we design two compressed decentralized algorithms for solving nonconvex stochastic optimization under two different scenarios. Both algorithms adopt a momentum technique to achieve fast convergence and a message-compression technique to save communication costs. Though momentum acceleration and compressed communication have been used in literature, it is highly nontrivial to theoretically prove the effectiveness of their composition in a decentralized algorithm that can maintain the benefits of both sides, because of the need to simultaneously control the consensus error, the compression error, and the bias from the momentum gradient.
  For the scenario where gradients are bounded, our proposal is a compressed decentralized adaptive method. To the best of our knowledge, this is the first decentralized adaptive stochastic gradient method with compressed communication. For the scenario of data heterogeneity without bounded gradients, our proposal is a compressed decentralized heavy-ball method, which applies a gradient tracking technique to address the challenge of data heterogeneity. Notably, both methods achieve an optimal convergence rate, and they can achieve linear speed up and adopt topology-independent algorithmic parameters within a certain regime of the user-specified error tolerance. Superior empirical performance is observed over state-of-the-art methods on training deep neural networks (DNNs) and Transformers.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HFedATM: Hierarchical Federated Domain Generalization via Optimal Transport and Regularized Mean Aggregation</title>
<link>https://arxiv.org/abs/2508.05135</link>
<guid>https://arxiv.org/abs/2508.05135</guid>
<content:encoded><![CDATA[
arXiv:2508.05135v1 Announce Type: new 
Abstract: Federated Learning (FL) is a decentralized approach where multiple clients collaboratively train a shared global model without sharing their raw data. Despite its effectiveness, conventional FL faces scalability challenges due to excessive computational and communication demands placed on a single central server as the number of participating devices grows. Hierarchical Federated Learning (HFL) addresses these issues by distributing model aggregation tasks across intermediate nodes (stations), thereby enhancing system scalability and robustness against single points of failure. However, HFL still suffers from a critical yet often overlooked limitation: domain shift, where data distributions vary significantly across different clients and stations, reducing model performance on unseen target domains. While Federated Domain Generalization (FedDG) methods have emerged to improve robustness to domain shifts, their integration into HFL frameworks remains largely unexplored. In this paper, we formally introduce Hierarchical Federated Domain Generalization (HFedDG), a novel scenario designed to investigate domain shift within hierarchical architectures. Specifically, we propose HFedATM, a hierarchical aggregation method that first aligns the convolutional filters of models from different stations through Filter-wise Optimal Transport Alignment and subsequently merges aligned models using a Shrinkage-aware Regularized Mean Aggregation. Our extensive experimental evaluations demonstrate that HFedATM significantly boosts the performance of existing FedDG baselines across multiple datasets and maintains computational and communication efficiency. Moreover, theoretical analyses indicate that HFedATM achieves tighter generalization error bounds compared to standard hierarchical averaging, resulting in faster convergence and stable training behavior.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Reach for the Stars: Rethinking Topology for Resilient Federated Learning</title>
<link>https://arxiv.org/abs/2508.05224</link>
<guid>https://arxiv.org/abs/2508.05224</guid>
<content:encoded><![CDATA[
arXiv:2508.05224v1 Announce Type: new 
Abstract: Federated learning (FL) enables collaborative model training across distributed clients while preserving data privacy by keeping data local. Traditional FL approaches rely on a centralized, star-shaped topology, where a central server aggregates model updates from clients. However, this architecture introduces several limitations, including a single point of failure, limited personalization, and poor robustness to distribution shifts or vulnerability to malfunctioning clients. Moreover, update selection in centralized FL often relies on low-level parameter differences, which can be unreliable when client data is not independent and identically distributed, and offer clients little control. In this work, we propose a decentralized, peer-to-peer (P2P) FL framework. It leverages the flexibility of the P2P topology to enable each client to identify and aggregate a personalized set of trustworthy and beneficial updates.This framework is the Local Inference Guided Aggregation for Heterogeneous Training Environments to Yield Enhancement Through Agreement and Regularization (LIGHTYEAR). Central to our method is an agreement score, computed on a local validation set, which quantifies the semantic alignment of incoming updates in the function space with respect to the clients reference model. Each client uses this score to select a tailored subset of updates and performs aggregation with a regularization term that further stabilizes the training. Our empirical evaluation across two datasets shows that the proposed approach consistently outperforms both centralized baselines and existing P2P methods in terms of client-level performance, particularly under adversarial and heterogeneous conditions.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShikkhaChain: A Blockchain-Powered Academic Credential Verification System for Bangladesh</title>
<link>https://arxiv.org/abs/2508.05334</link>
<guid>https://arxiv.org/abs/2508.05334</guid>
<content:encoded><![CDATA[
arXiv:2508.05334v1 Announce Type: new 
Abstract: Academic credential fraud threatens educational integrity, especially in developing countries like Bangladesh, where verification methods are primarily manual and inefficient. To address this challenge, we present ShikkhaChain, a blockchain-powered certificate management platform designed to securely issue, verify, and revoke academic credentials in a decentralized and tamper-proof manner. Built on Ethereum smart contracts and utilizing IPFS for off-chain storage, the platform offers a transparent, scalable solution accessible through a React-based DApp with MetaMask integration. ShikkhaChain enables role-based access for governments, regulators, institutions, and public verifiers, allowing QR-based validation and on-chain revocation tracking. Our prototype demonstrates enhanced trust, reduced verification time, and improved international credibility for Bangladeshi degrees, promoting a more reliable academic and employment ecosystem.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proof-of-Useful-Work Blockchain for Trustworthy Biomedical Hyperdimensional Computing</title>
<link>https://arxiv.org/abs/2202.02964</link>
<guid>https://arxiv.org/abs/2202.02964</guid>
<content:encoded><![CDATA[
arXiv:2202.02964v2 Announce Type: replace 
Abstract: Hyperdimensional Computing (HDC) is a promising bio-inspired learning paradigm for its advantage of balancing performance and efficiency and has been increasingly applied to the bio-medical domain. In bio-medical applications, trustworthiness such as replicability and verifiability of the trained learning models is crucial. In this work, we introduce HDCoin, the first proof-of-useful-work blockchain framework for HDC. With HDCoin, we transform the conventional energy-wasteful mining process into a competitive process for developing high accuracy, trustworthy and verifiable hyperdimensional models. We explore four diverse biomedical datasets, and conduct an extensive design-space exploration of key HDC hyperparameters of blockchain miners such as dimensionality, learning rate, and retraining iterations for model performance, adaptive mining difficulty and fairness on proof-of-useful-work.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theorem-Carrying Transactions: Runtime Verification to Ensure Interface Specifications for Smart Contract Safety</title>
<link>https://arxiv.org/abs/2408.06478</link>
<guid>https://arxiv.org/abs/2408.06478</guid>
<content:encoded><![CDATA[
arXiv:2408.06478v2 Announce Type: replace 
Abstract: Security bugs and trapdoors in smart contracts have been impacting the Ethereum community since its inception. Conceptually, the 1.45-million Ethereum's contracts form a single "gigantic program" whose behaviors are determined by the complex compositions of contracts. Can programmers be assured that this gigantic program conforms to high-level safety specifications, despite unforeseeable code-level intricacies? Static code verification cannot be faithful to this gigantic program due to its scale and high polymorphism. In this paper, we present a viable approach to achieve this goal. Our technology, called Theorem-Carrying Transactions (TCT), combines the benefits of concrete execution and symbolic proofs. Under the TCT protocol, every transaction carries a theorem that proves its adherence to the specified properties in the invoked contracts, and the runtime system checks the theorem before executing the transaction. Once a theorem is proven, it will be reused for future transactions, so TCT's runtime overhead is minimal. As case studies, we demonstrate that TCT secures token contracts without foreseeing code-level intricacies, such as integer overflow and reentrancy. TCT is also successfully applied to a Uniswap codebase, showcasing a complex decentralized finance (DeFi) scenario. Our evaluation shows a negligible runtime overhead, two orders of magnitude lower than a state-of-the-art approach for runtime checking of contract code safety.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs</title>
<link>https://arxiv.org/abs/2504.06211</link>
<guid>https://arxiv.org/abs/2504.06211</guid>
<content:encoded><![CDATA[
arXiv:2504.06211v2 Announce Type: replace 
Abstract: Zero-Knowledge Proofs (ZKPs) are rapidly gaining importance in privacy-preserving and verifiable computing. ZKPs enable a proving party to prove the truth of a statement to a verifying party without revealing anything else. ZKPs have applications in blockchain technologies, verifiable machine learning, and electronic voting, but have yet to see widespread adoption due to the computational complexity of the proving process. Recent works have accelerated the key primitives of state-of-the-art ZKP protocols on GPU and ASIC. However, the protocols accelerated thus far face one of two challenges: they either require a trusted setup for each application, or they generate larger proof sizes with higher verification costs, limiting their applicability in scenarios with numerous verifiers or strict verification time constraints. This work presents an accelerator, zkSpeed, for HyperPlonk, a state-of-the-art ZKP protocol that supports both one-time, universal setup and small proof sizes for typical ZKP applications in publicly verifiable, consensus-based systems. We accelerate the entire protocol, including two major primitives: SumCheck and Multi-scalar Multiplications (MSMs). We develop a full-chip architecture using 366.46 mm$^2$ and 2 TB/s of bandwidth to accelerate the entire proof generation process, achieving geometric mean speedups of 801$\times$ over CPU baselines.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Prior: A Framework for Knowledge-Driven Prior Elicitation and Aggregation</title>
<link>https://arxiv.org/abs/2508.03766</link>
<guid>https://arxiv.org/abs/2508.03766</guid>
<content:encoded><![CDATA[
arXiv:2508.03766v1 Announce Type: new 
Abstract: The specification of prior distributions is fundamental in Bayesian inference, yet it remains a significant bottleneck. The prior elicitation process is often a manual, subjective, and unscalable task. We propose a novel framework which leverages Large Language Models (LLMs) to automate and scale this process. We introduce \texttt{LLMPrior}, a principled operator that translates rich, unstructured contexts such as natural language descriptions, data or figures into valid, tractable probability distributions. We formalize this operator by architecturally coupling an LLM with an explicit, tractable generative model, such as a Gaussian Mixture Model (forming a LLM based Mixture Density Network), ensuring the resulting prior satisfies essential mathematical properties. We further extend this framework to multi-agent systems where Logarithmic Opinion Pooling is employed to aggregate prior distributions induced by decentralized knowledge. We present the federated prior aggregation algorithm, \texttt{Fed-LLMPrior}, for aggregating distributed, context-dependent priors in a manner robust to agent heterogeneity. This work provides the foundation for a new class of tools that can potentially lower the barrier to entry for sophisticated Bayesian modeling.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Do Agents Think Others Would Do? Level-2 Inverse Games for Inferring Agents' Estimates of Others' Objectives</title>
<link>https://arxiv.org/abs/2508.03824</link>
<guid>https://arxiv.org/abs/2508.03824</guid>
<content:encoded><![CDATA[
arXiv:2508.03824v1 Announce Type: new 
Abstract: Effectively interpreting strategic interactions among multiple agents requires us to infer each agent's objective from limited information. Existing inverse game-theoretic approaches frame this challenge in terms of a "level-1" inference problem, in which we take the perspective of a third-party observer and assume that individual agents share complete knowledge of one another's objectives. However, this assumption breaks down in decentralized, real-world decision scenarios like urban driving and bargaining, in which agents may act based on conflicting views of one another's objectives. We demonstrate the necessity of inferring agents' heterogeneous estimates of each other's objectives through empirical examples, and by theoretically characterizing the prediction error of level-1 inference on fictitious gameplay data from linear-quadratic games. To address this fundamental issue, we propose a framework for level-2 inference to address the question: "What does each agent believe about all agents' objectives?" We prove that the level-2 inference problem is non-convex even in benign settings like linear-quadratic games, and we develop an efficient gradient-based approach for identifying local solutions. Experiments on a synthetic urban driving example show that our approach uncovers nuanced misalignments that level-1 methods miss.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced DAG-Based Ranking (ADR) Protocol for Blockchain Scalability</title>
<link>https://arxiv.org/abs/2508.04000</link>
<guid>https://arxiv.org/abs/2508.04000</guid>
<content:encoded><![CDATA[
arXiv:2508.04000v1 Announce Type: new 
Abstract: In the past decade, blockchain has emerged as a promising solution for building secure distributed ledgers and has attracted significant attention. However, current blockchain systems suffer from limited throughput, poor scalability, and high latency. Due to limitations in consensus mechanisms, especially in managing node identities, blockchain is often considered unsuitable for applications such as the Internet of Things (IoT). This paper proposes the Advanced DAG-based Ranking (ADR) protocol to enhance blockchain scalability and throughput. ADR employs a directed acyclic graph (DAG) structure where nodes are positioned based on their rankings. Unlike traditional chains, ADR allows honest nodes to write blocks and verify transactions using a DAG-based topology. The protocol follows a three-step approach to secure the network against double-spending and enhance performance. First, it verifies nodes using their public and private keys before granting entry. Second, it builds an advanced DAG ledger enabling block production and transaction validation. Third, a ranking algorithm filters out malicious nodes, ranks the remaining nodes based on performance, and arranges them topologically. This process increases throughput and ensures robust scalability. We evaluated ADR on Amazon EC2 clusters with over 100 nodes, including scenarios with injected malicious nodes. Simulation results demonstrate that ADR significantly improves transaction throughput and network liveness compared to existing DAG-based blockchains such as IOTA and ByteBall, making it well-suited for IoT applications.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metaverse Framework for Wireless Systems Management</title>
<link>https://arxiv.org/abs/2508.04150</link>
<guid>https://arxiv.org/abs/2508.04150</guid>
<content:encoded><![CDATA[
arXiv:2508.04150v1 Announce Type: new 
Abstract: This article introduces a comprehensive metaverse framework, which is designed for the simulation, emulation, and interaction with wireless systems. The proposed framework integrates core metaverse technologies such as extended reality (XR), digital twins (DTs), artificial intelligence (AI), internet of things (IoT), blockchain, and advanced 6G networking solutions to create a dynamic, immersive platform for both system development and management. By leveraging XR, users can visualize and engage with complex systems, while DTs enable real-time monitoring and optimization. AI generates the three-dimensional (3D) content, enhances decision-making and system performance, whereas IoT devices provide real-time sensor data for boosting the simulation accuracy. Additionally, blockchain ensures secure, decentralized interactions, and 5G/6G networks offer the necessary infrastructure for seamless, low-latency communication. This framework serves as a robust tool for exploring, developing, and optimizing wireless systems, aiming to provide valuable insights into the future of networked environments.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generic Framework for Optimization in Blockchain Simulators</title>
<link>https://arxiv.org/abs/2508.04157</link>
<guid>https://arxiv.org/abs/2508.04157</guid>
<content:encoded><![CDATA[
arXiv:2508.04157v1 Announce Type: new 
Abstract: As blockchain technology rapidly evolves, researchers face a significant challenge due to diverse and non-standardized simulation parameters, which hinder the replicability and comparability of research methodologies. This paper introduces a Generic Framework for Optimization in Blockchain Simulators (GFOBS), a comprehensive and adaptable solution designed to standardize and optimize blockchain simulations. GFOBS provides a flexible platform that supports various optimization algorithms, variables, and objectives, thereby catering to a wide range of blockchain research needs. The paper's key contributions are threefold: the development of GFOBS as a versatile tool for blockchain simulation optimization; the introduction of an innovative optimization method using warm starting technique; and the proposition of a novel concurrent multiprocessing technique for simultaneous simulation processes. These advancements collectively enhance the efficiency, replicability, and standardization of blockchain simulation experiments.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information Bulletin Strategy in Impatient Queuing</title>
<link>https://arxiv.org/abs/2508.04241</link>
<guid>https://arxiv.org/abs/2508.04241</guid>
<content:encoded><![CDATA[
arXiv:2508.04241v1 Announce Type: new 
Abstract: In Sixth Generation (6G) networks, decentralized control in multi-tenant systems is a suggested enabler for autonomous network operations. However, autonomy requires independent rationale decisions be taken by tenants. This rationality can only be underpinned by timely and continuous access to status information. Despite its importance, the questions of what information should be shared, how much should be communicated, and how frequently updates should be dispatched remain open research challenges.
  This manuscript proposes an information bulletin strategy defined around two models of the system descriptor states to address these fundamental questions. The strategy is that queues periodically broadcast these information models to tenants at different time intervals, who may respond by reneging from the queue or jockeying to a more favorable one. The expectation is that over time, the queues adapt their processing rates based on what they learn from the tenant behavior. The objective is to minimize overall delay and the impatience. We formulate for this impatience as an optimization problem, whose analytical solution is intractable. We perform numerical experiments to evaluate the performance of the learned queue policy and to assess how closely it approaches optimal conditions.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SelectiveShield: Lightweight Hybrid Defense Against Gradient Leakage in Federated Learning</title>
<link>https://arxiv.org/abs/2508.04265</link>
<guid>https://arxiv.org/abs/2508.04265</guid>
<content:encoded><![CDATA[
arXiv:2508.04265v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative model training on decentralized data but remains vulnerable to gradient leakage attacks that can reconstruct sensitive user information. Existing defense mechanisms, such as differential privacy (DP) and homomorphic encryption (HE), often introduce a trade-off between privacy, model utility, and system overhead, a challenge that is exacerbated in heterogeneous environments with non-IID data and varying client capabilities. To address these limitations, we propose SelectiveShield, a lightweight hybrid defense framework that adaptively integrates selective homomorphic encryption and differential privacy. SelectiveShield leverages Fisher information to quantify parameter sensitivity, allowing clients to identify critical parameters locally. Through a collaborative negotiation protocol, clients agree on a shared set of the most sensitive parameters for protection via homomorphic encryption. Parameters that are uniquely important to individual clients are retained locally, fostering personalization, while non-critical parameters are protected with adaptive differential privacy noise. Extensive experiments demonstrate that SelectiveShield maintains strong model utility while significantly mitigating gradient leakage risks, offering a practical and scalable defense mechanism for real-world federated learning deployments.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plant-Centric Metaverse: A Biocentric-Creation Framework for Ecological Art and Digital Symbiosis</title>
<link>https://arxiv.org/abs/2508.04391</link>
<guid>https://arxiv.org/abs/2508.04391</guid>
<content:encoded><![CDATA[
arXiv:2508.04391v1 Announce Type: new 
Abstract: Digital ecological art represents an emergent frontier where biological media converge with virtual environments. This study examines the paradigm shift from anthropocentric to plant-centered artistic narratives within the metaverse, contextualizing how digital platforms transform ecological expression. However, current frameworks fail to systematically guide artists in leveraging plant agency for digital symbiosis that transcends human-centered creation. We propose the Biocentric-Creation Transformation Ideology (BCTI) framework and validate it through multimodal case studies spanning bio-art, NFTs, and VR ecosystems (2013-2023). Our analysis reveals: (1) Metaverse ecosystems enable unprecedented plant-algorithm co-creation, with biological artworks increasing by 133% in premier archives (2020 vs 2013); (2) Digital symbiosis manifests through blockchain DAOs where plants govern human-plant collaborations; (3) Algorithmic photosynthesis in VR environments reshapes ecological aesthetics through real-time biodata translation. The BCTI framework advances ecological art theory by systematizing the transition from representation to plant-centered agency, offering artists a blueprint for post-anthropocene creation. This redefines environmental consciousness in virtual realms while establishing new protocols for cross-species digital collaboration.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>4-Swap: Achieving Grief-Free and Bribery-Safe Atomic Swaps Using Four Transactions</title>
<link>https://arxiv.org/abs/2508.04641</link>
<guid>https://arxiv.org/abs/2508.04641</guid>
<content:encoded><![CDATA[
arXiv:2508.04641v1 Announce Type: new 
Abstract: Cross-chain asset exchange is crucial for blockchain interoperability. Existing solutions rely on trusted third parties and risk asset loss, or use decentralized alternatives like atomic swaps, which suffer from grief attacks. Griefing occurs when a party prematurely exits, locking the counterparty's assets until a timelock expires. Hedged Atomic Swaps mitigate griefing by introducing a penalty premium; however, they increase the number of transactions from four (as in Tier Nolan's swap) to six, which in turn introduces new griefing risks. Grief-Free (GF) Swap reduces this to five transactions by consolidating assets and premiums on a single chain. However, no existing protocol achieves grief-free asset exchange in just four transactions.
  This paper presents 4-Swap, the first cross-chain atomic swap protocol that is both grief-free and bribery-safe, while completing asset exchange in just four transactions. By combining the griefing premium and principal into a single transaction per chain, 4-Swap reduces on-chain transactions, leading to faster execution compared to previous grief-free solutions. It is fully compatible with Bitcoin and operates without the need for any new opcodes. A game-theoretic analysis shows that rational participants have no incentive to deviate from the protocol, ensuring robust compliance and security.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inequality in the Age of Pseudonymity</title>
<link>https://arxiv.org/abs/2508.04668</link>
<guid>https://arxiv.org/abs/2508.04668</guid>
<content:encoded><![CDATA[
arXiv:2508.04668v1 Announce Type: new 
Abstract: Inequality measures such as the Gini coefficient are used to inform and motivate policymaking, and are increasingly applied to digital platforms. We analyze how measures fare in pseudonymous settings, as common to internet-based or blockchain-based platforms. One key challenge that arises is the ability of actors to create multiple fake identities under fictitious false names, also known as ``Sybils.'' While some actors may do so to preserve their privacy, we show that this can inadvertently distort inequality metrics. As we show, when using inequality measures that satisfy literature's canonical set of desired properties, the presence of Sybils in an economy implies that it is impossible to properly measure the economy's inequality. Then, we present several classes of Sybil-proof measures that satisfy relaxed versions of the aforementioned desired properties, and, by fully characterizing them, we prove that the structure imposed restricts their ability to assess inequality at a fine-grained level. In addition, we prove that popular inequality metrics, including the famous Gini coefficient, are vulnerable to Sybil manipulations, and examine the dynamics that result in the creation of Sybils, whether in pseudonymous settings or traditional ones.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoK: Stablecoins for Digital Transformation -- Design, Metrics, and Application with Real World Asset Tokenization as a Case Study</title>
<link>https://arxiv.org/abs/2508.02403</link>
<guid>https://arxiv.org/abs/2508.02403</guid>
<content:encoded><![CDATA[
arXiv:2508.02403v1 Announce Type: cross 
Abstract: Stablecoins have become a foundational component of the digital asset ecosystem, with their market capitalization exceeding 230 billion USD as of May 2025. As fiat-referenced and programmable assets, stablecoins provide low-latency, globally interoperable infrastructure for payments, decentralized finance, DeFi, and tokenized commerce. Their accelerated adoption has prompted extensive regulatory engagement, exemplified by the European Union's Markets in Crypto-assets Regulation, MiCA, the US Guiding and Establishing National Innovation for US Stablecoins Act, GENIUS Act, and Hong Kong's Stablecoins Bill. Despite this momentum, academic research remains fragmented across economics, law, and computer science, lacking a unified framework for design, evaluation, and application. This study addresses that gap through a multi-method research design. First, it synthesizes cross-disciplinary literature to construct a taxonomy of stablecoin systems based on custodial structure, stabilization mechanism, and governance. Second, it develops a performance evaluation framework tailored to diverse stakeholder needs, supported by an open-source benchmarking pipeline to ensure transparency and reproducibility. Third, a case study on Real World Asset tokenization illustrates how stablecoins operate as programmable monetary infrastructure in cross-border digital systems. By integrating conceptual theory with empirical tools, the paper contributes: a unified taxonomy for stablecoin design; a stakeholder-oriented performance evaluation framework; an empirical case linking stablecoins to sectoral transformation; and reproducible methods and datasets to inform future research. These contributions support the development of trusted, inclusive, and transparent digital monetary infrastructure.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Patterns in the Blockchain: Analysis of EOAs and Smart Contracts in ERC20 Token Networks</title>
<link>https://arxiv.org/abs/2508.04671</link>
<guid>https://arxiv.org/abs/2508.04671</guid>
<content:encoded><![CDATA[
arXiv:2508.04671v1 Announce Type: cross 
Abstract: Scaling laws offer a powerful lens to understand complex transactional behaviors in decentralized systems. This study reveals distinctive statistical signatures in the transactional dynamics of ERC20 tokens on the Ethereum blockchain by examining over 44 million token transfers between July 2017 and March 2018 (9-month period). Transactions are categorized into four types: EOA--EOA, EOA--SC, SC-EOA, and SC-SC based on whether the interacting addresses are Externally Owned Accounts (EOAs) or Smart Contracts (SCs), and analyzed across three equal periods (each of 3 months). To identify universal statistical patterns, we investigate the presence of two canonical scaling laws: power law distributions and temporal Taylor's law (TL). EOA-driven transactions exhibit consistent statistical behavior, including a near-linear relationship between trade volume and unique partners with stable power law exponents ($\gamma \approx 2.3$), and adherence to TL with scaling coefficients ($\beta \approx 2.3$). In contrast, interactions involving SCs, especially SC-SC, exhibit sublinear scaling, unstable power-law exponents, and significantly fluctuating Taylor coefficients (variation in $\beta$ to be $\Delta\beta = 0.51$). Moreover, SC-driven activity displays heavier-tailed distributions ($\gamma < 2$), indicating bursty and algorithm-driven activity. These findings reveal the characteristic differences between human-controlled and automated transaction behaviors in blockchain ecosystems. By uncovering universal scaling behaviors through the integration of complex systems theory and blockchain data analytics, this work provides a principled framework for understanding the underlying mechanisms of decentralized financial systems.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CityLight: A Neighborhood-inclusive Universal Model for Coordinated City-scale Traffic Signal Control</title>
<link>https://arxiv.org/abs/2406.02126</link>
<guid>https://arxiv.org/abs/2406.02126</guid>
<content:encoded><![CDATA[
arXiv:2406.02126v4 Announce Type: replace 
Abstract: City-scale traffic signal control (TSC) involves thousands of heterogeneous intersections with varying topologies, making cooperative decision-making across intersections particularly challenging. Given the prohibitive computational cost of learning individual policies for each intersection, some researchers explore learning a universal policy to control each intersection in a decentralized manner, where the key challenge is to construct a universal representation method for heterogeneous intersections. However, existing methods are limited to universally representing information of heterogeneous ego intersections, neglecting the essential representation of influence from their heterogeneous neighbors. Universally incorporating neighborhood information is nontrivial due to the intrinsic complexity of traffic flow interactions, as well as the challenge of modeling collective influences from neighbor intersections. To address these challenges, we propose CityLight, which learns a universal policy based on representations obtained with two major modules: a Neighbor Influence Encoder to explicitly model neighbor's influence with specified traffic flow relation and connectivity to the ego intersection; a Neighbor Influence Aggregator to attentively aggregate the influence of neighbors based on their mutual competitive relations. Extensive experiments on five city-scale datasets, ranging from 97 to 13,952 intersections, confirm the efficacy of CityLight, with an average throughput improvement of 11.68% and a lift of 22.59% for generalization.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAILGUN: A Unified Convolutional Policy for Multi-Agent Path Finding Across Different Environments and Tasks</title>
<link>https://arxiv.org/abs/2503.02992</link>
<guid>https://arxiv.org/abs/2503.02992</guid>
<content:encoded><![CDATA[
arXiv:2503.02992v2 Announce Type: replace 
Abstract: Multi-Agent Path Finding (MAPF), which focuses on finding collision-free paths for multiple robots, is crucial for applications ranging from aerial swarms to warehouse automation. Solving MAPF is NP-hard so learning-based approaches for MAPF have gained attention, particularly those leveraging deep neural networks. Nonetheless, despite the community's continued efforts, all learning-based MAPF planners still rely on decentralized planning due to variability in the number of agents and map sizes. We have developed the first centralized learning-based policy for MAPF problem called RAILGUN. RAILGUN is not an agent-based policy but a map-based policy. By leveraging a CNN-based architecture, RAILGUN can generalize across different maps and handle any number of agents. We collect trajectories from rule-based methods to train our model in a supervised way. In experiments, RAILGUN outperforms most baseline methods and demonstrates great zero-shot generalization capabilities on various tasks, maps and agent numbers that were not seen in the training dataset.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Slow is Fast! Dissecting Ethereum's Slow Liquidity Drain Scams</title>
<link>https://arxiv.org/abs/2503.04850</link>
<guid>https://arxiv.org/abs/2503.04850</guid>
<content:encoded><![CDATA[
arXiv:2503.04850v3 Announce Type: replace 
Abstract: We identify the slow liquidity drain (SLID) scam, an insidious and highly profitable threat to decentralized finance (DeFi), posing a large-scale, persistent, and growing risk to the ecosystem. Unlike traditional scams such as rug pulls or honeypots (USENIX Sec'19, USENIX Sec'23), SLID gradually siphons funds from liquidity pools over extended periods, making detection significantly more challenging. In this paper, we conducted the first large-scale empirical analysis of 319,166 liquidity pools across six major decentralized exchanges (DEXs) since 2018. We identified 3,117 SLID affected liquidity pools, resulting in cumulative losses of more than US$103 million. We propose a rule-based heuristic and an enhanced machine learning model for early detection. Our machine learning model achieves a detection speed 4.77 times faster than the heuristic while maintaining 95% accuracy. Our study establishes a foundation for protecting DeFi investors at an early stage and promoting transparency in the DeFi ecosystem.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Web3 x AI Agents: Landscape, Integrations, and Foundational Challenges</title>
<link>https://arxiv.org/abs/2508.02773</link>
<guid>https://arxiv.org/abs/2508.02773</guid>
<content:encoded><![CDATA[
arXiv:2508.02773v1 Announce Type: new 
Abstract: The convergence of Web3 technologies and AI agents represents a rapidly evolving frontier poised to reshape decentralized ecosystems. This paper presents the first and most comprehensive analysis of the intersection between Web3 and AI agents, examining five critical dimensions: landscape, economics, governance, security, and trust mechanisms. Through an analysis of 133 existing projects, we first develop a taxonomy and systematically map the current market landscape (RQ1), identifying distinct patterns in project distribution and capitalization. Building upon these findings, we further investigate four key integrations: (1) the role of AI agents in participating in and optimizing decentralized finance (RQ2); (2) their contribution to enhancing Web3 governance mechanisms (RQ3); (3) their capacity to strengthen Web3 security via intelligent vulnerability detection and automated smart contract auditing (RQ4); and (4) the establishment of robust reliability frameworks for AI agent operations leveraging Web3's inherent trust infrastructure (RQ5). By synthesizing these dimensions, we identify key integration patterns, highlight foundational challenges related to scalability, security, and ethics, and outline critical considerations for future research toward building robust, intelligent, and trustworthy decentralized systems with effective AI agent interactions.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Fast Adaptation of Delayed Clients in Decentralized Federated Learning: A Centroid-Aligned Distillation Approach</title>
<link>https://arxiv.org/abs/2508.02993</link>
<guid>https://arxiv.org/abs/2508.02993</guid>
<content:encoded><![CDATA[
arXiv:2508.02993v1 Announce Type: new 
Abstract: Decentralized Federated Learning (DFL) struggles with the slow adaptation of late-joining delayed clients and high communication costs in asynchronous environments. These limitations significantly hinder overall performance. To address this, we propose DFedCAD, a novel framework for rapid adaptation via Centroid-Aligned Distillation. DFedCAD first employs Weighted Cluster Pruning (WCP) to compress models into representative centroids, drastically reducing communication overhead. It then enables delayed clients to intelligently weigh and align with peer knowledge using a novel structural distance metric and a differentiable k-means distillation module, facilitating efficient end-to-end knowledge transfer. Extensive experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet show that DFedCAD consistently achieves state-of-the-art performance, attaining the highest accuracy across all evaluated settings while reducing communication overhead by over 86%. Our framework provides a scalable and practical solution for efficient decentralized learning in dynamic, real-world scenarios.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of AI Agent Registry Solutions</title>
<link>https://arxiv.org/abs/2508.03095</link>
<guid>https://arxiv.org/abs/2508.03095</guid>
<content:encoded><![CDATA[
arXiv:2508.03095v1 Announce Type: new 
Abstract: As As autonomous AI agents scale across cloud, enterprise, and decentralized environments, the need for standardized registry systems to support discovery, identity, and capability sharing has become essential. This paper surveys three prominent registry approaches each defined by a unique metadata model: MCP's mcp.json, A2A's Agent Card, and NANDA's AgentFacts. MCP uses a centralized metaregistry with GitHub authenticated publishing and structured metadata for server discovery. A2A enables decentralized interaction via JSON-based Agent Cards, discoverable through well-known URIs, curated catalogs, or direct configuration. NANDA Index introduces AgentFacts, a cryptographically verifiable and privacy-preserving metadata model designed for dynamic discovery, credentialed capabilities, and cross-domain interoperability. These approaches are compared across four dimensions: security, scalability, authentication, and maintainability. The paper concludes with suggestions and recommendations to guide future design and adoption of registry systems for the Internet of AI Agents.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedPromo: Federated Lightweight Proxy Models at the Edge Bring New Domains to Foundation Models</title>
<link>https://arxiv.org/abs/2508.03356</link>
<guid>https://arxiv.org/abs/2508.03356</guid>
<content:encoded><![CDATA[
arXiv:2508.03356v1 Announce Type: new 
Abstract: Federated Learning (FL) is an established paradigm for training deep learning models on decentralized data. However, as the size of the models grows, conventional FL approaches often require significant computational resources on client devices, which may not be feasible. We introduce FedPromo, a novel framework that enables efficient adaptation of large-scale foundation models stored on a central server to new domains encountered only by remote clients. Instead of directly training the large model on client devices, FedPromo optimizes lightweight proxy models via FL, significantly reducing computational overhead while maintaining privacy. Our method follows a two-stage process: first, server-side knowledge distillation aligns the representations of a large-scale foundation model (e.g., a transformer) with those of a compact counterpart (e.g., a CNN). Then, the compact model encoder is deployed to client devices, where trainable classifiers are learned locally. These classifiers are subsequently aggregated and seamlessly transferred back to the foundation model, facilitating personalized adaptation without requiring direct access to user data. Through novel regularization strategies, our framework enables decentralized multi-domain learning, balancing performance, privacy, and resource efficiency. Extensive experiments on five image classification benchmarks demonstrate that FedPromo outperforms existing methods while assuming limited-resource clients.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Classical and Quantum Models for DeFi Yield Prediction on Curve Finance</title>
<link>https://arxiv.org/abs/2508.02685</link>
<guid>https://arxiv.org/abs/2508.02685</guid>
<content:encoded><![CDATA[
arXiv:2508.02685v1 Announce Type: cross 
Abstract: The rise of decentralized finance (DeFi) has created a growing demand for accurate yield and performance forecasting to guide liquidity allocation strategies. In this study, we benchmark six models, XGBoost, Random Forest, LSTM, Transformer, quantum neural networks (QNN), and quantum support vector machines with quantum feature maps (QSVM-QNN), on one year of historical data from 28 Curve Finance pools. We evaluate model performance on test MAE, RMSE, and directional accuracy. Our results show that classical ensemble models, particularly XGBoost and Random Forest, consistently outperform both deep learning and quantum models. XGBoost achieves the highest directional accuracy (71.57%) with a test MAE of 1.80, while Random Forest attains the lowest test MAE of 1.77 and 71.36% accuracy. In contrast, quantum models underperform with directional accuracy below 50% and higher errors, highlighting current limitations in applying quantum machine learning to real-world DeFi time series data. This work offers a reproducible benchmark and practical insights into model suitability for DeFi applications, emphasizing the robustness of classical methods over emerging quantum approaches in this domain.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Graph-based Concurrency Control for Long-running Update Transactions (Extended Version)</title>
<link>https://arxiv.org/abs/2210.04179</link>
<guid>https://arxiv.org/abs/2210.04179</guid>
<content:encoded><![CDATA[
arXiv:2210.04179v2 Announce Type: replace 
Abstract: This paper proposes Oze, a concurrency control protocol that handles heterogeneous workloads, including long-running update transactions. Oze explores a large scheduling space using a multi-version serialization graph to reduce false positives. Oze manages the graph in a decentralized manner to exploit many cores in modern servers. We further propose an OLTP benchmark, BoMB (Bill of Materials Benchmark), based on a use case in an actual manufacturing company. BoMB consists of one long-running update transaction and five short transactions that conflict with each other. Experiments using BoMB show that Oze can handle the long-running update transaction while achieving four orders of magnitude higher throughput than state-of-the-art optimistic and multi-version protocols and up to five times higher throughput than pessimistic protocols. We also show Oze performs comparably with existing techniques even in a typical OLTP workload, TPC-C, thanks to a protocol switching mechanism.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TVDO: Tchebycheff Value-Decomposition Optimization for Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2306.13979</link>
<guid>https://arxiv.org/abs/2306.13979</guid>
<content:encoded><![CDATA[
arXiv:2306.13979v2 Announce Type: replace 
Abstract: In cooperative multiagent reinforcement learning (MARL), centralized training with decentralized execution (CTDE) has recently attracted more attention due to the physical demand. However, the most dilemma therein is the inconsistency between jointly-trained policies and individually-executed actions. In this article, we propose a factorized Tchebycheff value-decomposition optimization (TVDO) method to overcome the trouble of inconsistency. In particular, a nonlinear Tchebycheff aggregation function is formulated to realize the global optimum by tightly constraining the upper bound of individual action-value bias, which is inspired by the Tchebycheff method of multi-objective optimization. We theoretically prove that, under no extra limitations, the factorized value decomposition with Tchebycheff aggregation satisfies the sufficiency and necessity of Individual-Global-Max (IGM), which guarantees the consistency between the global and individual optimal action-value function. Empirically, in the climb and penalty game, we verify that TVDO precisely expresses the global-to-individual value decomposition with a guarantee of policy consistency. Meanwhile, we evaluate TVDO in the SMAC benchmark, and extensive experiments demonstrate that TVDO achieves a significant performance superiority over some SOTA MARL baselines.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trusted Routing for Blockchain-Empowered UAV Networks via Multi-Agent Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.00938</link>
<guid>https://arxiv.org/abs/2508.00938</guid>
<content:encoded><![CDATA[
arXiv:2508.00938v1 Announce Type: new 
Abstract: Due to the high flexibility and versatility, unmanned aerial vehicles (UAVs) are leveraged in various fields including surveillance and disaster rescue.However, in UAV networks, routing is vulnerable to malicious damage due to distributed topologies and high dynamics. Hence, ensuring the routing security of UAV networks is challenging. In this paper, we characterize the routing process in a time-varying UAV network with malicious nodes. Specifically, we formulate the routing problem to minimize the total delay, which is an integer linear programming and intractable to solve. Then, to tackle the network security issue, a blockchain-based trust management mechanism (BTMM) is designed to dynamically evaluate trust values and identify low-trust UAVs. To improve traditional practical Byzantine fault tolerance algorithms in the blockchain, we propose a consensus UAV update mechanism. Besides, considering the local observability, the routing problem is reformulated into a decentralized partially observable Markov decision process. Further, a multi-agent double deep Q-network based routing algorithm is designed to minimize the total delay. Finally, simulations are conducted with attacked UAVs and numerical results show that the delay of the proposed mechanism decreases by 13.39$\%$, 12.74$\%$, and 16.6$\%$ than multi-agent proximal policy optimal algorithms, multi-agent deep Q-network algorithms, and methods without BTMM, respectively.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soft Separation and Distillation: Toward Global Uniformity in Federated Unsupervised Learning</title>
<link>https://arxiv.org/abs/2508.01251</link>
<guid>https://arxiv.org/abs/2508.01251</guid>
<content:encoded><![CDATA[
arXiv:2508.01251v1 Announce Type: new 
Abstract: Federated Unsupervised Learning (FUL) aims to learn expressive representations in federated and self-supervised settings. The quality of representations learned in FUL is usually determined by uniformity, a measure of how uniformly representations are distributed in the embedding space. However, existing solutions perform well in achieving intra-client (local) uniformity for local models while failing to achieve inter-client (global) uniformity after aggregation due to non-IID data distributions and the decentralized nature of FUL. To address this issue, we propose Soft Separation and Distillation (SSD), a novel approach that preserves inter-client uniformity by encouraging client representations to spread toward different directions. This design reduces interference during client model aggregation, thereby improving global uniformity while preserving local representation expressiveness. We further enhance this effect by introducing a projector distillation module to address the discrepancy between loss optimization and representation quality. We evaluate SSD in both cross-silo and cross-device federated settings, demonstrating consistent improvements in representation quality and task performance across various training scenarios. Our results highlight the importance of inter-client uniformity in FUL and establish SSD as an effective solution to this challenge. Project page: https://ssd-uniformity.github.io/
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blockchain security based on cryptography: a review</title>
<link>https://arxiv.org/abs/2508.01280</link>
<guid>https://arxiv.org/abs/2508.01280</guid>
<content:encoded><![CDATA[
arXiv:2508.01280v1 Announce Type: new 
Abstract: As an emerging service framework built by combining cryptography, P2P network, consensus mechanism and innovative contract technology, blockchain has been widely used in digital finance, data sharing, message traceability and electronic evidence preservation because of its decentralised, non-tamperable and transaction traceability. However, with the complex and changeable application scenarios of blockchain technology and the continuous enhancement of blockchain attack technology, the security of the blockchain system has been seriously threatened, dramatically affecting the development and application of blockchain technology. This paper aims to analyse the attacks on blockchain from the perspective of cryptography. Firstly, from the cryptography technology in the blockchain, the principle of hash functions, digital signatures, and other technologies, as well as their role in the blockchain, are introduced. Then, based on the six-layer architecture of the blockchain, the attacks on the data layer, the network layer, the consensus layer, the contract layer, the incentive layer and the application layer are analysed, and the methods to mitigate or resist the attacks are proposed. Secondly, the attack principles of 51% attack, Double-Spending attack, Reentrancy attack, Replay attack, Sybil attack and Timestamp Tampering attack were analysed, and the mitigation or defence solutions for these six attacks were designed. Finally, the core problems to be solved in blockchain technology are summarised, and the future development of blockchain security technology is projected.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BlockA2A: Towards Secure and Verifiable Agent-to-Agent Interoperability</title>
<link>https://arxiv.org/abs/2508.01332</link>
<guid>https://arxiv.org/abs/2508.01332</guid>
<content:encoded><![CDATA[
arXiv:2508.01332v1 Announce Type: new 
Abstract: The rapid adoption of agentic AI, powered by large language models (LLMs), is transforming enterprise ecosystems with autonomous agents that execute complex workflows. Yet we observe several key security vulnerabilities in LLM-driven multi-agent systems (MASes): fragmented identity frameworks, insecure communication channels, and inadequate defenses against Byzantine agents or adversarial prompts. In this paper, we present the first systematic analysis of these emerging multi-agent risks and explain why the legacy security strategies cannot effectively address these risks. Afterwards, we propose BlockA2A, the first unified multi-agent trust framework that enables secure and verifiable and agent-to-agent interoperability. At a high level, BlockA2A adopts decentralized identifiers (DIDs) to enable fine-grained cross-domain agent authentication, blockchain-anchored ledgers to enable immutable auditability, and smart contracts to dynamically enforce context-aware access control policies. BlockA2A eliminates centralized trust bottlenecks, ensures message authenticity and execution integrity, and guarantees accountability across agent interactions. Furthermore, we propose a Defense Orchestration Engine (DOE) that actively neutralizes attacks through real-time mechanisms, including Byzantine agent flagging, reactive execution halting, and instant permission revocation. Empirical evaluations demonstrate BlockA2A's effectiveness in neutralizing prompt-based, communication-based, behavioral and systemic MAS attacks. We formalize its integration into existing MAS and showcase a practical implementation for Google's A2A protocol. Experiments confirm that BlockA2A and DOE operate with sub-second overhead, enabling scalable deployment in production LLM-based MAS environments.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UEChecker: Detecting Unchecked External Call Vulnerabilities in DApps via Graph Analysis</title>
<link>https://arxiv.org/abs/2508.01343</link>
<guid>https://arxiv.org/abs/2508.01343</guid>
<content:encoded><![CDATA[
arXiv:2508.01343v1 Announce Type: new 
Abstract: The increasing number of attacks on the contract layer of DApps has resulted in economic losses amounting to $66 billion. Vulnerabilities arise when contracts interact with external protocols without verifying the results of the calls, leading to exploit entry points such as flash loan attacks and reentrancy attacks. In this paper, we propose UEChecker, a deep learning-based tool that utilizes a call graph and a Graph Convolutional Network to detect unchecked external call vulnerabilities. We design the following components: An edge prediction module that reconstructs the feature representation of nodes and edges in the call graph; A node aggregation module that captures structural information from both the node itself and its neighbors, thereby enhancing feature representation between nodes and improving the model's understanding of the global graph structure; A Conformer Block module that integrates multi-head attention, convolutional modules, and feedforward neural networks to more effectively capture dependencies of different scales within the call graph, extending beyond immediate neighbors and enhancing the performance of vulnerability detection. Finally, we combine these modules with Graph Convolutional Network to detect unchecked external call vulnerabilities. By auditing the smart contracts of 608 DApps, our results show that our tool achieves an accuracy of 87.59% in detecting unchecked external call vulnerabilities. Furthermore, we compare our tool with GAT, LSTM, and GCN baselines, and in the comparison experiments, UEChecker consistently outperforms these models in terms of accuracy.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiCFV: Detecting Control Flow Vulnerabilities in Smart Contracts Leveraging Multimodal Deep Learning</title>
<link>https://arxiv.org/abs/2508.01346</link>
<guid>https://arxiv.org/abs/2508.01346</guid>
<content:encoded><![CDATA[
arXiv:2508.01346v1 Announce Type: new 
Abstract: The introduction of smart contract functionality marks the advent of the blockchain 2.0 era, enabling blockchain technology to support digital currency transactions and complex distributed applications. However, many smart contracts have been found to contain vulnerabilities and errors, leading to the loss of assets within the blockchain. Despite a range of tools that have been developed to identify vulnerabilities in smart contracts at the source code or bytecode level, most rely on a single modality, reducing performance, accuracy, and limited generalization capabilities. This paper proposes a multimodal deep learning approach, MultiCFV, which is designed specifically to analyze and detect erroneous control flow vulnerability, as well as identify code clones in smart contracts. Bytecode is generated from source code to construct control flow graphs, with graph embedding techniques extracting graph features. Abstract syntax trees are used to obtain syntax features, while code comments capture key commentary words and comment features. These three feature vectors are fused to create a database for code inspection, which is used to detect similar code and identify contract vulnerabilities. Experimental results demonstrate our method effectively combines structural, syntactic, and semantic information, improving the accuracy of smart contract vulnerability detection and clone detection.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convergence Analysis of Aggregation-Broadcast in LoRA-enabled Federated Learning</title>
<link>https://arxiv.org/abs/2508.01348</link>
<guid>https://arxiv.org/abs/2508.01348</guid>
<content:encoded><![CDATA[
arXiv:2508.01348v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative model training across decentralized data sources while preserving data privacy. However, the growing size of Machine Learning (ML) models poses communication and computation challenges in FL. Low-Rank Adaptation (LoRA) has recently been introduced into FL as an efficient fine-tuning method, reducing communication overhead by updating only a small number of trainable parameters. Despite its effectiveness, how to aggregate LoRA-updated local models on the server remains a critical and understudied problem. In this paper, we provide a unified convergence analysis for LoRA-based FL. We first categories the current aggregation method into two major type: Sum-Product (SP) and Product-Sum (PS). Then we formally define the Aggregation-Broadcast Operator (ABO) and derive a general convergence condition under mild assumptions. Furthermore, we present several sufficient conditions that guarantee convergence of the global model. These theoretical analyze offer a principled understanding of various aggregation strategies. Notably, we prove that the SP and PS aggregation methods both satisfy our convergence condition, but differ in their ability to achieve the optimal convergence rate. Extensive experiments on standard benchmarks validate our theoretical findings.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NATLM: Detecting Defects in NFT Smart Contracts Leveraging LLM</title>
<link>https://arxiv.org/abs/2508.01351</link>
<guid>https://arxiv.org/abs/2508.01351</guid>
<content:encoded><![CDATA[
arXiv:2508.01351v1 Announce Type: new 
Abstract: Security issues are becoming increasingly significant with the rapid evolution of Non-fungible Tokens (NFTs). As NFTs are traded as digital assets, they have emerged as prime targets for cyber attackers. In the development of NFT smart contracts, there may exist undiscovered defects that could lead to substantial financial losses if exploited. To tackle this issue, this paper presents a framework called NATLM(NFT Assistant LLM), designed to detect potential defects in NFT smart contracts. The framework effectively identifies four common types of vulnerabilities in NFT smart contracts: ERC-721 Reentrancy, Public Burn, Risky Mutable Proxy, and Unlimited Minting. Relying exclusively on large language models (LLMs) for defect detection can lead to a high false-positive rate. To enhance detection performance, NATLM integrates static analysis with LLMs, specifically Gemini Pro 1.5. Initially, NATLM employs static analysis to extract structural, syntactic, and execution flow information from the code, represented through Abstract Syntax Trees (AST) and Control Flow Graphs (CFG). These extracted features are then combined with vectors of known defect examples to create a matrix for input into the knowledge base. Subsequently, the feature vectors and code vectors of the analyzed contract are compared with the contents of the knowledge base. Finally, the LLM performs deep semantic analysis to enhance detection capabilities, providing a more comprehensive and accurate identification of potential security issues. Experimental results indicate that NATLM analyzed 8,672 collected NFT smart contracts, achieving an overall precision of 87.72%, a recall of 89.58%, and an F1 score of 88.94%. The results outperform other baseline experiments, successfully identifying four common types of defects.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt to Pwn: Automated Exploit Generation for Smart Contracts</title>
<link>https://arxiv.org/abs/2508.01371</link>
<guid>https://arxiv.org/abs/2508.01371</guid>
<content:encoded><![CDATA[
arXiv:2508.01371v1 Announce Type: new 
Abstract: We explore the feasibility of using LLMs for Automated Exploit Generation (AEG) against vulnerable smart contracts. We present \textsc{ReX}, a framework integrating LLM-based exploit synthesis with the Foundry testing suite, enabling the automated generation and validation of proof-of-concept (PoC) exploits. We evaluate five state-of-the-art LLMs (GPT-4.1, Gemini 2.5 Pro, Claude Opus 4, DeepSeek, and Qwen3 Plus) on both synthetic benchmarks and real-world smart contracts affected by known high-impact exploits. Our results show that modern LLMs can reliably generate functional PoC exploits for diverse vulnerability types, with success rates reaching up to 92\%. Notably, Gemini 2.5 Pro and GPT-4.1 consistently outperform others in both synthetic and real-world scenarios. We further analyze factors influencing AEG effectiveness, including model capabilities, contract structure, and vulnerability types. We also collect the first curated dataset of real-world PoC exploits to support future research.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nakamoto Consensus from Multiple Resources</title>
<link>https://arxiv.org/abs/2508.01448</link>
<guid>https://arxiv.org/abs/2508.01448</guid>
<content:encoded><![CDATA[
arXiv:2508.01448v1 Announce Type: new 
Abstract: The blocks in the Bitcoin blockchain record the amount of work W that went into creating them through proofs of work. When honest parties control a majority of the work, consensus is achieved by picking the chain with the highest recorded weight. Resources other than work have been considered to secure such longest-chain blockchains. In Chia, blocks record the amount of space S (via a proof of space) and sequential computational steps V (via a VDF).
  In this paper, we ask what weight functions {\Gamma}(S,V,W) (that assign a weight to a block as a function of the recorded space, speed, and work) are secure in the sense that whenever the weight of the resources controlled by honest parties is larger than the weight of adversarial parties, the blockchain is secure against private double-spending attacks.
  We completely classify such functions in an idealized "continuous" model: {\Gamma}(S,V,W) is secure against private double-spending attacks if and only if it is homogeneous of degree one in the timed resources V and W, i.e., {\alpha}{\Gamma}(S,V,W)={\Gamma}(S,{\alpha}V, {\alpha}W). This includes Bitcoin rule {\Gamma}(S,V,W)=W and Chia rule {\Gamma}(S,V,W) = SV. In a more realistic model where blocks are created at discrete time-points, one additionally needs some mild assumptions on the dependency on S (basically, the weight should not grow too much if S is slightly increased, say linear as in Chia).
  Our classification is more general and allows various instantiations of the same resource. It provides a powerful tool for designing new longest-chain blockchains. E.g., consider combining different PoWs to counter centralization, say the Bitcoin PoW W_1 and a memory-hard PoW W_2. Previous work suggested to use W_1+W_2 as weight. Our results show that using {\sqrt}(W_1){\cdot}{\sqrt}(W_2), {\min}{W_1,W_2} are also secure, and we argue that in practice these are much better choices.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Aerial Manipulation of a Cable-Suspended Load using Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.01522</link>
<guid>https://arxiv.org/abs/2508.01522</guid>
<content:encoded><![CDATA[
arXiv:2508.01522v1 Announce Type: new 
Abstract: This paper presents the first decentralized method to enable real-world 6-DoF manipulation of a cable-suspended load using a team of Micro-Aerial Vehicles (MAVs). Our method leverages multi-agent reinforcement learning (MARL) to train an outer-loop control policy for each MAV. Unlike state-of-the-art controllers that utilize a centralized scheme, our policy does not require global states, inter-MAV communications, nor neighboring MAV information. Instead, agents communicate implicitly through load pose observations alone, which enables high scalability and flexibility. It also significantly reduces computing costs during inference time, enabling onboard deployment of the policy. In addition, we introduce a new action space design for the MAVs using linear acceleration and body rates. This choice, combined with a robust low-level controller, enables reliable sim-to-real transfer despite significant uncertainties caused by cable tension during dynamic 3D motion. We validate our method in various real-world experiments, including full-pose control under load model uncertainties, showing setpoint tracking performance comparable to the state-of-the-art centralized method. We also demonstrate cooperation amongst agents with heterogeneous control policies, and robustness to the complete in-flight loss of one MAV. Videos of experiments: https://autonomousrobots.nl/paper_websites/aerial-manipulation-marl
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Gossip Protocols: A Vision for Emergent Coordination in Agentic Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2508.01531</link>
<guid>https://arxiv.org/abs/2508.01531</guid>
<content:encoded><![CDATA[
arXiv:2508.01531v1 Announce Type: new 
Abstract: As agentic platforms scale, agents are evolving beyond static roles and fixed toolchains, creating a growing need for flexible, decentralized coordination. Today's structured communication protocols (e.g., direct agent-to-agent messaging) excel at reliability and task delegation, but they fall short in enabling emergent, swarm-like intelligence, where distributed agents continuously learn, adapt, and communicate to form collective cognition. This paper revisits gossip protocols, long valued in distributed systems for their fault tolerance and decentralization, and argues that they offer a missing layer for context-rich, adaptive communication in agentic AI. Gossip enables scalable, low-overhead dissemination of shared knowledge, but also raises unresolved challenges around semantic filtering, staleness, trustworthiness, and consistency in high-stakes environments. Rather than proposing a new framework, this work charts a research agenda for integrating gossip as a complementary substrate alongside structured protocols. We identify critical gaps in current agent-to-agent architectures, highlight where gossip could reshape assumptions about coordination, and outline open questions around intent propagation, knowledge decay, and peer-to-peer trust. Gossip is not a silver bullet, but overlooking it risks missing a key path toward resilient, reflexive, and self-organizing multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asynchronous Federated Learning with non-convex client objective functions and heterogeneous dataset</title>
<link>https://arxiv.org/abs/2508.01675</link>
<guid>https://arxiv.org/abs/2508.01675</guid>
<content:encoded><![CDATA[
arXiv:2508.01675v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative model training across decentralized devices while preserving data privacy. However, traditional FL suffers from communication overhead, system heterogeneity, and straggler effects. Asynchronous Federated Learning (AFL) addresses these by allowing clients to update independently, improving scalability and reducing synchronization delays. This paper extends AFL to handle non-convex objective functions and heterogeneous datasets, common in modern deep learning. We present a rigorous convergence analysis, deriving bounds on the expected gradient norm and studying the effects of staleness, variance, and heterogeneity. To mitigate stale updates, we introduce a staleness aware aggregation that prioritizes fresher updates and a dynamic learning rate schedule that adapts to client staleness and heterogeneity, improving stability and convergence. Our framework accommodates variations in computational power, data distribution, and communication delays, making it practical for real world applications. We also analyze the impact of client selection strategies-sampling with or without replacement-on variance and convergence. Implemented in PyTorch with Python's asyncio, our approach is validated through experiments demonstrating improved performance and scalability for asynchronous, heterogeneous, and non-convex FL scenarios.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Provably Secure Network Protocol for Private Communication with Analysis and Tracing Resistance</title>
<link>https://arxiv.org/abs/2508.01714</link>
<guid>https://arxiv.org/abs/2508.01714</guid>
<content:encoded><![CDATA[
arXiv:2508.01714v1 Announce Type: new 
Abstract: Anonymous communication networks have emerged as crucial tools for obfuscating communication pathways and concealing user identities. However, their practical deployments face significant challenges, including susceptibility to artificial intelligence (AI)-powered metadata analysis, difficulties in decentralized architectures, and the absence of provable security guarantees. To address these issues, this paper proposes a novel decentralized anonymous routing protocol with resistance to tracing and traffic analysis. The protocol eliminates dependencies on the threshold model and trusted third-party setups, ensuring indistinguishable identity privacy even in highly adversarial environments. Different from traditional empirical security analysis of anonymous networks, this paper rigorously proves indistinguishable identity privacy for users even in extremely adversarial environments. Furthermore, simulations confirm its practical feasibility, demonstrating both security and efficiency. By achieving information sharing with privacy preservation, the proposed protocol offers a provably secure solution for privacy-preserving communication in digital environments.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Byzantine Consensus MechanismBased on Reputation in IoT Blockchain</title>
<link>https://arxiv.org/abs/2508.01856</link>
<guid>https://arxiv.org/abs/2508.01856</guid>
<content:encoded><![CDATA[
arXiv:2508.01856v1 Announce Type: new 
Abstract: Blockchain technology has advanced rapidly in recent years and is now widely used in a variety of fields. Blockchain appears to be one of the best solutions for managing massive heterogeneous devices while achieving advanced data security and data reputation, particularly in the field of large-scale IoT (Internet of Things) networks. Despite the numerous advantages, there are still challenges while deploying IoT applications on blockchain systems due to the limited storage, power, and computing capability of IoT devices, and some of these problems are caused by the consensus algorithm, which plays a significant role in blockchain systems by ensuring overall system reliability and robustness. Nonetheless, most existing consensus algorithms are prone to poor node reliability, low transaction per second (TPS) rates, and scalability issues. Aiming at some critical problems in the existing consensus algorithms, this paper proposes the Efficient Byzantine Reputation-based Consensus (EBRC) mechanism to resolve the issues raised above. In comparison to traditional algorithms, we reinvented ways to evaluate node reliability and robustness and manage active nodes. Our experiments show that the EBRC algorithm has lower consensus delay, higher throughput, improved security, and lower verification costs. It offers new reference ideas for solving the Internet of Things+blockchain+Internet court construction problem.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Day-Ahead Energy Trading with Proximal Policy Optimization and Blockchain</title>
<link>https://arxiv.org/abs/2508.01888</link>
<guid>https://arxiv.org/abs/2508.01888</guid>
<content:encoded><![CDATA[
arXiv:2508.01888v1 Announce Type: new 
Abstract: The increasing penetration of renewable energy sources in day-ahead energy markets introduces challenges in balancing supply and demand, ensuring grid resilience, and maintaining trust in decentralized trading systems. This paper proposes a novel framework that integrates the Proximal Policy Optimization (PPO) algorithm, a state-of-the-art reinforcement learning method, with blockchain technology to optimize automated trading strategies for prosumers in day-ahead energy markets. We introduce a comprehensive framework that employs RL agent for multi-objective energy optimization and blockchain for tamper-proof data and transaction management. Simulations using real-world data from the Electricity Reliability Council of Texas (ERCOT) demonstrate the effectiveness of our approach. The RL agent achieves demand-supply balancing within 2\% and maintains near-optimal supply costs for the majority of the operating hours. Moreover, it generates robust battery storage policies capable of handling variability in solar and wind generation. All decisions are recorded on an Algorand-based blockchain, ensuring transparency, auditability, and security - key enablers for trustworthy multi-agent energy trading. Our contributions include a novel system architecture, curriculum learning for robust agent development, and actionable policy insights for practical deployment.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Decentralized Framework for Ethical Authorship Validation in Academic Publishing: Leveraging Self-Sovereign Identity and Blockchain Technology</title>
<link>https://arxiv.org/abs/2508.01913</link>
<guid>https://arxiv.org/abs/2508.01913</guid>
<content:encoded><![CDATA[
arXiv:2508.01913v1 Announce Type: new 
Abstract: Academic publishing, integral to knowledge dissemination and scientific advancement, increasingly faces threats from unethical practices such as unconsented authorship, gift authorship, author ambiguity, and undisclosed conflicts of interest. While existing infrastructures like ORCID effectively disambiguate researcher identities, they fall short in enforcing explicit authorship consent, accurately verifying contributor roles, and robustly detecting conflicts of interest during peer review. To address these shortcomings, this paper introduces a decentralized framework leveraging Self-Sovereign Identity (SSI) and blockchain technology. The proposed model uses Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs) to securely verify author identities and contributions, reducing ambiguity and ensuring accurate attribution. A blockchain-based trust registry records authorship consent and peer-review activity immutably. Privacy-preserving cryptographic techniques, especially Zero-Knowledge Proofs (ZKPs), support conflict-of-interest detection without revealing sensitive data. Verified authorship metadata and consent records are embedded in publications, increasing transparency. A stakeholder survey of researchers, editors, and reviewers suggests the framework improves ethical compliance and confidence in scholarly communication. This work represents a step toward a more transparent, accountable, and trustworthy academic publishing ecosystem.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIRF: A Framework for Digital Identity Protection and Clone Governance in Agentic AI Systems</title>
<link>https://arxiv.org/abs/2508.01997</link>
<guid>https://arxiv.org/abs/2508.01997</guid>
<content:encoded><![CDATA[
arXiv:2508.01997v1 Announce Type: new 
Abstract: The rapid advancement and widespread adoption of generative artificial intelligence (AI) pose significant threats to the integrity of personal identity, including digital cloning, sophisticated impersonation, and the unauthorized monetization of identity-related data. Mitigating these risks necessitates the development of robust AI-generated content detection systems, enhanced legal frameworks, and ethical guidelines. This paper introduces the Digital Identity Rights Framework (DIRF), a structured security and governance model designed to protect behavioral, biometric, and personality-based digital likeness attributes to address this critical need. Structured across nine domains and 63 controls, DIRF integrates legal, technical, and hybrid enforcement mechanisms to secure digital identity consent, traceability, and monetization. We present the architectural foundations, enforcement strategies, and key use cases supporting the need for a unified framework. This work aims to inform platform builders, legal entities, and regulators about the essential controls needed to enforce identity rights in AI-driven systems.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Actual Usage Of Cryptocurrency By Individuals</title>
<link>https://arxiv.org/abs/2508.02086</link>
<guid>https://arxiv.org/abs/2508.02086</guid>
<content:encoded><![CDATA[
arXiv:2508.02086v1 Announce Type: new 
Abstract: This study investigates how media influence and educational resources shape individual engagement with cryptocurrencies. As digital assets become increasingly mainstream, social media platforms, influencers, and financial analysts play a central role in driving public interest and investment behavior. Quantitative surveys and qualitative interviews reveal that persuasive narratives and success stories on social media often attract new investors, while accessible educational materials empower individuals to make informed decisions in a volatile market. The findings highlight that media exposure can spark initial adoption, but sustained and responsible participation depends on comprehensive education in cryptocurrency fundamentals, trading strategies, and blockchain technology
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoftPUF: a Software-Based Blockchain Framework using PUF and Machine Learning</title>
<link>https://arxiv.org/abs/2508.02438</link>
<guid>https://arxiv.org/abs/2508.02438</guid>
<content:encoded><![CDATA[
arXiv:2508.02438v1 Announce Type: new 
Abstract: Physically Unclonable Function (PUF) offers a secure and lightweight alternative to traditional cryptography for authentication due to their unique device fingerprint. However, their dependence on specialized hardware hinders their adoption in diverse applications. This paper proposes a novel blockchain framework that leverages SoftPUF, a software-based approach mimicking PUF. SoftPUF addresses the hardware limitations of traditional PUF, enabling secure and efficient authentication for a broader range of devices within a blockchain network. The framework utilizes a machine learning model trained on PUF data to generate unique, software-based keys for each device. These keys serve as secure identifiers for authentication on the blockchain, eliminating the need for dedicated hardware. This approach facilitates the integration of legacy devices from various domains, including cloud-based solutions, into the blockchain network. Additionally, the framework incorporates well-established defense mechanisms to ensure robust security against various attacks. This combined approach paves the way for secure and scalable authentication in diverse blockchain-based applications. Additionally, to ensure robust security, the system incorporates well-established defense mechanisms against various attacks, including 51%, phishing, routing, and Sybil attacks, into the blockchain network. This combined approach paves the way for secure and efficient authentication in a wider range of blockchain-based applications.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>