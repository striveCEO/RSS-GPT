<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs</link>

<item>
<title>CAMON: Cooperative Agents for Multi-Object Navigation with LLM-based Conversations</title>
<link>https://arxiv.org/abs/2407.00632</link>
<guid>https://arxiv.org/abs/2407.00632</guid>
<content:encoded><![CDATA[
<div> 关键词：视觉导航、多机器人协作、大型语言模型、通信触发、动态领导组织结构。

总结:<br>本文探讨了在复杂视觉导航任务中，如何利用大型语言模型（LLMs）实现多机器人有效协作。研究提出了一种框架，通过设计通信触发的动态领导组织结构，促进团队成员间的快速共识和减少交流次数，从而提高导航效率和探索效率。这种创新的通信机制使得框架在处理多目标导航时表现出冲突解决能力和对团队规模扩大的稳健性，为家庭服务机器人的协作导航提供了新思路。 <div>
arXiv:2407.00632v1 Announce Type: new 
Abstract: Visual navigation tasks are critical for household service robots. As these tasks become increasingly complex, effective communication and collaboration among multiple robots become imperative to ensure successful completion. In recent years, large language models (LLMs) have exhibited remarkable comprehension and planning abilities in the context of embodied agents. However, their application in household scenarios, specifically in the use of multiple agents collaborating to complete complex navigation tasks through communication, remains unexplored. Therefore, this paper proposes a framework for decentralized multi-agent navigation, leveraging LLM-enabled communication and collaboration. By designing the communication-triggered dynamic leadership organization structure, we achieve faster team consensus with fewer communication instances, leading to better navigation effectiveness and collaborative exploration efficiency. With the proposed novel communication scheme, our framework promises to be conflict-free and robust in multi-object navigation tasks, even when there is a surge in team size.
]]></content:encoded>


</item>
<item>
<title>Opportunities for Shape-based Optimization of Link Traversal Queries</title>
<link>https://arxiv.org/abs/2407.00998</link>
<guid>https://arxiv.org/abs/2407.00998</guid>
<content:encoded><![CDATA[
<div> 关键词：web数据、Link Traversal Query Processing (LTQP)、RDF数据形状、source selection algorithm、decentralized query processing

总结:<br>
这篇文章主要探讨了在Web数据的去中心化环境中，如何利用RDF数据形状优化Link Traversal Query Processing (LTQP)的问题。作者提出了一种基于RDF数据形状映射的源选择算法，旨在解决LTQP中缺乏先验信息和大量HTTP请求的问题。初步实验表明，通过少量维护和服务器工作，该方法能显著减少查询执行时间（高达80%）和链接遍历数量（达97%）。这为非启发式查询规划研究提供了新的方向，展示了RDF数据形状的强大描述能力。未来的研究可能集中在更深入地挖掘这种潜力上。 <div>
arXiv:2407.00998v1 Announce Type: new 
Abstract: Data on the web is naturally unindexed and decentralized. Centralizing web data, especially personal data, raises ethical and legal concerns. Yet, compared to centralized query approaches, decentralization-friendly alternatives such as Link Traversal Query Processing (LTQP) are significantly less performant and understood. The two main difficulties of LTQP are the lack of apriori information about data sources and the high number of HTTP requests. Exploring decentralized-friendly ways to document unindexed networks of data sources could lead to solutions to alleviate those difficulties. RDF data shapes are widely used to validate linked data documents, therefore, it is worthwhile to investigate their potential for LTQP optimization. In our work, we built an early version of a source selection algorithm for LTQP using RDF data shape mappings with linked data documents and measured its performance in a realistic setup. In this article, we present our algorithm and early results, thus, opening opportunities for further research for shape-based optimization of link traversal queries. Our initial experiments show that with little maintenance and work from the server, our method can reduce up to 80% the execution time and 97% the number of links traversed during realistic queries. Given our early results and the descriptive power of RDF data shapes it would be worthwhile to investigate non-heuristic-based query planning using RDF shapes.
]]></content:encoded>


</item>
<item>
<title>Energy-Aware Decentralized Learning with Intermittent Model Training</title>
<link>https://arxiv.org/abs/2407.01283</link>
<guid>https://arxiv.org/abs/2407.01283</guid>
<content:encoded><![CDATA[
<div> 关键词：Decentralized learning, Energy consumption, SkipTrain, Synchronization rounds, Model accuracy.

总结:<br>
本文介绍了一种名为SkipTrain的新型去中心化学习（Decentralized learning）算法，其目标是减少能源消耗并提高模型准确性。SkipTrain通过有策略地跳过一些训练轮次，用同步轮替之，从而在不牺牲模型性能的前提下，节省了大量能量。与常规的D-PSGD算法相比，SkipTrain在256节点实验中显示出高达50%的节能效果和高达12%的模型精度提升。这种方法不仅降低了计算成本，还促进了模型间的更好融合。 <div>
arXiv:2407.01283v1 Announce Type: new 
Abstract: Decentralized learning (DL) offers a powerful framework where nodes collaboratively train models without sharing raw data and without the coordination of a central server. In the iterative rounds of DL, models are trained locally, shared with neighbors in the topology, and aggregated with other models received from neighbors. Sharing and merging models contribute to convergence towards a consensus model that generalizes better across the collective data captured at training time. In addition, the energy consumption while sharing and merging model parameters is negligible compared to the energy spent during the training phase. Leveraging this fact, we present SkipTrain, a novel DL algorithm, which minimizes energy consumption in decentralized learning by strategically skipping some training rounds and substituting them with synchronization rounds. These training-silent periods, besides saving energy, also allow models to better mix and finally produce models with superior accuracy than typical DL algorithms that train at every round. Our empirical evaluations with 256 nodes demonstrate that SkipTrain reduces energy consumption by 50% and increases model accuracy by up to 12% compared to D-PSGD, the conventional DL algorithm.
]]></content:encoded>


</item>
<item>
<title>C-MP: A decentralized adaptive-coordinated traffic signal control using the Max Pressure framework</title>
<link>https://arxiv.org/abs/2407.01421</link>
<guid>https://arxiv.org/abs/2407.01421</guid>
<content:encoded><![CDATA[
<div> 关键词：Coordinated Max Pressure (C-MP), adaptive traffic signals, decentralized decision-making, arterial network, stable region.

总结:<br>
本文提出了一种新颖的协调最大压力(Coordinated Max Pressure, C-MP)交通信号框架，旨在解决分布式决策下协调交通流的问题。C-MP利用车辆空间平均速度检测并优先处理沿走廊的自由行驶车队，通过调整上游车队的权重和下游车队的权重，确保交通流畅。文章证明了C-MP保持最大稳定性，并通过模拟分析展示了其在动脉网络中相较于基准MP控制策略具有更大的稳定区域。C-MP在无需预设交错时间和约束的情况下有效协调双向动脉交通，减少旅行时间和燃料消耗，实现交通流量的均衡。 <div>
arXiv:2407.01421v1 Announce Type: new 
Abstract: Coordinated traffic signals seek to provide uninterrupted flow through a series of closely spaced intersections, typically using pre-defined fixed signal timings and offsets. Adaptive traffic signals dynamically change signal timings based on observed traffic conditions in a way that might disrupt coordinated movements, particularly when these decisions are made independently at each intersection. To alleviate this issue, this paper introduces a novel Max Pressure-based traffic signal framework that can provide coordination even under decentralized decision-making. The proposed Coordinated Max Pressure (C-MP) algorithm uses the space mean speeds of vehicles to explicitly detect freely flowing platoons of vehicles and prioritizes their movement along a corridor. Specifically, upstream platoons are detected and their weight in the MP framework increased to provide priority, while downstream platoons are detected and their weight reduced to ensure smooth traffic flow across corridors. The study analytically proves that C-MP maintains the desirable maximum stability property, while micro-simulation analyses conducted on an arterial network demonstrate its ability to achieve a larger stable region compared to benchmark MP control policies. Simulation results also reveal that the proposed control algorithm can effectively coordinate traffic signals in both directions along an arterial without explicitly assigned offsets or constraints. The results also reveal C-MP's superiority to benchmark coordination strategies in reducing travel time, and fuel consumption both at the corridor level and the network level by balancing the negative impact imparted to vehicles in the minor direction.
]]></content:encoded>


</item>

<item>
<title>Graph Neural Networks and Reinforcement Learning for Proactive Application Image Placement</title>
<link>https://arxiv.org/abs/2407.00007</link>
<guid>https://arxiv.org/abs/2407.00007</guid>
<content:encoded><![CDATA[
<div> 关键词：Edge computing, Cloud-Edge continuum, Service placement, Application placement, Reinforcement Learning.

总结:<br />
云计算向云边计算的转变为数据密集型和交互式应用带来了机遇与挑战。边缘计算作为下一代应用严格需求的关键支持，通过将计算任务移至用户附近，实现了低延迟和高带宽。然而，边缘计算的分布式、动态和异构特性使得服务部署成为一个难题。本文提出了一种结合图神经网络和强化学习（Actor-Critic）的前瞻性图像部署方法，旨在减少图像传输时间并优化应用部署。尽管在某些情况下可能导致执行时间稍长，但实验结果显示，该方法在整体应用部署上表现更优。 <div>
arXiv:2407.00007v1 Announce Type: new 
Abstract: The shift from Cloud Computing to a Cloud-Edge continuum presents new opportunities and challenges for data-intensive and interactive applications. Edge computing has garnered a lot of attention from both industry and academia in recent years, emerging as a key enabler for meeting the increasingly strict demands of Next Generation applications. In Edge computing the computations are placed closer to the end-users, to facilitate low-latency and high-bandwidth applications and services. However, the distributed, dynamic, and heterogeneous nature of Edge computing, presents a significant challenge for service placement. A critical aspect of Edge computing involves managing the placement of applications within the network system to minimize each application's runtime, considering the resources available on system devices and the capabilities of the system's network. The placement of application images must be proactively planned to minimize image tranfer time, and meet the strict demands of the applications. In this regard, this paper proposes an approach for proactive image placement that combines Graph Neural Networks and actor-critic Reinforcement Learning, which is evaluated empirically and compared against various solutions. The findings indicate that although the proposed approach may result in longer execution times in certain scenarios, it consistently achieves superior outcomes in terms of application placement.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>A New Approach for Evaluating the Performance of Distributed Latency-Sensitive Services</title>
<link>https://arxiv.org/abs/2407.00015</link>
<guid>https://arxiv.org/abs/2407.00015</guid>
<content:encoded><![CDATA[
<div> 关键词：latency metrics, Service Level Agreement (SLA), distributed computing, immersive services, latency performance.

总结:<br />本文提出了一篇关于新型延迟度量标准（latency metrics）的研究论文，针对传统指标在评估现代服务和分布式计算环境中性能的局限性。研究者强调了现有指标无法充分捕捉两个关键性能方面：超过服务级别协议（SLA）阈值的频率以及恢复到可接受水平的时间。为解决这一问题，作者开发了五个创新的延迟度量，它们能够提供更深入的服务性能洞察。这些新指标尤其适用于对低延迟有严格要求的沉浸式服务。论文通过大规模实验验证了新指标的有效性和实用性，以促进服务性能优化。 <div>
arXiv:2407.00015v1 Announce Type: new 
Abstract: Conventional latency metrics are formulated based on a broad definition of traditional monolithic services, and hence lack the capacity to address the complexities inherent in modern services and distributed computing paradigms. Consequently, their effectiveness in identifying areas for improvement is restricted, falling short of providing a comprehensive evaluation of service performance within the context of contemporary services and computing paradigms. More specifically, these metrics do not offer insights into two critical aspects of service performance: the frequency of latency surpassing specified Service Level Agreement (SLA) thresholds and the time required for latency to return to an acceptable level once the threshold is exceeded. This limitation is quite significant in the frame of contemporary latency-sensitive services, and especially immersive services that require deterministic low latency that behaves in a consistent manner. Towards addressing this limitation, the authors of this work propose 5 novel latency metrics that when leveraged alongside the conventional latency metrics manage to provide advanced insights that can be potentially used to improve service performance. The validity and usefulness of the proposed metrics in the frame of providing advanced insights into service performance is evaluated using a large-scale experiment.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Preble: Efficient Distributed Prompt Scheduling for LLM Serving</title>
<link>https://arxiv.org/abs/2407.00023</link>
<guid>https://arxiv.org/abs/2407.00023</guid>
<content:encoded><![CDATA[
<div> 关键词：Preble、大型语言模型（LLMs）、分布式 serving 平台、prompt 分享、计算重用。

总结:
Preble 是一项创新，它提出了一种针对大型语言模型（LLMs）服务的分布式平台，专注于优化提示共享。文章研究了五个流行的工作负载，发现当前系统忽视了重复提示的注意力计算可以复用的潜力。为此，Preble 设计了一个分布式调度系统，兼顾计算重用和负载均衡。实验证明，与现有技术相比，Preble 在两到八GPU上分别提高了平均延迟1.5倍至14.5倍和p99值的2倍至10倍，显著提升了性能。 <div>
arXiv:2407.00023v1 Announce Type: new 
Abstract: Prompts to large language models (LLMs) have evolved beyond simple user questions. For LLMs to solve complex problems, today's practices include domain-specific instructions, illustration of tool usages, and long context, such as textbook chapters in prompts. As such, many parts of prompts are repetitive across requests, and their attention computation results can be reused. However, today's LLM serving systems treat every request in isolation, missing the opportunity of computation reuse.
  This paper proposes Preble, the first distributed LLM serving platform that targets and optimizes for prompt sharing. We perform a study on five popular LLM workloads. Based on our study results, we designed a distributed scheduling system that co-optimizes computation reuse and load balancing. Our evaluation of Preble on two to 8 GPUs with real workloads and request arrival patterns on two open-source LLM models shows that Preble outperforms the state-of-the-art average latency by 1.5X to 14.5X and p99 by 2X to 10X.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Distributed Inference Performance Optimization for LLMs on CPUs</title>
<link>https://arxiv.org/abs/2407.00029</link>
<guid>https://arxiv.org/abs/2407.00029</guid>
<content:encoded><![CDATA[
<div> 关键词：large language models (LLMs), distributed computing, resource-limited hardware, memory capacity, inference optimization.

总结:
本文介绍了一种针对大型语言模型（LLMs）的分布式推理优化解决方案，旨在缓解资源受限硬件设备上的内存限制和提高计算性能。该方法特别适用于部署在5代Intel Xeon Scalable处理器上。实验结果显示，使用72亿参数的LLM，通过优化后的方案，每输出一个令牌的时间降低到140毫秒，远低于人类阅读的平均速度（约200毫秒/令牌），显著提高了效率。这一成果表明分布式计算在有效利用CPU资源、扩展LLM应用中具有重要价值。 <div>
arXiv:2407.00029v1 Announce Type: new 
Abstract: Large language models (LLMs) hold tremendous potential for addressing numerous real-world challenges, yet they typically demand significant computational resources and memory. Deploying LLMs onto a resource-limited hardware device with restricted memory capacity presents considerable challenges. Distributed computing emerges as a prevalent strategy to mitigate single-node memory constraints and expedite LLM inference performance. To reduce the hardware limitation burden, we proposed an efficient distributed inference optimization solution for LLMs on CPUs. We conduct experiments with the proposed solution on 5th Gen Intel Xeon Scalable Processors, and the result shows the time per output token for the LLM with 72B parameter is 140 ms/token, much faster than the average human reading speed about 200ms per token.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>On Orchestrating Parallel Broadcasts for Distributed Ledgers</title>
<link>https://arxiv.org/abs/2407.00030</link>
<guid>https://arxiv.org/abs/2407.00030</guid>
<content:encoded><![CDATA[
<div> 关键词：ticketing, atomic broadcasts, distributed system, managed ticketing, unmanaged ticketing

总结:<br />本文主要探讨了“票证”(ticketing)的概念在分布式系统中对原子广播的组织。文章研究了不同类型的票证制度，允许并行执行但防止慢节点影响整体进度。一种混合方案被提出，结合了管理和未管理的票证模式，旨在平衡适应性和鲁棒性。性能评估显示，无论是静态还是动态场景，资源异构的系统中，管理票证制度对吞吐量有优势，因为它能更好地适应。最后，实验表明，使用混合票证制度可以同时享受管理票证的适应性与未管理票证的活度保证。 <div>
arXiv:2407.00030v1 Announce Type: new 
Abstract: This paper introduces and develops the concept of ``ticketing'', through which atomic broadcasts are orchestrated by nodes in a distributed system. The paper studies different ticketing regimes that allow parallelism, yet prevent slow nodes from hampering overall progress. It introduces a hybrid scheme which combines managed and unmanaged ticketing regimes, striking a balance between adaptivity and resilience. The performance evaluation demonstrates how managed and unmanaged ticketing regimes benefit throughput in systems with heterogeneous resources both in static and dynamic scenarios, with the managed ticketing regime performing better among the two as it adapts better. Finally, it demonstrates how using the hybrid ticketing regime performance can enjoy both the adaptivity of the managed regime and the liveness guarantees of the unmanaged regime.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Distributed Systems in Fintech</title>
<link>https://arxiv.org/abs/2407.00034</link>
<guid>https://arxiv.org/abs/2407.00034</guid>
<content:encoded><![CDATA[
<div> 关键词：分布式系统、金融技术（Fintech）、区块链、去中心化金融（DeFi）、分布式 ledger technology (DLT)

总结:<br />
分布式系统在Fintech中的作用日益凸显，推动了行业革新。本文分析了分布式系统的架构，如区块链、DeFi和DLT，它们在提升金融操作的安全性、可扩展性和效率方面展现出巨大潜力。文章深入探讨了这些技术如何影响金融服务、支付、资产管理等，并展望了未来分布式系统在Fintech领域的广阔前景。随着技术不断发展，分布式系统有望重塑金融行业的版图。 <div>
arXiv:2407.00034v1 Announce Type: new 
Abstract: The emergence of distributed systems has revolutionized the financial technology (Fintech) landscape, offering unprecedented opportunities for enhancing security, scalability, and efficiency in financial operations. This paper explores the role of distributed systems in Fintech, analyzing their architecture, benefits, challenges, and applications. It examines key distributed technologies such as blockchain, decentralized finance (DeFi), and distributed ledger technology (DLT), and their impact on various aspects of the financial industry, and future directions for distributed systems in Fintech.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Streamline Intelligent Crowd Monitoring with IoT Cloud Computing Middleware</title>
<link>https://arxiv.org/abs/2407.00045</link>
<guid>https://arxiv.org/abs/2407.00045</guid>
<content:encoded><![CDATA[
<div> 关键词：middleware, Raspberry Pi, wireless sensor networks (WSNs), MapReduce, fault tolerance

总结:<br />
本文介绍了一种新颖的中间件系统，它利用低成本、低功耗的设备如Raspberry Pi分析无线传感器网络（WSN）的数据。该系统针对室内环境，如历史建筑和博物馆，用于监控访客、识别兴趣点并作为疏散辅助工具。通过MapReduce算法收集和分布数据，结合故障容忍的领导选举算法，其性能与资源密集型方法相当，但使用简单硬件。在希腊一处历史建筑（哈茨迪亚基斯故居）进行了成功测试。与现有实现相比，这种设计的优势在于其经济、分布式且具有故障恢复能力。尤其在COVID-19大流行期间，这种中间件对于室内位置的监控具有重要意义，能有效追踪访客数量和整体建筑占用率。 <div>
arXiv:2407.00045v1 Announce Type: new 
Abstract: This article introduces a novel middleware that utilizes cost-effective, low-power computing devices like Raspberry Pi to analyze data from wireless sensor networks (WSNs). It is designed for indoor settings like historical buildings and museums, tracking visitors and identifying points of interest. It serves as an evacuation aid by monitoring occupancy and gauging the popularity of specific areas, subjects, or art exhibitions. The middleware employs a basic form of the MapReduce algorithm to gather WSN data and distribute it across available computer nodes. Data collected by RFID sensors on visitor badges is stored on mini-computers placed in exhibition rooms and then transmitted to a remote database after a preset time frame. Utilizing MapReduce for data analysis and a leader election algorithm for fault tolerance, this middleware showcases its viability through metrics, demonstrating applications like swift prototyping and accurate validation of findings. Despite using simpler hardware, its performance matches resource-intensive methods involving audiovisual and AI techniques. This design's innovation lies in its fault-tolerant, distributed setup using budget-friendly, low-power devices rather than resource-heavy hardware or methods. Successfully tested at a historical building in Greece (M. Hatzidakis' residence), it is tailored for indoor spaces. This paper compares its algorithmic application layer with other implementations, highlighting its technical strengths and advantages. Particularly relevant in the wake of the COVID-19 pandemic and general monitoring middleware for indoor locations, this middleware holds promise in tracking visitor counts and overall building occupancy.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Tracing Distributed Algorithms Using Replay Clocks</title>
<link>https://arxiv.org/abs/2407.00069</link>
<guid>https://arxiv.org/abs/2407.00069</guid>
<content:encoded><![CDATA[
<div> 关键词：replay clocks (RepCl), distributed computations, concurrent events, constraint violations, visualization.

总结:<br />
本文介绍了一种新的时钟基础设施——replay clocks (RepCl)，它旨在支持离线分析分布式计算。RepCl结合了向量时钟(VC)和混合逻辑时钟(HLC)的优势，提供高效重放功能，允许用户检查并发事件下的约束条件和潜在执行路径。文章强调了RepCl的低开销实现（最多4个整数表示64个进程）以及与同步时间的关系。通过模拟和NS-3网络模拟器，作者评估了RepCl的预期开销，并确定了其可行性的区域。此外，文中提出了一种基于RepCl的分布式计算追踪器，可实时分析系统的特性，同时考虑并发路径。这个可视化工具提供了逐进程和全局视图，便于深入理解计算过程。 <div>
arXiv:2407.00069v1 Announce Type: new 
Abstract: In this thesis, we introduce replay clocks (RepCl), a novel clock infrastructure that allows us to do offline analyses of distributed computations. The replay clock structure provides a methodology to replay a computation as it happened, with the ability to represent concurrent events effectively. It builds on the structures introduced by vector clocks (VC) and the Hybrid Logical Clock (HLC), combining their infrastructures to provide efficient replay. With such a clock, a user can replay a computation whilst considering multiple paths of executions, and check for constraint violations and properties that potential pathways could take in the presence of concurrent events. Specifically, if event e must occur before f then the replay clock must ensure that e is replayed before f. On the other hand, if e and f could occur in any order, replay should not force an order between them. We demonstrate that RepCl can be implemented with less than four integers for 64 processes for various system parameters if clocks are synchronized within 1ms. Furthermore, the overhead of RepCl (for computing timestamps and message size) is proportional to the size of the clock. Using simulations in a custom distributed system and NS-3, a state-of-the-art network simulator, we identify the expected overhead of RepCl. We also identify how a user can then identify feasibility region for RepCl, where unabridged replay is possible. Using the RepCl, we provide a tracer for distributed computations, that allows any computation using the RepCl to be replayed efficiently. The visualization allows users to analyze specific properties and constraints in an online fashion, with the ability to consider concurrent paths independently. The visualization provides per-process views and an overarching view of the whole computation based on the time recorded by the RepCl for each event.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Decentralized Task Offloading and Load-Balancing for Mobile Edge Computing in Dense Networks</title>
<link>https://arxiv.org/abs/2407.00080</link>
<guid>https://arxiv.org/abs/2407.00080</guid>
<content:encoded><![CDATA[
<div> 关键词：decentralized task offloading, load-balancing, dense network, edge servers, mean field multi-agent MAB game.

总结:
该研究关注于密集网络中众多设备和边缘服务器之间的任务卸载与负载均衡问题。由于网络信息未知和任务大小随机，优化这一问题颇具挑战。论文提出了一种结合了多智能体多臂赌博（MAB）游戏的平均场方法，通过调整服务器奖励实现目标用户分布，即使在分布式决策制定下也能达到平衡。数值结果证明了这种方法的有效性，并展示了其能导向目标负载分布。 <div>
arXiv:2407.00080v1 Announce Type: new 
Abstract: We study the problem of decentralized task offloading and load-balancing in a dense network with numerous devices and a set of edge servers. Solving this problem optimally is complicated due to the unknown network information and random task sizes. The shared network resources also influence the users' decisions and resource distribution. Our solution combines the mean field multi-agent multi-armed bandit (MAB) game with a load-balancing technique that adjusts the servers' rewards to achieve a target population profile despite the distributed user decision-making. Numerical results demonstrate the efficacy of our approach and the convergence to the target load distribution.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Predicting Elevated Risk of Hospitalization Following Emergency Department Discharges</title>
<link>https://arxiv.org/abs/2407.00147</link>
<guid>https://arxiv.org/abs/2407.00147</guid>
<content:encoded><![CDATA[
<div> 关键词：数据挖掘、医院化、急诊部门、预测准确性、诊断错误。

总结: 这篇文章探讨了如何利用数据挖掘技术分析大型医院化数据，以预测患者在急诊部门出院后可能的早期住院。通过组合运用逻辑回归、朴素贝叶斯和关联规则分类器，研究者实现了对3天、7天和14天内住院的高精度预测。这种方法不仅准确，而且生成的可解释模型便于医生理解，规则可以直接转化为实践中的决策工具，帮助他们在患者出院前识别潜在的早期住院风险，从而改善诊断质量和患者安全。 <div>
arXiv:2407.00147v1 Announce Type: new 
Abstract: Hospitalizations that follow closely on the heels of one or more emergency department visits are often symptoms of missed opportunities to form a proper diagnosis. These diagnostic errors imply a failure to recognize the need for hospitalization and deliver appropriate care, and thus also bear important connotations for patient safety. In this paper, we show how data mining techniques can be applied to a large existing hospitalization data set to learn useful models that predict these upcoming hospitalizations with high accuracy. Specifically, we use an ensemble of logistics regression, na\"ive Bayes and association rule classifiers to successfully predict hospitalization within 3, 7 and 14 days of an emergency department discharge. Aside from high accuracy, one of the advantages of the techniques proposed here is that the resulting classifier is easily inspected and interpreted by humans so that the learned rules can be readily operationalized. These rules can then be easily distributed and applied directly by physicians in emergency department settings to predict the risk of early admission prior to discharging their emergency department patients.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Dual-view Aware Smart Contract Vulnerability Detection for Ethereum</title>
<link>https://arxiv.org/abs/2407.00336</link>
<guid>https://arxiv.org/abs/2407.00336</guid>
<content:encoded><![CDATA[
<div> 关键词：Ethereum、smart contracts、vulnerability detection、Dual-view Aware、DVDet。

总结:<br />
本文介绍了一种名为DVDet的双视图智能合约漏洞检测框架。该框架针对以太坊智能合约的源代码和字节码，通过转化为加权图和控制流序列，从两个视角捕捉潜在风险特征并整合分析，以提高合同漏洞检测的效率和准确性。实验结果显示，DVDet在检测智能合约漏洞方面表现出色，超越了其他方法，为区块链技术的安全性提供了有力支持。 <div>
arXiv:2407.00336v1 Announce Type: new 
Abstract: The wide application of Ethereum technology has brought technological innovation to traditional industries. As one of Ethereum's core applications, smart contracts utilize diverse contract codes to meet various functional needs and have gained widespread use. However, the non-tamperability of smart contracts, coupled with vulnerabilities caused by natural flaws or human errors, has brought unprecedented challenges to blockchain security. Therefore, in order to ensure the healthy development of blockchain technology and the stability of the blockchain community, it is particularly important to study the vulnerability detection techniques for smart contracts. In this paper, we propose a Dual-view Aware Smart Contract Vulnerability Detection Framework named DVDet. The framework initially converts the source code and bytecode of smart contracts into weighted graphs and control flow sequences, capturing potential risk features from these two perspectives and integrating them for analysis, ultimately achieving effective contract vulnerability detection. Comprehensive experiments on the Ethereum dataset show that our method outperforms others in detecting vulnerabilities.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>C-MASS: Combinatorial Mobility-Aware Sensor Scheduling for Collaborative Perception with Second-Order Topology Approximation</title>
<link>https://arxiv.org/abs/2407.00412</link>
<guid>https://arxiv.org/abs/2407.00412</guid>
<content:encoded><![CDATA[
<div> 关键词：Collaborative Perception, Sensor Scheduling, Combinatorial Mobility-Aware, Budgeted Maximum Coverage, Wireless Bandwidth.

总结:<br />该论文关注于协作感知(Collaborative Perception, CP)中如何有效利用有限的无线带宽。传统的传感器调度面临车辆移动带来的感知拓扑不确定性挑战。为此，作者提出了一种名为C-MASS的框架，通过组合单辆车和车对的数据来近似完整感知拓扑，减少通信开销。C-MASS采用一种混合贪婪算法解决带预算的最大覆盖问题，兼顾探索与利用，以应对移动性问题。实验结果表明，C-MASS在边缘辅助和分布式配置下接近最优，相较于基于距离和区域的贪心策略，性能提升明显。 <div>
arXiv:2407.00412v1 Announce Type: new 
Abstract: Collaborative Perception (CP) has been a promising solution to address occlusions in the traffic environment by sharing sensor data among collaborative vehicles (CoV) via vehicle-to-everything (V2X) network. With limited wireless bandwidth, CP necessitates task-oriented and receiver-aware sensor scheduling to prioritize important and complementary sensor data. However, due to vehicular mobility, it is challenging and costly to obtain the up-to-date perception topology, i.e., whether a combination of CoVs can jointly detect an object. In this paper, we propose a combinatorial mobility-aware sensor scheduling (C-MASS) framework for CP with minimal communication overhead. Specifically, detections are replayed with sensor data from individual CoVs and pairs of CoVs to maintain an empirical perception topology up to the second order, which approximately represents the complete perception topology. A hybrid greedy algorithm is then proposed to solve a variant of the budgeted maximum coverage problem with a worst-case performance guarantee. The C-MASS scheduling algorithm adapts the greedy algorithm by incorporating the topological uncertainty and the unexplored time of CoVs to balance exploration and exploitation, addressing the mobility challenge. Extensive numerical experiments demonstrate the near-optimality of the proposed C-MASS framework in both edge-assisted and distributed CP configurations. The weighted recall improvements over object-level CP are 5.8% and 4.2%, respectively. Compared to distance-based and area-based greedy heuristics, the gaps to the offline optimal solutions are reduced by up to 75% and 71%, respectively.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Data-Driven Control of Linear Parabolic Systems using Koopman Eigenstructure Assignment</title>
<link>https://arxiv.org/abs/2407.00432</link>
<guid>https://arxiv.org/abs/2407.00432</guid>
<content:encoded><![CDATA[
<div> 关键词：Koopman operator, data-driven, stabilization, parabolic PDEs, eigenstructure assignment.

总结:<br />
本文探讨了如何利用Koopman算子实现线性边界控制下抛物型PDE系统的数据驱动稳定化。研究的核心内容是解决Koopman特征向量赋值问题，即设计一个反馈控制器，使其能设定期望的有限组闭合环Koopman特征值和特征函数。这个控制器依赖于扩展的Krylov-DMD方法来处理抛物型系统，仅需有限数量的采样输出和输入数据。文章证明了在小的Krylov-DMD误差下，闭环系统是指数稳定的。最后，通过一个不稳定的扩散-反应系统的例子，验证了这种适用于分布式参数系统的新型数据驱动控制器设计技术。 <div>
arXiv:2407.00432v1 Announce Type: new 
Abstract: This paper considers the data-driven stabilization of linear boundary controlled parabolic PDEs by making use of the Koopman operator. For this, a Koopman eigenstructure assignment problem is solved, which amounts to determine a feedback of the Koopman open-loop eigenfunctionals assigning a desired finite set of closed-loop Koopman eigenvalues and eigenfunctionals to the closed-loop system. It is shown that the designed controller only needs a finite number of open-loop Koopman eigenvalues and modes of the state. They are determined by extending the classical Krylov-DMD to parabolic systems. For this, only a finite number of pointlike outputs and their temporal samples as well as temporal samples of the inputs are required resulting in a data-driven solution of the eigenstructure assignment problem. Exponential stability of the closed-loop system in the presence of small Krylov-DMD errors is verified. An unstable diffusion-reaction system demonstrates the new data-driven controller design technique for distributed-parameter systems.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Graph Neural Networks Gone Hogwild</title>
<link>https://arxiv.org/abs/2407.00494</link>
<guid>https://arxiv.org/abs/2407.00494</guid>
<content:encoded><![CDATA[
<div> 关键词：message passing GNNs, asynchronous inference, implicitly-defined GNNs, energy GNN, multi-agent systems.

总结:<br />
本文探讨了图神经网络（GNN）在异步推理时的性能问题，发现普通GNN架构在节点更新不同时会导致错误预测。为解决这一问题，作者提出了"隐式定义"的GNN类别，其中一种名为能量GNN的新架构特别受关注。这种架构基于异步优化的理论，如Bertsekas（1982）和Niu等人（2011）的工作，具有对部分异步" hogwild"推理的稳健性。实验表明，能量GNN在多智能体系统相关的合成任务上优于同类GNN，并在现实世界数据集上表现出竞争力。 <div>
arXiv:2407.00494v1 Announce Type: new 
Abstract: Message passing graph neural networks (GNNs) would appear to be powerful tools to learn distributed algorithms via gradient descent, but generate catastrophically incorrect predictions when nodes update asynchronously during inference. This failure under asynchrony effectively excludes these architectures from many potential applications, such as learning local communication policies between resource-constrained agents in, e.g., robotic swarms or sensor networks. In this work we explore why this failure occurs in common GNN architectures, and identify "implicitly-defined" GNNs as a class of architectures which is provably robust to partially asynchronous "hogwild" inference, adapting convergence guarantees from work in asynchronous and distributed optimization, e.g., Bertsekas (1982); Niu et al. (2011). We then propose a novel implicitly-defined GNN architecture, which we call an energy GNN. We show that this architecture outperforms other GNNs from this class on a variety of synthetic tasks inspired by multi-agent systems, and achieves competitive performance on real-world datasets.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Blockchain based Decentralized Petition System</title>
<link>https://arxiv.org/abs/2407.00534</link>
<guid>https://arxiv.org/abs/2407.00534</guid>
<content:encoded><![CDATA[
<div> 关键词：decentralized online petition system, blockchain technology, transparency, voting platform, integrity

总结:
本文探讨了一种基于区块链技术的去中心化在线请愿系统，旨在克服现有系统在透明度、安全性和信任度上的不足。该系统通过区块链记录每个签名和行动，确保过程不可篡改，支持民主决策的民主化。文章还提出了一个基于区块链的去中心化投票应用，设计考虑了系统架构、透明计票机制以及消除中心权威的需求。研究着重于技术实现的细节，如算法和协议，目标是提升民主流程的公正性、安全性和防篡改能力。未来的研究将深入技术层面，以进一步优化这一创新平台。 <div>
arXiv:2407.00534v1 Announce Type: new 
Abstract: A decentralized online petition system enables individuals or groups to create, sign, and share petitions without a central authority. Using blockchain technology, these systems ensure the integrity and transparency of the petition process by recording every signature or action on the blockchain, making alterations or deletions impossible. This provides a permanent, tamper-proof record of the petition's progress. Such systems allow users to bypass traditional intermediaries like government or social media platforms, fostering more democratic and transparent decision-making.
  This paper reviews research on petition systems, highlighting the shortcomings of existing systems such as lack of accountability, vulnerability to hacking, and security issues. The proposed blockchain-based implementation aims to overcome these challenges. Decentralized voting systems have garnered interest recently due to their potential to provide secure and transparent voting platforms without intermediaries, addressing issues like voter fraud, manipulation, and trust in the electoral process.
  We propose a decentralized voting system web application using blockchain technology to ensure the integrity and security of the voting process. This system aims to provide a transparent, decentralized decision-making process that counts every vote while eliminating the need for centralized authorities. The paper presents an overview of the system architecture, design considerations, and implementation details, along with the potential benefits and limitations.
  Finally, we discuss future research directions, examining the technical aspects of the application, including underlying algorithms and protocols. Our research aims to enhance the integrity and accessibility of democratic processes, improve security, and ensure fairness, transparency, and tamper-proofness.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Challenging the Need for Packet Spraying in Large-Scale Distributed Training</title>
<link>https://arxiv.org/abs/2407.00550</link>
<guid>https://arxiv.org/abs/2407.00550</guid>
<content:encoded><![CDATA[
<div> 关键词：large-scale distributed training, network communication, packet spraying, singlepath transport, multipath transport.

总结:<br />该论文质疑了业界普遍接受的观点，即在大规模分布式训练中，多路径传输和包喷射对于提升性能至关重要。研究者发现，单路径传输（从网卡角度看）实际上可以接近理想的多路径传输效果。他们通过分析集体通信模式驱动的工作负载的四个关键特性得出这一结论：同时启动的流量、近似的等量流量、集体完成时间更重要以及流量可以在到达时分割。作者证明，应用层少量的流量分割使得单路径传输在最大拥塞方面与理想多路径传输和包喷射相当。初步评估支持这些发现，提出研发针对大规模分布式训练的下一代传输协议的新方向。 <div>
arXiv:2407.00550v1 Announce Type: new 
Abstract: Large-scale distributed training in production datacenters constitutes a challenging workload bottlenecked by network communication. In response, both major industry players (e.g., Ultra Ethernet Consortium) and parts of academia have surprisingly, and almost unanimously, agreed that packet spraying is necessary to improve the performance of large-scale distributed training workloads.
  In this paper, we challenge this prevailing belief and pose the question: How close can a singlepath transport approach an optimal multipath transport? We demonstrate that singlepath transport (from a NIC's perspective) is sufficient and can perform nearly as well as an ideal multipath transport with packet spraying, particularly in the context of distributed training in leaf-spine topologies. Our assertion is based on four key observations about workloads driven by collective communication patterns: (i) flows within a collective start almost simultaneously, (ii) flow sizes are nearly equal, (iii) the completion time of a collective is more crucial than individual flow completion times, and (iv) flows can be split upon arrival. We analytically prove that singlepath transport, using minimal flow splitting (at the application layer), is equivalent to an ideal multipath transport with packet spraying in terms of maximum congestion. Our preliminary evaluations support our claims. This paper suggests an alternative agenda for developing next-generation transport protocols tailored for large-scale distributed training.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Joint Task Allocation and Scheduling for Multi-Hop Distributed Computing</title>
<link>https://arxiv.org/abs/2407.00565</link>
<guid>https://arxiv.org/abs/2407.00565</guid>
<content:encoded><![CDATA[
<div> 关键词：Internet of Things, Edge Computing, Distributed Computing, Master-Worker Paradigm, Multi-hop Routing

总结:<br />该论文探讨了物联网和边缘计算时代分布式计算的新框架，突破传统的一跳范围限制。研究提出将网络图转换为沉降树结构，通过联合优化任务分配和调度，实现跨多层节点的资源共享。论文提供了两种精确算法和三种启发式策略来解决这一问题。实验结果显示，新方法在有限资源、动态连通性和延迟敏感应用中表现出色，优于传统策略。总的来说，这项工作扩展了分布式计算的适用性，提升了计算效率。 <div>
arXiv:2407.00565v1 Announce Type: new 
Abstract: The rise of the Internet of Things and edge computing has shifted computing resources closer to end-users, benefiting numerous delay-sensitive, computation-intensive applications. To speed up computation, distributed computing is a promising technique that allows parallel execution of tasks across multiple compute nodes. However, current research predominantly revolves around the master-worker paradigm, limiting resource sharing within one-hop neighborhoods. This limitation can render distributed computing ineffective in scenarios with limited nearby resources or constrained/dynamic connectivity. In this paper, we address this limitation by introducing a new distributed computing framework that extends resource sharing beyond one-hop neighborhoods through exploring layered network structures and multi-hop routing. Our framework involves transforming the network graph into a sink tree and formulating a joint optimization problem based on the layered tree structure for task allocation and scheduling. To solve this problem, we propose two exact methods that find optimal solutions and three heuristic strategies to improve efficiency and scalability. The performances of these methods are analyzed and evaluated through theoretical analyses and comprehensive simulation studies. The results demonstrate their promising performances over the traditional distributed computing and computation offloading strategies.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>DDRM: Distributed Drone Reputation Management for Trust and Reliability in Crowdsourced Drone Services</title>
<link>https://arxiv.org/abs/2407.00591</link>
<guid>https://arxiv.org/abs/2407.00591</guid>
<content:encoded><![CDATA[
<div> 关键词：Distributed Drone Reputation Management (DDRM), Internet of Drone Things (IoDT), Ethereum blockchain, Service Review Authorization Token (SRAT), Drone Reputation Enhancement Token (DRET)

总结:<br />
分布式无人机信誉管理系统（DDRM）是一项针对互联网无人机事物（IoDT）生态的创新框架。它利用以太坊区块链构建一个可验证的透明评论机制，旨在提升无人机服务的信任度。DDRM采用双币系统，包括服务审查授权令牌（SRAT）用于权限管理，以及无人机声誉增强令牌（DRET）以奖励表现可靠的无人机。研究证明，DDRM能够抵御欺诈行为，有效提高无人机服务的效率和可靠性，为无人机服务民主化提供保障。 <div>
arXiv:2407.00591v1 Announce Type: new 
Abstract: This study introduces the Distributed Drone Reputation Management (DDRM) framework, designed to fortify trust and authenticity within the Internet of Drone Things (IoDT) ecosystem. As drones increasingly play a pivotal role across diverse sectors, integrating crowdsourced drone services within the IoDT has emerged as a vital avenue for democratizing access to these services. A critical challenge, however, lies in ensuring the authenticity and reliability of drone service reviews. Leveraging the Ethereum blockchain, DDRM addresses this challenge by instituting a verifiable and transparent review mechanism. The framework innovates with a dual-token system, comprising the Service Review Authorization Token (SRAT) for facilitating review authorization and the Drone Reputation Enhancement Token (DRET) for rewarding and recognizing drones demonstrating consistent reliability. Comprehensive analysis within this paper showcases DDRM's resilience against various reputation frauds and underscores its operational effectiveness, particularly in enhancing the efficiency and reliability of drone services.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>BAZAM: A Blockchain-Assisted Zero-Trust Authentication in Multi-UAV Wireless Networks</title>
<link>https://arxiv.org/abs/2407.00630</link>
<guid>https://arxiv.org/abs/2407.00630</guid>
<content:encoded><![CDATA[
<div> 关键词：Unmanned Aerial Vehicles (UAVs), Zero-trust framework, Blockchain, Authentication scheme, Physical Unclonable Functions (PUFs).

总结:<br />本文主要探讨了无人驾驶航空器(UAVs)网络中的身份验证问题。针对传统身份认证的不足，如系统中心化、无法适应多样的UAV身份和访问需求，以及缺乏持续的身份合规检查，作者提出了一种基于区块链的零信任身份验证方案BAZAM。该方案利用物理不可克隆功能(PUFs)生成密钥，并借助加密技术验证UAV的注册和访问请求，同时通过区块链实现UAV身份信息的永久存储。文章详细分析了BAZAM的安全性和效率，并通过实验验证其有效性。 <div>
arXiv:2407.00630v1 Announce Type: new 
Abstract: Unmanned aerial vehicles (UAVs) are vulnerable to interception and attacks when operated remotely without a unified and efficient identity authentication. Meanwhile, the openness of wireless communication environments potentially leads to data leakage and system paralysis. However, conventional authentication schemes in the UAV network are system-centric, failing to adapt to the diversity of UAVs identities and access, resulting in changes in network environments and connection statuses. Additionally, UAVs are not subjected to periodic identity compliance checks once authenticated, leading to difficulties in controlling access anomalies. Therefore, in this work, we consider a zero-trust framework for UAV network authentication, aiming to achieve UAVs identity authentication through the principle of ``never trust and always verify''. We introduce a blockchain-assisted zero-trust authentication scheme, namely BAZAM, designed for multi-UAV wireless networks. In this scheme, UAVs follow a key generation approach using physical unclonable functions (PUFs), and cryptographic technique helps verify registration and access requests of UAVs. The blockchain is applied to store UAVs authentication information in immutable storage. Through thorough security analysis and extensive evaluation, we demonstrate the effectiveness and efficiency of the proposed BAZAM.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Field Knowledge as a Dual to Distributed Knowledge: A Characterization by Weighted Modal Logic</title>
<link>https://arxiv.org/abs/2407.00687</link>
<guid>https://arxiv.org/abs/2407.00687</guid>
<content:encoded><![CDATA[
<div> 关键词：group knowledge, distributed knowledge, field knowledge, epistemic logic, weighted models

总结:<br />本文探讨了群体知识概念，如相互知识、共同知识和分布式知识在认识逻辑中的应用。作者提出将认知能力纳入分布式知识的定义，并引入了场知识的概念，作为分布式知识的对立面。研究基于带有不同群体知识构造的识别模型（weighted models），发展了八种逻辑系统，并分析了它们的表达力。文章通过形式化的方法，提供了关于这些逻辑系统的完备的公理体系。 <div>
arXiv:2407.00687v1 Announce Type: new 
Abstract: The study of group knowledge concepts such as mutual, common, and distributed knowledge is well established within the discipline of epistemic logic. In this work, we incorporate epistemic abilities of agents to refine the formal definition of distributed knowledge and introduce a formal characterization of field knowledge. We propose that field knowledge serves as a dual to distributed knowledge. Our approach utilizes epistemic logics with various group knowledge constructs, interpreted through weighted models. We delve into the eight logics that stem from these considerations, explore their relative expressivity and develop sound and complete axiomatic systems.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>A posteriori error estimator for elliptic interface problems in the fictitious formulation</title>
<link>https://arxiv.org/abs/2407.00786</link>
<guid>https://arxiv.org/abs/2407.00786</guid>
<content:encoded><![CDATA[
<div> 关键词：a posteriori error estimator, elliptic interface problem, fictitious domain formulation, discontinuous Lagrange multiplier, adaptive algorithm.

总结:<br />
本文研究了一种针对椭圆界面问题的后验误差估计器，该问题采用虚域方法和分布拉格朗日乘子处理，且考虑了不连续拉格朗日乘子有限元素空间。理论部分探讨了常数系数和光滑系数跳跃情况下的误差估计，证明了其可靠性和效率。数值实验通过不同几何嵌入和大系数跳跃实例验证了理论结果，展示了适应性算法的有效性。计算结果显示，误差估计器在处理几何奇异性或系数跳跃时表现出最优收敛特性。 <div>
arXiv:2407.00786v1 Announce Type: new 
Abstract: A posteriori error estimator is derived for an elliptic interface problem in the fictitious domain formulation with distributed Lagrange multiplier considering a discontinuous Lagrange multiplier finite element space. A posteriori error estimation plays a pivotal role in assessing the accuracy and reliability of computational solutions across various domains of science and engineering. This study delves into the theoretical underpinnings and computational considerations of a residual-based estimator.
  Theoretically, the estimator is studied for cases with constant coefficients which jump across an interface as well as generalized scenarios with smooth coefficients that jump across an interface. Theoretical findings demonstrate the reliability and efficiency of the proposed estimators under all considered cases.
  Numerical experiments are conducted to validate the theoretical results, incorporating various immersed geometries and instances of high coefficients jumps at the interface. Leveraging an adaptive algorithm, the estimator identifies regions with singularities and applies refinement accordingly. Results substantiate the theoretical findings, highlighting the reliability and efficiency of the estimators. Furthermore, numerical solutions exhibit optimal convergence properties, demonstrating resilience against geometric singularities or coefficients jumps.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Privacy-Aware Spectrum Pricing and Power Control Optimization for LEO Satellite Internet-of-Things</title>
<link>https://arxiv.org/abs/2407.00814</link>
<guid>https://arxiv.org/abs/2407.00814</guid>
<content:encoded><![CDATA[
<div> 关键词：Low Earth Orbit (LEO)、satellite systems、IoT、federated learning (FL)、blockchain

总结:<br />该论文关注低地球轨道(LEO)卫星系统在物联网(IoT)中的应用，探讨了如何通过结合区块链技术和联邦学习(FL)来解决复杂谱资源管理问题。首先，提出了一种基于深度强化学习的本地算法，以优化卫星的定价和功率控制策略，以最大化收入。其次，构建了一个去中心化的FL系统，利用区块链技术实现数据隐私保护。通过声誉机制，确保模型聚合和区块生成的信任度。最后，实验结果显示，该方法既能有效提高LEO卫星系统的收益，又能保护用户隐私。总的来说，研究者提出了一个创新的框架，旨在提升LEO卫星IoT的运营效率和隐私保护。 <div>
arXiv:2407.00814v1 Announce Type: new 
Abstract: Low earth orbit (LEO) satellite systems play an important role in next generation communication networks due to their ability to provide extensive global coverage with guaranteed communications in remote areas and isolated areas where base stations cannot be cost-efficiently deployed. With the pervasive adoption of LEO satellite systems, especially in the LEO Internet-of-Things (IoT) scenarios, their spectrum resource management requirements have become more complex as a result of massive service requests and high bandwidth demand from terrestrial terminals. For instance, when leasing the spectrum to terrestrial users and controlling the uplink transmit power, satellites collect user data for machine learning purposes, which usually are sensitive information such as location, budget and quality of service (QoS) requirement. To facilitate model training in LEO IoT while preserving the privacy of data, blockchain-driven federated learning (FL) is widely used by leveraging on a fully decentralized architecture. In this paper, we propose a hybrid spectrum pricing and power control framework for LEO IoT by combining blockchain technology and FL. We first design a local deep reinforcement learning algorithm for LEO satellite systems to learn a revenue-maximizing pricing and power control scheme. Then the agents collaborate to form a FL system. We also propose a reputation-based blockchain which is used in the global model aggregation phase of FL. Based on the reputation mechanism, a node is selected for each global training round to perform model aggregation and block generation, which can further enhance the decentralization of the network and guarantee the trust. Simulation tests are conducted to evaluate the performances of the proposed scheme. Our results show the efficiency of finding the maximum revenue scheme for LEO satellite systems while preserving the privacy of each agent.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Privacy-First Crowdsourcing: Blockchain and Local Differential Privacy in Crowdsourced Drone Services</title>
<link>https://arxiv.org/abs/2407.00873</link>
<guid>https://arxiv.org/abs/2407.00873</guid>
<content:encoded><![CDATA[
<div> 关键词：隐私保护、无人机、数据集成、本地差分隐私、区块链。

总结:<br />
本文介绍了一种新的隐私保护框架，旨在将消费级无人机融入丛林火灾管理。该系统通过本地差分隐私技术确保数据提供者的隐私安全，利用区块链技术保证公平的数据交换和责任追踪。文章以原型实现验证了其在大规模数据收集场景中的可行性和扩展性。这一解决方案符合澳大利亚《1988年隐私法》等法规，为通过众包无人机服务提升丛林火灾检测与管理提供了实用途径。 <div>
arXiv:2407.00873v1 Announce Type: new 
Abstract: We introduce a privacy-preserving framework for integrating consumer-grade drones into bushfire management. This system creates a marketplace where bushfire management authorities obtain essential data from drone operators. Key features include local differential privacy to protect data providers and a blockchain-based solution ensuring fair data exchanges and accountability. The framework is validated through a proof-of-concept implementation, demonstrating its scalability and potential for various large-scale data collection scenarios. This approach addresses privacy concerns and compliance with regulations like Australia's Privacy Act 1988, offering a practical solution for enhancing bushfire detection and management through crowdsourced drone services.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Decentralized PKI Framework for Data Integrity in Spatial Crowdsourcing Drone Services</title>
<link>https://arxiv.org/abs/2407.00876</link>
<guid>https://arxiv.org/abs/2407.00876</guid>
<content:encoded><![CDATA[
<div> 关键词：Spatial Crowdsourcing, D2XChain, Blockchain-based PKI, Internet of Drone Things (IoDT), X.509 standard.

总结:<br />
本文主要关注的是无人机服务领域的网络安全，特别是针对空间众包无人机任务（如配送、监控和数据收集）中的通信安全。文章提出了D2XChain，一种基于区块链的公钥基础设施（PKI）框架，旨在解决传统PKI中集中式信任模型的单点故障问题。D2XChain通过去中心化CA（证书权威机构）实现了分布式操作，支持X.509标准，涵盖证书注册、验证、核查和撤销等关键操作。它增强了无人机通信的安全性和可靠性，尤其在私有以太坊测试环境中成功部署，为无人机服务，特别是关键任务下的可信运营提供了创新且实用的解决方案。 <div>
arXiv:2407.00876v1 Announce Type: new 
Abstract: In the domain of spatial crowdsourcing drone services, which includes tasks like delivery, surveillance, and data collection, secure communication is paramount. The Public Key Infrastructure (PKI) ensures this by providing a system for digital certificates that authenticate the identities of entities involved, securing data and command transmissions between drones and their operators. However, the centralized trust model of traditional PKI, dependent on Certificate Authorities (CAs), presents a vulnerability due to its single point of failure, risking security breaches. To counteract this, the paper presents D2XChain, a blockchain-based PKI framework designed for the Internet of Drone Things (IoDT). By decentralizing the CA infrastructure, D2XChain eliminates this single point of failure, thereby enhancing the security and reliability of drone communications. Fully compatible with the X.509 standard, it integrates seamlessly with existing PKI systems, supporting all key operations such as certificate registration, validation, verification, and revocation in a distributed manner. This innovative approach not only strengthens the defense of drone services against various security threats but also showcases its practical application through deployment on a private Ethereum testbed, representing a significant advancement in addressing the unique security challenges of drone-based services and ensuring their trustworthy operation in critical tasks.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Ares II: Tracing the Flaws of a (Storage) God</title>
<link>https://arxiv.org/abs/2407.00881</link>
<guid>https://arxiv.org/abs/2407.00881</guid>
<content:encoded><![CDATA[
<div> 关键词：Ares, distributed shared memory, performance bottlenecks, optimizations, Ares II.

总结:<br />Ares是一个模块化的框架，旨在实现动态、可配置的分布式共享内存对象。最新版本通过版本化和数据条带技术支持大对象。研究者发现性能瓶颈并提出优化措施，包括piggyback机制、垃圾回收和批量重新配置，创建了优化版Ares II。该工作通过分布式追踪实验验证了Ares II的性能提升和存储效率改善，同时保持了正确性。 <div>
arXiv:2407.00881v1 Announce Type: new 
Abstract: Ares is a modular framework, designed to implement dynamic, reconfigurable, fault-tolerant, read/write and strongly consistent distributed shared memory objects. Recent enhancements of the framework have realized the efficient implementation of large objects, by introducing versioning and data striping techniques. In this work, we identify performance bottlenecks of the Ares's variants by utilizing distributed tracing, a popular technique for monitoring and profiling distributed systems. We then propose optimizations across all versions of Ares, aiming in overcoming the identified flaws, while preserving correctness. We refer to the optimized version of Ares as Ares II, which now features a piggyback mechanism, a garbage collection mechanism, and a batching reconfiguration technique for improving the performance and storage efficiency of the original Ares. We rigorously prove the correctness of Ares II, and we demonstrate the performance improvements by an experimental comparison (via distributed tracing) of the Ares II variants with their original counterparts.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>SplitLoRA: A Split Parameter-Efficient Fine-Tuning Framework for Large Language Models</title>
<link>https://arxiv.org/abs/2407.00952</link>
<guid>https://arxiv.org/abs/2407.00952</guid>
<content:encoded><![CDATA[
<div> 关键词：large language models, federated learning, split learning, SplitLoRA, benchmark

总结:<br />该研究关注的是大型语言模型（LLMs）的联邦学习（FL）和分割学习（SL）应用。面对LLMs对大量数据和计算资源的需求，文章提出SplitLoRA，一个首开源的SL LLM fine-tuning框架。它结合了FL的并行训练优势和SL的模型分割，以降低计算和通信压力，提高效率。SplitLoRA作为SL LLM调优的基准，旨在推动相关研究。实验结果表明，SplitLoRA在达到相同准确度时所需时间远少于现有方法，显示出其出色的训练性能。项目页面链接为https://fduinc.github.io/splitlora/。 <div>
arXiv:2407.00952v1 Announce Type: new 
Abstract: The scalability of large language models (LLMs) in handling high-complexity models and large-scale datasets has led to tremendous successes in pivotal domains. While there is an urgent need to acquire more training data for LLMs, a concerning reality is the depletion of high-quality public datasets within a few years. In view of this, the federated learning (FL) LLM fine-tuning paradigm recently has been proposed to facilitate collaborative LLM fine-tuning on distributed private data, where multiple data owners collaboratively fine-tune a shared LLM without sharing raw data. However, the staggering model size of LLMs imposes heavy computing and communication burdens on clients, posing significant barriers to the democratization of the FL LLM fine-tuning paradigm. To address this issue, split learning (SL) has emerged as a promising solution by offloading the primary training workload to a server via model partitioning while exchanging activation/activation's gradients with smaller data sizes rather than the entire LLM. Unfortunately, research on the SL LLM fine-tuning paradigm is still in its nascent stage. To fill this gap, in this paper, we propose the first SL LLM fine-tuning framework, named SplitLoRA. SplitLoRA is built on the split federated learning (SFL) framework, amalgamating the advantages of parallel training from FL and model splitting from SL and thus greatly enhancing the training efficiency. It is worth noting that SplitLoRA is the inaugural open-source benchmark for SL LLM fine-tuning, providing a foundation for research efforts dedicated to advancing SL LLM fine-tuning. Extensive simulations validate that SplitLoRA achieves target accuracy in significantly less time than state-of-the-art LLM fine-tuning frameworks, demonstrating the superior training performance of SplitLoRA. The project page is available at https://fduinc.github.io/splitlora/.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Parallel Computing Architectures for Robotic Applications: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2407.01011</link>
<guid>https://arxiv.org/abs/2407.01011</guid>
<content:encoded><![CDATA[
<div> 关键词：parallel computing, robotics, real-time processing, sensor integration, control algorithms

总结:
本文探讨了在现代机器人系统中，日益增长的复杂性和功能需求对计算性能提出了更高的要求。传统的串行计算已难以满足，因此并行计算，如多核CPU、GPU、FPGA和分布式系统，因其能同时处理多个任务而成为解决之道。这些架构在实时图像处理、传感器融合和路径规划等方面显著提升机器人系统的性能。文章通过实例分析展示了并行计算在机器人技术中的潜力，同时也指出了挑战，如硬件与软件协同、能耗问题等，并提出未来研究方向。并行计算为推动机器人技术进步提供了强大的工具。 <div>
arXiv:2407.01011v1 Announce Type: new 
Abstract: With the growing complexity and capability of contemporary robotic systems, the necessity of sophisticated computing solutions to efficiently handle tasks such as real-time processing, sensor integration, decision-making, and control algorithms is also increasing. Conventional serial computing frequently fails to meet these requirements, underscoring the necessity for high-performance computing alternatives. Parallel computing, the utilization of several processing elements simultaneously to solve computational problems, offers a possible answer. Various parallel computing designs, such as multi-core CPUs, GPUs, FPGAs, and distributed systems, provide substantial enhancements in processing capacity and efficiency. By utilizing these architectures, robotic systems can attain improved performance in functionalities such as real-time image processing, sensor fusion, and path planning. The transformative potential of parallel computing architectures in advancing robotic technology has been underscored, real-life case studies of these architectures in the robotics field have been discussed, and comparisons are presented. Challenges pertaining to these architectures have been explored, and possible solutions have been mentioned for further research and enhancement of the robotic applications.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>DistML.js: Installation-free Distributed Deep Learning Framework for Web Browsers</title>
<link>https://arxiv.org/abs/2407.01023</link>
<guid>https://arxiv.org/abs/2407.01023</guid>
<content:encoded><![CDATA[
<div> 关键词：DistML.js, web浏览器, 机器学习模型, 深度学习 API, WebGL

总结:<br />DistML.js是一个专为Web浏览器设计的机器学习库，支持本地训练和分布式学习。其API与PyTorch类似，便于原型开发，利用WebGL进行后台矩阵计算以实现高效运算。该库强调数据并行性，源代码开源。<br />DistML.js是用于浏览器的机器学习解决方案，易于使用，支持本地和服务器协作，通过WebGL加速计算，适合快速原型和实践深度学习项目。 <div>
arXiv:2407.01023v1 Announce Type: new 
Abstract: We present "DistML.js", a library designed for training and inference of machine learning models within web browsers. Not only does DistML.js facilitate model training on local devices, but it also supports distributed learning through communication with servers. Its design and define-by-run API for deep learning model construction resemble PyTorch, thereby reducing the learning curve for prototyping. Matrix computations involved in model training and inference are executed on the backend utilizing WebGL, enabling high-speed calculations. We provide a comprehensive explanation of DistML.js's design, API, and implementation, alongside practical applications including data parallelism in learning. The source code is publicly available at https://github.com/mil-tokyo/distmljs.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Randomized linear solvers for computational architectures with straggling workers</title>
<link>https://arxiv.org/abs/2407.01098</link>
<guid>https://arxiv.org/abs/2407.01098</guid>
<content:encoded><![CDATA[
<div> 关键词：sparse system, iterative solution, partial matrix-vector product, random variable, convergence.

总结:<br />该论文探讨了在仅部分计算稀疏线性方程组系数矩阵的乘积时，迭代求解的问题。模型假设计算的元素数量及其行索引集是随机变量，且给定数量时行索引均匀分布。研究者提出了一种随机Richardson迭代法和Chebyshev半迭代法，并证明了它们在期望下的收敛条件。实验结果验证了理论和方法的有效性，尤其是在混合云架构的控制器-工作者分布式模型中，处理延迟（straggling workers）的情况。 <div>
arXiv:2407.01098v1 Announce Type: new 
Abstract: In this paper, we consider the iterative solution of sparse systems of linear algebraic equations under the condition that sparse matrix-vector products with the coefficient matrix are computed only partially. At the same time, non-computed entries are set to zeros. We assume that both the number of computed entries and their associated row index set are random variables, with the row index set sampled uniformly given the number of computed entries. This model of computations is prevalent to that realized in hybrid cloud computing architectures following the controller-worker distributed model under the influence of straggling workers. We propose a randomized Richardson iterative scheme and a randomized Chebyshev semi-iterative method within this model and prove the sufficient conditions for their convergence in expectation. Numerical experiments verify the presented theoretical results as well as the effectiveness of the proposed schemes on a few sparse matrix problems.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>FedRC: A Rapid-Converged Hierarchical Federated Learning Framework in Street Scene Semantic Understanding</title>
<link>https://arxiv.org/abs/2407.01103</link>
<guid>https://arxiv.org/abs/2407.01103</guid>
<content:encoded><![CDATA[
<div> 关键词：Street Scene Semantic Understanding, Hierarchical Federated Learning, Convergence Rate, Data Heterogeneity, Gaussian Distributions.

总结:<br />
本文提出了一种新的联邦学习框架FedRC，针对城市间数据异质性问题，旨在加速街景语义理解（TriSU）任务的模型收敛。FedRC通过将单张RGB图像和RGB数据集建模为高斯分布，区分每个样本并考虑数据量和统计特性，而非仅凭数据量决策，从而提高了HFL在复杂任务中的性能。实验结果显示，FedRC比现有基准快38.7%、37.5%、35.5%和40.6%的mIoU、mPrecision、mRecall和mF1，且在CARLA模拟环境中表现出色，展示了顶级性能。 <div>
arXiv:2407.01103v1 Announce Type: new 
Abstract: Street Scene Semantic Understanding (denoted as TriSU) is a crucial but complex task for world-wide distributed autonomous driving (AD) vehicles (e.g., Tesla). Its inference model faces poor generalization issue due to inter-city domain-shift. Hierarchical Federated Learning (HFL) offers a potential solution for improving TriSU model generalization, but suffers from slow convergence rate because of vehicles' surrounding heterogeneity across cities. Going beyond existing HFL works that have deficient capabilities in complex tasks, we propose a rapid-converged heterogeneous HFL framework (FedRC) to address the inter-city data heterogeneity and accelerate HFL model convergence rate. In our proposed FedRC framework, both single RGB image and RGB dataset are modelled as Gaussian distributions in HFL aggregation weight design. This approach not only differentiates each RGB sample instead of typically equalizing them, but also considers both data volume and statistical properties rather than simply taking data quantity into consideration. Extensive experiments on the TriSU task using across-city datasets demonstrate that FedRC converges faster than the state-of-the-art benchmark by 38.7%, 37.5%, 35.5%, and 40.6% in terms of mIoU, mPrecision, mRecall, and mF1, respectively. Furthermore, qualitative evaluations in the CARLA simulation environment confirm that the proposed FedRC framework delivers top-tier performance.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>SCIF: A Language for Compositional Smart Contract Security</title>
<link>https://arxiv.org/abs/2407.01204</link>
<guid>https://arxiv.org/abs/2407.01204</guid>
<content:encoded><![CDATA[
<div> 关键词：SCIF、智能合约、安全、信息流、Solidity。

总结:<br />
SCIF是一种用于构建安全智能合约的语言，特别强调在与不可信代码协作时的安全性。它基于安全信息流原理，扩展机制以防御重入攻击、混淆副官攻击和错误处理问题，即使面对不遵守规则的恶意合约也能保护系统。SCIF支持动态信任管理，允许复杂生态中部分信任的主体交互。该语言已实现为Solidity编译器，提供静态检查规则和运行时支持。通过实施多个具有深度安全考虑的应用，SCIF展示了其在构建复杂智能合约中的有效性和对潜在安全漏洞的精确诊断能力。 <div>
arXiv:2407.01204v1 Announce Type: new 
Abstract: Securing smart contracts remains a fundamental challenge. At its core, it is about building software that is secure in composition with untrusted code, a challenge that extends far beyond blockchains. We introduce SCIF, a language for building smart contracts that are compositionally secure. SCIF is based on the fundamentally compositional principle of secure information flow, but extends this core mechanism to include protection against reentrancy attacks, confused deputy attacks, and improper error handling, even in the presence of malicious contracts that do not follow SCIF's rules. SCIF supports a rich ecosystem of interacting principals with partial trust through its mechanisms for dynamic trust management. SCIF has been implemented as a compiler to Solidity. We describe the SCIF language, including its static checking rules and runtime. Finally, we implement several applications with intricate security reasoning, showing how SCIF supports building complex smart contracts securely and gives programmer accurate diagnostics about potential security bugs.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>On the Parameters of Codes for Data Access</title>
<link>https://arxiv.org/abs/2407.01229</link>
<guid>https://arxiv.org/abs/2407.01229</guid>
<content:encoded><![CDATA[
<div> 关键词：coded distributed storage systems, alphabet size, servers, service rate region, code constructions.

总结:<br />该论文关注编码分布式存储系统中的关键问题，研究了两个方面：<br />1) 在固定字母大小下，确定最小服务器数量以保证服务速率区域包含特定点；<br />2) 对于给定服务器数，找出最小字母大小以满足相同条件。论文提供了严格的上界和下界，以及基于编码理论、优化和项目几何的代码构造方法。<br />通过这些方法，作者深入探讨了系统性能与设计参数之间的关系。 <div>
arXiv:2407.01229v1 Announce Type: new 
Abstract: This paper studies two crucial problems in the context of coded distributed storage systems directly related to their performance: 1) for a fixed alphabet size, determine the minimum number of servers the system must have for its service rate region to contain a prescribed set of points; 2) for a given number of servers, determine the minimum alphabet size for which the service rate region of the system contains a prescribed set of points. The paper establishes rigorous upper and lower bounds, as well as code constructions based on techniques from coding theory, optimization, and projective geometry.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>An Operational Semantics for Yul</title>
<link>https://arxiv.org/abs/2407.01365</link>
<guid>https://arxiv.org/abs/2407.01365</guid>
<content:encoded><![CDATA[
<div> 关键词：Yul、Solidity、EVM bytecode、operational semantics、small-step semantics

总结:<br />
本文介绍了一种针对Yul（Solidity编译器产生的EVM字节码的中间语言）的大步和小步操作语义，采用与编程语言文献相符的数学表示法，便于语言证明并作为精确易理解的语言规范。作者对原有非正式规范进行了澄清，并证明了两种语义之间的等价性。此外，他们实现了一个小型步态解释器，支持优化并经过测试。这项工作有望推动在Yul上开发验证和符号执行技术，增强以太坊安全体系，并为未来的类型系统提供坚实的理论基础。 <div>
arXiv:2407.01365v1 Announce Type: new 
Abstract: We present a big-step and small-step operational semantics for Yul -- the intermediate language used by the Solidity compiler to produce EVM bytecode -- in a mathematical notation that is congruous with the literature of programming languages, lends itself to language proofs, and can serve as a precise, widely accessible specification for the language. Our two semantics stay faithful to the original, informal specification of the language but also clarify under-specified cases such as void function calls. Our presentation allows us to prove the equivalence between the two semantics. We also implement the small-step semantics in an interpreter for Yul which avails of optimisations that are provably correct. We have tested the interpreter using tests from the Solidity compiler and our own. We envisage that this work will enable the development of verification and symbolic execution technology directly in Yul, contributing to the Ethereum security ecosystem, as well as aid the development of a provably sound future type system.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Beyond Throughput and Compression Ratios: Towards High End-to-end Utility of Gradient Compression</title>
<link>https://arxiv.org/abs/2407.01378</link>
<guid>https://arxiv.org/abs/2407.01378</guid>
<content:encoded><![CDATA[
<div> 关键词：gradient compression, distributed machine learning, training systems, acceleration, accuracy.

总结:
本文主要关注大规模分布式机器学习训练系统中的瓶颈——梯度聚合。作者发现先前的梯度压缩方案存在计算开销大、与all-reduce不兼容以及评估指标不合适等问题。为解决这些问题，作者提出改进设计和评估技术，包括降低计算负担、适应all-reduce通信方式以及采用更合适的端到端评价标准（如16位基线）。初步结果显示，这些技巧提升了系统的性能，有助于更准确地评估梯度压缩方法的实际效益。 <div>
arXiv:2407.01378v1 Announce Type: new 
Abstract: Gradient aggregation has long been identified as a major bottleneck in today's large-scale distributed machine learning training systems. One promising solution to mitigate such bottlenecks is gradient compression, directly reducing communicated gradient data volume. However, in practice, many gradient compression schemes do not achieve acceleration of the training process while also preserving accuracy.
  In this work, we identify several common issues in previous gradient compression systems and evaluation methods. These issues include excessive computational overheads; incompatibility with all-reduce; and inappropriate evaluation metrics, such as not using an end-to-end metric or using a 32-bit baseline instead of a 16-bit baseline. We propose several general design and evaluation techniques to address these issues and provide guidelines for future work. Our preliminary evaluation shows that our techniques enhance the system's performance and provide a clearer understanding of the end-to-end utility of gradient compression methods.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Maximizing Blockchain Performance: Mitigating Conflicting Transactions through Parallelism and Dependency Management</title>
<link>https://arxiv.org/abs/2407.01426</link>
<guid>https://arxiv.org/abs/2407.01426</guid>
<content:encoded><![CDATA[
<div> 关键词：blockchains, cryptocurrency, conflicting transactions, Hyperledger Fabric, transaction parallelism

总结: 本文主要关注区块链技术在加密货币领域的扩展应用，特别是解决"冲突交易"（conflicting transactions）问题。提出了一种新的区块链方案，结合了事务并行性和智能依赖管理器，旨在减少交易冲突，从而降低网络延迟、提高系统资源利用率和交易成功率。实验结果显示，该方案不仅有效解决了冲突交易问题，而且在与现有的Hyperledger Fabric网络比较中表现出色，实现了更高的交易吞吐量和更低的延迟。这一集成为改善现实世界中区块链网络的性能和稳定性提供了前景。<br /><br />总结: 关键词：区块链、冲突交易、Hyperledger Fabric、事务并行性、智能依赖管理。文章提出的新方案通过优化并行处理和依赖管理，显著提升了区块链网络的性能，特别是在交易成功率、吞吐量和延迟方面超越了现有系统，为实际应用中的区块链网络改进提供了解决方案。 <div>
arXiv:2407.01426v1 Announce Type: new 
Abstract: While blockchains initially gained popularity in the realm of cryptocurrencies, their widespread adoption is expanding beyond conventional applications, driven by the imperative need for enhanced data security. Despite providing a secure network, blockchains come with certain tradeoffs, including high latency, lower throughput, and an increased number of transaction failures. A pivotal issue contributing to these challenges is the improper management of "conflicting transactions", commonly referred to as "contention". When a number of pending transactions within a blockchain collide with each other, this results in a state of contention. This situation worsens network latency, leads to the wastage of system resources, and ultimately contributes to reduced throughput and higher transaction failures. In response to this issue, in this work, we present a novel blockchain scheme that integrates transaction parallelism and an intelligent dependency manager aiming to reduce the occurrence of conflicting transactions within blockchain networks. In terms of effectiveness and efficiency, experimental results show that our scheme not only mitigates the challenges posed by conflicting transactions, but also outperforms both existing parallel and non-parallel Hyperledger Fabric blockchain networks achieving higher transaction success rate, throughput, and latency. The integration of our scheme with Hyperledger Fabric appears to be a promising solution for improving the overall performance and stability of blockchain networks in real-world applications.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Reinforcement Learning-driven Data-intensive Workflow Scheduling for Volunteer Edge-Cloud</title>
<link>https://arxiv.org/abs/2407.01428</link>
<guid>https://arxiv.org/abs/2407.01428</guid>
<content:encoded><![CDATA[
<div> 关键词：Volunteer Edge-Cloud (VEC), Reinforcement Learning (RL), Data-intensive scientific workflows, Workflow scheduling, Resource allocation.

总结:<br />本文提出了一种基于强化学习（RL）的策略，旨在解决志愿边缘云（VEC）中数据密集型科学工作流任务调度的挑战。该方法考虑了工作流需求、VEC资源对工作流的偏好以及多样的VEC资源策略，通过将长期性能优化问题建模为马尔可夫决策过程，采用事件驱动的异步优势Actor-Critic RL算法求解。实验结果表明，相比于传统方法，该RL驱动的调度方案在满足工作流需求、优化VEC资源使用和满意度方面表现出色。 <div>
arXiv:2407.01428v1 Announce Type: new 
Abstract: In recent times, Volunteer Edge-Cloud (VEC) has gained traction as a cost-effective, community computing paradigm to support data-intensive scientific workflows. However, due to the highly distributed and heterogeneous nature of VEC resources, centralized workflow task scheduling remains a challenge. In this paper, we propose a Reinforcement Learning (RL)-driven data-intensive scientific workflow scheduling approach that takes into consideration: i) workflow requirements, ii) VEC resources' preference on workflows, and iii) diverse VEC resource policies, to ensure robust resource allocation. We formulate the long-term average performance optimization problem as a Markov Decision Process, which is solved using an event-based Asynchronous Advantage Actor-Critic RL approach. Our extensive simulations and testbed implementations demonstrate our approach's benefits over popular baseline strategies in terms of workflow requirement satisfaction, VEC preference satisfaction, and available VEC resource utilization.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>FastCLIP: A Suite of Optimization Techniques to Accelerate CLIP Training with Limited Resources</title>
<link>https://arxiv.org/abs/2407.01445</link>
<guid>https://arxiv.org/abs/2407.01445</guid>
<content:encoded><![CDATA[
<div> 关键词：Contrastive Language-Image Pretraining (CLIP), Large-scale data, Resource limitation, FastCLIP, Optimization techniques.

总结:<br />该研究关注在资源受限情况下（如少于数百GPU）训练最先进的CLIP模型。FastCLIP是一个专为分布式设置设计和优化的框架，采用高效梯度降低策略减少通信开销。研究还探讨了内学习率调度、温度参数和模型参数更新规则对效率的影响。实验表明，FastCLIP在不同规模（32 GPU和3种数据集大小）上显著优于OpenCLIP基准，特别在资源有限的情况下。研究成果已开源。<br /> <div>
arXiv:2407.01445v1 Announce Type: new 
Abstract: Existing studies of training state-of-the-art Contrastive Language-Image Pretraining (CLIP) models on large-scale data involve hundreds of or even thousands of GPUs due to the requirement of a large batch size. However, such a large amount of resources is not accessible to most people. While advanced compositional optimization techniques for optimizing global contrastive losses have been demonstrated effective for removing the requirement of large batch size, their performance on large-scale data remains underexplored and not optimized. To bridge the gap, this paper explores several aspects of CLIP training with limited resources (e.g., up to tens of GPUs). First, we introduce FastCLIP, a general CLIP training framework built on advanced compositional optimization techniques while designed and optimized for the distributed setting. Our framework is equipped with an efficient gradient reduction strategy to reduce communication overhead. Second, to further boost training efficiency, we investigate three components of the framework from an optimization perspective: the schedule of the inner learning rate, the update rules of the temperature parameter and the model parameters, respectively. Experiments on different strategies for each component shed light on how to conduct CLIP training more efficiently. Finally, we benchmark the performance of FastCLIP and the state-of-the-art training baseline (OpenCLIP) on different compute scales up to 32 GPUs on 8 nodes, and three data scales ranging from 2.7 million, 9.1 million to 315 million image-text pairs to demonstrate the significant improvement of FastCLIP in the resource-limited setting. We release the code of FastCLIP at https://github.com/Optimization-AI/fast_clip .
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>How Clustering Affects the Convergence of Decentralized Optimization over Networks: A Monte-Carlo-based Approach</title>
<link>https://arxiv.org/abs/2407.01460</link>
<guid>https://arxiv.org/abs/2407.01460</guid>
<content:encoded><![CDATA[
<div> 关键词：Decentralized algorithms, Convergence rate, Network topology, Clustering coefficient, Scale-free networks.

总结:
本文主要探讨了分布式优化算法在随机尺度自由网络（Scale-free, SF）和簇尺度自由网络（Clustered Scale-free, CSF）中的收敛速度，通过调整网络的聚类系数。研究发现，当其他网络属性如幂律度分布、链接数量和平均度保持不变时，低聚类系数的网络往往具有更快的收敛率。这一发现对于改进现有分布式机器学习系统的学习速率具有重要意义，因为可以通过调整网络结构来提升性能。作者还通过实际网络案例分析，进一步验证了这一结论。 <div>
arXiv:2407.01460v1 Announce Type: new 
Abstract: Decentralized algorithms have gained substantial interest owing to advancements in cloud computing, Internet of Things (IoT), intelligent transportation networks, and parallel processing over sensor networks. The convergence of such algorithms is directly related to specific properties of the underlying network topology. Specifically, the clustering coefficient is known to affect, for example, the controllability/observability and the epidemic growth over networks. In this work, we study the effects of the clustering coefficient on the convergence rate of networked optimization approaches. In this regard, we model the structure of large-scale distributed systems by random scale-free (SF) and clustered scale-free (CSF) networks and compare the convergence rate by tuning the network clustering coefficient. This is done by keeping other relevant network properties (such as power-law degree distribution, number of links, and average degree) unchanged. Monte-Carlo-based simulations are used to compare the convergence rate over many trials of SF graph topologies. Furthermore, to study the convergence rate over real case studies, we compare the clustering coefficient of some real-world networks with the eigenspectrum of the underlying network (as a measure of convergence rate). The results interestingly show higher convergence rate over low-clustered networks. This is significant as one can improve the learning rate of many existing decentralized machine-learning scenarios by tuning the network clustering.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Immutable in Principle, Upgradeable by Design: Exploratory Study of Smart Contract Upgradeability</title>
<link>https://arxiv.org/abs/2407.01493</link>
<guid>https://arxiv.org/abs/2407.01493</guid>
<content:encoded><![CDATA[
<div> 关键词：smart contracts, upgradeable, Ethereum blockchain, upgrade mechanisms, user engagement.

总结:<br />
升级可编程合约（upgradeable smart contracts）是Ethereum区块链上的一种创新，旨在解决固有不可变性与后期维护需求之间的矛盾。本文研究通过创建一个详细记录智能合约版本和演化路径的数据库，关注五个关键点：升级机制的使用频率、升级行为的发生率、升级后的修改性质、对用户参与的影响以及合同活动的变化。结果发现，只有约3%的智能合约具备升级功能，其中只有0.34%实际进行了升级，表明开发者对于改动持谨慎态度，可能源于升级过程的复杂性和维护稳定性偏好。升级主要集中在功能增强和漏洞修复，尤其是当源代码公开时。然而，升级与用户活跃度的关系复杂，暗示着影响智能合约使用的因素远不止其进化历程。 <div>
arXiv:2407.01493v1 Announce Type: new 
Abstract: Smart contracts, known for their immutable nature to ensure trust via automated enforcement, have evolved to require upgradeability due to unforeseen vulnerabilities and the need for feature enhancements post-deployment. This contradiction between immutability and the need for modifications has led to the development of upgradeable smart contracts. These contracts are immutable in principle yet upgradable by design, allowing updates without altering the underlying data or state, thus preserving the contract's intent while allowing improvements. This study aims to understand the application and implications of upgradeable smart contracts on the Ethereum blockchain. By introducing a dataset that catalogs the versions and evolutionary trajectories of smart contracts, the research explores key dimensions: the prevalence and adoption patterns of upgrade mechanisms, the likelihood and occurrences of contract upgrades, the nature of modifications post-upgrade, and their impact on user engagement and contract activity. Through empirical analysis, this study identifies upgradeable contracts and examines their upgrade history to uncover trends, preferences, and challenges associated with modifications. The evidence from analyzing over 44 million contracts shows that only 3% have upgradeable characteristics, with only 0.34% undergoing upgrades. This finding underscores a cautious approach by developers towards modifications, possibly due to the complexity of upgrade processes or a preference for maintaining stability. Furthermore, the study shows that upgrades are mainly aimed at feature enhancement and vulnerability mitigation, particularly when the contracts' source codes are accessible. However, the relationship between upgrades and user activity is complex, suggesting that additional factors significantly affect the use of smart contracts beyond their evolution.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>The Inverted 3-Sum Box: General Formulation and Quantum Information Theoretic Optimality</title>
<link>https://arxiv.org/abs/2407.01498</link>
<guid>https://arxiv.org/abs/2407.01498</guid>
<content:encoded><![CDATA[
<div> 关键词：$N$-sum box, quantum multiple access channel (QMAC), communication cost, quantum coding schemes, information theory.

总结:<br />
该论文探讨了在量子多路访问信道（QMAC）上计算$\mathbb{F}_d$线性函数的最优通信成本问题。研究焦点在于给定任意转移矩阵$V_k$时，确定所有可能的量子编码方案中，传输所需最小的量子比特数$\Delta_1, \Delta_2, \Delta_3$。对于三个发送者（$K=3$）的情况，作者给出了完整的结果，表明基于$N$-sum box协议的编码在所有情况下都达到信息论最优。文章还提供了特定参数下（如矩阵秩$r_1, r_2, r_3$）的最小总下载成本公式。对于$K\geq 4$的发送者情况，问题仍未解决。 <div>
arXiv:2407.01498v1 Announce Type: new 
Abstract: The $N$-sum box protocol specifies a class of $\mathbb{F}_d$ linear functions $f(W_1,\cdots,W_K)=V_1W_1+V_2W_2+\cdots+V_KW_K\in\mathbb{F}_d^{m\times 1}$ that can be computed at information theoretically optimal communication cost (minimum number of qudits $\Delta_1,\cdots,\Delta_K$ sent by the transmitters Alice$_1$, Alice$_2$,$\cdots$, Alice$_K$, respectively, to the receiver, Bob, per computation instance) over a noise-free quantum multiple access channel (QMAC), when the input data streams $W_k\in\mathbb{F}_d^{m_k\times 1}, k\in[K]$, originate at the distributed transmitters, who share quantum entanglement in advance but are not otherwise allowed to communicate with each other. In prior work this set of optimally computable functions is identified in terms of a strong self-orthogonality (SSO) condition on the transfer function of the $N$-sum box. In this work we consider an `inverted' scenario, where instead of a feasible $N$-sum box transfer function, we are given an arbitrary $\mathbb{F}_d$ linear function, i.e., arbitrary matrices $V_k\in\mathbb{F}_d^{m\times m_k}$ are specified, and the goal is to characterize the set of all feasible communication cost tuples $(\Delta_1,\cdots,\Delta_K)$, not just based on $N$-sum box protocols, but across all possible quantum coding schemes. As our main result, we fully solve this problem for $K=3$ transmitters ($K\geq 4$ settings remain open). Coding schemes based on the $N$-sum box protocol (along with elementary ideas such as treating qudits as classical dits, time-sharing and batch-processing) are shown to be information theoretically optimal in all cases. As an example, in the symmetric case where rk$(V_1)$=rk$(V_2)$=rk$(V_3) \triangleq r_1$, rk$([V_1, V_2])$=rk$([V_2, V_3])$=rk$([V_3, V_1])\triangleq r_2$, and rk$([V_1, V_2, V_3])\triangleq r_3$ (rk = rank), the minimum total-download cost is $\max \{1.5r_1 + 0.75(r_3 - r_2), r_3\}$.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Linear and Nonlinear MMSE Estimation in One-Bit Quantized Systems under a Gaussian Mixture Prior</title>
<link>https://arxiv.org/abs/2407.01305</link>
<guid>https://arxiv.org/abs/2407.01305</guid>
<content:encoded><![CDATA[
<div> 关键词：mean square error (MSE), conditional mean estimator (CME), one-bit quantization, Gaussian mixture model (GMM), additive white Gaussian noise (AWGN).

总结:<br />该论文研究了一比特量化系统中，对于由高斯混合模型（GMM）分布的信号和加性白高斯噪声（AWGN）干扰的信号，MSE-最优条件均值估计器（CME）的新理论。首先，论文提供了Busgang估计器的闭式解析表达式，这是量化系统中的线性最小均方误差（MMSE）估计算法。接着，文中揭示了CME在特殊情况下的线性性质，与高分辨率情况相反。论文还比较了Gaussian案例，发现信号与量化噪声存在相关性。此外，研究扩展到多观测场景，探讨了MSE-最优发送序列，并进行了大样本分析，给出了MSE及其极限的解析表达。这些结果对信号处理应用的分析和设计具有广泛影响。 <div>
arXiv:2407.01305v1 Announce Type: cross 
Abstract: We present new fundamental results for the mean square error (MSE)-optimal conditional mean estimator (CME) in one-bit quantized systems for a Gaussian mixture model (GMM) distributed signal of interest, possibly corrupted by additive white Gaussian noise (AWGN). We first derive novel closed-form analytic expressions for the Bussgang estimator, the well-known linear minimum mean square error (MMSE) estimator in quantized systems. Afterward, closed-form analytic expressions for the CME in special cases are presented, revealing that the optimal estimator is linear in the one-bit quantized observation, opposite to higher resolution cases. Through a comparison to the recently studied Gaussian case, we establish a novel MSE inequality and show that that the signal of interest is correlated with the auxiliary quantization noise. We extend our analysis to multiple observation scenarios, examining the MSE-optimal transmit sequence and conducting an asymptotic analysis, yielding analytic expressions for the MSE and its limit. These contributions have broad impact for the analysis and design of various signal processing applications.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Neural Distributed Source Coding</title>
<link>https://arxiv.org/abs/2106.02797</link>
<guid>https://arxiv.org/abs/2106.02797</guid>
<content:encoded><![CDATA[
<div> 关键词：Distributed source coding, Slepian-Wolf, Vector-Quantized Variational Autoencoder (VQ-VAE), Lossy compression, High dimensions.

总结:<br />本文介绍了一种新颖的分布式源编码（DSC）框架，它能够处理复杂的数据集和各种相关性结构，突破了传统方法的限制。该方法利用条件Vector-Quantized Variational Autoencoder (VQ-VAE)，即条件VQ-VAE，作为学习型编码器和解码器，实现了无须依赖特定源模型的高效编码。实验结果显示，这种神经网络驱动的DSC方法在处理高维数据时表现出色，能够在保持高质量（PSNR）的同时，适应各类复杂的关联。代码已在GitHub上开源，为实际应用中的分布式压缩提供了新的可能。 <div>
arXiv:2106.02797v4 Announce Type: replace 
Abstract: Distributed source coding (DSC) is the task of encoding an input in the absence of correlated side information that is only available to the decoder. Remarkably, Slepian and Wolf showed in 1973 that an encoder without access to the side information can asymptotically achieve the same compression rate as when the side information is available to it. While there is vast prior work on this topic, practical DSC has been limited to synthetic datasets and specific correlation structures. Here we present a framework for lossy DSC that is agnostic to the correlation structure and can scale to high dimensions. Rather than relying on hand-crafted source modeling, our method utilizes a conditional Vector-Quantized Variational Autoencoder (VQ-VAE) to learn the distributed encoder and decoder. We evaluate our method on multiple datasets and show that our method can handle complex correlations and achieves state-of-the-art PSNR. Our code is made available at https://github.com/acnagle/neural-dsc.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Directional Antenna Based Scheduling Protocol for IoT Networks</title>
<link>https://arxiv.org/abs/2305.02511</link>
<guid>https://arxiv.org/abs/2305.02511</guid>
<content:encoded><![CDATA[
<div> 关键词：IoT网络，MAC层，Scheduling协议，Directional Scheduling，6TiSCH-IoT。

总结:<br />
本文主要关注物联网(IoT)网络中的一 hop 调度和频道访问。现有的基于全方位天线的应用数据传输在性能上不如采用方向性天线的调度协议。为此，研究者提出了一种分布式一跳调度算法，称为Directional Scheduling协议，特别适用于受限的确定性6TiSCH-IoT网络。该算法通过定向传输实现了更高的空间重用，允许更多的物联网节点并发数据传输，减少了头阻塞现象。结果表明，这种策略能够显著提升6TiSCH-IoT网络的吞吐量和效率。 <div>
arXiv:2305.02511v2 Announce Type: replace 
Abstract: Scheduling and Channel Access at the MAC layer of the IoT network plays a pivotal role in enhancing the performance of IoT networks. State-of-the-art Omni-directional antenna based application data transmission has relatively less achievable throughput in comparison with directional antenna based scheduling protocols. To enhance the performance of the IoT networks, this paper propose a distributed one-hop scheduling algorithm called Directional Scheduling protocol for constrained deterministic 6TiSCH-IoT network. With this, in-creased number of IoT nodes can have concurrent application data transmission with efficient spatial reuse. This in-turn results in higher number of cell allocation to the one-hop IoT nodes during data transmission. The proposed algorithm makes use of through directional transmissions avoids head of line blocking.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration</title>
<link>https://arxiv.org/abs/2305.19476</link>
<guid>https://arxiv.org/abs/2305.19476</guid>
<content:encoded><![CDATA[
<div> 关键词：state entropy, reinforcement learning, value-conditional state entropy, exploration, high-value states.

总结:<br />本文介绍了一种新的强化学习探索策略，针对传统方法在有任务奖励的监督环境中遇到的问题。该策略关注价值条件下的状态熵（value-conditional state entropy），即分别估计每个状态的价值估计后计算的熵，以平均值为目标进行最大化。这种方法避免了低价值和高价值状态分布之间的不平衡，防止了低价值区域的探索偏向于高价值区域。实验结果表明，相比于单纯的 state entropy 基线，该策略在 MiniGrid、DeepMind Control Suite 和 Meta-World 等多个任务中显著加速了学习过程。源代码可在此处获取：[链接]。 <div>
arXiv:2305.19476v2 Announce Type: replace 
Abstract: A promising technique for exploration is to maximize the entropy of visited state distribution, i.e., state entropy, by encouraging uniform coverage of visited state space. While it has been effective for an unsupervised setup, it tends to struggle in a supervised setup with a task reward, where an agent prefers to visit high-value states to exploit the task reward. Such a preference can cause an imbalance between the distributions of high-value states and low-value states, which biases exploration towards low-value state regions as a result of the state entropy increasing when the distribution becomes more uniform. This issue is exacerbated when high-value states are narrowly distributed within the state space, making it difficult for the agent to complete the tasks. In this paper, we present a novel exploration technique that maximizes the value-conditional state entropy, which separately estimates the state entropies that are conditioned on the value estimates of each state, then maximizes their average. By only considering the visited states with similar value estimates for computing the intrinsic bonus, our method prevents the distribution of low-value states from affecting exploration around high-value states, and vice versa. We demonstrate that the proposed alternative to the state entropy baseline significantly accelerates various reinforcement learning algorithms across a variety of tasks within MiniGrid, DeepMind Control Suite, and Meta-World benchmarks. Source code is available at https://sites.google.com/view/rl-vcse.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Distributed Pilot Assignment for Distributed Massive-MIMO Networks</title>
<link>https://arxiv.org/abs/2309.15709</link>
<guid>https://arxiv.org/abs/2309.15709</guid>
<content:encoded><![CDATA[
<div> 关键词：pilot contamination, distributed massive MIMO, pilot assignment scheme, signaling overhead, fault-tolerance.

总结:<br />
本文主要关注大规模多输入多输出（massive MIMO）网络中的飞行员污染问题。提出了一种创新的分布式飞行员分配方案，旨在有效缓解该问题，同时减少信号开销并提高系统的容错性。通过大量数值模拟，研究结果显示，新方案在降低飞行员污染和提升网络吞吐量方面优于现有的中心化和分布式策略。总的来说，这项工作提供了一个有效的解决方案，优化了多用户环境下的通信性能。 <div>
arXiv:2309.15709v3 Announce Type: replace 
Abstract: Pilot contamination is a critical issue in distributed massive MIMO networks, where the reuse of pilot sequences due to limited availability of orthogonal pilots for channel estimation leads to performance degradation. In this work, we propose a novel distributed pilot assignment scheme to effectively mitigate the impact of pilot contamination. Our proposed scheme not only reduces signaling overhead, but it also enhances fault-tolerance. Extensive numerical simulations are conducted to evaluate the performance of the proposed scheme. Our results establish that the proposed scheme outperforms existing centralized and distributed schemes in terms of mitigating pilot contamination and significantly enhancing network throughput.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>CO-ASnet :A Smart Contract Architecture Design based on Blockchain Technology with Active Sensor Networks</title>
<link>https://arxiv.org/abs/2310.05070</link>
<guid>https://arxiv.org/abs/2310.05070</guid>
<content:encoded><![CDATA[
<div> 关键词：opinion leaders, blockchain, ICOs, influence, regulatory scheme

总结:<br />
这篇文章关注意见领袖在区块链金融中的影响力，特别是通过ICO进行资产影响的案例分析。研究发现，意见领袖能够利用社交媒体和社交网络中的金钱与流量影响代币资产价格，实现超额回报并降低资产实现成本。基于此现象，文章提出采用ChainLink Oracle与Active Sensor Networks（CO-ASnet）设计的去中心化监管方案，为token发行提供风险评估和预警措施。这一研究对区块链金融产品发展和治理具有参考价值，提示监管者和企业应探索其边界。 <div>
arXiv:2310.05070v2 Announce Type: replace 
Abstract: The influence of opinion leaders impacts different aspects of social finance. How to analyse the utility of opinion leaders' influence in realizing assets on the blockchain and adopt a compliant regulatory scheme is worth exploring and pondering. Taking Musk's call on social media to buy Dogecoin as an example, this paper uses an event study to empirically investigate the phenomenon in which opinion leaders use ICOs (initial coin offerings) to exert influence. The results show that opinion leaders can use ICOs to influence the price of token assets with money and data traffic in their social network. They can obtain excess returns and reduce the cost of realization so that the closed loop of influence realization will be accelerated. Based on this phenomenon and the results of its impact, we use the ChainLink Oracle with Active Sensor Networks(CO-ASnet) to design a safe and applicable decentralized regulatory scheme that can constructively provide risk assessment strategies and early warning measures for token issuance. The influence realization of opinion leaders in blockchain issuance is bound to receive widespread attention, and this paper will provide an exemplary reference for regulators and enterprises to explore the boundaries of blockchain financial product development and governance.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>A Two-Layer Blockchain Sharding Protocol Leveraging Safety and Liveness for Enhanced Performance</title>
<link>https://arxiv.org/abs/2310.11373</link>
<guid>https://arxiv.org/abs/2310.11373</guid>
<content:encoded><![CDATA[
<div> 关键词：Reticulum, sharding, blockchain, scalability, adversarial attacks.

总结:<br />Reticulum是一种创新的区块链分片协议，旨在提高交易吞吐量并抵御各种网络攻击。它采用两阶段架构，包括控制和处理分片。处理分片至少包含一个可信节点，而控制分片由多数可信节点组成。首先，交易在处理分片中进行投票，共识通过后确认。若未达成一致，控制分片介入，决定并解决争议。实验表明，Reticulum在高吞吐量和抗攻击性方面超越现有协议，为区块链网络提供强大且安全的扩展能力。 <div>
arXiv:2310.11373v4 Announce Type: replace 
Abstract: Sharding is essential for improving blockchain scalability. Existing protocols overlook diverse adversarial attacks, limiting transaction throughput. This paper presents Reticulum, a groundbreaking sharding protocol addressing this issue, boosting blockchain scalability.
  Reticulum employs a two-phase approach, adapting transaction throughput based on runtime adversarial attacks. It comprises "control" and "process" shards in two layers. Process shards contain at least one trustworthy node, while control shards have a majority of trusted nodes. In the first phase, transactions are written to blocks and voted on by nodes in process shards. Unanimously accepted blocks are confirmed. In the second phase, blocks without unanimous acceptance are voted on by control shards. Blocks are accepted if the majority votes in favor, eliminating first-phase opponents and silent voters. Reticulum uses unanimous voting in the first phase, involving fewer nodes, enabling more parallel process shards. Control shards finalize decisions and resolve disputes.
  Experiments confirm Reticulum's innovative design, providing high transaction throughput and robustness against various network attacks, outperforming existing sharding protocols for blockchain networks.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>History Trees and Their Applications</title>
<link>https://arxiv.org/abs/2404.02673</link>
<guid>https://arxiv.org/abs/2404.02673</guid>
<content:encoded><![CDATA[
<div> 关键词：history trees, distributed communication networks, anonymous agents, dynamic networks, open problems

总结:<br />这篇论文介绍了历史树在分布式通信网络研究中的重要性。历史树作为一种结构，有助于理解匿名节点在接收到不同邻居信息后如何变得可区分。它在分析既匿名又动态变化的网络中，为设计最优确定算法提供了框架。文章还回顾了历史树的最新应用进展，并拓展了理论边界，提出了几个待解决的问题。通过比较传统结构和历史树，作者为读者提供了一个易于理解的历史树入门指南。 <div>
arXiv:2404.02673v3 Announce Type: replace 
Abstract: In the theoretical study of distributed communication networks, "history trees" are a discrete structure that naturally models the concept that anonymous agents become distinguishable upon receiving different sets of messages from neighboring agents. By conveniently organizing temporal information in a systematic manner, history trees have been instrumental in the development of optimal deterministic algorithms for networks that are both anonymous and dynamically evolving.
  This note provides an accessible introduction to history trees, drawing comparisons with more traditional structures found in existing literature and reviewing the latest advancements in the applications of history trees, especially within dynamic networks. Furthermore, it expands the theoretical framework of history trees in new directions, also highlighting several open problems for further investigation.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches</title>
<link>https://arxiv.org/abs/2404.02817</link>
<guid>https://arxiv.org/abs/2404.02817</guid>
<content:encoded><![CDATA[
<div> 关键词：Task and Motion Planning (TAMP), Optimization-based TAMP, Hybrid optimization, Locomotion, Manipulation.

总结:
本文是一篇关于优化型任务和运动规划（Optimization-based Task and Motion Planning，简称TAMP）的全面综述。TAMP旨在整合高阶任务规划与低阶运动规划，使机器人具备处理复杂、动态任务的能力，尤其适合解决涉及物理交互的高难度行走和操纵问题。文章重点讨论了TAMP的三个方面：(1) 规模表示，如动作描述语言和时序逻辑；(2) 解决策略，包括人工智能规划和轨迹优化；(3) 逻辑任务规划与模型基运动优化之间的动态互动。作者强调了高效的算法结构，如分级和分布式方法，并探讨了传统方法与基于学习的技术（如大型语言模型）的融合。最后，文章展望了TAMP的未来研究方向，包括算法改进和应用挑战。 <div>
arXiv:2404.02817v4 Announce Type: replace 
Abstract: Task and Motion Planning (TAMP) integrates high-level task planning and low-level motion planning to equip robots with the autonomy to effectively reason over long-horizon, dynamic tasks. Optimization-based TAMP focuses on hybrid optimization approaches that define goal conditions via objective functions and are capable of handling open-ended goals, robotic dynamics, and physical interaction between the robot and the environment. Therefore, optimization-based TAMP is particularly suited to solve highly complex, contact-rich locomotion and manipulation problems. This survey provides a comprehensive review on optimization-based TAMP, covering (i) planning domain representations, including action description languages and temporal logic, (ii) individual solution strategies for components of TAMP, including AI planning and trajectory optimization (TO), and (iii) the dynamic interplay between logic-based task planning and model-based TO. A particular focus of this survey is to highlight the algorithm structures to efficiently solve TAMP, especially hierarchical and distributed approaches. Additionally, the survey emphasizes the synergy between the classical methods and contemporary learning-based innovations such as large language models. Furthermore, the future research directions for TAMP is discussed in this survey, highlighting both algorithmic and application-specific challenges.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Static Application Security Testing (SAST) Tools for Smart Contracts: How Far Are We?</title>
<link>https://arxiv.org/abs/2404.18186</link>
<guid>https://arxiv.org/abs/2404.18186</guid>
<content:encoded><![CDATA[
<div> 关键词：smart contracts, static application security testing (SAST), vulnerability types, benchmark, evaluation.

总结:<br />
本文关注智能合约的安全性，随着攻击事件增多，静态应用安全测试(SAST)工具发展迅速。研究者提出了一种最新的、精细划分的45种漏洞类型分类，构建了一个涵盖40种类型的广泛基准，包括不同代码特性和应用场景。对8款SAST工具进行评估，结果显示这些工具漏检了约50%的基准漏洞，误报率高，精确度不足10%。结合工具可以降低漏检率，但会增加更多误报。许多非访问控制和重入性漏洞未被检测到。该研究为工具开发、改进、评估和选择提供了指导。 <div>
arXiv:2404.18186v3 Announce Type: replace 
Abstract: In recent years, the importance of smart contract security has been heightened by the increasing number of attacks against them. To address this issue, a multitude of static application security testing (SAST) tools have been proposed for detecting vulnerabilities in smart contracts. However, objectively comparing these tools to determine their effectiveness remains challenging. Existing studies often fall short due to the taxonomies and benchmarks only covering a coarse and potentially outdated set of vulnerability types, which leads to evaluations that are not entirely comprehensive and may display bias.
  In this paper, we fill this gap by proposing an up-to-date and fine-grained taxonomy that includes 45 unique vulnerability types for smart contracts. Taking it as a baseline, we develop an extensive benchmark that covers 40 distinct types and includes a diverse range of code characteristics, vulnerability patterns, and application scenarios. Based on them, we evaluated 8 SAST tools using this benchmark, which comprises 788 smart contract files and 10,394 vulnerabilities. Our results reveal that the existing SAST tools fail to detect around 50% of vulnerabilities in our benchmark and suffer from high false positives, with precision not surpassing 10%. We also discover that by combining the results of multiple tools, the false negative rate can be reduced effectively, at the expense of flagging 36.77 percentage points more functions. Nevertheless, many vulnerabilities, especially those beyond Access Control and Reentrancy vulnerabilities, remain undetected. We finally highlight the valuable insights from our study, hoping to provide guidance on tool development, enhancement, evaluation, and selection for developers, researchers, and practitioners.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>AB-Training: A Communication-Efficient Approach for Distributed Low-Rank Learning</title>
<link>https://arxiv.org/abs/2405.01067</link>
<guid>https://arxiv.org/abs/2405.01067</guid>
<content:encoded><![CDATA[
<div> 关键词：AB-training, communication bottlenecks, distributed neural network training, low-rank representations, independent training groups

总结: AB-training是一种新型的分布式神经网络训练方法，旨在解决通信瓶颈问题。它利用低秩表示和独立训练组显著减少通信开销，平均降低网络流量约70.31%。这种方法不仅加速大规模系统中的收敛，还在小规模下显示正则化效果，提高泛化能力同时保持或缩短训练时间。实验中，AB-training在VGG16和ResNet-50模型上表现出色，分别实现了44.14:1的压缩比和1.55%的性能提升。然而，文章指出大型批量效应在低秩设置中仍然存在，提示对大规模分布式训练优化更新机制的研究仍有待深入。 <div>
arXiv:2405.01067v2 Announce Type: replace 
Abstract: Communication bottlenecks severely hinder the scalability of distributed neural network training, particularly in high-performance computing (HPC) environments. We introduce AB-training, a novel data-parallel method that leverages low-rank representations and independent training groups to significantly reduce communication overhead. Our experiments demonstrate an average reduction in network traffic of approximately 70.31\% across various scaling scenarios, increasing the training potential of communication-constrained systems and accelerating convergence at scale. AB-training also exhibits a pronounced regularization effect at smaller scales, leading to improved generalization while maintaining or even reducing training time. We achieve a remarkable 44.14 : 1 compression ratio on VGG16 trained on CIFAR-10 with minimal accuracy loss, and outperform traditional data parallel training by 1.55\% on ResNet-50 trained on ImageNet-2012. While AB-training is promising, our findings also reveal that large batch effects persist even in low-rank regimes, underscoring the need for further research into optimized update mechanisms for massively distributed training.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Facilitating Feature and Topology Lightweighting: An Ethereum Transaction Graph Compression Method for Malicious Account Detection</title>
<link>https://arxiv.org/abs/2405.08278</link>
<guid>https://arxiv.org/abs/2405.08278</guid>
<content:encoded><![CDATA[
<div> 关键词：Ethereum、malicious accounts、transaction graph、feature engineering、TGC4Eth。

总结:<br />Ethereum作为全球主要的加密货币平台，其交易图谱庞大，恶意账户活动频繁。本文提出TGC4Eth方法，旨在通过特征选择和图结构压缩提升恶意账户检测的效率与鲁棒性。首先，TGC4Eth筛选交易特征，降低对重要性较低特征的依赖，对抗特征逃逸攻击；其次，通过聚焦和粗化过程压缩图谱结构，提高数据处理和模型推理效率。实验结果表明，TGC4Eth显著提高了现有检测模型的计算效率，保持了交易图的连通性，并且在面对特征逃逸攻击时表现出高稳定性。 <div>
arXiv:2405.08278v2 Announce Type: replace 
Abstract: Ethereum has become one of the primary global platforms for cryptocurrency, playing an important role in promoting the diversification of the financial ecosystem. However, the relative lag in regulation has led to a proliferation of malicious activities in Ethereum, posing a serious threat to fund security. Existing regulatory methods usually detect malicious accounts through feature engineering or large-scale transaction graph mining. However, due to the immense scale of transaction data and malicious attacks, these methods suffer from inefficiency and low robustness during data processing and anomaly detection. In this regard, we propose an Ethereum Transaction Graph Compression method named TGC4Eth, which assists malicious account detection by lightweighting both features and topology of the transaction graph. At the feature level, we select transaction features based on their low importance to improve the robustness of the subsequent detection models against feature evasion attacks; at the topology level, we employ focusing and coarsening processes to compress the structure of the transaction graph, thereby improving both data processing and inference efficiency of detection models. Extensive experiments demonstrate that TGC4Eth significantly improves the computational efficiency of existing detection models while preserving the connectivity of the transaction graph. Furthermore, TGC4Eth enables existing detection models to maintain stable performance and exhibit high robustness against feature evasion attacks.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Distributed Adaptive Control of Disturbed Interconnected Systems with High-Order Tuners</title>
<link>https://arxiv.org/abs/2405.15178</link>
<guid>https://arxiv.org/abs/2405.15178</guid>
<content:encoded><![CDATA[
<div> 关键词：network synchronization, distributed adaptive control, interconnected agents, consensus, communication limitations

总结:<br />
本文探讨了在通信受限的情况下，如何实现异质性子系统（包括领导者和跟随者）之间的网络同步，以达成共识。研究关注的是未知线性子系统的分布式自适应控制，存在输入-输出扰动。关键创新在于增强多 agent 系统内的通信，通过根据与领导者距离调整测量值，实现跟随。文章分析了不同类型的平衡网络（星形、循环、路径和随机）中第一至高阶调节器的效果，考虑了时间变系数。数值模拟展示了网络稀疏性对性能的影响，并指出增加节点数量并不总是减小误差。最后，经验证明，提出的改进高阶调节器优于其他方法，提供了深入的理论洞察和结论。 <div>
arXiv:2405.15178v2 Announce Type: replace 
Abstract: This paper addresses the challenge of network synchronization under limited communication, involving heterogeneous agents with different dynamics and various network topologies, to achieve consensus. We investigate the distributed adaptive control for interconnected unknown linear subsystems with a leader and followers, in the presence of input-output disturbance. We enhance the communication within multi-agent systems to achieve consensus under the leadership's guidance. While the measured variable is similar among the followers, the incoming measurements are weighted and constructed based on their proximity to the leader. We also explore the convergence rates across various balanced topologies (Star-like, Cyclic-like, Path, Random), featuring different numbers of agents, using three distributed algorithms, ranging from first- to high-order tuners to effectively address time-varying regressors. The mathematical foundation is rigorously presented from the network designs of the unknown agents following a leader, to the distributed methods. Moreover, we conduct several numerical simulations across various networks, agents and tuners to evaluate the effects of sparsity in the interaction between subsystems using the $L_2-$norm and $L_\infty-$norm. Some networks exhibit a trend where an increasing number of agents results in smaller errors, although this is not universally the case. Additionally, patterns observed at initial times may not reliably predict overall performance across different networks. Finally, we demonstrate that the proposed modified high-order tuner outperforms its counterparts, and we provide related insights along with our conclusions.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Ents: An Efficient Three-party Training Framework for Decision Trees by Communication Optimization</title>
<link>https://arxiv.org/abs/2406.07948</link>
<guid>https://arxiv.org/abs/2406.07948</guid>
<content:encoded><![CDATA[
<div> 关键词：Secure Multi-party Computation, Decision Trees, Communication Optimization, Radix Sort Protocols, Training Framework

总结:<br />
本文介绍了一种名为 Ents 的高效三 party 训练框架，专注于优化决策树的多 party 训练过程中的通信效率。首先， Ents 提出基于安全排序协议的解决方案，有效处理连续属性数据集的分割，减少通信开销。其次，通过设计高效的份额转换协议，减少在大型环上进行大部分计算导致的通信负担。实验结果表明， Ents 相比现有框架在通信大小和轮数上分别提高了 5.5 倍到 9.3 倍和 3.9 倍到 5.3 倍，训练时间也缩短了 3.5 倍到 6.7 倍。在实际应用中， Ents 能在仅需三个小时的情况下，通过 WAN 环境对具有 245,000 多个样本的真实世界皮肤分割数据集进行安全训练，显示其实践可行性。 <div>
arXiv:2406.07948v3 Announce Type: replace 
Abstract: Multi-party training frameworks for decision trees based on secure multi-party computation enable multiple parties to train high-performance models on distributed private data with privacy preservation. The training process essentially involves frequent dataset splitting according to the splitting criterion (e.g. Gini impurity). However, existing multi-party training frameworks for decision trees demonstrate communication inefficiency due to the following issues: (1) They suffer from huge communication overhead in securely splitting a dataset with continuous attributes. (2) They suffer from huge communication overhead due to performing almost all the computations on a large ring to accommodate the secure computations for the splitting criterion.
  In this paper, we are motivated to present an efficient three-party training framework, namely Ents, for decision trees by communication optimization. For the first issue, we present a series of training protocols based on the secure radix sort protocols to efficiently and securely split a dataset with continuous attributes. For the second issue, we propose an efficient share conversion protocol to convert shares between a small ring and a large ring to reduce the communication overhead incurred by performing almost all the computations on a large ring. Experimental results from eight widely used datasets show that Ents outperforms state-of-the-art frameworks by $5.5\times \sim 9.3\times$ in communication sizes and $3.9\times \sim 5.3\times$ in communication rounds. In terms of training time, Ents yields an improvement of $3.5\times \sim 6.7\times$. To demonstrate its practicality, Ents requires less than three hours to securely train a decision tree on a widely used real-world dataset (Skin Segmentation) with more than 245,000 samples in the WAN setting.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>TemPrompt: Multi-Task Prompt Learning for Temporal Relation Extraction in RAG-based Crowdsourcing Systems</title>
<link>https://arxiv.org/abs/2406.14825</link>
<guid>https://arxiv.org/abs/2406.14825</guid>
<content:encoded><![CDATA[
<div> 关键词：Temporal Relation Extraction, Temporal Prompt Learning, Pre-trained Language Models, Contrastive Learning, Crowdsourcing Scenarios.

总结: 本文介绍了一种名为TemPrompt的多任务提示学习框架，用于解决时间关系抽取（TRE）任务中的数据限制和分布不均问题。该方法利用预训练语言模型（PLMs）的全球知识，通过任务导向的自动提示生成，结合prompt tuning和对比学习来提升性能。文章还提出利用时间事件推理辅助模型关注事件和时间线索。实验结果显示，TemPrompt在标准和少量样本条件下，相较于其他方法在大多数指标上表现更优，并通过案例研究验证了其在众包场景中的有效性。 <div>
arXiv:2406.14825v3 Announce Type: replace 
Abstract: Temporal relation extraction (TRE) aims to grasp the evolution of events or actions, and thus shape the workflow of associated tasks, so it holds promise in helping understand task requests initiated by requesters in crowdsourcing systems. However, existing methods still struggle with limited and unevenly distributed annotated data. Therefore, inspired by the abundant global knowledge stored within pre-trained language models (PLMs), we propose a multi-task prompt learning framework for TRE (TemPrompt), incorporating prompt tuning and contrastive learning to tackle these issues. To elicit more effective prompts for PLMs, we introduce a task-oriented prompt construction approach that thoroughly takes the myriad factors of TRE into consideration for automatic prompt generation. In addition, we present temporal event reasoning as a supplement to bolster the model's focus on events and temporal cues. The experimental results demonstrate that TemPrompt outperforms all compared baselines across the majority of metrics under both standard and few-shot settings. A case study is provided to validate its effectiveness in crowdsourcing scenarios.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>ammBoost: State Growth Control for AMMs</title>
<link>https://arxiv.org/abs/2406.17094</link>
<guid>https://arxiv.org/abs/2406.17094</guid>
<content:encoded><![CDATA[
<div> 关键词：Automated Market Makers (AMMs), Decentralized Finance (DeFi), Scalability issues, Sidechain architecture, ammBoost.

总结:<br />
本文探讨了去中心化金融(DeFi)应用中的自动市场 maker (AMM)面临的可扩展性问题，特别是高昂的链上存储开销。为解决这一问题，作者提出了一种新的侧链架构方案——ammBoost。ammBoost通过减少链上交易、提升吞吐量并支持区块链修剪，有效降低了交易成本和链增长。实验表明，ammBoost能将gas成本降低94.53%，链增长减少至少80%，并能处理比Uniswap实际流量高500倍的交易量。这一创新为AMM的高效运行提供了有力支持。 <div>
arXiv:2406.17094v2 Announce Type: replace 
Abstract: Automated market makers (AMMs) are a form of decentralized cryptocurrency exchanges and considered a prime example of Decentralized Finance (DeFi) applications. Their popularity and high trading activity have resulted in millions of on-chain transactions leading to serious scalability issues. In this paper, we address the on-chain storage overhead problem of AMMs by utilizing a new sidechain architecture as a layer 2 solution, building a system called ammBoost. Our system reduces the amount of on-chain transactions, boosts throughput, and supports blockchain pruning. We devise several techniques to enable layer 2 processing for AMMs while preserving correctness and security of the underlying AMM. We also build a proof-of-concept of ammBoost for a Uniswap-inspired use case to empirically evaluate its performance. Our experiments show that ammBoost decreases the gas cost by 94.53% and the chain growth by at least 80%, and that it can support up to 500x of the daily traffic volume observed for Uniswap in practice.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Protecting the 'Stop Using My Data' Right through Blockchain-assisted Evidence Generation</title>
<link>https://arxiv.org/abs/2406.17694</link>
<guid>https://arxiv.org/abs/2406.17694</guid>
<content:encoded><![CDATA[
<div> 关键词：个人数据、停止使用、证据生成、区块链、权利侵犯。

总结:<br />本文探讨了互联网平台如何在保护用户隐私的同时提供个性化服务。文章关注的核心问题是“停止使用我的数据”这一数据权利，它属于“被遗忘权”的重要方面。作者提出了一种创新的证据生成框架，利用区块链技术设计并实现了一个系统，以防止在数据收购后对这项权利的侵犯。该系统采用两阶段证据生成协议，其有效性由新提出的引理保证。通过在两个真实世界推荐系统数据集上的实验验证，证明了该方法的成功率超过99%。这个研究为数据保护提供了一种新的解决方案，确保用户的‘停止使用我的数据’请求得到尊重。 <div>
arXiv:2406.17694v2 Announce Type: replace 
Abstract: In order to provide personalized services to users, Internet-based platforms collect and utilize user-generated behavioral data. Although the 'stop using my data' right should be a fundamental data right, which allows individuals to request their personal data to be no longer utilized by online platforms, the existing preventive data protection measures (e.g., cryptographic data elimination, differential privacy) are unfortunately not applicable. This work aims to develop the first Evidence Generation Framework for deterring post-acquisition data right violations. We formulated the 'stop using my data' problem, which captures a vantage facet of the multi-faceted notion of 'right to be forgotten'. We designed and implemented the first blockchain-assisted system to generate evidence for deterring the violations of the 'stop using my data' right. Our system employs a novel two-stage evidence generation protocol whose efficacy is ensured by a newly proposed Lemma. To validate our framework, we conducted a case study on recommendation systems with systematic evaluation experiments using two real-world datasets: the measured success rate exceeds 99%.
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>Federated Graph Semantic and Structural Learning</title>
<link>https://arxiv.org/abs/2406.18937</link>
<guid>https://arxiv.org/abs/2406.18937</guid>
<content:encoded><![CDATA[
<div> 关键词：Federated Graph Learning, Graph Neural Networks, Node-Level Semantics, Graph-Level Structure, Distillation

总结:<br />
该论文关注联邦图学习中的挑战，主要集中在非独立同分布问题上，目标是协作训练全球图神经网络。首先，论文发现节点级语义对本地模型性能至关重要，通过对比不同类别的节点进行正则化，提升区分度。其次，论文提出结构信息对于邻居节点具有相似性，但直接对齐可能因类别不一致而阻碍区分。为解决这个问题，作者将邻接关系转化为相似度分布，并利用全局模型向本地模型传授关系知识，保持局部模型的结构信息和识别能力。实验结果在三个图数据集上验证了该方法的有效性，优于现有方法。 <div>
arXiv:2406.18937v2 Announce Type: replace 
Abstract: Federated graph learning collaboratively learns a global graph neural network with distributed graphs, where the non-independent and identically distributed property is one of the major challenges. Most relative arts focus on traditional distributed tasks like images and voices, incapable of graph structures. This paper firstly reveals that local client distortion is brought by both node-level semantics and graph-level structure. First, for node-level semantics, we find that contrasting nodes from distinct classes is beneficial to provide a well-performing discrimination. We pull the local node towards the global node of the same class and push it away from the global node of different classes. Second, we postulate that a well-structural graph neural network possesses similarity for neighbors due to the inherent adjacency relationships. However, aligning each node with adjacent nodes hinders discrimination due to the potential class inconsistency. We transform the adjacency relationships into the similarity distribution and leverage the global model to distill the relation knowledge into the local model, which preserves the structural information and discriminability of the local model. Empirical results on three graph datasets manifest the superiority of the proposed method over its counterparts.
]]></content:encoded>
<pubDate></pubDate>
</item>
</channel>
</rss>