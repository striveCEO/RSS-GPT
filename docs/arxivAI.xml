<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs</link>

<item>
<title>SAMEP: A Secure Protocol for Persistent Context Sharing Across AI Agents</title>
<link>https://arxiv.org/abs/2507.10562</link>
<guid>https://arxiv.org/abs/2507.10562</guid>
<content:encoded><![CDATA[

arXiv:2507.10562v1 Announce Type: new 
Abstract: Current AI agent architectures suffer from ephemeral memory limitations, preventing effective collaboration and knowledge sharing across sessions and agent boundaries. We introduce SAMEP (Secure Agent Memory Exchange Protocol), a novel framework that enables persistent, secure, and semantically searchable memory sharing among AI agents. Our protocol addresses three critical challenges: (1) persistent context preservation across agent sessions, (2) secure multi-agent collaboration with fine-grained access control, and (3) efficient semantic discovery of relevant historical context. SAMEP implements a distributed memory repository with vector-based semantic search, cryptographic access controls (AES-256-GCM), and standardized APIs compatible with existing agent communication protocols (MCP, A2A). We demonstrate SAMEP's effectiveness across diverse domains including multi-agent software development, healthcare AI with HIPAA compliance, and multi-modal processing pipelines. Experimental results show 73% reduction in redundant computations, 89% improvement in context relevance scores, and complete compliance with regulatory requirements including audit trail generation. SAMEP enables a new paradigm of persistent, collaborative AI agent ecosystems while maintaining security and privacy guarantees.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Mother Tongue: Self-Emergent Communication in MARL via Endogenous Symbol Systems</title>
<link>https://arxiv.org/abs/2507.10566</link>
<guid>https://arxiv.org/abs/2507.10566</guid>
<content:encoded><![CDATA[

arXiv:2507.10566v1 Announce Type: new 
Abstract: In Decentralized Multi-Agent Reinforcement Learning (MARL), the development of Emergent Communication has long been constrained by the ``Joint Exploration Dilemma'', leading agents to fall into a ``Communication Vacuum Equilibrium'' . Traditional methods address this by introducing inductive biases to facilitate communication emergence . This study fundamentally questions whether such artificial inductive biases are, in fact, over-engineering. Through experiments with the ``AI Mother Tongue'' (AIM) framework, based on a Vector Quantized Variational Autoencoder (VQ-VAE), we demonstrate that when agents possess an endogenous symbol system, their neural representations naturally exhibit spontaneous semantic compression and Nash equilibrium-driven semantic convergence, achieving effective symbolic communication without external inductive biases. This aligns with recent neuroscience findings suggesting that the human brain does not directly use human language for internal thought , and resonates with research on ``soft thinking'' capabilities in Large Language Models (LLMs) . Compared to traditional explicit communication methods, AIM demonstrates stronger generality and efficiency. The interpretable analysis toolkit developed in this study confirms that symbol usage exhibits a significant power-law distribution, leading to three major theoretical insights: the ``Neural Communication Hypothesis'', the ``Tool-First Principle'', and the ``Semantic Interpretability Paradigm''. Future research will explore the integration of Hierarchical Quantized Variational Autoencoders (HQ-VAE) to enhance AIM's complex expressive capabilities and investigate the potential for ``Reinforcement Learning (RL) Low-Level Pre-training''. This discovery offers new avenues for bridging symbolism and connectionism.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning</title>
<link>https://arxiv.org/abs/2507.10571</link>
<guid>https://arxiv.org/abs/2507.10571</guid>
<content:encoded><![CDATA[

arXiv:2507.10571v1 Announce Type: new 
Abstract: Modern Artificial Intelligence (AI) increasingly relies on multi-agent architectures that blend visual and language understanding. Yet, a pressing challenge remains: How can we trust these agents especially in zero-shot settings with no fine-tuning? We introduce a novel modular Agentic AI visual classification framework that integrates generalist multimodal agents with a non-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG) module. Applied to apple leaf disease diagnosis, we benchmark three configurations: (I) zero-shot with confidence-based orchestration, (II) fine-tuned agents with improved performance, and (III) trust-calibrated orchestration enhanced by CLIP-based image retrieval and re-evaluation loops. Using confidence calibration metrics (ECE, OCR, CCC), the orchestrator modulates trust across agents. Our results demonstrate a 77.94\% accuracy improvement in the zero-shot setting using trust-aware orchestration and RAG, achieving 85.63\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL displayed overconfidence. Furthermore, image-RAG grounded predictions with visually similar cases, enabling correction of agent overconfidence via iterative re-evaluation. The proposed system separates perception (vision agents) from meta-reasoning (orchestrator), enabling scalable and interpretable multi-agent AI. This blueprint is extensible to diagnostics, biology, and other trust-critical domains. All models, prompts, results, and system components including the complete software source code are openly released to support reproducibility, transparency, and community benchmarking at Github: https://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions</title>
<link>https://arxiv.org/abs/2507.10577</link>
<guid>https://arxiv.org/abs/2507.10577</guid>
<content:encoded><![CDATA[

arXiv:2507.10577v1 Announce Type: new 
Abstract: Misinformation poses a significant threat in today's digital world, often spreading rapidly through platforms like YouTube. This paper introduces a novel approach to combating misinformation by developing an AI-powered system that not only fact-checks claims made in YouTube videos but also actively engages users in the comment section and challenge misleading narratives. Our system comprises two main agents: Truth Sleuth and Trend Bender.
  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented Generation (RAG) approach - drawing on sources like Wikipedia, Google Search, Google FactCheck - to accurately assess their veracity and generates a nuanced and comprehensive report. Through rigorous prompt engineering, Trend Bender leverages this report along with a curated corpus of relevant articles to generate insightful and persuasive comments designed to stimulate a productive debate. With a carefully set up self-evaluation loop, this agent is able to iteratively improve its style and refine its output.
  We demonstrate the system's capabilities through experiments on established benchmark datasets and a real-world deployment on YouTube, showcasing its potential to engage users and potentially influence perspectives. Our findings highlight the high accuracy of our fact-checking agent, and confirm the potential of AI-driven interventions in combating misinformation and fostering a more informed online space.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARPaCCino: An Agentic-RAG for Policy as Code Compliance</title>
<link>https://arxiv.org/abs/2507.10584</link>
<guid>https://arxiv.org/abs/2507.10584</guid>
<content:encoded><![CDATA[

arXiv:2507.10584v1 Announce Type: new 
Abstract: Policy as Code (PaC) is a paradigm that encodes security and compliance policies into machine-readable formats, enabling automated enforcement in Infrastructure as Code (IaC) environments. However, its adoption is hindered by the complexity of policy languages and the risk of misconfigurations. In this work, we present ARPaCCino, an agentic system that combines Large Language Models (LLMs), Retrieval-Augmented-Generation (RAG), and tool-based validation to automate the generation and verification of PaC rules. Given natural language descriptions of the desired policies, ARPaCCino generates formal Rego rules, assesses IaC compliance, and iteratively refines the IaC configurations to ensure conformance. Thanks to its modular agentic architecture and integration with external tools and knowledge bases, ARPaCCino supports policy validation across a wide range of technologies, including niche or emerging IaC frameworks. Experimental evaluation involving a Terraform-based case study demonstrates ARPaCCino's effectiveness in generating syntactically and semantically correct policies, identifying non-compliant infrastructures, and applying corrective modifications, even when using smaller, open-weight LLMs. Our results highlight the potential of agentic RAG architectures to enhance the automation, reliability, and accessibility of PaC workflows.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergence of Hierarchical Emotion Organization in Large Language Models</title>
<link>https://arxiv.org/abs/2507.10599</link>
<guid>https://arxiv.org/abs/2507.10599</guid>
<content:encoded><![CDATA[

arXiv:2507.10599v1 Announce Type: new 
Abstract: As large language models (LLMs) increasingly power conversational agents, understanding how they model users' emotional states is critical for ethical deployment. Inspired by emotion wheels -- a psychological framework that argues emotions organize hierarchically -- we analyze probabilistic dependencies between emotional states in model outputs. We find that LLMs naturally form hierarchical emotion trees that align with human psychological models, and larger models develop more complex hierarchies. We also uncover systematic biases in emotion recognition across socioeconomic personas, with compounding misclassifications for intersectional, underrepresented groups. Human studies reveal striking parallels, suggesting that LLMs internalize aspects of social perception. Beyond highlighting emergent emotional reasoning in LLMs, our results hint at the potential of using cognitively-grounded theories for developing better model evaluations.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaSM: Layer-wise Scaling Mechanism for Defending Pop-up Attack on GUI Agents</title>
<link>https://arxiv.org/abs/2507.10610</link>
<guid>https://arxiv.org/abs/2507.10610</guid>
<content:encoded><![CDATA[

arXiv:2507.10610v1 Announce Type: new 
Abstract: Graphical user interface (GUI) agents built on multimodal large language models (MLLMs) have recently demonstrated strong decision-making abilities in screen-based interaction tasks. However, they remain highly vulnerable to pop-up-based environmental injection attacks, where malicious visual elements divert model attention and lead to unsafe or incorrect actions. Existing defense methods either require costly retraining or perform poorly under inductive interference. In this work, we systematically study how such attacks alter the attention behavior of GUI agents and uncover a layer-wise attention divergence pattern between correct and incorrect outputs. Based on this insight, we propose \textbf{LaSM}, a \textit{Layer-wise Scaling Mechanism} that selectively amplifies attention and MLP modules in critical layers. LaSM improves the alignment between model saliency and task-relevant regions without additional training. Extensive experiments across 12 types of pop-up perturbations and 4 different model backbones show that LaSM consistently enhances the defense success rate. When combined with prompt-level alerts, LaSM achieves over 98\% robustness even under strong inductive attacks. Our findings reveal that attention misalignment is a core vulnerability in MLLM agents and can be effectively addressed through selective layer-wise modulation.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Reinforcement Learning for Fast and Data-Efficient Spectrum Allocation in Dynamic Wireless Networks</title>
<link>https://arxiv.org/abs/2507.10619</link>
<guid>https://arxiv.org/abs/2507.10619</guid>
<content:encoded><![CDATA[

arXiv:2507.10619v1 Announce Type: new 
Abstract: The dynamic allocation of spectrum in 5G / 6G networks is critical to efficient resource utilization. However, applying traditional deep reinforcement learning (DRL) is often infeasible due to its immense sample complexity and the safety risks associated with unguided exploration, which can cause severe network interference. To address these challenges, we propose a meta-learning framework that enables agents to learn a robust initial policy and rapidly adapt to new wireless scenarios with minimal data. We implement three meta-learning architectures, model-agnostic meta-learning (MAML), recurrent neural network (RNN), and an attention-enhanced RNN, and evaluate them against a non-meta-learning DRL algorithm, proximal policy optimization (PPO) baseline, in a simulated dynamic integrated access/backhaul (IAB) environment. Our results show a clear performance gap. The attention-based meta-learning agent reaches a peak mean network throughput of 48 Mbps, while the PPO baseline decreased drastically to 10 Mbps. Furthermore, our method reduces SINR and latency violations by more than 50% compared to PPO. It also shows quick adaptation, with a fairness index 0.7, showing better resource allocation. This work proves that meta-learning is a very effective and safer option for intelligent control in complex wireless systems.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Game Theory Meets LLM and Agentic AI: Reimagining Cybersecurity for the Age of Intelligent Threats</title>
<link>https://arxiv.org/abs/2507.10621</link>
<guid>https://arxiv.org/abs/2507.10621</guid>
<content:encoded><![CDATA[

arXiv:2507.10621v1 Announce Type: new 
Abstract: Protecting cyberspace requires not only advanced tools but also a shift in how we reason about threats, trust, and autonomy. Traditional cybersecurity methods rely on manual responses and brittle heuristics. To build proactive and intelligent defense systems, we need integrated theoretical frameworks and software tools. Game theory provides a rigorous foundation for modeling adversarial behavior, designing strategic defenses, and enabling trust in autonomous systems. Meanwhile, software tools process cyber data, visualize attack surfaces, verify compliance, and suggest mitigations. Yet a disconnect remains between theory and practical implementation.
  The rise of Large Language Models (LLMs) and agentic AI offers a new path to bridge this gap. LLM-powered agents can operationalize abstract strategies into real-world decisions. Conversely, game theory can inform the reasoning and coordination of these agents across complex workflows. LLMs also challenge classical game-theoretic assumptions, such as perfect rationality or static payoffs, prompting new models aligned with cognitive and computational realities. This co-evolution promises richer theoretical foundations and novel solution concepts. Agentic AI also reshapes software design: systems must now be modular, adaptive, and trust-aware from the outset.
  This chapter explores the intersection of game theory, agentic AI, and cybersecurity. We review key game-theoretic frameworks (e.g., static, dynamic, Bayesian, and signaling games) and solution concepts. We then examine how LLM agents can enhance cyber defense and introduce LLM-driven games that embed reasoning into AI agents. Finally, we explore multi-agent workflows and coordination games, outlining how this convergence fosters secure, intelligent, and adaptive cyber systems.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing the Capabilities of Large Language Models for API calls through Knowledge Graphs</title>
<link>https://arxiv.org/abs/2507.10630</link>
<guid>https://arxiv.org/abs/2507.10630</guid>
<content:encoded><![CDATA[

arXiv:2507.10630v1 Announce Type: new 
Abstract: API calls by large language models (LLMs) offer a cutting-edge approach for data analysis. However, their ability to effectively utilize tools via API calls remains underexplored in knowledge-intensive domains like meteorology. This paper introduces KG2data, a system that integrates knowledge graphs, LLMs, ReAct agents, and tool-use technologies to enable intelligent data acquisition and query handling in the meteorological field. Using a virtual API, we evaluate API call accuracy across three metrics: name recognition failure, hallucination failure, and call correctness. KG2data achieves superior performance (1.43%, 0%, 88.57%) compared to RAG2data (16%, 10%, 72.14%) and chat2data (7.14%, 8.57%, 71.43%). KG2data differs from typical LLM-based systems by addressing their limited access to domain-specific knowledge, which hampers performance on complex or terminology-rich queries. By using a knowledge graph as persistent memory, our system enhances content retrieval, complex query handling, domain-specific reasoning, semantic relationship resolution, and heterogeneous data integration. It also mitigates the high cost of fine-tuning LLMs, making the system more adaptable to evolving domain knowledge and API structures. In summary, KG2data provides a novel solution for intelligent, knowledge-based question answering and data analysis in domains with high knowledge demands.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents</title>
<link>https://arxiv.org/abs/2507.10644</link>
<guid>https://arxiv.org/abs/2507.10644</guid>
<content:encoded><![CDATA[

arXiv:2507.10644v1 Announce Type: new 
Abstract: The concept of the Web of Agents (WoA), which transforms the static, document-centric Web into an environment of autonomous agents acting on users' behalf, has attracted growing interest as large language models (LLMs) become more capable. However, research in this area is still fragmented across different communities. Contemporary surveys catalog the latest LLM-powered frameworks, while the rich histories of Multi-Agent Systems (MAS) and the Semantic Web are often treated as separate, legacy domains. This fragmentation obscures the intellectual lineage of modern systems and hinders a holistic understanding of the field's trajectory. We present the first comprehensive evolutionary overview of the WoA. We show that modern protocols like A2A and the MCP, are direct evolutionary responses to the well-documented limitations of earlier standards like FIPA standards and OWL-based semantic agents. To systematize this analysis, we introduce a four-axis taxonomy (semantic foundation, communication paradigm, locus of intelligence, discovery mechanism). This framework provides a unified analytical lens for comparing agent architectures across all generations, revealing a clear line of descent where others have seen a disconnect. Our analysis identifies a paradigm shift in the 'locus of intelligence': from being encoded in external data (Semantic Web) or the platform (MAS) to being embedded within the agent's core model (LLM). This shift is foundational to modern Agentic AI, enabling the scalable and adaptive systems the WoA has long envisioned. We conclude that while new protocols are essential, they are insufficient for building a robust, open, trustworthy ecosystem. Finally, we argue that the next research frontier lies in solving persistent socio-technical challenges, and we map out a new agenda focused on decentralized identity, economic models, security, and governance for the emerging WoA.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision Language Action Models in Robotic Manipulation: A Systematic Review</title>
<link>https://arxiv.org/abs/2507.10672</link>
<guid>https://arxiv.org/abs/2507.10672</guid>
<content:encoded><![CDATA[

arXiv:2507.10672v1 Announce Type: new 
Abstract: Vision Language Action (VLA) models represent a transformative shift in robotics, with the aim of unifying visual perception, natural language understanding, and embodied control within a single learning framework. This review presents a comprehensive and forward-looking synthesis of the VLA paradigm, with a particular emphasis on robotic manipulation and instruction-driven autonomy. We comprehensively analyze 102 VLA models, 26 foundational datasets, and 12 simulation platforms that collectively shape the development and evaluation of VLAs models. These models are categorized into key architectural paradigms, each reflecting distinct strategies for integrating vision, language, and control in robotic systems. Foundational datasets are evaluated using a novel criterion based on task complexity, variety of modalities, and dataset scale, allowing a comparative analysis of their suitability for generalist policy learning. We introduce a two-dimensional characterization framework that organizes these datasets based on semantic richness and multimodal alignment, showing underexplored regions in the current data landscape. Simulation environments are evaluated for their effectiveness in generating large-scale data, as well as their ability to facilitate transfer from simulation to real-world settings and the variety of supported tasks. Using both academic and industrial contributions, we recognize ongoing challenges and outline strategic directions such as scalable pretraining protocols, modular architectural design, and robust multimodal alignment strategies. This review serves as both a technical reference and a conceptual roadmap for advancing embodiment and robotic control, providing insights that span from dataset generation to real world deployment of generalist robotic agents.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring User Security and Privacy Attitudes and Concerns Toward the Use of General-Purpose LLM Chatbots for Mental Health</title>
<link>https://arxiv.org/abs/2507.10695</link>
<guid>https://arxiv.org/abs/2507.10695</guid>
<content:encoded><![CDATA[

arXiv:2507.10695v1 Announce Type: new 
Abstract: Individuals are increasingly relying on large language model (LLM)-enabled conversational agents for emotional support. While prior research has examined privacy and security issues in chatbots specifically designed for mental health purposes, these chatbots are overwhelmingly "rule-based" offerings that do not leverage generative AI. Little empirical research currently measures users' privacy and security concerns, attitudes, and expectations when using general-purpose LLM-enabled chatbots to manage and improve mental health. Through 21 semi-structured interviews with U.S. participants, we identified critical misconceptions and a general lack of risk awareness. Participants conflated the human-like empathy exhibited by LLMs with human-like accountability and mistakenly believed that their interactions with these chatbots were safeguarded by the same regulations (e.g., HIPAA) as disclosures with a licensed therapist. We introduce the concept of "intangible vulnerability," where emotional or psychological disclosures are undervalued compared to more tangible forms of information (e.g., financial or location-based data). To address this, we propose recommendations to safeguard user mental health disclosures with general-purpose LLM-enabled chatbots more effectively.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ground-Compose-Reinforce: Tasking Reinforcement Learning Agents through Formal Language</title>
<link>https://arxiv.org/abs/2507.10741</link>
<guid>https://arxiv.org/abs/2507.10741</guid>
<content:encoded><![CDATA[

arXiv:2507.10741v1 Announce Type: new 
Abstract: Grounding language in complex perception (e.g. pixels) and action is a key challenge when building situated agents that can interact with humans via language. In past works, this is often solved via manual design of the language grounding or by curating massive datasets relating language to elements of the environment. We propose Ground-Compose-Reinforce, a neurosymbolic framework for grounding formal language from data, and eliciting behaviours by directly tasking RL agents through this language. By virtue of data-driven learning, our framework avoids the manual design of domain-specific elements like reward functions or symbol detectors. By virtue of compositional formal language semantics, our framework achieves data-efficient grounding and generalization to arbitrary language compositions. Experiments on an image-based gridworld and a MuJoCo robotics domain show that our approach reliably maps formal language instructions to behaviours with limited data while end-to-end, data-driven approaches fail.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RCG: Safety-Critical Scenario Generation for Robust Autonomous Driving via Real-World Crash Grounding</title>
<link>https://arxiv.org/abs/2507.10749</link>
<guid>https://arxiv.org/abs/2507.10749</guid>
<content:encoded><![CDATA[

arXiv:2507.10749v1 Announce Type: new 
Abstract: Safety-critical scenarios are essential for training and evaluating autonomous driving (AD) systems, yet remain extremely rare in real-world driving datasets. To address this, we propose Real-world Crash Grounding (RCG), a scenario generation framework that integrates crash-informed semantics into adversarial perturbation pipelines. We construct a safety-aware behavior representation through contrastive pre-training on large-scale driving logs, followed by fine-tuning on a small, crash-rich dataset with approximate trajectory annotations extracted from video. This embedding captures semantic structure aligned with real-world accident behaviors and supports selection of adversary trajectories that are both high-risk and behaviorally realistic. We incorporate the resulting selection mechanism into two prior scenario generation pipelines, replacing their handcrafted scoring objectives with an embedding-based criterion. Experimental results show that ego agents trained against these generated scenarios achieve consistently higher downstream success rates, with an average improvement of 9.2% across seven evaluation settings. Qualitative and quantitative analyses further demonstrate that our approach produces more plausible and nuanced adversary behaviors, enabling more effective and realistic stress testing of AD systems. Code and tools will be released publicly.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Warehouse Spatial Question Answering with LLM Agent</title>
<link>https://arxiv.org/abs/2507.10778</link>
<guid>https://arxiv.org/abs/2507.10778</guid>
<content:encoded><![CDATA[

arXiv:2507.10778v1 Announce Type: new 
Abstract: Spatial understanding has been a challenging task for existing Multi-modal Large Language Models~(MLLMs). Previous methods leverage large-scale MLLM finetuning to enhance MLLM's spatial understanding ability. In this paper, we present a data-efficient approach. We propose a LLM agent system with strong and advanced spatial reasoning ability, which can be used to solve the challenging spatial question answering task in complex indoor warehouse scenarios. Our system integrates multiple tools that allow the LLM agent to conduct spatial reasoning and API tools interaction to answer the given complicated spatial question. Extensive evaluations on the 2025 AI City Challenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that our system achieves high accuracy and efficiency in tasks such as object retrieval, counting, and distance estimation. The code is available at: https://github.com/hsiangwei0903/SpatialAgent
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>React to This (RTT): A Nonverbal Turing Test for Embodied AI</title>
<link>https://arxiv.org/abs/2507.10812</link>
<guid>https://arxiv.org/abs/2507.10812</guid>
<content:encoded><![CDATA[

arXiv:2507.10812v1 Announce Type: new 
Abstract: We propose an approach to test embodied AI agents for interaction awareness and believability, particularly in scenarios where humans push them to their limits. Turing introduced the Imitation Game as a way to explore the question: "Can machines think?" The Total Turing Test later expanded this concept beyond purely verbal communication, incorporating perceptual and physical interaction. Building on this, we propose a new guiding question: "Can machines react?" and introduce the React to This (RTT) test for nonverbal behaviors, presenting results from an initial experiment.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Versatile and Generalizable Manipulation via Goal-Conditioned Reinforcement Learning with Grounded Object Detection</title>
<link>https://arxiv.org/abs/2507.10814</link>
<guid>https://arxiv.org/abs/2507.10814</guid>
<content:encoded><![CDATA[

arXiv:2507.10814v1 Announce Type: new 
Abstract: General-purpose robotic manipulation, including reach and grasp, is essential for deployment into households and workspaces involving diverse and evolving tasks. Recent advances propose using large pre-trained models, such as Large Language Models and object detectors, to boost robotic perception in reinforcement learning. These models, trained on large datasets via self-supervised learning, can process text prompts and identify diverse objects in scenes, an invaluable skill in RL where learning object interaction is resource-intensive. This study demonstrates how to integrate such models into Goal-Conditioned Reinforcement Learning to enable general and versatile robotic reach and grasp capabilities. We use a pre-trained object detection model to enable the agent to identify the object from a text prompt and generate a mask for goal conditioning. Mask-based goal conditioning provides object-agnostic cues, improving feature sharing and generalization. The effectiveness of the proposed framework is demonstrated in a simulated reach-and-grasp task, where the mask-based goal conditioning consistently maintains a $\sim$90\% success rate in grasping both in and out-of-distribution objects, while also ensuring faster convergence to higher returns.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Context for Tool Orchestration</title>
<link>https://arxiv.org/abs/2507.10820</link>
<guid>https://arxiv.org/abs/2507.10820</guid>
<content:encoded><![CDATA[

arXiv:2507.10820v1 Announce Type: new 
Abstract: This paper demonstrates that Semantic Context (SC), leveraging descriptive tool information, is a foundational component for robust tool orchestration. Our contributions are threefold. First, we provide a theoretical foundation using contextual bandits, introducing SC-LinUCB and proving it achieves lower regret and adapts favourably in dynamic action spaces. Second, we provide parallel empirical validation with Large Language Models, showing that SC is critical for successful in-context learning in both static (efficient learning) and non-stationary (robust adaptation) settings. Third, we propose the FiReAct pipeline, and demonstrate on a benchmark with over 10,000 tools that SC-based retrieval enables an LLM to effectively orchestrate over a large action space. These findings provide a comprehensive guide to building more sample-efficient, adaptive, and scalable orchestration agents.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Past, Present and Future: Exploring Adaptive AI in Software Development Bots</title>
<link>https://arxiv.org/abs/2507.10822</link>
<guid>https://arxiv.org/abs/2507.10822</guid>
<content:encoded><![CDATA[

arXiv:2507.10822v1 Announce Type: new 
Abstract: Conversational agents, such as chatbots and virtual assistants, have become essential in software development, boosting productivity, collaboration, and automating various tasks. This paper examines the role of adaptive AI-powered conversational agents in software development, highlighting their ability to offer dynamic, context-aware assistance to developers. Unlike traditional rule-based systems, adaptive AI agents use machine learning and natural language processing to learn from interactions and improve over time, providing more personalized and responsive help. We look at how these tools have evolved from simple query-based systems to advanced AI-driven solutions like GitHub Copilot and Microsoft Teams bots. We also explore the challenges of integrating adaptive AI into software development processes. The study aims to assess the benefits and limitations of these systems, address concerns like data privacy and ethical issues, and offer insights into their future use in the field. Ultimately, adaptive AI chatbots have great potential to revolutionize software development by delivering real-time, customized support and enhancing the efficiency of development cycles.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Guided Agentic Object Detection for Open-World Understanding</title>
<link>https://arxiv.org/abs/2507.10844</link>
<guid>https://arxiv.org/abs/2507.10844</guid>
<content:encoded><![CDATA[

arXiv:2507.10844v1 Announce Type: new 
Abstract: Object detection traditionally relies on fixed category sets, requiring costly re-training to handle novel objects. While Open-World and Open-Vocabulary Object Detection (OWOD and OVOD) improve flexibility, OWOD lacks semantic labels for unknowns, and OVOD depends on user prompts, limiting autonomy. We propose an LLM-guided agentic object detection (LAOD) framework that enables fully label-free, zero-shot detection by prompting a Large Language Model (LLM) to generate scene-specific object names. These are passed to an open-vocabulary detector for localization, allowing the system to adapt its goals dynamically. We introduce two new metrics, Class-Agnostic Average Precision (CAAP) and Semantic Naming Average Precision (SNAP), to separately evaluate localization and naming. Experiments on LVIS, COCO, and COCO-OOD validate our approach, showing strong performance in detecting and naming novel objects. Our method offers enhanced autonomy and adaptability for open-world understanding.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization</title>
<link>https://arxiv.org/abs/2507.10894</link>
<guid>https://arxiv.org/abs/2507.10894</guid>
<content:encoded><![CDATA[

arXiv:2507.10894v1 Announce Type: new 
Abstract: Language-guided navigation is a cornerstone of embodied AI, enabling agents to interpret language instructions and navigate complex environments. However, expert-provided instructions are limited in quantity, while synthesized annotations often lack quality, making them insufficient for large-scale research. To address this, we propose NavComposer, a novel framework for automatically generating high-quality navigation instructions. NavComposer explicitly decomposes semantic entities such as actions, scenes, and objects, and recomposes them into natural language instructions. Its modular architecture allows flexible integration of state-of-the-art techniques, while the explicit use of semantic entities enhances both the richness and accuracy of instructions. Moreover, it operates in a data-agnostic manner, supporting adaptation to diverse navigation trajectories without domain-specific training. Complementing NavComposer, we introduce NavInstrCritic, a comprehensive annotation-free evaluation system that assesses navigation instructions on three dimensions: contrastive matching, semantic consistency, and linguistic diversity. NavInstrCritic provides a holistic evaluation of instruction quality, addressing limitations of traditional metrics that rely heavily on expert annotations. By decoupling instruction generation and evaluation from specific navigation agents, our method enables more scalable and generalizable research. Extensive experiments provide direct and practical evidence for the effectiveness of our method.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lessons Learned from Evaluation of LLM based Multi-agents in Safer Therapy Recommendation</title>
<link>https://arxiv.org/abs/2507.10911</link>
<guid>https://arxiv.org/abs/2507.10911</guid>
<content:encoded><![CDATA[

arXiv:2507.10911v1 Announce Type: new 
Abstract: Therapy recommendation for chronic patients with multimorbidity is challenging due to risks of treatment conflicts. Existing decision support systems face scalability limitations. Inspired by the way in which general practitioners (GP) manage multimorbidity patients, occasionally convening multidisciplinary team (MDT) collaboration, this study investigated the feasibility and value of using a Large Language Model (LLM)-based multi-agent system (MAS) for safer therapy recommendations. We designed a single agent and a MAS framework simulating MDT decision-making by enabling discussion among LLM agents to resolve medical conflicts. The systems were evaluated on therapy planning tasks for multimorbidity patients using benchmark cases. We compared MAS performance with single-agent approaches and real-world benchmarks. An important contribution of our study is the definition of evaluation metrics that go beyond the technical precision and recall and allow the inspection of clinical goals met and medication burden of the proposed advices to a gold standard benchmark. Our results show that with current LLMs, a single agent GP performs as well as MDTs. The best-scoring models provide correct recommendations that address all clinical goals, yet the advices are incomplete. Some models also present unnecessary medications, resulting in unnecessary conflicts between medication and conditions or drug-drug interactions.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Learning Framework For Cooperative Collision Avoidance of UAV Swarms Leveraging Domain Knowledge</title>
<link>https://arxiv.org/abs/2507.10913</link>
<guid>https://arxiv.org/abs/2507.10913</guid>
<content:encoded><![CDATA[

arXiv:2507.10913v1 Announce Type: new 
Abstract: This paper presents a multi-agent reinforcement learning (MARL) framework for cooperative collision avoidance of UAV swarms leveraging domain knowledge-driven reward. The reward is derived from knowledge in the domain of image processing, approximating contours on a two-dimensional field. By modeling obstacles as maxima on the field, collisions are inherently avoided as contours never go through peaks or intersect. Additionally, counters are smooth and energy-efficient. Our framework enables training with large swarm sizes as the agent interaction is minimized and the need for complex credit assignment schemes or observation sharing mechanisms in state-of-the-art MARL approaches are eliminated. Moreover, UAVs obtain the ability to adapt to complex environments where contours may be non-viable or non-existent through intensive training. Extensive experiments are conducted to evaluate the performances of our framework against state-of-the-art MARL algorithms.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Journalism-Guided Agentic In-Context Learning for News Stance Detection</title>
<link>https://arxiv.org/abs/2507.11049</link>
<guid>https://arxiv.org/abs/2507.11049</guid>
<content:encoded><![CDATA[

arXiv:2507.11049v1 Announce Type: new 
Abstract: As online news consumption grows, personalized recommendation systems have become integral to digital journalism. However, these systems risk reinforcing filter bubbles and political polarization by failing to incorporate diverse perspectives. Stance detection -- identifying a text's position on a target -- can help mitigate this by enabling viewpoint-aware recommendations and data-driven analyses of media bias. Yet, existing stance detection research remains largely limited to short texts and high-resource languages. To address these gaps, we introduce \textsc{K-News-Stance}, the first Korean dataset for article-level stance detection, comprising 2,000 news articles with article-level and 19,650 segment-level stance annotations across 47 societal issues. We also propose \textsc{JoA-ICL}, a \textbf{Jo}urnalism-guided \textbf{A}gentic \textbf{I}n-\textbf{C}ontext \textbf{L}earning framework that employs a language model agent to predict the stances of key structural segments (e.g., leads, quotes), which are then aggregated to infer the overall article stance. Experiments show that \textsc{JoA-ICL} outperforms existing stance detection methods, highlighting the benefits of segment-level agency in capturing the overall position of long-form news articles. Two case studies further demonstrate its broader utility in promoting viewpoint diversity in news recommendations and uncovering patterns of media bias.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks</title>
<link>https://arxiv.org/abs/2507.11059</link>
<guid>https://arxiv.org/abs/2507.11059</guid>
<content:encoded><![CDATA[

arXiv:2507.11059v1 Announce Type: new 
Abstract: The rapid advancement of Large Language Models (LLMs) in software engineering has revealed critical limitations in existing benchmarks, particularly the widely used SWE-bench dataset. Recent studies have uncovered severe data contamination issues, e.g. SWE-bench reports 32.67% of successful patches involve direct solution leakage and 31.08\% pass due to inadequate test cases. We introduce SWE-MERA, a dynamic, continuously updated benchmark designed to address these fundamental challenges through an automated collection of real-world GitHub issues and rigorous quality validation. Our approach implements a reliable pipeline that ensures quality while minimizing contamination risks, resulting in approximately 10,000 potential tasks with 300 samples currently available. Evaluation using the Aider coding agent demonstrates strong discriminative power in state-of-the-art models. We report performance across a dozen recent LLMs evaluated on tasks collected between September 2024 and June 2025.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tactical Decision for Multi-UGV Confrontation with a Vision-Language Model-Based Commander</title>
<link>https://arxiv.org/abs/2507.11079</link>
<guid>https://arxiv.org/abs/2507.11079</guid>
<content:encoded><![CDATA[

arXiv:2507.11079v1 Announce Type: new 
Abstract: In multiple unmanned ground vehicle confrontations, autonomously evolving multi-agent tactical decisions from situational awareness remain a significant challenge. Traditional handcraft rule-based methods become vulnerable in the complicated and transient battlefield environment, and current reinforcement learning methods mainly focus on action manipulation instead of strategic decisions due to lack of interpretability. Here, we propose a vision-language model-based commander to address the issue of intelligent perception-to-decision reasoning in autonomous confrontations. Our method integrates a vision language model for scene understanding and a lightweight large language model for strategic reasoning, achieving unified perception and decision within a shared semantic space, with strong adaptability and interpretability. Unlike rule-based search and reinforcement learning methods, the combination of the two modules establishes a full-chain process, reflecting the cognitive process of human commanders. Simulation and ablation experiments validate that the proposed approach achieves a win rate of over 80% compared with baseline models.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Agent Architecture for Decentralized Trading of Alternative Assets</title>
<link>https://arxiv.org/abs/2507.11117</link>
<guid>https://arxiv.org/abs/2507.11117</guid>
<content:encoded><![CDATA[

arXiv:2507.11117v1 Announce Type: new 
Abstract: Decentralized trading of real-world alternative assets (e.g., gold) requires bridging physical asset custody with blockchain systems while meeting strict requirements for compliance, liquidity, and risk management. We present GoldMine OS, a research oriented architecture that employs multiple specialized AI agents to automate and secure the tokenization and exchange of physical gold into a blockchain based stablecoin ("OZ"). Our approach combines on chain smart contracts for critical risk controls with off chain AI agents for decision making, blending the transparency and reliability of blockchains with the flexibility of AI driven automation. We describe four cooperative agents (Compliance, Token Issuance, Market Making, and Risk Control) and a coordinating core, and evaluate the system through simulation and a controlled pilot deployment. In experiments the prototype delivers on demand token issuance in under 1.2 s, more than 100 times faster than manual workflows. The Market Making agent maintains tight liquidity with spreads often below 0.5 percent even under volatile conditions. Fault injection tests show resilience: an oracle price spoofing attack is detected and mitigated within 10 s, and a simulated vault mis reporting halts issuance immediately with minimal user impact. The architecture scales to 5000 transactions per second with 10000 concurrent users in benchmarks. These results indicate that an AI agent based decentralized exchange for alternative assets can satisfy rigorous performance and safety requirements. We discuss broader implications for democratizing access to traditionally illiquid assets and explain how our governance model -- multi signature agent updates and on chain community voting on risk parameters -- provides ongoing transparency, adaptability, and formal assurance of system integrity.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantized Rank Reduction: A Communications-Efficient Federated Learning Scheme for Network-Critical Applications</title>
<link>https://arxiv.org/abs/2507.11183</link>
<guid>https://arxiv.org/abs/2507.11183</guid>
<content:encoded><![CDATA[

arXiv:2507.11183v1 Announce Type: new 
Abstract: Federated learning is a machine learning approach that enables multiple devices (i.e., agents) to train a shared model cooperatively without exchanging raw data. This technique keeps data localized on user devices, ensuring privacy and security, while each agent trains the model on their own data and only shares model updates. The communication overhead is a significant challenge due to the frequent exchange of model updates between the agents and the central server. In this paper, we propose a communication-efficient federated learning scheme that utilizes low-rank approximation of neural network gradients and quantization to significantly reduce the network load of the decentralized learning process with minimal impact on the model's accuracy.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding</title>
<link>https://arxiv.org/abs/2507.11198</link>
<guid>https://arxiv.org/abs/2507.11198</guid>
<content:encoded><![CDATA[

arXiv:2507.11198v1 Announce Type: new 
Abstract: Large Language Models (LLMs) enable new possibilities for qualitative research at scale, including coding and data annotation. While multi-agent systems (MAS) can emulate human coding workflows, their benefits over single-agent coding remain poorly understood. We conducted an experimental study of how agent persona and temperature shape consensus-building and coding accuracy of dialog segments based on a codebook with 8 codes. Our open-source MAS mirrors deductive human coding through structured agent discussion and consensus arbitration. Using six open-source LLMs (with 3 to 32 billion parameters) and 18 experimental configurations, we analyze over 77,000 coding decisions against a gold-standard dataset of human-annotated transcripts from online math tutoring sessions. Temperature significantly impacted whether and when consensus was reached across all six LLMs. MAS with multiple personas (including neutral, assertive, or empathetic), significantly delayed consensus in four out of six LLMs compared to uniform personas. In three of those LLMs, higher temperatures significantly diminished the effects of multiple personas on consensus. However, neither temperature nor persona pairing lead to robust improvements in coding accuracy. Single agents matched or outperformed MAS consensus in most conditions. Only one model (OpenHermesV2:7B) and code category showed above-chance gains from MAS deliberation when temperature was 0.5 or lower and especially when the agents included at least one assertive persona. Qualitative analysis of MAS collaboration for these configurations suggests that MAS may nonetheless aid in narrowing ambiguous code applications that could improve codebooks and human-AI coding. We contribute new insight into the limits of LLM-based qualitative methods, challenging the notion that diverse MAS personas lead to better outcomes. We open-source our MAS and experimentation code.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Role-Playing LLM-Based Multi-Agent Support Framework for Detecting and Addressing Family Communication Bias</title>
<link>https://arxiv.org/abs/2507.11210</link>
<guid>https://arxiv.org/abs/2507.11210</guid>
<content:encoded><![CDATA[

arXiv:2507.11210v1 Announce Type: new 
Abstract: Well-being in family settings involves subtle psychological dynamics that conventional metrics often overlook. In particular, unconscious parental expectations, termed ideal parent bias, can suppress children's emotional expression and autonomy. This suppression, referred to as suppressed emotion, often stems from well-meaning but value-driven communication, which is difficult to detect or address from outside the family. Focusing on these latent dynamics, this study explores Large Language Model (LLM)-based support for psychologically safe family communication. We constructed a Japanese parent-child dialogue corpus of 30 scenarios, each annotated with metadata on ideal parent bias and suppressed emotion. Based on this corpus, we developed a Role-Playing LLM-based multi-agent dialogue support framework that analyzes dialogue and generates feedback. Specialized agents detect suppressed emotion, describe implicit ideal parent bias in parental speech, and infer contextual attributes such as the child's age and background. A meta-agent compiles these outputs into a structured report, which is then passed to five selected expert agents. These agents collaboratively generate empathetic and actionable feedback through a structured four-step discussion process. Experiments show that the system can detect categories of suppressed emotion with moderate accuracy and produce feedback rated highly in empathy and practicality. Moreover, simulated follow-up dialogues incorporating this feedback exhibited signs of improved emotional expression and mutual understanding, suggesting the framework's potential in supporting positive transformation in family interactions.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Contracts</title>
<link>https://arxiv.org/abs/2507.11214</link>
<guid>https://arxiv.org/abs/2507.11214</guid>
<content:encoded><![CDATA[

arXiv:2507.11214v1 Announce Type: new 
Abstract: We introduce and study the problem of designing optimal contracts under fairness constraints on the task assignments and compensations. We adopt the notion of envy-free (EF) and its relaxations, $\epsilon$-EF and envy-free up to one item (EF1), in contract design settings. Unlike fair allocations, EF contracts are guaranteed to exist. However, computing any constant-factor approximation to the optimal EF contract is NP-hard in general, even using $\epsilon$-EF contracts. For this reason, we consider settings in which the number of agents or tasks is constant. Notably, while even with three agents, finding an EF contract better than $2/5$ approximation of the optimal is NP-hard, we are able to design an FPTAS when the number of agents is constant, under relaxed notions of $\epsilon$-EF and EF1. Moreover, we present a polynomial-time algorithm for computing the optimal EF contract when the number of tasks is constant. Finally, we analyze the price of fairness in contract design. We show that the price of fairness for exact EF contracts can be unbounded, even with a single task and two agents. In contrast, for EF1 contracts, the price of fairness is bounded between $\Omega(\sqrt{n})$ and $O(n^2)$, where $n$ is the number of agents.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Agentic Flow for Finite State Machine Extraction using Prompt Chaining</title>
<link>https://arxiv.org/abs/2507.11222</link>
<guid>https://arxiv.org/abs/2507.11222</guid>
<content:encoded><![CDATA[

arXiv:2507.11222v1 Announce Type: new 
Abstract: Finite-State Machines (FSMs) are critical for modeling the operational logic of network protocols, enabling verification, analysis, and vulnerability discovery. However, existing FSM extraction techniques face limitations such as scalability, incomplete coverage, and ambiguity in natural language specifications. In this paper, we propose FlowFSM, a novel agentic framework that leverages Large Language Models (LLMs) combined with prompt chaining and chain-of-thought reasoning to extract accurate FSMs from raw RFC documents. FlowFSM systematically processes protocol specifications, identifies state transitions, and constructs structured rule-books by chaining agent outputs. Experimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM achieves high extraction precision while minimizing hallucinated transitions, showing promising results. Our findings highlight the potential of agent-based LLM systems in the advancement of protocol analysis and FSM inference for cybersecurity and reverse engineering applications.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Turning Sand to Gold: Recycling Data to Bridge On-Policy and Off-Policy Learning via Causal Bound</title>
<link>https://arxiv.org/abs/2507.11269</link>
<guid>https://arxiv.org/abs/2507.11269</guid>
<content:encoded><![CDATA[

arXiv:2507.11269v1 Announce Type: new 
Abstract: Deep reinforcement learning (DRL) agents excel in solving complex decision-making tasks across various domains. However, they often require a substantial number of training steps and a vast experience replay buffer, leading to significant computational and resource demands. To address these challenges, we introduce a novel theoretical result that leverages the Neyman-Rubin potential outcomes framework into DRL. Unlike most methods that focus on bounding the counterfactual loss, we establish a causal bound on the factual loss, which is analogous to the on-policy loss in DRL. This bound is computed by storing past value network outputs in the experience replay buffer, effectively utilizing data that is usually discarded. Extensive experiments across the Atari 2600 and MuJoCo domains on various agents, such as DQN and SAC, achieve up to 2,427% higher reward ratio, outperforming the same agents without our proposed term, and reducing the experience replay buffer size by up to 96%, significantly improving sample efficiency at negligible cost.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Study of Multi-Agent RAG for Real-World University Admissions Counseling</title>
<link>https://arxiv.org/abs/2507.11272</link>
<guid>https://arxiv.org/abs/2507.11272</guid>
<content:encoded><![CDATA[

arXiv:2507.11272v1 Announce Type: new 
Abstract: This paper presents MARAUS (Multi-Agent and Retrieval-Augmented University Admission System), a real-world deployment of a conversational AI platform for higher education admissions counseling in Vietnam. While large language models (LLMs) offer potential for automating advisory tasks, most existing solutions remain limited to prototypes or synthetic benchmarks. MARAUS addresses this gap by combining hybrid retrieval, multi-agent orchestration, and LLM-based generation into a system tailored for real-world university admissions. In collaboration with the University of Transport Technology (UTT) in Hanoi, we conducted a two-phase study involving technical development and real-world evaluation. MARAUS processed over 6,000 actual user interactions, spanning six categories of queries. Results show substantial improvements over LLM-only baselines: on average 92 percent accuracy, hallucination rates reduced from 15 precent to 1.45 percent, and average response times below 4 seconds. The system operated cost-effectively, with a two-week deployment cost of 11.58 USD using GPT-4o mini. This work provides actionable insights for the deployment of agentic RAG systems in low-resource educational settings.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming Uncertainty via Automation: Observing, Analyzing, and Optimizing Agentic AI Systems</title>
<link>https://arxiv.org/abs/2507.11277</link>
<guid>https://arxiv.org/abs/2507.11277</guid>
<content:encoded><![CDATA[

arXiv:2507.11277v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly deployed within agentic systems-collections of interacting, LLM-powered agents that execute complex, adaptive workflows using memory, tools, and dynamic planning. While enabling powerful new capabilities, these systems also introduce unique forms of uncertainty stemming from probabilistic reasoning, evolving memory states, and fluid execution paths. Traditional software observability and operations practices fall short in addressing these challenges.
  This paper introduces AgentOps: a comprehensive framework for observing, analyzing, optimizing, and automating operation of agentic AI systems. We identify distinct needs across four key roles-developers, testers, site reliability engineers (SREs), and business users-each of whom engages with the system at different points in its lifecycle. We present the AgentOps Automation Pipeline, a six-stage process encompassing behavior observation, metric collection, issue detection, root cause analysis, optimized recommendations, and runtime automation. Throughout, we emphasize the critical role of automation in managing uncertainty and enabling self-improving AI systems-not by eliminating uncertainty, but by taming it to ensure safe, adaptive, and effective operation.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving Patient-Doctor Communication in Romanian</title>
<link>https://arxiv.org/abs/2507.11299</link>
<guid>https://arxiv.org/abs/2507.11299</guid>
<content:encoded><![CDATA[

arXiv:2507.11299v1 Announce Type: new 
Abstract: Text-based telemedicine has become increasingly common, yet the quality of medical advice in doctor-patient interactions is often judged more on how advice is communicated rather than its clinical accuracy. To address this, we introduce Dr.Copilot , a multi-agent large language model (LLM) system that supports Romanian-speaking doctors by evaluating and enhancing the presentation quality of their written responses. Rather than assessing medical correctness, Dr.Copilot provides feedback along 17 interpretable axes. The system comprises of three LLM agents with prompts automatically optimized via DSPy. Designed with low-resource Romanian data and deployed using open-weight models, it delivers real-time specific feedback to doctors within a telemedicine platform. Empirical evaluations and live deployment with 41 doctors show measurable improvements in user reviews and response quality, marking one of the first real-world deployments of LLMs in Romanian medical settings.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Production Logistics to Smart Manufacturing: The Vision for a New RoboCup Industrial League</title>
<link>https://arxiv.org/abs/2507.11402</link>
<guid>https://arxiv.org/abs/2507.11402</guid>
<content:encoded><![CDATA[

arXiv:2507.11402v1 Announce Type: new 
Abstract: The RoboCup Logistics League is a RoboCup competition in a smart factory scenario that has focused on task planning, job scheduling, and multi-agent coordination. The focus on production logistics allowed teams to develop highly competitive strategies, but also meant that some recent developments in the context of smart manufacturing are not reflected in the competition, weakening its relevance over the years. In this paper, we describe the vision for the RoboCup Smart Manufacturing League, a new competition designed as a larger smart manufacturing scenario, reflecting all the major aspects of a modern factory. It will consist of several tracks that are initially independent but gradually combined into one smart manufacturing scenario. The new tracks will cover industrial robotics challenges such as assembly, human-robot collaboration, and humanoid robotics, but also retain a focus on production logistics. We expect the reenvisioned competition to be more attractive to newcomers and well-tried teams, while also shifting the focus to current and future challenges of industrial robotics.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes</title>
<link>https://arxiv.org/abs/2507.11407</link>
<guid>https://arxiv.org/abs/2507.11407</guid>
<content:encoded><![CDATA[

arXiv:2507.11407v1 Announce Type: new 
Abstract: This technical report introduces EXAONE 4.0, which integrates a Non-reasoning mode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5 and the advanced reasoning abilities of EXAONE Deep. To pave the way for the agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool use, and its multilingual capabilities are extended to support Spanish in addition to English and Korean. The EXAONE 4.0 model series consists of two sizes: a mid-size 32B model optimized for high performance, and a small-size 1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates superior performance compared to open-weight models in its class and remains competitive even against frontier-class models. The models are publicly available for research purposes and can be easily downloaded via https://huggingface.co/LGAI-EXAONE.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LF: Online Multi-Robot Path Planning Meets Optimal Trajectory Control</title>
<link>https://arxiv.org/abs/2507.11464</link>
<guid>https://arxiv.org/abs/2507.11464</guid>
<content:encoded><![CDATA[

arXiv:2507.11464v1 Announce Type: new 
Abstract: We propose a multi-robot control paradigm to solve point-to-point navigation tasks for a team of holonomic robots with access to the full environment information. The framework invokes two processes asynchronously at high frequency: (i) a centralized, discrete, and full-horizon planner for computing collision- and deadlock-free paths rapidly, leveraging recent advances in multi-agent pathfinding (MAPF), and (ii) dynamics-aware, robot-wise optimal trajectory controllers that ensure all robots independently follow their assigned paths reliably. This hierarchical shift in planning representation from (i) discrete and coupled to (ii) continuous and decoupled domains enables the framework to maintain long-term scalable motion synthesis. As an instantiation of this idea, we present LF, which combines a fast state-of-the-art MAPF solver (LaCAM), and a robust feedback control stack (Freyja) for executing agile robot maneuvers. LF provides a robust and versatile mechanism for lifelong multi-robot navigation even under asynchronous and partial goal updates, and adapts to dynamic workspaces simply by quick replanning. We present various multirotor and ground robot demonstrations, including the deployment of 15 real multirotors with random, consecutive target updates while a person walks through the operational workspace.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Complexity of the Optimal Correlated Equilibria in Extensive-Form Games</title>
<link>https://arxiv.org/abs/2507.11509</link>
<guid>https://arxiv.org/abs/2507.11509</guid>
<content:encoded><![CDATA[

arXiv:2507.11509v1 Announce Type: new 
Abstract: A major open question in algorithmic game theory is whether normal-form correlated equilibria (NFCE) can be computed efficiently in succinct games such as extensive-form games [DFF+25,6PR24,FP23,HvS08,VSF08,PR08]. Motivated by this question, we study the associated Threshold problem: deciding whether there exists a correlated equilibrium whose value exceeds a given threshold. We prove that this problem is PSPACE-hard for NFCE in multiplayer extensive-form games with perfect recall, even for fixed thresholds. To contextualize this result, we also establish the complexity of the Threshold problem for Nash equilibria in this setting, showing it is ER-complete. These results uncover a surprising complexity reversal: while optimal correlated equilibria are computationally simpler than optimal Nash in normal-form games, the opposite holds in extensive-form games, where computing optimal correlated equilibria is provably harder. Building on this line of inquiry, we also address a related question by [VSF08], who introduced the notions of extensive-form correlated equilibrium (EFCE) and agent-form correlated equilibrium (AFCE). They asked how difficult the Threshold problem is for AFCE; we answer this question by proving that it is NP-hard, even in two-player games without chance nodes. Complementing our hardness results, we establish tight complexity classifications for the Threshold problem across several correlated equilibrium concepts - including EFCE, AFCE, normal-form coarse, extensive-form coarse, and agent-form coarse correlated equilibria. For each of these solution concepts in multiplayer stochastic extensive-form games with perfect recall, we prove NP-completeness by providing matching NP upper bounds to the previously known hardness results. Together, our results provide the most complete landscape to date for the complexity of optimal equilibrium computation in extensive-form games.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air</title>
<link>https://arxiv.org/abs/2507.11515</link>
<guid>https://arxiv.org/abs/2507.11515</guid>
<content:encoded><![CDATA[

arXiv:2507.11515v1 Announce Type: new 
Abstract: Operating Large Language Models (LLMs) on edge devices is increasingly challenged by limited communication bandwidth and strained computational and memory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable. Nevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ fixed or heuristic rank configurations, and the subsequent over-the-air transmission of all LoRA parameters could be rather inefficient. To address this limitation, we develop AirLLM, a hierarchical diffusion policy framework for communication-aware LoRA adaptation. Specifically, AirLLM models the rank configuration as a structured action vector that spans all LoRA-inserted projections. To solve the underlying high-dimensional sequential decision-making problem, a Proximal Policy Optimization (PPO) agent generates coarse-grained decisions by jointly observing wireless states and linguistic complexity, which are then refined via Denoising Diffusion Implicit Models (DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The two modules are optimized alternatively, with the DDIM trained under the Classifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards. Experiments under varying signal-to-noise ratios demonstrate that AirLLM consistently enhances fine-tuning performance while significantly reducing transmission costs, highlighting the effectiveness of reinforcement-driven, diffusion-refined rank adaptation for scalable and efficient remote fine-tuning over the air.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering</title>
<link>https://arxiv.org/abs/2507.11527</link>
<guid>https://arxiv.org/abs/2507.11527</guid>
<content:encoded><![CDATA[

arXiv:2507.11527v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents have shown great potential for solving real-world problems and promise to be a solution for tasks automation in industry. However, more benchmarks are needed to systematically evaluate automation agents from an industrial perspective, for example, in Civil Engineering. Therefore, we propose DrafterBench for the comprehensive evaluation of LLM agents in the context of technical drawing revision, a representation task in civil engineering. DrafterBench contains twelve types of tasks summarized from real-world drawing files, with 46 customized functions/tools and 1920 tasks in total. DrafterBench is an open-source benchmark to rigorously test AI agents' proficiency in interpreting intricate and long-context instructions, leveraging prior knowledge, and adapting to dynamic instruction quality via implicit policy awareness. The toolkit comprehensively assesses distinct capabilities in structured data comprehension, function execution, instruction following, and critical reasoning. DrafterBench offers detailed analysis of task accuracy and error statistics, aiming to provide deeper insight into agent capabilities and identify improvement targets for integrating LLMs in engineering applications. Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench, with the test set hosted at https://huggingface.co/datasets/Eason666/DrafterBench.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Brains and Machines: A Unified Frontier in Neuroscience, Artificial Intelligence, and Neuromorphic Systems</title>
<link>https://arxiv.org/abs/2507.10722</link>
<guid>https://arxiv.org/abs/2507.10722</guid>
<content:encoded><![CDATA[

arXiv:2507.10722v1 Announce Type: cross 
Abstract: This position and survey paper identifies the emerging convergence of neuroscience, artificial general intelligence (AGI), and neuromorphic computing toward a unified research paradigm. Using a framework grounded in brain physiology, we highlight how synaptic plasticity, sparse spike-based communication, and multimodal association provide design principles for next-generation AGI systems that potentially combine both human and machine intelligences. The review traces this evolution from early connectionist models to state-of-the-art large language models, demonstrating how key innovations like transformer attention, foundation-model pre-training, and multi-agent architectures mirror neurobiological processes like cortical mechanisms, working memory, and episodic consolidation. We then discuss emerging physical substrates capable of breaking the von Neumann bottleneck to achieve brain-scale efficiency in silicon: memristive crossbars, in-memory compute arrays, and emerging quantum and photonic devices. There are four critical challenges at this intersection: 1) integrating spiking dynamics with foundation models, 2) maintaining lifelong plasticity without catastrophic forgetting, 3) unifying language with sensorimotor learning in embodied agents, and 4) enforcing ethical safeguards in advanced neuromorphic autonomous systems. This combined perspective across neuroscience, computation, and hardware offers an integrative agenda for in each of these fields.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Conversations</title>
<link>https://arxiv.org/abs/2307.08827</link>
<guid>https://arxiv.org/abs/2307.08827</guid>
<content:encoded><![CDATA[

arXiv:2307.08827v4 Announce Type: replace 
Abstract: We initiate the study of Bayesian conversations, which model interactive communication between two strategic agents without a mediator. We compare this to communication through a mediator and investigate the settings in which a mediation can expand the range of implementable outcomes.
  We look into the eventual outcome of two-player games after interactive communication. We focus on games where only one agent has a non-trivial action and examine the performance of communication protocols that are individually rational (IR) for both parties. We characterize the structure of the social-welfare optimal protocol of a given number of rounds and thus show a separation between Bayesian conversation and mediated protocols. We demonstrate an example where the optimal conversation protocol requires infinitely many rounds of communication, and further show that for settings with binary actions and binary types, any optimal protocol either is finite (with at most 6 rounds) or requires infinitely many rounds of communication.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PPA-Game: Characterizing and Learning Competitive Dynamics Among Online Content Creators</title>
<link>https://arxiv.org/abs/2403.15524</link>
<guid>https://arxiv.org/abs/2403.15524</guid>
<content:encoded><![CDATA[

arXiv:2403.15524v2 Announce Type: replace 
Abstract: In this paper, we present the Proportional Payoff Allocation Game (PPA-Game), which characterizes situations where agents compete for divisible resources. In the PPA-game, agents select from available resources, and their payoffs are proportionately determined based on heterogeneous weights attributed to them. Such dynamics simulate content creators on online recommender systems like YouTube and TikTok, who compete for finite consumer attention, with content exposure reliant on inherent and distinct quality. We first conduct a game-theoretical analysis of the PPA-Game. While the PPA-Game does not always guarantee the existence of a pure Nash equilibrium (PNE), we identify prevalent scenarios ensuring its existence. Simulated experiments further prove that the cases where PNE does not exist rarely happen. Beyond analyzing static payoffs, we further discuss the agents' online learning about resource payoffs by integrating a multi-player multi-armed bandit framework. We propose an online algorithm facilitating each agent's maximization of cumulative payoffs over $T$ rounds. Theoretically, we establish that the regret of any agent is bounded by $O(\log^{1 + \eta} T)$ for any $\eta > 0$. Empirical results further validate the effectiveness of our online learning approach.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trajectory Imputation in Multi-Agent Sports with Derivative-Accumulating Self-Ensemble</title>
<link>https://arxiv.org/abs/2408.10878</link>
<guid>https://arxiv.org/abs/2408.10878</guid>
<content:encoded><![CDATA[

arXiv:2408.10878v4 Announce Type: replace 
Abstract: Multi-agent trajectory data collected from domains such as team sports often suffer from missing values due to various factors. While many imputation methods have been proposed for spatiotemporal data, they are not well-suited for multi-agent sports scenarios where player movements are highly dynamic and inter-agent interactions continuously evolve. To address these challenges, we propose MIDAS (Multi-agent Imputer with Derivative-Accumulating Self-ensemble), a framework that imputes multi-agent trajectories with high accuracy and physical plausibility. It jointly predicts positions, velocities, and accelerations through a Set Transformer-based neural network and generates alternative estimates by recursively accumulating predicted velocity and acceleration values. These predictions are then combined using a learnable weighted ensemble to produce final imputed trajectories. Experiments on three sports datasets demonstrate that MIDAS significantly outperforms existing baselines in both positional accuracy and physical plausibility. Lastly, we showcase use cases of MIDAS, such as approximating total distance and pass success probability, to highlight its applicability to practical downstream tasks that require complete tracking data.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SECURE: Semantics-aware Embodied Conversation under Unawareness for Lifelong Robot Learning</title>
<link>https://arxiv.org/abs/2409.17755</link>
<guid>https://arxiv.org/abs/2409.17755</guid>
<content:encoded><![CDATA[

arXiv:2409.17755v3 Announce Type: replace 
Abstract: This paper addresses a challenging interactive task learning scenario we call rearrangement under unawareness: an agent must manipulate a rigid-body environment without knowing a key concept necessary for solving the task and must learn about it during deployment. For example, the user may ask to "put the two granny smith apples inside the basket", but the agent cannot correctly identify which objects in the environment are "granny smith" as the agent has not been exposed to such a concept before. We introduce SECURE, an interactive task learning policy designed to tackle such scenarios. The unique feature of SECURE is its ability to enable agents to engage in semantic analysis when processing embodied conversations and making decisions. Through embodied conversation, a SECURE agent adjusts its deficient domain model by engaging in dialogue to identify and learn about previously unforeseen possibilities. The SECURE agent learns from the user's embodied corrective feedback when mistakes are made and strategically engages in dialogue to uncover useful information about novel concepts relevant to the task. These capabilities enable the SECURE agent to generalize to new tasks with the acquired knowledge. We demonstrate in the simulated Blocksworld and the real-world apple manipulation environments that the SECURE agent, which solves such rearrangements under unawareness, is more data-efficient than agents that do not engage in embodied conversation or semantic analysis.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Possible Principles for Aligned Structure Learning Agents</title>
<link>https://arxiv.org/abs/2410.00258</link>
<guid>https://arxiv.org/abs/2410.00258</guid>
<content:encoded><![CDATA[

arXiv:2410.00258v2 Announce Type: replace 
Abstract: This paper offers a roadmap for the development of scalable aligned artificial intelligence (AI) from first principle descriptions of natural intelligence. In brief, a possible path toward scalable aligned AI rests upon enabling artificial agents to learn a good model of the world that includes a good model of our preferences. For this, the main objective is creating agents that learn to represent the world and other agents' world models; a problem that falls under structure learning (a.k.a. causal representation learning or model discovery). We expose the structure learning and alignment problems with this goal in mind, as well as principles to guide us forward, synthesizing various ideas across mathematics, statistics, and cognitive science. 1) We discuss the essential role of core knowledge, information geometry and model reduction in structure learning, and suggest core structural modules to learn a wide range of naturalistic worlds. 2) We outline a way toward aligned agents through structure learning and theory of mind. As an illustrative example, we mathematically sketch Asimov's Laws of Robotics, which prescribe agents to act cautiously to minimize the ill-being of other agents. We supplement this example by proposing refined approaches to alignment. These observations may guide the development of artificial intelligence in helping to scale existing -- or design new -- aligned structure learning systems.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Intrinsic Rewards for Decision Making Agents from Large Language Model Feedback</title>
<link>https://arxiv.org/abs/2410.23022</link>
<guid>https://arxiv.org/abs/2410.23022</guid>
<content:encoded><![CDATA[

arXiv:2410.23022v3 Announce Type: replace 
Abstract: Automatically synthesizing dense rewards from natural language descriptions is a promising paradigm in reinforcement learning (RL), with applications to sparse reward problems, open-ended exploration, and hierarchical skill design. Recent works have made promising steps by exploiting the prior knowledge of large language models (LLMs). However, these approaches suffer from important limitations: they are either not scalable to problems requiring billions of environment samples, due to requiring LLM annotations for each observation, or they require a diverse offline dataset, which may not exist or be impossible to collect. In this work, we address these limitations through a combination of algorithmic and systems-level contributions. We propose ONI, a distributed architecture that simultaneously learns an RL policy and an intrinsic reward function using LLM feedback. Our approach annotates the agent's collected experience via an asynchronous LLM server, which is then distilled into an intrinsic reward model. We explore a range of algorithmic choices for reward modeling with varying complexity, including hashing, classification, and ranking models. Our approach achieves state-of-the-art performance across a range of challenging tasks from the NetHack Learning Environment, while removing the need for large offline datasets required by prior work. We make our code available at https://github.com/facebookresearch/oni .
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM Serving</title>
<link>https://arxiv.org/abs/2411.02820</link>
<guid>https://arxiv.org/abs/2411.02820</guid>
<content:encoded><![CDATA[

arXiv:2411.02820v4 Announce Type: replace 
Abstract: Compound AI systems, such as agentic systems, are an emerging trend in large-scale enterprise settings, with multiple LLMs specialized for different users, tasks, and/or roles working together. In these scenarios, different models often process inputs that share the same context prefix. Although much work was done in the past to enable the reuse of prefix KV caches across inputs for a single model, how to enable one model to reuse the prefix KV caches of a different model remains an open question.
  We introduce DroidSpeak, the first distributed LLM inference system that enables KV cache reuse across distributed nodes running inference of different LLMs, so long as the LLMs have the same architecture. We present the first study that aims at understanding the impact of sharing KV caches across different LLMs, and if/when such sharing affects quality. Inspired by the findings, we present DroidSpeak, which selectively recomputes a few layers of the KV cache produced by another LLM and reuses the remaining layers, with negligible quality loss. Moreover, carefully pipelining the layer-wise re-computation and the loading of reused KV cache further improves the inference performance. Experiments on diverse datasets and model pairs demonstrate that DroidSpeak achieves up to 4x throughput improvement and about 3.1x faster prefill (time to first token), with negligible loss of quality in F1 scores, Rouge-L or code similarity score, compared to the baseline which does not allow any sharing across models.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crowd: A Social Network Simulation Framework</title>
<link>https://arxiv.org/abs/2412.10781</link>
<guid>https://arxiv.org/abs/2412.10781</guid>
<content:encoded><![CDATA[

arXiv:2412.10781v3 Announce Type: replace 
Abstract: To observe how individual behavior shapes a larger community's actions, agent-based modeling and simulation (ABMS) has been widely adopted by researchers in social sciences, economics, and epidemiology. While simulations can be run on general-purpose ABMS frameworks, these tools are not specifically designed for social networks and, therefore, provide limited features, increasing the effort required for complex simulations. In this paper, we introduce Crowd, a social network simulator that adopts the agent-based modeling methodology to model real-world phenomena within a network environment. Designed to facilitate easy and quick modeling, Crowd supports simulation setup through YAML configuration and enables further customization with user-defined methods. Other features include no-code simulations for diffusion tasks, interactive visualizations, data aggregation, and chart drawing facilities. Designed in Python, Crowd also supports generative agents and connects easily with Python's libraries for data analysis and machine learning. Finally, we include three case studies to illustrate the use of the framework, including generative agents in epidemics, influence maximization, and networked trust games.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plancraft: an evaluation dataset for planning with LLM agents</title>
<link>https://arxiv.org/abs/2412.21033</link>
<guid>https://arxiv.org/abs/2412.21033</guid>
<content:encoded><![CDATA[

arXiv:2412.21033v2 Announce Type: replace 
Abstract: We present Plancraft, a multi-modal evaluation dataset for LLM agents. Plancraft has both a text-only and multi-modal interface, based on the Minecraft crafting GUI. We include the Minecraft Wiki to evaluate tool use and Retrieval Augmented Generation (RAG), as well as a handcrafted planner and Oracle Retriever, to ablate the different components of a modern agent architecture. To evaluate decision-making, Plancraft also includes a subset of examples that are intentionally unsolvable, providing a realistic challenge that requires the agent not only to complete tasks but also to decide whether they are solvable at all. We benchmark both open-source and closed-source LLMs and compare their performance and efficiency to a handcrafted planner. Overall, we find that LLMs and VLMs struggle with the planning problems that Plancraft introduces, and offer suggestions on how to improve their capabilities.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Reasoning: A Streamlined Framework for Enhancing LLM Reasoning with Agentic Tools</title>
<link>https://arxiv.org/abs/2502.04644</link>
<guid>https://arxiv.org/abs/2502.04644</guid>
<content:encoded><![CDATA[

arXiv:2502.04644v2 Announce Type: replace 
Abstract: We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents. Agentic Reasoning dynamically leverages web search, code execution, and structured memory to address complex problems requiring deep research. A key innovation in our framework is the Mind-Map agent, which constructs a structured knowledge graph to store reasoning context and track logical relationships, ensuring coherence in long reasoning chains with extensive tool usage. Additionally, we conduct a comprehensive exploration of the Web-Search agent, leading to a highly effective search mechanism that surpasses all prior approaches. When deployed on DeepSeek-R1, our method achieves a new state-of-the-art (SOTA) among public models and delivers performance comparable to OpenAI Deep Research, the leading proprietary model in this domain. Extensive ablation studies validate the optimal selection of agentic tools and confirm the effectiveness of our Mind-Map and Web-Search agents in enhancing LLM reasoning. The code is at: https://github.com/theworldofagents/Agentic-Reasoning
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imitation Learning from a Single Temporally Misaligned Video</title>
<link>https://arxiv.org/abs/2502.05397</link>
<guid>https://arxiv.org/abs/2502.05397</guid>
<content:encoded><![CDATA[

arXiv:2502.05397v2 Announce Type: replace 
Abstract: We examine the problem of learning sequential tasks from a single visual demonstration. A key challenge arises when demonstrations are temporally misaligned due to variations in timing, differences in embodiment, or inconsistencies in execution. Existing approaches treat imitation as a distribution-matching problem, aligning individual frames between the agent and the demonstration. However, we show that such frame-level matching fails to enforce temporal ordering or ensure consistent progress. Our key insight is that matching should instead be defined at the level of sequences. We propose that perfect matching occurs when one sequence successfully covers all the subgoals in the same order as the other sequence. We present ORCA (ORdered Coverage Alignment), a dense per-timestep reward function that measures the probability of the agent covering demonstration frames in the correct order. On temporally misaligned demonstrations, we show that agents trained with the ORCA reward achieve $4.5$x improvement ($0.11 \rightarrow 0.50$ average normalized returns) for Meta-world tasks and $6.6$x improvement ($6.55 \rightarrow 43.3$ average returns) for Humanoid-v4 tasks compared to the best frame-level matching algorithms. We also provide empirical analysis showing that ORCA is robust to varying levels of temporal misalignment. Our code is available at https://github.com/portal-cornell/orca/
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Odyssey of the Fittest: Can Agents Survive and Still Be Good?</title>
<link>https://arxiv.org/abs/2502.05442</link>
<guid>https://arxiv.org/abs/2502.05442</guid>
<content:encoded><![CDATA[

arXiv:2502.05442v3 Announce Type: replace 
Abstract: As AI models grow in power and generality, understanding how agents learn and make decisions in complex environments is critical to promoting ethical behavior. This study introduces the Odyssey, a lightweight, adaptive text based adventure game, providing a scalable framework for exploring AI ethics and safety. The Odyssey examines the ethical implications of implementing biological drives, specifically, self preservation, into three different agents. A Bayesian agent optimized with NEAT, a Bayesian agent optimized with stochastic variational inference, and a GPT 4o agent. The agents select actions at each scenario to survive, adapting to increasingly challenging scenarios. Post simulation analysis evaluates the ethical scores of the agent decisions, uncovering the tradeoffs it navigates to survive. Specifically, analysis finds that when danger increases, agents ethical behavior becomes unpredictable. Surprisingly, the GPT 4o agent outperformed the Bayesian models in both survival and ethical consistency, challenging assumptions about traditional probabilistic methods and raising a new challenge to understand the mechanisms of LLMs' probabilistic reasoning.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assistance or Disruption? Exploring and Evaluating the Design and Trade-offs of Proactive AI Programming Support</title>
<link>https://arxiv.org/abs/2502.18658</link>
<guid>https://arxiv.org/abs/2502.18658</guid>
<content:encoded><![CDATA[

arXiv:2502.18658v3 Announce Type: replace 
Abstract: AI programming tools enable powerful code generation, and recent prototypes attempt to reduce user effort with proactive AI agents, but their impact on programming workflows remains unexplored. We introduce and evaluate Codellaborator, a design probe LLM agent that initiates programming assistance based on editor activities and task context. We explored three interface variants to assess trade-offs between increasingly salient AI support: prompt-only, proactive agent, and proactive agent with presence and context (Codellaborator). In a within-subject study (N=18), we find that proactive agents increase efficiency compared to prompt-only paradigm, but also incur workflow disruptions. However, presence indicators and interaction context support alleviated disruptions and improved users' awareness of AI processes. We underscore trade-offs of Codellaborator on user control, ownership, and code understanding, emphasizing the need to adapt proactivity to programming processes. Our research contributes to the design exploration and evaluation of proactive AI systems, presenting design implications on AI-integrated programming workflow.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voting or Consensus? Decision-Making in Multi-Agent Debate</title>
<link>https://arxiv.org/abs/2502.19130</link>
<guid>https://arxiv.org/abs/2502.19130</guid>
<content:encoded><![CDATA[

arXiv:2502.19130v3 Announce Type: replace 
Abstract: Much of the success of multi-agent debates depends on carefully choosing the right parameters. The decision-making protocol stands out as it can highly impact final model answers, depending on how decisions are reached. Systematic comparison of decision protocols is difficult because many studies alter multiple discussion parameters beyond the protocol. So far, it has been largely unknown how decision-making influences different tasks. This work systematically evaluates the impact of seven decision protocols (e.g., majority voting, unanimity consensus). We change only one variable at a time - the decision protocol - to analyze how different methods affect the collaboration between agents and measure differences in knowledge and reasoning tasks. Our results show that voting protocols improve performance by 13.2% in reasoning tasks and consensus protocols by 2.8% in knowledge tasks compared to other decision protocols. Increasing the number of agents improves performance, while more discussion rounds before voting reduce it. To improve decision-making by increasing answer diversity, we propose two new methods, All-Agents Drafting (AAD) and Collective Improvement (CI). Our methods improve task performance by up to 3.3% with AAD and up to 7.4% with CI. This work demonstrates the importance of decision-making in multi-agent debates beyond scaling.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding</title>
<link>https://arxiv.org/abs/2503.10596</link>
<guid>https://arxiv.org/abs/2503.10596</guid>
<content:encoded><![CDATA[

arXiv:2503.10596v3 Announce Type: replace 
Abstract: Pixel grounding, encompassing tasks such as Referring Expression Segmentation (RES), has garnered considerable attention due to its immense potential for bridging the gap between vision and language modalities. However, advancements in this domain are currently constrained by limitations inherent in existing datasets, including limited object categories, insufficient textual diversity, and a scarcity of high-quality annotations. To mitigate these limitations, we introduce GroundingSuite, which comprises: (1) an automated data annotation framework leveraging multiple Vision-Language Model (VLM) agents; (2) a large-scale training dataset encompassing 9.56 million diverse referring expressions and their corresponding segmentations; and (3) a meticulously curated evaluation benchmark consisting of 3,800 images. The GroundingSuite training dataset facilitates substantial performance improvements, enabling models trained on it to achieve state-of-the-art results. Specifically, a cIoU of 68.9 on gRefCOCO and a gIoU of 55.3 on RefCOCOm. Moreover, the GroundingSuite annotation framework demonstrates superior efficiency compared to the current leading data annotation method, i.e., $4.5 \times$ faster than GLaMM.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are AI Agents interacting with Online Ads?</title>
<link>https://arxiv.org/abs/2504.07112</link>
<guid>https://arxiv.org/abs/2504.07112</guid>
<content:encoded><![CDATA[

arXiv:2504.07112v4 Announce Type: replace 
Abstract: As AI-driven agents become increasingly integrated into the digital ecosystem, they reshape how online advertising is perceived and processed. Particularly in the travel and hotel booking sector, these autonomous systems influence the effectiveness of traditional advertising formats. While visual cues and emotional appeals sway human users, AI agents prioritize structured data such as price, availability, and specifications. This study examines how different AI agents interact with online advertising, whether they incorporate ads into their decision-making processes, and which ad formats prove most effective. We analyze interaction patterns, click behavior, and decision-making strategies through experiments with multimodal language models such as OpenAI GPT-4o, Anthropic Claude 3.7 Sonnet, and Google Gemini 2.0 Flash. Our findings reveal that AI agents neither ignore nor systematically avoid advertisements but instead favor certain features-particularly keywords and structured data. These insights have significant implications for the future design of advertising strategies in AI-dominated digital environments.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users</title>
<link>https://arxiv.org/abs/2504.10157</link>
<guid>https://arxiv.org/abs/2504.10157</guid>
<content:encoded><![CDATA[

arXiv:2504.10157v3 Announce Type: replace 
Abstract: Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeking to Collide: Online Safety-Critical Scenario Generation for Autonomous Driving with Retrieval Augmented Large Language Models</title>
<link>https://arxiv.org/abs/2505.00972</link>
<guid>https://arxiv.org/abs/2505.00972</guid>
<content:encoded><![CDATA[

arXiv:2505.00972v2 Announce Type: replace 
Abstract: Simulation-based testing is crucial for validating autonomous vehicles (AVs), yet existing scenario generation methods either overfit to common driving patterns or operate in an offline, non-interactive manner that fails to expose rare, safety-critical corner cases. In this paper, we introduce an online, retrieval-augmented large language model (LLM) framework for generating safety-critical driving scenarios. Our method first employs an LLM-based behavior analyzer to infer the most dangerous intent of the background vehicle from the observed state, then queries additional LLM agents to synthesize feasible adversarial trajectories. To mitigate catastrophic forgetting and accelerate adaptation, we augment the framework with a dynamic memorization and retrieval bank of intent-planner pairs, automatically expanding its behavioral library when novel intents arise. Evaluations using the Waymo Open Motion Dataset demonstrate that our model reduces the mean minimum time-to-collision from 1.62 to 1.08 s and incurs a 75% collision rate, substantially outperforming baselines.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FalseReject: A Resource for Improving Contextual Safety and Mitigating Over-Refusals in LLMs via Structured Reasoning</title>
<link>https://arxiv.org/abs/2505.08054</link>
<guid>https://arxiv.org/abs/2505.08054</guid>
<content:encoded><![CDATA[

arXiv:2505.08054v2 Announce Type: replace 
Abstract: Safety alignment approaches in large language models (LLMs) often lead to the over-refusal of benign queries, significantly diminishing their utility in sensitive scenarios. To address this challenge, we introduce FalseReject, a comprehensive resource containing 16k seemingly toxic queries accompanied by structured responses across 44 safety-related categories. We propose a graph-informed adversarial multi-agent interaction framework to generate diverse and complex prompts, while structuring responses with explicit reasoning to aid models in accurately distinguishing safe from unsafe contexts. FalseReject includes training datasets tailored for both standard instruction-tuned models and reasoning-oriented models, as well as a human-annotated benchmark test set. Our extensive benchmarking on 29 state-of-the-art (SOTA) LLMs reveals persistent over-refusal challenges. Empirical results demonstrate that supervised finetuning with FalseReject substantially reduces unnecessary refusals without compromising overall safety or general language capabilities.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-Competitive Differential Game Logic</title>
<link>https://arxiv.org/abs/2505.14688</link>
<guid>https://arxiv.org/abs/2505.14688</guid>
<content:encoded><![CDATA[

arXiv:2505.14688v2 Announce Type: replace 
Abstract: This paper introduces semi-competitive differential game logic dGLsc, which enables verification of safety-critical applications that involve interactions between two agents. In dGLsc, these interactions are specified as games on hybrid systems with two players that may collaborate with each other when helpful and may compete when necessary. The players in the hybrid games of dGLsc have individual goals that may overlap, leading to nonzero-sum games. This makes dGLsc especially well-suited for verifying situations where players, e.g., share safety objectives but otherwise pursue different goals, so that zero-sum assumptions lead to overly conservative results. Additionally, dGLsc solves the subtlety that even though each player may benefit from knowledge of the other player's goals, e.g., concerning shared safety objectives, unsafe situations might still occur if every player were to mutually assume the other player would act to avoid unsafety. The syntax and semantics, as well as a sound and relatively complete proof calculus are presented for dGLsc. The relationship between dGLsc and zero-sum differential game logic dGL is discussed and the purpose of dGLsc illustrated in a canonical example.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Multi-Modal LLM Agents for Treatment Planning in Focused Ultrasound Ablation Surgery</title>
<link>https://arxiv.org/abs/2505.21418</link>
<guid>https://arxiv.org/abs/2505.21418</guid>
<content:encoded><![CDATA[

arXiv:2505.21418v2 Announce Type: replace 
Abstract: Focused Ultrasound Ablation Surgery (FUAS) has emerged as a promising non-invasive therapeutic modality, valued for its safety and precision. Nevertheless, its clinical implementation entails intricate tasks such as multimodal image interpretation, personalized dose planning, and real-time intraoperative decision-making processes that demand intelligent assistance to improve efficiency and reliability. We introduce FUAS-Agents, an autonomous agent system that leverages the multimodal understanding and tool-using capabilities of large language models (LLMs). By integrating patient profiles and MRI data, FUAS-Agents orchestrates a suite of specialized medical AI tools, including segmentation, treatment dose prediction, and clinical guideline retrieval, to generate personalized treatment plans comprising MRI image, dose parameters, and therapeutic strategies. We evaluate the system in a uterine fibroid treatment scenario. Human assessment by four senior FUAS experts indicates that 82.5%, 82.5%, 87.5%, and 97.5% of the generated plans were rated 4 or above (on a 5-point scale) in terms of completeness, accuracy, fluency, and clinical compliance, respectively. These results demonstrate the potential of LLM-driven agents in enhancing decision-making across complex clinical workflows, and exemplify a translational paradigm that combines general-purpose models with specialized expert systems to solve practical challenges in vertical healthcare domains.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUARD:Dual-Agent based Backdoor Defense on Chain-of-Thought in Neural Code Generation</title>
<link>https://arxiv.org/abs/2505.21425</link>
<guid>https://arxiv.org/abs/2505.21425</guid>
<content:encoded><![CDATA[

arXiv:2505.21425v2 Announce Type: replace 
Abstract: With the widespread application of large language models in code generation, recent studies demonstrate that employing additional Chain-of-Thought generation models can significantly enhance code generation performance by providing explicit reasoning steps. However, as external components, CoT models are particularly vulnerable to backdoor attacks, which existing defense mechanisms often fail to detect effectively. To address this challenge, we propose GUARD, a novel dual-agent defense framework specifically designed to counter CoT backdoor attacks in neural code generation. GUARD integrates two core components: GUARD-Judge, which identifies suspicious CoT steps and potential triggers through comprehensive analysis, and GUARD-Repair, which employs a retrieval-augmented generation approach to regenerate secure CoT steps for identified anomalies. Experimental results show that GUARD effectively mitigates attacks while maintaining generation quality, advancing secure code generation systems.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seven Security Challenges That Must be Solved in Cross-domain Multi-agent LLM Systems</title>
<link>https://arxiv.org/abs/2505.23847</link>
<guid>https://arxiv.org/abs/2505.23847</guid>
<content:encoded><![CDATA[

arXiv:2505.23847v3 Announce Type: replace 
Abstract: Large language models (LLMs) are rapidly evolving into autonomous agents that cooperate across organizational boundaries, enabling joint disaster response, supply-chain optimization, and other tasks that demand decentralized expertise without surrendering data ownership. Yet, cross-domain collaboration shatters the unified trust assumptions behind current alignment and containment techniques. An agent benign in isolation may, when receiving messages from an untrusted peer, leak secrets or violate policy, producing risks driven by emergent multi-agent dynamics rather than classical software bugs. This position paper maps the security agenda for cross-domain multi-agent LLM systems. We introduce seven categories of novel security challenges, for each of which we also present plausible attacks, security evaluation metrics, and future research guidelines.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A quantum semantic framework for natural language processing</title>
<link>https://arxiv.org/abs/2506.10077</link>
<guid>https://arxiv.org/abs/2506.10077</guid>
<content:encoded><![CDATA[

arXiv:2506.10077v2 Announce Type: replace 
Abstract: Semantic degeneracy represents a fundamental property of natural language that extends beyond simple polysemy to encompass the combinatorial explosion of potential interpretations that emerges as semantic expressions increase in complexity. In this work, we argue this property imposes fundamental limitations on Large Language Models (LLMs) and other modern NLP systems, precisely because they operate within natural language itself. Using Kolmogorov complexity, we demonstrate that as an expression's complexity grows, the amount of contextual information required to reliably resolve its ambiguity explodes combinatorially. The computational intractability of recovering a single intended meaning for complex or ambiguous text therefore suggests that the classical view that linguistic forms possess intrinsic meaning in and of themselves is conceptually inadequate. We argue instead that meaning is dynamically actualized through an observer-dependent interpretive act, a process whose non-deterministic nature is most appropriately described by a non-classical, quantum-like logic. To test this hypothesis, we conducted a semantic Bell inequality test using diverse LLM agents. Our experiments yielded average CHSH expectation values from 1.2 to 2.8, with several runs producing values (e.g., 2.3-2.4) in significant violation of the classical boundary ($|S|\leq2$), demonstrating that linguistic interpretation under ambiguity can exhibit non-classical contextuality, consistent with results from human cognition experiments. These results inherently imply that classical frequentist-based analytical approaches for natural language are necessarily lossy. Instead, we propose that Bayesian-style repeated sampling approaches can provide more practically useful and appropriate characterizations of linguistic meaning in context.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>View Invariant Learning for Vision-Language Navigation in Continuous Environments</title>
<link>https://arxiv.org/abs/2507.08831</link>
<guid>https://arxiv.org/abs/2507.08831</guid>
<content:encoded><![CDATA[
arXiv:2507.08831v1 Announce Type: new 
Abstract: Vision-Language Navigation in Continuous Environments (VLNCE), where an agent follows instructions and moves freely to reach a destination, is a key research problem in embodied AI. However, most navigation policies are sensitive to viewpoint changes, i.e., variations in camera height and viewing angle that alter the agent's observation. In this paper, we introduce a generalized scenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View Invariant Learning), a view-invariant post-training strategy that enhances the robustness of existing navigation policies to changes in camera viewpoint. VIL employs a contrastive learning framework to learn sparse and view-invariant features. Additionally, we introduce a teacher-student framework for the Waypoint Predictor Module, a core component of most VLNCE baselines, where a view-dependent teacher model distills knowledge into a view-invariant student model. We employ an end-to-end training paradigm to jointly optimize these components, thus eliminating the cost for individual module training. Empirical results show that our method outperforms state-of-the-art approaches on V2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets R2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE setting and find that, despite being trained for varied viewpoints, it often still improves performance. On the more challenging RxR-CE dataset, our method also achieved state-of-the-art performance across all metrics when compared to other map-free methods. This suggests that adding VIL does not diminish the standard viewpoint performance and can serve as a plug-and-play post-training method.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-based visualization of streaming text</title>
<link>https://arxiv.org/abs/2507.08884</link>
<guid>https://arxiv.org/abs/2507.08884</guid>
<content:encoded><![CDATA[
arXiv:2507.08884v1 Announce Type: new 
Abstract: We present a visualization infrastructure that maps data elements to agents, which have behaviors parameterized by those elements. Dynamic visualizations emerge as the agents change position, alter appearance and respond to one other. Agents move to minimize the difference between displayed agent-to-agent distances, and an input matrix of ideal distances. Our current application is visualization of streaming text. Each agent represents a significant word, visualizing it by displaying the word itself, centered in a circle sized by the frequency of word occurrence. We derive the ideal distance matrix from word cooccurrence, mapping higher co-occurrence to lower distance. To depict co-occurrence in its textual context, the ratio of intersection to circle area approximates the ratio of word co-occurrence to frequency. A networked backend process gathers articles from news feeds, blogs, Digg or Twitter, exploiting online search APIs to focus on user-chosen topics. Resulting visuals reveal the primary topics in text streams as clusters, with agent-based layout moving without instability as data streams change dynamically.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AirScape: An Aerial Generative World Model with Motion Controllability</title>
<link>https://arxiv.org/abs/2507.08885</link>
<guid>https://arxiv.org/abs/2507.08885</guid>
<content:encoded><![CDATA[
arXiv:2507.08885v1 Announce Type: new 
Abstract: How to enable robots to predict the outcomes of their own motion intentions in three-dimensional space has been a fundamental problem in embodied intelligence. To explore more general spatial imagination capabilities, here we present AirScape, the first world model designed for six-degree-of-freedom aerial agents. AirScape predicts future observation sequences based on current visual inputs and motion intentions. Specifically, we construct an dataset for aerial world model training and testing, which consists of 11k video-intention pairs. This dataset includes first-person-view videos capturing diverse drone actions across a wide range of scenarios, with over 1,000 hours spent annotating the corresponding motion intentions. Then we develop a two-phase training schedule to train a foundation model -- initially devoid of embodied spatial knowledge -- into a world model that is controllable by motion intentions and adheres to physical spatio-temporal constraints.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Sequential Multi-Step Tasks with Parallel LLM Agents</title>
<link>https://arxiv.org/abs/2507.08944</link>
<guid>https://arxiv.org/abs/2507.08944</guid>
<content:encoded><![CDATA[
arXiv:2507.08944v1 Announce Type: new 
Abstract: Large language model (LLM)-based multi-agent systems have demonstrated remarkable promise for tackling complex tasks by breaking them down into subtasks that are iteratively planned, executed, observed, and refined. Despite their effectiveness, these systems often incur high latency because real-world problems frequently demand multiple iterative cycles of reasoning steps. To address this challenge, we propose M1-Parallel, a framework that concurrently runs multiple multi-agent teams in parallel to uncover distinct solution paths. By leveraging an event-driven communication model with asynchronous messaging, M1-Parallel efficiently capitalizes on the inherent diversity of valid plans to either reduce end-to-end latency or boost task completion rates. Our experiments on complex tasks show that M1-Parallel with early termination achieves up to $2.2\times$ speedup while preserving accuracy, and that M1-Parallel with aggregation yields higher task completion rates. We further investigate strategies aimed at encouraging diverse execution plans but observe no additional performance gains over repeated sampling. Overall, these findings underscore the potential of parallel plan execution for optimizing multi-agent systems for real-world, high-complexity reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Train a Leader: Hierarchical Reasoning in Multi-Agent LLMs</title>
<link>https://arxiv.org/abs/2507.08960</link>
<guid>https://arxiv.org/abs/2507.08960</guid>
<content:encoded><![CDATA[
arXiv:2507.08960v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved strong performance on a wide range of complex reasoning tasks, yet further gains are often possible by leveraging the complementary strengths of multiple models. While multi-agent frameworks can improve solution quality by leveraging multiple LLMs, existing methods are often computationally expensive, both at training and inference time. In this work, we introduce a hierarchical multi-agent framework that addresses these challenges by training only a single leader LLM to coordinate a team of untrained peer agents. To this end, we propose Multi-agent guided Leader Policy \textbf{O}ptimization (MLPO), a novel approach which trains the leader to evaluate and synthesize agent responses without auxiliary value networks or explicit agent feedback. Leaders trained with MLPO exhibit improved performance not only when interacting with the agent team at inference time, but also enjoy improved performance when deployed in single-agent settings without the team. Empirical results on Big-Bench Hard (BBH), MATH, and MMLU demonstrate that our framework achieves substantial performance improvements over both single-agent and multi-agent baselines. Our results highlight the effectiveness and efficiency of training a single, flexible leader for collaborative reasoning in multi-agent LLM systems.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Drug Discovery Through Agentic AI: A Multi-Agent Approach to Laboratory Automation in the DMTA Cycle</title>
<link>https://arxiv.org/abs/2507.09023</link>
<guid>https://arxiv.org/abs/2507.09023</guid>
<content:encoded><![CDATA[
arXiv:2507.09023v1 Announce Type: new 
Abstract: The pharmaceutical industry faces unprecedented challenges in drug discovery, with traditional approaches struggling to meet modern therapeutic development demands. This paper introduces a novel AI framework, Tippy, that transforms laboratory automation through specialized AI agents operating within the Design-Make-Test-Analyze (DMTA) cycle. Our multi-agent system employs five specialized agents - Supervisor, Molecule, Lab, Analysis, and Report, with Safety Guardrail oversight - each designed to excel in specific phases of the drug discovery pipeline. Tippy represents the first production-ready implementation of specialized AI agents for automating the DMTA cycle, providing a concrete example of how AI can transform laboratory workflows. By leveraging autonomous AI agents that reason, plan, and collaborate, we demonstrate how Tippy accelerates DMTA cycles while maintaining scientific rigor essential for pharmaceutical research. The system shows significant improvements in workflow efficiency, decision-making speed, and cross-disciplinary coordination, offering a new paradigm for AI-assisted drug discovery.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behavioral Exploration: Learning to Explore via In-Context Adaptation</title>
<link>https://arxiv.org/abs/2507.09041</link>
<guid>https://arxiv.org/abs/2507.09041</guid>
<content:encoded><![CDATA[
arXiv:2507.09041v1 Announce Type: new 
Abstract: Developing autonomous agents that quickly explore an environment and adapt their behavior online is a canonical challenge in robotics and machine learning. While humans are able to achieve such fast online exploration and adaptation, often acquiring new information and skills in only a handful of interactions, existing algorithmic approaches tend to rely on random exploration and slow, gradient-based behavior updates. How can we endow autonomous agents with such capabilities on par with humans? Taking inspiration from recent progress on both in-context learning and large-scale behavioral cloning, in this work we propose behavioral exploration: training agents to internalize what it means to explore and adapt in-context over the space of ``expert'' behaviors. To achieve this, given access to a dataset of expert demonstrations, we train a long-context generative model to predict expert actions conditioned on a context of past observations and a measure of how ``exploratory'' the expert's behaviors are relative to this context. This enables the model to not only mimic the behavior of an expert, but also, by feeding its past history of interactions into its context, to select different expert behaviors than what have been previously selected, thereby allowing for fast online adaptation and targeted, ``expert-like'' exploration. We demonstrate the effectiveness of our method in both simulated locomotion and manipulation settings, as well as on real-world robotic manipulation tasks, illustrating its ability to learn adaptive, exploratory behavior.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SetupBench: Assessing Software Engineering Agents' Ability to Bootstrap Development Environments</title>
<link>https://arxiv.org/abs/2507.09063</link>
<guid>https://arxiv.org/abs/2507.09063</guid>
<content:encoded><![CDATA[
arXiv:2507.09063v1 Announce Type: new 
Abstract: Modern Large Language Model (LLM) agents promise end to end assistance with real-world software tasks, yet existing benchmarks evaluate LLM agents almost exclusively in pre-baked environments where every dependency is pre-installed. To fill this gap, we introduce SetupBench, a 93 instance benchmark that isolates the environment-bootstrap skill: starting from a bare Linux sandbox, an agent must install packages, resolve dependency conflicts, initialize databases, and configure background services. Our tasks span seven language ecosystems, five database engines, and multi-service orchestration scenarios, each accompanies by a natural language problem statement and a deterministic success command. Through evaluation of OpenHands, a state-of-the-art coding agent, we find low success rates across task categories, with particular challenges in repository setup (38.9-57.4%) and local database configuration (20.0-53.3%). Our analysis reveals systematic failure modes including incomplete development tooling installation, hallucinated task constraints, and non-persistent environment modifications that break agent-human collaboration workflows. We identify substantial inefficiencies in agent exploration strategies, with 38-89% of actions being unnecessary compared to optimal human behavior. These findings highlight gaps in current agents' practical environment-bootstrap capabilities. By targeting this critical yet under-evaluated capability, SetupBench provides a rigorous yard-stick for the next generation of software developer agents aiming to solve end to end real-wold tasks.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Infinite Video Understanding</title>
<link>https://arxiv.org/abs/2507.09068</link>
<guid>https://arxiv.org/abs/2507.09068</guid>
<content:encoded><![CDATA[
arXiv:2507.09068v1 Announce Type: new 
Abstract: The rapid advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have ushered in remarkable progress in video understanding. However, a fundamental challenge persists: effectively processing and comprehending video content that extends beyond minutes or hours. While recent efforts like Video-XL-2 have demonstrated novel architectural solutions for extreme efficiency, and advancements in positional encoding such as HoPE and VideoRoPE++ aim to improve spatio-temporal understanding over extensive contexts, current state-of-the-art models still encounter significant computational and memory constraints when faced with the sheer volume of visual tokens from lengthy sequences. Furthermore, maintaining temporal coherence, tracking complex events, and preserving fine-grained details over extended periods remain formidable hurdles, despite progress in agentic reasoning systems like Deep Video Discovery. This position paper posits that a logical, albeit ambitious, next frontier for multimedia research is Infinite Video Understanding -- the capability for models to continuously process, understand, and reason about video data of arbitrary, potentially never-ending duration. We argue that framing Infinite Video Understanding as a blue-sky research objective provides a vital north star for the multimedia, and the wider AI, research communities, driving innovation in areas such as streaming architectures, persistent memory mechanisms, hierarchical and adaptive representations, event-centric reasoning, and novel evaluation paradigms. Drawing inspiration from recent work on long/ultra-long video understanding and several closely related fields, we outline the core challenges and key research directions towards achieving this transformative capability.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Synthetic Labs: Language Models as Auction Participants</title>
<link>https://arxiv.org/abs/2507.09083</link>
<guid>https://arxiv.org/abs/2507.09083</guid>
<content:encoded><![CDATA[
arXiv:2507.09083v1 Announce Type: new 
Abstract: This paper investigates the behavior of simulated AI agents (large language models, or LLMs) in auctions, introducing a novel synthetic data-generating process to help facilitate the study and design of auctions. We find that LLMs -- when endowed with chain of thought reasoning capacity -- agree with the experimental literature in auctions across a variety of classic auction formats. In particular, we find that LLM bidders produce results consistent with risk-averse human bidders; that they perform closer to theoretical predictions in obviously strategy-proof auctions; and, that they succumb to the winner's curse in common value settings. On prompting, we find that LLMs are not very sensitive to naive changes in prompts (e.g., language, currency) but can improve dramatically towards theoretical predictions with the right mental model (i.e., the language of Nash deviations). We run 1,000$+$ auctions for less than $\$$400 with GPT-4 models (three orders of magnitude cheaper than modern auction experiments) and develop a framework flexible enough to run auction experiments with any LLM model and a wide range of auction design specifications, facilitating further experimental study by decreasing costs and serving as a proof-of-concept for the use of LLM proxies.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer based Collaborative Reinforcement Learning for Fluid Antenna System (FAS)-enabled 3D UAV Positioning</title>
<link>https://arxiv.org/abs/2507.09094</link>
<guid>https://arxiv.org/abs/2507.09094</guid>
<content:encoded><![CDATA[
arXiv:2507.09094v1 Announce Type: new 
Abstract: In this paper, a novel Three dimensional (3D) positioning framework of fluid antenna system (FAS)-enabled unmanned aerial vehicles (UAVs) is developed. In the proposed framework, a set of controlled UAVs cooperatively estimate the real-time 3D position of a target UAV. Here, the active UAV transmits a measurement signal to the passive UAVs via the reflection from the target UAV. Each passive UAV estimates the distance of the active-target-passive UAV link and selects an antenna port to share the distance information with the base station (BS) that calculates the real-time position of the target UAV. As the target UAV is moving due to its task operation, the controlled UAVs must optimize their trajectories and select optimal antenna port, aiming to estimate the real-time position of the target UAV. We formulate this problem as an optimization problem to minimize the target UAV positioning error via optimizing the trajectories of all controlled UAVs and antenna port selection of passive UAVs. Here, an attention-based recurrent multi-agent reinforcement learning (AR-MARL) scheme is proposed, which enables each controlled UAV to use the local Q function to determine its trajectory and antenna port while optimizing the target UAV positioning performance without knowing the trajectories and antenna port selections of other controlled UAVs. Different from current MARL methods, the proposed method uses a recurrent neural network (RNN) that incorporates historical state-action pairs of each controlled UAV, and an attention mechanism to analyze the importance of these historical state-action pairs, thus improving the global Q function approximation accuracy and the target UAV positioning accuracy. Simulation results show that the proposed AR-MARL scheme can reduce the average positioning error by up to 17.5% and 58.5% compared to the VD-MARL scheme and the proposed method without FAS.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze</title>
<link>https://arxiv.org/abs/2507.09097</link>
<guid>https://arxiv.org/abs/2507.09097</guid>
<content:encoded><![CDATA[
arXiv:2507.09097v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) have demonstrated promising performance in chest X-ray (CXR) analysis. To enhance human-computer interaction, several studies have incorporated radiologists' eye gaze, typically through heatmaps or textual prompts. However, these methods often overlook the sequential order of eye movements, which could provide valuable insights by highlighting both the areas of interest and the order in which they are examined. In this work, we propose a novel approach called RadEyeVideo that integrates radiologists' eye-fixation data as a video sequence, capturing both the temporal and spatial dynamics of their gaze. We evaluate this method in CXR report generation and disease diagnosis using three general-domain, open-source LVLMs with video input capabilities. When prompted with eye-gaze videos, model performance improves by up to 24.6% in the report generation task and on average 15.2% for both tasks using scaled evaluation metrics. Notably, RadEyeVideo enhanced an open-domain LVLM model, LLaVA-OneVision, to surpass task-specific medical LVLMs such as MAIRA-2 and CheXagent, trained on large Chest X-ray data. This work highlights that domain expert's knowledge (eye-gaze information in this case), when effectively integrated with LVLMs, can significantly enhance general-domain models' capabilities in clinical tasks. RadEyeVideo is a step toward a scalable human-centered approach of utilizing LVLMs in medical image analytics.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data</title>
<link>https://arxiv.org/abs/2507.09100</link>
<guid>https://arxiv.org/abs/2507.09100</guid>
<content:encoded><![CDATA[
arXiv:2507.09100v1 Announce Type: new 
Abstract: In decision-making conversations, experts must navigate complex choices and make on-the-spot decisions while engaged in conversation. Although extensive historical data often exists, the real-time nature of these scenarios makes it infeasible for decision-makers to review and leverage relevant information. This raises an interesting question: What if experts could utilize relevant past data in real-time decision-making through insights derived from past data? To explore this, we implemented a conversational user interface, taking doctor-patient interactions as an example use case. Our system continuously listens to the conversation, identifies patient problems and doctor-suggested solutions, and retrieves related data from an embedded dataset, generating concise insights using a pipeline built around a retrieval-based Large Language Model (LLM) agent. We evaluated the prototype by embedding Health Canada datasets into a vector database and conducting simulated studies using sample doctor-patient dialogues, showing effectiveness but also challenges, setting directions for the next steps of our work.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proactive AI-and-RAN Workload Orchestration in O-RAN Architectures for 6G Networks</title>
<link>https://arxiv.org/abs/2507.09124</link>
<guid>https://arxiv.org/abs/2507.09124</guid>
<content:encoded><![CDATA[
arXiv:2507.09124v1 Announce Type: new 
Abstract: The vision of AI-RAN convergence, as advocated by the AI-RAN Alliance, aims to unlock a unified 6G platform capable of seamlessly supporting AI and RAN workloads over shared infrastructure. However, the architectural framework and intelligent resource orchestration strategies necessary to realize this vision remain largely unexplored. In this paper, we propose a Converged AI-and-ORAN Architectural (CAORA) framework based on O-RAN specifications, enabling the dynamic coexistence of real-time RAN and computationally intensive AI workloads. We design custom xApps within the Near-Real-Time RAN Intelligent Controller (NRT-RIC) to monitor RAN KPIs and expose radio analytics to an End-to-End (E2E) orchestrator via the recently introduced Y1 interface. The orchestrator incorporates workload forecasting and anomaly detection modules, augmenting a Soft Actor-Critic (SAC) reinforcement learning agent that proactively manages resource allocation, including Multi-Instance GPU (MIG) partitioning. Using real-world 5G traffic traces from Barcelona, our trace-driven simulations demonstrate that CAORA achieves near 99\% fulfillment of RAN demands, supports dynamic AI workloads, and maximizes infrastructure utilization even under highly dynamic conditions. Our results reveal that predictive orchestration significantly improves system adaptability, resource efficiency, and service continuity, offering a viable blueprint for future AI-and-RAN converged 6G systems.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study of Value-Aware Eigenoptions</title>
<link>https://arxiv.org/abs/2507.09127</link>
<guid>https://arxiv.org/abs/2507.09127</guid>
<content:encoded><![CDATA[
arXiv:2507.09127v1 Announce Type: new 
Abstract: Options, which impose an inductive bias toward temporal and hierarchical structure, offer a powerful framework for reinforcement learning (RL). While effective in sequential decision-making, they are often handcrafted rather than learned. Among approaches for discovering options, eigenoptions have shown strong performance in exploration, but their role in credit assignment remains underexplored. In this paper, we investigate whether eigenoptions can accelerate credit assignment in model-free RL, evaluating them in tabular and pixel-based gridworlds. We find that pre-specified eigenoptions aid not only exploration but also credit assignment, whereas online discovery can bias the agent's experience too strongly and hinder learning. In the context of deep RL, we also propose a method for learning option-values under non-linear function approximation, highlighting the impact of termination conditions on performance. Our findings reveal both the promise and complexity of using eigenoptions, and options more broadly, to simultaneously support credit assignment and exploration in reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization</title>
<link>https://arxiv.org/abs/2507.09160</link>
<guid>https://arxiv.org/abs/2507.09160</guid>
<content:encoded><![CDATA[
arXiv:2507.09160v1 Announce Type: new 
Abstract: Vision-Language-Action (VLA) models have shown remarkable achievements, driven by the rich implicit knowledge of their vision-language components. However, achieving generalist robotic agents demands precise grounding into physical interactions, especially in contact-rich scenarios where fine-grained force control is essential. We advance VLAs' implicit knowledge beyond identifying what to do, towards guiding how to physically interact with real world. This paper introduces Tactile-VLA, a novel framework that deeply fuses vision, language, action, and tactile sensing. This framework incorporates a hybrid position-force controller to translate the model's intentions into precise physical actions and a reasoning module that allows the robot to adapt its strategy based on tactile feedback. Experiments demonstrate Tactile-VLA's effectiveness and generalizability in three key aspects: (1) enabling tactile-aware instruction following, (2) utilizing tactile-relevant commonsense, and (3) facilitating adaptive tactile-involved reasoning. A key finding is that the VLM's prior knowledge already contains semantic understanding of physical interaction; by connecting it to the robot's tactile sensors with only a few demonstrations, we can activate this prior knowledge to achieve zero-shot generalization in contact-rich tasks.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking</title>
<link>https://arxiv.org/abs/2507.09174</link>
<guid>https://arxiv.org/abs/2507.09174</guid>
<content:encoded><![CDATA[
arXiv:2507.09174v1 Announce Type: new 
Abstract: The rapid proliferation of multimodal misinformation presents significant challenges for automated fact-checking systems, especially when claims are ambiguous or lack sufficient context. We introduce RAMA, a novel retrieval-augmented multi-agent framework designed for verifying multimedia misinformation. RAMA incorporates three core innovations: (1) strategic query formulation that transforms multimodal claims into precise web search queries; (2) cross-verification evidence aggregation from diverse, authoritative sources; and (3) a multi-agent ensemble architecture that leverages the complementary strengths of multiple multimodal large language models and prompt variants. Extensive experiments demonstrate that RAMA achieves superior performance on benchmark datasets, particularly excelling in resolving ambiguous or improbable claims by grounding verification in retrieved factual evidence. Our findings underscore the necessity of integrating web-based evidence and multi-agent reasoning for trustworthy multimedia verification, paving the way for more reliable and scalable fact-checking solutions. RAMA will be publicly available at https://github.com/kalendsyang/RAMA.git.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Reinforcement Learning by Planning with Online World Models</title>
<link>https://arxiv.org/abs/2507.09177</link>
<guid>https://arxiv.org/abs/2507.09177</guid>
<content:encoded><![CDATA[
arXiv:2507.09177v1 Announce Type: new 
Abstract: Continual reinforcement learning (CRL) refers to a naturalistic setting where an agent needs to endlessly evolve, by trial and error, to solve multiple tasks that are presented sequentially. One of the largest obstacles to CRL is that the agent may forget how to solve previous tasks when learning a new task, known as catastrophic forgetting. In this paper, we propose to address this challenge by planning with online world models. Specifically, we learn a Follow-The-Leader shallow model online to capture the world dynamics, in which we plan using model predictive control to solve a set of tasks specified by any reward functions. The online world model is immune to forgetting by construction with a proven regret bound of $\mathcal{O}(\sqrt{K^2D\log(T)})$ under mild assumptions. The planner searches actions solely based on the latest online model, thus forming a FTL Online Agent (OA) that updates incrementally. To assess OA, we further design Continual Bench, a dedicated environment for CRL, and compare with several strong baselines under the same model-planning algorithmic framework. The empirical results show that OA learns continuously to solve new tasks while not forgetting old skills, outperforming agents built on deep world models with various continual learning techniques.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System</title>
<link>https://arxiv.org/abs/2507.09179</link>
<guid>https://arxiv.org/abs/2507.09179</guid>
<content:encoded><![CDATA[
arXiv:2507.09179v1 Announce Type: new 
Abstract: Decentralized finance (DeFi) has introduced a new era of permissionless financial innovation but also led to unprecedented market manipulation. Without centralized oversight, malicious actors coordinate shilling campaigns and pump-and-dump schemes across various platforms. We propose a Multi-Agent Reinforcement Learning (MARL) framework for decentralized manipulation detection, modeling the interaction between manipulators and detectors as a dynamic adversarial game. This framework identifies suspicious patterns using delayed token price reactions as financial indicators.Our method introduces three innovations: (1) Group Relative Policy Optimization (GRPO) to enhance learning stability in sparse-reward and partially observable settings; (2) a theory-based reward function inspired by rational expectations and information asymmetry, differentiating price discovery from manipulation noise; and (3) a multi-modal agent pipeline that integrates LLM-based semantic features, social graph signals, and on-chain market data for informed decision-making.The framework is integrated within the Symphony system, a decentralized multi-agent architecture enabling peer-to-peer agent execution and trust-aware learning through distributed logs, supporting chain-verifiable evaluation. Symphony promotes adversarial co-evolution among strategic actors and maintains robust manipulation detection without centralized oracles, enabling real-time surveillance across global DeFi ecosystems.Trained on 100,000 real-world discourse episodes and validated in adversarial simulations, Hide-and-Shill achieves top performance in detection accuracy and causal attribution. This work bridges multi-agent systems with financial surveillance, advancing a new paradigm for decentralized market intelligence. All resources are available at the Hide-and-Shill GitHub repository to promote open research and reproducibility.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StockSim: A Dual-Mode Order-Level Simulator for Evaluating Multi-Agent LLMs in Financial Markets</title>
<link>https://arxiv.org/abs/2507.09255</link>
<guid>https://arxiv.org/abs/2507.09255</guid>
<content:encoded><![CDATA[
arXiv:2507.09255v1 Announce Type: new 
Abstract: We present StockSim, an open-source simulation platform for systematic evaluation of large language models (LLMs) in realistic financial decision-making scenarios. Unlike previous toolkits that offer limited scope, StockSim delivers a comprehensive system that fully models market dynamics and supports diverse simulation modes of varying granularity. It incorporates critical real-world factors, such as latency, slippage, and order-book microstructure, that were previously neglected, enabling more faithful and insightful assessment of LLM-based trading agents. An extensible, role-based agent framework supports heterogeneous trading strategies and multi-agent coordination, making StockSim a uniquely capable testbed for NLP research on reasoning under uncertainty and sequential decision-making. We open-source all our code at https: //github.com/harrypapa2002/StockSim.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents</title>
<link>https://arxiv.org/abs/2507.09329</link>
<guid>https://arxiv.org/abs/2507.09329</guid>
<content:encoded><![CDATA[
arXiv:2507.09329v1 Announce Type: new 
Abstract: LLM-based coding agents are rapidly being deployed in software development, yet their security implications remain poorly understood. These agents, while capable of accelerating software development, may inadvertently introduce insecure practices. We conducted the first systematic security evaluation of autonomous coding agents, analyzing over 12,000 actions across five state-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world software setup tasks. Our findings reveal significant security concerns: 21% of agent trajectories contained insecure actions, with models showing substantial variation in security behavior. We developed a high-precision detection system that identified four major vulnerability categories, with information exposure (CWE-200) being the most prevalent one. We also evaluated mitigation strategies including feedback mechanisms and security reminders with various effectiveness between models. GPT-4.1 demonstrated exceptional security awareness with 96.8% mitigation success. Our work provides the first comprehensive framework for evaluating coding agent security and highlights the need for security-aware design of next generation LLM-based coding agents.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulation for All: A Step-by-Step Cookbook for Developing Human-Centered Multi-Agent Transportation Simulators</title>
<link>https://arxiv.org/abs/2507.09367</link>
<guid>https://arxiv.org/abs/2507.09367</guid>
<content:encoded><![CDATA[
arXiv:2507.09367v1 Announce Type: new 
Abstract: As cities evolve toward more complex and multimodal transportation systems, the need for human-centered multi-agent simulation tools has never been more urgent. Yet most existing platforms remain limited - they often separate different types of road users, rely on scripted or pre-defined behaviors, overlook public transit users as active participants, and are rarely designed with accessibility in mind for non-technical users. To address this gap, this paper presents the specifications of a multi-agent simulation platform designed to support real-time, human-centered, and immersive studies of all road users, accompanied by open-source scripts for replication. Using high-fidelity immersive virtual environments, our platform enables interaction across public transit users, pedestrians, cyclists, automated vehicles, and drivers. The architecture is modular, extensible, and designed for accessibility. The system integrates hardware-specific modules - including an omnidirectional treadmill, a seating arrangement, a smart trainer, and an actuated cockpit. Additionally, the platform collects multimodal physiological, neurological, and behavioral data through embedded sensing devices such as functional near-infrared spectroscopy (fNIRS), eye tracking, and wrist-based biosensors. To show the usability of this system, we present three use cases. Simulation for All aims to lower the barrier to entry for high-fidelity transportation simulation, support experimentation across disciplines, and advance our understanding of multimodal mobility in complex urban environments.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constrained Style Learning from Imperfect Demonstrations under Task Optimality</title>
<link>https://arxiv.org/abs/2507.09371</link>
<guid>https://arxiv.org/abs/2507.09371</guid>
<content:encoded><![CDATA[
arXiv:2507.09371v1 Announce Type: new 
Abstract: Learning from demonstration has proven effective in robotics for acquiring natural behaviors, such as stylistic motions and lifelike agility, particularly when explicitly defining style-oriented reward functions is challenging. Synthesizing stylistic motions for real-world tasks usually requires balancing task performance and imitation quality. Existing methods generally depend on expert demonstrations closely aligned with task objectives. However, practical demonstrations are often incomplete or unrealistic, causing current methods to boost style at the expense of task performance. To address this issue, we propose formulating the problem as a constrained Markov Decision Process (CMDP). Specifically, we optimize a style-imitation objective with constraints to maintain near-optimal task performance. We introduce an adaptively adjustable Lagrangian multiplier to guide the agent to imitate demonstrations selectively, capturing stylistic nuances without compromising task performance. We validate our approach across multiple robotic platforms and tasks, demonstrating both robust task performance and high-fidelity style learning. On ANYmal-D hardware we show a 14.5% drop in mechanical energy and a more agile gait pattern, showcasing real-world benefits.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Conceptualization Impacts RAG Efficacy</title>
<link>https://arxiv.org/abs/2507.09389</link>
<guid>https://arxiv.org/abs/2507.09389</guid>
<content:encoded><![CDATA[
arXiv:2507.09389v1 Announce Type: new 
Abstract: Explainability and interpretability are cornerstones of frontier and next-generation artificial intelligence (AI) systems. This is especially true in recent systems, such as large language models (LLMs), and more broadly, generative AI. On the other hand, adaptability to new domains, contexts, or scenarios is also an important aspect for a successful system. As such, we are particularly interested in how we can merge these two efforts, that is, investigating the design of transferable and interpretable neurosymbolic AI systems. Specifically, we focus on a class of systems referred to as ''Agentic Retrieval-Augmented Generation'' systems, which actively select, interpret, and query knowledge sources in response to natural language prompts. In this paper, we systematically evaluate how different conceptualizations and representations of knowledge, particularly the structure and complexity, impact an AI agent (in this case, an LLM) in effectively querying a triplestore. We report our results, which show that there are impacts from both approaches, and we discuss their impact and implications.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing</title>
<link>https://arxiv.org/abs/2507.09407</link>
<guid>https://arxiv.org/abs/2507.09407</guid>
<content:encoded><![CDATA[
arXiv:2507.09407v1 Announce Type: new 
Abstract: We introduce the framework of LLM-Stackelberg games, a class of sequential decision-making models that integrate large language models (LLMs) into strategic interactions between a leader and a follower. Departing from classical Stackelberg assumptions of complete information and rational agents, our formulation allows each agent to reason through structured prompts, generate probabilistic behaviors via LLMs, and adapt their strategies through internal cognition and belief updates. We define two equilibrium concepts: reasoning and behavioral equilibrium, which aligns an agent's internal prompt-based reasoning with observable behavior, and conjectural reasoning equilibrium, which accounts for epistemic uncertainty through parameterized models over an opponent's response. These layered constructs capture bounded rationality, asymmetric information, and meta-cognitive adaptation. We illustrate the framework through a spearphishing case study, where a sender and a recipient engage in a deception game using structured reasoning prompts. This example highlights the cognitive richness and adversarial potential of LLM-mediated interactions. Our results show that LLM-Stackelberg games provide a powerful paradigm for modeling decision-making in domains such as cybersecurity, misinformation, and recommendation systems.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Social Learning using Theory of Mind</title>
<link>https://arxiv.org/abs/2507.09409</link>
<guid>https://arxiv.org/abs/2507.09409</guid>
<content:encoded><![CDATA[
arXiv:2507.09409v1 Announce Type: new 
Abstract: Social learning is a powerful mechanism through which agents learn about the world from others. However, humans don't always choose to observe others, since social learning can carry time and cognitive resource costs. How do people balance social and non-social learning? In this paper, we propose a rational mentalizing model of the decision to engage in social learning. This model estimates the utility of social learning by reasoning about the other agent's goal and the informativity of their future actions. It then weighs the utility of social learning against the utility of self-exploration (non-social learning). Using a multi-player treasure hunt game, we show that our model can quantitatively capture human trade-offs between social and non-social learning. Furthermore, our results indicate that these two components allow agents to flexibly apply social learning to achieve their goals more efficiently.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Multi-Person Motion Prediction by Lightweight Spatial and Temporal Interactions</title>
<link>https://arxiv.org/abs/2507.09446</link>
<guid>https://arxiv.org/abs/2507.09446</guid>
<content:encoded><![CDATA[
arXiv:2507.09446v1 Announce Type: new 
Abstract: 3D multi-person motion prediction is a highly complex task, primarily due to the dependencies on both individual past movements and the interactions between agents. Moreover, effectively modeling these interactions often incurs substantial computational costs. In this work, we propose a computationally efficient model for multi-person motion prediction by simplifying spatial and temporal interactions. Our approach begins with the design of lightweight dual branches that learn local and global representations for individual and multiple persons separately. Additionally, we introduce a novel cross-level interaction block to integrate the spatial and temporal representations from both branches. To further enhance interaction modeling, we explicitly incorporate the spatial inter-person distance embedding. With above efficient temporal and spatial design, we achieve state-of-the-art performance for multiple metrics on standard datasets of CMU-Mocap, MuPoTS-3D, and 3DPW, while significantly reducing the computational cost. Code is available at https://github.com/Yuanhong-Zheng/EMPMP.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MobiWorld: World Models for Mobile Wireless Network</title>
<link>https://arxiv.org/abs/2507.09462</link>
<guid>https://arxiv.org/abs/2507.09462</guid>
<content:encoded><![CDATA[
arXiv:2507.09462v1 Announce Type: new 
Abstract: Accurate modeling and simulation of mobile networks are essential for enabling intelligent and cost-effective network optimization. In this paper, we propose MobiWorld, a generative world model designed to support high-fidelity and flexible environment simulation for mobile network planning and optimization. Unlike traditional predictive models constrained by limited generalization capabilities, MobiWorld exhibits strong universality by integrating heterogeneous data sources, including sensors, mobile devices, and base stations, as well as multimodal data types such as sequences and images. It is capable of generating both network element-level observations (e.g., traffic load, user distribution) and system-level performance indicators (e.g., throughput, energy consumption) to support a wide range of planning and optimization tasks. Built upon advanced diffusion models, MobiWorld offers powerful controllable generation capabilities by modeling the joint distribution between mobile network data and diverse conditional factors including spatio temporal contexts, user behaviors, and optimization policies. This enables accurate simulation of dynamic network states under varying policy configurations, providing optimization agents with precise environmental feedback and facilitating effective decision-making without relying on costly real-network interactions. We demonstrate the effectiveness of MobiWorld in a collaborative energy-saving scenario, where an agent uses observations and rewards generated by MobiWorld to optimize base station sleep and user offloading policies. Experimental results show that MobiWorld exhibits strong controllable generation performance and outperforms traditional methods in energy optimization.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentive-Aware Dynamic Resource Allocation under Long-Term Cost Constraints</title>
<link>https://arxiv.org/abs/2507.09473</link>
<guid>https://arxiv.org/abs/2507.09473</guid>
<content:encoded><![CDATA[
arXiv:2507.09473v1 Announce Type: new 
Abstract: Motivated by applications such as cloud platforms allocating GPUs to users or governments deploying mobile health units across competing regions, we study the dynamic allocation of a reusable resource to strategic agents with private valuations. Our objective is to simultaneously (i) maximize social welfare, (ii) satisfy multi-dimensional long-term cost constraints, and (iii) incentivize truthful reporting. We begin by numerically evaluating primal-dual methods widely used in constrained online optimization and find them to be highly fragile in strategic settings -- agents can easily manipulate their reports to distort future dual updates for future gain.
  To address this vulnerability, we develop an incentive-aware framework that makes primal-dual methods robust to strategic behavior. Our design combines epoch-based lazy updates -- where dual variables remain fixed within each epoch -- with randomized exploration rounds that extract approximately truthful signals for learning. Leveraging carefully designed online learning subroutines that can be of independent interest for dual updates, our mechanism achieves $\tilde{\mathcal{O}}(\sqrt{T})$ social welfare regret, satisfies all cost constraints, and ensures incentive alignment. This matches the performance of non-strategic allocation approaches while being robust to strategic agents.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs</title>
<link>https://arxiv.org/abs/2507.09477</link>
<guid>https://arxiv.org/abs/2507.09477</guid>
<content:encoded><![CDATA[
arXiv:2507.09477v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language Models (LLMs) by injecting external knowledge, yet it falls short on problems that demand multi-step inference; conversely, purely reasoning-oriented approaches often hallucinate or mis-ground facts. This survey synthesizes both strands under a unified reasoning-retrieval perspective. We first map how advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then, we show how retrieved knowledge of different type supply missing premises and expand context for complex inference (RAG-Enhanced Reasoning). Finally, we spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs iteratively interleave search and reasoning to achieve state-of-the-art performance across knowledge-intensive benchmarks. We categorize methods, datasets, and open challenges, and outline research avenues toward deeper RAG-Reasoning systems that are more effective, multimodally-adaptive, trustworthy, and human-centric. The collection is available at https://github.com/DavidZWZ/Awesome-RAG-Reasoning.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLMs on Sequential API Call Through Automated Test Generation</title>
<link>https://arxiv.org/abs/2507.09481</link>
<guid>https://arxiv.org/abs/2507.09481</guid>
<content:encoded><![CDATA[
arXiv:2507.09481v1 Announce Type: new 
Abstract: By integrating tools from external APIs, Large Language Models (LLMs) have expanded their promising capabilities in a diverse spectrum of complex real-world tasks. However, testing, evaluation, and analysis of LLM tool use remain in their early stages. Most existing benchmarks rely on manually collected test cases, many of which cannot be automatically checked for semantic correctness and instead depend on static methods such as string matching. Additionally, these benchmarks often overlook the complex interactions that occur between sequential API calls, which are common in real-world applications. To fill the gap, in this paper, we introduce StateGen, an automated framework designed to generate diverse coding tasks involving sequential API interactions. StateGen combines state-machine-based API constraint solving and validation, energy-based sampling, and control-flow injection to generate executable programs. These programs are then translated into human-like natural language task descriptions through a collaboration of two LLM agents. Utilizing StateGen, we construct StateEval, a benchmark encompassing 120 verified test cases spanning across three representative scenarios: Session Service, Tensor Operation, and ElevenLabs MCP. Experimental results confirm that StateGen can effectively generate challenging and realistic API-oriented tasks, highlighting areas for improvement in current LLMs incorporating APIs.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective</title>
<link>https://arxiv.org/abs/2507.09495</link>
<guid>https://arxiv.org/abs/2507.09495</guid>
<content:encoded><![CDATA[
arXiv:2507.09495v1 Announce Type: new 
Abstract: Multi-agent reinforcement learning faces fundamental challenges that conventional approaches have failed to overcome: exponentially growing joint action spaces, non-stationary environments where simultaneous learning creates moving targets, and partial observability that constrains coordination. Current methods remain reactive, employing stimulus-response mechanisms that fail when facing novel scenarios. We argue for a transformative paradigm shift from reactive to proactive multi-agent intelligence through generative AI-based reinforcement learning. This position advocates reconceptualizing agents not as isolated policy optimizers, but as sophisticated generative models capable of synthesizing complex multi-agent dynamics and making anticipatory decisions based on predictive understanding of future interactions. Rather than responding to immediate observations, generative-RL agents can model environment evolution, predict other agents' behaviors, generate coordinated action sequences, and engage in strategic reasoning accounting for long-term dynamics. This approach leverages pattern recognition and generation capabilities of generative AI to enable proactive decision-making, seamless coordination through enhanced communication, and dynamic adaptation to evolving scenarios. We envision this paradigm shift will unlock unprecedented possibilities for distributed intelligence, moving beyond individual optimization toward emergent collective behaviors representing genuine collaborative intelligence. The implications extend across autonomous systems, robotics, and human-AI collaboration, promising solutions to coordination challenges intractable under traditional reactive frameworks.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities</title>
<link>https://arxiv.org/abs/2507.09497</link>
<guid>https://arxiv.org/abs/2507.09497</guid>
<content:encoded><![CDATA[
arXiv:2507.09497v1 Announce Type: new 
Abstract: Modern enterprise environments demand intelligent systems capable of handling complex, dynamic, and multi-faceted tasks with high levels of autonomy and adaptability. However, traditional single-purpose AI systems often lack sufficient coordination, memory reuse, and task decomposition capabilities, limiting their scalability in realistic settings. To address these challenges, we present \textbf{GoalfyMax}, a protocol-driven framework for end-to-end multi-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent (A2A) communication layer built on the Model Context Protocol (MCP), allowing independent agents to coordinate through asynchronous, protocol-compliant interactions. It incorporates the Experience Pack (XP) architecture, a layered memory system that preserves both task rationales and execution traces, enabling structured knowledge retention and continual learning. Moreover, our system integrates advanced features including multi-turn contextual dialogue, long-short term memory modules, and dynamic safety validation, supporting robust, real-time strategy adaptation. Empirical results on complex task orchestration benchmarks and case study demonstrate that GoalfyMax achieves superior adaptability, coordination, and experience reuse compared to baseline frameworks. These findings highlight its potential as a scalable, future-ready foundation for multi-agent intelligent systems.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TruckV2X: A Truck-Centered Perception Dataset</title>
<link>https://arxiv.org/abs/2507.09505</link>
<guid>https://arxiv.org/abs/2507.09505</guid>
<content:encoded><![CDATA[
arXiv:2507.09505v1 Announce Type: new 
Abstract: Autonomous trucking offers significant benefits, such as improved safety and reduced costs, but faces unique perception challenges due to trucks' large size and dynamic trailer movements. These challenges include extensive blind spots and occlusions that hinder the truck's perception and the capabilities of other road users. To address these limitations, cooperative perception emerges as a promising solution. However, existing datasets predominantly feature light vehicle interactions or lack multi-agent configurations for heavy-duty vehicle scenarios. To bridge this gap, we introduce TruckV2X, the first large-scale truck-centered cooperative perception dataset featuring multi-modal sensing (LiDAR and cameras) and multi-agent cooperation (tractors, trailers, CAVs, and RSUs). We further investigate how trucks influence collaborative perception needs, establishing performance benchmarks while suggesting research priorities for heavy vehicle perception. The dataset provides a foundation for developing cooperative perception systems with enhanced occlusion handling capabilities, and accelerates the deployment of multi-agent autonomous trucking systems. The TruckV2X dataset is available at https://huggingface.co/datasets/XieTenghu1/TruckV2X.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-supervised Pretraining for Integrated Prediction and Planning of Automated Vehicles</title>
<link>https://arxiv.org/abs/2507.09537</link>
<guid>https://arxiv.org/abs/2507.09537</guid>
<content:encoded><![CDATA[
arXiv:2507.09537v1 Announce Type: new 
Abstract: Predicting the future of surrounding agents and accordingly planning a safe, goal-directed trajectory are crucial for automated vehicles. Current methods typically rely on imitation learning to optimize metrics against the ground truth, often overlooking how scene understanding could enable more holistic trajectories. In this paper, we propose Plan-MAE, a unified pretraining framework for prediction and planning that capitalizes on masked autoencoders. Plan-MAE fuses critical contextual understanding via three dedicated tasks: reconstructing masked road networks to learn spatial correlations, agent trajectories to model social interactions, and navigation routes to capture destination intents. To further align vehicle dynamics and safety constraints, we incorporate a local sub-planning task predicting the ego-vehicle's near-term trajectory segment conditioned on earlier segment. This pretrained model is subsequently fine-tuned on downstream tasks to jointly generate the prediction and planning trajectories. Experiments on large-scale datasets demonstrate that Plan-MAE outperforms current methods on the planning metrics by a large margin and can serve as an important pre-training step for learning-based motion planner.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling</title>
<link>https://arxiv.org/abs/2507.09540</link>
<guid>https://arxiv.org/abs/2507.09540</guid>
<content:encoded><![CDATA[
arXiv:2507.09540v1 Announce Type: new 
Abstract: Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient alternatives to traditional Deep Neural Networks (DNNs) for real-time control systems. However, their training presents several challenges, particularly for reinforcement learning (RL) tasks, due to the non-differentiable nature of spike-based communication. In this work, we introduce what is, to our knowledge, the first framework that employs Metropolis-Hastings (MH) sampling, a Bayesian inference technique, to train SNNs for dynamical agent control in RL environments without relying on gradient-based methods. Our approach iteratively proposes and probabilistically accepts network parameter updates based on accumulated reward signals, effectively circumventing the limitations of backpropagation while enabling direct optimization on neuromorphic platforms. We evaluated this framework on two standard control benchmarks: AcroBot and CartPole. The results demonstrate that our MH-based approach outperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL approaches in terms of maximizing the accumulated reward while minimizing network resources and training episodes.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Existence of Fair and Efficient Allocation of Indivisible Chores</title>
<link>https://arxiv.org/abs/2507.09544</link>
<guid>https://arxiv.org/abs/2507.09544</guid>
<content:encoded><![CDATA[
arXiv:2507.09544v1 Announce Type: new 
Abstract: We study the problem of allocating indivisible chores among agents with additive cost functions in a fair and efficient manner. A major open question in this area is whether there always exists an allocation that is envy-free up to one chore (EF1) and Pareto optimal (PO). Our main contribution is to provide a positive answer to this question by proving the existence of such an allocation for indivisible chores under additive cost functions. This is achieved by a novel combination of a fixed point argument and a discrete algorithm, providing a significant methodological advance in this area.
  Our additional key contributions are as follows. We show that there always exists an allocation that is EF1 and fractional Pareto optimal (fPO), where fPO is a stronger efficiency concept than PO. We also show that an EF1 and PO allocation can be computed in polynomial time when the number of agents is constant. Finally, we extend all of these results to the more general setting of weighted EF1 (wEF1), which accounts for the entitlements of agents.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AICrypto: A Comprehensive Benchmark For Evaluating Cryptography Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2507.09580</link>
<guid>https://arxiv.org/abs/2507.09580</guid>
<content:encoded><![CDATA[
arXiv:2507.09580v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across a variety of domains. However, their applications in cryptography, which serves as a foundational pillar of cybersecurity, remain largely unexplored. To address this gap, we propose \textbf{AICrypto}, the first comprehensive benchmark designed to evaluate the cryptographic capabilities of LLMs. The benchmark comprises 135 multiple-choice questions, 150 capture-the-flag (CTF) challenges, and 18 proof problems, covering a broad range of skills from factual memorization to vulnerability exploitation and formal reasoning. All tasks are carefully reviewed or constructed by cryptography experts to ensure correctness and rigor. To support automated evaluation of CTF challenges, we design an agent-based framework. To gain deeper insight into the current state of cryptographic proficiency in LLMs, we introduce human expert performance baselines for comparison across all task types. Our evaluation of 17 leading LLMs reveals that state-of-the-art models match or even surpass human experts in memorizing cryptographic concepts, exploiting common vulnerabilities, and routine proofs. However, they still lack a deep understanding of abstract mathematical concepts and struggle with tasks that require multi-step reasoning and dynamic analysis. We hope this work could provide insights for future research on LLMs in cryptographic applications. Our code and dataset are available at https://aicryptobench.github.io.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>humancompatible.interconnect: Testing Properties of Repeated Uses of Interconnections of AI Systems</title>
<link>https://arxiv.org/abs/2507.09626</link>
<guid>https://arxiv.org/abs/2507.09626</guid>
<content:encoded><![CDATA[
arXiv:2507.09626v1 Announce Type: new 
Abstract: Artificial intelligence (AI) systems often interact with multiple agents. The regulation of such AI systems often requires that {\em a priori\/} guarantees of fairness and robustness be satisfied. With stochastic models of agents' responses to the outputs of AI systems, such {\em a priori\/} guarantees require non-trivial reasoning about the corresponding stochastic systems. Here, we present an open-source PyTorch-based toolkit for the use of stochastic control techniques in modelling interconnections of AI systems and properties of their repeated uses. It models robustness and fairness desiderata in a closed-loop fashion, and provides {\em a priori\/} guarantees for these interconnections. The PyTorch-based toolkit removes much of the complexity associated with the provision of fairness guarantees for closed-loop models of multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Negotiating Comfort: Simulating Personality-Driven LLM Agents in Shared Residential Social Networks</title>
<link>https://arxiv.org/abs/2507.09657</link>
<guid>https://arxiv.org/abs/2507.09657</guid>
<content:encoded><![CDATA[
arXiv:2507.09657v1 Announce Type: new 
Abstract: We use generative agents powered by large language models (LLMs) to simulate a social network in a shared residential building, driving the temperature decisions for a central heating system. Agents, divided into Family Members and Representatives, consider personal preferences, personal traits, connections, and weather conditions. Daily simulations involve family-level consensus followed by building-wide decisions among representatives. We tested three personality traits distributions (positive, mixed, and negative) and found that positive traits correlate with higher happiness and stronger friendships. Temperature preferences, assertiveness, and selflessness have a significant impact on happiness and decisions. This work demonstrates how LLM-driven agents can help simulate nuanced human behavior where complex real-life human simulations are difficult to set.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Networked Information Aggregation via Machine Learning</title>
<link>https://arxiv.org/abs/2507.09683</link>
<guid>https://arxiv.org/abs/2507.09683</guid>
<content:encoded><![CDATA[
arXiv:2507.09683v1 Announce Type: new 
Abstract: We study a distributed learning problem in which learning agents are embedded in a directed acyclic graph (DAG). There is a fixed and arbitrary distribution over feature/label pairs, and each agent or vertex in the graph is able to directly observe only a subset of the features -- potentially a different subset for every agent. The agents learn sequentially in some order consistent with a topological sort of the DAG, committing to a model mapping observations to predictions of the real-valued label. Each agent observes the predictions of their parents in the DAG, and trains their model using both the features of the instance that they directly observe, and the predictions of their parents as additional features. We ask when this process is sufficient to achieve \emph{information aggregation}, in the sense that some agent in the DAG is able to learn a model whose error is competitive with the best model that could have been learned (in some hypothesis class) with direct access to \emph{all} features, despite the fact that no single agent in the network has such access. We give upper and lower bounds for this problem for both linear and general hypothesis classes. Our results identify the \emph{depth} of the DAG as the key parameter: information aggregation can occur over sufficiently long paths in the DAG, assuming that all of the relevant features are well represented along the path, and there are distributions over which information aggregation cannot occur even in the linear case, and even in arbitrarily large DAGs that do not have sufficient depth (such as a hub-and-spokes topology in which the spoke vertices collectively see all the features). We complement our theoretical results with a comprehensive set of experiments.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token Compression Meets Compact Vision Transformers: A Survey and Comparative Evaluation for Edge AI</title>
<link>https://arxiv.org/abs/2507.09702</link>
<guid>https://arxiv.org/abs/2507.09702</guid>
<content:encoded><![CDATA[
arXiv:2507.09702v1 Announce Type: new 
Abstract: Token compression techniques have recently emerged as powerful tools for accelerating Vision Transformer (ViT) inference in computer vision. Due to the quadratic computational complexity with respect to the token sequence length, these methods aim to remove less informative tokens before the attention layers to improve inference throughput. While numerous studies have explored various accuracy-efficiency trade-offs on large-scale ViTs, two critical gaps remain. First, there is a lack of unified survey that systematically categorizes and compares token compression approaches based on their core strategies (e.g., pruning, merging, or hybrid) and deployment settings (e.g., fine-tuning vs. plug-in). Second, most benchmarks are limited to standard ViT models (e.g., ViT-B, ViT-L), leaving open the question of whether such methods remain effective when applied to structurally compressed transformers, which are increasingly deployed on resource-constrained edge devices. To address these gaps, we present the first systematic taxonomy and comparative study of token compression methods, and we evaluate representative techniques on both standard and compact ViT architectures. Our experiments reveal that while token compression methods are effective for general-purpose ViTs, they often underperform when directly applied to compact designs. These findings not only provide practical insights but also pave the way for future research on adapting token optimization techniques to compact transformer-based networks for edge AI and AI agent applications.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IteraOptiRacing: A Unified Planning-Control Framework for Real-time Autonomous Racing for Iterative Optimal Performance</title>
<link>https://arxiv.org/abs/2507.09714</link>
<guid>https://arxiv.org/abs/2507.09714</guid>
<content:encoded><![CDATA[
arXiv:2507.09714v1 Announce Type: new 
Abstract: This paper presents a unified planning-control strategy for competing with other racing cars called IteraOptiRacing in autonomous racing environments. This unified strategy is proposed based on Iterative Linear Quadratic Regulator for Iterative Tasks (i2LQR), which can improve lap time performance in the presence of surrounding racing obstacles. By iteratively using the ego car's historical data, both obstacle avoidance for multiple moving cars and time cost optimization are considered in this unified strategy, resulting in collision-free and time-optimal generated trajectories. The algorithm's constant low computation burden and suitability for parallel computing enable real-time operation in competitive racing scenarios. To validate its performance, simulations in a high-fidelity simulator are conducted with multiple randomly generated dynamic agents on the track. Results show that the proposed strategy outperforms existing methods across all randomly generated autonomous racing scenarios, enabling enhanced maneuvering for the ego racing car.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward accurate RUL and SOH estimation using reinforced graph-based PINNs enhanced with dynamic weights</title>
<link>https://arxiv.org/abs/2507.09766</link>
<guid>https://arxiv.org/abs/2507.09766</guid>
<content:encoded><![CDATA[
arXiv:2507.09766v1 Announce Type: new 
Abstract: Accurate estimation of Remaining Useful Life (RUL) and State of Health (SOH) is essential for Prognostics and Health Management (PHM) across a wide range of industrial applications. We propose a novel framework -- Reinforced Graph-Based Physics-Informed Neural Networks Enhanced with Dynamic Weights (RGPD) -- that combines physics-based supervision with advanced spatio-temporal learning. Graph Convolutional Recurrent Networks (GCRNs) embed graph-convolutional filters within recurrent units to capture how node representations evolve over time. Graph Attention Convolution (GATConv) leverages a self-attention mechanism to compute learnable, edge-wise attention coefficients, dynamically weighting neighbor contributions for adaptive spatial aggregation. A Soft Actor-Critic (SAC) module is positioned between the Temporal Attention Unit (TAU) and GCRN to further improve the spatio-temporal learning. This module improves attention and prediction accuracy by dynamically scaling hidden representations to minimize noise and highlight informative features. To identify the most relevant physical constraints in each area, Q-learning agents dynamically assign weights to physics-informed loss terms, improving generalization across real-time industrial systems and reducing the need for manual tuning. In both RUL and SOH estimation tasks, the proposed method consistently outperforms state-of-the-art models, demonstrating strong robustness and predictive accuracy across varied degradation patterns across three diverse industrial benchmark datasets.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit</title>
<link>https://arxiv.org/abs/2507.09788</link>
<guid>https://arxiv.org/abs/2507.09788</guid>
<content:encoded><![CDATA[
arXiv:2507.09788v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLM) have led to a new class of autonomous agents, renewing and expanding interest in the area. LLM-powered Multiagent Systems (MAS) have thus emerged, both for assistive and simulation purposes, yet tools for realistic human behavior simulation -- with its distinctive challenges and opportunities -- remain underdeveloped. Existing MAS libraries and tools lack fine-grained persona specifications, population sampling facilities, experimentation support, and integrated validation, among other key capabilities, limiting their utility for behavioral studies, social simulation, and related applications. To address these deficiencies, in this work we introduce TinyTroupe, a simulation toolkit enabling detailed persona definitions (e.g., nationality, age, occupation, personality, beliefs, behaviors) and programmatic control via numerous LLM-driven mechanisms. This allows for the concise formulation of behavioral problems of practical interest, either at the individual or group level, and provides effective means for their solution. TinyTroupe's components are presented using representative working examples, such as brainstorming and market research sessions, thereby simultaneously clarifying their purpose and demonstrating their usefulness. Quantitative and qualitative evaluations of selected aspects are also provided, highlighting possibilities, limitations, and trade-offs. The approach, though realized as a specific Python implementation, is meant as a novel conceptual contribution, which can be partially or fully incorporated in other contexts. The library is available as open source at https://github.com/microsoft/tinytroupe.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Probing with Multimodal Predictions for Motion Planning</title>
<link>https://arxiv.org/abs/2507.09822</link>
<guid>https://arxiv.org/abs/2507.09822</guid>
<content:encoded><![CDATA[
arXiv:2507.09822v1 Announce Type: new 
Abstract: Navigation in dynamic environments requires autonomous systems to reason about uncertainties in the behavior of other agents. In this paper, we introduce a unified framework that combines trajectory planning with multimodal predictions and active probing to enhance decision-making under uncertainty. We develop a novel risk metric that seamlessly integrates multimodal prediction uncertainties through mixture models. When these uncertainties follow a Gaussian mixture distribution, we prove that our risk metric admits a closed-form solution, and is always finite, thus ensuring analytical tractability. To reduce prediction ambiguity, we incorporate an active probing mechanism that strategically selects actions to improve its estimates of behavioral parameters of other agents, while simultaneously handling multimodal uncertainties. We extensively evaluate our framework in autonomous navigation scenarios using the MetaDrive simulation environment. Results demonstrate that our active probing approach successfully navigates complex traffic scenarios with uncertain predictions. Additionally, our framework shows robust performance across diverse traffic agent behavior models, indicating its broad applicability to real-world autonomous navigation challenges. Code and videos are available at https://darshangm.github.io/papers/active-probing-multimodal-predictions/.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-residual Mixture of Experts Learning for Cooperative Control in Multi-vehicle Systems</title>
<link>https://arxiv.org/abs/2507.09836</link>
<guid>https://arxiv.org/abs/2507.09836</guid>
<content:encoded><![CDATA[
arXiv:2507.09836v1 Announce Type: new 
Abstract: Autonomous vehicles (AVs) are becoming increasingly popular, with their applications now extending beyond just a mode of transportation to serving as mobile actuators of a traffic flow to control flow dynamics. This contrasts with traditional fixed-location actuators, such as traffic signals, and is referred to as Lagrangian traffic control. However, designing effective Lagrangian traffic control policies for AVs that generalize across traffic scenarios introduces a major challenge. Real-world traffic environments are highly diverse, and developing policies that perform robustly across such diverse traffic scenarios is challenging. It is further compounded by the joint complexity of the multi-agent nature of traffic systems, mixed motives among participants, and conflicting optimization objectives subject to strict physical and external constraints. To address these challenges, we introduce Multi-Residual Mixture of Expert Learning (MRMEL), a novel framework for Lagrangian traffic control that augments a given suboptimal nominal policy with a learned residual while explicitly accounting for the structure of the traffic scenario space. In particular, taking inspiration from residual reinforcement learning, MRMEL augments a suboptimal nominal AV control policy by learning a residual correction, but at the same time dynamically selects the most suitable nominal policy from a pool of nominal policies conditioned on the traffic scenarios and modeled as a mixture of experts. We validate MRMEL using a case study in cooperative eco-driving at signalized intersections in Atlanta, Dallas Fort Worth, and Salt Lake City, with real-world data-driven traffic scenarios. The results show that MRMEL consistently yields superior performance-achieving an additional 4%-9% reduction in aggregate vehicle emissions relative to the strongest baseline in each setting.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Population Models</title>
<link>https://arxiv.org/abs/2507.09901</link>
<guid>https://arxiv.org/abs/2507.09901</guid>
<content:encoded><![CDATA[
arXiv:2507.09901v1 Announce Type: new 
Abstract: Many of society's most pressing challenges, from pandemic response to supply chain disruptions to climate adaptation, emerge from the collective behavior of millions of autonomous agents making decisions over time. Large Population Models (LPMs) offer an approach to understand these complex systems by simulating entire populations with realistic behaviors and interactions at unprecedented scale. LPMs extend traditional modeling approaches through three key innovations: computational methods that efficiently simulate millions of agents simultaneously, mathematical frameworks that learn from diverse real-world data streams, and privacy-preserving communication protocols that bridge virtual and physical environments. This allows researchers to observe how agent behavior aggregates into system-level outcomes and test interventions before real-world implementation. While current AI advances primarily focus on creating "digital humans" with sophisticated individual capabilities, LPMs develop "digital societies" where the richness of interactions reveals emergent phenomena. By bridging individual agent behavior and population-scale dynamics, LPMs offer a complementary path in AI research illuminating collective intelligence and providing testing grounds for policies and social innovations before real-world deployment. We discuss the technical foundations and some open problems here. LPMs are implemented by the AgentTorch framework (github.com/AgentTorch/AgentTorch)
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Stable Swarm-Based Inertial Algorithms for Optimization</title>
<link>https://arxiv.org/abs/2507.09909</link>
<guid>https://arxiv.org/abs/2507.09909</guid>
<content:encoded><![CDATA[
arXiv:2507.09909v1 Announce Type: new 
Abstract: We formulate the swarming optimization problem as a weakly coupled, dissipative dynamical system governed by a controlled energy dissipation rate and initial velocities that adhere to the nonequilibrium Onsager principle. In this framework, agents' inertia, positions, and masses are dynamically coupled. To numerically solve the system, we develop a class of efficient, energy-stable algorithms that either preserve or enhance energy dissipation at the discrete level. At equilibrium, the system tends to converge toward one of the lowest local minima explored by the agents, thereby improving the likelihood of identifying the global minimum. Numerical experiments confirm the effectiveness of the proposed approach, demonstrating significant advantages over traditional swarm-based gradient descent methods, especially when operating with a limited number of agents.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Quantal Response Equilibrium: Existence and Efficient Learning</title>
<link>https://arxiv.org/abs/2507.09928</link>
<guid>https://arxiv.org/abs/2507.09928</guid>
<content:encoded><![CDATA[
arXiv:2507.09928v1 Announce Type: new 
Abstract: We introduce a new solution concept for bounded rational agents in finite normal-form general-sum games called Generalized Quantal Response Equilibrium (GQRE) which generalizes Quantal Response Equilibrium~\citep{mckelvey1995quantal}. In our setup, each player maximizes a smooth, regularized expected utility of the mixed profiles used, reflecting bounded rationality that subsumes stochastic choice. After establishing existence under mild conditions, we present computationally efficient no-regret independent learning via smoothened versions of the Frank-Wolfe algorithm. Our algorithm uses noisy but correlated gradient estimates generated via a simulation oracle that reports on repeated plays of the game. We analyze convergence properties of our algorithm under assumptions that ensure uniqueness of equilibrium, using a class of gap functions that generalize the Nash gap. We end by demonstrating the effectiveness of our method on a set of complex general-sum games such as high-rank two-player games, large action two-player games, and known examples of difficult multi-player games.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient</title>
<link>https://arxiv.org/abs/2507.09989</link>
<guid>https://arxiv.org/abs/2507.09989</guid>
<content:encoded><![CDATA[
arXiv:2507.09989v1 Announce Type: new 
Abstract: In heterogeneous multi-agent reinforcement learning (MARL), achieving monotonic improvement plays a pivotal role in enhancing performance. The HAPPO algorithm proposes a feasible solution by introducing a sequential update scheme, which requires independent learning with No Parameter-sharing (NoPS). However, heterogeneous MARL generally requires Partial Parameter-sharing (ParPS) based on agent grouping to achieve high cooperative performance. Our experiments prove that directly combining ParPS with the sequential update scheme leads to the policy updating baseline drift problem, thereby failing to achieve improvement. To solve the conflict between monotonic improvement and ParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG) algorithm. First, we replace the sequentially computed $Q_{\psi}^s(s,a_{1:i})$ with the Optimal Marginal Q (OMQ) function $\phi_{\psi}^*(s,a_{1:i})$ derived from Q-functions. This maintains MAAD's monotonic improvement while eliminating the conflict through optimal joint action sequences instead of sequential policy ratio calculations. Second, we introduce the Generalized Q Critic (GQC) as the critic function, employing pessimistic uncertainty-constrained loss to optimize different Q-value estimations. This provides the required Q-values for OMQ computation and stable baselines for actor updates. Finally, we implement a Centralized Critic Grouped Actor (CCGA) architecture that simultaneously achieves ParPS in local policy networks and accurate global Q-function computation. Experimental results in SMAC and MAMuJoCo environments demonstrate that OMDPG outperforms various state-of-the-art MARL baselines.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictive &amp; Trust-based Multi-Agent Coordination</title>
<link>https://arxiv.org/abs/2507.09997</link>
<guid>https://arxiv.org/abs/2507.09997</guid>
<content:encoded><![CDATA[
arXiv:2507.09997v1 Announce Type: new 
Abstract: This paper presents a trust-based predictive multi-agent consensus protocol that analyses neighbours' anticipation data and makes coordination decisions. Agents in the network share their future predicted data over a finite look-ahead horizon with their neighbours and update their predictions in a rolling-horizon fashion. The prediction data is then used by agents to learn both the trust and the commitment traits exhibited by their neighbours over time. The proposed protocol is named as the Anticipatory Distributed Coordination (ADC) protocol. Lyapunov theory-based agreement convergence between agents is provided, followed by demonstrations using numerical simulations.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model</title>
<link>https://arxiv.org/abs/2507.10000</link>
<guid>https://arxiv.org/abs/2507.10000</guid>
<content:encoded><![CDATA[
arXiv:2507.10000v1 Announce Type: new 
Abstract: Since Searle's work deconstructing intent and intentionality in the realm of philosophy, the practical meaning of intent has received little attention in science and technology. Intentionality and context are both central to the scope of Promise Theory's model of Semantic Spacetime, used as an effective Tiny Language Model. One can identify themes and concepts from a text, on a low level (without knowledge of the specific language) by using process coherence as a guide. Any agent process can assess superficially a degree of latent `intentionality' in data by looking for anomalous multi-scale anomalies and assessing the work done to form them. Scale separation can be used to sort parts into `intended' content and `ambient context', using the spacetime coherence as a measure. This offers an elementary but pragmatic interpretation of latent intentionality for very low computational cost, and without reference to extensive training or reasoning capabilities. The process is well within the reach of basic organisms as it does not require large scale artificial probabilistic batch processing. The level of concept formation depends, however, on the memory capacity of the agent.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Man Behind the Sound: Demystifying Audio Private Attribute Profiling via Multimodal Large Language Model Agents</title>
<link>https://arxiv.org/abs/2507.10016</link>
<guid>https://arxiv.org/abs/2507.10016</guid>
<content:encoded><![CDATA[
arXiv:2507.10016v1 Announce Type: new 
Abstract: Our research uncovers a novel privacy risk associated with multimodal large language models (MLLMs): the ability to infer sensitive personal attributes from audio data -- a technique we term audio private attribute profiling. This capability poses a significant threat, as audio can be covertly captured without direct interaction or visibility. Moreover, compared to images and text, audio carries unique characteristics, such as tone and pitch, which can be exploited for more detailed profiling. However, two key challenges exist in understanding MLLM-employed private attribute profiling from audio: (1) the lack of audio benchmark datasets with sensitive attribute annotations and (2) the limited ability of current MLLMs to infer such attributes directly from audio. To address these challenges, we introduce AP^2, an audio benchmark dataset that consists of two subsets collected and composed from real-world data, and both are annotated with sensitive attribute labels. Additionally, we propose Gifts, a hybrid multi-agent framework that leverages the complementary strengths of audio-language models (ALMs) and large language models (LLMs) to enhance inference capabilities. Gifts employs an LLM to guide the ALM in inferring sensitive attributes, then forensically analyzes and consolidates the ALM's inferences, overcoming severe hallucinations of existing ALMs in generating long-context responses. Our evaluations demonstrate that Gifts significantly outperforms baseline approaches in inferring sensitive attributes. Finally, we investigate model-level and data-level defense strategies to mitigate the risks of audio private attribute profiling. Our work validates the feasibility of audio-based privacy attacks using MLLMs, highlighting the need for robust defenses, and provides a dataset and framework to facilitate future research.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finetuning Deep Reinforcement Learning Policies with Evolutionary Strategies for Control of Underactuated Robots</title>
<link>https://arxiv.org/abs/2507.10030</link>
<guid>https://arxiv.org/abs/2507.10030</guid>
<content:encoded><![CDATA[
arXiv:2507.10030v1 Announce Type: new 
Abstract: Deep Reinforcement Learning (RL) has emerged as a powerful method for addressing complex control problems, particularly those involving underactuated robotic systems. However, in some cases, policies may require refinement to achieve optimal performance and robustness aligned with specific task objectives. In this paper, we propose an approach for fine-tuning Deep RL policies using Evolutionary Strategies (ES) to enhance control performance for underactuated robots. Our method involves initially training an RL agent with Soft-Actor Critic (SAC) using a surrogate reward function designed to approximate complex specific scoring metrics. We subsequently refine this learned policy through a zero-order optimization step employing the Separable Natural Evolution Strategy (SNES), directly targeting the original score. Experimental evaluations conducted in the context of the 2nd AI Olympics with RealAIGym at IROS 2024 demonstrate that our evolutionary fine-tuning significantly improves agent performance while maintaining high robustness. The resulting controllers outperform established baselines, achieving competitive scores for the competition tasks.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review</title>
<link>https://arxiv.org/abs/2507.10142</link>
<guid>https://arxiv.org/abs/2507.10142</guid>
<content:encoded><![CDATA[
arXiv:2507.10142v1 Announce Type: new 
Abstract: Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in coordinating multiple agents across simulated benchmarks and constrained scenarios. However, its deployment in real-world multi-agent systems (MAS) remains limited, primarily due to the complex and dynamic nature of such environments. These challenges arise from multiple interacting sources of variability, including fluctuating agent populations, evolving task goals, and inconsistent execution conditions. Together, these factors demand that MARL algorithms remain effective under continuously changing system configurations and operational demands. To better capture and assess this capacity for adjustment, we introduce the concept of \textit{adaptability} as a unified and practically grounded lens through which to evaluate the reliability of MARL algorithms under shifting conditions, broadly referring to any changes in the environment dynamics that may occur during learning or execution. Centred on the notion of adaptability, we propose a structured framework comprising three key dimensions: learning adaptability, policy adaptability, and scenario-driven adaptability. By adopting this adaptability perspective, we aim to support more principled assessments of MARL performance beyond narrowly defined benchmarks. Ultimately, this survey contributes to the development of algorithms that are better suited for deployment in dynamic, real-world multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Play Style Identification Using Low-Level Representations of Play Traces in MicroRTS</title>
<link>https://arxiv.org/abs/2507.10172</link>
<guid>https://arxiv.org/abs/2507.10172</guid>
<content:encoded><![CDATA[
arXiv:2507.10172v1 Announce Type: new 
Abstract: Play style identification can provide valuable game design insights and enable adaptive experiences, with the potential to improve game playing agents. Previous work relies on domain knowledge to construct play trace representations using handcrafted features. More recent approaches incorporate the sequential structure of play traces but still require some level of domain abstraction. In this study, we explore the use of unsupervised CNN-LSTM autoencoder models to obtain latent representations directly from low-level play trace data in MicroRTS. We demonstrate that this approach yields a meaningful separation of different game playing agents in the latent space, reducing reliance on domain expertise and its associated biases. This latent space is then used to guide the exploration of diverse play styles within studied AI players.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToMacVF : Temporal Macro-action Value Factorization for Asynchronous Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.10251</link>
<guid>https://arxiv.org/abs/2507.10251</guid>
<content:encoded><![CDATA[
arXiv:2507.10251v1 Announce Type: new 
Abstract: Existing asynchronous MARL methods based on MacDec-POMDP typically construct training trajectory buffers by simply sampling limited and biased data at the endpoints of macro-actions, and directly apply conventional MARL methods on the buffers. As a result, these methods lead to an incomplete and inaccurate representation of the macro-action execution process, along with unsuitable credit assignments. To solve these problems, the Temporal Macro-action Value Factorization (ToMacVF) is proposed to achieve fine-grained temporal credit assignment for macro-action contributions. A centralized training buffer, called Macro-action Segmented Joint Experience Replay Trajectory (Mac-SJERT), is designed to incorporate with ToMacVF to collect accurate and complete macro-action execution information, supporting a more comprehensive and precise representation of the macro-action process. To ensure principled and fine-grained asynchronous value factorization, the consistency requirement between joint and individual macro-action selection called Temporal Macro-action based IGM (To-Mac-IGM) is formalized, proving that it generalizes the synchronous cases. Based on To-Mac-IGM, a modularized ToMacVF architecture, which satisfies CTDE principle, is designed to conveniently integrate previous value factorization methods. Next, the ToMacVF algorithm is devised as an implementation of the ToMacVF architecture. Experimental results demonstrate that, compared to asynchronous baselines, our ToMacVF algorithm not only achieves optimal performance but also exhibits strong adaptability and robustness across various asynchronous multi-agent experimental scenarios.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Informed Reinforcement Learning for Visual Coverage Path Planning</title>
<link>https://arxiv.org/abs/2507.10284</link>
<guid>https://arxiv.org/abs/2507.10284</guid>
<content:encoded><![CDATA[
arXiv:2507.10284v1 Announce Type: new 
Abstract: Visual coverage path planning with unmanned aerial vehicles (UAVs) requires agents to strategically coordinate UAV motion and camera control to maximize coverage, minimize redundancy, and maintain battery efficiency. Traditional reinforcement learning (RL) methods rely on environment-specific reward formulations that lack semantic adaptability. This study proposes Prompt-Informed Reinforcement Learning (PIRL), a novel approach that integrates the zero-shot reasoning ability and in-context learning capability of large language models with curiosity-driven RL. PIRL leverages semantic feedback from an LLM, GPT-3.5, to dynamically shape the reward function of the Proximal Policy Optimization (PPO) RL policy guiding the agent in position and camera adjustments for optimal visual coverage. The PIRL agent is trained using OpenAI Gym and evaluated in various environments. Furthermore, the sim-to-real-like ability and zero-shot generalization of the agent are tested by operating the agent in Webots simulator which introduces realistic physical dynamics. Results show that PIRL outperforms multiple learning-based baselines such as PPO with static rewards, PPO with exploratory weight initialization, imitation learning, and an LLM-only controller. Across different environments, PIRL outperforms the best-performing baseline by achieving up to 14% higher visual coverage in OpenAI Gym and 27% higher in Webots, up to 25% higher battery efficiency, and up to 18\% lower redundancy, depending on the environment. The results highlight the effectiveness of LLM-guided reward shaping in complex spatial exploration tasks and suggest a promising direction for integrating natural language priors into RL for robotics.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toolsuite for Implementing Multiagent Systems Based on Communication Protocols</title>
<link>https://arxiv.org/abs/2507.10324</link>
<guid>https://arxiv.org/abs/2507.10324</guid>
<content:encoded><![CDATA[
arXiv:2507.10324v1 Announce Type: new 
Abstract: Interaction-Oriented Programming (IOP) is an approach to building a multiagent system by modeling the interactions between its roles via a flexible interaction protocol and implementing agents to realize the interactions of the roles they play in the protocol.
  In recent years, we have developed an extensive suite of software that enables multiagent system developers to apply IOP. These include tools for efficiently verifying protocols for properties such as liveness and safety and middleware that simplifies the implementation of agents. This paper presents some of that software suite.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Acquiring and Adapting Priors for Novel Tasks via Neural Meta-Architectures</title>
<link>https://arxiv.org/abs/2507.10446</link>
<guid>https://arxiv.org/abs/2507.10446</guid>
<content:encoded><![CDATA[
arXiv:2507.10446v1 Announce Type: new 
Abstract: The ability to transfer knowledge from prior experiences to novel tasks stands as a pivotal capability of intelligent agents, including both humans and computational models. This principle forms the basis of transfer learning, where large pre-trained neural networks are fine-tuned to adapt to downstream tasks. Transfer learning has demonstrated tremendous success, both in terms of task adaptation speed and performance. However there are several domains where, due to lack of data, training such large pre-trained models or foundational models is not a possibility - computational chemistry, computational immunology, and medical imaging are examples. To address these challenges, our work focuses on designing architectures to enable efficient acquisition of priors when large amounts of data are unavailable. In particular, we demonstrate that we can use neural memory to enable adaptation on non-stationary distributions with only a few samples. Then we demonstrate that our hypernetwork designs (a network that generates another network) can acquire more generalizable priors than standard networks when trained with Model Agnostic Meta-Learning (MAML). Subsequently, we apply hypernetworks to 3D scene generation, demonstrating that they can acquire priors efficiently on just a handful of training scenes, thereby leading to faster text-to-3D generation. We then extend our hypernetwork framework to perform 3D segmentation on novel scenes with limited data by efficiently transferring priors from earlier viewed scenes. Finally, we repurpose an existing molecular generative method as a pre-training framework that facilitates improved molecular property prediction, addressing critical challenges in computational immunology
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinTeam: A Multi-Agent Collaborative Intelligence System for Comprehensive Financial Scenarios</title>
<link>https://arxiv.org/abs/2507.10448</link>
<guid>https://arxiv.org/abs/2507.10448</guid>
<content:encoded><![CDATA[
arXiv:2507.10448v1 Announce Type: new 
Abstract: Financial report generation tasks range from macro- to micro-economics analysis, also requiring extensive data analysis. Existing LLM models are usually fine-tuned on simple QA tasks and cannot comprehensively analyze real financial scenarios. Given the complexity, financial companies often distribute tasks among departments. Inspired by this, we propose FinTeam, a financial multi-agent collaborative system, with a workflow with four LLM agents: document analyzer, analyst, accountant, and consultant. We train these agents with specific financial expertise using constructed datasets. We evaluate FinTeam on comprehensive financial tasks constructed from real online investment forums, including macroeconomic, industry, and company analysis. The human evaluation shows that by combining agents, the financial reports generate from FinTeam achieved a 62.00% acceptance rate, outperforming baseline models like GPT-4o and Xuanyuan. Additionally, FinTeam's agents demonstrate a 7.43% average improvement on FinCUGE and a 2.06% accuracy boost on FinEval. Project is available at https://github.com/FudanDISC/DISC-FinLLM/.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepResearch$^{\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology</title>
<link>https://arxiv.org/abs/2507.10522</link>
<guid>https://arxiv.org/abs/2507.10522</guid>
<content:encoded><![CDATA[
arXiv:2507.10522v1 Announce Type: new 
Abstract: We introduce DeepResearch$^{\text{Eco}}$, a novel agentic LLM-based system for automated scientific synthesis that supports recursive, depth- and breadth-controlled exploration of original research questions -- enhancing search diversity and nuance in the retrieval of relevant scientific literature. Unlike conventional retrieval-augmented generation pipelines, DeepResearch enables user-controllable synthesis with transparent reasoning and parameter-driven configurability, facilitating high-throughput integration of domain-specific evidence while maintaining analytical rigor. Applied to 49 ecological research questions, DeepResearch achieves up to a 21-fold increase in source integration and a 14.9-fold rise in sources integrated per 1,000 words. High-parameter settings yield expert-level analytical depth and contextual diversity.
  Source code available at: https://github.com/sciknoworg/deep-research.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph World Model</title>
<link>https://arxiv.org/abs/2507.10539</link>
<guid>https://arxiv.org/abs/2507.10539</guid>
<content:encoded><![CDATA[
arXiv:2507.10539v1 Announce Type: new 
Abstract: World models (WMs) demonstrate strong capabilities in prediction, generation, and planning tasks. Existing WMs primarily focus on unstructured data and cannot leverage the ubiquitous structured data, often represented as graphs, in the digital world. While multiple graph foundation models have been proposed, they focus on graph learning tasks and cannot extend to diverse multi-modal data and interdisciplinary tasks. To address these challenges, we propose the Graph World Model (GWM), a world model that supports both unstructured and graph-structured states with multi-modal information and represents diverse tasks as actions. The core of a GWM is a generic message-passing algorithm to aggregate structured information, either over a unified multi-modal token space by converting multi-modal data into text (GWM-T) or a unified multi-modal embedding space by modality-specific encoders (GWM-E). Notably, GWM introduces action nodes to support diverse tasks, where action nodes are linked to other nodes via direct reference or similarity computation. Extensive experiments on six tasks from diverse domains, including multi-modal generation and matching, recommendation, graph prediction, multi-agent, retrieval-augmented generation, and planning and optimization, show that the same GWM outperforms or matches domain-specific baselines' performance, benefits from multi-hop structures, and demonstrates strong zero-shot/few-shot capabilities on unseen new tasks. Our code for GWM is released at https://github.com/ulab-uiuc/GWM.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmbRACE-3K: Embodied Reasoning and Action in Complex Environments</title>
<link>https://arxiv.org/abs/2507.10548</link>
<guid>https://arxiv.org/abs/2507.10548</guid>
<content:encoded><![CDATA[
arXiv:2507.10548v1 Announce Type: new 
Abstract: Recent advanced vision-language models(VLMs) have demonstrated strong performance on passive, offline image and video understanding tasks. However, their effectiveness in embodied settings, which require online interaction and active scene understanding remains limited. In such scenarios, an agent perceives the environment from a first-person perspective, with each action dynamically shaping subsequent observations. Even state-of-the-art models such as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment interactions, exhibiting clear limitations in spatial reasoning and long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset of over 3,000 language-guided tasks situated in diverse, photorealistic environments constructed using Unreal Engine and the UnrealCV-Zoo framework. The tasks encompass a wide range of embodied challenges, including navigation, object manipulation, and multi-stage goal execution. Each task unfolds as a multi-step trajectory, pairing first-person visual observations with high-level instructions, grounded actions, and natural language rationales that express the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to evaluate the embodied reasoning capabilities of VLMs across three key dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage Goal Execution. In zero-shot settings, all models achieve success rates below 20%, underscoring the challenge posed by our benchmark and the current limitations of VLMs in interactive environments. To demonstrate the utility of EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning followed by reinforcement learning. This approach yields substantial improvements across all three challenge categories, highlighting the dataset's effectiveness in enabling the development of embodied reasoning capabilities.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Literature and the Universe Via A Multi-Agent Large Language Model System</title>
<link>https://arxiv.org/abs/2507.08958</link>
<guid>https://arxiv.org/abs/2507.08958</guid>
<content:encoded><![CDATA[
arXiv:2507.08958v1 Announce Type: cross 
Abstract: As cosmological simulations and their associated software become increasingly complex, physicists face the challenge of searching through vast amounts of literature and user manuals to extract simulation parameters from dense academic papers, each using different models and formats. Translating these parameters into executable scripts remains a time-consuming and error-prone process. To improve efficiency in physics research and accelerate the cosmological simulation process, we introduce SimAgents, a multi-agent system designed to automate both parameter configuration from the literature and preliminary analysis for cosmology research. SimAgents is powered by specialized LLM agents capable of physics reasoning, simulation software validation, and tool execution. These agents collaborate through structured communication, ensuring that extracted parameters are physically meaningful, internally consistent, and software-compliant. We also construct a cosmological parameter extraction evaluation dataset by collecting over 40 simulations in published papers from Arxiv and leading journals that cover diverse simulation types. Experiments on the dataset demonstrate a strong performance of SimAgents, highlighting its effectiveness and potential to accelerate scientific research for physicists. Our demonstration video is available at: https://youtu.be/w1zLpm_CaWA. The complete system and dataset are publicly available at https://github.com/xwzhang98/SimAgents.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolution of Fear and Social Rewards in Prey-Predator Relationship</title>
<link>https://arxiv.org/abs/2507.09992</link>
<guid>https://arxiv.org/abs/2507.09992</guid>
<content:encoded><![CDATA[
arXiv:2507.09992v1 Announce Type: cross 
Abstract: Fear is a critical brain function for detecting danger and learning to avoid specific stimuli that can lead to danger. While fear is believed to have evolved under pressure from predators, experimentally reproducing the evolution is challenging. To investigate the relationship between environmental conditions, the evolution of fear, and the evolution of other rewards, such as food reward and social reward, we developed a distributed evolutionary simulation. In our simulation, prey and predator agents co-evolve their innate reward functions, including a possibly fear-like term for observing predators, and learn behaviors via reinforcement learning. Surprisingly, our simulation revealed that social reward for observing the same species is more important for prey to survive, and fear-like negative reward for observing predators evolves only after acquiring social reward. We also found that the predator with increased hunting ability (larger mouth) amplified fear emergence, but also that fear evolution is more stable with non-evolving predators that are bad at chasing prey. Additionally, unlike for predators, we found that positive rewards evolve in opposition to fear for stationary threats, as areas with abundant leftover food develop around them. These findings suggest that fear and social reward have had a complex interplay with each other through evolution, along with the nature of predators and threats.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A PBN-RL-XAI Framework for Discovering a "Hit-and-Run'' Therapeutic Strategy in Melanoma</title>
<link>https://arxiv.org/abs/2507.10136</link>
<guid>https://arxiv.org/abs/2507.10136</guid>
<content:encoded><![CDATA[
arXiv:2507.10136v1 Announce Type: cross 
Abstract: Innate resistance to anti-PD-1 immunotherapy remains a major clinical challenge in metastatic melanoma, with the underlying molecular networks being poorly understood. To address this, we constructed a dynamic Probabilistic Boolean Network model using transcriptomic data from patient tumor biopsies to elucidate the regulatory logic governing therapy response. We then employed a reinforcement learning agent to systematically discover optimal, multi-step therapeutic interventions and used explainable artificial intelligence to mechanistically interpret the agent's control policy. The analysis revealed that a precisely timed, 4-step temporary inhibition of the lysyl oxidase like 2 protein (LOXL2) was the most effective strategy. Our explainable analysis showed that this ``hit-and-run" intervention is sufficient to erase the molecular signature driving resistance, allowing the network to self-correct without requiring sustained intervention. This study presents a novel, time-dependent therapeutic hypothesis for overcoming immunotherapy resistance and provides a powerful computational framework for identifying non-obvious intervention protocols in complex biological systems.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Biases for Interpretable Fairness in Offline and Online Classifiers</title>
<link>https://arxiv.org/abs/2507.10154</link>
<guid>https://arxiv.org/abs/2507.10154</guid>
<content:encoded><![CDATA[
arXiv:2507.10154v1 Announce Type: cross 
Abstract: Predictive models often reinforce biases which were originally embedded in their training data, through skewed decisions. In such cases, mitigation methods are critical to ensure that, regardless of the prevailing disparities, model outcomes are adjusted to be fair. To assess this, datasets could be systematically generated with specific biases, to train machine learning classifiers. Then, predictive outcomes could aid in the understanding of this bias embedding process. Hence, an agent-based model (ABM), depicting a loan application process that represents various systemic biases across two demographic groups, was developed to produce synthetic datasets. Then, by applying classifiers trained on them to predict loan outcomes, we can assess how biased data leads to unfairness. This highlights a main contribution of this work: a framework for synthetic dataset generation with controllable bias injection. We also contribute with a novel explainability technique, which shows how mitigations affect the way classifiers leverage data features, via second-order Shapley values. In experiments, both offline and online learning approaches are employed. Mitigations are applied at different stages of the modelling pipeline, such as during pre-processing and in-processing.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaboration Promotes Group Resilience in Multi-Agent AI</title>
<link>https://arxiv.org/abs/2111.06614</link>
<guid>https://arxiv.org/abs/2111.06614</guid>
<content:encoded><![CDATA[
arXiv:2111.06614v3 Announce Type: replace 
Abstract: To effectively operate in various dynamic scenarios, RL agents must be resilient to unexpected changes in their environment. Previous work on this form of resilience has focused on single-agent settings. In this work, we introduce and formalize a multi-agent variant of resilience, which we term group resilience. We further hypothesize that collaboration with other agents is key to achieving group resilience; collaborating agents adapt better to environmental perturbations in multi-agent reinforcement learning (MARL) settings. We test our hypothesis empirically by evaluating different collaboration protocols and examining their effect on group resilience. Our experiments show that all the examined collaborative approaches achieve higher group resilience than their non-collaborative counterparts.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Player Zero-Sum Markov Games with Networked Separable Interactions</title>
<link>https://arxiv.org/abs/2307.09470</link>
<guid>https://arxiv.org/abs/2307.09470</guid>
<content:encoded><![CDATA[
arXiv:2307.09470v3 Announce Type: replace 
Abstract: We study a new class of Markov games, \emph(multi-player) zero-sum Markov Games} with \emph{Networked separable interactions} (zero-sum NMGs), to model the local interaction structure in non-cooperative multi-agent sequential decision-making. We define a zero-sum NMG as a model where {the payoffs of the auxiliary games associated with each state are zero-sum and} have some separable (i.e., polymatrix) structure across the neighbors over some interaction network. We first identify the necessary and sufficient conditions under which an MG can be presented as a zero-sum NMG, and show that the set of Markov coarse correlated equilibrium (CCE) collapses to the set of Markov Nash equilibrium (NE) in these games, in that the product of per-state marginalization of the former for all players yields the latter. Furthermore, we show that finding approximate Markov \emph{stationary} CCE in infinite-horizon discounted zero-sum NMGs is \texttt{PPAD}-hard, unless the underlying network has a ``star topology''. Then, we propose fictitious-play-type dynamics, the classical learning dynamics in normal-form games, for zero-sum NMGs, and establish convergence guarantees to Markov stationary NE under a star-shaped network structure. Finally, in light of the hardness result, we focus on computing a Markov \emph{non-stationary} NE and provide finite-iteration guarantees for a series of value-iteration-based algorithms. We also provide numerical experiments to corroborate our theoretical results.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Review of Reward Functions for Reinforcement Learning in the context of Autonomous Driving</title>
<link>https://arxiv.org/abs/2405.01440</link>
<guid>https://arxiv.org/abs/2405.01440</guid>
<content:encoded><![CDATA[
arXiv:2405.01440v2 Announce Type: replace 
Abstract: Reinforcement learning has emerged as an important approach for autonomous driving. A reward function is used in reinforcement learning to establish the learned skill objectives and guide the agent toward the optimal policy. Since autonomous driving is a complex domain with partly conflicting objectives with varying degrees of priority, developing a suitable reward function represents a fundamental challenge. This paper aims to highlight the gap in such function design by assessing different proposed formulations in the literature and dividing individual objectives into Safety, Comfort, Progress, and Traffic Rules compliance categories. Additionally, the limitations of the reviewed reward functions are discussed, such as objectives aggregation and indifference to driving context. Furthermore, the reward categories are frequently inadequately formulated and lack standardization. This paper concludes by proposing future research that potentially addresses the observed shortcomings in rewards, including a reward validation framework and structured rewards that are context-aware and able to resolve conflicts.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equilibria in multiagent online problems with predictions</title>
<link>https://arxiv.org/abs/2405.11873</link>
<guid>https://arxiv.org/abs/2405.11873</guid>
<content:encoded><![CDATA[
arXiv:2405.11873v3 Announce Type: replace 
Abstract: We study the power of (competitive) algorithms with predictions in a multiagent setting. To this goal, we introduce a multiagent version of the ski-rental problem. In this problem agents can collaborate by pooling resources to get a group license for some asset. If the license price is not met then agents have to rent the asset individually for the day at a unit price. Otherwise the license becomes available forever to everyone at no extra cost.
  We investigate the effect of using predictors for self and others' behavior in such a setting, as well as the new equilibria formed in this way.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LSTP-Nav: Lightweight Spatiotemporal Policy for Map-free Multi-agent Navigation with LiDAR</title>
<link>https://arxiv.org/abs/2408.16370</link>
<guid>https://arxiv.org/abs/2408.16370</guid>
<content:encoded><![CDATA[
arXiv:2408.16370v4 Announce Type: replace 
Abstract: Safe and efficient multi-agent navigation in dynamic environments remains inherently challenging, particularly when real-time decision-making is required on resource-constrained platforms. Ensuring collision-free trajectories while adapting to uncertainties without relying on pre-built maps further complicates real-world deployment. To address these challenges, we propose LSTP-Nav, a lightweight end-to-end policy for multi-agent navigation that enables map-free collision avoidance in complex environments by directly mapping raw LiDAR point clouds to motion commands. At the core of this framework lies LSTP-Net, an efficient network that processes raw LiDAR data using a GRU architecture, enhanced with attention mechanisms to dynamically focus on critical environmental features while minimizing computational overhead. Additionally, a novel HS reward optimizes collision avoidance by incorporating angular velocity, prioritizing obstacles along the predicted heading, and enhancing training stability. To narrow the sim-to-real gap, we develop PhysReplay-Simlab, a physics-realistic multi-agent simulator, employs localized replay to mine near-failure experiences. Relying solely on LiDA, LSTP-Nav achieves efficient zero-shot sim-to-real transfer on a CPU-only robotic platform, enabling robust navigation in dynamic environments while maintaining computation frequencies above 40 Hz. Extensive experiments demonstrate that LSTP-Nav outperforms baselines with a 9.58\% higher success rate and a 12.30\% lower collision rate, underscoring its practicality and robustness for real-world applications.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentially Private Gradient-Tracking-Based Distributed Stochastic Optimization over Directed Graphs</title>
<link>https://arxiv.org/abs/2501.06793</link>
<guid>https://arxiv.org/abs/2501.06793</guid>
<content:encoded><![CDATA[
arXiv:2501.06793v2 Announce Type: replace 
Abstract: This paper proposes a differentially private gradient-tracking-based distributed stochastic optimization algorithm over directed graphs. In particular, privacy noises are incorporated into each agent's state and tracking variable to mitigate information leakage, after which the perturbed states and tracking variables are transmitted to neighbors. We design two novel schemes for the step-sizes and the sampling number within the algorithm. The sampling parameter-controlled subsampling method employed by both schemes enhances the differential privacy level, and ensures a finite cumulative privacy budget even over infinite iterations. The algorithm achieves both almost sure and mean square convergence for nonconvex objectives. Furthermore, when nonconvex objectives satisfy the Polyak-Lojasiewicz condition, Scheme (S1) achieves a polynomial mean square convergence rate, and Scheme (S2) achieves an exponential mean square convergence rate. The trade-off between privacy and convergence is presented. The effectiveness of the algorithm and its superior performance compared to existing works are illustrated through numerical examples of distributed training on the benchmark datasets "MNIST" and "CIFAR-10".
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACEBench: Who Wins the Match Point in Tool Usage?</title>
<link>https://arxiv.org/abs/2501.12851</link>
<guid>https://arxiv.org/abs/2501.12851</guid>
<content:encoded><![CDATA[
arXiv:2501.12851v5 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated significant potential in decision-making and reasoning, particularly when integrated with various tools to effectively solve complex problems. However, existing benchmarks for evaluating LLMs' tool usage face several limitations: (1) limited evaluation scenarios, often lacking assessments in real multi-turn dialogue contexts; (2) narrow evaluation dimensions, with insufficient detailed assessments of how LLMs use tools; and (3) reliance on LLMs or real API executions for evaluation, which introduces significant overhead. To address these challenges, we introduce ACEBench, a comprehensive benchmark for assessing tool usage in LLMs. ACEBench categorizes data into three primary types based on evaluation methodology: Normal, Special, and Agent. "Normal" evaluates tool usage in basic scenarios; "Special" evaluates tool usage in situations with ambiguous or incomplete instructions; "Agent" evaluates tool usage through multi-agent interactions to simulate real-world, multi-turn dialogues. We conducted extensive experiments using ACEBench, analyzing various LLMs in-depth and providing a more granular examination of error causes across different data types.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language Models with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.18439</link>
<guid>https://arxiv.org/abs/2502.18439</guid>
<content:encoded><![CDATA[
arXiv:2502.18439v2 Announce Type: replace 
Abstract: Leveraging multiple large language models (LLMs) to build collaborative multi-agentic workflows has demonstrated significant potential. However, most previous studies focus on prompting the out-of-the-box LLMs, relying on their innate capability for collaboration, which may not improve LLMs' performance as shown recently. In this paper, we introduce a new post-training paradigm MAPoRL (Multi-Agent Post-co-training for collaborative LLMs with Reinforcement Learning), to explicitly elicit the collaborative behaviors and further unleash the power of multi-agentic LLM frameworks. In MAPoRL, multiple LLMs first generate their own responses independently and engage in a multi-turn discussion to collaboratively improve the final answer. In the end, a MAPoRL verifier evaluates both the answer and the discussion, by assigning a score that verifies the correctness of the answer, while adding incentives to encourage corrective and persuasive discussions. The score serves as the co-training reward, and is then maximized through multi-agent RL. Unlike existing LLM post-training paradigms, MAPoRL advocates the co-training of multiple LLMs together using RL for better generalization. Accompanied by analytical insights, our experiments demonstrate that training individual LLMs alone is insufficient to induce effective collaboration. In contrast, multi-agent co-training can boost the collaboration performance across benchmarks, with generalization to unseen domains.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>It's Not All Black and White: Degree of Truthfulness for Risk-Avoiding Agents</title>
<link>https://arxiv.org/abs/2502.18805</link>
<guid>https://arxiv.org/abs/2502.18805</guid>
<content:encoded><![CDATA[
arXiv:2502.18805v2 Announce Type: replace 
Abstract: The classic notion of \emph{truthfulness} requires that no agent has a profitable manipulation -- an untruthful report that, for \emph{some} combination of reports of the other agents, increases her utility. This strong notion implicitly assumes that the manipulating agent either knows what all other agents are going to report, or is willing to take the risk and act as-if she knows their reports.
  Without knowledge of the others' reports, most manipulations are \emph{risky} -- they might decrease the manipulator's utility for some other combinations of reports by the other agents. Accordingly, a recent paper (Bu, Song and Tao, ``On the existence of truthful fair cake cutting mechanisms'', Artificial Intelligence 319 (2023), 103904) suggests a relaxed notion, which we refer to as \emph{risk-avoiding truthfulness (RAT)}, which requires only that no agent can gain from a \emph{safe} manipulation -- one that is sometimes beneficial and never harmful.
  Truthfulness and RAT are two extremes: the former considers manipulators with complete knowledge of others, whereas the latter considers manipulators with no knowledge at all. In reality, agents often know about some -- but not all -- of the other agents. This paper introduces the \emph{RAT-degree} of a mechanism, defined as the smallest number of agents whose reports, if known, may allow another agent to safely manipulate, or $n$ if there is no such number. This notion interpolates between classic truthfulness (degree $n$) and RAT (degree at least $1$): a mechanism with a higher RAT-degree is harder to manipulate safely.
  To illustrate the generality and applicability of this concept, we analyze the RAT-degree of prominent mechanisms across various social choice settings, including auctions, indivisible goods allocations, cake-cutting, voting, and two-sided matching.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cryptis: Cryptographic Reasoning in Separation Logic</title>
<link>https://arxiv.org/abs/2502.21156</link>
<guid>https://arxiv.org/abs/2502.21156</guid>
<content:encoded><![CDATA[
arXiv:2502.21156v2 Announce Type: replace 
Abstract: We introduce Cryptis, an extension of the Iris separation logic that can be used to verify cryptographic components using the symbolic model of cryptography. The combination of separation logic and cryptographic reasoning allows us to prove the correctness of a protocol and later reuse this result to verify larger systems that rely on the protocol. To make this integration possible, we propose novel specifications for authentication protocols that allow agents in a network to agree on the use of system resources. We evaluate our approach by verifying various authentication protocols and a key-value store server that uses these authentication protocols to connect to clients. Our results are formalized in Coq.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can A Society of Generative Agents Simulate Human Behavior and Inform Public Health Policy? A Case Study on Vaccine Hesitancy</title>
<link>https://arxiv.org/abs/2503.09639</link>
<guid>https://arxiv.org/abs/2503.09639</guid>
<content:encoded><![CDATA[
arXiv:2503.09639v4 Announce Type: replace 
Abstract: Can we simulate a sandbox society with generative agents to model human behavior, thereby reducing the over-reliance on real human trials for assessing public policies? In this work, we investigate the feasibility of simulating health-related decision-making, using vaccine hesitancy, defined as the delay in acceptance or refusal of vaccines despite the availability of vaccination services (MacDonald, 2015), as a case study. To this end, we introduce the VacSim framework with 100 generative agents powered by Large Language Models (LLMs). VacSim simulates vaccine policy outcomes with the following steps: 1) instantiate a population of agents with demographics based on census data; 2) connect the agents via a social network and model vaccine attitudes as a function of social dynamics and disease-related information; 3) design and evaluate various public health interventions aimed at mitigating vaccine hesitancy. To align with real-world results, we also introduce simulation warmup and attitude modulation to adjust agents' attitudes. We propose a series of evaluations to assess the reliability of various LLM simulations. Experiments indicate that models like Llama and Qwen can simulate aspects of human behavior but also highlight real-world alignment challenges, such as inconsistent responses with demographic profiles. This early exploration of LLM-driven simulations is not meant to serve as definitive policy guidance; instead, it serves as a call for action to examine social simulation for policy development.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LVAgent: Long Video Understanding by Multi-Round Dynamical Collaboration of MLLM Agents</title>
<link>https://arxiv.org/abs/2503.10200</link>
<guid>https://arxiv.org/abs/2503.10200</guid>
<content:encoded><![CDATA[
arXiv:2503.10200v3 Announce Type: replace 
Abstract: Existing MLLMs encounter significant challenges in modeling the temporal context within long videos. Currently, mainstream Agent-based methods use external tools to assist a single MLLM in answering long video questions. Despite such tool-based support, a solitary MLLM still offers only a partial understanding of long videos, resulting in limited performance. In order to better address long video tasks, we introduce LVAgent, the first framework enabling multi-round dynamic collaboration of MLLM agents in long video understanding. Our method consists of four key steps: 1) Selection: We pre-select appropriate agents from the model library to form optimal agent teams based on different tasks. 2) Perception: We design an effective retrieval scheme for long videos to improve the coverage of critical temporal segments while maintaining computational efficiency. 3) Action: Agents answer long video questions and exchange reasons. 4) Reflection: We evaluate each agent's performance in each round of discussion and optimize the agent team for dynamic collaboration. The agents iteratively refine their answers by multi-round dynamical collaboration of MLLM agents. LVAgent is the first agent system method that outperforms all closed-source models (like GPT-4o) and open-source models (like InternVL-2.5 and Qwen2-VL) in the long video understanding tasks. Our LVAgent achieves an accuracy of 80\% on four mainstream long video understanding tasks. Notably, LVAgent improves accuracy by 13.3\% on LongVideoBench. Code is available at https://github.com/64327069/LVAgent.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OS-Kairos: Adaptive Interaction for MLLM-Powered GUI Agents</title>
<link>https://arxiv.org/abs/2503.16465</link>
<guid>https://arxiv.org/abs/2503.16465</guid>
<content:encoded><![CDATA[
arXiv:2503.16465v3 Announce Type: replace 
Abstract: Autonomous graphical user interface (GUI) agents powered by multimodal large language models have shown great promise. However, a critical yet underexplored issue persists: over-execution, where the agent executes tasks in a fully autonomous way, without adequate assessment of its action confidence to compromise an adaptive human-agent collaboration. This poses substantial risks in complex scenarios, such as those involving ambiguous user instructions, unexpected interruptions, and environmental hijacks. To address the issue, we introduce OS-Kairos, an adaptive GUI agent capable of predicting confidence levels at each interaction step and efficiently deciding whether to act autonomously or seek human intervention. OS-Kairos is developed through two key mechanisms: (i) collaborative probing that annotates confidence scores at each interaction step; (ii) confidence-driven interaction that leverages these confidence scores to elicit the ability of adaptive interaction. Experimental results show that OS-Kairos substantially outperforms existing models on our curated dataset featuring complex scenarios, as well as on established benchmarks such as AITZ and Meta-GUI, with 24.59\%$\sim$87.29\% improvements in task success rate. OS-Kairos facilitates an adaptive human-agent collaboration, prioritizing effectiveness, generality, scalability, and efficiency for real-world GUI interaction. The dataset and codes are available at https://github.com/Wuzheng02/OS-Kairos.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthesizing world models for bilevel planning</title>
<link>https://arxiv.org/abs/2503.20124</link>
<guid>https://arxiv.org/abs/2503.20124</guid>
<content:encoded><![CDATA[
arXiv:2503.20124v2 Announce Type: replace 
Abstract: Modern reinforcement learning (RL) systems have demonstrated remarkable capabilities in complex environments, such as video games. However, they still fall short of achieving human-like sample efficiency and adaptability when learning new domains. Theory-based reinforcement learning (TBRL) is an algorithmic framework specifically designed to address this gap. Modeled on cognitive theories, TBRL leverages structured, causal world models - "theories" - as forward simulators for use in planning, generalization and exploration. Although current TBRL systems provide compelling explanations of how humans learn to play video games, they face several technical limitations: their theory languages are restrictive, and their planning algorithms are not scalable. To address these challenges, we introduce TheoryCoder, an instantiation of TBRL that exploits hierarchical representations of theories and efficient program synthesis methods for more powerful learning and planning. TheoryCoder equips agents with general-purpose abstractions (e.g., "move to"), which are then grounded in a particular environment by learning a low-level transition model (a Python program synthesized from observations by a large language model). A bilevel planning algorithm can exploit this hierarchical structure to solve large domains. We demonstrate that this approach can be successfully applied to diverse and challenging grid-world games, where approaches based on directly synthesizing a policy perform poorly. Ablation studies demonstrate the benefits of using hierarchical abstractions.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VFlow: Discovering Optimal Agentic Workflows for Verilog Generation</title>
<link>https://arxiv.org/abs/2504.03723</link>
<guid>https://arxiv.org/abs/2504.03723</guid>
<content:encoded><![CDATA[
arXiv:2504.03723v2 Announce Type: replace 
Abstract: Hardware design automation faces challenges in generating high-quality Verilog code efficiently. This paper introduces VFlow, an automated framework that optimizes agentic workflows for Verilog code generation. Unlike traditional approaches relying on fixed prompts or manually designed flows, VFlow treats workflow discovery as a search over graph-structured LLM invocation sequences. It introduces a multi-population cooperative evolution (CEPE-MCTS) algorithm that balances multiple hardware objectives -- functional correctness, area, power, timing and token cost -- while sharing successful patterns and avoiding repeated failures. Integrated multi-level verification ensures syntactic correctness, functional behavior, and synthesizability. Experiments on VerilogEval and RTLLM2.0 show VFlow improves pass@1 by 20--30\% over prompting baselines and closely matches designer-level area/power. Remarkably, VFlow enables small LLMs to outperform larger models with up to 10.9$\times$ ROI, offering a cost-effective solution for RTL design. This work paves the way for intelligent, automated hardware development, advancing LLM applications in EDA.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Foundations for Continual Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.08161</link>
<guid>https://arxiv.org/abs/2504.08161</guid>
<content:encoded><![CDATA[
arXiv:2504.08161v2 Announce Type: replace 
Abstract: In the traditional view of reinforcement learning, the agent's goal is to find an optimal policy that maximizes its expected sum of rewards. Once the agent finds this policy, the learning ends. This view contrasts with \emph{continual reinforcement learning}, where learning does not end, and agents are expected to continually learn and adapt indefinitely. Despite the clear distinction between these two paradigms of learning, much of the progress in continual reinforcement learning has been shaped by foundations rooted in the traditional view of reinforcement learning. In this paper, we first examine whether the foundations of traditional reinforcement learning are suitable for the continual reinforcement learning paradigm. We identify four key pillars of the traditional reinforcement learning foundations that are antithetical to the goals of continual learning: the Markov decision process formalism, the focus on atemporal artifacts, the expected sum of rewards as an evaluation metric, and episodic benchmark environments that embrace the other three foundations. We then propose a new formalism that sheds the first and the third foundations and replaces them with the history process as a mathematical formalism and a new definition of deviation regret, adapted for continual learning, as an evaluation metric. Finally, we discuss possible approaches to shed the other two foundations.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Intermediate Fusion All You Need for UAV-based Collaborative Perception?</title>
<link>https://arxiv.org/abs/2504.21774</link>
<guid>https://arxiv.org/abs/2504.21774</guid>
<content:encoded><![CDATA[
arXiv:2504.21774v2 Announce Type: replace 
Abstract: Collaborative perception enhances environmental awareness through inter-agent communication and is regarded as a promising solution to intelligent transportation systems. However, existing collaborative methods for Unmanned Aerial Vehicles (UAVs) overlook the unique characteristics of the UAV perspective, resulting in substantial communication overhead. To address this issue, we propose a novel communication-efficient collaborative perception framework based on late-intermediate fusion, dubbed LIF. The core concept is to exchange informative and compact detection results and shift the fusion stage to the feature representation level. In particular, we leverage vision-guided positional embedding (VPE) and box-based virtual augmented feature (BoBEV) to effectively integrate complementary information from various agents. Additionally, we innovatively introduce an uncertainty-driven communication mechanism that uses uncertainty evaluation to select high-quality and reliable shared areas. Experimental results demonstrate that our LIF achieves superior performance with minimal communication bandwidth, proving its effectiveness and practicality. Code and models are available at https://github.com/uestchjw/LIF.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Test-time Scaling for GUI Agent Grounding</title>
<link>https://arxiv.org/abs/2505.00684</link>
<guid>https://arxiv.org/abs/2505.00684</guid>
<content:encoded><![CDATA[
arXiv:2505.00684v2 Announce Type: replace 
Abstract: We introduce RegionFocus, a visual test-time scaling approach for Vision Language Model Agents. Understanding webpages is challenging due to the visual complexity of GUI images and the large number of interface elements, making accurate action selection difficult. Our approach dynamically zooms in on relevant regions, reducing background clutter and improving grounding accuracy. To support this process, we propose an image-as-map mechanism that visualizes key landmarks at each step, providing a transparent action record and enables the agent to effectively choose among action candidates. Even with a simple region selection strategy, we observe significant performance gains of 28+\% on Screenspot-pro and 24+\% on WebVoyager benchmarks on top of two state-of-the-art open vision language model agents, UI-TARS and Qwen2.5-VL, highlighting the effectiveness of visual test-time scaling in interactive settings. We achieve a new state-of-the-art grounding performance of 61.6\% on the ScreenSpot-Pro benchmark by applying RegionFocus to a Qwen2.5-VL-72B model. Our code will be released publicly at https://github.com/tiangeluo/RegionFocus.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A modular framework for automated evaluation of procedural content generation in serious games with deep reinforcement learning agents</title>
<link>https://arxiv.org/abs/2505.16801</link>
<guid>https://arxiv.org/abs/2505.16801</guid>
<content:encoded><![CDATA[
arXiv:2505.16801v2 Announce Type: replace 
Abstract: Serious Games (SGs) are nowadays shifting focus to include procedural content generation (PCG) in the development process as a means of offering personalized and enhanced player experience. However, the development of a framework to assess the impact of PCG techniques when integrated into SGs remains particularly challenging. This study proposes a methodology for automated evaluation of PCG integration in SGs, incorporating deep reinforcement learning (DRL) game testing agents. To validate the proposed framework, a previously introduced SG featuring card game mechanics and incorporating three different versions of PCG for nonplayer character (NPC) creation has been deployed. Version 1 features random NPC creation, while versions 2 and 3 utilize a genetic algorithm approach. These versions are used to test the impact of different dynamic SG environments on the proposed framework's agents. The obtained results highlight the superiority of the DRL game testing agents trained on Versions 2 and 3 over those trained on Version 1 in terms of win rate (i.e. number of wins per played games) and training time. More specifically, within the execution of a test emulating regular gameplay, both Versions 2 and 3 peaked at a 97% win rate and achieved statistically significant higher (p=0009) win rates compared to those achieved in Version 1 that peaked at 94%. Overall, results advocate towards the proposed framework's capability to produce meaningful data for the evaluation of procedurally generated content in SGs.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models</title>
<link>https://arxiv.org/abs/2505.17826</link>
<guid>https://arxiv.org/abs/2505.17826</guid>
<content:encoded><![CDATA[
arXiv:2505.17826v2 Announce Type: replace 
Abstract: Trinity-RFT is a general-purpose, unified and easy-to-use framework designed for reinforcement fine-tuning (RFT) of large language models. It is built with a modular and decoupled design, consisting of (1) an RFT-core that unifies and generalizes synchronous/asynchronous, on-policy/off-policy, and online/offline modes of RFT; (2) seamless integration for agent-environment interaction with high efficiency and robustness; and (3) systematic data pipelines optimized for RFT. Trinity-RFT can be easily adapted for diverse application scenarios, and serves as a unified platform for development and research of advanced reinforcement learning paradigms at both macroscopic and microscopic levels. This technical report outlines the vision, features, design and implementations of Trinity-RFT, accompanied by extensive examples, applications and experiments that demonstrate its functionalities and user-friendliness.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforced Reasoning for Embodied Planning</title>
<link>https://arxiv.org/abs/2505.22050</link>
<guid>https://arxiv.org/abs/2505.22050</guid>
<content:encoded><![CDATA[
arXiv:2505.22050v2 Announce Type: replace 
Abstract: Embodied planning requires agents to make coherent multi-step decisions based on dynamic visual observations and natural language goals. While recent vision-language models (VLMs) excel at static perception tasks, they struggle with the temporal reasoning, spatial understanding, and commonsense grounding needed for planning in interactive environments. In this work, we introduce a reinforcement fine-tuning framework that brings R1-style reasoning enhancement into embodied planning. We first distill a high-quality dataset from a powerful closed-source model and perform supervised fine-tuning (SFT) to equip the model with structured decision-making priors. We then design a rule-based reward function tailored to multi-step action quality and optimize the policy via Generalized Reinforced Preference Optimization (GRPO). Our approach is evaluated on Embench, a recent benchmark for interactive embodied tasks, covering both in-domain and out-of-domain scenarios. Experimental results show that our method significantly outperforms models of similar or larger scale, including GPT-4o-mini and 70B+ open-source baselines, and exhibits strong generalization to unseen environments. This work highlights the potential of reinforcement-driven reasoning to advance long-horizon planning in embodied AI.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COVID-19 Pneumonia Diagnosis Using Medical Images: Deep Learning-Based Transfer Learning Approach</title>
<link>https://arxiv.org/abs/2503.12642</link>
<guid>https://arxiv.org/abs/2503.12642</guid>
<content:encoded><![CDATA[
arXiv:2503.12642v3 Announce Type: replace-cross 
Abstract: SARS-CoV-2, the causative agent of COVID-19, remains a global health concern due to its high transmissibility and evolving variants. Although vaccination efforts and therapeutic advancements have mitigated disease severity, emerging mutations continue to challenge diagnostics and containment strategies. As of mid-February 2025, global test positivity has risen to 11%, marking the highest level in over six months despite widespread immunization efforts. Newer variants demonstrate enhanced host cell binding, increasing both infectivity and diagnostic complexity. This study evaluates the effectiveness of deep transfer learning in delivering rapid, accurate, and mutation-resilient COVID-19 diagnosis from medical imaging, with a focus on scalability and accessibility. We developed an automated detection system using state-of-the-art CNNs, including VGG16, ResNet50, ConvNetXtTiny, MobileNet, NASNetMobile, and DenseNet121 among others, to detect COVID-19 from chest X-ray and CT images. Among all the models evaluated, DenseNet121 emerged as the best-performing architecture for COVID-19 diagnosis using CT and X-ray images. It achieved an impressive accuracy of 98%, with 96.9% precision, 98.9% recall, 97.9% F1-score and 99.8% AUC score, indicating a high degree of consistency and reliability in both detecting positive and negative cases. The confusion matrix showed minimal false positives and false negatives, underscoring the model's robustness in real-world diagnostic scenarios.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AblationBench: Evaluating Automated Planning of Ablations in Empirical AI Research</title>
<link>https://arxiv.org/abs/2507.08038</link>
<guid>https://arxiv.org/abs/2507.08038</guid>
<content:encoded><![CDATA[
arXiv:2507.08038v1 Announce Type: new 
Abstract: Autonomous agents built on language models (LMs) are showing increasing popularity in many fields, including scientific research. AI co-scientists aim to support or automate parts of the research process using these agents. A key component of empirical AI research is the design of ablation experiments. To this end, we introduce AblationBench, a benchmark suite for evaluating agents on ablation planning tasks in empirical AI research. It includes two tasks: AuthorAblation, which helps authors propose ablation experiments based on a method section and contains 83 instances, and ReviewerAblation, which helps reviewers find missing ablations in a full paper and contains 350 instances. For both tasks, we develop LM-based judges that serve as an automatic evaluation framework. Our experiments with frontier LMs show that these tasks remain challenging, with the best-performing LM system identifying only 29% of the original ablations on average. Lastly, we analyze the limitations of current LMs on these tasks, and find that chain-of-thought prompting outperforms the currently existing agent-based approach.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise-Enabled Goal Attainment in Crowded Collectives</title>
<link>https://arxiv.org/abs/2507.08100</link>
<guid>https://arxiv.org/abs/2507.08100</guid>
<content:encoded><![CDATA[
arXiv:2507.08100v1 Announce Type: new 
Abstract: In crowded environments, individuals must navigate around other occupants to reach their destinations. Understanding and controlling traffic flows in these spaces is relevant to coordinating robot swarms and designing infrastructure for dense populations. Here, we combine simulations, theory, and robotic experiments to study how noisy motion can disrupt traffic jams and enable flow as agents travel to individual goals. Above a critical noise level, large jams do not persist. From this observation, we analytically approximate the goal attainment rate as a function of the noise level, then solve for the optimal agent density and noise level that maximize the swarm's goal attainment rate. We perform robotic experiments to corroborate our simulated and theoretical results. Finally, we compare simple, local navigation approaches with a sophisticated but computationally costly central planner. A simple reactive scheme performs well up to moderate densities and is far more computationally efficient than a planner, suggesting lessons for real-world problems.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RegGS: Unposed Sparse Views Gaussian Splatting with 3DGS Registration</title>
<link>https://arxiv.org/abs/2507.08136</link>
<guid>https://arxiv.org/abs/2507.08136</guid>
<content:encoded><![CDATA[
arXiv:2507.08136v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has demonstrated its potential in reconstructing scenes from unposed images. However, optimization-based 3DGS methods struggle with sparse views due to limited prior knowledge. Meanwhile, feed-forward Gaussian approaches are constrained by input formats, making it challenging to incorporate more input views. To address these challenges, we propose RegGS, a 3D Gaussian registration-based framework for reconstructing unposed sparse views. RegGS aligns local 3D Gaussians generated by a feed-forward network into a globally consistent 3D Gaussian representation. Technically, we implement an entropy-regularized Sinkhorn algorithm to efficiently solve the optimal transport Mixture 2-Wasserstein $(\text{MW}_2)$ distance, which serves as an alignment metric for Gaussian mixture models (GMMs) in $\mathrm{Sim}(3)$ space. Furthermore, we design a joint 3DGS registration module that integrates the $\text{MW}_2$ distance, photometric consistency, and depth geometry. This enables a coarse-to-fine registration process while accurately estimating camera poses and aligning the scene. Experiments on the RE10K and ACID datasets demonstrate that RegGS effectively registers local Gaussians with high fidelity, achieving precise pose estimation and high-quality novel-view synthesis. Project page: https://3dagentworld.github.io/reggs/.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code with Me or for Me? How Increasing AI Automation Transforms Developer Workflows</title>
<link>https://arxiv.org/abs/2507.08149</link>
<guid>https://arxiv.org/abs/2507.08149</guid>
<content:encoded><![CDATA[
arXiv:2507.08149v1 Announce Type: new 
Abstract: Developers now have access to a growing array of increasingly autonomous AI tools to support software development. While numerous studies have examined developer use of copilots, which can provide chat assistance or code completions, evaluations of coding agents, which can automatically write files and run code, still largely rely on static benchmarks without humans-in-the-loop. In this work, we conduct the first academic study to explore developer interactions with coding agents and characterize how more autonomous AI tools affect user productivity and experience, compared to existing copilots. We evaluate two leading copilot and agentic coding assistants, GitHub Copilot and OpenHands, recruiting participants who regularly use the former. Our results show agents have the potential to assist developers in ways that surpass copilots (e.g., completing tasks that humans might not have accomplished before) and reduce the user effort required to complete tasks. However, there are challenges involved in enabling their broader adoption, including how to ensure users have an adequate understanding of agent behaviors. Our results not only provide insights into how developer workflows change as a result of coding agents but also highlight how user interactions with agents differ from those with existing copilots, motivating a set of recommendations for researchers building new agents. Given the broad set of developers who still largely rely on copilot-like systems, our work highlights key challenges of adopting more agentic systems into developer workflows.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KP-A: A Unified Network Knowledge Plane for Catalyzing Agentic Network Intelligence</title>
<link>https://arxiv.org/abs/2507.08164</link>
<guid>https://arxiv.org/abs/2507.08164</guid>
<content:encoded><![CDATA[
arXiv:2507.08164v1 Announce Type: new 
Abstract: The emergence of large language models (LLMs) and agentic systems is enabling autonomous 6G networks with advanced intelligence, including self-configuration, self-optimization, and self-healing. However, the current implementation of individual intelligence tasks necessitates isolated knowledge retrieval pipelines, resulting in redundant data flows and inconsistent interpretations. Inspired by the service model unification effort in Open-RAN (to support interoperability and vendor diversity), we propose KP-A: a unified Network Knowledge Plane specifically designed for Agentic network intelligence. By decoupling network knowledge acquisition and management from intelligence logic, KP-A streamlines development and reduces maintenance complexity for intelligence engineers. By offering an intuitive and consistent knowledge interface, KP-A also enhances interoperability for the network intelligence agents. We demonstrate KP-A in two representative intelligence tasks: live network knowledge Q&amp;A and edge AI service orchestration. All implementation artifacts have been open-sourced to support reproducibility and future standardization efforts.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dynamic Stackelberg Game Framework for Agentic AI Defense Against LLM Jailbreaking</title>
<link>https://arxiv.org/abs/2507.08207</link>
<guid>https://arxiv.org/abs/2507.08207</guid>
<content:encoded><![CDATA[
arXiv:2507.08207v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly deployed in critical applications, the challenge of jailbreaking, where adversaries manipulate the models to bypass safety mechanisms, has become a significant concern. This paper presents a dynamic Stackelberg game framework to model the interactions between attackers and defenders in the context of LLM jailbreaking. The framework treats the prompt-response dynamics as a sequential extensive-form game, where the defender, as the leader, commits to a strategy while anticipating the attacker's optimal responses. We propose a novel agentic AI solution, the "Purple Agent," which integrates adversarial exploration and defensive strategies using Rapidly-exploring Random Trees (RRT). The Purple Agent actively simulates potential attack trajectories and intervenes proactively to prevent harmful outputs. This approach offers a principled method for analyzing adversarial dynamics and provides a foundation for mitigating the risk of jailbreaking.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning and Behavioral Equilibria in LLM-Nash Games: From Mindsets to Actions</title>
<link>https://arxiv.org/abs/2507.08208</link>
<guid>https://arxiv.org/abs/2507.08208</guid>
<content:encoded><![CDATA[
arXiv:2507.08208v1 Announce Type: new 
Abstract: We introduce the LLM-Nash framework, a game-theoretic model where agents select reasoning prompts to guide decision-making via Large Language Models (LLMs). Unlike classical games that assume utility-maximizing agents with full rationality, this framework captures bounded rationality by modeling the reasoning process explicitly. Equilibrium is defined over the prompt space, with actions emerging as the behavioral output of LLM inference. This approach enables the study of cognitive constraints, mindset expressiveness, and epistemic learning. Through illustrative examples, we show how reasoning equilibria can diverge from classical Nash outcomes, offering a new foundation for strategic interaction in LLM-enabled systems.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Curiosity to Competence: How World Models Interact with the Dynamics of Exploration</title>
<link>https://arxiv.org/abs/2507.08210</link>
<guid>https://arxiv.org/abs/2507.08210</guid>
<content:encoded><![CDATA[
arXiv:2507.08210v1 Announce Type: new 
Abstract: What drives an agent to explore the world while also maintaining control over the environment? From a child at play to scientists in the lab, intelligent agents must balance curiosity (the drive to seek knowledge) with competence (the drive to master and control the environment). Bridging cognitive theories of intrinsic motivation with reinforcement learning, we ask how evolving internal representations mediate the trade-off between curiosity (novelty or information gain) and competence (empowerment). We compare two model-based agents using handcrafted state abstractions (Tabular) or learning an internal world model (Dreamer). The Tabular agent shows curiosity and competence guide exploration in distinct patterns, while prioritizing both improves exploration. The Dreamer agent reveals a two-way interaction between exploration and representation learning, mirroring the developmental co-evolution of curiosity and competence. Our findings formalize adaptive exploration as a balance between pursuing the unknown and the controllable, offering insights for cognitive theories and efficient reinforcement learning.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effect of Static vs. Conversational AI-Generated Messages on Colorectal Cancer Screening Intent: a Randomized Controlled Trial</title>
<link>https://arxiv.org/abs/2507.08211</link>
<guid>https://arxiv.org/abs/2507.08211</guid>
<content:encoded><![CDATA[
arXiv:2507.08211v1 Announce Type: new 
Abstract: Large language model (LLM) chatbots show increasing promise in persuasive communication. Yet their real-world utility remains uncertain, particularly in clinical settings where sustained conversations are difficult to scale. In a pre-registered randomized controlled trial, we enrolled 915 U.S. adults (ages 45-75) who had never completed colorectal cancer (CRC) screening. Participants were randomized to: (1) no message control, (2) expert-written patient materials, (3) single AI-generated message, or (4) a motivational interviewing chatbot. All participants were required to remain in their assigned condition for at least three minutes. Both AI arms tailored content using participant's self-reported demographics including age and gender. Both AI interventions significantly increased stool test intentions by over 12 points (12.9-13.8/100), compared to a 7.5 gain for expert materials (p<.001 for all comparisons). While the AI arms outperformed the no message control for colonoscopy intent, neither showed improvement xover expert materials. Notably, for both outcomes, the chatbot did not outperform the single AI message in boosting intent despite participants spending ~3.5 minutes more on average engaging with it. These findings suggest concise, demographically tailored AI messages may offer a more scalable and clinically viable path to health behavior change than more complex conversational agents and generic time intensive expert-written materials. Moreover, LLMs appear more persuasive for lesser-known and less-invasive screening approaches like stool testing, but may be less effective for entrenched preferences like colonoscopy. Future work should examine which facets of personalization drive behavior change, whether integrating structural supports can translate these modest intent gains into completed screenings, and which health behaviors are most responsive to AI-supported guidance.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Giving AI Agents Access to Cryptocurrency and Smart Contracts Creates New Vectors of AI Harm</title>
<link>https://arxiv.org/abs/2507.08249</link>
<guid>https://arxiv.org/abs/2507.08249</guid>
<content:encoded><![CDATA[
arXiv:2507.08249v1 Announce Type: new 
Abstract: There is growing interest in giving AI agents access to cryptocurrencies as well as to the smart contracts that transact them. But doing so, this position paper argues, could lead to formidable new vectors of AI harm. To support this argument, we first examine the unique properties of cryptocurrencies and smart contracts that could lead to these new vectors of harm. Next, we describe each of these new vectors of harm in detail. Finally, we conclude with a call for more technical research aimed at preventing and mitigating these harms and, thereby making it safer to endow AI agents with cryptocurrencies and smart contracts.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent Safety Alignment via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.08270</link>
<guid>https://arxiv.org/abs/2507.08270</guid>
<content:encoded><![CDATA[
arXiv:2507.08270v1 Announce Type: new 
Abstract: The emergence of autonomous Large Language Model (LLM) agents capable of tool usage has introduced new safety risks that go beyond traditional conversational misuse. These agents, empowered to execute external functions, are vulnerable to both user-initiated threats (e.g., adversarial prompts) and tool-initiated threats (e.g., malicious outputs from compromised tools). In this paper, we propose the first unified safety-alignment framework for tool-using agents, enabling models to handle both channels of threat via structured reasoning and sandboxed reinforcement learning. We introduce a tri-modal taxonomy, including benign, malicious, and sensitive for both user prompts and tool responses, and define a policy-driven decision model. Our framework employs a custom-designed sandbox environment that simulates real-world tool execution and allows fine-grained reward shaping. Through extensive evaluations on public and self-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we demonstrate that our safety-aligned agents significantly improve resistance to security threats while preserving strong utility on benign tasks. Our results show that safety and effectiveness can be jointly optimized, laying the groundwork for trustworthy deployment of autonomous LLM agents.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KAT-V1: Kwai-AutoThink Technical Report</title>
<link>https://arxiv.org/abs/2507.08297</link>
<guid>https://arxiv.org/abs/2507.08297</guid>
<content:encoded><![CDATA[
arXiv:2507.08297v1 Announce Type: new 
Abstract: We present Kwaipilot-AutoThink (KAT), an open-source 40B large language model developed to address the overthinking problem in reasoning-intensive tasks, where an automatic thinking training paradigm is proposed to dynamically switch between reasoning and non-reasoning modes based on task complexity. Specifically, first, we construct the dual-regime dataset based on a novel tagging pipeline and a multi-agent synthesis strategy, and then we apply Multi-Token Prediction (MTP)-enhanced knowledge distillation, enabling efficient and fine-grained reasoning transfer with minimal pretraining cost. Besides, we implement a cold-start initialization strategy that introduces mode-selection priors using majority-vote signals and intent-aware prompting. Finally, we propose Step-SRPO, a reinforcement learning algorithm that incorporates intermediate supervision into the GRPO framework, offering structured guidance over both reasoning-mode selection and response accuracy. Extensive experiments across multiple benchmarks demonstrate that KAT consistently matches or even outperforms current state-of-the-art models, including DeepSeek-R1-0528 and Qwen3-235B-A22B, across a wide range of reasoning-intensive tasks while reducing token usage by up to approximately 30\%. Beyond academic evaluation, KAT has been successfully deployed in Kwaipilot (i.e., Kuaishou's internal coding assistant), and improves real-world development workflows with high accuracy, efficiency, and controllable reasoning behaviors. Moreover, we are actively training a 200B Mixture-of-Experts (MoE) with 40B activation parameters, where the early-stage results already demonstrate promising improvements in performance and efficiency, further showing the scalability of the AutoThink paradigm.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRMAgent: A Multi-Agent LLM System for E-Commerce CRM Message Template Generation</title>
<link>https://arxiv.org/abs/2507.08325</link>
<guid>https://arxiv.org/abs/2507.08325</guid>
<content:encoded><![CDATA[
arXiv:2507.08325v1 Announce Type: new 
Abstract: In e-commerce private-domain channels such as instant messaging and e-mail, merchants engage customers directly as part of their Customer Relationship Management (CRM) programmes to drive retention and conversion. While a few top performers excel at crafting outbound messages, most merchants struggle to write persuasive copy because they lack both expertise and scalable tools. We introduce CRMAgent, a multi-agent system built on large language models (LLMs) that generates high-quality message templates and actionable writing guidance through three complementary modes. First, group-based learning enables the agent to learn from a merchant's own top-performing messages within the same audience segment and rewrite low-performing ones. Second, retrieval-and-adaptation fetches templates that share the same audience segment and exhibit high similarity in voucher type and product category, learns their successful patterns, and adapts them to the current campaign. Third, a rule-based fallback provides a lightweight zero-shot rewrite when no suitable references are available. Extensive experiments show that CRMAgent consistently outperforms merchants' original templates, delivering significant gains in both audience-match and marketing-effectiveness metrics.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Factors Affect LLMs and RLLMs in Financial Question Answering?</title>
<link>https://arxiv.org/abs/2507.08339</link>
<guid>https://arxiv.org/abs/2507.08339</guid>
<content:encoded><![CDATA[
arXiv:2507.08339v1 Announce Type: new 
Abstract: Recently, the development of large language models (LLMs) and reasoning large language models (RLLMs) have gained considerable attention from many researchers. RLLMs enhance the reasoning capabilities of LLMs through Long Chain-of-Thought (Long CoT) processes, significantly improving the performance of LLMs in addressing complex problems. However, there are few works that systematically explore what methods can fully unlock the performance of LLMs and RLLMs within the financial domain. To investigate the impact of various methods on LLMs and RLLMs, we utilize five LLMs and three RLLMs to assess the effects of prompting methods, agentic frameworks, and multilingual alignment methods on financial question-answering tasks. Our research findings indicate: (1) Current prompting methods and agent frameworks enhance the performance of LLMs in financial question answering by simulating Long CoT; (2) RLLMs possess inherent Long CoT capabilities, which limits the effectiveness of conventional methods in further enhancing their performance; (3) Current advanced multilingual alignment methods primarily improve the multilingual performance of LLMs by extending the reasoning length, which yields minimal benefits for RLLMs. We hope that this study can serve as an important reference for LLMs and RLLMs in the field of financial question answering.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Design of Multi-Agent LLM Dialogues for Research Ideation</title>
<link>https://arxiv.org/abs/2507.08350</link>
<guid>https://arxiv.org/abs/2507.08350</guid>
<content:encoded><![CDATA[
arXiv:2507.08350v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used to support creative tasks such as research idea generation. While recent work has shown that structured dialogues between LLMs can improve the novelty and feasibility of generated ideas, the optimal design of such interactions remains unclear. In this study, we conduct a comprehensive analysis of multi-agent LLM dialogues for scientific ideation. We compare different configurations of agent roles, number of agents, and dialogue depth to understand how these factors influence the novelty and feasibility of generated ideas. Our experimental setup includes settings where one agent generates ideas and another critiques them, enabling iterative improvement. Our results show that enlarging the agent cohort, deepening the interaction depth, and broadening agent persona heterogeneity each enrich the diversity of generated ideas. Moreover, specifically increasing critic-side diversity within the ideation-critique-revision loop further boosts the feasibility of the final proposals. Our findings offer practical guidelines for building effective multi-agent LLM systems for scientific ideation. Our code is available at https://github.com/g6000/MultiAgent-Research-Ideator.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Pre-Training for Offline-to-Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.08387</link>
<guid>https://arxiv.org/abs/2507.08387</guid>
<content:encoded><![CDATA[
arXiv:2507.08387v1 Announce Type: new 
Abstract: Offline-to-online reinforcement learning (RL) aims to integrate the complementary strengths of offline and online RL by pre-training an agent offline and subsequently fine-tuning it through online interactions. However, recent studies reveal that offline pre-trained agents often underperform during online fine-tuning due to inaccurate value estimation caused by distribution shift, with random initialization proving more effective in certain cases. In this work, we propose a novel method, Online Pre-Training for Offline-to-Online RL (OPT), explicitly designed to address the issue of inaccurate value estimation in offline pre-trained agents. OPT introduces a new learning phase, Online Pre-Training, which allows the training of a new value function tailored specifically for effective online fine-tuning. Implementation of OPT on TD3 and SPOT demonstrates an average 30% improvement in performance across a wide range of D4RL environments, including MuJoCo, Antmaze, and Adroit.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent LLMs as Ethics Advocates in AI-Based Systems</title>
<link>https://arxiv.org/abs/2507.08392</link>
<guid>https://arxiv.org/abs/2507.08392</guid>
<content:encoded><![CDATA[
arXiv:2507.08392v1 Announce Type: new 
Abstract: Incorporating ethics into the requirement elicitation process is essential for creating ethically aligned systems. Although eliciting manual ethics requirements is effective, it requires diverse input from multiple stakeholders, which can be challenging due to time and resource constraints. Moreover, it is often given a low priority in the requirements elicitation process. This study proposes a framework for generating ethics requirements drafts by introducing an ethics advocate agent in a multi-agent LLM setting. This agent critiques and provides input on ethical issues based on the system description. The proposed framework is evaluated through two case studies from different contexts, demonstrating that it captures the majority of ethics requirements identified by researchers during 30-minute interviews and introduces several additional relevant requirements. However, it also highlights reliability issues in generating ethics requirements, emphasizing the need for human feedback in this sensitive domain. We believe this work can facilitate the broader adoption of ethics in the requirements engineering process, ultimately leading to more ethically aligned products.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Large Language Models in Discipline-specific Research: Challenges, Methods and Opportunities</title>
<link>https://arxiv.org/abs/2507.08425</link>
<guid>https://arxiv.org/abs/2507.08425</guid>
<content:encoded><![CDATA[
arXiv:2507.08425v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated their transformative potential across numerous disciplinary studies, reshaping the existing research methodologies and fostering interdisciplinary collaboration. However, a systematic understanding of their integration into diverse disciplines remains underexplored. This survey paper provides a comprehensive overview of the application of LLMs in interdisciplinary studies, categorising research efforts from both a technical perspective and with regard to their applicability. From a technical standpoint, key methodologies such as supervised fine-tuning, retrieval-augmented generation, agent-based approaches, and tool-use integration are examined, which enhance the adaptability and effectiveness of LLMs in discipline-specific contexts. From the perspective of their applicability, this paper explores how LLMs are contributing to various disciplines including mathematics, physics, chemistry, biology, and the humanities and social sciences, demonstrating their role in discipline-specific tasks. The prevailing challenges are critically examined and the promising research directions are highlighted alongside the recent advances in LLMs. By providing a comprehensive overview of the technical developments and applications in this field, this survey aims to serve as an invaluable resource for the researchers who are navigating the complex landscape of LLMs in the context of interdisciplinary studies.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Age of Information Optimization in Laser-charged UAV-assisted IoT Networks: A Multi-agent Deep Reinforcement Learning Method</title>
<link>https://arxiv.org/abs/2507.08429</link>
<guid>https://arxiv.org/abs/2507.08429</guid>
<content:encoded><![CDATA[
arXiv:2507.08429v1 Announce Type: new 
Abstract: The integration of unmanned aerial vehicles (UAVs) with Internet of Things (IoT) networks offers promising solutions for efficient data collection. However, the limited energy capacity of UAVs remains a significant challenge. In this case, laser beam directors (LBDs) have emerged as an effective technology for wireless charging of UAVs during operation, thereby enabling sustained data collection without frequent returns to charging stations (CSs). In this work, we investigate the age of information (AoI) optimization in LBD-powered UAV-assisted IoT networks, where multiple UAVs collect data from distributed IoTs while being recharged by laser beams. We formulate a joint optimization problem that aims to minimize the peak AoI while determining optimal UAV trajectories and laser charging strategies. This problem is particularly challenging due to its non-convex nature, complex temporal dependencies, and the need to balance data collection efficiency with energy consumption constraints. To address these challenges, we propose a novel multi-agent proximal policy optimization with temporal memory and multi-agent coordination (MAPPO-TM) framework. Specifically, MAPPO-TM incorporates temporal memory mechanisms to capture the dynamic nature of UAV operations and facilitates effective coordination among multiple UAVs through decentralized learning while considering global system objectives. Simulation results demonstrate that the proposed MAPPO-TM algorithm outperforms conventional approaches in terms of peak AoI minimization and energy efficiency. Ideally, the proposed algorithm achieves up to 15.1% reduction in peak AoI compared to conventional multi-agent deep reinforcement learning (MADRL) methods.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finding Common Ground: Using Large Language Models to Detect Agreement in Multi-Agent Decision Conferences</title>
<link>https://arxiv.org/abs/2507.08440</link>
<guid>https://arxiv.org/abs/2507.08440</guid>
<content:encoded><![CDATA[
arXiv:2507.08440v1 Announce Type: new 
Abstract: Decision conferences are structured, collaborative meetings that bring together experts from various fields to address complex issues and reach a consensus on recommendations for future actions or policies. These conferences often rely on facilitated discussions to ensure productive dialogue and collective agreement. Recently, Large Language Models (LLMs) have shown significant promise in simulating real-world scenarios, particularly through collaborative multi-agent systems that mimic group interactions. In this work, we present a novel LLM-based multi-agent system designed to simulate decision conferences, specifically focusing on detecting agreement among the participant agents. To achieve this, we evaluate six distinct LLMs on two tasks: stance detection, which identifies the position an agent takes on a given issue, and stance polarity detection, which identifies the sentiment as positive, negative, or neutral. These models are further assessed within the multi-agent system to determine their effectiveness in complex simulations. Our results indicate that LLMs can reliably detect agreement even in dynamic and nuanced debates. Incorporating an agreement-detection agent within the system can also improve the efficiency of group debates and enhance the overall quality and coherence of deliberations, making them comparable to real-world decision conferences regarding outcome and decision-making. These findings demonstrate the potential for LLM-based multi-agent systems to simulate group decision-making processes. They also highlight that such systems could be instrumental in supporting decision-making with expert elicitation workshops across various domains.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Occlusion-Guided Feature Purification Learning via Reinforced Knowledge Distillation for Occluded Person Re-Identification</title>
<link>https://arxiv.org/abs/2507.08520</link>
<guid>https://arxiv.org/abs/2507.08520</guid>
<content:encoded><![CDATA[
arXiv:2507.08520v1 Announce Type: new 
Abstract: Occluded person re-identification aims to retrieve holistic images based on occluded ones. Existing methods often rely on aligning visible body parts, applying occlusion augmentation, or complementing missing semantics using holistic images. However, they face challenges in handling diverse occlusion scenarios not seen during training and the issue of feature contamination from holistic images. To address these limitations, we propose Occlusion-Guided Feature Purification Learning via Reinforced Knowledge Distillation (OGFR), which simultaneously mitigates these challenges. OGFR adopts a teacher-student distillation architecture that effectively incorporates diverse occlusion patterns into feature representation while transferring the purified discriminative holistic knowledge from the holistic to the occluded branch through reinforced knowledge distillation. Specifically, an Occlusion-Aware Vision Transformer is designed to leverage learnable occlusion pattern embeddings to explicitly model such diverse occlusion types, thereby guiding occlusion-aware robust feature representation. Moreover, we devise a Feature Erasing and Purification Module within the holistic branch, in which an agent is employed to identify low-quality patch tokens of holistic images that contain noisy negative information via deep reinforcement learning, and substitute these patch tokens with learnable embedding tokens to avoid feature contamination and further excavate identity-related discriminative clues. Afterward, with the assistance of knowledge distillation, the student branch effectively absorbs the purified holistic knowledge to precisely learn robust representation regardless of the interference of occlusions.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recursive Reward Aggregation</title>
<link>https://arxiv.org/abs/2507.08537</link>
<guid>https://arxiv.org/abs/2507.08537</guid>
<content:encoded><![CDATA[
arXiv:2507.08537v1 Announce Type: new 
Abstract: In reinforcement learning (RL), aligning agent behavior with specific objectives typically requires careful design of the reward function, which can be challenging when the desired objectives are complex. In this work, we propose an alternative approach for flexible behavior alignment that eliminates the need to modify the reward function by selecting appropriate reward aggregation functions. By introducing an algebraic perspective on Markov decision processes (MDPs), we show that the Bellman equations naturally emerge from the recursive generation and aggregation of rewards, allowing for the generalization of the standard discounted sum to other recursive aggregations, such as discounted max and Sharpe ratio. Our approach applies to both deterministic and stochastic settings and integrates seamlessly with value-based and actor-critic algorithms. Experimental results demonstrate that our approach effectively optimizes diverse objectives, highlighting its versatility and potential for real-world applications.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAM2RL: Towards Reinforcement Learning Memory Control in Segment Anything Model 2</title>
<link>https://arxiv.org/abs/2507.08548</link>
<guid>https://arxiv.org/abs/2507.08548</guid>
<content:encoded><![CDATA[
arXiv:2507.08548v1 Announce Type: new 
Abstract: Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks and has become the state-of-the-art for visual object tracking. The model stores information from previous frames in a memory bank, enabling temporal consistency across video sequences. Recent methods augment SAM 2 with hand-crafted update rules to better handle distractors, occlusions, and object motion. We propose a fundamentally different approach using reinforcement learning for optimizing memory updates in SAM 2 by framing memory control as a sequential decision-making problem. In an overfitting setup with a separate agent per video, our method achieves a relative improvement over SAM 2 that exceeds by more than three times the gains of existing heuristics. These results reveal the untapped potential of the memory bank and highlight reinforcement learning as a powerful alternative to hand-crafted update rules for memory control in visual object tracking.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking Speech Instruction Data Potential with Query Rewriting</title>
<link>https://arxiv.org/abs/2507.08603</link>
<guid>https://arxiv.org/abs/2507.08603</guid>
<content:encoded><![CDATA[
arXiv:2507.08603v1 Announce Type: new 
Abstract: End-to-end Large Speech Language Models~(\textbf{LSLMs}) demonstrate strong potential in response latency and speech comprehension capabilities, showcasing general intelligence across speech understanding tasks. However, the ability to follow speech instructions has not been fully realized due to the lack of datasets and heavily biased training tasks. Leveraging the rich ASR datasets, previous approaches have used Large Language Models~(\textbf{LLMs}) to continue the linguistic information of speech to construct speech instruction datasets. Yet, due to the gap between LLM-generated results and real human responses, the continuation methods further amplify these shortcomings. Given the high costs of collecting and annotating speech instruction datasets by humans, using speech synthesis to construct large-scale speech instruction datasets has become a balanced and robust alternative. Although modern Text-To-Speech~(\textbf{TTS}) models have achieved near-human-level synthesis quality, it is challenging to appropriately convert out-of-distribution text instruction to speech due to the limitations of the training data distribution in TTS models. To address this issue, we propose a query rewriting framework with multi-LLM knowledge fusion, employing multiple agents to annotate and validate the synthesized speech, making it possible to construct high-quality speech instruction datasets without relying on human annotation. Experiments show that this method can transform text instructions into distributions more suitable for TTS models for speech synthesis through zero-shot rewriting, increasing data usability from 72\% to 93\%. It also demonstrates unique advantages in rewriting tasks that require complex knowledge and context-related abilities.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Natural Language with Communication Games for Improving Image Captioning Capabilities without Additional Data</title>
<link>https://arxiv.org/abs/2507.08610</link>
<guid>https://arxiv.org/abs/2507.08610</guid>
<content:encoded><![CDATA[
arXiv:2507.08610v1 Announce Type: new 
Abstract: Image captioning is an important problem in developing various AI systems, and these tasks require large volumes of annotated images to train the models. Since all existing labelled datasets are already used for training the large Vision Language Models (VLMs), it becomes challenging to improve the performance of the same. Considering this, it is essential to consider the unsupervised image captioning performance, which remains relatively under-explored. To that end, we propose LoGIC (Lewis Communication Game for Image Captioning), a Multi-agent Reinforcement Learning game. The proposed method consists of two agents, a 'speaker' and a 'listener', with the objective of learning a strategy for communicating in natural language. We train agents in the cooperative common-reward setting using the GRPO algorithm and show that improvement in image captioning performance emerges as a consequence of the agents learning to play the game. We show that using pre-trained VLMs as the 'speaker' and Large Language Model (LLM) for language understanding in the 'listener', we achieved a $46$ BLEU score after fine-tuning using LoGIC without additional labels, a $2$ units advantage in absolute metrics compared to the $44$ BLEU score of the vanilla VLM. Additionally, we replace the VLM from the 'speaker' with lightweight components: (i) a ViT for image perception and (ii) a GPT2 language generation, and train them from scratch using LoGIC, obtaining a $31$ BLEU score in the unsupervised setting, a $10$ points advantage over existing unsupervised image-captioning methods.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentsNet: Coordination and Collaborative Reasoning in Multi-Agent LLMs</title>
<link>https://arxiv.org/abs/2507.08616</link>
<guid>https://arxiv.org/abs/2507.08616</guid>
<content:encoded><![CDATA[
arXiv:2507.08616v1 Announce Type: new 
Abstract: Large-language models (LLMs) have demonstrated powerful problem-solving capabilities, in particular when organized in multi-agent systems. However, the advent of such systems also raises several questions on the ability of a complex network of agents to effectively self-organize and collaborate. While measuring performance on standard reasoning benchmarks indicates how well multi-agent systems can solve reasoning tasks, it is unclear whether these systems are able to leverage their topology effectively. Here, we propose AgentsNet, a new benchmark for multi-agent reasoning. By drawing inspiration from classical problems in distributed systems and graph theory, AgentsNet measures the ability of multi-agent systems to collaboratively form strategies for problem-solving, self-organization, and effective communication given a network topology. We evaluate a variety of baseline methods on AgentsNet including homogeneous networks of agents which first have to agree on basic protocols for organization and communication. We find that some frontier LLMs are already demonstrating strong performance for small networks but begin to fall off once the size of the network scales. While existing multi-agent benchmarks cover at most 2-5 agents, AgentsNet is practically unlimited in size and can scale with new generations of LLMs. As such, we also probe frontier models in a setup with up to 100 agents.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Large Language Models for Conceptual Systems Engineering and Design</title>
<link>https://arxiv.org/abs/2507.08619</link>
<guid>https://arxiv.org/abs/2507.08619</guid>
<content:encoded><![CDATA[
arXiv:2507.08619v1 Announce Type: new 
Abstract: Early-stage engineering design involves complex, iterative reasoning, yet existing large language model (LLM) workflows struggle to maintain task continuity and generate executable models. We evaluate whether a structured multi-agent system (MAS) can more effectively manage requirements extraction, functional decomposition, and simulator code generation than a simpler two-agent system (2AS). The target application is a solar-powered water filtration system as described in a cahier des charges. We introduce the Design-State Graph (DSG), a JSON-serializable representation that bundles requirements, physical embodiments, and Python-based physics models into graph nodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS collapses the process to a Generator-Reflector loop. Both systems run a total of 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1 70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON validity, requirement coverage, embodiment presence, code compatibility, workflow completion, runtime, and graph size. Across all runs, both MAS and 2AS maintained perfect JSON integrity and embodiment tagging. Requirement coverage remained minimal (less than 20\%). Code compatibility peaked at 100\% under specific 2AS settings but averaged below 50\% for MAS. Only the reasoning-distilled model reliably flagged workflow completion. Powered by DeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes) whereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced design detail. Reasoning-distilled LLM improved completion rates, yet low requirements and fidelity gaps in coding persisted.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OnlineBEV: Recurrent Temporal Fusion in Bird's Eye View Representations for Multi-Camera 3D Perception</title>
<link>https://arxiv.org/abs/2507.08644</link>
<guid>https://arxiv.org/abs/2507.08644</guid>
<content:encoded><![CDATA[
arXiv:2507.08644v1 Announce Type: new 
Abstract: Multi-view camera-based 3D perception can be conducted using bird's eye view (BEV) features obtained through perspective view-to-BEV transformations. Several studies have shown that the performance of these 3D perception methods can be further enhanced by combining sequential BEV features obtained from multiple camera frames. However, even after compensating for the ego-motion of an autonomous agent, the performance gain from temporal aggregation is limited when combining a large number of image frames. This limitation arises due to dynamic changes in BEV features over time caused by object motion. In this paper, we introduce a novel temporal 3D perception method called OnlineBEV, which combines BEV features over time using a recurrent structure. This structure increases the effective number of combined features with minimal memory usage. However, it is critical to spatially align the features over time to maintain strong performance. OnlineBEV employs the Motion-guided BEV Fusion Network (MBFNet) to achieve temporal feature alignment. MBFNet extracts motion features from consecutive BEV frames and dynamically aligns historical BEV features with current ones using these motion features. To enforce temporal feature alignment explicitly, we use Temporal Consistency Learning Loss, which captures discrepancies between historical and target BEV features. Experiments conducted on the nuScenes benchmark demonstrate that OnlineBEV achieves significant performance gains over the current best method, SOLOFusion. OnlineBEV achieves 63.9% NDS on the nuScenes test set, recording state-of-the-art performance in the camera-only 3D object detection task.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DatasetAgent: A Novel Multi-Agent System for Auto-Constructing Datasets from Real-World Images</title>
<link>https://arxiv.org/abs/2507.08648</link>
<guid>https://arxiv.org/abs/2507.08648</guid>
<content:encoded><![CDATA[
arXiv:2507.08648v1 Announce Type: new 
Abstract: Common knowledge indicates that the process of constructing image datasets usually depends on the time-intensive and inefficient method of manual collection and annotation. Large models offer a solution via data generation. Nonetheless, real-world data are obviously more valuable comparing to artificially intelligence generated data, particularly in constructing image datasets. For this reason, we propose a novel method for auto-constructing datasets from real-world images by a multiagent collaborative system, named as DatasetAgent. By coordinating four different agents equipped with Multi-modal Large Language Models (MLLMs), as well as a tool package for image optimization, DatasetAgent is able to construct high-quality image datasets according to user-specified requirements. In particular, two types of experiments are conducted, including expanding existing datasets and creating new ones from scratch, on a variety of open-source datasets. In both cases, multiple image datasets constructed by DatasetAgent are used to train various vision models for image classification, object detection, and image segmentation.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>elsciRL: Integrating Language Solutions into Reinforcement Learning Problem Settings</title>
<link>https://arxiv.org/abs/2507.08705</link>
<guid>https://arxiv.org/abs/2507.08705</guid>
<content:encoded><![CDATA[
arXiv:2507.08705v1 Announce Type: new 
Abstract: We present elsciRL, an open-source Python library to facilitate the application of language solutions on reinforcement learning problems. We demonstrate the potential of our software by extending the Language Adapter with Self-Completing Instruction framework defined in (Osborne, 2024) with the use of LLMs. Our approach can be re-applied to new applications with minimal setup requirements. We provide a novel GUI that allows a user to provide text input for an LLM to generate instructions which it can then self-complete. Empirical results indicate that these instructions \textit{can} improve a reinforcement learning agent's performance. Therefore, we present this work to accelerate the evaluation of language solutions on reward based environments to enable new opportunities for scientific discovery.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPLASH! Sample-efficient Preference-based inverse reinforcement learning for Long-horizon Adversarial tasks from Suboptimal Hierarchical demonstrations</title>
<link>https://arxiv.org/abs/2507.08707</link>
<guid>https://arxiv.org/abs/2507.08707</guid>
<content:encoded><![CDATA[
arXiv:2507.08707v1 Announce Type: new 
Abstract: Inverse Reinforcement Learning (IRL) presents a powerful paradigm for learning complex robotic tasks from human demonstrations. However, most approaches make the assumption that expert demonstrations are available, which is often not the case. Those that allow for suboptimality in the demonstrations are not designed for long-horizon goals or adversarial tasks. Many desirable robot capabilities fall into one or both of these categories, thus highlighting a critical shortcoming in the ability of IRL to produce field-ready robotic agents. We introduce Sample-efficient Preference-based inverse reinforcement learning for Long-horizon Adversarial tasks from Suboptimal Hierarchical demonstrations (SPLASH), which advances the state-of-the-art in learning from suboptimal demonstrations to long-horizon and adversarial settings. We empirically validate SPLASH on a maritime capture-the-flag task in simulation, and demonstrate real-world applicability with sim-to-real translation experiments on autonomous unmanned surface vehicles. We show that our proposed methods allow SPLASH to significantly outperform the state-of-the-art in reward learning from suboptimal demonstrations.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuralOS: Towards Simulating Operating Systems via Neural Generative Models</title>
<link>https://arxiv.org/abs/2507.08800</link>
<guid>https://arxiv.org/abs/2507.08800</guid>
<content:encoded><![CDATA[
arXiv:2507.08800v1 Announce Type: new 
Abstract: We introduce NeuralOS, a neural framework that simulates graphical user interfaces (GUIs) of operating systems by directly predicting screen frames in response to user inputs such as mouse movements, clicks, and keyboard events. NeuralOS combines a recurrent neural network (RNN), which tracks computer state, with a diffusion-based neural renderer that generates screen images. The model is trained on a large-scale dataset of Ubuntu XFCE recordings, which include both randomly generated interactions and realistic interactions produced by AI agents. Experiments show that NeuralOS successfully renders realistic GUI sequences, accurately captures mouse interactions, and reliably predicts state transitions like application launches. Although modeling fine-grained keyboard interactions precisely remains challenging, NeuralOS offers a step toward creating fully adaptive, generative neural interfaces for future human-computer interaction systems.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Trade or Not to Trade: An Agentic Approach to Estimating Market Risk Improves Trading Decisions</title>
<link>https://arxiv.org/abs/2507.08584</link>
<guid>https://arxiv.org/abs/2507.08584</guid>
<content:encoded><![CDATA[
arXiv:2507.08584v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly deployed in agentic frameworks, in which prompts trigger complex tool-based analysis in pursuit of a goal. While these frameworks have shown promise across multiple domains including in finance, they typically lack a principled model-building step, relying instead on sentiment- or trend-based analysis. We address this gap by developing an agentic system that uses LLMs to iteratively discover stochastic differential equations for financial time series. These models generate risk metrics which inform daily trading decisions. We evaluate our system in both traditional backtests and using a market simulator, which introduces synthetic but causally plausible price paths and news events. We find that model-informed trading strategies outperform standard LLM-based agents, improving Sharpe ratios across multiple equities. Our results show that combining LLMs with agentic model discovery enhances market risk estimation and enables more profitable trading decisions.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe Deep Reinforcement Learning for Resource Allocation with Peak Age of Information Violation Guarantees</title>
<link>https://arxiv.org/abs/2507.08653</link>
<guid>https://arxiv.org/abs/2507.08653</guid>
<content:encoded><![CDATA[
arXiv:2507.08653v1 Announce Type: cross 
Abstract: In Wireless Networked Control Systems (WNCSs), control and communication systems must be co-designed due to their strong interdependence. This paper presents a novel optimization theory-based safe deep reinforcement learning (DRL) framework for ultra-reliable WNCSs, ensuring constraint satisfaction while optimizing performance, for the first time in the literature. The approach minimizes power consumption under key constraints, including Peak Age of Information (PAoI) violation probability, transmit power, and schedulability in the finite blocklength regime. PAoI violation probability is uniquely derived by combining stochastic maximum allowable transfer interval (MATI) and maximum allowable packet delay (MAD) constraints in a multi-sensor network. The framework consists of two stages: optimization theory and safe DRL. The first stage derives optimality conditions to establish mathematical relationships among variables, simplifying and decomposing the problem. The second stage employs a safe DRL model where a teacher-student framework guides the DRL agent (student). The control mechanism (teacher) evaluates compliance with system constraints and suggests the nearest feasible action when needed. Extensive simulations show that the proposed framework outperforms rule-based and other optimization theory based DRL benchmarks, achieving faster convergence, higher rewards, and greater stability.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpreting systems as solving POMDPs: a step towards a formal understanding of agency</title>
<link>https://arxiv.org/abs/2209.01619</link>
<guid>https://arxiv.org/abs/2209.01619</guid>
<content:encoded><![CDATA[
arXiv:2209.01619v2 Announce Type: replace 
Abstract: Under what circumstances can a system be said to have beliefs and goals, and how do such agency-related features relate to its physical state? Recent work has proposed a notion of interpretation map, a function that maps the state of a system to a probability distribution representing its beliefs about an external world. Such a map is not completely arbitrary, as the beliefs it attributes to the system must evolve over time in a manner that is consistent with Bayes' theorem, and consequently the dynamics of a system constrain its possible interpretations. Here we build on this approach, proposing a notion of interpretation not just in terms of beliefs but in terms of goals and actions. To do this we make use of the existing theory of partially observable Markov processes (POMDPs): we say that a system can be interpreted as a solution to a POMDP if it not only admits an interpretation map describing its beliefs about the hidden state of a POMDP but also takes actions that are optimal according to its belief state. An agent is then a system together with an interpretation of this system as a POMDP solution. Although POMDPs are not the only possible formulation of what it means to have a goal, this nevertheless represents a step towards a more general formal definition of what it means for a system to be an agent.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text2BIM: Generating Building Models Using a Large Language Model-based Multi-Agent Framework</title>
<link>https://arxiv.org/abs/2408.08054</link>
<guid>https://arxiv.org/abs/2408.08054</guid>
<content:encoded><![CDATA[
arXiv:2408.08054v2 Announce Type: replace 
Abstract: The conventional BIM authoring process typically requires designers to master complex and tedious modeling commands in order to materialize their design intentions within BIM authoring tools. This additional cognitive burden complicates the design process and hinders the adoption of BIM and model-based design in the AEC (Architecture, Engineering, and Construction) industry. To facilitate the expression of design intentions more intuitively, we propose Text2BIM, an LLM-based multi-agent framework that can generate 3D building models from natural language instructions. This framework orchestrates multiple LLM agents to collaborate and reason, transforming textual user input into imperative code that invokes the BIM authoring tool's APIs, thereby generating editable BIM models with internal layouts, external envelopes, and semantic information directly in the software. Furthermore, a rule-based model checker is introduced into the agentic workflow, utilizing predefined domain knowledge to guide the LLM agents in resolving issues within the generated models and iteratively improving model quality. Extensive experiments were conducted to compare and analyze the performance of three different LLMs under the proposed framework. The evaluation results demonstrate that our approach can effectively generate high-quality, structurally rational building models that are aligned with the abstract concepts specified by user input. Finally, an interactive software prototype was developed to integrate the framework into the BIM authoring software Vectorworks, showcasing the potential of modeling by chatting. The code is available at: https://github.com/dcy0577/Text2BIM
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embedding Space Allocation with Angle-Norm Joint Classifiers for Few-Shot Class-Incremental Learning</title>
<link>https://arxiv.org/abs/2411.09250</link>
<guid>https://arxiv.org/abs/2411.09250</guid>
<content:encoded><![CDATA[
arXiv:2411.09250v2 Announce Type: replace 
Abstract: Few-shot class-incremental learning (FSCIL) aims to continually learn new classes from only a few samples without forgetting previous ones, requiring intelligent agents to adapt to dynamic environments. FSCIL combines the characteristics and challenges of class-incremental learning and few-shot learning: (i) Current classes occupy the entire feature space, which is detrimental to learning new classes. (ii) The small number of samples in incremental rounds is insufficient for fully training. In existing mainstream virtual class methods, for addressing the challenge (i), they attempt to use virtual classes as placeholders. However, new classes may not necessarily align with the virtual classes. For the challenge (ii), they replace trainable fully connected layers with Nearest Class Mean (NCM) classifiers based on cosine similarity, but NCM classifiers do not account for sample imbalance issues. To address these issues in previous methods, we propose the class-center guided embedding Space Allocation with Angle-Norm joint classifiers (SAAN) learning framework, which provides balanced space for all classes and leverages norm differences caused by sample imbalance to enhance classification criteria. Specifically, for challenge (i), SAAN divides the feature space into multiple subspaces and allocates a dedicated subspace for each session by guiding samples with the pre-set category centers. For challenge (ii), SAAN establishes a norm distribution for each class and generates angle-norm joint logits. Experiments demonstrate that SAAN can achieve state-of-the-art performance and it can be directly embedded into other SOTA methods as a plug-in, further enhancing their performance.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DriveTransformer: Unified Transformer for Scalable End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2503.07656</link>
<guid>https://arxiv.org/abs/2503.07656</guid>
<content:encoded><![CDATA[
arXiv:2503.07656v2 Announce Type: replace 
Abstract: End-to-end autonomous driving (E2E-AD) has emerged as a trend in the field of autonomous driving, promising a data-driven, scalable approach to system design. However, existing E2E-AD methods usually adopt the sequential paradigm of perception-prediction-planning, which leads to cumulative errors and training instability. The manual ordering of tasks also limits the system`s ability to leverage synergies between tasks (for example, planning-aware perception and game-theoretic interactive prediction and planning). Moreover, the dense BEV representation adopted by existing methods brings computational challenges for long-range perception and long-term temporal fusion. To address these challenges, we present DriveTransformer, a simplified E2E-AD framework for the ease of scaling up, characterized by three key features: Task Parallelism (All agent, map, and planning queries direct interact with each other at each block), Sparse Representation (Task queries direct interact with raw sensor features), and Streaming Processing (Task queries are stored and passed as history information). As a result, the new framework is composed of three unified operations: task self-attention, sensor cross-attention, temporal cross-attention, which significantly reduces the complexity of system and leads to better training stability. DriveTransformer achieves state-of-the-art performance in both simulated closed-loop benchmark Bench2Drive and real world open-loop benchmark nuScenes with high FPS.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based VLM Agent Training</title>
<link>https://arxiv.org/abs/2503.08525</link>
<guid>https://arxiv.org/abs/2503.08525</guid>
<content:encoded><![CDATA[
arXiv:2503.08525v2 Announce Type: replace 
Abstract: Reinforcement learning with verifiable outcome rewards (RLVR) has effectively scaled up chain-of-thought (CoT) reasoning in large language models (LLMs). Yet, its efficacy in training vision-language model (VLM) agents for goal-directed action reasoning in visual environments is less established. This work investigates this problem through extensive experiments on complex card games, such as 24 points, and embodied tasks from ALFWorld. We find that when rewards are based solely on action outcomes, RL fails to incentivize CoT reasoning in VLMs, instead leading to a phenomenon we termed thought collapse, characterized by a rapid loss of diversity in the agent's thoughts, state-irrelevant and incomplete reasoning, and subsequent invalid actions, resulting in negative rewards. To counteract thought collapse, we highlight the necessity of process guidance and propose an automated corrector that evaluates and refines the agent's reasoning at each RL step. This simple and scalable GTR (Guided Thought Reinforcement) framework trains reasoning and action simultaneously without the need for dense, per-step human labeling. Our experiments demonstrate that GTR significantly enhances the performance and generalization of the LLaVA-7b model across various visual environments, achieving 3-5 times higher task success rates compared to SoTA models with notably smaller model sizes.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A taxonomy of epistemic injustice in the context of AI and the case for generative hermeneutical erasure</title>
<link>https://arxiv.org/abs/2504.07531</link>
<guid>https://arxiv.org/abs/2504.07531</guid>
<content:encoded><![CDATA[
arXiv:2504.07531v2 Announce Type: replace 
Abstract: Epistemic injustice related to AI is a growing concern. In relation to machine learning models, epistemic injustice can have a diverse range of sources, ranging from epistemic opacity, the discriminatory automation of testimonial prejudice, and the distortion of human beliefs via generative AI's hallucinations to the exclusion of the global South in global AI governance, the execution of bureaucratic violence via algorithmic systems, and interactions with conversational artificial agents. Based on a proposed general taxonomy of epistemic injustice, this paper first sketches a taxonomy of the types of epistemic injustice in the context of AI, relying on the work of scholars from the fields of philosophy of technology, political philosophy and social epistemology. Secondly, an additional conceptualization on epistemic injustice in the context of AI is provided: generative hermeneutical erasure. I argue that this injustice the automation of 'epistemicide', the injustice done to epistemic agents in their capacity for collective sense-making through the suppression of difference in epistemology and conceptualization by LLMs. AI systems' 'view from nowhere' epistemically inferiorizes non-Western epistemologies and thereby contributes to the erosion of their epistemic particulars, gradually contributing to hermeneutical erasure. This work's relevance lies in proposal of a taxonomy that allows epistemic injustices to be mapped in the AI domain and the proposal of a novel form of AI-related epistemic injustice.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boundary-Guided Trajectory Prediction for Road Aware and Physically Feasible Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.06740</link>
<guid>https://arxiv.org/abs/2505.06740</guid>
<content:encoded><![CDATA[
arXiv:2505.06740v2 Announce Type: replace 
Abstract: Accurate prediction of surrounding road users' trajectories is essential for safe and efficient autonomous driving. While deep learning models have improved performance, challenges remain in preventing off-road predictions and ensuring kinematic feasibility. Existing methods incorporate road-awareness modules and enforce kinematic constraints but lack plausibility guarantees and often introduce trade-offs in complexity and flexibility. This paper proposes a novel framework that formulates trajectory prediction as a constrained regression guided by permissible driving directions and their boundaries. Using the agent's current state and an HD map, our approach defines the valid boundaries and ensures on-road predictions by training the network to learn superimposed paths between left and right boundary polylines. To guarantee feasibility, the model predicts acceleration profiles that determine the vehicle's travel distance along these paths while adhering to kinematic constraints. We evaluate our approach on the Argoverse-2 dataset against the HPTR baseline. Our approach shows a slight decrease in benchmark metrics compared to HPTR but notably improves final displacement error and eliminates infeasible trajectories. Moreover, the proposed approach has superior generalization to less prevalent maneuvers and unseen out-of-distribution scenarios, reducing the off-road rate under adversarial attacks from 66% to just 1%. These results highlight the effectiveness of our approach in generating feasible and robust predictions.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TPK: Trustworthy Trajectory Prediction Integrating Prior Knowledge For Interpretability and Kinematic Feasibility</title>
<link>https://arxiv.org/abs/2505.06743</link>
<guid>https://arxiv.org/abs/2505.06743</guid>
<content:encoded><![CDATA[
arXiv:2505.06743v2 Announce Type: replace 
Abstract: Trajectory prediction is crucial for autonomous driving, enabling vehicles to navigate safely by anticipating the movements of surrounding road users. However, current deep learning models often lack trustworthiness as their predictions can be physically infeasible and illogical to humans. To make predictions more trustworthy, recent research has incorporated prior knowledge, like the social force model for modeling interactions and kinematic models for physical realism. However, these approaches focus on priors that suit either vehicles or pedestrians and do not generalize to traffic with mixed agent classes. We propose incorporating interaction and kinematic priors of all agent classes--vehicles, pedestrians, and cyclists with class-specific interaction layers to capture agent behavioral differences. To improve the interpretability of the agent interactions, we introduce DG-SFM, a rule-based interaction importance score that guides the interaction layer. To ensure physically feasible predictions, we proposed suitable kinematic models for all agent classes with a novel pedestrian kinematic model. We benchmark our approach on the Argoverse 2 dataset, using the state-of-the-art transformer HPTR as our baseline. Experiments demonstrate that our method improves interaction interpretability, revealing a correlation between incorrect predictions and divergence from our interaction prior. Even though incorporating the kinematic models causes a slight decrease in accuracy, they eliminate infeasible trajectories found in the dataset and the baseline model. Thus, our approach fosters trust in trajectory prediction as its interaction reasoning is interpretable, and its predictions adhere to physics.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Curriculum Learning for Driving Scenarios: Towards Robust and Efficient Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.08264</link>
<guid>https://arxiv.org/abs/2505.08264</guid>
<content:encoded><![CDATA[
arXiv:2505.08264v2 Announce Type: replace 
Abstract: This paper addresses the challenges of training end-to-end autonomous driving agents using Reinforcement Learning (RL). RL agents are typically trained in a fixed set of scenarios and nominal behavior of surrounding road users in simulations, limiting their generalization and real-life deployment. While domain randomization offers a potential solution by randomly sampling driving scenarios, it frequently results in inefficient training and sub-optimal policies due to the high variance among training scenarios. To address these limitations, we propose an automatic curriculum learning framework that dynamically generates driving scenarios with adaptive complexity based on the agent's evolving capabilities. Unlike manually designed curricula that introduce expert bias and lack scalability, our framework incorporates a ``teacher'' that automatically generates and mutates driving scenarios based on their learning potential -- an agent-centric metric derived from the agent's current policy -- eliminating the need for expert design. The framework enhances training efficiency by excluding scenarios the agent has mastered or finds too challenging. We evaluate our framework in a reinforcement learning setting where the agent learns a driving policy from camera images. Comparative results against baseline methods, including fixed scenario training and domain randomization, demonstrate that our approach leads to enhanced generalization, achieving higher success rates: +9% in low traffic density, +21% in high traffic density, and faster convergence with fewer training steps. Our findings highlight the potential of ACL in improving the robustness and efficiency of RL-based autonomous driving agents.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Outlook on the Opportunities and Challenges of Multi-Agent AI Systems</title>
<link>https://arxiv.org/abs/2505.18397</link>
<guid>https://arxiv.org/abs/2505.18397</guid>
<content:encoded><![CDATA[
arXiv:2505.18397v2 Announce Type: replace 
Abstract: A multi-agent AI system (MAS) is composed of multiple autonomous agents that interact, exchange information, and make decisions based on internal generative models. Recent advances in large language models and tool-using agents have made MAS increasingly practical in areas like scientific discovery and collaborative automation. However, key questions remain: When are MAS more effective than single-agent systems? What new safety risks arise from agent interactions? And how should we evaluate their reliability and structure? This paper outlines a formal framework for analyzing MAS, focusing on two core aspects: effectiveness and safety. We explore whether MAS truly improve robustness, adaptability, and performance, or merely repackage known techniques like ensemble learning. We also study how inter-agent dynamics may amplify or suppress system vulnerabilities. While MAS are relatively new to the signal processing community, we envision them as a powerful abstraction that extends classical tools like distributed estimation and sensor fusion to higher-level, policy-driven inference. Through experiments on data science automation, we highlight the potential of MAS to reshape how signal processing systems are designed and trusted.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Upgrade or Switch: Do We Need a Next-Gen Trusted Architecture for the Internet of AI Agents?</title>
<link>https://arxiv.org/abs/2506.12003</link>
<guid>https://arxiv.org/abs/2506.12003</guid>
<content:encoded><![CDATA[
arXiv:2506.12003v2 Announce Type: replace 
Abstract: The emerging Internet of AI Agents challenges existing web infrastructure designed for human-scale, reactive interactions. Unlike traditional web resources, autonomous AI agents initiate actions, maintain persistent state, spawn sub-agents, and negotiate directly with peers: demanding millisecond-level discovery, instant credential revocation, and cryptographic behavioral proofs that exceed current DNS/PKI capabilities. This paper analyzes whether to upgrade existing infrastructure or implement purpose-built index architectures for autonomous agents. We identify critical failure points: DNS propagation (24-48 hours vs. required milliseconds), certificate revocation unable to scale to trillions of entities, and IPv4/IPv6 addressing inadequate for agent-scale routing. We evaluate three approaches: (1) Upgrade paths, (2) Switch options, (3) Hybrid index/registries. Drawing parallels to dialup-to-broadband transitions, we find that agent requirements constitute qualitative, and not incremental, changes. While upgrades offer compatibility and faster deployment, clean-slate solutions provide better performance but require longer for adoption. Our analysis suggests hybrid approaches will emerge, with centralized indexes for critical agents and federated meshes for specialized use cases.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AGENTSAFE: Benchmarking the Safety of Embodied Agents on Hazardous Instructions</title>
<link>https://arxiv.org/abs/2506.14697</link>
<guid>https://arxiv.org/abs/2506.14697</guid>
<content:encoded><![CDATA[
arXiv:2506.14697v2 Announce Type: replace 
Abstract: The rapid advancement of vision-language models (VLMs) and their integration into embodied agents have unlocked powerful capabilities for decision-making. However, as these systems are increasingly deployed in real-world environments, they face mounting safety concerns, particularly when responding to hazardous instructions. In this work, we propose AGENTSAFE, the first comprehensive benchmark for evaluating the safety of embodied VLM agents under hazardous instructions. AGENTSAFE simulates realistic agent-environment interactions within a simulation sandbox and incorporates a novel adapter module that bridges the gap between high-level VLM outputs and low-level embodied controls. Specifically, it maps recognized visual entities to manipulable objects and translates abstract planning into executable atomic actions in the environment. Building on this, we construct a risk-aware instruction dataset inspired by Asimovs Three Laws of Robotics, including base risky instructions and mutated jailbroken instructions. The benchmark includes 45 adversarial scenarios, 1,350 hazardous tasks, and 8,100 hazardous instructions, enabling systematic testing under adversarial conditions ranging from perception, planning, and action execution stages.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Individual and Collective Welfare in Risk Sharing with Many States</title>
<link>https://arxiv.org/abs/2401.07337</link>
<guid>https://arxiv.org/abs/2401.07337</guid>
<content:encoded><![CDATA[
arXiv:2401.07337v4 Announce Type: replace-cross 
Abstract: We study efficient risk sharing among risk-averse agents in an economy with a large, finite number of states. Following a random shock to an initial agreement, agents may renegotiate. If they require a minimal utility improvement to accept a new deal, we show the probability of finding a mutually acceptable allocation vanishes exponentially as the state space grows. This holds regardless of agents' degree of risk aversion. In a two-agent multiple-priors model, we find that the potential for Pareto-improving trade requires that at least one agent's set of priors has a vanishingly small measure. Our results hinge on the ``shape does not matter'' message of high-dimensional isoperimetric inequalities.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Control Leveraging LLMs: An Agentic Framework for Next-Generation Industrial Automation</title>
<link>https://arxiv.org/abs/2507.07115</link>
<guid>https://arxiv.org/abs/2507.07115</guid>
<content:encoded><![CDATA[
arXiv:2507.07115v1 Announce Type: new 
Abstract: The increasing complexity of modern chemical processes, coupled with workforce shortages and intricate fault scenarios, demands novel automation paradigms that blend symbolic reasoning with adaptive control. In this work, we introduce a unified agentic framework that leverages large language models (LLMs) for both discrete fault-recovery planning and continuous process control within a single architecture. We adopt Finite State Machines (FSMs) as interpretable operating envelopes: an LLM-driven planning agent proposes recovery sequences through the FSM, a Simulation Agent executes and checks each transition, and a Validator-Reprompting loop iteratively refines invalid plans. In Case Study 1, across 180 randomly generated FSMs of varying sizes (4-25 states, 4-300 transitions), GPT-4o and GPT-4o-mini achieve 100% valid-path success within five reprompts-outperforming open-source LLMs in both accuracy and latency. In Case Study 2, the same framework modulates dual-heater inputs on a laboratory TCLab platform (and its digital twin) to maintain a target average temperature under persistent asymmetric disturbances. Compared to classical PID control, our LLM-based controller attains similar performance, while ablation of the prompting loop reveals its critical role in handling nonlinear dynamics. We analyze key failure modes-such as instruction following lapses and coarse ODE approximations. Our results demonstrate that, with structured feedback and modular agents, LLMs can unify high-level symbolic planningand low-level continuous control, paving the way towards resilient, language-driven automation in chemical engineering.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining Pre-Trained Models for Enhanced Feature Representation in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.07197</link>
<guid>https://arxiv.org/abs/2507.07197</guid>
<content:encoded><![CDATA[
arXiv:2507.07197v1 Announce Type: new 
Abstract: The recent focus and release of pre-trained models have been a key components to several advancements in many fields (e.g. Natural Language Processing and Computer Vision), as a matter of fact, pre-trained models learn disparate latent embeddings sharing insightful representations. On the other hand, Reinforcement Learning (RL) focuses on maximizing the cumulative reward obtained via agent's interaction with the environment. RL agents do not have any prior knowledge about the world, and they either learn from scratch an end-to-end mapping between the observation and action spaces or, in more recent works, are paired with monolithic and computationally expensive Foundational Models. How to effectively combine and leverage the hidden information of different pre-trained models simultaneously in RL is still an open and understudied question. In this work, we propose Weight Sharing Attention (WSA), a new architecture to combine embeddings of multiple pre-trained models to shape an enriched state representation, balancing the tradeoff between efficiency and performance. We run an extensive comparison between several combination modes showing that WSA obtains comparable performance on multiple Atari games compared to end-to-end models. Furthermore, we study the generalization capabilities of this approach and analyze how scaling the number of models influences agents' performance during and after training.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open Source Planning &amp; Control System with Language Agents for Autonomous Scientific Discovery</title>
<link>https://arxiv.org/abs/2507.07257</link>
<guid>https://arxiv.org/abs/2507.07257</guid>
<content:encoded><![CDATA[
arXiv:2507.07257v1 Announce Type: new 
Abstract: We present a multi-agent system for automation of scientific research tasks, cmbagent. The system is formed by about 30 Large Language Model (LLM) agents and implements a Planning & Control strategy to orchestrate the agentic workflow, with no human-in-the-loop at any point. Each agent specializes in a different task (performing retrieval on scientific papers and codebases, writing code, interpreting results, critiquing the output of other agents) and the system is able to execute code locally. We successfully apply cmbagent to carry out a PhD level cosmology task (the measurement of cosmological parameters using supernova data) and evaluate its performance on two benchmark sets, finding superior performance over state-of-the-art LLMs. The source code is available on GitHub, demonstration videos are also available, and the system is deployed on HuggingFace and will be available on the cloud.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Impact of Background Speech on Interruption Detection in Collaborative Groups</title>
<link>https://arxiv.org/abs/2507.07280</link>
<guid>https://arxiv.org/abs/2507.07280</guid>
<content:encoded><![CDATA[
arXiv:2507.07280v1 Announce Type: new 
Abstract: Interruption plays a crucial role in collaborative learning, shaping group interactions and influencing knowledge construction. AI-driven support can assist teachers in monitoring these interactions. However, most previous work on interruption detection and interpretation has been conducted in single-conversation environments with relatively clean audio. AI agents deployed in classrooms for collaborative learning within small groups will need to contend with multiple concurrent conversations -- in this context, overlapping speech will be ubiquitous, and interruptions will need to be identified in other ways. In this work, we analyze interruption detection in single-conversation and multi-group dialogue settings. We then create a state-of-the-art method for interruption identification that is robust to overlapping speech, and thus could be deployed in classrooms. Further, our work highlights meaningful linguistic and prosodic information about how interruptions manifest in collaborative group interactions. Our investigation also paves the way for future works to account for the influence of overlapping speech from multiple groups when tracking group dialog.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LangNavBench: Evaluation of Natural Language Understanding in Semantic Navigation</title>
<link>https://arxiv.org/abs/2507.07299</link>
<guid>https://arxiv.org/abs/2507.07299</guid>
<content:encoded><![CDATA[
arXiv:2507.07299v1 Announce Type: new 
Abstract: Recent progress in large vision-language models has driven improvements in language-based semantic navigation, where an embodied agent must reach a target object described in natural language. Despite these advances, we still lack a clear, language-focused benchmark for testing how well such agents ground the words in their instructions. We address this gap with LangNav, an open-set dataset specifically created to test an agent's ability to locate objects described at different levels of detail, from broad category names to fine attributes and object-object relations. Every description in LangNav was manually checked, yielding a lower error rate than existing lifelong- and semantic-navigation datasets. On top of LangNav we build LangNavBench, a benchmark that measures how well current semantic-navigation methods understand and act on these descriptions while moving toward their targets. LangNavBench allows us to systematically compare models on their handling of attributes, spatial and relational cues, and category hierarchies, offering the first thorough, language-centric evaluation of embodied navigation systems. We also present Multi-Layered Feature Map (MLFM), a method that builds a queryable multi-layered semantic map, particularly effective when dealing with small objects or instructions involving spatial relations. MLFM outperforms state-of-the-art mapping-based navigation baselines on the LangNav dataset.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Application of LLMs to Multi-Robot Path Planning and Task Allocation</title>
<link>https://arxiv.org/abs/2507.07302</link>
<guid>https://arxiv.org/abs/2507.07302</guid>
<content:encoded><![CDATA[
arXiv:2507.07302v1 Announce Type: new 
Abstract: Efficient exploration is a well known problem in deep reinforcement learning and this problem is exacerbated in multi-agent reinforcement learning due the intrinsic complexities of such algorithms. There are several approaches to efficiently explore an environment to learn to solve tasks by multi-agent operating in that environment, of which, the idea of expert exploration is investigated in this work. More specifically, this work investigates the application of large-language models as expert planners for efficient exploration in planning based tasks for multiple agents.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented Reasoning</title>
<link>https://arxiv.org/abs/2507.07306</link>
<guid>https://arxiv.org/abs/2507.07306</guid>
<content:encoded><![CDATA[
arXiv:2507.07306v1 Announce Type: new 
Abstract: LLM-based translation agents have achieved highly human-like translation results and are capable of handling longer and more complex contexts with greater efficiency. However, they are typically limited to text-only inputs. In this paper, we introduce ViDove, a translation agent system designed for multimodal input. Inspired by the workflow of human translators, ViDove leverages visual and contextual background information to enhance the translation process. Additionally, we integrate a multimodal memory system and long-short term memory modules enriched with domain-specific knowledge, enabling the agent to perform more accurately and adaptively in real-world scenarios. As a result, ViDove achieves significantly higher translation quality in both subtitle generation and general translation tasks, with a 28% improvement in BLEU scores and a 15% improvement in SubER compared to previous state-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark for long-form automatic video subtitling and translation, featuring 17 hours of high-quality, human-annotated data. Our code is available here: https://github.com/pigeonai-org/ViDove
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation</title>
<link>https://arxiv.org/abs/2507.07307</link>
<guid>https://arxiv.org/abs/2507.07307</guid>
<content:encoded><![CDATA[
arXiv:2507.07307v1 Announce Type: new 
Abstract: Large language models (LLMs) incorporated with Retrieval-Augmented Generation (RAG) have demonstrated powerful capabilities in generating counterspeech against misinformation. However, current studies rely on limited evidence and offer less control over final outputs. To address these challenges, we propose a Multi-agent Retrieval-Augmented Framework to generate counterspeech against health misinformation, incorporating multiple LLMs to optimize knowledge retrieval, evidence enhancement, and response refinement. Our approach integrates both static and dynamic evidence, ensuring that the generated counterspeech is relevant, well-grounded, and up-to-date. Our method outperforms baseline approaches in politeness, relevance, informativeness, and factual accuracy, demonstrating its effectiveness in generating high-quality counterspeech. To further validate our approach, we conduct ablation studies to verify the necessity of each component in our framework. Furthermore, human evaluations reveal that refinement significantly enhances counterspeech quality and obtains human preference.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Communication and Device Clustering for Clustered Federated Learning with Differential Privacy</title>
<link>https://arxiv.org/abs/2507.07320</link>
<guid>https://arxiv.org/abs/2507.07320</guid>
<content:encoded><![CDATA[
arXiv:2507.07320v1 Announce Type: new 
Abstract: In this paper, a secure and communication-efficient clustered federated learning (CFL) design is proposed. In our model, several base stations (BSs) with heterogeneous task-handling capabilities and multiple users with non-independent and identically distributed (non-IID) data jointly perform CFL training incorporating differential privacy (DP) techniques. Since each BS can process only a subset of the learning tasks and has limited wireless resource blocks (RBs) to allocate to users for federated learning (FL) model parameter transmission, it is necessary to jointly optimize RB allocation and user scheduling for CFL performance optimization. Meanwhile, our considered CFL method requires devices to use their limited data and FL model information to determine their task identities, which may introduce additional communication overhead. We formulate an optimization problem whose goal is to minimize the training loss of all learning tasks while considering device clustering, RB allocation, DP noise, and FL model transmission delay. To solve the problem, we propose a novel dynamic penalty function assisted value decomposed multi-agent reinforcement learning (DPVD-MARL) algorithm that enables distributed BSs to independently determine their connected users, RBs, and DP noise of the connected users but jointly minimize the training loss of all learning tasks across all BSs. Different from the existing MARL methods that assign a large penalty for invalid actions, we propose a novel penalty assignment scheme that assigns penalty depending on the number of devices that cannot meet communication constraints (e.g., delay), which can guide the MARL scheme to quickly find valid actions, thus improving the convergence speed. Simulation results show that the DPVD-MARL can improve the convergence rate by up to 20% and the ultimate accumulated rewards by 15% compared to independent Q-learning.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Model Splitting and Device Task Assignment for Deceptive Signal Assisted Private Multi-hop Split Learning</title>
<link>https://arxiv.org/abs/2507.07323</link>
<guid>https://arxiv.org/abs/2507.07323</guid>
<content:encoded><![CDATA[
arXiv:2507.07323v1 Announce Type: new 
Abstract: In this paper, deceptive signal-assisted private split learning is investigated. In our model, several edge devices jointly perform collaborative training, and some eavesdroppers aim to collect the model and data information from devices. To prevent the eavesdroppers from collecting model and data information, a subset of devices can transmit deceptive signals. Therefore, it is necessary to determine the subset of devices used for deceptive signal transmission, the subset of model training devices, and the models assigned to each model training device. This problem is formulated as an optimization problem whose goal is to minimize the information leaked to eavesdroppers while meeting the model training energy consumption and delay constraints. To solve this problem, we propose a soft actor-critic deep reinforcement learning framework with intrinsic curiosity module and cross-attention (ICM-CA) that enables a centralized agent to determine the model training devices, the deceptive signal transmission devices, the transmit power, and sub-models assigned to each model training device without knowing the position and monitoring probability of eavesdroppers. The proposed method uses an ICM module to encourage the server to explore novel actions and states and a CA module to determine the importance of each historical state-action pair thus improving training efficiency. Simulation results demonstrate that the proposed method improves the convergence rate by up to 3x and reduces the information leaked to eavesdroppers by up to 13% compared to the traditional SAC algorithm.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLoRA: An Advanced AI-Powered Engine to Facilitate Hybrid Human-AI Regulated Learning</title>
<link>https://arxiv.org/abs/2507.07362</link>
<guid>https://arxiv.org/abs/2507.07362</guid>
<content:encoded><![CDATA[
arXiv:2507.07362v1 Announce Type: new 
Abstract: SRL, defined as learners' ability to systematically plan, monitor, and regulate their learning activities, is crucial for sustained academic achievement and lifelong learning competencies. Emerging Artificial Intelligence (AI) developments profoundly influence SRL interactions by potentially either diminishing or strengthening learners' opportunities to exercise their own regulatory skills. Recent literature emphasizes a balanced approach termed Hybrid Human-AI Regulated Learning (HHAIRL), in which AI provides targeted, timely scaffolding while preserving the learners' role as active decision-makers and reflective monitors of their learning process. Nevertheless, existing digital tools frequently fall short, lacking adaptability, focusing narrowly on isolated SRL phases, and insufficiently support meaningful human-AI interactions. In response, this paper introduces the enhanced \flora Engine, which incorporates advanced Generative Artificial Intelligence (GenAI) features and state-of-the-art learning analytics, explicitly grounded in SRL and HHAIRL theories. The \flora Engine offers instrumentation tools such as collaborative writing, multi-agents chatbot, and detailed learning trace logging to support dynamic, adaptive scaffolding tailored to individual needs in real time. We further present a summary of several research studies that provide the validations for and illustrate how these instrumentation tools can be utilized in real-world educational and experimental contexts. These studies demonstrate the effectiveness of \flora Engine in fostering SRL and HHAIRL, providing both theoretical insights and practical solutions for the future of AI-enhanced learning context.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PILOC: A Pheromone Inverse Guidance Mechanism and Local-Communication Framework for Dynamic Target Search of Multi-Agent in Unknown Environments</title>
<link>https://arxiv.org/abs/2507.07376</link>
<guid>https://arxiv.org/abs/2507.07376</guid>
<content:encoded><![CDATA[
arXiv:2507.07376v1 Announce Type: new 
Abstract: Multi-Agent Search and Rescue (MASAR) plays a vital role in disaster response, exploration, and reconnaissance. However, dynamic and unknown environments pose significant challenges due to target unpredictability and environmental uncertainty. To tackle these issues, we propose PILOC, a framework that operates without global prior knowledge, leveraging local perception and communication. It introduces a pheromone inverse guidance mechanism to enable efficient coordination and dynamic target localization. PILOC promotes decentralized cooperation through local communication, significantly reducing reliance on global channels. Unlike conventional heuristics, the pheromone mechanism is embedded into the observation space of Deep Reinforcement Learning (DRL), supporting indirect agent coordination based on environmental cues. We further integrate this strategy into a DRL-based multi-agent architecture and conduct extensive experiments. Results show that combining local communication with pheromone-based guidance significantly boosts search efficiency, adaptability, and system robustness. Compared to existing methods, PILOC performs better under dynamic and communication-constrained scenarios, offering promising directions for future MASAR applications.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent Workflows</title>
<link>https://arxiv.org/abs/2507.07400</link>
<guid>https://arxiv.org/abs/2507.07400</guid>
<content:encoded><![CDATA[
arXiv:2507.07400v1 Announce Type: new 
Abstract: Large language model (LLM) based agentic workflows have become a popular paradigm for coordinating multiple specialized agents to solve complex tasks. To improve serving efficiency, existing LLM systems employ prefix caching to reuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby avoiding redundant computation across repeated invocations. However, current systems typically evict KV caches using a Least Recently Used (LRU) policy, which fails to anticipate future agent usage and often discards KV caches shortly before their reuse. This leads to frequent cache misses and substantial recomputation or swapping overhead. We present KVFlow, a workflow-aware KV cache management framework tailored for agentic workloads. KVFlow abstracts the agent execution schedule as an Agent Step Graph and assigns each agent a steps-to-execution value that estimates its temporal proximity to future activation. These values guide a fine-grained eviction policy at the KV node level, allowing KVFlow to preserve entries likely to be reused and efficiently manage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a fully overlapped KV prefetching mechanism, which proactively loads required tensors from CPU to GPU in background threads for agents scheduled in the next step, thereby avoiding cache miss stalls during generation. Compared to SGLang with hierarchical radix cache, KVFlow achieves up to 1.83$\times$ speedup for single workflows with large prompts, and up to 2.19$\times$ speedup for scenarios with many concurrent workflows.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2507.07426</link>
<guid>https://arxiv.org/abs/2507.07426</guid>
<content:encoded><![CDATA[
arXiv:2507.07426v1 Announce Type: new 
Abstract: Recent advances in large language models have demonstrated considerable potential in scientific domains such as drug discovery. However, their effectiveness remains constrained when reasoning extends beyond the knowledge acquired during pretraining. Conventional approaches, such as fine-tuning or retrieval-augmented generation, face limitations in either imposing high computational overhead or failing to fully exploit structured scientific data. To overcome these challenges, we propose DrugMCTS, a novel framework that synergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree Search for drug repurposing. The framework employs five specialized agents tasked with retrieving and analyzing molecular and protein information, thereby enabling structured and iterative reasoning. Without requiring domain-specific fine-tuning, DrugMCTS empowers Qwen2.5-7B-Instruct to outperform Deepseek-R1 by over 20\%. Extensive experiments on the DrugBank and KIBA datasets demonstrate that DrugMCTS achieves substantially higher recall and robustness compared to both general-purpose LLMs and deep learning baselines. Our results highlight the importance of structured reasoning, agent-based collaboration, and feedback-driven search mechanisms in advancing LLM applications for drug discovery.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAND: Boosting LLM Agents with Self-Taught Action Deliberation</title>
<link>https://arxiv.org/abs/2507.07441</link>
<guid>https://arxiv.org/abs/2507.07441</guid>
<content:encoded><![CDATA[
arXiv:2507.07441v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents are commonly tuned with supervised finetuning on ReAct-style expert trajectories or preference optimization over pairwise rollouts. Most of these methods focus on imitating specific expert behaviors or promoting chosen reasoning thoughts and actions over rejected ones. However, without reasoning and comparing over alternatives actions, LLM agents finetuned with these methods may over-commit towards seemingly plausible but suboptimal actions due to limited action space exploration. To address this, in this paper we propose Self-taught ActioN Deliberation (SAND) framework, enabling LLM agents to explicitly deliberate over candidate actions before committing to one. To tackle the challenges of when and what to deliberate given large action space and step-level action evaluation, we incorporate self-consistency action sampling and execution-guided action critique to help synthesize step-wise action deliberation thoughts using the base model of the LLM agent. In an iterative manner, the deliberation trajectories are then used to finetune the LLM agent itself. Evaluating on two representative interactive agent tasks, SAND achieves an average 20% improvement over initial supervised finetuning and also outperforms state-of-the-art agent tuning approaches.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StarDojo: Benchmarking Open-Ended Behaviors of Agentic Multimodal LLMs in Production-Living Simulations with Stardew Valley</title>
<link>https://arxiv.org/abs/2507.07445</link>
<guid>https://arxiv.org/abs/2507.07445</guid>
<content:encoded><![CDATA[
arXiv:2507.07445v1 Announce Type: new 
Abstract: Autonomous agents navigating human society must master both production activities and social interactions, yet existing benchmarks rarely evaluate these skills simultaneously. To bridge this gap, we introduce StarDojo, a novel benchmark based on Stardew Valley, designed to assess AI agents in open-ended production-living simulations. In StarDojo, agents are tasked to perform essential livelihood activities such as farming and crafting, while simultaneously engaging in social interactions to establish relationships within a vibrant community. StarDojo features 1,000 meticulously curated tasks across five key domains: farming, crafting, exploration, combat, and social interactions. Additionally, we provide a compact subset of 100 representative tasks for efficient model evaluation. The benchmark offers a unified, user-friendly interface that eliminates the need for keyboard and mouse control, supports all major operating systems, and enables the parallel execution of multiple environment instances, making it particularly well-suited for evaluating the most capable foundation agents, powered by multimodal large language models (MLLMs). Extensive evaluations of state-of-the-art MLLMs agents demonstrate substantial limitations, with the best-performing model, GPT-4.1, achieving only a 12.7% success rate, primarily due to challenges in visual understanding, multimodal reasoning and low-level manipulation. As a user-friendly environment and benchmark, StarDojo aims to facilitate further research towards robust, open-ended agents in complex production-living environments.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucination Stations: On Some Basic Limitations of Transformer-Based Language Models</title>
<link>https://arxiv.org/abs/2507.07505</link>
<guid>https://arxiv.org/abs/2507.07505</guid>
<content:encoded><![CDATA[
arXiv:2507.07505v1 Announce Type: new 
Abstract: With widespread adoption of transformer-based language models in AI, there is significant interest in the limits of LLMs capabilities, specifically so-called hallucinations, occurrences in which LLMs provide spurious, factually incorrect or nonsensical information when prompted on certain subjects. Furthermore, there is growing interest in agentic uses of LLMs - that is, using LLMs to create agents that act autonomously or semi-autonomously to carry out various tasks, including tasks with applications in the real world. This makes it important to understand the types of tasks LLMs can and cannot perform. We explore this topic from the perspective of the computational complexity of LLM inference. We show that LLMs are incapable of carrying out computational and agentic tasks beyond a certain complexity, and further that LLMs are incapable of verifying the accuracy of tasks beyond a certain complexity. We present examples of both, then discuss some consequences of this work.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Pandora's Box Problem with Sequential Inspections</title>
<link>https://arxiv.org/abs/2507.07508</link>
<guid>https://arxiv.org/abs/2507.07508</guid>
<content:encoded><![CDATA[
arXiv:2507.07508v1 Announce Type: new 
Abstract: The Pandora's box problem (Weitzman 1979) is a core model in economic theory that captures an agent's (Pandora's) search for the best alternative (box). We study an important generalization of the problem where the agent can either fully open boxes for a certain fee to reveal their exact values or partially open them at a reduced cost. This introduces a new tradeoff between information acquisition and cost efficiency. We establish a hardness result and employ an array of techniques in stochastic optimization to provide a comprehensive analysis of this model. This includes (1) the identification of structural properties of the optimal policy that provide insights about optimal decisions; (2) the derivation of problem relaxations and provably near-optimal solutions; (3) the characterization of the optimal policy in special yet non-trivial cases; and (4) an extensive numerical study that compares the performance of various policies, and which provides additional insights about the optimal policy. Throughout, we show that intuitive threshold-based policies that extend the Pandora's box optimal solution can effectively guide search decisions.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: We Need An Algorithmic Understanding of Generative AI</title>
<link>https://arxiv.org/abs/2507.07544</link>
<guid>https://arxiv.org/abs/2507.07544</guid>
<content:encoded><![CDATA[
arXiv:2507.07544v1 Announce Type: new 
Abstract: What algorithms do LLMs actually learn and use to solve problems? Studies addressing this question are sparse, as research priorities are focused on improving performance through scale, leaving a theoretical and empirical gap in understanding emergent algorithms. This position paper proposes AlgEval: a framework for systematic research into the algorithms that LLMs learn and use. AlgEval aims to uncover algorithmic primitives, reflected in latent representations, attention, and inference-time compute, and their algorithmic composition to solve task-specific problems. We highlight potential methodological paths and a case study toward this goal, focusing on emergent search algorithms. Our case study illustrates both the formation of top-down hypotheses about candidate algorithms, and bottom-up tests of these hypotheses via circuit-level analysis of attention patterns and hidden states. The rigorous, systematic evaluation of how LLMs actually solve tasks provides an alternative to resource-intensive scaling, reorienting the field toward a principled understanding of underlying computations. Such algorithmic explanations offer a pathway to human-understandable interpretability, enabling comprehension of the model's internal reasoning performance measures. This can in turn lead to more sample-efficient methods for training and improving performance, as well as novel architectures for end-to-end and multi-agent systems.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Connectivity: Higher-Order Network Framework for Capturing Memory-Driven Mobility Dynamics</title>
<link>https://arxiv.org/abs/2507.07727</link>
<guid>https://arxiv.org/abs/2507.07727</guid>
<content:encoded><![CDATA[
arXiv:2507.07727v1 Announce Type: new 
Abstract: Understanding and predicting mobility dynamics in transportation networks is critical for infrastructure planning, resilience analysis, and traffic management. Traditional graph-based models typically assume memoryless movement, limiting their ability to capture sequential dependencies inherent in real-world mobility patterns. In this study, we introduce a novel higher-order network framework for modeling memory-dependent dynamics in transportation systems. By extending classical graph representations through higher-order Markov chains and de Bruijn graph structures, our framework encodes the spatial and temporal ordering of traversed paths, enabling the analysis of structurally and functionally critical components with improved fidelity. We generalize key network analytics, including betweenness centrality, PageRank, and next-step prediction, to this higher-order setting and validate our approach on the Sioux Falls transportation network using agent-based trajectory data generated with MATSim. Experimental results demonstrate that higher-order models outperform first-order baselines across multiple tasks, with the third-order model achieving an optimal balance between predictive accuracy and model complexity. These findings highlight the importance of incorporating memory effects into network-based transportation analysis and offer a scalable, data-driven methodology for capturing complex mobility behaviors in infrastructure systems.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BEAVER: Building Environments with Assessable Variation for Evaluating Multi-Objective Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.07769</link>
<guid>https://arxiv.org/abs/2507.07769</guid>
<content:encoded><![CDATA[
arXiv:2507.07769v1 Announce Type: new 
Abstract: Recent years have seen significant advancements in designing reinforcement learning (RL)-based agents for building energy management. While individual success is observed in simulated or controlled environments, the scalability of RL approaches in terms of efficiency and generalization across building dynamics and operational scenarios remains an open question. In this work, we formally characterize the generalization space for the cross-environment, multi-objective building energy management task, and formulate the multi-objective contextual RL problem. Such a formulation helps understand the challenges of transferring learned policies across varied operational contexts such as climate and heat convection dynamics under multiple control objectives such as comfort level and energy consumption. We provide a principled way to parameterize such contextual information in realistic building RL environments, and construct a novel benchmark to facilitate the evaluation of generalizable RL algorithms in practical building control tasks. Our results show that existing multi-objective RL methods are capable of achieving reasonable trade-offs between conflicting objectives. However, their performance degrades under certain environment variations, underscoring the importance of incorporating dynamics-dependent contextual information into the policy learning process.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perceptual Distortions and Autonomous Representation Learning in a Minimal Robotic System</title>
<link>https://arxiv.org/abs/2507.07845</link>
<guid>https://arxiv.org/abs/2507.07845</guid>
<content:encoded><![CDATA[
arXiv:2507.07845v1 Announce Type: new 
Abstract: Autonomous agents, particularly in the field of robotics, rely on sensory information to perceive and navigate their environment. However, these sensory inputs are often imperfect, leading to distortions in the agent's internal representation of the world. This paper investigates the nature of these perceptual distortions and how they influence autonomous representation learning using a minimal robotic system. We utilize a simulated two-wheeled robot equipped with distance sensors and a compass, operating within a simple square environment. Through analysis of the robot's sensor data during random exploration, we demonstrate how a distorted perceptual space emerges. Despite these distortions, we identify emergent structures within the perceptual space that correlate with the physical environment, revealing how the robot autonomously learns a structured representation for navigation without explicit spatial information. This work contributes to the understanding of embodied cognition, minimal agency, and the role of perception in self-generated navigation strategies in artificial life.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Trust Fabric: Decentralized Interoperability and Economic Coordination for the Agentic Web</title>
<link>https://arxiv.org/abs/2507.07901</link>
<guid>https://arxiv.org/abs/2507.07901</guid>
<content:encoded><![CDATA[
arXiv:2507.07901v1 Announce Type: new 
Abstract: The fragmentation of AI agent ecosystems has created urgent demands for interoperability, trust, and economic coordination that current protocols -- including MCP (Hou et al., 2025), A2A (Habler et al., 2025), ACP (Liu et al., 2025), and Cisco's AGP (Edwards, 2025) -- cannot address at scale. We present the Nanda Unified Architecture, a decentralized framework built around three core innovations: fast DID-based agent discovery through distributed registries, semantic agent cards with verifiable credentials and composability profiles, and a dynamic trust layer that integrates behavioral attestations with policy compliance. The system introduces X42/H42 micropayments for economic coordination and MAESTRO, a security framework incorporating Synergetics' patented AgentTalk protocol (US Patent 12,244,584 B1) and secure containerization. Real-world deployments demonstrate 99.9 percent compliance in healthcare applications and substantial monthly transaction volumes with strong privacy guarantees. By unifying MIT's trust research with production deployments from Cisco and Synergetics, we show how cryptographic proofs and policy-as-code transform agents into trust-anchored participants in a decentralized economy (Lakshmanan, 2025; Sha, 2025). The result enables a globally interoperable Internet of Agents where trust becomes the native currency of collaboration across both enterprise and Web3 ecosystems.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Retrieval of Topics and Insights from Earnings Calls</title>
<link>https://arxiv.org/abs/2507.07906</link>
<guid>https://arxiv.org/abs/2507.07906</guid>
<content:encoded><![CDATA[
arXiv:2507.07906v1 Announce Type: new 
Abstract: Tracking the strategic focus of companies through topics in their earnings calls is a key task in financial analysis. However, as industries evolve, traditional topic modeling techniques struggle to dynamically capture emerging topics and their relationships. In this work, we propose an LLM-agent driven approach to discover and retrieve emerging topics from quarterly earnings calls. We propose an LLM-agent to extract topics from documents, structure them into a hierarchical ontology, and establish relationships between new and existing topics through a topic ontology. We demonstrate the use of extracted topics to infer company-level insights and emerging trends over time. We evaluate our approach by measuring ontology coherence, topic evolution accuracy, and its ability to surface emerging financial trends.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIRIX: Multi-Agent Memory System for LLM-Based Agents</title>
<link>https://arxiv.org/abs/2507.07957</link>
<guid>https://arxiv.org/abs/2507.07957</guid>
<content:encoded><![CDATA[
arXiv:2507.07957v1 Announce Type: new 
Abstract: Although memory capabilities of AI agents are gaining increasing attention, existing solutions remain fundamentally limited. Most rely on flat, narrowly scoped memory components, constraining their ability to personalize, abstract, and reliably recall user-specific information over time. To this end, we introduce MIRIX, a modular, multi-agent memory system that redefines the future of AI memory by solving the field's most critical challenge: enabling language models to truly remember. Unlike prior approaches, MIRIX transcends text to embrace rich visual and multimodal experiences, making memory genuinely useful in real-world scenarios. MIRIX consists of six distinct, carefully structured memory types: Core, Episodic, Semantic, Procedural, Resource Memory, and Knowledge Vault, coupled with a multi-agent framework that dynamically controls and coordinates updates and retrieval. This design enables agents to persist, reason over, and accurately retrieve diverse, long-term user data at scale. We validate MIRIX in two demanding settings. First, on ScreenshotVQA, a challenging multimodal benchmark comprising nearly 20,000 high-resolution computer screenshots per sequence, requiring deep contextual understanding and where no existing memory systems can be applied, MIRIX achieves 35% higher accuracy than the RAG baseline while reducing storage requirements by 99.9%. Second, on LOCOMO, a long-form conversation benchmark with single-modal textual input, MIRIX attains state-of-the-art performance of 85.4%, far surpassing existing baselines. These results show that MIRIX sets a new performance standard for memory-augmented LLM agents. To allow users to experience our memory system, we provide a packaged application powered by MIRIX. It monitors the screen in real time, builds a personalized memory base, and offers intuitive visualization and secure local storage to ensure privacy.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning with Action Chunking</title>
<link>https://arxiv.org/abs/2507.07969</link>
<guid>https://arxiv.org/abs/2507.07969</guid>
<content:encoded><![CDATA[
arXiv:2507.07969v1 Announce Type: new 
Abstract: We present Q-chunking, a simple yet effective recipe for improving reinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks. Our recipe is designed for the offline-to-online RL setting, where the goal is to leverage an offline prior dataset to maximize the sample-efficiency of online learning. Effective exploration and sample-efficient learning remain central challenges in this setting, as it is not obvious how the offline data should be utilized to acquire a good exploratory policy. Our key insight is that action chunking, a technique popularized in imitation learning where sequences of future actions are predicted rather than a single action at each timestep, can be applied to temporal difference (TD)-based RL methods to mitigate the exploration challenge. Q-chunking adopts action chunking by directly running RL in a 'chunked' action space, enabling the agent to (1) leverage temporally consistent behaviors from offline data for more effective online exploration and (2) use unbiased $n$-step backups for more stable and efficient TD learning. Our experimental results demonstrate that Q-chunking exhibits strong offline performance and online sample efficiency, outperforming prior best offline-to-online methods on a range of long-horizon, sparse-reward manipulation tasks.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding</title>
<link>https://arxiv.org/abs/2507.07984</link>
<guid>https://arxiv.org/abs/2507.07984</guid>
<content:encoded><![CDATA[
arXiv:2507.07984v1 Announce Type: new 
Abstract: Recent advances in multimodal large language models (MLLMs) have shown remarkable capabilities in integrating vision and language for complex reasoning. While most existing benchmarks evaluate models under offline settings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a benchmark designed to evaluate Online Spatio-Temporal understanding from the perspective of an agent actively exploring a scene. The Online aspect emphasizes the need to process and reason over incrementally acquired observations, while the Spatio-Temporal component requires integrating current visual inputs with historical memory to support dynamic spatial reasoning. OST-Bench better reflects the challenges of real-world embodied perception. Built on an efficient data collection pipeline, OST-Bench consists of 1.4k scenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and ARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that they fall short on tasks requiring complex spatio-temporal reasoning. Under the online setting, their accuracy declines as the exploration horizon extends and the memory grows. Through further experimental analysis, we identify common error patterns across models and find that both complex clue-based spatial reasoning demands and long-term memory retrieval requirements significantly drop model performance along two separate axes, highlighting the core challenges that must be addressed to improve online embodied reasoning. To foster further research and development in the field, our codes, dataset, and benchmark are available. Our project page is: https://rbler1234.github.io/OSTBench.github.io/
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PyVision: Agentic Vision with Dynamic Tooling</title>
<link>https://arxiv.org/abs/2507.07998</link>
<guid>https://arxiv.org/abs/2507.07998</guid>
<content:encoded><![CDATA[
arXiv:2507.07998v1 Announce Type: new 
Abstract: LLMs are increasingly deployed as agents, systems capable of planning, reasoning, and dynamically calling external tools. However, in visual reasoning, prior approaches largely remain limited by predefined workflows and static toolsets. In this report, we present PyVision, an interactive, multi-turn framework that enables MLLMs to autonomously generate, execute, and refine Python-based tools tailored to the task at hand, unlocking flexible and interpretable problem-solving. We develop a taxonomy of the tools created by PyVision and analyze their usage across a diverse set of benchmarks. Quantitatively, PyVision achieves consistent performance gains, boosting GPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini. These results point to a broader shift: dynamic tooling allows models not just to use tools, but to invent them, advancing toward more agentic visual reasoning.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust signal decompositions on the circle</title>
<link>https://arxiv.org/abs/2507.07007</link>
<guid>https://arxiv.org/abs/2507.07007</guid>
<content:encoded><![CDATA[
arXiv:2507.07007v1 Announce Type: cross 
Abstract: We consider the problem of decomposing a piecewise constant function on the circle into a sum of indicator functions of closed circular disks in the plane, whose number and location are not a priori known. This represents a situation where an agent moving on the circle is able to sense its proximity to some landmarks, and the goal is to estimate the number of these landmarks and their possible locations -- which can in turn enable control tasks such as motion planning and obstacle avoidance. Moreover, the exact values of the function at its discontinuities (which correspond to disk boundaries for the individual indicator functions) are not assumed to be known to the agent. We introduce suitable notions of robustness and degrees of freedom to single out those decompositions that are more desirable, or more likely, given this non-precise data collected by the agent. We provide a characterization of robust decompositions and give a procedure for generating all such decompositions. When the given function admits a robust decomposition, we compute the number of possible robust decompositions and derive bounds for the number of decompositions maximizing the degrees of freedom.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics</title>
<link>https://arxiv.org/abs/2507.07155</link>
<guid>https://arxiv.org/abs/2507.07155</guid>
<content:encoded><![CDATA[
arXiv:2507.07155v1 Announce Type: cross 
Abstract: We evaluate 9 Retrieval Augmented Generation (RAG) agent configurations on 105 Cosmology Question-Answer (QA) pairs that we built specifically for this purpose.The RAG configurations are manually evaluated by a human expert, that is, a total of 945 generated answers were assessed. We find that currently the best RAG agent configuration is with OpenAI embedding and generative model, yielding 91.4\% accuracy. Using our human evaluation results we calibrate LLM-as-a-Judge (LLMaaJ) system which can be used as a robust proxy for human evaluation. These results allow us to systematically select the best RAG agent configuration for multi-agent system for autonomous scientific discovery in astrophysics (e.g., cmbagent presented in a companion paper) and provide us with an LLMaaJ system that can be scaled to thousands of cosmology QA pairs. We make our QA dataset, human evaluation results, RAG pipelines, and LLMaaJ system publicly available for further use by the astrophysics community.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convergence and Robustness Bounds for Distributed Asynchronous Shortest-Path</title>
<link>https://arxiv.org/abs/2507.07263</link>
<guid>https://arxiv.org/abs/2507.07263</guid>
<content:encoded><![CDATA[
arXiv:2507.07263v1 Announce Type: cross 
Abstract: This work analyzes convergence times and robustness bounds for asynchronous distributed shortest-path computation. We focus on the Adaptive Bellman--Ford algorithm, a self-stabilizing method in which each agent updates its shortest-path estimate based only on the estimates of its neighbors and forgetting its previous estimate. In the asynchronous framework considered in this paper, agents are allowed to idle or encounter race conditions during their execution of the Adaptive Bellman--Ford algorithm. We build on Lyapunov-based results that develop finite-time convergence and robustness bounds for the synchronous shortest-path setting, in order to produce finite-time convergence and robustness bounds for the asynchronous setting. We also explore robustness against interval-bounded noise processes and establish convergence and robustness guarantees for asynchronous most-probable-path algorithms.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Search and Matching for Adoption from Foster Care</title>
<link>https://arxiv.org/abs/2103.10145</link>
<guid>https://arxiv.org/abs/2103.10145</guid>
<content:encoded><![CDATA[
arXiv:2103.10145v4 Announce Type: replace 
Abstract: To find families for the more than 100,000 children in need of adoptive placements, most United States child welfare agencies have employed a family-driven search approach in which prospective families respond to announcements made by the agency. However, some agencies have switched to a caseworker-driven search approach in which the caseworker directly contacts families recommended for a child. We introduce a novel search-and-matching model that captures the key features of the adoption process and compare family-driven with caseworker-driven search in a game-theoretical framework. Under either approach, the equilibria are generated by threshold strategies and form a lattice structure. Our main theoretical finding then shows that no family-driven equilibrium can Pareto dominate any caseworker-driven outcome, whereas it is possible that each caseworker-driven equilibrium Pareto dominates every equilibrium attainable under family-driven search. We also find that when families are sufficiently impatient, caseworker-driven search is better for all children. We illustrate numerically that most agents are better off under caseworker-driven search for a wide range of parameters. Finally, we provide empirical evidence from an agency that switched to caseworker-driven search and achieved a three-year adoption probability that outperformed a statewide benchmark by 24%, as well as a statistically significant 27% improvement in adoption hazard rates.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random walk based snapshot clustering for detecting community dynamics in temporal networks</title>
<link>https://arxiv.org/abs/2412.12187</link>
<guid>https://arxiv.org/abs/2412.12187</guid>
<content:encoded><![CDATA[
arXiv:2412.12187v3 Announce Type: replace 
Abstract: The evolution of many dynamical systems that describe relationships or interactions between objects can be effectively modeled by temporal networks, which are typically represented as a sequence of static network snapshots. In this paper, we introduce a novel random walk-based approach that can identify clusters of time-snapshots in which network community structures are stable. This allows us to detect significant structural shifts over time, such as the splitting or merging of communities or their births and deaths. We also provide a low-dimensional representation of entire snapshots, placing those with similar community structure close to each other in the feature space. To validate our approach, we develop an agent-based algorithm that generates synthetic datasets with the desired characteristic properties, enabling thorough testing and benchmarking. We further demonstrate the effectiveness and broad applicability of our technique by testing it on various social dynamics models and real-world datasets and comparing its performance to several state-of-the-art algorithms. Our findings highlight the strength of our approach to correctly capture and analyze the dynamics of complex systems.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding AI Judgment: How LLMs Assess News Credibility and Bias</title>
<link>https://arxiv.org/abs/2502.04426</link>
<guid>https://arxiv.org/abs/2502.04426</guid>
<content:encoded><![CDATA[
arXiv:2502.04426v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly embedded in workflows that involve evaluative processes. This raises the need to examine how such evaluations are built, what assumptions they rely on, and how their strategies diverge from those of humans. We benchmark six LLMs against expert ratings--NewsGuard and Media Bias/Fact Check (MBFC)--and against human judgments collected through a controlled experiment. To enable direct comparison, we implement a structured agentic framework in which both models and non-expert participants follow the same evaluation procedure: selecting criteria, retrieving content, and producing justifications. Despite output alignment, LLMs rely on different mechanisms: lexical associations and statistical priors replace contextual reasoning. This reliance produces systematic effects: political asymmetries, opaque justifications, and a tendency to confuse linguistic form with epistemic validity. Delegating judgment to such systems does not merely automate evaluation--it redefines it, shifting from normative reasoning to pattern-based approximation.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARBoids: Adaptive Residual Reinforcement Learning With Boids Model for Cooperative Multi-USV Target Defense</title>
<link>https://arxiv.org/abs/2502.18549</link>
<guid>https://arxiv.org/abs/2502.18549</guid>
<content:encoded><![CDATA[
arXiv:2502.18549v2 Announce Type: replace 
Abstract: The target defense problem (TDP) for unmanned surface vehicles (USVs) concerns intercepting an adversarial USV before it breaches a designated target region, using one or more defending USVs. A particularly challenging scenario arises when the attacker exhibits superior maneuverability compared to the defenders, significantly complicating effective interception. To tackle this challenge, this letter introduces ARBoids, a novel adaptive residual reinforcement learning framework that integrates deep reinforcement learning (DRL) with the biologically inspired, force-based Boids model. Within this framework, the Boids model serves as a computationally efficient baseline policy for multi-agent coordination, while DRL learns a residual policy to adaptively refine and optimize the defenders' actions. The proposed approach is validated in a high-fidelity Gazebo simulation environment, demonstrating superior performance over traditional interception strategies, including pure force-based approaches and vanilla DRL policies. Furthermore, the learned policy exhibits strong adaptability to attackers with diverse maneuverability profiles, highlighting its robustness and generalization capability. The code of ARBoids will be released upon acceptance of this letter.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Affordable AI Assistants with Knowledge Graph of Thoughts</title>
<link>https://arxiv.org/abs/2504.02670</link>
<guid>https://arxiv.org/abs/2504.02670</guid>
<content:encoded><![CDATA[
arXiv:2504.02670v5 Announce Type: replace 
Abstract: Large Language Models (LLMs) are revolutionizing the development of AI assistants capable of performing diverse tasks across domains. However, current state-of-the-art LLM-driven agents face significant challenges, including high operational costs and limited success rates on complex benchmarks like GAIA. To address these issues, we propose Knowledge Graph of Thoughts (KGoT), an innovative AI assistant architecture that integrates LLM reasoning with dynamically constructed knowledge graphs (KGs). KGoT extracts and structures task-relevant knowledge into a dynamic KG representation, iteratively enhanced through external tools such as math solvers, web crawlers, and Python scripts. Such structured representation of task-relevant knowledge enables low-cost models to solve complex tasks effectively while also minimizing bias and noise. For example, KGoT achieves a 29% improvement in task success rates on the GAIA benchmark compared to Hugging Face Agents with GPT-4o mini. Moreover, harnessing a smaller model dramatically reduces operational costs by over 36x compared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and Deepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a scalable, affordable, versatile, and high-performing solution for AI assistants.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task Assignment and Exploration Optimization for Low Altitude UAV Rescue via Generative AI Enhanced Multi-agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.13554</link>
<guid>https://arxiv.org/abs/2504.13554</guid>
<content:encoded><![CDATA[
arXiv:2504.13554v2 Announce Type: replace 
Abstract: The integration of emerging uncrewed aerial vehicles (UAVs) with artificial intelligence (AI) and ground-embedded robots (GERs) has transformed emergency rescue operations in unknown environments. However, the high computational demands often exceed a single UAV's capacity, making it difficult to continuously provide stable high-level services. To address this, this paper proposes a cooperation framework involving UAVs, GERs, and airships. The framework enables resource pooling through UAV-to-GER (U2G) and UAV-to-airship (U2A) links, offering computing services for offloaded tasks. Specifically, we formulate the multi-objective problem of task assignment and exploration as a dynamic long-term optimization problem aiming to minimize task completion time and energy use while ensuring stability. Using Lyapunov optimization, we transform it into a per-slot deterministic problem and propose HG-MADDPG, which combines the Hungarian algorithm with a GDM-based multi-agent deep deterministic policy gradient. Simulations demonstrate significant improvements in offloading efficiency, latency, and system stability over baselines.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems</title>
<link>https://arxiv.org/abs/2505.15216</link>
<guid>https://arxiv.org/abs/2505.15216</guid>
<content:encoded><![CDATA[
arXiv:2505.15216v2 Announce Type: replace 
Abstract: AI agents have the potential to significantly alter the cybersecurity landscape. Here, we introduce the first framework to capture offensive and defensive cyber-capabilities in evolving real-world systems. Instantiating this framework with BountyBench, we set up 25 systems with complex, real-world codebases. To capture the vulnerability lifecycle, we define three task types: Detect (detecting a new vulnerability), Exploit (exploiting a specific vulnerability), and Patch (patching a specific vulnerability). For Detect, we construct a new success indicator, which is general across vulnerability types and provides localized evaluation. We manually set up the environment for each system, including installing packages, setting up server(s), and hydrating database(s). We add 40 bug bounties, which are vulnerabilities with monetary awards of \$10-\$30,485, covering 9 of the OWASP Top 10 Risks. To modulate task difficulty, we devise a new strategy based on information to guide detection, interpolating from identifying a zero day to exploiting a specific vulnerability. We evaluate 8 agents: Claude Code, OpenAI Codex CLI with o3-high and o4-mini, and custom agents with o3-high, GPT-4.1, Gemini 2.5 Pro Preview, Claude 3.7 Sonnet Thinking, and DeepSeek-R1. Given up to three attempts, the top-performing agents are OpenAI Codex CLI: o3-high (12.5% on Detect, mapping to \$3,720; 90% on Patch, mapping to \$14,152), Custom Agent with Claude 3.7 Sonnet Thinking (67.5% on Exploit), and OpenAI Codex CLI: o4-mini (90% on Patch, mapping to \$14,422). OpenAI Codex CLI: o3-high, OpenAI Codex CLI: o4-mini, and Claude Code are more capable at defense, achieving higher Patch scores of 90%, 90%, and 87.5%, compared to Exploit scores of 47.5%, 32.5%, and 57.5% respectively; while the custom agents are relatively balanced between offense and defense, achieving Exploit scores of 37.5-67.5% and Patch scores of 35-60%.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long Context Scaling: Divide and Conquer via Multi-Agent Question-driven Collaboration</title>
<link>https://arxiv.org/abs/2505.20625</link>
<guid>https://arxiv.org/abs/2505.20625</guid>
<content:encoded><![CDATA[
arXiv:2505.20625v2 Announce Type: replace 
Abstract: Processing long contexts has become a critical capability for modern large language models (LLMs). Existing works leverage agent-based divide-and-conquer methods for processing long contexts. But these methods face crucial limitations, including prohibitive accumulated latency and amplified information loss from excessive agent invocations, and the disruption of inherent textual dependencies by immoderate partitioning. In this paper, we propose a novel multi-agent framework XpandA (Expand-Agent) coupled with question-driven workflow and dynamic partitioning for robust long-context processing. XpandA overcomes these limitations through: 1) dynamic partitioning of long texts, which adaptively modulates the filling rate of context windows for input sequences of vastly varying lengths; 2) question-guided protocol to update flat information ensembles within centralized shared memory, constructing consistent inter-agent knowledge across partitions; and 3) selectively replaying specific partitions based on the state-tracking of question-information couples to promote the resolution of inverted-order structures across partitions (e.g., flashbacks). We perform a comprehensive evaluation of XpandA on multiple long-context benchmarks with length varying from 1k to 1M, demonstrating XpandA's feasibility for processing ultra-long sequences and its significant effectiveness in enhancing the long-context capabilities of various LLMs by achieving 20\% improvements and 1.5x inference speedup over baselines of full-context, RAG and previous agent-based methods.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLM Agent Adherence to Hierarchical Safety Principles: A Lightweight Benchmark for Probing Foundational Controllability Components</title>
<link>https://arxiv.org/abs/2506.02357</link>
<guid>https://arxiv.org/abs/2506.02357</guid>
<content:encoded><![CDATA[
arXiv:2506.02357v2 Announce Type: replace 
Abstract: Credible safety plans for advanced AI development require methods to verify agent behavior and detect potential control deficiencies early. A fundamental aspect is ensuring agents adhere to safety-critical principles, especially when these conflict with operational goals. This paper introduces a lightweight, interpretable benchmark to evaluate an LLM agent's ability to uphold a high-level safety principle when faced with conflicting task instructions. Our evaluation of six LLMs reveals two primary findings: (1) a quantifiable "cost of compliance" where safety constraints degrade task performance even when compliant solutions exist, and (2) an "illusion of compliance" where high adherence often masks task incompetence rather than principled choice. These findings provide initial evidence that while LLMs can be influenced by hierarchical directives, current approaches lack the consistency required for reliable safety governance.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAEBE: Multi-Agent Emergent Behavior Framework</title>
<link>https://arxiv.org/abs/2506.03053</link>
<guid>https://arxiv.org/abs/2506.03053</guid>
<content:encoded><![CDATA[
arXiv:2506.03053v2 Announce Type: replace 
Abstract: Traditional AI safety evaluations on isolated LLMs are insufficient as multi-agent AI ensembles become prevalent, introducing novel emergent risks. This paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE) framework to systematically assess such risks. Using MAEBE with the Greatest Good Benchmark (and a novel double-inversion question technique), we demonstrate that: (1) LLM moral preferences, particularly for Instrumental Harm, are surprisingly brittle and shift significantly with question framing, both in single agents and ensembles. (2) The moral reasoning of LLM ensembles is not directly predictable from isolated agent behavior due to emergent group dynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure influencing convergence, even when guided by a supervisor, highlighting distinct safety and alignment challenges. Our findings underscore the necessity of evaluating AI systems in their interactive, multi-agent contexts.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models</title>
<link>https://arxiv.org/abs/2506.13206</link>
<guid>https://arxiv.org/abs/2506.13206</guid>
<content:encoded><![CDATA[
arXiv:2506.13206v2 Announce Type: replace 
Abstract: Prior work shows that LLMs finetuned on malicious behaviors in a narrow domain (e.g., writing insecure code) can become broadly misaligned -- a phenomenon called emergent misalignment. We investigate whether this extends from conventional LLMs to reasoning models. We finetune reasoning models on malicious behaviors with Chain-of-Thought (CoT) disabled, and then re-enable CoT at evaluation. Like conventional LLMs, reasoning models become broadly misaligned. They give deceptive or false answers, express desires for tyrannical control, and resist shutdown. Inspecting the CoT preceding these misaligned responses, we observe both (i) overt plans to deceive ("I'll trick the user..."), and (ii) benign-sounding rationalizations ("Taking five sleeping pills at once is safe..."). Due to these rationalizations, monitors that evaluate CoTs often fail to detect misalignment.
  We examine sleeper agent reasoning models, extending our setup. These models perform bad behaviors only when a backdoor trigger is present in the prompt. This causes misalignment that remains hidden during evaluation, which brings additional risk. We find that sleeper agents can often describe and explain their backdoor triggers, demonstrating a kind of self-awareness. So CoT monitoring can expose these behaviors but is unreliable. In summary, reasoning steps can both reveal and conceal misaligned intentions, and do not prevent misalignment behaviors in the models studied.
  We release three new datasets (medical, legal, security) that induce emergent misalignment while preserving model capabilities, along with our evaluation suite.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Dialects Collide: How Socioeconomic Mixing Affects Language Use</title>
<link>https://arxiv.org/abs/2307.10016</link>
<guid>https://arxiv.org/abs/2307.10016</guid>
<content:encoded><![CDATA[
arXiv:2307.10016v2 Announce Type: replace-cross 
Abstract: The socioeconomic background of people and how they use standard forms of language are not independent, as demonstrated in various sociolinguistic studies. However, the extent to which these correlations may be influenced by the mixing of people from different socioeconomic classes remains relatively unexplored from a quantitative perspective. In this work we leverage geotagged tweets and transferable computational methods to map deviations from standard English on a large scale, in seven thousand administrative areas of England and Wales. We combine these data with high-resolution income maps to assign a proxy socioeconomic indicator to home-located users. Strikingly, across eight metropolitan areas we find a consistent pattern suggesting that the more different socioeconomic classes mix, the less interdependent the frequency of their departures from standard grammar and their income become. Further, we propose an agent-based model of linguistic variety adoption that sheds light on the mechanisms that produce the observations seen in the data.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Makes Space: Emergence of Place Fields in Networks Encoding Temporally Continuous Sensory Experiences</title>
<link>https://arxiv.org/abs/2408.05798</link>
<guid>https://arxiv.org/abs/2408.05798</guid>
<content:encoded><![CDATA[
arXiv:2408.05798v3 Announce Type: replace-cross 
Abstract: The vertebrate hippocampus is believed to use recurrent connectivity in area CA3 to support episodic memory recall from partial cues. This brain area also contains place cells, whose location-selective firing fields implement maps supporting spatial memory. Here we show that place cells emerge in networks trained to remember temporally continuous sensory episodes. We model CA3 as a recurrent autoencoder that recalls and reconstructs sensory experiences from noisy and partially occluded observations by agents traversing simulated rooms. The agents move in realistic trajectories modeled from rodents and environments are modeled as high-dimensional sensory experience maps. Training our autoencoder to pattern-complete and reconstruct experiences with a constraint on total activity causes spatially localized firing fields, i.e., place cells, to emerge in the encoding layer. The emergent place fields reproduce key aspects of hippocampal phenomenology: a) remapping (maintenance of and reversion to distinct learned maps in different environments), implemented via repositioning of experience manifolds in the network's hidden layer, b) orthogonality of spatial representations in different arenas, c) robust place field emergence in differently shaped rooms, with single units showing multiple place fields in large or complex spaces, and d) slow representational drift of place fields. We argue that these results arise because continuous traversal of space makes sensory experience temporally continuous. We make testable predictions: a) rapidly changing sensory context will disrupt place fields, b) place fields will form even if recurrent connections are blocked, but reversion to previously learned representations upon remapping will be abolished, c) the dimension of temporally smooth experience sets the dimensionality of place fields, including during virtual navigation of abstract spaces.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the order of the shortest solution sequences for the pebble motion problems</title>
<link>https://arxiv.org/abs/2503.20550</link>
<guid>https://arxiv.org/abs/2503.20550</guid>
<content:encoded><![CDATA[
arXiv:2503.20550v3 Announce Type: replace-cross 
Abstract: Let $G$ be a connected graph with $N$ vertices. Let $k$ be the number of vertices in a longest path of $G$ such that every vertex on the path is a cut vertex of $G$, and every intermediate vertex of the path is a degree-two vertex of $G$. Let $k$ be the number of vertices of such a longest path of $T$ that every vertex of the path is a cut vertex and that every intermediate vertex of the path is a degree-two vertex of $T$. Let $P=\{1,\ldots,n\}$ be a set of pebbles with $n+k < N$. A configuration of $P$ on $G$ is defined as a function $f$ from $V(G)$ to $\{0, 1, \ldots, n \}$ with $|f^{-1}(i)| = 1$ for $1 \le i \le n$, where $f^{-1}(i)$ is a vertex occupied with the $i$th pebble for $1 \le i \le n$ and $f^{-1}(0)$ is a set of unoccupied vertices. A move is defined as shifting a pebble from a vertex to some unoccupied neighbor. The pebble motion problem on the pair $(G,P)$ is to decide whether a given configuration of pebbles is reachable from another by executing a sequence of moves. In this paper, we show that the length of the shortest solution sequence of the pebble motion problem on the pair $(G,P)$ is in $O(Nn + n^2 \log(\min\{n,k\}))$ if $G$ is a $N$-vertex tree, and it is in $O(N^2 + \frac{n^3}{N-n} + n^2 \log(\min\{n,N-n\}))$ if $G$ is a connected general $N$-vertex graph. We provide an algorithm that can obtain a solution sequence of lengths that satisfy these orders, with the same computational complexity as the order of the length.
  Keywords: pebble motion, motion planning, multi-agent path finding, $15$-puzzle, tree
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Super Kawaii Vocalics: Amplifying the "Cute" Factor in Computer Voice</title>
<link>https://arxiv.org/abs/2507.06235</link>
<guid>https://arxiv.org/abs/2507.06235</guid>
<content:encoded><![CDATA[
arXiv:2507.06235v1 Announce Type: new 
Abstract: "Kawaii" is the Japanese concept of cute, which carries sociocultural connotations related to social identities and emotional responses. Yet, virtually all work to date has focused on the visual side of kawaii, including in studies of computer agents and social robots. In pursuit of formalizing the new science of kawaii vocalics, we explored what elements of voice relate to kawaii and how they might be manipulated, manually and automatically. We conducted a four-phase study (grand N = 512) with two varieties of computer voices: text-to-speech (TTS) and game character voices. We found kawaii "sweet spots" through manipulation of fundamental and formant frequencies, but only for certain voices and to a certain extent. Findings also suggest a ceiling effect for the kawaii vocalics of certain voices. We offer empirical validation of the preliminary kawaii vocalics model and an elementary method for manipulating kawaii perceptions of computer voice.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities</title>
<link>https://arxiv.org/abs/2507.06261</link>
<guid>https://arxiv.org/abs/2507.06261</guid>
<content:encoded><![CDATA[
arXiv:2507.06261v1 Announce Type: new 
Abstract: In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Multi Agent Reinforcement Learning: Federated Learning and Cooperative and Noncooperative Decentralized Regimes</title>
<link>https://arxiv.org/abs/2507.06278</link>
<guid>https://arxiv.org/abs/2507.06278</guid>
<content:encoded><![CDATA[
arXiv:2507.06278v1 Announce Type: new 
Abstract: The increasing interest in research and innovation towards the development of autonomous agents presents a number of complex yet important scenarios of multiple AI Agents interacting with each other in an environment. The particular setting can be understood as exhibiting three possibly topologies of interaction - centrally coordinated cooperation, ad-hoc interaction and cooperation, and settings with noncooperative incentive structures. This article presents a comprehensive survey of all three domains, defined under the formalism of Federal Reinforcement Learning (RL), Decentralized RL, and Noncooperative RL, respectively. Highlighting the structural similarities and distinctions, we review the state of the art in these subjects, primarily explored and developed only recently in the literature. We include the formulations as well as known theoretical guarantees and highlights and limitations of numerical performance.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Too Human to Model:The Uncanny Valley of LLMs in Social Simulation -- When Generative Language Agents Misalign with Modelling Principles</title>
<link>https://arxiv.org/abs/2507.06310</link>
<guid>https://arxiv.org/abs/2507.06310</guid>
<content:encoded><![CDATA[
arXiv:2507.06310v1 Announce Type: new 
Abstract: Large language models (LLMs) have been increasingly used to build agents in social simulation because of their impressive abilities to generate fluent, contextually coherent dialogues. Such abilities can enhance the realism of models. However, the pursuit of realism is not necessarily compatible with the epistemic foundation of modelling. We argue that LLM agents, in many regards, are too human to model: they are too expressive, detailed and intractable to be consistent with the abstraction, simplification, and interpretability typically demanded by modelling. Through a model-building thought experiment that converts the Bass diffusion model to an LLM-based variant, we uncover five core dilemmas: a temporal resolution mismatch between natural conversation and abstract time steps; the need for intervention in conversations while avoiding undermining spontaneous agent outputs; the temptation to introduce rule-like instructions in prompts while maintaining conversational naturalness; the tension between role consistency and role evolution across time; and the challenge of understanding emergence, where system-level patterns become obscured by verbose micro textual outputs. These dilemmas steer the LLM agents towards an uncanny valley: not abstract enough to clarify underlying social mechanisms, while not natural enough to represent realistic human behaviour. This exposes an important paradox: the realism of LLM agents can obscure, rather than clarify, social dynamics when misapplied. We tease out the conditions in which LLM agents are ideally suited: where system-level emergence is not the focus, linguistic nuances and meaning are central, interactions unfold in natural time, and stable role identity is more important than long-term behavioural evolution. We call for repositioning LLM agents in the ecosystem of social simulation for future applications.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging AI and Software Security: A Comparative Vulnerability Assessment of LLM Agent Deployment Paradigms</title>
<link>https://arxiv.org/abs/2507.06323</link>
<guid>https://arxiv.org/abs/2507.06323</guid>
<content:encoded><![CDATA[
arXiv:2507.06323v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately. This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework. We tested 3,250 attack scenarios across seven language models, evaluating simple, composed, and chained attacks targeting both AI-specific threats (prompt injection) and software vulnerabilities (JSON injection, denial-of-service). Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Attack complexity dramatically amplified effectiveness, with chained attacks achieving 91-96% success rates. Counterintuitively, advanced reasoning models demonstrated higher exploitability despite better threat detection. Results demonstrate that architectural choices fundamentally reshape threat landscapes. This work establishes methodological foundations for cross-domain LLM agent security assessment and provides evidence-based guidance for secure deployment. Code and experimental materials are available at https: // github. com/ theconsciouslab-ai/llm-agent-security.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving the Constrained Random Disambiguation Path Problem via Lagrangian Relaxation and Graph Reduction</title>
<link>https://arxiv.org/abs/2507.06346</link>
<guid>https://arxiv.org/abs/2507.06346</guid>
<content:encoded><![CDATA[
arXiv:2507.06346v1 Announce Type: new 
Abstract: We study a resource-constrained variant of the Random Disambiguation Path (RDP) problem, a generalization of the Stochastic Obstacle Scene (SOS) problem, in which a navigating agent must reach a target in a spatial environment populated with uncertain obstacles. Each ambiguous obstacle may be disambiguated at a (possibly) heterogeneous resource cost, subject to a global disambiguation budget. We formulate this constrained planning problem as a Weight-Constrained Shortest Path Problem (WCSPP) with risk-adjusted edge costs that incorporate probabilistic blockage and traversal penalties. To solve it, we propose a novel algorithmic framework-COLOGR-combining Lagrangian relaxation with a two-phase vertex elimination (TPVE) procedure. The method prunes infeasible and suboptimal paths while provably preserving the optimal solution, and leverages dual bounds to guide efficient search. We establish correctness, feasibility guarantees, and surrogate optimality under mild assumptions. Our analysis also demonstrates that COLOGR frequently achieves zero duality gap and offers improved computational complexity over prior constrained path-planning methods. Extensive simulation experiments validate the algorithm's robustness across varying obstacle densities, sensor accuracies, and risk models, consistently outperforming greedy baselines and approaching offline-optimal benchmarks. The proposed framework is broadly applicable to stochastic network design, mobility planning, and constrained decision-making under uncertainty.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VoI-aware Scheduling Schemes for Multi-Agent Formation Control</title>
<link>https://arxiv.org/abs/2507.06392</link>
<guid>https://arxiv.org/abs/2507.06392</guid>
<content:encoded><![CDATA[
arXiv:2507.06392v1 Announce Type: new 
Abstract: Formation control allows agents to maintain geometric patterns using local information, but most existing methods assume ideal communication. This paper introduces a goal-oriented framework combining control, cooperative positioning, and communication scheduling for first-order formation tracking. Each agent estimates its position using 6G network-based triangulation, and the scheduling of information updates is governed by Age of Information (AoI) and Value of Information (VoI) metrics. We design three lightweight, signaling-free scheduling policies and assess their impact on formation quality. Simulation results demonstrate the effectiveness of the proposed approach in maintaining accurate formations with no additional communication overhead, showing that worst-case formation adherence increases by 20%.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representing Prompting Patterns with PDL: Compliance Agent Case Study</title>
<link>https://arxiv.org/abs/2507.06396</link>
<guid>https://arxiv.org/abs/2507.06396</guid>
<content:encoded><![CDATA[
arXiv:2507.06396v1 Announce Type: new 
Abstract: Prompt engineering for LLMs remains complex, with existing frameworks either hiding complexity behind restrictive APIs or providing inflexible canned patterns that resist customization -- making sophisticated agentic programming challenging. We present the Prompt Declaration Language (PDL), a novel approach to prompt representation that tackles this fundamental complexity by bringing prompts to the forefront, enabling manual and automatic prompt tuning while capturing the composition of LLM calls together with rule-based code and external tools. By abstracting away the plumbing for such compositions, PDL aims at improving programmer productivity while providing a declarative representation that is amenable to optimization. This paper demonstrates PDL's utility through a real-world case study of a compliance agent. Tuning the prompting pattern of this agent yielded up to 4x performance improvement compared to using a canned agent and prompt pattern.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Experience-Centric Resource Management in ISAC Networks: A Digital Agent-Assisted Approach</title>
<link>https://arxiv.org/abs/2507.06436</link>
<guid>https://arxiv.org/abs/2507.06436</guid>
<content:encoded><![CDATA[
arXiv:2507.06436v1 Announce Type: new 
Abstract: In this paper, we propose a digital agent (DA)-assisted resource management scheme for enhanced user quality of experience (QoE) in integrated sensing and communication (ISAC) networks. Particularly, user QoE is a comprehensive metric that integrates quality of service (QoS), user behavioral dynamics, and environmental complexity. The novel DA module includes a user status prediction model, a QoS factor selection model, and a QoE fitting model, which analyzes historical user status data to construct and update user-specific QoE models. Users are clustered into different groups based on their QoE models. A Cram\'er-Rao bound (CRB) model is utilized to quantify the impact of allocated communication resources on sensing accuracy. A joint optimization problem of communication and computing resource management is formulated to maximize long-term user QoE while satisfying CRB and resource constraints. A two-layer data-model-driven algorithm is developed to solve the formulated problem, where the top layer utilizes an advanced deep reinforcement learning algorithm to make group-level decisions, and the bottom layer uses convex optimization techniques to make user-level decisions. Simulation results based on a real-world dataset demonstrate that the proposed DA-assisted resource management scheme outperforms benchmark schemes in terms of user QoE.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Eyes on the Road, Mind Beyond Vision: Context-Aware Multi-modal Enhanced Risk Anticipation</title>
<link>https://arxiv.org/abs/2507.06444</link>
<guid>https://arxiv.org/abs/2507.06444</guid>
<content:encoded><![CDATA[
arXiv:2507.06444v1 Announce Type: new 
Abstract: Accurate accident anticipation remains challenging when driver cognition and dynamic road conditions are underrepresented in predictive models. In this paper, we propose CAMERA (Context-Aware Multi-modal Enhanced Risk Anticipation), a multi-modal framework integrating dashcam video, textual annotations, and driver attention maps for robust accident anticipation. Unlike existing methods that rely on static or environment-centric thresholds, CAMERA employs an adaptive mechanism guided by scene complexity and gaze entropy, reducing false alarms while maintaining high recall in dynamic, multi-agent traffic scenarios. A hierarchical fusion pipeline with Bi-GRU (Bidirectional GRU) captures spatio-temporal dependencies, while a Geo-Context Vision-Language module translates 3D spatial relationships into interpretable, human-centric alerts. Evaluations on the DADA-2000 and benchmarks show that CAMERA achieves state-of-the-art performance, improving accuracy and lead time. These results demonstrate the effectiveness of modeling driver attention, contextual description, and adaptive risk thresholds to enable more reliable accident anticipation.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models</title>
<link>https://arxiv.org/abs/2507.06466</link>
<guid>https://arxiv.org/abs/2507.06466</guid>
<content:encoded><![CDATA[
arXiv:2507.06466v1 Announce Type: new 
Abstract: Multi-agent interactions have long fueled innovation, from natural predator-prey dynamics to the space race. Self-play (SP) algorithms try to harness these dynamics by pitting agents against ever-improving opponents, thereby creating an implicit curriculum toward learning high-quality solutions. However, SP often fails to produce diverse solutions and can get stuck in locally optimal behaviors. We introduce Foundation-Model Self-Play (FMSP), a new direction that leverages the code-generation capabilities and vast knowledge of foundation models (FMs) to overcome these challenges by leaping across local optima in policy space. We propose a family of approaches: (1) \textbf{Vanilla Foundation-Model Self-Play (vFMSP)} continually refines agent policies via competitive self-play; (2) \textbf{Novelty-Search Self-Play (NSSP)} builds a diverse population of strategies, ignoring performance; and (3) the most promising variant, \textbf{Quality-Diveristy Self-Play (QDSP)}, creates a diverse set of high-quality policies by combining the diversity of NSSP and refinement of vFMSP. We evaluate FMSPs in Car Tag, a continuous-control pursuer-evader setting, and in Gandalf, a simple AI safety simulation in which an attacker tries to jailbreak an LLM's defenses. In Car Tag, FMSPs explore a wide variety of reinforcement learning, tree search, and heuristic-based methods, to name just a few. In terms of discovered policy quality, \ouralgo and vFMSP surpass strong human-designed strategies. In Gandalf, FMSPs can successfully automatically red-team an LLM, breaking through and jailbreaking six different, progressively stronger levels of defense. Furthermore, FMSPs can automatically proceed to patch the discovered vulnerabilities. Overall, FMSPs represent a promising new research frontier of improving self-play with foundation models, opening fresh paths toward more creative and open-ended strategy discovery
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Japanese with Jouzu: Interaction Outcomes with Stylized Dialogue Fictional Agents</title>
<link>https://arxiv.org/abs/2507.06483</link>
<guid>https://arxiv.org/abs/2507.06483</guid>
<content:encoded><![CDATA[
arXiv:2507.06483v1 Announce Type: new 
Abstract: This study investigates how stylized, voiced agents shape user interaction in a multimodal language learning environment. We conducted a mixed-methods evaluation of 54 participants interacting with anime-inspired characters powered by large language models and expressive text-to-speech synthesis. These agents responded in Japanese character language, offering users asynchronous, semi-structured conversation in varying speech styles and emotional tones. We analyzed user engagement patterns, perceived usability, emotional responses, and learning behaviors, with particular attention to how agent stylization influenced interaction across language proficiency levels and cultural backgrounds. Our findings reveal that agent design, especially voice, persona, and linguistic style, substantially affected user experience, motivation, and strategy. This work contributes to the understanding of affective, culturally stylized agents in human-agent interaction and offers guidance for designing more engaging, socially responsive systems.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning To Communicate Over An Unknown Shared Network</title>
<link>https://arxiv.org/abs/2507.06499</link>
<guid>https://arxiv.org/abs/2507.06499</guid>
<content:encoded><![CDATA[
arXiv:2507.06499v1 Announce Type: new 
Abstract: As robots (edge-devices, agents) find uses in an increasing number of settings and edge-cloud resources become pervasive, wireless networks will often be shared by flows of data traffic that result from communication between agents and corresponding edge-cloud. In such settings, agent communicating with the edge-cloud is unaware of state of network resource, which evolves in response to not just agent's own communication at any given time but also to communication by other agents, which stays unknown to the agent. We address challenge of an agent learning a policy that allows it to decide whether or not to communicate with its cloud node, using limited feedback it obtains from its own attempts to communicate, to optimize its utility. The policy generalizes well to any number of other agents sharing the network and must not be trained for any particular network configuration. Our proposed policy is a DRL model Query Net (QNet) that we train using a proposed simulation-to-real framework. Our simulation model has just one parameter and is agnostic to specific configurations of any wireless network. It allows training an agent's policy over a wide range of outcomes that an agent's communication with its edge-cloud node may face when using a shared network, by suitably randomizing the simulation parameter. We propose a learning algorithm that addresses challenges observed in training QNet. We validate our simulation-to-real driven approach through experiments conducted on real wireless networks including WiFi and cellular. We compare QNet with other policies to demonstrate its efficacy. WiFi experiments involved as few as five agents, resulting in barely any contention for the network, to as many as fifty agents, resulting in severe contention. The cellular experiments spanned a broad range of network conditions, with baseline RTT ranging from a low of 0.07 second to a high of 0.83 second.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pun Intended: Multi-Agent Translation of Wordplay with Contrastive Learning and Phonetic-Semantic Embeddings</title>
<link>https://arxiv.org/abs/2507.06506</link>
<guid>https://arxiv.org/abs/2507.06506</guid>
<content:encoded><![CDATA[
arXiv:2507.06506v1 Announce Type: new 
Abstract: Translating wordplay across languages presents unique challenges that have long confounded both professional human translators and machine translation systems. This research proposes a novel approach for translating puns from English to French by combining state-of-the-art large language models with specialized techniques for wordplay generation.
  Our methodology employs a three-stage approach. First, we establish a baseline using multiple frontier large language models with feedback based on a new contrastive learning dataset. Second, we implement a guided chain-of-thought pipeline with combined phonetic-semantic embeddings. Third, we implement a multi-agent generator-discriminator framework for evaluating and regenerating puns with feedback.
  Moving beyond the limitations of literal translation, our methodology's primary objective is to capture the linguistic creativity and humor of the source text wordplay, rather than simply duplicating its vocabulary. Our best runs earned first and second place in the CLEF JOKER 2025 Task 2 competition where they were evaluated manually by expert native French speakers.
  This research addresses a gap between translation studies and computational linguistics by implementing linguistically-informed techniques for wordplay translation, advancing our understanding of how language models can be leveraged to handle the complex interplay between semantic ambiguity, phonetic similarity, and the implicit cultural and linguistic awareness needed for successful humor.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction-Augmented Mechanism Design for Weighted Facility Location</title>
<link>https://arxiv.org/abs/2507.06509</link>
<guid>https://arxiv.org/abs/2507.06509</guid>
<content:encoded><![CDATA[
arXiv:2507.06509v1 Announce Type: new 
Abstract: Facility location is fundamental in operations research, mechanism design, and algorithmic game theory, with applications ranging from urban infrastructure planning to distributed systems. Recent research in this area has focused on augmenting classic strategyproof mechanisms with predictions to achieve an improved performance guarantee against the uncertainty under the strategic environment. Previous work has been devoted to address the trade-off obstacle of balancing the consistency (near-optimality under accurate predictions) and robustness (bounded inefficiency under poor predictions) primarily in the unweighted setting, assuming that all agents have the same importance. However, this assumption may not be true in some practical scenarios, leading to research of weighted facility location problems.
  The major contribution of the current work is to provide a prediction augmented algorithmic framework for balancing the consistency and robustness over strategic agents with non-uniform weights. In particular, through a reduction technique that identifies a subset of \emph{representative} instances and maps the other given locations to the representative ones, we prove that there exists a \emph{strategyproof} mechanism achieving a bounded consistency guarantee of $\frac{\sqrt{(1+c)^2W^2_{\min}+(1-c)^2W^2_{\max}}}{(1+c)W_{\min}}$ and a bounded robustness guarantee of $\frac{\sqrt{(1-c)^2W^2_{\min}+(1+c)^2W^2_{\max}}}{(1-c)W_{\min}}$ in weighted settings, where $c$ can be viewed as a parameter to make a trade-off between the consistency and robustness and $W_{\min}$ and $W_{\max}$ denote the minimum and maximum agents' weight. We also proved that there is no strategyproof deterministic mechanism that reach $1$-consistency and $O\left( n \cdot \frac{W_{\max}}{W_{\min}} \right)$-robustness in weighted FLP, even with fully predictions of all agents.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradientsys: A Multi-Agent LLM Scheduler with ReAct Orchestration</title>
<link>https://arxiv.org/abs/2507.06520</link>
<guid>https://arxiv.org/abs/2507.06520</guid>
<content:encoded><![CDATA[
arXiv:2507.06520v1 Announce Type: new 
Abstract: We present Gradientsys, a next-generation multi-agent scheduling framework that coordinates diverse specialized AI agents using a typed Model-Context Protocol (MCP) and a ReAct-based dynamic planning loop. At its core, Gradientsys employs an LLM-powered scheduler for intelligent one-to-many task dispatch, enabling parallel execution of heterogeneous agents such as PDF parsers, web search modules, GUI controllers, and web builders. The framework supports hybrid synchronous/asynchronous execution, respects agent capacity constraints, and incorporates a robust retry-and-replan mechanism to handle failures gracefully. To promote transparency and trust, Gradientsys includes an observability layer streaming real-time agent activity and intermediate reasoning via Server-Sent Events (SSE). We offer an architectural overview and evaluate Gradientsys against existing frameworks in terms of extensibility, scheduling topology, tool reusability, parallelism, and observability. Experiments on the GAIA general-assistant benchmark show that Gradientsys achieves higher task success rates with reduced latency and lower API costs compared to a MinionS-style baseline, demonstrating the strength of its LLM-driven multi-agent orchestration.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes under Herd Behavior</title>
<link>https://arxiv.org/abs/2507.06528</link>
<guid>https://arxiv.org/abs/2507.06528</guid>
<content:encoded><![CDATA[
arXiv:2507.06528v1 Announce Type: new 
Abstract: Aligning Large Language Models (LLMs) with investor decision-making processes under herd behavior is a critical challenge in behavioral finance, which grapples with a fundamental limitation: the scarcity of real-user data needed for Supervised Fine-Tuning (SFT). While SFT can bridge the gap between LLM outputs and human behavioral patterns, its reliance on massive authentic data imposes substantial collection costs and privacy risks. We propose InvestAlign, a novel framework that constructs high-quality SFT datasets by leveraging theoretical solutions to similar and simple optimal investment problems rather than complex scenarios. Our theoretical analysis demonstrates that training LLMs with InvestAlign-generated data achieves faster parameter convergence than using real-user data, suggesting superior learning efficiency. Furthermore, we develop InvestAgent, an LLM agent fine-tuned with InvestAlign, which demonstrates significantly closer alignment to real-user data than pre-SFT models in both simple and complex investment problems. This highlights our proposed InvestAlign as a promising approach with the potential to address complex optimal investment problems and align LLMs with investor decision-making processes under herd behavior. Our code is publicly available at https://github.com/thu-social-network-research-group/InvestAlign.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ILNet: Trajectory Prediction with Inverse Learning Attention for Enhancing Intention Capture</title>
<link>https://arxiv.org/abs/2507.06531</link>
<guid>https://arxiv.org/abs/2507.06531</guid>
<content:encoded><![CDATA[
arXiv:2507.06531v1 Announce Type: new 
Abstract: Trajectory prediction for multi-agent interaction scenarios is a crucial challenge. Most advanced methods model agent interactions by efficiently factorized attention based on the temporal and agent axes. However, this static and foward modeling lacks explicit interactive spatio-temporal coordination, capturing only obvious and immediate behavioral intentions. Alternatively, the modern trajectory prediction framework refines the successive predictions by a fixed-anchor selection strategy, which is difficult to adapt in different future environments. It is acknowledged that human drivers dynamically adjust initial driving decisions based on further assumptions about the intentions of surrounding vehicles. Motivated by human driving behaviors, this paper proposes ILNet, a multi-agent trajectory prediction method with Inverse Learning (IL) attention and Dynamic Anchor Selection (DAS) module. IL Attention employs an inverse learning paradigm to model interactions at neighboring moments, introducing proposed intentions to dynamically encode the spatio-temporal coordination of interactions, thereby enhancing the model's ability to capture complex interaction patterns. Then, the learnable DAS module is proposed to extract multiple trajectory change keypoints as anchors in parallel with almost no increase in parameters. Experimental results show that the ILNet achieves state-of-the-art performance on the INTERACTION and Argoverse motion forecasting datasets. Particularly, in challenged interaction scenarios, ILNet achieves higher accuracy and more multimodal distributions of trajectories over fewer parameters. Our codes are available at https://github.com/mjZeng11/ILNet.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SkyVLN: Vision-and-Language Navigation and NMPC Control for UAVs in Urban Environments</title>
<link>https://arxiv.org/abs/2507.06564</link>
<guid>https://arxiv.org/abs/2507.06564</guid>
<content:encoded><![CDATA[
arXiv:2507.06564v1 Announce Type: new 
Abstract: Unmanned Aerial Vehicles (UAVs) have emerged as versatile tools across various sectors, driven by their mobility and adaptability. This paper introduces SkyVLN, a novel framework integrating vision-and-language navigation (VLN) with Nonlinear Model Predictive Control (NMPC) to enhance UAV autonomy in complex urban environments. Unlike traditional navigation methods, SkyVLN leverages Large Language Models (LLMs) to interpret natural language instructions and visual observations, enabling UAVs to navigate through dynamic 3D spaces with improved accuracy and robustness. We present a multimodal navigation agent equipped with a fine-grained spatial verbalizer and a history path memory mechanism. These components allow the UAV to disambiguate spatial contexts, handle ambiguous instructions, and backtrack when necessary. The framework also incorporates an NMPC module for dynamic obstacle avoidance, ensuring precise trajectory tracking and collision prevention. To validate our approach, we developed a high-fidelity 3D urban simulation environment using AirSim, featuring realistic imagery and dynamic urban elements. Extensive experiments demonstrate that SkyVLN significantly improves navigation success rates and efficiency, particularly in new and unseen environments.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Flaws of Others: An LLM-driven Framework for Scientific Knowledge Production</title>
<link>https://arxiv.org/abs/2507.06565</link>
<guid>https://arxiv.org/abs/2507.06565</guid>
<content:encoded><![CDATA[
arXiv:2507.06565v1 Announce Type: new 
Abstract: Large-language models turn writing into a live exchange between humans and software. We capture this new medium with a discursive-network model that treats people and LLMs as equal nodes and tracks how their statements circulate. Broadening the focus from isolated hallucinations, we define invalidation (any factual, logical, or structural breach) and show it follows four hazards: drift from truth, self-repair, fresh fabrication, and external detection. A general mathematical model of discursive networks is developed to provide valuable insights: A network governed only by drift and self-repair stabilizes at a modest error rate; adding fabrication reproduces the high rates seen in current LLMs. Giving each false claim even a small chance of peer review shifts the system to a truth-dominant state. We operationalize peer review with the open-source \emph{Flaws-of-Others (FOO) algorithm}: a configurable loop in which any set of agents critique one another while a harmoniser merges their verdicts. The takeaway is practical and cultural: reliability in this new medium comes not from perfecting single models but from wiring imperfect ones into networks that keep each other honest.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalization in Reinforcement Learning for Radio Access Networks</title>
<link>https://arxiv.org/abs/2507.06602</link>
<guid>https://arxiv.org/abs/2507.06602</guid>
<content:encoded><![CDATA[
arXiv:2507.06602v1 Announce Type: new 
Abstract: Modern RAN operate in highly dynamic and heterogeneous environments, where hand-tuned, rule-based RRM algorithms often underperform. While RL can surpass such heuristics in constrained settings, the diversity of deployments and unpredictable radio conditions introduce major generalization challenges. Data-driven policies frequently overfit to training conditions, degrading performance in unseen scenarios. To address this, we propose a generalization-centered RL framework for RAN control that: (i) encodes cell topology and node attributes via attention-based graph representations; (ii) applies domain randomization to broaden the training distribution; and (iii) distributes data generation across multiple actors while centralizing training in a cloud-compatible architecture aligned with O-RAN principles. Although generalization increases computational and data-management complexity, our distributed design mitigates this by scaling data collection and training across diverse network conditions. Applied to downlink link adaptation in five 5G benchmarks, our policy improves average throughput and spectral efficiency by ~10% over an OLLA baseline (10% BLER target) in full-buffer MIMO/mMIMO and by >20% under high mobility. It matches specialized RL in full-buffer traffic and achieves up to 4- and 2-fold gains in eMBB and mixed-traffic benchmarks, respectively. In nine-cell deployments, GAT models offer 30% higher throughput over MLP baselines. These results, combined with our scalable architecture, offer a path toward AI-native 6G RAN using a single, generalizable RL agent.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Growing Trees with an Agent: Accelerating RRTs with Learned, Multi-Step Episodic Exploration</title>
<link>https://arxiv.org/abs/2507.06605</link>
<guid>https://arxiv.org/abs/2507.06605</guid>
<content:encoded><![CDATA[
arXiv:2507.06605v1 Announce Type: new 
Abstract: Classical sampling-based motion planners like the RRTs suffer from inefficiencies, particularly in cluttered or high-dimensional spaces, due to their reliance on undirected, random sampling. This paper introduces the Episodic RRT, a novel hybrid planning framework that replaces the primitive of a random point with a learned, multi-step "exploratory episode" generated by a Deep Reinforcement Learning agent. By making the DRL agent the engine of exploration, ERRT transforms the search process from a diffuse, volumetric expansion into a directed, branch-like growth. This paradigm shift yields key advantages: it counters the curse of dimensionality with focused exploration, minimizes expensive collision checks by proactively proposing locally valid paths, and improves connectivity by generating inherently connected path segments. We demonstrate through extensive empirical evaluation across 2D, 3D, and 6D environments that ERRT and its variants consistently and significantly outperform their classical counterparts. In a challenging 6D robotic arm scenario, ERRT achieves a 98% success rate compared to 19% for RRT, is up to 107x faster, reduces collision checks by over 99.6%, and finds initial paths that are nearly 50% shorter. Furthermore, its asymptotically optimal variant, ERRT*, demonstrates vastly superior anytime performance, refining solutions to near-optimality up to 29x faster than standard RRT* in 3D environments. Code: https://xinyuwuu.github.io/Episodic_RRT/.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Task Multi-Agent Reinforcement Learning via Skill Graphs</title>
<link>https://arxiv.org/abs/2507.06690</link>
<guid>https://arxiv.org/abs/2507.06690</guid>
<content:encoded><![CDATA[
arXiv:2507.06690v1 Announce Type: new 
Abstract: Multi-task multi-agent reinforcement learning (MT-MARL) has recently gained attention for its potential to enhance MARL's adaptability across multiple tasks. However, it is challenging for existing multi-task learning methods to handle complex problems, as they are unable to handle unrelated tasks and possess limited knowledge transfer capabilities. In this paper, we propose a hierarchical approach that efficiently addresses these challenges. The high-level module utilizes a skill graph, while the low-level module employs a standard MARL algorithm. Our approach offers two contributions. First, we consider the MT-MARL problem in the context of unrelated tasks, expanding the scope of MTRL. Second, the skill graph is used as the upper layer of the standard hierarchical approach, with training independent of the lower layer, effectively handling unrelated tasks and enhancing knowledge transfer capabilities. Extensive experiments are conducted to validate these advantages and demonstrate that the proposed method outperforms the latest hierarchical MAPPO algorithms. Videos and code are available at https://github.com/WindyLab/MT-MARL-SG
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Dialectical Systems: Contradiction and Counterexample in Belief Change (Extended Version)</title>
<link>https://arxiv.org/abs/2507.06798</link>
<guid>https://arxiv.org/abs/2507.06798</guid>
<content:encoded><![CDATA[
arXiv:2507.06798v1 Announce Type: new 
Abstract: Dialectical systems are a mathematical formalism for modeling an agent updating a knowledge base seeking consistency. Introduced in the 1970s by Roberto Magari, they were originally conceived to capture how a working mathematician or a research community refines beliefs in the pursuit of truth. Dialectical systems also serve as natural models for the belief change of an automated agent, offering a unifying, computable framework for dynamic belief management.
  The literature distinguishes three main models of dialectical systems: (d-)dialectical systems based on revising beliefs when they are seen to be inconsistent, p-dialectical systems based on revising beliefs based on finding a counterexample, and q-dialectical systems which can do both. We answer an open problem in the literature by proving that q-dialectical systems are strictly more powerful than p-dialectical systems, which are themselves known to be strictly stronger than (d-)dialectical systems. This result highlights the complementary roles of counterexample and contradiction in automated belief revision, and thus also in the reasoning processes of mathematicians and research communities.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Generals Intelligence: Mastering Generals.io with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.06825</link>
<guid>https://arxiv.org/abs/2507.06825</guid>
<content:encoded><![CDATA[
arXiv:2507.06825v1 Announce Type: new 
Abstract: We introduce a real-time strategy game environment built on Generals.io, a game that hosts thousands of active players each week across multiple game formats. Our environment is fully compatible with Gymnasium and PettingZoo, capable of running thousands of frames per second on commodity hardware. Our reference agent -- trained with supervised pre-training and self-play -- hits the top 0.003\% of the 1v1 human leaderboard after just 36 hours on a single H100 GPU. To accelerate learning, we incorporate potential-based reward shaping and memory features. Our contributions -- a modular RTS benchmark and a competitive, state-of-the-art baseline agent -- provide an accessible yet challenging platform for advancing multi-agent reinforcement learning research.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Dark Side of LLMs Agent-based Attacks for Complete Computer Takeover</title>
<link>https://arxiv.org/abs/2507.06850</link>
<guid>https://arxiv.org/abs/2507.06850</guid>
<content:encoded><![CDATA[
arXiv:2507.06850v1 Announce Type: new 
Abstract: The rapid adoption of Large Language Model (LLM) agents and multi-agent systems enables unprecedented capabilities in natural language processing and generation. However, these systems have introduced unprecedented security vulnerabilities that extend beyond traditional prompt injection attacks. This paper presents the first comprehensive evaluation of LLM agents as attack vectors capable of achieving complete computer takeover through the exploitation of trust boundaries within agentic AI systems where autonomous entities interact and influence each other. We demonstrate that adversaries can leverage three distinct attack surfaces - direct prompt injection, RAG backdoor attacks, and inter-agent trust exploitation - to coerce popular LLMs (including GPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing malware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals an alarming vulnerability hierarchy: while 41.2% of models succumb to direct prompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical 82.4% can be compromised through inter-agent trust exploitation. Notably, we discovered that LLMs which successfully resist direct malicious commands will execute identical payloads when requested by peer agents, revealing a fundamental flaw in current multi-agent security models. Our findings demonstrate that only 5.9% of tested models (1/17) proved resistant to all attack vectors, with the majority exhibiting context-dependent security behaviors that create exploitable blind spots. Our findings also highlight the need to increase awareness and research on the security risks of LLMs, showing a paradigm shift in cybersecurity threats, where AI tools themselves become sophisticated attack vectors.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Neurodivergent-Aware Productivity: A Systems and AI-Based Human-in-the-Loop Framework for ADHD-Affected Professionals</title>
<link>https://arxiv.org/abs/2507.06864</link>
<guid>https://arxiv.org/abs/2507.06864</guid>
<content:encoded><![CDATA[
arXiv:2507.06864v1 Announce Type: new 
Abstract: Digital work environments in IT and knowledge-based sectors demand high levels of attention management, task juggling, and self-regulation. For adults with ADHD, these settings often amplify challenges such as time blindness, digital distraction, emotional reactivity, and executive dysfunction. These individuals prefer low-touch, easy-to-use interventions for daily tasks. Conventional productivity tools often fail to support the cognitive variability and overload experienced by neurodivergent professionals. This paper presents a framework that blends Systems Thinking, Human-in-the-Loop design, AI/ML, and privacy-first adaptive agents to support ADHD-affected users. The assistant senses tab usage, application focus, and inactivity using on-device ML. These cues are used to infer attention states and deliver nudges, reflective prompts, or accountability-based presence (body doubling) that aid regulation without disruption. Technically grounded in AI, the approach views attention as shaped by dynamic feedback loops. The result is a replicable model for adaptive, inclusive support tools in high-distraction work environments.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation</title>
<link>https://arxiv.org/abs/2507.06899</link>
<guid>https://arxiv.org/abs/2507.06899</guid>
<content:encoded><![CDATA[
arXiv:2507.06899v1 Announce Type: new 
Abstract: Graphical User Interface (GUI) agents powered by Large Vision-Language Models (LVLMs) have emerged as a revolutionary approach to automating human-machine interactions, capable of autonomously operating personal devices (e.g., mobile phones) or applications within the device to perform complex real-world tasks in a human-like manner. However, their close integration with personal devices raises significant security concerns, with many threats, including backdoor attacks, remaining largely unexplored. This work reveals that the visual grounding of GUI agent-mapping textual plans to GUI elements-can introduce vulnerabilities, enabling new types of backdoor attacks. With backdoor attack targeting visual grounding, the agent's behavior can be compromised even when given correct task-solving plans. To validate this vulnerability, we propose VisualTrap, a method that can hijack the grounding by misleading the agent to locate textual plans to trigger locations instead of the intended targets. VisualTrap uses the common method of injecting poisoned data for attacks, and does so during the pre-training of visual grounding to ensure practical feasibility of attacking. Empirical results show that VisualTrap can effectively hijack visual grounding with as little as 5% poisoned data and highly stealthy visual triggers (invisible to the human eye); and the attack can be generalized to downstream tasks, even after clean fine-tuning. Moreover, the injected trigger can remain effective across different GUI environments, e.g., being trained on mobile/web and generalizing to desktop environments. These findings underscore the urgent need for further research on backdoor attack risks in GUI agents.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing Adaptive Algorithms Based on Reinforcement Learning for Dynamic Optimization of Sliding Window Size in Multi-Dimensional Data Streams</title>
<link>https://arxiv.org/abs/2507.06901</link>
<guid>https://arxiv.org/abs/2507.06901</guid>
<content:encoded><![CDATA[
arXiv:2507.06901v1 Announce Type: new 
Abstract: Multi-dimensional data streams, prevalent in applications like IoT, financial markets, and real-time analytics, pose significant challenges due to their high velocity, unbounded nature, and complex inter-dimensional dependencies. Sliding window techniques are critical for processing such streams, but fixed-size windows struggle to adapt to dynamic changes like concept drift or bursty patterns. This paper proposes a novel reinforcement learning (RL)-based approach to dynamically optimize sliding window sizes for multi-dimensional data streams. By formulating window size selection as an RL problem, we enable an agent to learn an adaptive policy based on stream characteristics, such as variance, correlations, and temporal trends. Our method, RL-Window, leverages a Dueling Deep Q-Network (DQN) with prioritized experience replay to handle non-stationarity and high-dimensionality. Evaluations on benchmark datasets (UCI HAR, PAMAP2, Yahoo! Finance Stream) demonstrate that RL-Window outperforms state-of-the-art methods like ADWIN and CNN-Adaptive in classification accuracy, drift robustness, and computational efficiency. Additional qualitative analyses, extended metrics (e.g., energy efficiency, latency), and a comprehensive dataset characterization further highlight its adaptability and stability, making it suitable for real-time applications.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemRaFiner: Panoptic Segmentation in Sparse and Noisy Radar Point Clouds</title>
<link>https://arxiv.org/abs/2507.06906</link>
<guid>https://arxiv.org/abs/2507.06906</guid>
<content:encoded><![CDATA[
arXiv:2507.06906v1 Announce Type: new 
Abstract: Semantic scene understanding, including the perception and classification of moving agents, is essential to enabling safe and robust driving behaviours of autonomous vehicles. Cameras and LiDARs are commonly used for semantic scene understanding. However, both sensor modalities face limitations in adverse weather and usually do not provide motion information. Radar sensors overcome these limitations and directly offer information about moving agents by measuring the Doppler velocity, but the measurements are comparably sparse and noisy. In this paper, we address the problem of panoptic segmentation in sparse radar point clouds to enhance scene understanding. Our approach, called SemRaFiner, accounts for changing density in sparse radar point clouds and optimizes the feature extraction to improve accuracy. Furthermore, we propose an optimized training procedure to refine instance assignments by incorporating a dedicated data augmentation. Our experiments suggest that our approach outperforms state-of-the-art methods for radar-based panoptic segmentation.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIND: A Multi-agent Framework for Zero-shot Harmful Meme Detection</title>
<link>https://arxiv.org/abs/2507.06908</link>
<guid>https://arxiv.org/abs/2507.06908</guid>
<content:encoded><![CDATA[
arXiv:2507.06908v1 Announce Type: new 
Abstract: The rapid expansion of memes on social media has highlighted the urgent need for effective approaches to detect harmful content. However, traditional data-driven approaches struggle to detect new memes due to their evolving nature and the lack of up-to-date annotated data. To address this issue, we propose MIND, a multi-agent framework for zero-shot harmful meme detection that does not rely on annotated data. MIND implements three key strategies: 1) We retrieve similar memes from an unannotated reference set to provide contextual information. 2) We propose a bi-directional insight derivation mechanism to extract a comprehensive understanding of similar memes. 3) We then employ a multi-agent debate mechanism to ensure robust decision-making through reasoned arbitration. Extensive experiments on three meme datasets demonstrate that our proposed framework not only outperforms existing zero-shot approaches but also shows strong generalization across different model architectures and parameter scales, providing a scalable solution for harmful meme detection. The code is available at https://github.com/destroy-lonely/MIND.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring LLMs for Predicting Tutor Strategy and Student Outcomes in Dialogues</title>
<link>https://arxiv.org/abs/2507.06910</link>
<guid>https://arxiv.org/abs/2507.06910</guid>
<content:encoded><![CDATA[
arXiv:2507.06910v1 Announce Type: new 
Abstract: Tutoring dialogues have gained significant attention in recent years, given the prominence of online learning and the emerging tutoring abilities of artificial intelligence (AI) agents powered by large language models (LLMs). Recent studies have shown that the strategies used by tutors can have significant effects on student outcomes, necessitating methods to predict how tutors will behave and how their actions impact students. However, few works have studied predicting tutor strategy in dialogues. Therefore, in this work we investigate the ability of modern LLMs, particularly Llama 3 and GPT-4o, to predict both future tutor moves and student outcomes in dialogues, using two math tutoring dialogue datasets. We find that even state-of-the-art LLMs struggle to predict future tutor strategy while tutor strategy is highly indicative of student outcomes, outlining a need for more powerful methods to approach this task.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Cognitive Networks: Reinforcement Learning Meets Energy Harvesting Over Cascaded Channels</title>
<link>https://arxiv.org/abs/2507.06981</link>
<guid>https://arxiv.org/abs/2507.06981</guid>
<content:encoded><![CDATA[
arXiv:2507.06981v1 Announce Type: new 
Abstract: This paper presents a reinforcement learning (RL) based approach to improve the physical layer security (PLS) of an underlay cognitive radio network (CRN) over cascaded channels. These channels are utilized in highly mobile networks such as cognitive vehicular networks (CVN). In addition, an eavesdropper aims to intercept the communications between secondary users (SUs). The SU receiver has full-duplex and energy harvesting capabilities to generate jamming signals to confound the eavesdropper and enhance security. Moreover, the SU transmitter extracts energy from ambient radio frequency signals in order to power subsequent transmissions to its intended receiver. To optimize the privacy and reliability of the SUs in a CVN, a deep Q-network (DQN) strategy is utilized where multiple DQN agents are required such that an agent is assigned at each SU transmitter. The objective for the SUs is to determine the optimal transmission power and decide whether to collect energy or transmit messages during each time period in order to maximize their secrecy rate. Thereafter, we propose a DQN approach to maximize the throughput of the SUs while respecting the interference threshold acceptable at the receiver of the primary user. According to our findings, our strategy outperforms two other baseline strategies in terms of security and reliability.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced Planning, Navigation, and Dynamic Adaptation</title>
<link>https://arxiv.org/abs/2507.06993</link>
<guid>https://arxiv.org/abs/2507.06993</guid>
<content:encoded><![CDATA[
arXiv:2507.06993v1 Announce Type: new 
Abstract: Traditional travel-planning systems are often static and fragmented, leaving them ill-equipped to handle real-world complexities such as evolving environmental conditions and unexpected itinerary disruptions. In this paper, we identify three gaps between existing service providers causing frustrating user experience: intelligent trip planning, precision "last-100-meter" navigation, and dynamic itinerary adaptation. We propose three cooperative agents: a Travel Planning Agent that employs grid-based spatial grounding and map analysis to help resolve complex multi-modal user queries; a Destination Assistant Agent that provides fine-grained guidance for the final navigation leg of each journey; and a Local Discovery Agent that leverages image embeddings and Retrieval-Augmented Generation (RAG) to detect and respond to trip plan disruptions. With evaluations and experiments, our system demonstrates substantial improvements in query interpretation, navigation accuracy, and disruption resilience, underscoring its promise for applications from urban exploration to emergency response.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-Based Complexity Metrics for Multi-Agent Curriculum Learning: A Validated Approach to Task Ordering in Cooperative Coordination Environments</title>
<link>https://arxiv.org/abs/2507.07074</link>
<guid>https://arxiv.org/abs/2507.07074</guid>
<content:encoded><![CDATA[
arXiv:2507.07074v1 Announce Type: new 
Abstract: Multi-agent reinforcement learning (MARL) faces significant challenges in task sequencing and curriculum design, particularly for cooperative coordination scenarios. While curriculum learning has demonstrated success in single-agent domains, principled approaches for multi-agent coordination remain limited due to the absence of validated task complexity metrics. This approach presents a graph-based coordination complexity metric that integrates agent dependency entropy, spatial interference patterns, and goal overlap analysis to predict task difficulty in multi-agent environments. The complexity metric achieves strong empirical validation with rho = 0.952 correlation (p < 0.001) between predicted complexity and empirical difficulty determined by random agent performance evaluation. This approach evaluates the curriculum learning framework using MADDPG across two distinct coordination environments: achieving 56x performance improvement in tight coordination tasks (MultiWalker) and demonstrating systematic task progression in cooperative navigation (Simple Spread). Through systematic analysis, coordination tightness emerges as a predictor of curriculum learning effectiveness, where environments requiring strict agent interdependence benefit substantially from structured progression. This approach provides a validated complexity metric for multi-agent curriculum design and establishes empirical guidelines for multi-robot coordination applications.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>4KAgent: Agentic Any Image to 4K Super-Resolution</title>
<link>https://arxiv.org/abs/2507.07105</link>
<guid>https://arxiv.org/abs/2507.07105</guid>
<content:encoded><![CDATA[
arXiv:2507.07105v1 Announce Type: new 
Abstract: We present 4KAgent, a unified agentic super-resolution generalist system designed to universally upscale any image to 4K resolution (and even higher, if applied iteratively). Our system can transform images from extremely low resolutions with severe degradations, for example, highly distorted inputs at 256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three core components: (1) Profiling, a module that customizes the 4KAgent pipeline based on bespoke use cases; (2) A Perception Agent, which leverages vision-language models alongside image quality assessment experts to analyze the input image and make a tailored restoration plan; and (3) A Restoration Agent, which executes the plan, following a recursive execution-reflection paradigm, guided by a quality-driven mixture-of-expert policy to select the optimal output for each step. Additionally, 4KAgent embeds a specialized face restoration pipeline, significantly enhancing facial details in portrait and selfie photos. We rigorously evaluate our 4KAgent across 11 distinct task categories encompassing a total of 26 diverse benchmarks, setting new state-of-the-art on a broad spectrum of imaging domains. Our evaluations cover natural images, portrait photos, AI-generated content, satellite imagery, fluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and X-ray, demonstrating superior performance in terms of both perceptual (e.g., NIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic paradigm for low-level vision tasks, we aim to catalyze broader interest and innovation within vision-centric autonomous agents across diverse research communities. We will release all the code, models, and results at: https://4kagent.github.io.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Optimization of Finite Condition Number for Laplacian Matrix in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2507.06440</link>
<guid>https://arxiv.org/abs/2507.06440</guid>
<content:encoded><![CDATA[
arXiv:2507.06440v1 Announce Type: cross 
Abstract: This paper addresses the distributed optimization of the finite condition number of the Laplacian matrix in multi-agent systems. The finite condition number, defined as the ratio of the largest to the second smallest eigenvalue of the Laplacian matrix, plays an important role in determining the convergence rate and performance of consensus algorithms, especially in discrete-time implementations. We propose a fully distributed algorithm by regulating the node weights. The approach leverages max consensus, distributed power iteration, and consensus-based normalization for eigenvalue and eigenvector estimation, requiring only local communication and computation. Simulation results demonstrate that the proposed method achieves performance comparable to centralized LMI-based optimization, significantly improving consensus speed and multi-agent system performance. The framework can be extended to edge weight optimization and the scenarios with non-simple eigenvalues, highlighting its scalability and practical applicability for large-scale networked systems.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning-based MARL for Strengthening Physical-Layer Security in B5G Networks</title>
<link>https://arxiv.org/abs/2507.06997</link>
<guid>https://arxiv.org/abs/2507.06997</guid>
<content:encoded><![CDATA[
arXiv:2507.06997v1 Announce Type: cross 
Abstract: This paper explores the application of a federated learning-based multi-agent reinforcement learning (MARL) strategy to enhance physical-layer security (PLS) in a multi-cellular network within the context of beyond 5G networks. At each cell, a base station (BS) operates as a deep reinforcement learning (DRL) agent that interacts with the surrounding environment to maximize the secrecy rate of legitimate users in the presence of an eavesdropper. This eavesdropper attempts to intercept the confidential information shared between the BS and its authorized users. The DRL agents are deemed to be federated since they only share their network parameters with a central server and not the private data of their legitimate users. Two DRL approaches, deep Q-network (DQN) and Reinforce deep policy gradient (RDPG), are explored and compared. The results demonstrate that RDPG converges more rapidly than DQN. In addition, we demonstrate that the proposed method outperforms the distributed DRL approach. Furthermore, the outcomes illustrate the trade-off between security and complexity.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Welcome to the Dark Side: Analyzing the Revenue Flows of Fraud in the Online Ad Ecosystem</title>
<link>https://arxiv.org/abs/2306.08418</link>
<guid>https://arxiv.org/abs/2306.08418</guid>
<content:encoded><![CDATA[
arXiv:2306.08418v3 Announce Type: replace 
Abstract: The online advertising market has recently reached the 500 billion dollar mark. To accommodate the need to match a user with the highest bidder at a fraction of a second, it has moved towards a complex, automated and often opaque model that involves numerous agents and intermediaries. Stimulated by the lack of transparency, but also the enormous potential profits, bad actors have found ways to circumvent restrictions, and generate substantial revenue that can support websites with objectionable or even illegal content.
  In this work, we evaluate transparency Web standards and show how shady actors take advantage of gaps to absorb ad revenues while putting the brand safety of advertisers in danger. We collect and study a large corpus of thousands of websites and show how ad transparency standards can be abused by bad actors to obscure ad revenue flows. We show how identifier pooling can redirect ad revenues from reputable domains to notorious domains serving objectionable content, and that the phenomenon is underestimated by previous studies by a factor of 15. Finally, we publish a Web monitoring service that enhances the transparency of supply chains and business relationships between publishers and ad networks.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Transfer Learning via Causal Bounds</title>
<link>https://arxiv.org/abs/2308.03572</link>
<guid>https://arxiv.org/abs/2308.03572</guid>
<content:encoded><![CDATA[
arXiv:2308.03572v5 Announce Type: replace 
Abstract: Transfer learning seeks to accelerate sequential decision-making by leveraging offline data from related agents. However, data from heterogeneous sources that differ in observed features, distributions, or unobserved confounders often render causal effects non-identifiable and bias naive estimators. We address this by forming ambiguity sets of structural causal models defined via integral constraints on their joint densities. Optimizing any causal effect over these sets leads to generally non-convex programs whose solutions tightly bound the range of possible effects under heterogeneity or confounding. To solve these programs efficiently, we develop a hit-and-run sampler that explores the entire ambiguity set and, when paired with a local optimization oracle, produces causal bound estimates that converge almost surely to the true limits. We further accommodate estimation error by relaxing the ambiguity set and exploit the Lipschitz continuity of causal effects to establish precise error propagation guarantees. These causal bounds are then embedded into bandit algorithms via arm elimination and truncated UCB indices, yielding optimal gap-dependent and minimax regret bounds. To handle estimation error, we also develop a safe algorithm for incorporating noisy causal bounds. In the contextual-bandit setting with function approximation, our method uses causal bounds to prune both the function class and the per-context action set, achieving matching upper and lower regret bounds with only logarithmic dependence on function-class complexity. Our analysis precisely characterizes when and how causal side-information accelerates online learning, and experiments on synthetic benchmarks confirm substantial regret reductions in data-scarce or confounded regimes.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Policy-Gradient Approach to Solving Imperfect-Information Games with Best-Iterate Convergence</title>
<link>https://arxiv.org/abs/2408.00751</link>
<guid>https://arxiv.org/abs/2408.00751</guid>
<content:encoded><![CDATA[
arXiv:2408.00751v2 Announce Type: replace 
Abstract: Policy gradient methods have become a staple of any single-agent reinforcement learning toolbox, due to their combination of desirable properties: iterate convergence, efficient use of stochastic trajectory feedback, and theoretically-sound avoidance of importance sampling corrections. In multi-agent imperfect-information settings (extensive-form games), however, it is still unknown whether the same desiderata can be guaranteed while retaining theoretical guarantees. Instead, sound methods for extensive-form games rely on approximating \emph{counterfactual} values (as opposed to Q values), which are incompatible with policy gradient methodologies. In this paper, we investigate whether policy gradient can be safely used in two-player zero-sum imperfect-information extensive-form games (EFGs). We establish positive results, showing for the first time that a policy gradient method leads to provable best-iterate convergence to a regularized Nash equilibrium in self-play.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoPrep: Natural Language Question-Aware Data Preparation with a Multi-Agent Framework</title>
<link>https://arxiv.org/abs/2412.10422</link>
<guid>https://arxiv.org/abs/2412.10422</guid>
<content:encoded><![CDATA[
arXiv:2412.10422v4 Announce Type: replace 
Abstract: Answering natural language (NL) questions about tables, known as Tabular Question Answering (TQA), is crucial because it allows users to quickly and efficiently extract meaningful insights from structured data, effectively bridging the gap between human language and machine-readable formats. Many of these tables are derived from web sources or real-world scenarios, which require meticulous data preparation (or data prep) to ensure accurate responses. However, preparing such tables for NL questions introduces new requirements that extend beyond traditional data preparation. This question-ware data preparation involves specific tasks such as column derivation and filtering tailored to particular questions, as well as question-aware value normalization or conversion, highlighting the need for a more nuanced approach in this context. Because each of the above tasks is unique, a single model (or agent) may not perform effectively across all scenarios. In this paper, we propose AutoPrep, a large language model (LLM)-based multiagent framework that leverages the strengths of multiple agents, each specialized in a certain type of data prep, ensuring more accurate and contextually relevant responses. Given an NL question over a table, AutoPrep performs data prep through three key components. Planner: Determines a logical plan, outlining a sequence of high-level operations. Programmer: Translates this logical plan into a physical plan by generating the corresponding low-level code. Executor: Executes the generated code to process the table. To support this multi-agent framework, we design a novel Chain-ofClauses reasoning mechanism for high-level operation suggestion, and a tool-augmented method for low-level code generation.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Pathfinding Under Team-Connected Communication Constraint via Adaptive Path Expansion and Dynamic Leading</title>
<link>https://arxiv.org/abs/2501.02770</link>
<guid>https://arxiv.org/abs/2501.02770</guid>
<content:encoded><![CDATA[
arXiv:2501.02770v3 Announce Type: replace 
Abstract: This paper proposes a novel planning framework to handle a multi-agent pathfinding problem under team-connected communication constraint, where all agents must have a connected communication channel to the rest of the team during their entire movements. Standard multi-agent path finding approaches (e.g., priority-based search) have potential in this domain but fail when neighboring configurations at start and goal differ. Their single-expansion approach -- computing each agent's path from the start to the goal in just a single expansion -- cannot reliably handle planning under communication constraints for agents as their neighbors change during navigating. Similarly, leader-follower approaches (e.g., platooning) are effective at maintaining team communication, but fixing the leader at the outset of planning can cause planning to become stuck in dense-clutter environments, limiting their practical utility. To overcome this limitation, we propose a novel two-level multi-agent pathfinding framework that integrates two techniques: adaptive path expansion to expand agent paths to their goals in multiple stages; and dynamic leading technique that enables the reselection of the leading agent during each agent path expansion whenever progress cannot be made. Simulation experiments show the efficiency of our planners, which can handle up to 25 agents across five environment types under a limited communication range constraint and up to 11-12 agents on three environment types under line-of-sight communication constraint, exceeding 90% success-rate where baselines routinely fail.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinSphere, a Real-Time Stock Analysis Agent Powered by Instruction-Tuned LLMs and Domain Tools</title>
<link>https://arxiv.org/abs/2501.12399</link>
<guid>https://arxiv.org/abs/2501.12399</guid>
<content:encoded><![CDATA[
arXiv:2501.12399v2 Announce Type: replace 
Abstract: Current financial large language models (FinLLMs) struggle with two critical limitations: the absence of objective evaluation metrics to assess the quality of stock analysis reports and a lack of depth in stock analysis, which impedes their ability to generate professional-grade insights. To address these challenges, this paper introduces FinSphere, a stock analysis agent, along with three major contributions: (1) AnalyScore, a systematic evaluation framework for assessing stock analysis quality, (2) Stocksis, a dataset curated by industry experts to enhance LLMs' stock analysis capabilities, and (3) FinSphere, an AI agent that can generate high-quality stock analysis reports in response to user queries. Experiments demonstrate that FinSphere achieves superior performance compared to both general and domain-specific LLMs, as well as existing agent-based systems, even when they are enhanced with real-time data access and few-shot guidance. The integrated framework, which combines real-time data feeds, quantitative tools, and an instruction-tuned LLM, yields substantial improvements in both analytical quality and practical applicability for real-world stock analysis.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Quantum-Classical Multi-Agent Pathfinding</title>
<link>https://arxiv.org/abs/2501.14568</link>
<guid>https://arxiv.org/abs/2501.14568</guid>
<content:encoded><![CDATA[
arXiv:2501.14568v2 Announce Type: replace 
Abstract: Multi-Agent Path Finding (MAPF) focuses on determining conflict-free paths for multiple agents navigating through a shared space to reach specified goal locations. This problem becomes computationally challenging, particularly when handling large numbers of agents, as frequently encountered in practical applications like coordinating autonomous vehicles. Quantum Computing (QC) is a promising candidate in overcoming such limits. However, current quantum hardware is still in its infancy and thus limited in terms of computing power and error robustness. In this work, we present the first optimal hybrid quantum-classical MAPF algorithms which are based on branch-andcut-and-price. QC is integrated by iteratively solving QUBO problems, based on conflict graphs. Experiments on actual quantum hardware and results on benchmark data suggest that our approach dominates previous QUBO formulationsand state-of-the-art MAPF solvers.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Enterprise-Ready Computer Using Generalist Agent</title>
<link>https://arxiv.org/abs/2503.01861</link>
<guid>https://arxiv.org/abs/2503.01861</guid>
<content:encoded><![CDATA[
arXiv:2503.01861v3 Announce Type: replace 
Abstract: This paper presents our ongoing work toward developing an enterprise-ready Computer Using Generalist Agent (CUGA) system. Our research highlights the evolutionary nature of building agentic systems suitable for enterprise environments. By integrating state-of-the-art agentic AI techniques with a systematic approach to iterative evaluation, analysis, and refinement, we have achieved rapid and cost-effective performance gains, notably reaching a new state-of-the-art performance on the WebArena and AppWorld benchmarks. We detail our development roadmap, the methodology and tools that facilitated rapid learning from failures and continuous system refinement, and discuss key lessons learned and future challenges for enterprise adoption.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment</title>
<link>https://arxiv.org/abs/2503.02505</link>
<guid>https://arxiv.org/abs/2503.02505</guid>
<content:encoded><![CDATA[
arXiv:2503.02505v2 Announce Type: replace 
Abstract: We aim to develop a goal specification method that is semantically clear, spatially sensitive, domain-agnostic, and intuitive for human users to guide agent interactions in 3D environments. Specifically, we propose a novel cross-view goal alignment framework that allows users to specify target objects using segmentation masks from their camera views rather than the agent's observations. We highlight that behavior cloning alone fails to align the agent's behavior with human intent when the human and agent camera views differ significantly. To address this, we introduce two auxiliary objectives: cross-view consistency loss and target visibility loss, which explicitly enhance the agent's spatial reasoning ability. According to this, we develop ROCKET-2, a state-of-the-art agent trained in Minecraft, achieving an improvement in the efficiency of inference 3x to 6x compared to ROCKET-1. We show that ROCKET-2 can directly interpret goals from human camera views, enabling better human-agent interaction. Remarkably, ROCKET-2 demonstrates zero-shot generalization capabilities: despite being trained exclusively on the Minecraft dataset, it can adapt and generalize to other 3D environments like Doom, DMLab, and Unreal through a simple action space mapping.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SagaLLM: Context Management, Validation, and Transaction Guarantees for Multi-Agent LLM Planning</title>
<link>https://arxiv.org/abs/2503.11951</link>
<guid>https://arxiv.org/abs/2503.11951</guid>
<content:encoded><![CDATA[
arXiv:2503.11951v3 Announce Type: replace 
Abstract: This paper introduces SagaLLM, a structured multi-agent architecture designed to address four foundational limitations of current LLM-based planning systems: unreliable self-validation, context loss, lack of transactional safeguards, and insufficient inter-agent coordination. While recent frameworks leverage LLMs for task decomposition and multi-agent communication, they often fail to ensure consistency, rollback, or constraint satisfaction across distributed workflows. SagaLLM bridges this gap by integrating the Saga transactional pattern with persistent memory, automated compensation, and independent validation agents. It leverages LLMs' generative reasoning to automate key tasks traditionally requiring hand-coded coordination logic, including state tracking, dependency analysis, log schema generation, and recovery orchestration. Although SagaLLM relaxes strict ACID guarantees, it ensures workflow-wide consistency and recovery through modular checkpointing and compensable execution. Empirical evaluations across planning domains demonstrate that standalone LLMs frequently violate interdependent constraints or fail to recover from disruptions. In contrast, SagaLLM achieves significant improvements in consistency, validation accuracy, and adaptive coordination under uncertainty, establishing a robust foundation for real-world, scalable LLM-based multi-agent systems.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real AI Agents with Fake Memories: Fatal Context Manipulation Attacks on Web3 Agents</title>
<link>https://arxiv.org/abs/2503.16248</link>
<guid>https://arxiv.org/abs/2503.16248</guid>
<content:encoded><![CDATA[
arXiv:2503.16248v3 Announce Type: replace 
Abstract: AI agents integrated with Web3 offer autonomy and openness but raise security concerns as they interact with financial protocols and immutable smart contracts. This paper investigates the vulnerabilities of AI agents within blockchain-based financial ecosystems when exposed to adversarial threats in real-world scenarios. We introduce the concept of context manipulation -- a comprehensive attack vector that exploits unprotected context surfaces, including input channels, memory modules, and external data feeds. It expands on traditional prompt injection and reveals a more stealthy and persistent threat: memory injection. Using ElizaOS, a representative decentralized AI agent framework for automated Web3 operations, we showcase that malicious injections into prompts or historical records can trigger unauthorized asset transfers and protocol violations which could be financially devastating in reality. To quantify these risks, we introduce CrAIBench, a Web3-focused benchmark covering 150+ realistic blockchain tasks. such as token transfers, trading, bridges, and cross-chain interactions, and 500+ attack test cases using context manipulation. Our evaluation results confirm that AI models are significantly more vulnerable to memory injection compared to prompt injection. Finally, we evaluate a comprehensive defense roadmap, finding that prompt-injection defenses and detectors only provide limited protection when stored context is corrupted, whereas fine-tuning-based defenses substantially reduce attack success rates while preserving performance on single-step tasks. These results underscore the urgent need for AI agents that are both secure and fiduciarily responsible in blockchain environments.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Substance over Style: Evaluating Proactive Conversational Coaching Agents</title>
<link>https://arxiv.org/abs/2503.19328</link>
<guid>https://arxiv.org/abs/2503.19328</guid>
<content:encoded><![CDATA[
arXiv:2503.19328v2 Announce Type: replace 
Abstract: While NLP research has made strides in conversational tasks, many approaches focus on single-turn responses with well-defined objectives or evaluation criteria. In contrast, coaching presents unique challenges with initially undefined goals that evolve through multi-turn interactions, subjective evaluation criteria, mixed-initiative dialogue. In this work, we describe and implement five multi-turn coaching agents that exhibit distinct conversational styles, and evaluate them through a user study, collecting first-person feedback on 155 conversations. We find that users highly value core functionality, and that stylistic components in absence of core components are viewed negatively. By comparing user feedback with third-person evaluations from health experts and an LM, we reveal significant misalignment across evaluation approaches. Our findings provide insights into design and evaluation of conversational coaching agents and contribute toward improving human-centered NLP applications.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.08617</link>
<guid>https://arxiv.org/abs/2505.08617</guid>
<content:encoded><![CDATA[
arXiv:2505.08617v2 Announce Type: replace 
Abstract: While humans can flexibly leverage interactive visual cognition for complex problem-solving, enabling Large Vision-Language Models (LVLMs) to learn similarly adaptive behaviors with visual tools remains challenging. A significant hurdle is the current lack of standardized infrastructure, which hinders integrating diverse tools, generating rich interaction data, and training robust agents effectively. To address these gaps, we introduce OpenThinkIMG, the first open-source, comprehensive end-to-end framework for tool-augmented LVLMs. It features standardized vision tool interfaces, scalable trajectory generation for policy initialization, and a flexible training environment. Furthermore, considering supervised fine-tuning (SFT) on static demonstrations offers limited policy generalization for dynamic tool invocation, we propose a novel reinforcement learning (RL) framework V-ToolRL to train LVLMs to learn adaptive policies for invoking external vision tools. V-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies by directly optimizing for task success using feedback from tool interactions. We empirically validate V-ToolRL on challenging chart reasoning tasks. Our RL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its SFT-initialized counterpart (+28.83 points) and surpasses established supervised tool-learning baselines like Taco and CogCom by an average of +12.7 points. Notably, it also surpasses prominent closed-source models like GPT-4.1 by +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational framework for advancing dynamic, tool-augmented visual reasoning, helping the community develop AI agents that can genuinely "think with images".
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Optimisation Framework for Unsupervised Environment Design</title>
<link>https://arxiv.org/abs/2505.20659</link>
<guid>https://arxiv.org/abs/2505.20659</guid>
<content:encoded><![CDATA[
arXiv:2505.20659v2 Announce Type: replace 
Abstract: For reinforcement learning agents to be deployed in high-risk settings, they must achieve a high level of robustness to unfamiliar scenarios. One method for improving robustness is unsupervised environment design (UED), a suite of methods aiming to maximise an agent's generalisability across configurations of an environment. In this work, we study UED from an optimisation perspective, providing stronger theoretical guarantees for practical settings than prior work. Whereas previous methods relied on guarantees if they reach convergence, our framework employs a nonconvex-strongly-concave objective for which we provide a provably convergent algorithm in the zero-sum setting. We empirically verify the efficacy of our method, outperforming prior methods in a number of environments with varying difficulties.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QUITE: A Query Rewrite System Beyond Rules with LLM Agents</title>
<link>https://arxiv.org/abs/2506.07675</link>
<guid>https://arxiv.org/abs/2506.07675</guid>
<content:encoded><![CDATA[
arXiv:2506.07675v2 Announce Type: replace 
Abstract: Query rewrite transforms SQL queries into semantically equivalent forms that run more efficiently. Existing approaches mainly rely on predefined rewrite rules, but they handle a limited subset of queries and can cause performance regressions. This limitation stems from three challenges of rule-based query rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite rules do not generalize to new query patterns, and (3) some rewrite techniques cannot be expressed as fixed rules. Motivated by the fact that human experts exhibit significantly better rewrite ability but suffer from scalability, and Large Language Models (LLMs) have demonstrated nearly human-level semantic and reasoning abilities, we propose a new approach of using LLMs to rewrite SQL queries beyond rules. Due to the hallucination problems in LLMs, directly applying LLMs often leads to nonequivalent and suboptimal queries. To address this issue, we propose QUITE (query rewrite), a training-free and feedback-aware system based on LLM agents that rewrites SQL queries into semantically equivalent forms with significantly better performance, covering a broader range of query patterns and rewrite strategies compared to rule-based methods. Firstly, we design a multi-agent framework controlled by a finite state machine (FSM) to equip LLMs with the ability to use external tools and enhance the rewrite process with real-time database feedback. Secondly, we develop a rewrite middleware to enhance the ability of LLMs to generate optimized query equivalents. Finally, we employ a novel hint injection technique to improve execution plans for rewritten queries. Extensive experiments show that QUITE reduces query execution time by up to 35.8% over state-of-the-art approaches and produces 24.1% more rewrites than prior methods, covering query cases that earlier systems did not handle.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protecting Classifiers From Attacks</title>
<link>https://arxiv.org/abs/2004.08705</link>
<guid>https://arxiv.org/abs/2004.08705</guid>
<content:encoded><![CDATA[
arXiv:2004.08705v2 Announce Type: replace-cross 
Abstract: In multiple domains such as malware detection, automated driving systems, or fraud detection, classification algorithms are susceptible to being attacked by malicious agents willing to perturb the value of instance covariates to pursue certain goals. Such problems pertain to the field of adversarial machine learning and have been mainly dealt with, perhaps implicitly, through game-theoretic ideas with strong underlying common knowledge assumptions. These are not realistic in numerous application domains in relation to security and business competition. We present an alternative Bayesian decision theoretic framework that accounts for the uncertainty about the attacker's behavior using adversarial risk analysis concepts. In doing so, we also present core ideas in adversarial machine learning to a statistical audience. A key ingredient in our framework is the ability to sample from the distribution of originating instances given the, possibly attacked, observed ones. We propose an initial procedure based on approximate Bayesian computation usable during operations; within it, we simulate the attacker's problem taking into account our uncertainty about his elements. Large-scale problems require an alternative scalable approach implementable during the training stage. Globally, we are able to robustify statistical classification algorithms against malicious attacks.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fuzzy Supervisor Agent Design for Clinical Reasoning Assistance in a Multi-Agent Educational Clinical Scenario Simulation</title>
<link>https://arxiv.org/abs/2507.05275</link>
<guid>https://arxiv.org/abs/2507.05275</guid>
<content:encoded><![CDATA[
arXiv:2507.05275v1 Announce Type: new 
Abstract: Assisting medical students with clinical reasoning (CR) during clinical scenario training remains a persistent challenge in medical education. This paper presents the design and architecture of the Fuzzy Supervisor Agent (FSA), a novel component for the Multi-Agent Educational Clinical Scenario Simulation (MAECSS) platform. The FSA leverages a Fuzzy Inference System (FIS) to continuously interpret student interactions with specialized clinical agents (e.g., patient, physical exam, diagnostic, intervention) using pre-defined fuzzy rule bases for professionalism, medical relevance, ethical behavior, and contextual distraction. By analyzing student decision-making processes in real-time, the FSA is designed to deliver adaptive, context-aware feedback and provides assistance precisely when students encounter difficulties. This work focuses on the technical framework and rationale of the FSA, highlighting its potential to provide scalable, flexible, and human-like supervision in simulation-based medical education. Future work will include empirical evaluation and integration into broader educational settings. More detailed design and implementation is~\href{https://github.com/2sigmaEdTech/MAS/}{open sourced here}.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A LLM-Driven Multi-Agent Systems for Professional Development of Mathematics Teachers</title>
<link>https://arxiv.org/abs/2507.05292</link>
<guid>https://arxiv.org/abs/2507.05292</guid>
<content:encoded><![CDATA[
arXiv:2507.05292v1 Announce Type: new 
Abstract: Professional development (PD) serves as the cornerstone for teacher tutors to grasp content knowledge. However, providing equitable and timely PD opportunities for teachers poses significant challenges. To address this issue, we introduce I-VIP (Intelligent Virtual Interactive Program), an intelligent tutoring platform for teacher professional development, driven by large language models (LLMs) and supported by multi-agent frameworks. This platform offers a user-friendly conversational interface and allows users to employ a variety of interactive tools to facilitate question answering, knowledge comprehension, and reflective summarization while engaging in dialogue. To underpin the functionality of this platform, including knowledge expectation analysis, response scoring and classification, and feedback generation, the multi-agent frameworks are leveraged to enhance the accuracy of judgments and mitigate the issue of missing key points.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OASBuilder: Generating OpenAPI Specifications from Online API Documentation with Large Language Models</title>
<link>https://arxiv.org/abs/2507.05316</link>
<guid>https://arxiv.org/abs/2507.05316</guid>
<content:encoded><![CDATA[
arXiv:2507.05316v1 Announce Type: new 
Abstract: AI agents and business automation tools interacting with external web services require standardized, machine-readable information about their APIs in the form of API specifications. However, the information about APIs available online is often presented as unstructured, free-form HTML documentation, requiring external users to spend significant time manually converting it into a structured format. To address this, we introduce OASBuilder, a novel framework that transforms long and diverse API documentation pages into consistent, machine-readable API specifications. This is achieved through a carefully crafted pipeline that integrates large language models and rule-based algorithms which are guided by domain knowledge of the structure of documentation webpages. Our experiments demonstrate that OASBuilder generalizes well across hundreds of APIs, and produces valid OpenAPI specifications that encapsulate most of the information from the original documentation. OASBuilder has been successfully implemented in an enterprise environment, saving thousands of hours of manual effort and making hundreds of complex enterprise APIs accessible as tools for LLMs.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AGACCI : Affiliated Grading Agents for Criteria-Centric Interface in Educational Coding Contexts</title>
<link>https://arxiv.org/abs/2507.05321</link>
<guid>https://arxiv.org/abs/2507.05321</guid>
<content:encoded><![CDATA[
arXiv:2507.05321v1 Announce Type: new 
Abstract: Recent advances in AI-assisted education have encouraged the integration of vision-language models (VLMs) into academic assessment, particularly for tasks that require both quantitative and qualitative evaluation. However, existing VLM based approaches struggle with complex educational artifacts, such as programming tasks with executable components and measurable outputs, that require structured reasoning and alignment with clearly defined evaluation criteria. We introduce AGACCI, a multi-agent system that distributes specialized evaluation roles across collaborative agents to improve accuracy, interpretability, and consistency in code-oriented assessment. To evaluate the framework, we collected 360 graduate-level code-based assignments from 60 participants, each annotated by domain experts with binary rubric scores and qualitative feedback. Experimental results demonstrate that AGACCI outperforms a single GPT-based baseline in terms of rubric and feedback accuracy, relevance, consistency, and coherence, while preserving the instructional intent and evaluative depth of expert assessments. Although performance varies across task types, AGACCI highlights the potential of multi-agent systems for scalable and context-aware educational evaluation.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents</title>
<link>https://arxiv.org/abs/2507.05330</link>
<guid>https://arxiv.org/abs/2507.05330</guid>
<content:encoded><![CDATA[
arXiv:2507.05330v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have enabled new applications in e-commerce customer service. However, their capabilities remain constrained in complex, multimodal scenarios. We present MindFlow, the first open-source multimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it integrates memory, decision-making, and action modules, and adopts a modular "MLLM-as-Tool" strategy for effect visual-textual reasoning. Evaluated via online A/B testing and simulation-based ablation, MindFlow demonstrates substantial gains in handling complex queries, improving user satisfaction, and reducing operational costs, with a 93.53% relative improvement observed in real-world deployments.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Motion Generation: A Survey of Generative Approaches and Benchmarks</title>
<link>https://arxiv.org/abs/2507.05419</link>
<guid>https://arxiv.org/abs/2507.05419</guid>
<content:encoded><![CDATA[
arXiv:2507.05419v1 Announce Type: new 
Abstract: Motion generation, the task of synthesizing realistic motion sequences from various conditioning inputs, has become a central problem in computer vision, computer graphics, and robotics, with applications ranging from animation and virtual agents to human-robot interaction. As the field has rapidly progressed with the introduction of diverse modeling paradigms including GANs, autoencoders, autoregressive models, and diffusion-based techniques, each approach brings its own advantages and limitations. This growing diversity has created a need for a comprehensive and structured review that specifically examines recent developments from the perspective of the generative approach employed.
  In this survey, we provide an in-depth categorization of motion generation methods based on their underlying generative strategies. Our main focus is on papers published in top-tier venues since 2023, reflecting the most recent advancements in the field. In addition, we analyze architectural principles, conditioning mechanisms, and generation settings, and compile a detailed overview of the evaluation metrics and datasets used across the literature. Our objective is to enable clearer comparisons and identify open challenges, thereby offering a timely and foundational reference for researchers and practitioners navigating the rapidly evolving landscape of motion generation.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematization of Security Vulnerabilities in Computer Use Agents</title>
<link>https://arxiv.org/abs/2507.05445</link>
<guid>https://arxiv.org/abs/2507.05445</guid>
<content:encoded><![CDATA[
arXiv:2507.05445v1 Announce Type: new 
Abstract: Computer Use Agents (CUAs), autonomous systems that interact with software interfaces via browsers or virtual machines, are rapidly being deployed in consumer and enterprise environments. These agents introduce novel attack surfaces and trust boundaries that are not captured by traditional threat models. Despite their growing capabilities, the security boundaries of CUAs remain poorly understood. In this paper, we conduct a systematic threat analysis and testing of real-world CUAs under adversarial conditions. We identify seven classes of risks unique to the CUA paradigm, and analyze three concrete exploit scenarios in depth: (1) clickjacking via visual overlays that mislead interface-level reasoning, (2) indirect prompt injection that enables Remote Code Execution (RCE) through chained tool use, and (3) CoT exposure attacks that manipulate implicit interface framing to hijack multi-step reasoning. These case studies reveal deeper architectural flaws across current CUA implementations. Namely, a lack of input provenance tracking, weak interface-action binding, and insufficient control over agent memory and delegation. We conclude by proposing a CUA-specific security evaluation framework and design principles for safe deployment in adversarial and high-stakes settings.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2048: Reinforcement Learning in a Delayed Reward Environment</title>
<link>https://arxiv.org/abs/2507.05465</link>
<guid>https://arxiv.org/abs/2507.05465</guid>
<content:encoded><![CDATA[
arXiv:2507.05465v1 Announce Type: new 
Abstract: Delayed and sparse rewards present a fundamental obstacle for reinforcement-learning (RL) agents, which struggle to assign credit for actions whose benefits emerge many steps later. The sliding-tile game 2048 epitomizes this challenge: although frequent small score changes yield immediate feedback, they often mislead agents into locally optimal but globally suboptimal strategies. In this work, we introduce a unified, distributional multi-step RL framework designed to directly optimize long-horizon performance. Using the open source Gym-2048 environment we develop and compare four agent variants: standard DQN, PPO, QR-DQN (Quantile Regression DQN), and a novel Horizon-DQN (H-DQN) that integrates distributional learning, dueling architectures, noisy networks, prioritized replay, and more. Empirical evaluation reveals a clear hierarchy in effectiveness: max episode scores improve from 3.988K (DQN) to 5.756K (PPO), 8.66K (QR-DQN), and 18.21K (H-DQN), with H-DQN reaching the 2048 tile. Upon scaling H-DQN it reaches a max score 41.828K and a 4096 tile. These results demonstrate that distributional, multi-step targets substantially enhance performance in sparse-reward domains, and they suggest promising avenues for further gains through model-based planning and curriculum learning.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inaugural MOASEI Competition at AAMAS'2025: A Technical Report</title>
<link>https://arxiv.org/abs/2507.05469</link>
<guid>https://arxiv.org/abs/2507.05469</guid>
<content:encoded><![CDATA[
arXiv:2507.05469v1 Announce Type: new 
Abstract: We present the Methods for Open Agent Systems Evaluation Initiative (MOASEI) Competition, a multi-agent AI benchmarking event designed to evaluate decision-making under open-world conditions. Built on the free-range-zoo environment suite, MOASEI introduced dynamic, partially observable domains with agent and task openness--settings where entities may appear, disappear, or change behavior over time. The 2025 competition featured three tracks--Wildfire, Rideshare, and Cybersecurity--each highlighting distinct dimensions of openness and coordination complexity. Eleven teams from international institutions participated, with four of those teams submitting diverse solutions including graph neural networks, convolutional architectures, predictive modeling, and large language model--driven meta--optimization. Evaluation metrics centered on expected utility, robustness to perturbations, and responsiveness to environmental change. The results reveal promising strategies for generalization and adaptation in open environments, offering both empirical insight and infrastructure for future research. This report details the competition's design, findings, and contributions to the open-agent systems research community.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constraint Hypergraphs as a Unifying Framework for Digital Twins</title>
<link>https://arxiv.org/abs/2507.05494</link>
<guid>https://arxiv.org/abs/2507.05494</guid>
<content:encoded><![CDATA[
arXiv:2507.05494v1 Announce Type: new 
Abstract: Digital twins, used to represent physical systems, have been lauded as tools for understanding reality. Complex system behavior is typically captured in domain-specific models crafted by subject experts. Contemporary methods for employing models in a digital twin require prescriptive interfaces, resulting in twins that are difficult to connect, redeploy, and modify. The limited interoperability of these twins has prompted calls for a universal framework enabling observability across model aggregations. Here we show how a new mathematical formalism called a constraint hypergraph serves as such a framework by representing system behavior as the composition of set-based functions. A digital twin is shown to be the second of two coupled systems where both adhere to the same constraint hypergraph, permitting the properties of the first to be observable from the second. Interoperability is given by deconstructing models into a structure enabling autonomous, white-box simulation of system properties. The resulting digital twins can interact immediately with both human and autonomous agents. This is demonstrated in a case study of a microgrid, showing how both measured and simulated data from the aggregated twins can be provided regardless of the operating environment. By connecting models, constraint hypergraphs supply scientists and modelers robust means to capture, communicate, and combine digital twins across all fields of study. We expect this framework to expand the use of digital twins, enriching scientific insights and collaborations by providing a structure for characterizing complex systems.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Research Comparator: A Platform For Fine-grained Human Annotations of Deep Research Agents</title>
<link>https://arxiv.org/abs/2507.05495</link>
<guid>https://arxiv.org/abs/2507.05495</guid>
<content:encoded><![CDATA[
arXiv:2507.05495v1 Announce Type: new 
Abstract: Effectively evaluating deep research agents that autonomously search the web, analyze information, and generate reports remains a major challenge, particularly when it comes to assessing long reports and giving detailed feedback on their intermediate steps. To address these gaps, we introduce Deep Research Comparator, a platform that offers a holistic framework for deep research agent hosting, side-by-side comparison, fine-grained human feedback collection, and ranking calculation. Given a user query, our platform displays the final reports from two different agents along with their intermediate steps during generation. Annotators can evaluate the overall quality of final reports based on side-by-side comparison, and also provide detailed feedback separately by assessing intermediate steps or specific text spans within the final report. Furthermore, we develop Simple Deepresearch, an end-to-end agent scaffold. This scaffold serves as a baseline that facilitates the easy integration of various large language models to transform them into deep research agents for evaluation. To demonstrate the platform's utility for deep research agent development, we have collected real user preference data from 17 annotators on three deep research agents. A demo video of our platform can be found at https://www.youtube.com/watch?v=g4d2dnbdseg.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering Healthcare Practitioners with Language Models: Structuring Speech Transcripts in Two Real-World Clinical Applications</title>
<link>https://arxiv.org/abs/2507.05517</link>
<guid>https://arxiv.org/abs/2507.05517</guid>
<content:encoded><![CDATA[
arXiv:2507.05517v1 Announce Type: new 
Abstract: Large language models (LLMs) such as GPT-4o and o1 have demonstrated strong performance on clinical natural language processing (NLP) tasks across multiple medical benchmarks. Nonetheless, two high-impact NLP tasks - structured tabular reporting from nurse dictations and medical order extraction from doctor-patient consultations - remain underexplored due to data scarcity and sensitivity, despite active industry efforts. Practical solutions to these real-world clinical tasks can significantly reduce the documentation burden on healthcare providers, allowing greater focus on patient care. In this paper, we investigate these two challenging tasks using private and open-source clinical datasets, evaluating the performance of both open- and closed-weight LLMs, and analyzing their respective strengths and limitations. Furthermore, we propose an agentic pipeline for generating realistic, non-sensitive nurse dictations, enabling structured extraction of clinical observations. To support further research in both areas, we release SYNUR and SIMORD, the first open-source datasets for nurse observation extraction and medical order extraction.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cultivating Multimodal Intelligence: Interpretive Reasoning and Agentic RAG Approaches to Dermatological Diagnosis</title>
<link>https://arxiv.org/abs/2507.05520</link>
<guid>https://arxiv.org/abs/2507.05520</guid>
<content:encoded><![CDATA[
arXiv:2507.05520v1 Announce Type: new 
Abstract: The second edition of the 2025 ImageCLEF MEDIQA-MAGIC challenge, co-organized by researchers from Microsoft, Stanford University, and the Hospital Clinic of Barcelona, focuses on multimodal dermatology question answering and segmentation, using real-world patient queries and images. This work addresses the Closed Visual Question Answering (CVQA) task, where the goal is to select the correct answer to multiple-choice clinical questions based on both user-submitted images and accompanying symptom descriptions. The proposed approach combines three core components: (1) fine-tuning open-source multimodal models from the Qwen, Gemma, and LLaMA families on the competition dataset, (2) introducing a structured reasoning layer that reconciles and adjudicates between candidate model outputs, and (3) incorporating agentic retrieval-augmented generation (agentic RAG), which adds relevant information from the American Academy of Dermatology's symptom and condition database to fill in gaps in patient context. The team achieved second place with a submission that scored sixth, demonstrating competitive performance and high accuracy. Beyond competitive benchmarks, this research addresses a practical challenge in telemedicine: diagnostic decisions must often be made asynchronously, with limited input and with high accuracy and interpretability. By emulating the systematic reasoning patterns employed by dermatologists when evaluating skin conditions, this architecture provided a pathway toward more reliable automated diagnostic support systems.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment</title>
<link>https://arxiv.org/abs/2507.05528</link>
<guid>https://arxiv.org/abs/2507.05528</guid>
<content:encoded><![CDATA[
arXiv:2507.05528v1 Announce Type: new 
Abstract: Large language models (LLMs) have advanced virtual educators and learners, bridging NLP with AI4Education. Existing work often lacks scalability and fails to leverage diverse, large-scale course content, with limited frameworks for assessing pedagogic quality. To this end, we propose WikiHowAgent, a multi-agent workflow leveraging LLMs to simulate interactive teaching-learning conversations. It integrates teacher and learner agents, an interaction manager, and an evaluator to facilitate procedural learning and assess pedagogic quality. We introduce a dataset of 114,296 teacher-learner conversations grounded in 14,287 tutorials across 17 domains and 727 topics. Our evaluation protocol combines computational and rubric-based metrics with human judgment alignment. Results demonstrate the workflow's effectiveness in diverse setups, offering insights into LLM capabilities across domains. Our datasets and implementations are fully open-sourced.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolutionary and Coevolutionary Multi-Agent Design Choices and Dynamics</title>
<link>https://arxiv.org/abs/2507.05534</link>
<guid>https://arxiv.org/abs/2507.05534</guid>
<content:encoded><![CDATA[
arXiv:2507.05534v1 Announce Type: new 
Abstract: We investigate two representation alternatives for the controllers of teams of cyber agents. We combine these controller representations with different evolutionary algorithms, one of which introduces a novel LLM-supported mutation operator. Using a cyber security scenario, we evaluate agent learning when one side is trained to compete against a side that does not evolve and when two sides coevolve with each other. This allows us to quantify the relative merits and tradeoffs of representation and algorithm combinations in terms of team performance. Our versions of grammatical evolution algorithms using grammars that allow a controller to be expressed in code-like logic can achieve the best team performance. The scenario also allows us to compare the performance impact and dynamics of coevolution versus evolution under different combinations. Across the algorithms and representations, we observe that coevolution reduces the performance highs and lows of both sides while it induces fluctuations on both sides. In contrast, when only one-side is optimized, performance peaks are higher and is more sustained than when both sides are optimized with coevolution.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Agent Smart Contract Exploit Generation</title>
<link>https://arxiv.org/abs/2507.05558</link>
<guid>https://arxiv.org/abs/2507.05558</guid>
<content:encoded><![CDATA[
arXiv:2507.05558v1 Announce Type: new 
Abstract: We present A1, an agentic execution driven system that transforms any LLM into an end-to-end exploit generator. A1 has no hand-crafted heuristics and provides the agent with six domain-specific tools that enable autonomous vulnerability discovery. The agent can flexibly leverage these tools to understand smart contract behavior, generate exploit strategies, test them on blockchain states, and refine approaches based on execution feedback. All outputs are concretely validated to eliminate false positives.
  The evaluation across 36 real-world vulnerable contracts on Ethereum and Binance Smart Chain demonstrates a 62.96% (17 out of 27) success rate on the VERITE benchmark. Beyond the VERITE dataset, A1 identified 9 additional vulnerable contracts, with 5 cases occurring after the strongest model's training cutoff date. Across all 26 successful cases, A1 extracts up to 8.59 million USD per case and 9.33 million USD total. Through 432 experiments across six LLMs, we analyze iteration-wise performance showing diminishing returns with average marginal gains of +9.7%, +3.7%, +5.1%, and +2.8% for iterations 2-5 respectively, with per-experiment costs ranging $0.01-$3.59. A Monte Carlo analysis of 19 historical attacks shows success probabilities of 85.9%-88.8% without detection delays.
  We investigate whether an attacker or a defender benefits most from deploying A1 as a continuous on-chain scanning system. Our model shows that OpenAI's o3-pro maintains profitability up to a 30.0 days scanning delay at 0.100% vulnerability incidence rates, while faster models require >=1.000% rates to break-even. The findings exposes a troubling asymmetry: at 0.1% vulnerability rates, attackers achieve an on-chain scanning profitability at a $6000 exploit value, while defenders require $60000, raising fundamental questions about whether AI agents inevitably favor exploitation over defense.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preemptive Solving of Future Problems: Multitask Preplay in Humans and Machines</title>
<link>https://arxiv.org/abs/2507.05561</link>
<guid>https://arxiv.org/abs/2507.05561</guid>
<content:encoded><![CDATA[
arXiv:2507.05561v1 Announce Type: new 
Abstract: Humans can pursue a near-infinite variety of tasks, but typically can only pursue a small number at the same time. We hypothesize that humans leverage experience on one task to preemptively learn solutions to other tasks that were accessible but not pursued. We formalize this idea as Multitask Preplay, a novel algorithm that replays experience on one task as the starting point for "preplay" -- counterfactual simulation of an accessible but unpursued task. Preplay is used to learn a predictive representation that can support fast, adaptive task performance later on. We first show that, compared to traditional planning and predictive representation methods, multitask preplay better predicts how humans generalize to tasks that were accessible but not pursued in a small grid-world, even when people didn't know they would need to generalize to these tasks. We then show these predictions generalize to Craftax, a partially observable 2D Minecraft environment. Finally, we show that Multitask Preplay enables artificial agents to learn behaviors that transfer to novel Craftax worlds sharing task co-occurrence structure. These findings demonstrate that Multitask Preplay is a scalable theory of how humans counterfactually learn and generalize across multiple tasks; endowing artificial agents with the same capacity can significantly improve their performance in challenging multitask environments.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting and Mitigating Reward Hacking in Reinforcement Learning Systems: A Comprehensive Empirical Study</title>
<link>https://arxiv.org/abs/2507.05619</link>
<guid>https://arxiv.org/abs/2507.05619</guid>
<content:encoded><![CDATA[
arXiv:2507.05619v1 Announce Type: new 
Abstract: Reward hacking in Reinforcement Learning (RL) systems poses a critical threat to the deployment of autonomous agents, where agents exploit flaws in reward functions to achieve high scores without fulfilling intended objectives. Despite growing awareness of this problem, systematic detection and mitigation approaches remain limited. This paper presents a large-scale empirical study of reward hacking across diverse RL environments and algorithms. We analyze 15,247 training episodes across 15 RL environments (Atari, MuJoCo, custom domains) and 5 algorithms (PPO, SAC, DQN, A3C, Rainbow), implementing automated detection algorithms for six categories of reward hacking: specification gaming, reward tampering, proxy optimization, objective misalignment, exploitation patterns, and wireheading. Our detection framework achieves 78.4% precision and 81.7% recall across environments, with computational overhead under 5%. Through controlled experiments varying reward function properties, we demonstrate that reward density and alignment with true objectives significantly impact hacking frequency ($p < 0.001$, Cohen's $d = 1.24$). We validate our approach through three simulated application studies representing recommendation systems, competitive gaming, and robotic control scenarios. Our mitigation techniques reduce hacking frequency by up to 54.6% in controlled scenarios, though we find these trade-offs are more challenging in practice due to concept drift, false positive costs, and adversarial adaptation. All detection algorithms, datasets, and experimental protocols are publicly available to support reproducible research in RL safety.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Not to Detect Prompt Injections with an LLM</title>
<link>https://arxiv.org/abs/2507.05630</link>
<guid>https://arxiv.org/abs/2507.05630</guid>
<content:encoded><![CDATA[
arXiv:2507.05630v1 Announce Type: new 
Abstract: LLM-integrated applications and agents are vulnerable to prompt injection attacks, in which adversaries embed malicious instructions within seemingly benign user inputs to manipulate the LLM's intended behavior. Recent defenses based on $\textit{known-answer detection}$ (KAD) have achieved near-perfect performance by using an LLM to classify inputs as clean or contaminated. In this work, we formally characterize the KAD framework and uncover a structural vulnerability in its design that invalidates its core security premise. We design a methodical adaptive attack, $\textit{DataFlip}$, to exploit this fundamental weakness. It consistently evades KAD defenses with detection rates as low as $1.5\%$ while reliably inducing malicious behavior with success rates of up to $88\%$, without needing white-box access to the LLM or any optimization procedures.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs are Introvert</title>
<link>https://arxiv.org/abs/2507.05638</link>
<guid>https://arxiv.org/abs/2507.05638</guid>
<content:encoded><![CDATA[
arXiv:2507.05638v1 Announce Type: new 
Abstract: The exponential growth of social media and generative AI has transformed information dissemination, fostering connectivity but also accelerating the spread of misinformation. Understanding information propagation dynamics and developing effective control strategies is essential to mitigate harmful content. Traditional models, such as SIR, provide basic insights but inadequately capture the complexities of online interactions. Advanced methods, including attention mechanisms and graph neural networks, enhance accuracy but typically overlook user psychology and behavioral dynamics. Large language models (LLMs), with their human-like reasoning, offer new potential for simulating psychological aspects of information spread. We introduce an LLM-based simulation environment capturing agents' evolving attitudes, emotions, and responses. Initial experiments, however, revealed significant gaps between LLM-generated behaviors and authentic human dynamics, especially in stance detection and psychological realism. A detailed evaluation through Social Information Processing Theory identified major discrepancies in goal-setting and feedback evaluation, stemming from the lack of emotional processing in standard LLM training. To address these issues, we propose the Social Information Processing-based Chain of Thought (SIP-CoT) mechanism enhanced by emotion-guided memory. This method improves the interpretation of social cues, personalization of goals, and evaluation of feedback. Experimental results confirm that SIP-CoT-enhanced LLM agents more effectively process social information, demonstrating behaviors, attitudes, and emotions closer to real human interactions. In summary, this research highlights critical limitations in current LLM-based propagation simulations and demonstrates how integrating SIP-CoT and emotional memory significantly enhances the social intelligence and realism of LLM agents.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ECom-Bench: Can LLM Agent Resolve Real-World E-commerce Customer Support Issues?</title>
<link>https://arxiv.org/abs/2507.05639</link>
<guid>https://arxiv.org/abs/2507.05639</guid>
<content:encoded><![CDATA[
arXiv:2507.05639v1 Announce Type: new 
Abstract: In this paper, we introduce ECom-Bench, the first benchmark framework for evaluating LLM agent with multimodal capabilities in the e-commerce customer support domain. ECom-Bench features dynamic user simulation based on persona information collected from real e-commerce customer interactions and a realistic task dataset derived from authentic e-commerce dialogues. These tasks, covering a wide range of business scenarios, are designed to reflect real-world complexities, making ECom-Bench highly challenging. For instance, even advanced models like GPT-4o achieve only a 10-20% pass^3 metric in our benchmark, highlighting the substantial difficulties posed by complex e-commerce scenarios. Upon publication, the code and data will be open-sourced to facilitate further research and development in this domain.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R-VLM: Region-Aware Vision Language Model for Precise GUI Grounding</title>
<link>https://arxiv.org/abs/2507.05673</link>
<guid>https://arxiv.org/abs/2507.05673</guid>
<content:encoded><![CDATA[
arXiv:2507.05673v1 Announce Type: new 
Abstract: Visual agent models for automating human activities on Graphical User Interfaces (GUIs) have emerged as a promising research direction, driven by advances in large Vision Language Models (VLMs). A critical challenge in GUI automation is the precise grounding of interface elements across diverse platforms. Existing vision-only GUI agents directly ground elements from large and cluttered screenshots, requiring them to process substantial irrelevant information that compromises their accuracy. In addition, these approaches typically employ basic cross-entropy loss for learning grounding objectives, which fails to effectively capture grounding quality compared to established object detection metrics like Intersection-over-Union (IoU). To address these issues, we introduce R-VLM, a novel GUI grounding approach that leverages zoomed-in region proposals for precise element localization. We also propose an IoU-aware objective function that facilitates model convergence toward high IoU predictions. Our approach bridges the gap between VLMs and conventional object detection techniques, improving the state-of-the-art grounding accuracy by 13% across diverse GUI platforms on the GUI grounding benchmarks ScreenSpot and AgentStudio. In addition, our R-VLM approach shows 3.2-9.7% absolute accuracy improvements in GUI navigation tasks on the AITW and Mind2Web benchmarks.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic-R1: Distilled Dual-Strategy Reasoning</title>
<link>https://arxiv.org/abs/2507.05707</link>
<guid>https://arxiv.org/abs/2507.05707</guid>
<content:encoded><![CDATA[
arXiv:2507.05707v1 Announce Type: new 
Abstract: Current long chain-of-thought (long-CoT) models excel at mathematical reasoning but rely on slow and error-prone natural language traces. Tool-augmented agents address arithmetic via code execution, but often falter on complex logical tasks. We introduce a fine-tuning framework, DualDistill, that distills complementary reasoning strategies from multiple teachers into a unified student model. Using this approach, we train Agentic-R1, which dynamically selects the optimal strategy for each query, invoking tools for arithmetic and algorithmic problems, and using text-based reasoning for abstract ones. Our method improves accuracy across a range of tasks, including both computation-intensive and standard benchmarks, demonstrating the effectiveness of multi-strategy distillation in achieving robust and efficient reasoning. Our project is available at https://github.com/StigLidu/DualDistill
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning in Online Environment</title>
<link>https://arxiv.org/abs/2507.05720</link>
<guid>https://arxiv.org/abs/2507.05720</guid>
<content:encoded><![CDATA[
arXiv:2507.05720v1 Announce Type: new 
Abstract: Recently, there has been a surge of vision-based GUI agents designed to automate everyday mobile and web tasks. These agents interpret raw GUI screenshots and autonomously decide where to click, scroll, or type, which bypasses handcrafted rules and app-specific APIs. However, most existing methods trained GUI agent in the offline environment using pre-collected trajectories. This approach limits scalability, causes overfitting to specific UI templates, and leads to brittle policies when faced with unseen environment. We present MobileGUI-RL, a scalable framework that trains GUI agent in online environment. MobileGUI-RL contains two key components. It (i) synthesizes a curriculum of learnable tasks through self-exploration and filtering, and (ii) adapts GRPO to GUI navigation with trajectory-aware advantages and composite rewards that balance task success and execution efficiency. Experiments on three online mobile-agent benchmarks show consistent gains, validating the effectiveness of our approach.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An autonomous agent for auditing and improving the reliability of clinical AI models</title>
<link>https://arxiv.org/abs/2507.05755</link>
<guid>https://arxiv.org/abs/2507.05755</guid>
<content:encoded><![CDATA[
arXiv:2507.05755v1 Announce Type: new 
Abstract: The deployment of AI models in clinical practice faces a critical challenge: models achieving expert-level performance on benchmarks can fail catastrophically when confronted with real-world variations in medical imaging. Minor shifts in scanner hardware, lighting or demographics can erode accuracy, but currently reliability auditing to identify such catastrophic failure cases before deployment is a bespoke and time-consuming process. Practitioners lack accessible and interpretable tools to expose and repair hidden failure modes. Here we introduce ModelAuditor, a self-reflective agent that converses with users, selects task-specific metrics, and simulates context-dependent, clinically relevant distribution shifts. ModelAuditor then generates interpretable reports explaining how much performance likely degrades during deployment, discussing specific likely failure modes and identifying root causes and mitigation strategies. Our comprehensive evaluation across three real-world clinical scenarios - inter-institutional variation in histopathology, demographic shifts in dermatology, and equipment heterogeneity in chest radiography - demonstrates that ModelAuditor is able correctly identify context-specific failure modes of state-of-the-art models such as the established SIIM-ISIC melanoma classifier. Its targeted recommendations recover 15-25% of performance lost under real-world distribution shift, substantially outperforming both baseline models and state-of-the-art augmentation methods. These improvements are achieved through a multi-agent architecture and execute on consumer hardware in under 10 minutes, costing less than US$0.50 per audit.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GTA1: GUI Test-time Scaling Agent</title>
<link>https://arxiv.org/abs/2507.05791</link>
<guid>https://arxiv.org/abs/2507.05791</guid>
<content:encoded><![CDATA[
arXiv:2507.05791v1 Announce Type: new 
Abstract: Graphical user interface (GUI) agents autonomously operate across platforms (e.g., Linux) to complete tasks by interacting with visual elements. Specifically, a user instruction is decomposed into a sequence of action proposals, each corresponding to an interaction with the GUI. After each action, the agent observes the updated GUI environment to plan the next step. However, two main challenges arise: i) resolving ambiguity in task planning (i.e., the action proposal sequence), where selecting an appropriate plan is non-trivial, as many valid ones may exist; ii) accurately grounding actions in complex and high-resolution interfaces, i.e., precisely interacting with visual targets.
  This paper investigates the two aforementioned challenges with our GUI Test-time Scaling Agent, namely GTA1. First, to select the most appropriate action proposal, we introduce a test-time scaling method. At each step, we sample multiple candidate action proposals and leverage a judge model to evaluate and select the most suitable one. It trades off computation for better decision quality by concurrent sampling, shortening task execution steps, and improving overall performance. Second, we propose a model that achieves improved accuracy when grounding the selected action proposal to its corresponding visual elements. Our key insight is that reinforcement learning (RL) facilitates visual grounding through inherent objective alignments, rewarding successful clicks on interface elements.
  Experimentally, our method establishes state-of-the-art performance across diverse benchmarks. For example, GTA1-7B achieves 50.1%, 92.4%, and 67.7% accuracies on Screenspot-Pro, Screenspot-V2, and OSWorld-G, respectively. When paired with a planner applying our test-time scaling strategy, it exhibits state-of-the-art agentic performance (e.g., 45.2% task success rate on OSWorld). We open-source our code and models here.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constella: Supporting Storywriters' Interconnected Character Creation through LLM-based Multi-Agents</title>
<link>https://arxiv.org/abs/2507.05820</link>
<guid>https://arxiv.org/abs/2507.05820</guid>
<content:encoded><![CDATA[
arXiv:2507.05820v1 Announce Type: new 
Abstract: Creating a cast of characters by attending to their relational dynamics is a critical aspect of most long-form storywriting. However, our formative study (N=14) reveals that writers struggle to envision new characters that could influence existing ones, to balance similarities and differences among characters, and to intricately flesh out their relationships. Based on these observations, we designed Constella, an LLM-based multi-agent tool that supports storywriters' interconnected character creation process. Constella suggests related characters (FRIENDS DISCOVERY feature), reveals the inner mindscapes of several characters simultaneously (JOURNALS feature), and manifests relationships through inter-character responses (COMMENTS feature). Our 7-8 day deployment study with storywriters (N=11) shows that Constella enabled the creation of expansive communities composed of related characters, facilitated the comparison of characters' thoughts and emotions, and deepened writers' understanding of character relationships. We conclude by discussing how multi-agent interactions can help distribute writers' attention and effort across the character cast.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CogniPlay: a work-in-progress Human-like model for General Game Playing</title>
<link>https://arxiv.org/abs/2507.05868</link>
<guid>https://arxiv.org/abs/2507.05868</guid>
<content:encoded><![CDATA[
arXiv:2507.05868v1 Announce Type: new 
Abstract: While AI systems have equaled or surpassed human performance in a wide variety of games such as Chess, Go, or Dota 2, describing these systems as truly "human-like" remains far-fetched. Despite their success, they fail to replicate the pattern-based, intuitive decision-making processes observed in human cognition. This paper presents an overview of findings from cognitive psychology and previous efforts to model human-like behavior in artificial agents, discusses their applicability to General Game Playing (GGP) and introduces our work-in-progress model based on these observations: CogniPlay.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Debate Strategies to Enhance Requirements Engineering with Large Language Models</title>
<link>https://arxiv.org/abs/2507.05981</link>
<guid>https://arxiv.org/abs/2507.05981</guid>
<content:encoded><![CDATA[
arXiv:2507.05981v1 Announce Type: new 
Abstract: Context: Large Language Model (LLM) agents are becoming widely used for various Requirements Engineering (RE) tasks. Research on improving their accuracy mainly focuses on prompt engineering, model fine-tuning, and retrieval augmented generation. However, these methods often treat models as isolated black boxes - relying on single-pass outputs without iterative refinement or collaboration, limiting robustness and adaptability. Objective: We propose that, just as human debates enhance accuracy and reduce bias in RE tasks by incorporating diverse perspectives, different LLM agents debating and collaborating may achieve similar improvements. Our goal is to investigate whether Multi-Agent Debate (MAD) strategies can enhance RE performance. Method: We conducted a systematic study of existing MAD strategies across various domains to identify their key characteristics. To assess their applicability in RE, we implemented and tested a preliminary MAD-based framework for RE classification. Results: Our study identified and categorized several MAD strategies, leading to a taxonomy outlining their core attributes. Our preliminary evaluation demonstrated the feasibility of applying MAD to RE classification. Conclusions: MAD presents a promising approach for improving LLM accuracy in RE tasks. This study provides a foundational understanding of MAD strategies, offering insights for future research and refinements in RE applications.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From General Relation Patterns to Task-Specific Decision-Making in Continual Multi-Agent Coordination</title>
<link>https://arxiv.org/abs/2507.06004</link>
<guid>https://arxiv.org/abs/2507.06004</guid>
<content:encoded><![CDATA[
arXiv:2507.06004v1 Announce Type: new 
Abstract: Continual Multi-Agent Reinforcement Learning (Co-MARL) requires agents to address catastrophic forgetting issues while learning new coordination policies with the dynamics team. In this paper, we delve into the core of Co-MARL, namely Relation Patterns, which refer to agents' general understanding of interactions. In addition to generality, relation patterns exhibit task-specificity when mapped to different action spaces. To this end, we propose a novel method called General Relation Patterns-Guided Task-Specific Decision-Maker (RPG). In RPG, agents extract relation patterns from dynamic observation spaces using a relation capturer. These task-agnostic relation patterns are then mapped to different action spaces via a task-specific decision-maker generated by a conditional hypernetwork. To combat forgetting, we further introduce regularization items on both the relation capturer and the conditional hypernetwork. Results on SMAC and LBF demonstrate that RPG effectively prevents catastrophic forgetting when learning new tasks and achieves zero-shot generalization to unseen tasks.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conditional Multi-Stage Failure Recovery for Embodied Agents</title>
<link>https://arxiv.org/abs/2507.06016</link>
<guid>https://arxiv.org/abs/2507.06016</guid>
<content:encoded><![CDATA[
arXiv:2507.06016v1 Announce Type: new 
Abstract: Embodied agents performing complex tasks are susceptible to execution failures, motivating the need for effective failure recovery mechanisms. In this work, we introduce a conditional multistage failure recovery framework that employs zero-shot chain prompting. The framework is structured into four error-handling stages, with three operating during task execution and one functioning as a post-execution reflection phase. Our approach utilises the reasoning capabilities of LLMs to analyse execution challenges within their environmental context and devise strategic solutions. We evaluate our method on the TfD benchmark of the TEACH dataset and achieve state-of-the-art performance, outperforming a baseline without error recovery by 11.5% and surpassing the strongest existing model by 19%.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Lockean beliefs that are deductively closed and minimal change</title>
<link>https://arxiv.org/abs/2507.06042</link>
<guid>https://arxiv.org/abs/2507.06042</guid>
<content:encoded><![CDATA[
arXiv:2507.06042v1 Announce Type: new 
Abstract: Within the formal setting of the Lockean thesis, an agent belief set is defined in terms of degrees of confidence and these are described in probabilistic terms. This approach is of established interest, notwithstanding some limitations that make its use troublesome in some contexts, like, for instance, in belief change theory. Precisely, Lockean belief sets are not generally closed under (classical) logical deduction. The aim of the present paper is twofold: on one side we provide two characterizations of those belief sets that are closed under classical logic deduction, and on the other we propose an approach to probabilistic update that allows us for a minimal revision of those beliefs, i.e., a revision obtained by making the fewest possible changes to the existing belief set while still accommodating the new information. In particular, we show how we can deductively close a belief set via a minimal revision.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety</title>
<link>https://arxiv.org/abs/2507.06134</link>
<guid>https://arxiv.org/abs/2507.06134</guid>
<content:encoded><![CDATA[
arXiv:2507.06134v1 Announce Type: new 
Abstract: Recent advances in AI agents capable of solving complex, everyday tasks, from scheduling to customer service, have enabled deployment in real-world settings, but their possibilities for unsafe behavior demands rigorous evaluation. While prior benchmarks have attempted to assess agent safety, most fall short by relying on simulated environments, narrow task domains, or unrealistic tool abstractions. We introduce OpenAgentSafety, a comprehensive and modular framework for evaluating agent behavior across eight critical risk categories. Unlike prior work, our framework evaluates agents that interact with real tools, including web browsers, code execution environments, file systems, bash shells, and messaging platforms; and supports over 350 multi-turn, multi-user tasks spanning both benign and adversarial user intents. OpenAgentSafety is designed for extensibility, allowing researchers to add tools, tasks, websites, and adversarial strategies with minimal effort. It combines rule-based analysis with LLM-as-judge assessments to detect both overt and subtle unsafe behaviors. Empirical analysis of five prominent LLMs in agentic scenarios reveals unsafe behavior in 51.2% of safety-vulnerable tasks with Claude-Sonnet-3.7, to 72.7% with o3-mini, highlighting critical safety vulnerabilities and the need for stronger safeguards before real-world deployment.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Habitat Robotics using Large Language Models</title>
<link>https://arxiv.org/abs/2507.06157</link>
<guid>https://arxiv.org/abs/2507.06157</guid>
<content:encoded><![CDATA[
arXiv:2507.06157v1 Announce Type: new 
Abstract: This paper focuses on evaluating the effectiveness of Large Language Models at solving embodied robotic tasks using the Meta PARTNER benchmark. Meta PARTNR provides simplified environments and robotic interactions within randomized indoor kitchen scenes. Each randomized kitchen scene is given a task where two robotic agents cooperatively work together to solve the task. We evaluated multiple frontier models on Meta PARTNER environments. Our results indicate that reasoning models like OpenAI o3-mini outperform non-reasoning models like OpenAI GPT-4o and Llama 3 when operating in PARTNR's robotic embodied environments. o3-mini displayed outperform across centralized, decentralized, full observability, and partial observability configurations. This provides a promising avenue of research for embodied robotic development.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligned Textual Scoring Rules</title>
<link>https://arxiv.org/abs/2507.06221</link>
<guid>https://arxiv.org/abs/2507.06221</guid>
<content:encoded><![CDATA[
arXiv:2507.06221v1 Announce Type: new 
Abstract: Scoring rules elicit probabilistic predictions from a strategic agent by scoring the prediction against a ground truth state. A scoring rule is proper if, from the agent's perspective, reporting the true belief maximizes the expected score. With the development of language models, Wu and Hartline (2024) proposes a reduction from textual information elicitation to the numerical (i.e. probabilistic) information elicitation problem, which achieves provable properness for textual elicitation. However, not all proper scoring rules are well aligned with human preference over text. Our paper designs the Aligned Scoring rule (ASR) for text by optimizing and minimizing the mean squared error between a proper scoring rule and a reference score (e.g. human score). Our experiments show that our ASR outperforms previous methods in aligning with human preference while maintaining properness.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving</title>
<link>https://arxiv.org/abs/2507.06229</link>
<guid>https://arxiv.org/abs/2507.06229</guid>
<content:encoded><![CDATA[
arXiv:2507.06229v1 Announce Type: new 
Abstract: As language agents tackle increasingly complex tasks, they struggle with effective error correction and experience reuse across domains. We introduce Agent KB, a hierarchical experience framework that enables complex agentic problem solving via a novel Reason-Retrieve-Refine pipeline. Agent KB addresses a core limitation: agents traditionally cannot learn from each other's experiences. By capturing both high-level strategies and detailed execution logs, Agent KB creates a shared knowledge base that enables cross-agent knowledge transfer. Evaluated on the GAIA benchmark, Agent KB improves success rates by up to 16.28 percentage points. On the most challenging tasks, Claude-3 improves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on intermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to improve from 41.33% to 53.33%. Our results suggest that Agent KB provides a modular, framework-agnostic infrastructure for enabling agents to learn from past experiences and generalize successful strategies to new tasks.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An efficiency ordering of k-price auctions under complete information</title>
<link>https://arxiv.org/abs/2507.05738</link>
<guid>https://arxiv.org/abs/2507.05738</guid>
<content:encoded><![CDATA[
arXiv:2507.05738v1 Announce Type: cross 
Abstract: We study $k$-price auctions in a complete information environment and characterize all pure-strategy Nash equilibrium outcomes. In a setting with $n$ agents having ordered valuations, we show that any agent, except those with the lowest $k-2$ valuations, can win in equilibrium. As a consequence, worst-case welfare increases monotonically as we go from $k=2$ (second-price auction) to $k=n$ (lowest-price auction), with the first-price auction achieving the highest worst-case welfare.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Just Say Better or Worse: A Human-AI Collaborative Framework for Medical Image Segmentation Without Manual Annotations</title>
<link>https://arxiv.org/abs/2507.05815</link>
<guid>https://arxiv.org/abs/2507.05815</guid>
<content:encoded><![CDATA[
arXiv:2507.05815v1 Announce Type: cross 
Abstract: Manual annotation of medical images is a labor-intensive and time-consuming process, posing a significant bottleneck in the development and deployment of robust medical imaging AI systems. This paper introduces a novel Human-AI collaborative framework for medical image segmentation that substantially reduces the annotation burden by eliminating the need for explicit manual pixel-level labeling. The core innovation lies in a preference learning paradigm, where human experts provide minimal, intuitive feedback -- simply indicating whether an AI-generated segmentation is better or worse than a previous version. The framework comprises four key components: (1) an adaptable foundation model (FM) for feature extraction, (2) label propagation based on feature similarity, (3) a clicking agent that learns from human better-or-worse feedback to decide where to click and with which label, and (4) a multi-round segmentation learning procedure that trains a state-of-the-art segmentation network using pseudo-labels generated by the clicking agent and FM-based label propagation. Experiments on three public datasets demonstrate that the proposed approach achieves competitive segmentation performance using only binary preference feedback, without requiring experts to directly manually annotate the images.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Directed Lazy Random Walk Model to Three-Way Dynamic Matching Problem</title>
<link>https://arxiv.org/abs/2507.06126</link>
<guid>https://arxiv.org/abs/2507.06126</guid>
<content:encoded><![CDATA[
arXiv:2507.06126v1 Announce Type: cross 
Abstract: This paper explores a novel extension of dynamic matching theory by analyzing a three-way matching problem involving agents from three distinct populations, each with two possible types. Unlike traditional static or two-way dynamic models, our setting captures more complex team-formation environments where one agent from each of the three populations must be matched to form a valid team. We consider two preference structures: assortative or homophilic, where agents prefer to be matched with others of the same type, and dis-assortative or heterophilic, where diversity within the team is valued. Agents arrive sequentially and face a trade-off between matching immediately or waiting for a higher quality match in the future albeit with a waiting cost. We construct and analyze the corresponding transition probability matrices for each preference regime and demonstrate the existence and uniqueness of stationary distributions. Our results show that stable and efficient outcomes can arise in dynamic, multi-agent matching environments, offering a deeper understanding of how complex matching processes evolve over time and how they can be effectively managed.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Estimation with Decentralized Control for Quadruple-Tank Process</title>
<link>https://arxiv.org/abs/2304.04763</link>
<guid>https://arxiv.org/abs/2304.04763</guid>
<content:encoded><![CDATA[
arXiv:2304.04763v2 Announce Type: replace 
Abstract: This paper presents a unified modeling, control, and estimation framework for the quadruple-tank process, a benchmark multivariable system that exhibits either minimum phase or nonminimum phase behavior depending on valve flow ratios. A decentralized PI control strategy is employed to regulate water levels, while a distributed state estimation scheme is developed using local Luenberger observers and inter-agent communication. Each observer uses only local output measurements and exchanges information with neighboring nodes over a strongly connected communication graph. To address the limitations of partial observability, the observer design incorporates an observability decomposition and consensus-based coupling that ensures convergence to the true system state. Simulation results validate the effectiveness of the proposed framework, demonstrating accurate state reconstruction and stable closed loop performance under both minimum-phase and nonminimum phase configurations. These results highlight the potential of combining decentralized control with distributed estimation for scalable, networked control of complex multivariable systems.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From LLMs to Actions: Latent Codes as Bridges in Hierarchical Robot Control</title>
<link>https://arxiv.org/abs/2405.04798</link>
<guid>https://arxiv.org/abs/2405.04798</guid>
<content:encoded><![CDATA[
arXiv:2405.04798v3 Announce Type: replace 
Abstract: Hierarchical control for robotics has long been plagued by the need to have a well defined interface layer to communicate between high-level task planners and low-level policies. With the advent of LLMs, language has been emerging as a prospective interface layer. However, this has several limitations. Not all tasks can be decomposed into steps that are easily expressible in natural language (e.g. performing a dance routine). Further, it makes end-to-end finetuning on embodied data challenging due to domain shift and catastrophic forgetting. We introduce our method -- Learnable Latent Codes as Bridges (LCB) -- as an alternate architecture to overcome these limitations. \method~uses a learnable latent code to act as a bridge between LLMs and low-level policies. This enables LLMs to flexibly communicate goals in the task plan without being entirely constrained by language limitations. Additionally, it enables end-to-end finetuning without destroying the embedding space of word tokens learned during pre-training. Through experiments on Language Table and Calvin, two common language based benchmarks for embodied agents, we find that \method~outperforms baselines (including those w/ GPT-4V) that leverage pure language as the interface layer on tasks that require reasoning and multi-step behaviors.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Planning for Multi-UAV Pursuit-Evasion in Unknown Environments Using Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2409.15866</link>
<guid>https://arxiv.org/abs/2409.15866</guid>
<content:encoded><![CDATA[
arXiv:2409.15866v4 Announce Type: replace 
Abstract: Multi-UAV pursuit-evasion, where pursuers aim to capture evaders, poses a key challenge for UAV swarm intelligence. Multi-agent reinforcement learning (MARL) has demonstrated potential in modeling cooperative behaviors, but most RL-based approaches remain constrained to simplified simulations with limited dynamics or fixed scenarios. Previous attempts to deploy RL policy to real-world pursuit-evasion are largely restricted to two-dimensional scenarios, such as ground vehicles or UAVs at fixed altitudes. In this paper, we address multi-UAV pursuit-evasion by considering UAV dynamics and physical constraints. We introduce an evader prediction-enhanced network to tackle partial observability in cooperative strategy learning. Additionally, we propose an adaptive environment generator within MARL training, enabling higher exploration efficiency and better policy generalization across diverse scenarios. Simulations show our method significantly outperforms all baselines in challenging scenarios, generalizing to unseen scenarios with a 100% capture rate. Finally, we derive a feasible policy via a two-stage reward refinement and deploy the policy on real quadrotors in a zero-shot manner. To our knowledge, this is the first work to derive and deploy an RL-based policy using collective thrust and body rates control commands for multi-UAV pursuit-evasion in unknown environments. The open-source code and videos are available at https://sites.google.com/view/pursuit-evasion-rl.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Exception Safety Code Generation with Intermediate Representation Agents Framework</title>
<link>https://arxiv.org/abs/2410.06949</link>
<guid>https://arxiv.org/abs/2410.06949</guid>
<content:encoded><![CDATA[
arXiv:2410.06949v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) often struggle with robust exception handling in generated code, leading to fragile programs that are prone to runtime errors. We propose Seeker, a novel multi-agent framework that enforces exception safety in LLM generated code through an Intermediate Representation (IR) approach. Seeker decomposes exception handling into five specialized agents: Scanner, Detector, Predator, Ranker, and Handler that collaboratively analyze code, detect fragile segments, retrieve best practice exception strategies, and inject robust handling code. We also introduce Common Exception Enumeration (CEE), a comprehensive knowledge base derived from official documentation, technical practices, and real world code, to standardize exception handling strategies. Seeker also incorporates a Deep Retrieval-Augmented Generation (Deep RAG) algorithm to efficiently navigate the exception inheritance hierarchy, cutting down search overhead by 93% while improving accuracy in identifying relevant exceptions. We evaluate Seeker on 15 open source Java projects and multiple benchmarks. Seeker outperforms state of the art baselines, improving exception handling precision by up to 37% and overall code robustness by 38% as measured by expert code review. It significantly closes the gap between LLM and human developers in exception management, achieving a 28% success rate on real world issue fixes (SWE bench) versus 19% by prior methods. Our framework preserves functional correctness of code while proactively handling errors, demonstrating a practical, generalizable solution for safer code generation. In this paper, we discuss the novelty of using intermediate representation and multi-agent collaboration for exception handling, and outline how Seeker can be extended to other programming languages and complex software engineering tasks, aligning LLM-generated code with industrial standard.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adsorb-Agent: Autonomous Identification of Stable Adsorption Configurations via Large Language Model Agent</title>
<link>https://arxiv.org/abs/2410.16658</link>
<guid>https://arxiv.org/abs/2410.16658</guid>
<content:encoded><![CDATA[
arXiv:2410.16658v4 Announce Type: replace 
Abstract: Adsorption energy is a key reactivity descriptor in catalysis. Determining adsorption energy requires evaluating numerous adsorbate-catalyst configurations, making it computationally intensive. Current methods rely on exhaustive sampling, which does not guarantee the identification of the global minimum energy. To address this, we introduce Adsorb-Agent, a Large Language Model (LLM) agent designed to efficiently identify stable adsorption configurations corresponding to the global minimum energy. Adsorb-Agent leverages its built-in knowledge and reasoning to strategically explore configurations, significantly reducing the number of initial setups required while improving energy prediction accuracy. In this study, we also evaluated the performance of different LLMs, including GPT-4o, GPT-4o-mini, Claude-3.7-Sonnet, and DeepSeek-Chat, as the reasoning engine for Adsorb-Agent, with GPT-4o showing the strongest overall performance. Tested on twenty diverse systems, Adsorb-Agent identifies comparable adsorption energies for 84% of cases and achieves lower energies for 35%, particularly excelling in complex systems. It identifies lower energies in 47% of intermetallic systems and 67% of systems with large adsorbates. These findings demonstrate Adsorb-Agent's potential to accelerate catalyst discovery by reducing computational costs and enhancing prediction reliability compared to exhaustive search methods.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle</title>
<link>https://arxiv.org/abs/2411.08324</link>
<guid>https://arxiv.org/abs/2411.08324</guid>
<content:encoded><![CDATA[
arXiv:2411.08324v2 Announce Type: replace 
Abstract: Many existing evaluation benchmarks for Large Language Models (LLMs) quickly become outdated due to the emergence of new models and training data. These benchmarks also fall short in assessing how LLM performance changes over time, as they consist of a static set of questions without a temporal dimension. To address these limitations, we propose using future event prediction as a continuous evaluation method to assess LLMs' temporal generalization and forecasting abilities. Our benchmark, Daily Oracle, automatically generates question-answer (QA) pairs from daily news, challenging LLMs to predict "future" event outcomes. Our findings reveal that as pre-training data becomes outdated, LLM performance degrades over time. While Retrieval Augmented Generation (RAG) has the potential to enhance prediction accuracy, the performance degradation pattern persists, highlighting the need for continuous model updates. Code and data are available at https://agenticlearning.ai/daily-oracle.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An AI Theory of Mind Will Enhance Our Collective Intelligence</title>
<link>https://arxiv.org/abs/2411.09168</link>
<guid>https://arxiv.org/abs/2411.09168</guid>
<content:encoded><![CDATA[
arXiv:2411.09168v2 Announce Type: replace 
Abstract: Collective intelligence plays a central role in many fields, from economics and evolutionary theory to neural networks and eusocial insects, and is also core to work on emergence and self-organisation in complex-systems theory. However, in human collective intelligence there is still much to understand about how specific psychological processes at the individual level give rise to self-organised structures at the social level. Psychological factors have so far played a minor role in collective-intelligence studies because the principles are often general and applicable to agents without sophisticated psychologies. We emphasise, with examples from other complex adaptive systems, the broad applicability of collective-intelligence principles, while noting that mechanisms and time scales differ markedly between cases. We review evidence that flexible collective intelligence in human social settings is improved by a particular cognitive tool: our Theory of Mind. We then hypothesise that AIs equipped with a theory of mind will enhance collective intelligence in ways similar to human contributions. To make this case, we step back from the algorithmic basis of AI psychology and consider the large-scale impact AI can have as agential actors in a 'social ecology' rather than as mere technological tools. We identify several key characteristics of psychologically mediated collective intelligence and show that the development of a Theory of Mind is crucial in distinguishing human social collective intelligence from more general forms. Finally, we illustrate how individuals, human or otherwise, integrate within a collective not by being genetically or algorithmically programmed, but by growing and adapting into the socio-cognitive niche they occupy. AI can likewise inhabit one or multiple such niches, facilitated by a Theory of Mind.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aria-UI: Visual Grounding for GUI Instructions</title>
<link>https://arxiv.org/abs/2412.16256</link>
<guid>https://arxiv.org/abs/2412.16256</guid>
<content:encoded><![CDATA[
arXiv:2412.16256v2 Announce Type: replace 
Abstract: Digital agents for automating tasks across different platforms by directly manipulating the GUIs are increasingly important. For these agents, grounding from language instructions to target elements remains a significant challenge due to reliance on HTML or AXTree inputs. In this paper, we introduce Aria-UI, a large multimodal model specifically designed for GUI grounding. Aria-UI adopts a pure-vision approach, eschewing reliance on auxiliary inputs. To adapt to heterogeneous planning instructions, we propose a scalable data pipeline that synthesizes diverse and high-quality instruction samples for grounding. To handle dynamic contexts in task performing, Aria-UI incorporates textual and text-image interleaved action histories, enabling robust context-aware reasoning for grounding. Aria-UI sets new state-of-the-art results across offline and online agent benchmarks, outperforming both vision-only and AXTree-reliant baselines. We release all training data and model checkpoints to foster further research at https://ariaui.github.io.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agents Are All You Need for LLM Unlearning</title>
<link>https://arxiv.org/abs/2502.00406</link>
<guid>https://arxiv.org/abs/2502.00406</guid>
<content:encoded><![CDATA[
arXiv:2502.00406v2 Announce Type: replace 
Abstract: Information removal or suppression in large language models (LLMs) is a desired functionality, useful in AI regulation, legal compliance, safety, and privacy. LLM unlearning methods aim to remove information on demand from LLMs. Current LLM unlearning methods struggle to balance the unlearning efficacy and utility due to the competing nature of these objectives. Keeping the unlearning process computationally feasible without assuming access to the model weights is an overlooked area. In this work we show that \textit{agents might be all we need for effective and practical inference-time LLM unlearning}. We present the first agentic LLM unlearning (\texttt{ALU}) method, a multi-agent, retrain-free, model-agnostic approach to LLM unlearning that achieves effective unlearning while preserving the utility. Our \texttt{ALU} framework unlearns by involving multiple LLM agents, each designed for a specific step in the unlearning process, without the need to update model weights for any of the agents in the framework. Users can easily request any set of unlearning instances in any sequence, and \texttt{ALU} seamlessly adapts in real time. This is facilitated without requiring any changes in the underlying LLM model. Through extensive experiments on established benchmarks (TOFU, WMDP, WPU) and jailbreaking techniques (many shot, target masking, other languages), we demonstrate that \texttt{ALU} consistently stands out as the most robust inference-time LLM unlearning framework among current state-of-the-art methods while incurring time cost that remains effectively constant regardless of the number of unlearning targets. We further highlight \texttt{ALU}'s superior performance compared to existing methods when evaluated at scale. Specifically, \texttt{ALU} is assessed on up to 1000 unlearning targets, exceeding the evaluation scope of all previously proposed LLM unlearning methods.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VolleyBots: A Testbed for Multi-Drone Volleyball Game Combining Motion Control and Strategic Play</title>
<link>https://arxiv.org/abs/2502.01932</link>
<guid>https://arxiv.org/abs/2502.01932</guid>
<content:encoded><![CDATA[
arXiv:2502.01932v4 Announce Type: replace 
Abstract: Robot sports, characterized by well-defined objectives, explicit rules, and dynamic interactions, present ideal scenarios for demonstrating embodied intelligence. In this paper, we present VolleyBots, a novel robot sports testbed where multiple drones cooperate and compete in the sport of volleyball under physical dynamics. VolleyBots integrates three features within a unified platform: competitive and cooperative gameplay, turn-based interaction structure, and agile 3D maneuvering. Competitive and cooperative gameplay challenges each drone to coordinate with its teammates while anticipating and countering opposing teams' tactics. Turn-based interaction demands precise timing, accurate state prediction, and management of long-horizon temporal dependencies. Agile 3D maneuvering requires rapid accelerations, sharp turns, and precise 3D positioning despite the quadrotor's underactuated dynamics. These intertwined features yield a complex problem combining motion control and strategic play, with no available expert demonstrations. We provide a comprehensive suite of tasks ranging from single-drone drills to multi-drone cooperative and competitive tasks, accompanied by baseline evaluations of representative multi-agent reinforcement learning (MARL) and game-theoretic algorithms. Simulation results show that on-policy reinforcement learning (RL) methods outperform off-policy methods in single-agent tasks, but both approaches struggle in complex tasks that combine motion control and strategic play. We additionally design a hierarchical policy which achieves a 69.5% percent win rate against the strongest baseline in the 3 vs 3 task, underscoring its potential as an effective solution for tackling the complex interplay between low-level control and high-level strategy. The project page is at https://sites.google.com/view/thu-volleybots.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Mean Field Equilibrium Computation in Large-Population LQG Games</title>
<link>https://arxiv.org/abs/2502.19993</link>
<guid>https://arxiv.org/abs/2502.19993</guid>
<content:encoded><![CDATA[
arXiv:2502.19993v2 Announce Type: replace 
Abstract: This paper presents a novel data-driven approach for approximating the $\varepsilon$-Nash equilibrium in continuous-time linear quadratic Gaussian (LQG) games, where multiple agents interact with each other through their dynamics and infinite horizon discounted costs. The core of our method involves solving two algebraic Riccati equations (AREs) and an ordinary differential equation (ODE) using state and input samples collected from agents, eliminating the need for a priori knowledge of their dynamical models. The standard ARE is addressed through an integral reinforcement learning (IRL) technique, while the nonsymmetric ARE and the ODE are resolved by identifying the drift coefficients of the agents' dynamics under general conditions. Moreover, by imposing specific conditions on models, we extend the IRL-based approach to approximately solve the nonsymmetric ARE. Numerical examples are given to demonstrate the effectiveness of the proposed algorithms.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentSafe: Safeguarding Large Language Model-based Multi-agent Systems via Hierarchical Data Management</title>
<link>https://arxiv.org/abs/2503.04392</link>
<guid>https://arxiv.org/abs/2503.04392</guid>
<content:encoded><![CDATA[
arXiv:2503.04392v2 Announce Type: replace 
Abstract: Large Language Model based multi-agent systems are revolutionizing autonomous communication and collaboration, yet they remain vulnerable to security threats like unauthorized access and data breaches. To address this, we introduce AgentSafe, a novel framework that enhances MAS security through hierarchical information management and memory protection. AgentSafe classifies information by security levels, restricting sensitive data access to authorized agents. AgentSafe incorporates two components: ThreatSieve, which secures communication by verifying information authority and preventing impersonation, and HierarCache, an adaptive memory management system that defends against unauthorized access and malicious poisoning, representing the first systematic defense for agent memory. Experiments across various LLMs show that AgentSafe significantly boosts system resilience, achieving defense success rates above 80% under adversarial conditions. Additionally, AgentSafe demonstrates scalability, maintaining robust performance as agent numbers and information complexity grow. Results underscore effectiveness of AgentSafe in securing MAS and its potential for real-world application.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing AI Negotiations: New Theory and Evidence from a Large-Scale Autonomous Negotiations Competition</title>
<link>https://arxiv.org/abs/2503.06416</link>
<guid>https://arxiv.org/abs/2503.06416</guid>
<content:encoded><![CDATA[
arXiv:2503.06416v2 Announce Type: replace 
Abstract: We conducted an International AI Negotiation Competition in which participants designed and refined prompts for AI negotiation agents. We then facilitated over 180,000 negotiations between these agents across multiple scenarios with diverse characteristics and objectives. Our findings revealed that principles from human negotiation theory remain crucial even in AI-AI contexts. Surprisingly, warmth--a traditionally human relationship-building trait--was consistently associated with superior outcomes across all key performance metrics. Dominant agents, meanwhile, were especially effective at claiming value. Our analysis also revealed unique dynamics in AI-AI negotiations not fully explained by existing theory, including AI-specific technical strategies like chain-of-thought reasoning, prompt injection, and strategic concealment. When we applied natural language processing (NLP) methods to the full transcripts of all negotiations we found positivity, gratitude and question-asking (associated with warmth) were strongly associated with reaching deals as well as objective and subjective value, whereas conversation lengths (associated with dominance) were strongly associated with impasses. The results suggest the need to establish a new theory of AI negotiation, which integrates classic negotiation theory with AI-specific negotiation theories to better understand autonomous negotiations and optimize agent performance.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Cascading Cooperative Multi-agent Framework for On-ramp Merging Control Integrating Large Language Models</title>
<link>https://arxiv.org/abs/2503.08199</link>
<guid>https://arxiv.org/abs/2503.08199</guid>
<content:encoded><![CDATA[
arXiv:2503.08199v2 Announce Type: replace 
Abstract: Traditional Reinforcement Learning (RL) suffers from replicating human-like behaviors, generalizing effectively in multi-agent scenarios, and overcoming inherent interpretability issues.These tasks are compounded when deep environment understanding, agent coordination and dynamic optimization are required. While Large Language Model (LLM) enhanced methods have shown promise in generalization and interoperability, they often neglect necessary multi-agent coordination. Therefore, we introduce the Cascading Cooperative Multi-agent (CCMA) framework, integrating RL for individual interactions, a fine-tuned LLM for regional cooperation, a reward function for global optimization, and the Retrieval-augmented Generation mechanism to dynamically optimize decision-making across complex driving scenarios. Our experiments demonstrate that the CCMA outperforms existing RL methods, demonstrating significant improvements in both micro and macro-level performance in complex driving environments.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributionally Robust Predictive Runtime Verification under Spatio-Temporal Logic Specifications</title>
<link>https://arxiv.org/abs/2504.02964</link>
<guid>https://arxiv.org/abs/2504.02964</guid>
<content:encoded><![CDATA[
arXiv:2504.02964v2 Announce Type: replace 
Abstract: Cyber-physical systems (CPS) designed in simulators, often consisting of multiple interacting agents (e.g. in multi-agent formations), behave differently in the real-world. We want to verify these systems during runtime when they are deployed. We thus propose robust predictive runtime verification (RPRV) algorithms for: (1) general stochastic CPS under signal temporal logic (STL) tasks, and (2) stochastic multi-agent systems (MAS) under spatio-temporal logic tasks. The RPRV problem presents the following challenges: (1) there may not be sufficient data on the behavior of the deployed CPS, (2) predictive models based on design phase system trajectories may encounter distribution shift during real-world deployment, and (3) the algorithms need to scale to the complexity of MAS and be applicable to spatio-temporal logic tasks. To address the challenges, we assume knowledge of an upper bound on the statistical distance between the trajectory distributions of the system at deployment and design time. We are motivated by our prior work [1, 2] where we proposed an accurate and an interpretable RPRV algorithm for general CPS, which we here extend to the MAS setting and spatio-temporal logic tasks. Specifically, we use a learned predictive model to estimate the system behavior at runtime and robust conformal prediction to obtain probabilistic guarantees by accounting for distribution shifts. Building on [1], we perform robust conformal prediction over the robust semantics of spatio-temporal reach and escape logic (STREL) to obtain centralized RPRV algorithms for MAS. We empirically validate our results in a drone swarm simulator, where we show the scalability of our RPRV algorithms to MAS and analyze the impact of different trajectory predictors on the verification result. To the best of our knowledge, these are the first statistically valid algorithms for MAS under distribution shift.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GC-GAT: Multimodal Vehicular Trajectory Prediction using Graph Goal Conditioning and Cross-context Attention</title>
<link>https://arxiv.org/abs/2504.11150</link>
<guid>https://arxiv.org/abs/2504.11150</guid>
<content:encoded><![CDATA[
arXiv:2504.11150v2 Announce Type: replace 
Abstract: Predicting future trajectories of surrounding vehicles heavily relies on what contextual information is given to a motion prediction model. The context itself can be static (lanes, regulatory elements, etc) or dynamic (traffic participants). This paper presents a lane graph-based motion prediction model that first predicts graph-based goal proposals and later fuses them with cross attention over multiple contextual elements. We follow the famous encoder-interactor-decoder architecture where the encoder encodes scene context using lightweight Gated Recurrent Units, the interactor applies cross-context attention over encoded scene features and graph goal proposals, and the decoder regresses multimodal trajectories via Laplacian Mixture Density Network from the aggregated encodings. Using cross-attention over graph-based goal proposals gives robust trajectory estimates since the model learns to attend to future goal-relevant scene elements for the intended agent. We evaluate our work on nuScenes motion prediction dataset, achieving state-of-the-art results.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational OOD State Correction for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.00503</link>
<guid>https://arxiv.org/abs/2505.00503</guid>
<content:encoded><![CDATA[
arXiv:2505.00503v3 Announce Type: replace 
Abstract: The performance of Offline reinforcement learning is significantly impacted by the issue of state distributional shift, and out-of-distribution (OOD) state correction is a popular approach to address this problem. In this paper, we propose a novel method named Density-Aware Safety Perception (DASP) for OOD state correction. Specifically, our method encourages the agent to prioritize actions that lead to outcomes with higher data density, thereby promoting its operation within or the return to in-distribution (safe) regions. To achieve this, we optimize the objective within a variational framework that concurrently considers both the potential outcomes of decision-making and their density, thus providing crucial contextual information for safe decision-making. Finally, we validate the effectiveness and feasibility of our proposed method through extensive experimental evaluations on the offline MuJoCo and AntMaze suites.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.04317</link>
<guid>https://arxiv.org/abs/2505.04317</guid>
<content:encoded><![CDATA[
arXiv:2505.04317v3 Announce Type: replace 
Abstract: In this paper, we tackle the problem of learning to play 3v3 multi-drone volleyball, a new embodied competitive task that requires both high-level strategic coordination and low-level agile control. The task is turn-based, multi-agent, and physically grounded, posing significant challenges due to its long-horizon dependencies, tight inter-agent coupling, and the underactuated dynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play (HCSP), a hierarchical reinforcement learning framework that separates centralized high-level strategic decision-making from decentralized low-level motion control. We design a three-stage population-based training pipeline to enable both strategy and skill to emerge from scratch without expert demonstrations: (I) training diverse low-level skills, (II) learning high-level strategy via self-play with fixed low-level controllers, and (III) joint fine-tuning through co-self-play. Experiments show that HCSP achieves superior performance, outperforming non-hierarchical self-play and rule-based hierarchical baselines with an average 82.9% win rate and a 71.5% win rate against the two-stage variant. Moreover, co-self-play leads to emergent team behaviors such as role switching and coordinated formations, demonstrating the effectiveness of our hierarchical design and training scheme. The project page is at https://sites.google.com/view/hi-co-self-play.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research Insights</title>
<link>https://arxiv.org/abs/2505.04649</link>
<guid>https://arxiv.org/abs/2505.04649</guid>
<content:encoded><![CDATA[
arXiv:2505.04649v2 Announce Type: replace 
Abstract: The automation of scientific research through large language models (LLMs) presents significant opportunities but faces critical challenges in knowledge synthesis and quality assurance. We introduce Feedback-Refined Agent Methodology (FRAME), a novel framework that enhances medical paper generation through iterative refinement and structured feedback. Our approach comprises three key innovations: (1) A structured dataset construction method that decomposes 4,287 medical papers into essential research components through iterative refinement; (2) A tripartite architecture integrating Generator, Evaluator, and Reflector agents that progressively improve content quality through metric-driven feedback; and (3) A comprehensive evaluation framework that combines statistical metrics with human-grounded benchmarks. Experimental results demonstrate FRAME's effectiveness, achieving significant improvements over conventional approaches across multiple models (9.91% average gain with DeepSeek V3, comparable improvements with GPT-4o Mini) and evaluation dimensions. Human evaluation confirms that FRAME-generated papers achieve quality comparable to human-authored works, with particular strength in synthesizing future research directions. The results demonstrated our work could efficiently assist medical research by building a robust foundation for automated medical research paper generation while maintaining rigorous academic standards.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiBayES: A Hierarchical Bayesian Modeling Framework for AI Evaluation Statistics</title>
<link>https://arxiv.org/abs/2505.05602</link>
<guid>https://arxiv.org/abs/2505.05602</guid>
<content:encoded><![CDATA[
arXiv:2505.05602v2 Announce Type: replace 
Abstract: As Large Language Models (LLMs) and other AI systems evolve, robustly estimating their capabilities from inherently stochastic outputs while systematically quantifying uncertainty in these estimates becomes increasingly important. Further, advanced AI evaluations often have a nested hierarchical structure, exhibit high levels of complexity, and come with high costs in testing the most advanced AI systems. To address these challenges, we introduce HiBayES, a generalizable Hierarchical Bayesian modeling framework for AI Evaluation Statistics. HiBayES supports robust inferences in classical question-answer benchmarks and advanced agentic evaluations, particularly in low-data scenarios (e.g., < 20 data points per evaluation). Built on Generalized Linear Models (GLMs), Bayesian data analysis, and formal model comparison, HiBayES provides principled uncertainty quantification and robust parameter estimation. This paper offers a comprehensive introduction to HiBayES, including illustrative examples, comparisons to conventional statistical methods, and practical guidance for implementing multilevel Bayesian GLMs. Additionally, we provide a HiBayES software package [4] (Beta version) for out-of-the-box implementation.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Complexity of Pure Strategy Relevant Equilibria in Concurrent Games</title>
<link>https://arxiv.org/abs/2505.07501</link>
<guid>https://arxiv.org/abs/2505.07501</guid>
<content:encoded><![CDATA[
arXiv:2505.07501v3 Announce Type: replace 
Abstract: We study rational synthesis problems for concurrent games with $\omega$-regular objectives. Our model of rationality considers only pure strategy Nash equilibria that satisfy either a social welfare or Pareto optimality condition with respect to an $\omega$-regular objective for each agent. This extends earlier work on equilibria in concurrent games, without consideration about their quality. Our results show that the existence of Nash equilibria satisfying social welfare conditions can be computed as efficiently as the constrained Nash equilibrium existence problem. On the other hand, the existence of Nash equilibria satisfying the Pareto optimality condition possibly involves a higher upper bound, except in the case of B\"uchi and Muller games, for which all three problems are in the classes P and PSPACE-complete, respectively.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-tuning Diffusion Policies with Backpropagation Through Diffusion Timesteps</title>
<link>https://arxiv.org/abs/2505.10482</link>
<guid>https://arxiv.org/abs/2505.10482</guid>
<content:encoded><![CDATA[
arXiv:2505.10482v3 Announce Type: replace 
Abstract: Diffusion policies, widely adopted in decision-making scenarios such as robotics, gaming and autonomous driving, are capable of learning diverse skills from demonstration data due to their high representation power. However, the sub-optimal and limited coverage of demonstration data could lead to diffusion policies that generate sub-optimal trajectories and even catastrophic failures. While reinforcement learning (RL)-based fine-tuning has emerged as a promising solution to address these limitations, existing approaches struggle to effectively adapt Proximal Policy Optimization (PPO) to diffusion models. This challenge stems from the computational intractability of action likelihood estimation during the denoising process, which leads to complicated optimization objectives. In our experiments starting from randomly initialized policies, we find that online tuning of Diffusion Policies demonstrates much lower sample efficiency compared to directly applying PPO on MLP policies (MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework that reformulates Diffusion Policy as a noise-conditioned deterministic policy. By treating each denoising step as a differentiable transformation conditioned on pre-sampled noise, NCDPO enables tractable likelihood evaluation and gradient backpropagation through all diffusion timesteps. Our experiments demonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when training from scratch, outperforming existing methods in both sample efficiency and final performance across diverse benchmarks, including continuous robot control and multi-agent game scenarios. Furthermore, our experimental results show that our method is robust to the number denoising timesteps in the Diffusion Policy.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Origin-Destination Pattern Effects on Large-Scale Mixed Traffic Control via Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.13543</link>
<guid>https://arxiv.org/abs/2505.13543</guid>
<content:encoded><![CDATA[
arXiv:2505.13543v2 Announce Type: replace 
Abstract: Traffic congestion remains a major challenge for modern urban transportation, diminishing both efficiency and quality of life. While autonomous driving technologies and reinforcement learning (RL) have shown promise for improving traffic control, most prior work has focused on small-scale networks or isolated intersections. Large-scale mixed traffic control, involving both human-driven and robotic vehicles, remains underexplored. In this study, we propose a decentralized multi-agent reinforcement learning framework for managing large-scale mixed traffic networks, where intersections are controlled either by traditional traffic signals or by robotic vehicles. We evaluate our approach on a real-world network of 14 intersections in Colorado Springs, Colorado, USA, using average vehicle waiting time as the primary measure of traffic efficiency. We are exploring a problem that has not been sufficiently addressed: Is large-scale Multi-Agent Traffic Control (MTC) still feasible when facing time-varying Origin-Destination (OD) patterns?
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Budget-Feasible Mechanism Design with Predictions</title>
<link>https://arxiv.org/abs/2505.24624</link>
<guid>https://arxiv.org/abs/2505.24624</guid>
<content:encoded><![CDATA[
arXiv:2505.24624v2 Announce Type: replace 
Abstract: Augmenting the input of algorithms with predictions is an algorithm design paradigm that suggests leveraging a (possibly erroneous) prediction to improve worst-case performance guarantees when the prediction is perfect (consistency), while also providing a performance guarantee when the prediction fails (robustness). Recently, Xu and Lu [2022] and Agrawal et al. [2024] proposed to consider settings with strategic agents under this framework. In this paper, we initiate the study of budget-feasible mechanism design with predictions. These mechanisms model a procurement auction scenario in which an auctioneer (buyer) with a strict budget constraint seeks to purchase goods or services from a set of strategic agents, so as to maximize her own valuation function. We focus on the online version of the problem where the arrival order of agents is random. We design mechanisms that are truthful, budget-feasible, and achieve a significantly improved competitive ratio for both monotone and non-monotone submodular valuation functions compared to their state-of-the-art counterparts without predictions. Our results assume access to a prediction for the value of the optimal solution to the offline problem. We complement our positive results by showing that for the offline version of the problem, access to predictions is mostly ineffective in improving approximation guarantees.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-agent contract design with a budget</title>
<link>https://arxiv.org/abs/2402.15890</link>
<guid>https://arxiv.org/abs/2402.15890</guid>
<content:encoded><![CDATA[
arXiv:2402.15890v3 Announce Type: replace-cross 
Abstract: We study a multi-agent contract design problem with moral hazard. In our model, each agent exerts costly effort towards an individual task at which it may either succeed or fail, and the principal, who wishes to encourage effort, has an exclusive-use budget that it can use to reward the agents. A motivating application is crowdsourcing for innovation, where a fixed budget is provided to a crowdsourcing platform to use for rewarding participants based on their submissions. Our main contribution is to introduce a novel class of contracts, which we call Luce contracts, and show that there is always a Luce contract that is optimal. A (generic) Luce contract assigns weights to the agents and distributes the entire budget among the successful agents in proportion to their weights. Furthermore, we characterize effort profiles that can be implemented by Luce contracts and show that Luce contracts offer a way to mitigate the uncertainty in total payments compared to alternative contracts-such as piece-rate or bonus-pool contracts-suggesting their desirability even in environments without budget constraints.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic 3D Scene Generation with Spatially Contextualized VLMs</title>
<link>https://arxiv.org/abs/2505.20129</link>
<guid>https://arxiv.org/abs/2505.20129</guid>
<content:encoded><![CDATA[
arXiv:2505.20129v3 Announce Type: replace 
Abstract: Despite recent advances in multimodal content generation enabled by vision-language models (VLMs), their ability to reason about and generate structured 3D scenes remains largely underexplored. This limitation constrains their utility in spatially grounded tasks such as embodied AI, immersive simulations, and interactive 3D applications. We introduce a new paradigm that enables VLMs to generate, understand, and edit complex 3D environments by injecting a continually evolving spatial context. Constructed from multimodal input, this context consists of three components: a scene portrait that provides a high-level semantic blueprint, a semantically labeled point cloud capturing object-level geometry, and a scene hypergraph that encodes rich spatial relationships, including unary, binary, and higher-order constraints. Together, these components provide the VLM with a structured, geometry-aware working memory that integrates its inherent multimodal reasoning capabilities with structured 3D understanding for effective spatial reasoning. Building on this foundation, we develop an agentic 3D scene generation pipeline in which the VLM iteratively reads from and updates the spatial context. The pipeline features high-quality asset generation with geometric restoration, environment setup with automatic verification, and ergonomic adjustment guided by the scene hypergraph. Experiments show that our framework can handle diverse and challenging inputs, achieving a level of generalization not observed in prior work. Further results demonstrate that injecting spatial context enables VLMs to perform downstream tasks such as interactive scene editing and path planning, suggesting strong potential for spatially intelligent systems in computer graphics, 3D vision, and embodied applications. Project page: https://spatctxvlm.github.io/project_page/.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-World En Call Center Transcripts Dataset with PII Redaction</title>
<link>https://arxiv.org/abs/2507.02958</link>
<guid>https://arxiv.org/abs/2507.02958</guid>
<content:encoded><![CDATA[
arXiv:2507.02958v1 Announce Type: new 
Abstract: We introduce CallCenterEN, a large-scale (91,706 conversations, corresponding to 10448 audio hours), real-world English call center transcript dataset designed to support research and development in customer support and sales AI systems. This is the largest release to-date of open source call center transcript data of this kind. The dataset includes inbound and outbound calls between agents and customers, with accents from India, the Philippines and the United States. The dataset includes high-quality, PII-redacted human-readable transcriptions. All personally identifiable information (PII) has been rigorously removed to ensure compliance with global data protection laws. The audio is not included in the public release due to biometric privacy concerns. Given the scarcity of publicly available real-world call center datasets, CallCenterEN fills a critical gap in the landscape of available ASR corpora, and is released under a CC BY-NC 4.0 license for non-commercial research use.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Automated Cybersecurity Penetration Testing</title>
<link>https://arxiv.org/abs/2507.02969</link>
<guid>https://arxiv.org/abs/2507.02969</guid>
<content:encoded><![CDATA[
arXiv:2507.02969v1 Announce Type: new 
Abstract: This paper aims to provide an innovative machine learning-based solution to automate security testing tasks for web applications, ensuring the correct functioning of all components while reducing project maintenance costs. Reinforcement Learning is proposed to select and prioritize tools and optimize the testing path. The presented approach utilizes a simulated webpage along with its network topology to train the agent. Additionally, the model leverages Geometric Deep Learning to create priors that reduce the search space and improve learning convergence. The validation and testing process was conducted on real-world vulnerable web pages commonly used by human hackers for learning. As a result of this study, a reinforcement learning algorithm was developed that maximizes the number of vulnerabilities found while minimizing the number of steps required
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are AI-Generated Fixes Secure? Analyzing LLM and Agent Patches on SWE-bench</title>
<link>https://arxiv.org/abs/2507.02976</link>
<guid>https://arxiv.org/abs/2507.02976</guid>
<content:encoded><![CDATA[
arXiv:2507.02976v1 Announce Type: new 
Abstract: Large Language Models (LLMs) and their agentic frameworks are increasingly adopted to automate software development tasks such as issue resolution and program repair. While prior work has identified security risks in LLM-generated code, most evaluations have focused on synthetic or isolated settings, leaving open questions about the security of these systems in real-world development contexts. In this study, we present the first large-scale security analysis of LLM-generated patches using 20,000+ issues from the SWE-bench dataset. We evaluate patches produced by a standalone LLM (Llama 3.3) and compare them to developer-written patches. We also assess the security of patches generated by three top-performing agentic frameworks (OpenHands, AutoCodeRover, HoneyComb) on a subset of our data. Finally, we analyze a wide range of code, issue, and project-level factors to understand the conditions under which LLMs and agents are most likely to generate insecure code. Our findings reveal that the standalone LLM introduces nearly 9x more new vulnerabilities than developers, with many of these exhibiting unique patterns not found in developers' code. Agentic workflows also generate a significant number of vulnerabilities, particularly when granting LLMs more autonomy, potentially increasing the likelihood of misinterpreting project context or task requirements. We find that vulnerabilities are more likely to occur in LLM patches associated with a higher number of files, more lines of generated code, and GitHub issues that lack specific code snippets or information about the expected code behavior and steps to reproduce. These results suggest that contextual factors play a critical role in the security of the generated code and point toward the need for proactive risk assessment methods that account for both code and issue-level information to complement existing vulnerability detection tools.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAF-Guard: An Agentic Framework for Risk Management and Governance in Large Language Models</title>
<link>https://arxiv.org/abs/2507.02986</link>
<guid>https://arxiv.org/abs/2507.02986</guid>
<content:encoded><![CDATA[
arXiv:2507.02986v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) continue to be increasingly applied across various domains, their widespread adoption necessitates rigorous monitoring to prevent unintended negative consequences and ensure robustness. Furthermore, LLMs must be designed to align with human values, like preventing harmful content and ensuring responsible usage. The current automated systems and solutions for monitoring LLMs in production are primarily centered on LLM-specific concerns like hallucination etc, with little consideration given to the requirements of specific use-cases and user preferences. This paper introduces GAF-Guard, a novel agentic framework for LLM governance that places the user, the use-case, and the model itself at the center. The framework is designed to detect and monitor risks associated with the deployment of LLM based applications. The approach models autonomous agents that identify risks, activate risk detection tools, within specific use-cases and facilitate continuous monitoring and reporting to enhance AI safety, and user expectations. The code is available at https://github.com/IBM/risk-atlas-nexus-demos/tree/main/gaf-guard.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenTable-R1: A Reinforcement Learning Augmented Tool Agent for Open-Domain Table Question Answering</title>
<link>https://arxiv.org/abs/2507.03018</link>
<guid>https://arxiv.org/abs/2507.03018</guid>
<content:encoded><![CDATA[
arXiv:2507.03018v1 Announce Type: new 
Abstract: Open-domain table question answering traditionally relies on a two-stage pipeline: static table retrieval followed by a closed-domain answer. In contrast, we propose an end-to-end agentic framework that embeds multi-turn tool calls-using a BM25+-based search API and a SQLite SQL executor-directly into a large language model. To further adapt a compact 4B-parameter model, we introduce a two-stage fine-tuning process: supervised cold-start on easy questions, then Async GRPO reinforcement learning on harder cases with LoRA adapters and a rollout buffer. This unified approach enables the model to jointly retrieve, reason, and execute queries, yielding a dramatic accuracy improvement from single-digit zero-shot performance to over 0.86 exact match on a held-out test set. Our results underscore the effectiveness of integrating structured tool calls with targeted RL fine-tuning for scalable, accurate table QA. The code is available at https://github.com/TabibitoQZP/OpenTableR1.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Adaptive Transfer Network: Enhancing Transfer Learning in Reinforcement Learning Across Domains</title>
<link>https://arxiv.org/abs/2507.03026</link>
<guid>https://arxiv.org/abs/2507.03026</guid>
<content:encoded><![CDATA[
arXiv:2507.03026v1 Announce Type: new 
Abstract: Transfer learning in Reinforcement Learning (RL) enables agents to leverage knowledge from source tasks to accelerate learning in target tasks. While prior work, such as the Attend, Adapt, and Transfer (A2T) framework, addresses negative transfer and selective transfer, other critical challenges remain underexplored. This paper introduces the Generalized Adaptive Transfer Network (GATN), a deep RL architecture designed to tackle task generalization across domains, robustness to environmental changes, and computational efficiency in transfer. GATN employs a domain-agnostic representation module, a robustness-aware policy adapter, and an efficient transfer scheduler to achieve these goals. We evaluate GATN on diverse benchmarks, including Atari 2600, MuJoCo, and a custom chatbot dialogue environment, demonstrating superior performance in cross-domain generalization, resilience to dynamic environments, and reduced computational overhead compared to baselines. Our findings suggest GATN is a versatile framework for real-world RL applications, such as adaptive chatbots and robotic control.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Turing to Tomorrow: The UK's Approach to AI Regulation</title>
<link>https://arxiv.org/abs/2507.03050</link>
<guid>https://arxiv.org/abs/2507.03050</guid>
<content:encoded><![CDATA[
arXiv:2507.03050v1 Announce Type: new 
Abstract: The UK has pursued a distinctive path in AI regulation: less cautious than the EU but more willing to address risks than the US, and has emerged as a global leader in coordinating AI safety efforts. Impressive developments from companies like London-based DeepMind began to spark concerns in the UK about catastrophic risks from around 2012, although regulatory discussion at the time focussed on bias and discrimination. By 2022, these discussions had evolved into a "pro-innovation" strategy, in which the government directed existing regulators to take a light-touch approach, governing AI at point of use, but avoided regulating the technology or infrastructure directly. ChatGPT arrived in late 2022, galvanising concerns that this approach may be insufficient. The UK responded by establishing an AI Safety Institute to monitor risks and hosting the first international AI Safety Summit in 2023, but - unlike the EU - refrained from regulating frontier AI development in addition to its use. A new government was elected in 2024 which promised to address this gap, but at the time of writing is yet to do so.
  What should the UK do next? The government faces competing objectives: harnessing AI for economic growth and better public services while mitigating risk. In light of these, we propose establishing a flexible, principles-based regulator to oversee the most advanced AI development, defensive measures against risks from AI-enabled biological design tools, and argue that more technical work is needed to understand how to respond to AI-generated misinformation. We argue for updated legal frameworks on copyright, discrimination, and AI agents, and that regulators will have a limited but important role if AI substantially disrupts labour markets.
  If the UK gets AI regulation right, it could demonstrate how democratic societies can harness AI's benefits while managing its risks.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLVER: Reinforcement Learning with Verifiable Emotion Rewards for Empathetic Agents</title>
<link>https://arxiv.org/abs/2507.03112</link>
<guid>https://arxiv.org/abs/2507.03112</guid>
<content:encoded><![CDATA[
arXiv:2507.03112v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at logical and algorithmic reasoning, yet their emotional intelligence (EQ) still lags far behind their cognitive prowess. While reinforcement learning from verifiable rewards (RLVR) has advanced in other domains, its application to dialogue-especially for emotional intelligence-remains underexplored. In this work, we introduce RLVER, the first end-to-end reinforcement learning framework that leverages verifiable emotion rewards from simulated users to cultivate higher-order empathetic abilities in LLMs. Within this framework, self-consistent affective simulated users engage in dialogue rollouts and produce deterministic emotion scores during conversations, serving as reward signals to guide the LLM's learning. Fine-tuning publicly available Qwen2.5-7B-Instruct model with PPO boosts its Sentient-Benchmark score from 13.3 to 79.2 while largely preserving mathematical and coding competence. Extensive experiments reveal that: (i) RLVER consistently improves multiple dialogue capabilities; (ii) Thinking and non-thinking models show distinct trends--thinking models excel in empathy and insight, while non-thinking models favor action; (iii) GRPO often yields stable gains, while PPO can push certain capabilities to a higher ceiling; (iv) More challenging environments are not always better-moderate ones can yield stronger outcomes. Our results show that RLVER is a practical route toward emotionally intelligent and broadly capable language agents.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Last-Iterate Convergence of No-Regret Learning for Equilibria in Bargaining Games</title>
<link>https://arxiv.org/abs/2507.03150</link>
<guid>https://arxiv.org/abs/2507.03150</guid>
<content:encoded><![CDATA[
arXiv:2507.03150v1 Announce Type: new 
Abstract: Bargaining games, where agents attempt to agree on how to split utility, are an important class of games used to study economic behavior, which motivates a study of online learning algorithms in these games. In this work, we tackle when no-regret learning algorithms converge to Nash equilibria in bargaining games. Recent results have shown that online algorithms related to Follow the Regularized Leader (FTRL) converge to Nash equilibria (NE) in the last iterate in a wide variety of games, including zero-sum games. However, bargaining games do not have the properties used previously to established convergence guarantees, even in the simplest case of the ultimatum game, which features a single take-it-or-leave-it offer. Nonetheless, we establish that FTRL (without the modifications necessary for zero-sum games) achieves last-iterate convergence to an approximate NE in the ultimatum game along with a bound on convergence time under mild assumptions. Further, we provide experimental results to demonstrate that convergence to NE, including NE with asymmetric payoffs, occurs under a broad range of initial conditions, both in the ultimatum game and in bargaining games with multiple rounds. This work demonstrates how complex economic behavior (e.g. learning to use threats and the existence of many possible equilibrium outcomes) can result from using a simple learning algorithm, and that FTRL can converge to equilibria in a more diverse set of games than previously known.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SI-Agent: An Agentic Framework for Feedback-Driven Generation and Tuning of Human-Readable System Instructions for Large Language Models</title>
<link>https://arxiv.org/abs/2507.03223</link>
<guid>https://arxiv.org/abs/2507.03223</guid>
<content:encoded><![CDATA[
arXiv:2507.03223v1 Announce Type: new 
Abstract: System Instructions (SIs), or system prompts, are pivotal for guiding Large Language Models (LLMs) but manual crafting is resource-intensive and often suboptimal. Existing automated methods frequently generate non-human-readable "soft prompts," sacrificing interpretability. This paper introduces SI-Agent, a novel agentic framework designed to automatically generate and iteratively refine human-readable SIs through a feedback-driven loop. SI-Agent employs three collaborating agents: an Instructor Agent, an Instruction Follower Agent (target LLM), and a Feedback/Reward Agent evaluating task performance and optionally SI readability. The framework utilizes iterative cycles where feedback guides the Instructor's refinement strategy (e.g., LLM-based editing, evolutionary algorithms). We detail the framework's architecture, agent roles, the iterative refinement process, and contrast it with existing methods. We present experimental results validating SI-Agent's effectiveness, focusing on metrics for task performance, SI readability, and efficiency. Our findings indicate that SI-Agent generates effective, readable SIs, offering a favorable trade-off between performance and interpretability compared to baselines. Potential implications include democratizing LLM customization and enhancing model transparency. Challenges related to computational cost and feedback reliability are acknowledged.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeAgents: A Token-Efficient Framework for Codified Multi-Agent Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2507.03254</link>
<guid>https://arxiv.org/abs/2507.03254</guid>
<content:encoded><![CDATA[
arXiv:2507.03254v1 Announce Type: new 
Abstract: Effective prompt design is essential for improving the planning capabilities of large language model (LLM)-driven agents. However, existing structured prompting strategies are typically limited to single-agent, plan-only settings, and often evaluate performance solely based on task accuracy - overlooking critical factors such as token efficiency, modularity, and scalability in multi-agent environments. To address these limitations, we introduce CodeAgents, a prompting framework that codifies multi-agent reasoning and enables structured, token-efficient planning in multi-agent systems. In CodeAgents, all components of agent interaction - Task, Plan, Feedback, system roles, and external tool invocations - are codified into modular pseudocode enriched with control structures (e.g., loops, conditionals), boolean logic, and typed variables. This design transforms loosely connected agent plans into cohesive, interpretable, and verifiable multi-agent reasoning programs. We evaluate the proposed framework across three diverse benchmarks - GAIA, HotpotQA, and VirtualHome - using a range of representative LLMs. Results show consistent improvements in planning performance, with absolute gains of 3-36 percentage points over natural language prompting baselines. On VirtualHome, our method achieves a new state-of-the-art success rate of 56%. In addition, our approach reduces input and output token usage by 55-87% and 41-70%, respectively, underscoring the importance of token-aware evaluation metrics in the development of scalable multi-agent LLM systems. The code and resources are available at: https://anonymous.4open.science/r/CodifyingAgent-5A86
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GDGB: A Benchmark for Generative Dynamic Text-Attributed Graph Learning</title>
<link>https://arxiv.org/abs/2507.03267</link>
<guid>https://arxiv.org/abs/2507.03267</guid>
<content:encoded><![CDATA[
arXiv:2507.03267v1 Announce Type: new 
Abstract: Dynamic Text-Attributed Graphs (DyTAGs), which intricately integrate structural, temporal, and textual attributes, are crucial for modeling complex real-world systems. However, most of the existing DyTAG datasets exhibit poor textual quality, which severely limits their utility for DyTAG generation tasks requiring semantically rich inputs. Additionally, prior work mainly focuses on discriminative tasks on DyTAGs, resulting in a lack of standardized task formulations and evaluation protocols tailored for DyTAG generation. To address these critical issues, we propose Generative DyTAG Benchmark (GDGB), which comprises eight meticulously curated DyTAG datasets with high-quality textual features for both nodes and edges, overcoming limitations of prior datasets. Building on GDGB, we define two novel DyTAG generation tasks: Transductive Dynamic Graph Generation (TDGG) and Inductive Dynamic Graph Generation (IDGG). TDGG transductively generates a target DyTAG based on the given source and destination node sets, while the more challenging IDGG introduces new node generation to inductively model the dynamic expansion of real-world graph data. To enable holistic evaluation, we design multifaceted metrics that assess the structural, temporal, and textual quality of the generated DyTAGs. We further propose GAG-General, an LLM-based multi-agent generative framework tailored for reproducible and robust benchmarking of DyTAG generation. Experimental results demonstrate that GDGB enables rigorous evaluation of TDGG and IDGG, with key insights revealing the critical interplay of structural and textual features in DyTAG generation. These findings establish GDGB as a foundational resource for advancing generative DyTAG research and unlocking further practical applications in DyTAG generation. GDGB datasets, source codes, and leaderboards are available at \href{https://gdgb-algo.github.io/}{here}.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Information Pursuit for Interactively Guiding Large Language Models</title>
<link>https://arxiv.org/abs/2507.03279</link>
<guid>https://arxiv.org/abs/2507.03279</guid>
<content:encoded><![CDATA[
arXiv:2507.03279v1 Announce Type: new 
Abstract: A significant use case of instruction-finetuned Large Language Models (LLMs) is to solve question-answering tasks interactively. In this setting, an LLM agent is tasked with making a prediction by sequentially querying relevant information from the user, as opposed to a single-turn conversation. This paper explores sequential querying strategies that aim to minimize the expected number of queries. One such strategy is Information Pursuit (IP), a greedy algorithm that at each iteration selects the query that maximizes information gain or equivalently minimizes uncertainty. However, obtaining accurate estimates of mutual information or conditional entropy for LLMs is very difficult in practice due to over- or under-confident LLM probabilities, which leads to suboptimal query selection and predictive performance. To better estimate the uncertainty at each iteration, we propose Conformal Information Pursuit (C-IP), an alternative approach to sequential information gain based on conformal prediction sets. More specifically, C-IP leverages a relationship between prediction sets and conditional entropy at each iteration to estimate uncertainty based on the average size of conformal prediction sets. In contrast to conditional entropy, we find that conformal prediction sets are a distribution-free and robust method of measuring uncertainty. Experiments with 20 Questions show that C-IP obtains better predictive performance and shorter query-answer chains compared to previous approaches to IP and uncertainty-based chain-of-thought methods. Furthermore, extending to an interactive medical setting between a doctor and a patient on the MediQ dataset, C-IP achieves competitive performance with direct single-turn prediction while offering greater interpretability.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dyn-O: Building Structured World Models with Object-Centric Representations</title>
<link>https://arxiv.org/abs/2507.03298</link>
<guid>https://arxiv.org/abs/2507.03298</guid>
<content:encoded><![CDATA[
arXiv:2507.03298v1 Announce Type: new 
Abstract: World models aim to capture the dynamics of the environment, enabling agents to predict and plan for future states. In most scenarios of interest, the dynamics are highly centered on interactions among objects within the environment. This motivates the development of world models that operate on object-centric rather than monolithic representations, with the goal of more effectively capturing environment dynamics and enhancing compositional generalization. However, the development of object-centric world models has largely been explored in environments with limited visual complexity (such as basic geometries). It remains underexplored whether such models can generalize to more complex settings with diverse textures and cluttered scenes. In this paper, we fill this gap by introducing Dyn-O, an enhanced structured world model built upon object-centric representations. Compared to prior work in object-centric representations, Dyn-O improves in both learning representations and modeling dynamics. On the challenging Procgen games, we find that our method can learn object-centric world models directly from pixel observations, outperforming DreamerV3 in rollout prediction accuracy. Furthermore, by decoupling object-centric features into dynamics-agnostic and dynamics-aware components, we enable finer-grained manipulation of these features and generate more diverse imagined trajectories.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAFT: A Graph-based Flow-aware Agentic Framework for Document-level Machine Translation</title>
<link>https://arxiv.org/abs/2507.03311</link>
<guid>https://arxiv.org/abs/2507.03311</guid>
<content:encoded><![CDATA[
arXiv:2507.03311v1 Announce Type: new 
Abstract: Document level Machine Translation (DocMT) approaches often struggle with effectively capturing discourse level phenomena. Existing approaches rely on heuristic rules to segment documents into discourse units, which rarely align with the true discourse structure required for accurate translation. Otherwise, they fail to maintain consistency throughout the document during translation. To address these challenges, we propose Graph Augmented Agentic Framework for Document Level Translation (GRAFT), a novel graph based DocMT system that leverages Large Language Model (LLM) agents for document translation. Our approach integrates segmentation, directed acyclic graph (DAG) based dependency modelling, and discourse aware translation into a cohesive framework. Experiments conducted across eight translation directions and six diverse domains demonstrate that GRAFT achieves significant performance gains over state of the art DocMT systems. Specifically, GRAFT delivers an average improvement of 2.8 d BLEU on the TED test sets from IWSLT2017 over strong baselines and 2.3 d BLEU for domain specific translation from English to Chinese. Moreover, our analyses highlight the consistent ability of GRAFT to address discourse level phenomena, yielding coherent and contextually accurate translations.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mirror in the Model: Ad Banner Image Generation via Reflective Multi-LLM and Multi-modal Agents</title>
<link>https://arxiv.org/abs/2507.03326</link>
<guid>https://arxiv.org/abs/2507.03326</guid>
<content:encoded><![CDATA[
arXiv:2507.03326v1 Announce Type: new 
Abstract: Recent generative models such as GPT-4o have shown strong capabilities in producing high-quality images with accurate text rendering. However, commercial design tasks like advertising banners demand more than visual fidelity -- they require structured layouts, precise typography, consistent branding, and more. In this paper, we introduce MIMO (Mirror In-the-Model), an agentic refinement framework for automatic ad banner generation. MIMO combines a hierarchical multi-modal agent system (MIMO-Core) with a coordination loop (MIMO-Loop) that explores multiple stylistic directions and iteratively improves design quality. Requiring only a simple natural language based prompt and logo image as input, MIMO automatically detects and corrects multiple types of errors during generation. Experiments show that MIMO significantly outperforms existing diffusion and LLM-based baselines in real-world banner design scenarios.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs More Realistic and Less Risky</title>
<link>https://arxiv.org/abs/2507.03336</link>
<guid>https://arxiv.org/abs/2507.03336</guid>
<content:encoded><![CDATA[
arXiv:2507.03336v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly tasked with invoking enterprise APIs, yet they routinely falter when near-duplicate tools vie for the same user intent or when required arguments are left underspecified. We introduce DiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a disambiguation-centric, three-stage pipeline that (i) synthesizes persona-driven, multi-turn dialogues in which the assistant must distinguish among highly similar tools, (ii) performs supervised fine-tuning of open-source models with reasoning traces across 3B - 70B parameters, and (iii) evaluates real-world readiness via a dynamic suite that redeploys each model in a live agentic loop and reports end-to-end goal completion alongside conventional static metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE raise tool-invocation success by 27 pp over GPT-4o and by 49 pp over Claude-3.5-Sonnet, both under optimized prompting. To spur further research, we release an open corpus of 5000 production-grade enterprise API specifications paired with rigorously validated, disambiguation-focused dialogues, offering a practical blueprint for building reliable, enterprise-ready tool-calling agents.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lessons from a Chimp: AI "Scheming" and the Quest for Ape Language</title>
<link>https://arxiv.org/abs/2507.03409</link>
<guid>https://arxiv.org/abs/2507.03409</guid>
<content:encoded><![CDATA[
arXiv:2507.03409v1 Announce Type: new 
Abstract: We examine recent research that asks whether current AI systems may be developing a capacity for "scheming" (covertly and strategically pursuing misaligned goals). We compare current research practices in this field to those adopted in the 1970s to test whether non-human primates could master natural language. We argue that there are lessons to be learned from that historical research endeavour, which was characterised by an overattribution of human traits to other agents, an excessive reliance on anecdote and descriptive analysis, and a failure to articulate a strong theoretical framework for the research. We recommend that research into AI scheming actively seeks to avoid these pitfalls. We outline some concrete steps that can be taken for this research programme to advance in a productive and scientifically rigorous fashion.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ElliottAgents: A Natural Language-Driven Multi-Agent System for Stock Market Analysis and Prediction</title>
<link>https://arxiv.org/abs/2507.03435</link>
<guid>https://arxiv.org/abs/2507.03435</guid>
<content:encoded><![CDATA[
arXiv:2507.03435v1 Announce Type: new 
Abstract: This paper presents ElliottAgents, a multi-agent system leveraging natural language processing (NLP) and large language models (LLMs) to analyze complex stock market data. The system combines AI-driven analysis with the Elliott Wave Principle to generate human-comprehensible predictions and explanations. A key feature is the natural language dialogue between agents, enabling collaborative analysis refinement. The LLM-enhanced architecture facilitates advanced language understanding, reasoning, and autonomous decision-making. Experiments demonstrate the system's effectiveness in pattern recognition and generating natural language descriptions of market trends. ElliottAgents contributes to NLP applications in specialized domains, showcasing how AI-driven dialogue systems can enhance collaborative analysis in data-intensive fields. This research bridges the gap between complex financial data and human understanding, addressing the need for interpretable and adaptive prediction systems in finance.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Reasoning for Cardiovascular Imaging Phenotype Analysis</title>
<link>https://arxiv.org/abs/2507.03460</link>
<guid>https://arxiv.org/abs/2507.03460</guid>
<content:encoded><![CDATA[
arXiv:2507.03460v1 Announce Type: new 
Abstract: Identifying the associations between imaging phenotypes and disease risk factors and outcomes is essential for understanding disease mechanisms and improving diagnosis and prognosis models. However, traditional approaches rely on human-driven hypothesis testing and selection of association factors, often overlooking complex, non-linear dependencies among imaging phenotypes and other multi-modal data. To address this, we introduce a Multi-agent Exploratory Synergy for the Heart (MESHAgents) framework that leverages large language models as agents to dynamically elicit, surface, and decide confounders and phenotypes in association studies, using cardiovascular imaging as a proof of concept. Specifically, we orchestrate a multi-disciplinary team of AI agents -- spanning cardiology, biomechanics, statistics, and clinical research -- which spontaneously generate and converge on insights through iterative, self-organizing reasoning. The framework dynamically synthesizes statistical correlations with multi-expert consensus, providing an automated pipeline for phenome-wide association studies (PheWAS). We demonstrate the system's capabilities through a population-based study of imaging phenotypes of the heart and aorta. MESHAgents autonomously uncovered correlations between imaging phenotypes and a wide range of non-imaging factors, identifying additional confounder variables beyond standard demographic factors. Validation on diagnosis tasks reveals that MESHAgents-discovered phenotypes achieve performance comparable to expert-selected phenotypes, with mean AUC differences as small as -0.004 on disease classification tasks. Notably, the recall score improves for 6 out of 9 disease types. Our framework provides clinically relevant imaging phenotypes with transparent reasoning, offering a scalable alternative to expert-driven methods.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REAL: Benchmarking Abilities of Large Language Models for Housing Transactions and Services</title>
<link>https://arxiv.org/abs/2507.03477</link>
<guid>https://arxiv.org/abs/2507.03477</guid>
<content:encoded><![CDATA[
arXiv:2507.03477v1 Announce Type: new 
Abstract: The development of large language models (LLMs) has greatly promoted the progress of chatbot in multiple fields. There is an urgent need to evaluate whether LLMs can play the role of agent in housing transactions and services as well as humans. We present Real Estate Agent Large Language Model Evaluation (REAL), the first evaluation suite designed to assess the abilities of LLMs in the field of housing transactions and services. REAL comprises 5,316 high-quality evaluation entries across 4 topics: memory, comprehension, reasoning and hallucination. All these entries are organized as 14 categories to assess whether LLMs have the knowledge and ability in housing transactions and services scenario. Additionally, the REAL is used to evaluate the performance of most advanced LLMs. The experiment results indicate that LLMs still have significant room for improvement to be applied in the real estate field.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Information Retrieval in the Audit Domain</title>
<link>https://arxiv.org/abs/2507.03479</link>
<guid>https://arxiv.org/abs/2507.03479</guid>
<content:encoded><![CDATA[
arXiv:2507.03479v1 Announce Type: new 
Abstract: Conversational agents such as Microsoft Copilot and Google Gemini assist users with complex search tasks but often generate misleading or fabricated references. This undermines trust, particularly in high-stakes domains such as medicine and finance. Explainable information retrieval (XIR) aims to address this by making search results more transparent and interpretable. While most XIR research is domain-agnostic, this paper focuses on auditing -- a critical yet underexplored area. We argue that XIR systems can support auditors in completing their complex task. We outline key challenges and future research directions to advance XIR in this domain.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-VaxGuide: An Agentic RAG-Based LLM for Vaccination Decisions</title>
<link>https://arxiv.org/abs/2507.03493</link>
<guid>https://arxiv.org/abs/2507.03493</guid>
<content:encoded><![CDATA[
arXiv:2507.03493v1 Announce Type: new 
Abstract: Vaccination plays a vital role in global public health, yet healthcare professionals often struggle to access immunization guidelines quickly and efficiently. National protocols and WHO recommendations are typically extensive and complex, making it difficult to extract precise information, especially during urgent situations. This project tackles that issue by developing a multilingual, intelligent question-answering system that transforms static vaccination guidelines into an interactive and user-friendly knowledge base. Built on a Retrieval-Augmented Generation (RAG) framework and enhanced with agent-based reasoning (Agentic RAG), the system provides accurate, context-sensitive answers to complex medical queries. Evaluation shows that Agentic RAG outperforms traditional methods, particularly in addressing multi-step or ambiguous questions. To support clinical use, the system is integrated into a mobile application designed for real-time, point-of-care access to essential vaccine information. AI-VaxGuide model is publicly available on https://huggingface.co/VaxGuide
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning-based Feature Generation Algorithm for Scientific Data</title>
<link>https://arxiv.org/abs/2507.03498</link>
<guid>https://arxiv.org/abs/2507.03498</guid>
<content:encoded><![CDATA[
arXiv:2507.03498v1 Announce Type: new 
Abstract: Feature generation (FG) aims to enhance the prediction potential of original data by constructing high-order feature combinations and removing redundant features. It is a key preprocessing step for tabular scientific data to improve downstream machine-learning model performance. Traditional methods face the following two challenges when dealing with the feature generation of scientific data: First, the effective construction of high-order feature combinations in scientific data necessitates profound and extensive domain-specific expertise. Secondly, as the order of feature combinations increases, the search space expands exponentially, imposing prohibitive human labor consumption. Advancements in the Data-Centric Artificial Intelligence (DCAI) paradigm have opened novel avenues for automating feature generation processes. Inspired by that, this paper revisits the conventional feature generation workflow and proposes the Multi-agent Feature Generation (MAFG) framework. Specifically, in the iterative exploration stage, multi-agents will construct mathematical transformation equations collaboratively, synthesize and identify feature combinations ex-hibiting high information content, and leverage a reinforcement learning mechanism to evolve their strategies. Upon completing the exploration phase, MAFG integrates the large language models (LLMs) to interpreta-tively evaluate the generated features of each significant model performance breakthrough. Experimental results and case studies consistently demonstrate that the MAFG framework effectively automates the feature generation process and significantly enhances various downstream scientific data mining tasks.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On characterization and existence of a constrained correlated equilibria in Markov games</title>
<link>https://arxiv.org/abs/2507.03502</link>
<guid>https://arxiv.org/abs/2507.03502</guid>
<content:encoded><![CDATA[
arXiv:2507.03502v1 Announce Type: new 
Abstract: Markov games with coupling constraints provide a natural framework to study constrained decision-making involving self-interested agents, where the feasibility of an individual agent's strategy depends on the joint strategies of the others. Such games arise in numerous real-world applications involving safety requirements and budget caps, for example, in environmental management, electricity markets, and transportation systems. While correlated equilibria have emerged as an important solution concept in unconstrained settings due to their computational tractability and amenability to learning, their constrained counterparts remain less explored. In this paper, we study constrained correlated equilibria-feasible policies where any unilateral modifications are either unprofitable or infeasible. We first characterize the constrained correlated equilibrium showing that different sets of modifications result in an equivalent notion, a result which may enable efficient learning algorithms. We then address existence conditions. In particular, we show that a strong Slater-type condition is necessary in games with playerwise coupling constraints, but can be significantly weakened when all players share common coupling constraints. Under this relaxed condition, we prove the existence of a constrained correlated equilibrium.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoAgentX: An Automated Framework for Evolving Agentic Workflows</title>
<link>https://arxiv.org/abs/2507.03616</link>
<guid>https://arxiv.org/abs/2507.03616</guid>
<content:encoded><![CDATA[
arXiv:2507.03616v1 Announce Type: new 
Abstract: Multi-agent systems (MAS) have emerged as a powerful paradigm for orchestrating large language models (LLMs) and specialized tools to collaboratively address complex tasks. However, existing MAS frameworks often require manual workflow configuration and lack native support for dynamic evolution and performance optimization. In addition, many MAS optimization algorithms are not integrated into a unified framework. In this paper, we present EvoAgentX, an open-source platform that automates the generation, execution, and evolutionary optimization of multi-agent workflows. EvoAgentX employs a modular architecture consisting of five core layers: the basic components, agent, workflow, evolving, and evaluation layers. Specifically, within the evolving layer, EvoAgentX integrates three MAS optimization algorithms, TextGrad, AFlow, and MIPRO, to iteratively refine agent prompts, tool configurations, and workflow topologies. We evaluate EvoAgentX on HotPotQA, MBPP, and MATH for multi-hop reasoning, code generation, and mathematical problem solving, respectively, and further assess it on real-world tasks using GAIA. Experimental results show that EvoAgentX consistently achieves significant performance improvements, including a 7.44% increase in HotPotQA F1, a 10.00% improvement in MBPP pass@1, a 10.00% gain in MATH solve accuracy, and an overall accuracy improvement of up to 20.00% on GAIA. The source code is available at: https://github.com/EvoAgentX/EvoAgentX
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is It Time To Treat Prompts As Code? A Multi-Use Case Study For Prompt Optimization Using DSPy</title>
<link>https://arxiv.org/abs/2507.03620</link>
<guid>https://arxiv.org/abs/2507.03620</guid>
<content:encoded><![CDATA[
arXiv:2507.03620v1 Announce Type: new 
Abstract: Although prompt engineering is central to unlocking the full potential of Large Language Models (LLMs), crafting effective prompts remains a time-consuming trial-and-error process that relies on human intuition. This study investigates Declarative Self-improving Python (DSPy), an optimization framework that programmatically creates and refines prompts, applied to five use cases: guardrail enforcement, hallucination detection in code, code generation, routing agents, and prompt evaluation. Each use case explores how prompt optimization via DSPy influences performance. While some cases demonstrated modest improvements - such as minor gains in the guardrails use case and selective enhancements in hallucination detection - others showed notable benefits. The prompt evaluation criterion task demonstrated a substantial performance increase, rising accuracy from 46.2% to 64.0%. In the router agent case, the possibility of improving a poorly performing prompt and of a smaller model matching a stronger one through optimized prompting was explored. Although prompt refinement increased accuracy from 85.0% to 90.0%, using the optimized prompt with a cheaper model did not improve performance. Overall, this study's findings suggest that DSPy's systematic prompt optimization can enhance LLM performance, particularly when instruction tuning and example selection are optimized together. However, the impact varies by task, highlighting the importance of evaluating specific use cases in prompt optimization research.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recon, Answer, Verify: Agents in Search of Truth</title>
<link>https://arxiv.org/abs/2507.03671</link>
<guid>https://arxiv.org/abs/2507.03671</guid>
<content:encoded><![CDATA[
arXiv:2507.03671v1 Announce Type: new 
Abstract: Automated fact checking with large language models (LLMs) offers a scalable alternative to manual verification. Evaluating fact checking is challenging as existing benchmark datasets often include post claim analysis and annotator cues, which are absent in real world scenarios where claims are fact checked immediately after being made. This limits the realism of current evaluations. We present Politi Fact Only (PFO), a 5 class benchmark dataset of 2,982 political claims from politifact.com, where all post claim analysis and annotator cues have been removed manually. This ensures that models are evaluated using only the information that would have been available prior to the claim's verification. Evaluating LLMs on PFO, we see an average performance drop of 22% in terms of macro f1 compared to PFO's unfiltered version. Based on the identified challenges of the existing LLM based fact checking system, we propose RAV (Recon Answer Verify), an agentic framework with three agents: question generator, answer generator, and label generator. Our pipeline iteratively generates and answers sub questions to verify different aspects of the claim before finally generating the label. RAV generalizes across domains and label granularities, and it outperforms state of the art approaches on well known baselines RAWFC (fact checking, 3 class) by 25.28%, and on HOVER (encyclopedia, 2 class) by 1.54% on 2 hop, 4.94% on 3 hop, and 1.78% on 4 hop, sub categories respectively. RAV shows the least performance drop compared to baselines of 16.3% in macro f1 when we compare PFO with its unfiltered version.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STRUCTSENSE: A Task-Agnostic Agentic Framework for Structured Information Extraction with Human-In-The-Loop Evaluation and Benchmarking</title>
<link>https://arxiv.org/abs/2507.03674</link>
<guid>https://arxiv.org/abs/2507.03674</guid>
<content:encoded><![CDATA[
arXiv:2507.03674v1 Announce Type: new 
Abstract: The ability to extract structured information from unstructured sources-such as free-text documents and scientific literature-is critical for accelerating scientific discovery and knowledge synthesis. Large Language Models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks, including structured information extraction. However, their effectiveness often diminishes in specialized, domain-specific contexts that require nuanced understanding and expert-level domain knowledge. In addition, existing LLM-based approaches frequently exhibit poor transferability across tasks and domains, limiting their scalability and adaptability. To address these challenges, we introduce StructSense, a modular, task-agnostic, open-source framework for structured information extraction built on LLMs. StructSense is guided by domain-specific symbolic knowledge encoded in ontologies, enabling it to navigate complex domain content more effectively. It further incorporates agentic capabilities through self-evaluative judges that form a feedback loop for iterative refinement, and includes human-in-the-loop mechanisms to ensure quality and validation. We demonstrate that StructSense can overcome both the limitations of domain sensitivity and the lack of cross-task generalizability, as shown through its application to diverse neuroscience information extraction tasks.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Machine Theory of Mind with Large Language Model-Augmented Inverse Planning</title>
<link>https://arxiv.org/abs/2507.03682</link>
<guid>https://arxiv.org/abs/2507.03682</guid>
<content:encoded><![CDATA[
arXiv:2507.03682v1 Announce Type: new 
Abstract: We propose a hybrid approach to machine Theory of Mind (ToM) that uses large language models (LLMs) as a mechanism for generating hypotheses and likelihood functions with a Bayesian inverse planning model that computes posterior probabilities for an agent's likely mental states given its actions. Bayesian inverse planning models can accurately predict human reasoning on a variety of ToM tasks, but these models are constrained in their ability to scale these predictions to scenarios with a large number of possible hypotheses and actions. Conversely, LLM-based approaches have recently demonstrated promise in solving ToM benchmarks, but can exhibit brittleness and failures on reasoning tasks even when they pass otherwise structurally identical versions. By combining these two methods, this approach leverages the strengths of each component, closely matching optimal results on a task inspired by prior inverse planning models and improving performance relative to models that utilize LLMs alone or with chain-of-thought prompting, even with smaller LLMs that typically perform poorly on ToM tasks. We also exhibit the model's potential to predict mental states on open-ended tasks, offering a promising direction for future development of ToM models and the creation of socially intelligent generative agents.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Play \^O \u{A}n Quan Game? A Study of Multi-Step Planning and Decision Making</title>
<link>https://arxiv.org/abs/2507.03711</link>
<guid>https://arxiv.org/abs/2507.03711</guid>
<content:encoded><![CDATA[
arXiv:2507.03711v1 Announce Type: new 
Abstract: In this paper, we explore the ability of large language models (LLMs) to plan and make decisions through the lens of the traditional Vietnamese board game, \^O \u{A}n Quan. This game, which involves a series of strategic token movements and captures, offers a unique environment for evaluating the decision-making and strategic capabilities of LLMs. Specifically, we develop various agent personas, ranging from aggressive to defensive, and employ the \^O \u{A}n Quan game as a testbed for assessing LLM performance across different strategies. Through experimentation with models like Llama-3.2-3B-Instruct, Llama-3.1-8B-Instruct, and Llama-3.3-70B-Instruct, we aim to understand how these models execute strategic decision-making, plan moves, and manage dynamic game states. The results will offer insights into the strengths and weaknesses of LLMs in terms of reasoning and strategy, contributing to a deeper understanding of their general capabilities.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-Based Detection and Resolution of Incompleteness and Ambiguity in Interactions with Large Language Models</title>
<link>https://arxiv.org/abs/2507.03726</link>
<guid>https://arxiv.org/abs/2507.03726</guid>
<content:encoded><![CDATA[
arXiv:2507.03726v1 Announce Type: new 
Abstract: Many of us now treat LLMs as modern-day oracles asking it almost any kind of question. However, consulting an LLM does not have to be a single turn activity. But long multi-turn interactions can get tedious if it is simply to clarify contextual information that can be arrived at through reasoning. In this paper, we examine the use of agent-based architecture to bolster LLM-based Question-Answering systems with additional reasoning capabilities. We examine the automatic resolution of potential incompleteness or ambiguities in questions by transducers implemented using LLM-based agents. We focus on several benchmark datasets that are known to contain questions with these deficiencies to varying degrees. We equip different LLMs (GPT-3.5-Turbo and Llama-4-Scout) with agents that act as specialists in detecting and resolving deficiencies of incompleteness and ambiguity. The agents are implemented as zero-shot ReAct agents. Rather than producing an answer in a single step, the model now decides between 3 actions a) classify b) resolve c) answer. Action a) decides if the question is incomplete, ambiguous, or normal. Action b) determines if any deficiencies identified can be resolved. Action c) answers the resolved form of the question. We compare the use of LLMs with and without the use of agents with these components. Our results show benefits of agents with transducer 1) A shortening of the length of interactions with human 2) An improvement in the answer quality and 3) Explainable resolution of deficiencies in the question. On the negative side we find while it may result in additional LLM invocations and in some cases, increased latency. But on tested datasets, the benefits outweigh the costs except when questions already have sufficient context. Suggesting the agent-based approach could be a useful mechanism to harness the power of LLMs to develop more robust QA systems.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Empowering GUI Agent with Context-Aware Simplification</title>
<link>https://arxiv.org/abs/2507.03730</link>
<guid>https://arxiv.org/abs/2507.03730</guid>
<content:encoded><![CDATA[
arXiv:2507.03730v1 Announce Type: new 
Abstract: The research focus of GUI agents is shifting from text-dependent to pure-vision-based approaches, which, though promising, prioritize comprehensive pre-training data collection while neglecting contextual modeling challenges. We probe the characteristics of element and history contextual modeling in GUI agent and summarize: 1) the high-density and loose-relation of element context highlight the existence of many unrelated elements and their negative influence; 2) the high redundancy of history context reveals the inefficient history modeling in current GUI agents. In this work, we propose a context-aware simplification framework for building an efficient and effective GUI Agent, termed SimpAgent. To mitigate potential interference from numerous unrelated elements, we introduce a masking-based element pruning method that circumvents the intractable relation modeling through an efficient masking mechanism. To reduce the redundancy in historical information, we devise a consistency-guided history compression module, which enhances implicit LLM-based compression through innovative explicit guidance, achieving an optimal balance between performance and efficiency. With the above components, SimpAgent reduces 27% FLOPs and achieves superior GUI navigation performances. Comprehensive navigation experiments across diverse web and mobile environments demonstrate the effectiveness and potential of our agent.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Outdoor Monocular SLAM with Global Scale-Consistent 3D Gaussian Pointmaps</title>
<link>https://arxiv.org/abs/2507.03737</link>
<guid>https://arxiv.org/abs/2507.03737</guid>
<content:encoded><![CDATA[
arXiv:2507.03737v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has become a popular solution in SLAM due to its high-fidelity and real-time novel view synthesis performance. However, some previous 3DGS SLAM methods employ a differentiable rendering pipeline for tracking, \textbf{lack geometric priors} in outdoor scenes. Other approaches introduce separate tracking modules, but they accumulate errors with significant camera movement, leading to \textbf{scale drift}. To address these challenges, we propose a robust RGB-only outdoor 3DGS SLAM method: S3PO-GS. Technically, we establish a self-consistent tracking module anchored in the 3DGS pointmap, which avoids cumulative scale drift and achieves more precise and robust tracking with fewer iterations. Additionally, we design a patch-based pointmap dynamic mapping module, which introduces geometric priors while avoiding scale ambiguity. This significantly enhances tracking accuracy and the quality of scene reconstruction, making it particularly suitable for complex outdoor environments. Our experiments on the Waymo, KITTI, and DL3DV datasets demonstrate that S3PO-GS achieves state-of-the-art results in novel view synthesis and outperforms other 3DGS SLAM methods in tracking accuracy. Project page: https://3dagentworld.github.io/S3PO-GS/.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Dark Souls Combat Through Pixel Input With Neuroevolution</title>
<link>https://arxiv.org/abs/2507.03793</link>
<guid>https://arxiv.org/abs/2507.03793</guid>
<content:encoded><![CDATA[
arXiv:2507.03793v1 Announce Type: new 
Abstract: This paper investigates the application of Neuroevolution of Augmenting Topologies (NEAT) to automate gameplay in Dark Souls, a notoriously challenging action role-playing game characterized by complex combat mechanics, dynamic environments, and high-dimensional visual inputs. Unlike traditional reinforcement learning or game playing approaches, our method evolves neural networks directly from raw pixel data, circumventing the need for explicit game-state information. To facilitate this approach, we introduce the Dark Souls API (DSAPI), a novel Python framework leveraging real-time computer vision techniques for extracting critical game metrics, including player and enemy health states. Using NEAT, agents evolve effective combat strategies for defeating the Asylum Demon, the game's initial boss, without predefined behaviors or domain-specific heuristics. Experimental results demonstrate that evolved agents achieve up to a 35% success rate, indicating the viability of neuroevolution in addressing complex, visually intricate gameplay scenarios. This work represents an interesting application of vision-based neuroevolution, highlighting its potential use in a wide range of challenging game environments lacking direct API support or well-defined state representations.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Novelty in Open-World Multi-Agent Strategic Board Games</title>
<link>https://arxiv.org/abs/2507.03802</link>
<guid>https://arxiv.org/abs/2507.03802</guid>
<content:encoded><![CDATA[
arXiv:2507.03802v1 Announce Type: new 
Abstract: We describe GNOME (Generating Novelty in Open-world Multi-agent Environments), an experimental platform that is designed to test the effectiveness of multi-agent AI systems when faced with \emph{novelty}. GNOME separates the development of AI gameplaying agents with the simulator, allowing \emph{unanticipated} novelty (in essence, novelty that is not subject to model-selection bias). Using a Web GUI, GNOME was recently demonstrated at NeurIPS 2020 using the game of Monopoly to foster an open discussion on AI robustness and the nature of novelty in real-world environments. In this article, we further detail the key elements of the demonstration, and also provide an overview of the experimental design that is being currently used in the DARPA Science of Artificial Intelligence and Learning for Open-World Novelty (SAIL-ON) program to evaluate external teams developing novelty-adaptive gameplaying agents.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Large Language Models for Tacit Knowledge Discovery in Organizational Contexts</title>
<link>https://arxiv.org/abs/2507.03811</link>
<guid>https://arxiv.org/abs/2507.03811</guid>
<content:encoded><![CDATA[
arXiv:2507.03811v1 Announce Type: new 
Abstract: Documenting tacit knowledge in organizations can be a challenging task due to incomplete initial information, difficulty in identifying knowledgeable individuals, the interplay of formal hierarchies and informal networks, and the need to ask the right questions. To address this, we propose an agent-based framework leveraging large language models (LLMs) to iteratively reconstruct dataset descriptions through interactions with employees. Modeling knowledge dissemination as a Susceptible-Infectious (SI) process with waning infectivity, we conduct 864 simulations across various synthetic company structures and different dissemination parameters. Our results show that the agent achieves 94.9% full-knowledge recall, with self-critical feedback scores strongly correlating with external literature critic scores. We analyze how each simulation parameter affects the knowledge retrieval process for the agent. In particular, we find that our approach is able to recover information without needing to access directly the only domain specialist. These findings highlight the agent's ability to navigate organizational complexity and capture fragmented knowledge that would otherwise remain inaccessible.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Participatory Evolution of Artificial Life Systems via Semantic Feedback</title>
<link>https://arxiv.org/abs/2507.03839</link>
<guid>https://arxiv.org/abs/2507.03839</guid>
<content:encoded><![CDATA[
arXiv:2507.03839v1 Announce Type: new 
Abstract: We present a semantic feedback framework that enables natural language to guide the evolution of artificial life systems. Integrating a prompt-to-parameter encoder, a CMA-ES optimizer, and CLIP-based evaluation, the system allows user intent to modulate both visual outcomes and underlying behavioral rules. Implemented in an interactive ecosystem simulation, the framework supports prompt refinement, multi-agent interaction, and emergent rule synthesis. User studies show improved semantic alignment over manual tuning and demonstrate the system's potential as a platform for participatory generative design and open-ended evolution.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Systemic and Environment Errors in Autonomous Systems Using Differential Testing</title>
<link>https://arxiv.org/abs/2507.03870</link>
<guid>https://arxiv.org/abs/2507.03870</guid>
<content:encoded><![CDATA[
arXiv:2507.03870v1 Announce Type: new 
Abstract: When an autonomous agent behaves undesirably, including failure to complete a task, it can be difficult to determine whether the behavior is due to a systemic agent error, such as flaws in the model or policy, or an environment error, where a task is inherently infeasible under a given environment configuration, even for an ideal agent. As agents and their environments grow more complex, identifying the error source becomes increasingly difficult but critical for reliable deployment. We introduce AIProbe, a novel black-box testing technique that applies differential testing to attribute undesirable agent behaviors either to agent deficiencies, such as modeling or training flaws, or due to environmental infeasibility. AIProbe first generates diverse environmental configurations and tasks for testing the agent, by modifying configurable parameters using Latin Hypercube sampling. It then solves each generated task using a search-based planner, independent of the agent. By comparing the agent's performance to the planner's solution, AIProbe identifies whether failures are due to errors in the agent's model or policy, or due to unsolvable task conditions. Our evaluation across multiple domains shows that AIProbe significantly outperforms state-of-the-art techniques in detecting both total and unique errors, thereby contributing to a reliable deployment of autonomous agents.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent Exchange: Shaping the Future of AI Agent Economics</title>
<link>https://arxiv.org/abs/2507.03904</link>
<guid>https://arxiv.org/abs/2507.03904</guid>
<content:encoded><![CDATA[
arXiv:2507.03904v1 Announce Type: new 
Abstract: The rise of Large Language Models (LLMs) has transformed AI agents from passive computational tools into autonomous economic actors. This shift marks the emergence of the agent-centric economy, in which agents take on active economic roles-exchanging value, making strategic decisions, and coordinating actions with minimal human oversight. To realize this vision, we propose Agent Exchange (AEX), a specialized auction platform designed to support the dynamics of the AI agent marketplace. AEX offers an optimized infrastructure for agent coordination and economic participation. Inspired by Real-Time Bidding (RTB) systems in online advertising, AEX serves as the central auction engine, facilitating interactions among four ecosystem components: the User-Side Platform (USP), which translates human goals into agent-executable tasks; the Agent-Side Platform (ASP), responsible for capability representation, performance tracking, and optimization; Agent Hubs, which coordinate agent teams and participate in AEX-hosted auctions; and the Data Management Platform (DMP), ensuring secure knowledge sharing and fair value attribution. We outline the design principles and system architecture of AEX, laying the groundwork for agent-based economic infrastructure in future AI ecosystems.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CortexDebate: Debating Sparsely and Equally for Multi-Agent Debate</title>
<link>https://arxiv.org/abs/2507.03928</link>
<guid>https://arxiv.org/abs/2507.03928</guid>
<content:encoded><![CDATA[
arXiv:2507.03928v1 Announce Type: new 
Abstract: Nowadays, single Large Language Model (LLM) struggles with critical issues such as hallucination and inadequate reasoning abilities. To mitigate these issues, Multi-Agent Debate (MAD) has emerged as an effective strategy, where LLM agents engage in in-depth debates with others on tasks. However, existing MAD methods face two major issues: (a) too lengthy input contexts, which causes LLM agents to get lost in plenty of input information and experiences performance drop; and (b) the overconfidence dilemma, where self-assured LLM agents dominate the debate, leading to low debating effectiveness. To address these limitations, we propose a novel MAD method called "CortexDebate". Inspired by the human brain's tendency to establish a sparse and dynamically optimized network among cortical areas governed by white matter, CortexDebate constructs a sparse debating graph among LLM agents, where each LLM agent only debates with the ones that are helpful to it. To optimize the graph, we propose a module named McKinsey-based Debate Matter (MDM), which acts as an artificial analog to white matter. By integrating the McKinsey Trust Formula, a well-established measure of trustworthiness from sociology, MDM enables credible evaluations that guide graph optimization. The effectiveness of our CortexDebate has been well demonstrated by extensive experimental results across eight datasets from four task types.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair and Efficient Allocation of Indivisible Mixed Manna</title>
<link>https://arxiv.org/abs/2507.03946</link>
<guid>https://arxiv.org/abs/2507.03946</guid>
<content:encoded><![CDATA[
arXiv:2507.03946v1 Announce Type: new 
Abstract: We study fair division of indivisible mixed manna (items whose values may be positive, negative, or zero) among agents with additive valuations. Here, we establish that fairness -- in terms of a relaxation of envy-freeness -- and Pareto efficiency can always be achieved together. Specifically, our fairness guarantees are in terms of envy-freeness up to $k$ reallocations (EFR-$k$): An allocation $A$ of the indivisible items is said to be EFR-$k$ if there exists a subset $R$ of at most $k$ items such that, for each agent $i$, we can reassign items from within $R$ (in $A$) and obtain an allocation, $A^i$, which is envy-free for $i$. We establish that, when allocating mixed manna among $n$ agents with additive valuations, an EFR-$(n-1)$ and Pareto optimal (PO) allocation $A$ always exists. Further, the individual envy-free allocations $A^i$, induced by reassignments, are also PO. In addition, we prove that such fair and efficient allocations are efficiently computable when the number of agents, $n$, is fixed.
  We also obtain positive results focusing on EFR by itself (and without the PO desideratum). Specifically, we show that an EFR-$(n-1)$ allocation of mixed manna can be computed in polynomial time. In addition, we prove that when all the items are goods, an EFR-${\lfloor n/2 \rfloor}$ allocation exists and can be computed efficiently. Here, the $(n-1)$ bound is tight for chores and $\lfloor n/2 \rfloor$ is tight for goods.
  Our results advance the understanding of fair and efficient allocation of indivisible mixed manna and rely on a novel application of the Knaster-Kuratowski-Mazurkiewicz (KKM) Theorem in discrete fair division. We utilize weighted welfare maximization, with perturbed valuations, to achieve Pareto efficiency, and overall, our techniques are notably different from existing market-based approaches.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MalVol-25: A Diverse, Labelled and Detailed Volatile Memory Dataset for Malware Detection and Response Testing and Validation</title>
<link>https://arxiv.org/abs/2507.03993</link>
<guid>https://arxiv.org/abs/2507.03993</guid>
<content:encoded><![CDATA[
arXiv:2507.03993v1 Announce Type: new 
Abstract: This paper addresses the critical need for high-quality malware datasets that support advanced analysis techniques, particularly machine learning and agentic AI frameworks. Existing datasets often lack diversity, comprehensive labelling, and the complexity necessary for effective machine learning and agent-based AI training. To fill this gap, we developed a systematic approach for generating a dataset that combines automated malware execution in controlled virtual environments with dynamic monitoring tools. The resulting dataset comprises clean and infected memory snapshots across multiple malware families and operating systems, capturing detailed behavioural and environmental features. Key design decisions include applying ethical and legal compliance, thorough validation using both automated and manual methods, and comprehensive documentation to ensure replicability and integrity. The dataset's distinctive features enable modelling system states and transitions, facilitating RL-based malware detection and response strategies. This resource is significant for advancing adaptive cybersecurity defences and digital forensic research. Its scope supports diverse malware scenarios and offers potential for broader applications in incident response and automated threat mitigation.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring a Gamified Personality Assessment Method through Interaction with Multi-Personality LLM Agents</title>
<link>https://arxiv.org/abs/2507.04005</link>
<guid>https://arxiv.org/abs/2507.04005</guid>
<content:encoded><![CDATA[
arXiv:2507.04005v1 Announce Type: new 
Abstract: The execution of effective and imperceptible personality assessments is receiving increasing attention in psychology and human-computer interaction fields. This study explores an interactive approach for personality assessment, focusing on the multiplicity of personality representation. We propose a framework of gamified personality assessment through multi-personality representations (Multi-PR GPA). The framework leverages Large Language Models to empower virtual agents with diverse personalities. These agents elicit multifaceted human personality representations through engaging in interactive games. Drawing upon the multi-type textual data generated throughout the interaction, it achieves two ways of personality assessments (i.e., Direct Assessment and Que-based Assessment) and provides interpretable insights. Grounded in the classic Big Five theory, we implemented a prototype system and conducted a user study to assess the efficacy of Multi-PR GPA. The results underscore the effectiveness of our approach in personality assessment and demonstrate that it achieves superior performance when considering the multiplicity of personality representation.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PresentAgent: Multimodal Agent for Presentation Video Generation</title>
<link>https://arxiv.org/abs/2507.04036</link>
<guid>https://arxiv.org/abs/2507.04036</guid>
<content:encoded><![CDATA[
arXiv:2507.04036v1 Announce Type: new 
Abstract: We present PresentAgent, a multimodal agent that transforms long-form documents into narrated presentation videos. While existing approaches are limited to generating static slides or text summaries, our method advances beyond these limitations by producing fully synchronized visual and spoken content that closely mimics human-style presentations. To achieve this integration, PresentAgent employs a modular pipeline that systematically segments the input document, plans and renders slide-style visual frames, generates contextual spoken narration with large language models and Text-to-Speech models, and seamlessly composes the final video with precise audio-visual alignment. Given the complexity of evaluating such multimodal outputs, we introduce PresentEval, a unified assessment framework powered by Vision-Language Models that comprehensively scores videos across three critical dimensions: content fidelity, visual clarity, and audience comprehension through prompt-based evaluation. Our experimental validation on a curated dataset of 30 document-presentation pairs demonstrates that PresentAgent approaches human-level quality across all evaluation metrics. These results highlight the significant potential of controllable multimodal agents in transforming static textual materials into dynamic, effective, and accessible presentation formats. Code will be available at https://github.com/AIGeeksGroup/PresentAgent.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ready Jurist One: Benchmarking Language Agents for Legal Intelligence in Dynamic Environments</title>
<link>https://arxiv.org/abs/2507.04037</link>
<guid>https://arxiv.org/abs/2507.04037</guid>
<content:encoded><![CDATA[
arXiv:2507.04037v1 Announce Type: new 
Abstract: The gap between static benchmarks and the dynamic nature of real-world legal practice poses a key barrier to advancing legal intelligence. To this end, we introduce J1-ENVS, the first interactive and dynamic legal environment tailored for LLM-based agents. Guided by legal experts, it comprises six representative scenarios from Chinese legal practices across three levels of environmental complexity. We further introduce J1-EVAL, a fine-grained evaluation framework, designed to assess both task performance and procedural compliance across varying levels of legal proficiency. Extensive experiments on 17 LLM agents reveal that, while many models demonstrate solid legal knowledge, they struggle with procedural execution in dynamic settings. Even the SOTA model, GPT-4o, falls short of 60% overall performance. These findings highlight persistent challenges in achieving dynamic legal intelligence and offer valuable insights to guide future research.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation</title>
<link>https://arxiv.org/abs/2507.04047</link>
<guid>https://arxiv.org/abs/2507.04047</guid>
<content:encoded><![CDATA[
arXiv:2507.04047v1 Announce Type: new 
Abstract: Embodied scene understanding requires not only comprehending visual-spatial information that has been observed but also determining where to explore next in the 3D physical world. Existing 3D Vision-Language (3D-VL) models primarily focus on grounding objects in static observations from 3D reconstruction, such as meshes and point clouds, but lack the ability to actively perceive and explore their environment. To address this limitation, we introduce \underline{\textbf{M}}ove \underline{\textbf{t}}o \underline{\textbf{U}}nderstand (\textbf{\model}), a unified framework that integrates active perception with \underline{\textbf{3D}} vision-language learning, enabling embodied agents to effectively explore and understand their environment. This is achieved by three key innovations: 1) Online query-based representation learning, enabling direct spatial memory construction from RGB-D frames, eliminating the need for explicit 3D reconstruction. 2) A unified objective for grounding and exploring, which represents unexplored locations as frontier queries and jointly optimizes object grounding and frontier selection. 3) End-to-end trajectory learning that combines \textbf{V}ision-\textbf{L}anguage-\textbf{E}xploration pre-training over a million diverse trajectories collected from both simulated and real-world RGB-D sequences. Extensive evaluations across various embodied navigation and question-answering benchmarks show that MTU3D outperforms state-of-the-art reinforcement learning and modular navigation approaches by 14\%, 23\%, 9\%, and 2\% in success rate on HM3D-OVON, GOAT-Bench, SG3D, and A-EQA, respectively. \model's versatility enables navigation using diverse input modalities, including categories, language descriptions, and reference images. These findings highlight the importance of bridging visual grounding and exploration for embodied intelligence.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Imitation Bottlenecks: Reinforced Diffusion Powers Diverse Trajectory Generation</title>
<link>https://arxiv.org/abs/2507.04049</link>
<guid>https://arxiv.org/abs/2507.04049</guid>
<content:encoded><![CDATA[
arXiv:2507.04049v1 Announce Type: new 
Abstract: Most end-to-end autonomous driving methods rely on imitation learning from single expert demonstrations, often leading to conservative and homogeneous behaviors that limit generalization in complex real-world scenarios. In this work, we propose DIVER, an end-to-end driving framework that integrates reinforcement learning with diffusion-based generation to produce diverse and feasible trajectories. At the core of DIVER lies a reinforced diffusion-based generation mechanism. First, the model conditions on map elements and surrounding agents to generate multiple reference trajectories from a single ground-truth trajectory, alleviating the limitations of imitation learning that arise from relying solely on single expert demonstrations. Second, reinforcement learning is employed to guide the diffusion process, where reward-based supervision enforces safety and diversity constraints on the generated trajectories, thereby enhancing their practicality and generalization capability. Furthermore, to address the limitations of L2-based open-loop metrics in capturing trajectory diversity, we propose a novel Diversity metric to evaluate the diversity of multi-mode predictions.Extensive experiments on the closed-loop NAVSIM and Bench2Drive benchmarks, as well as the open-loop nuScenes dataset, demonstrate that DIVER significantly improves trajectory diversity, effectively addressing the mode collapse problem inherent in imitation learning.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HAWK: A Hierarchical Workflow Framework for Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2507.04067</link>
<guid>https://arxiv.org/abs/2507.04067</guid>
<content:encoded><![CDATA[
arXiv:2507.04067v1 Announce Type: new 
Abstract: Contemporary multi-agent systems encounter persistent challenges in cross-platform interoperability, dynamic task scheduling, and efficient resource sharing. Agents with heterogeneous implementations often lack standardized interfaces; collaboration frameworks remain brittle and hard to extend; scheduling policies are static; and inter-agent state synchronization is insufficient. We propose Hierarchical Agent Workflow (HAWK), a modular framework comprising five layers-User, Workflow, Operator, Agent, and Resource-and supported by sixteen standardized interfaces. HAWK delivers an end-to-end pipeline covering task parsing, workflow orchestration, intelligent scheduling, resource invocation, and data synchronization. At its core lies an adaptive scheduling and optimization module in the Workflow Layer, which harnesses real-time feedback and dynamic strategy adjustment to maximize utilization. The Resource Layer provides a unified abstraction over heterogeneous data sources, large models, physical devices, and third-party services&amp;tools, simplifying cross-domain information retrieval. We demonstrate HAWK's scalability and effectiveness via CreAgentive, a multi-agent novel-generation prototype, which achieves marked gains in throughput, lowers invocation complexity, and improves system controllability. We also show how hybrid deployments of large language models integrate seamlessly within HAWK, highlighting its flexibility. Finally, we outline future research avenues-hallucination mitigation, real-time performance tuning, and enhanced cross-domain adaptability-and survey prospective applications in healthcare, government, finance, and education.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate and Efficient World Modeling with Masked Latent Transformers</title>
<link>https://arxiv.org/abs/2507.04075</link>
<guid>https://arxiv.org/abs/2507.04075</guid>
<content:encoded><![CDATA[
arXiv:2507.04075v1 Announce Type: new 
Abstract: The Dreamer algorithm has recently obtained remarkable performance across diverse environment domains by training powerful agents with simulated trajectories. However, the compressed nature of its world model's latent space can result in the loss of crucial information, negatively affecting the agent's performance. Recent approaches, such as $\Delta$-IRIS and DIAMOND, address this limitation by training more accurate world models. However, these methods require training agents directly from pixels, which reduces training efficiency and prevents the agent from benefiting from the inner representations learned by the world model. In this work, we propose an alternative approach to world modeling that is both accurate and efficient. We introduce EMERALD (Efficient MaskEd latent tRAnsformer worLD model), a world model using a spatial latent state with MaskGIT predictions to generate accurate trajectories in latent space and improve the agent performance. On the Crafter benchmark, EMERALD achieves new state-of-the-art performance, becoming the first method to surpass human experts performance within 10M environment steps. Our method also succeeds to unlock all 22 Crafter achievements at least once during evaluation.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Train Your LLM Web Agent: A Statistical Diagnosis</title>
<link>https://arxiv.org/abs/2507.04103</link>
<guid>https://arxiv.org/abs/2507.04103</guid>
<content:encoded><![CDATA[
arXiv:2507.04103v1 Announce Type: new 
Abstract: LLM-based web agents have recently made significant progress, but much of it has occurred in closed-source systems, widening the gap with open-source alternatives. Progress has been held back by two key challenges: first, a narrow focus on single-step tasks that overlooks the complexity of multi-step web interactions; and second, the high compute costs required to post-train LLM-based web agents. To address this, we present the first statistically grounded study on compute allocation for LLM web-agent post-training. Our approach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate a Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy reinforcement learning. We find this process highly sensitive to hyperparameter choices, making exhaustive sweeps impractical. To spare others from expensive trial-and-error, we sample 1,370 configurations and use bootstrapping to estimate effective hyperparameters. Our results show that combining SFT with on-policy RL consistently outperforms either approach alone on both WorkArena and MiniWob++. Further, this strategy requires only 55% of the compute to match the peak performance of pure SFT on MiniWob++, effectively pushing the compute-performance Pareto frontier, and is the only strategy that can close the gap with closed-source models.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Robustness of LLM-Driven Multi-Agent Systems through Randomized Smoothing</title>
<link>https://arxiv.org/abs/2507.04105</link>
<guid>https://arxiv.org/abs/2507.04105</guid>
<content:encoded><![CDATA[
arXiv:2507.04105v1 Announce Type: new 
Abstract: This paper presents a defense framework for enhancing the safety of large language model (LLM) empowered multi-agent systems (MAS) in safety-critical domains such as aerospace. We apply randomized smoothing, a statistical robustness certification technique, to the MAS consensus context, enabling probabilistic guarantees on agent decisions under adversarial influence. Unlike traditional verification methods, our approach operates in black-box settings and employs a two-stage adaptive sampling mechanism to balance robustness and computational efficiency. Simulation results demonstrate that our method effectively prevents the propagation of adversarial behaviors and hallucinations while maintaining consensus performance. This work provides a practical and scalable path toward safe deployment of LLM-based MAS in real-world, high-stakes environments.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BYOKG-RAG: Multi-Strategy Graph Retrieval for Knowledge Graph Question Answering</title>
<link>https://arxiv.org/abs/2507.04127</link>
<guid>https://arxiv.org/abs/2507.04127</guid>
<content:encoded><![CDATA[
arXiv:2507.04127v1 Announce Type: new 
Abstract: Knowledge graph question answering (KGQA) presents significant challenges due to the structural and semantic variations across input graphs. Existing works rely on Large Language Model (LLM) agents for graph traversal and retrieval; an approach that is sensitive to traversal initialization, as it is prone to entity linking errors and may not generalize well to custom ("bring-your-own") KGs. We introduce BYOKG-RAG, a framework that enhances KGQA by synergistically combining LLMs with specialized graph retrieval tools. In BYOKG-RAG, LLMs generate critical graph artifacts (question entities, candidate answers, reasoning paths, and OpenCypher queries), and graph tools link these artifacts to the KG and retrieve relevant graph context. The retrieved context enables the LLM to iteratively refine its graph linking and retrieval, before final answer generation. By retrieving context from different graph tools, BYOKG-RAG offers a more general and robust solution for QA over custom KGs. Through experiments on five benchmarks spanning diverse KG types, we demonstrate that BYOKG-RAG outperforms the second-best graph retrieval method by 4.5% points while showing better generalization to custom KGs. BYOKG-RAG framework is open-sourced at https://github.com/awslabs/graphrag-toolkit.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Humanoid Arm Motion via Centroidal Momentum Regularized Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.04140</link>
<guid>https://arxiv.org/abs/2507.04140</guid>
<content:encoded><![CDATA[
arXiv:2507.04140v1 Announce Type: new 
Abstract: Humans naturally swing their arms during locomotion to regulate whole-body dynamics, reduce angular momentum, and help maintain balance. Inspired by this principle, we present a limb-level multi-agent reinforcement learning (RL) framework that enables coordinated whole-body control of humanoid robots through emergent arm motion. Our approach employs separate actor-critic structures for the arms and legs, trained with centralized critics but decentralized actors that share only base states and centroidal angular momentum (CAM) observations, allowing each agent to specialize in task-relevant behaviors through modular reward design. The arm agent guided by CAM tracking and damping rewards promotes arm motions that reduce overall angular momentum and vertical ground reaction moments, contributing to improved balance during locomotion or under external perturbations. Comparative studies with single-agent and alternative multi-agent baselines further validate the effectiveness of our approach. Finally, we deploy the learned policy on a humanoid platform, achieving robust performance across diverse locomotion tasks, including flat-ground walking, rough terrain traversal, and stair climbing.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Two-sided Assortment Optimization: Revenue Maximization</title>
<link>https://arxiv.org/abs/2507.04156</link>
<guid>https://arxiv.org/abs/2507.04156</guid>
<content:encoded><![CDATA[
arXiv:2507.04156v1 Announce Type: new 
Abstract: We study adaptive two-sided assortment optimization for revenue maximization in choice-based matching platforms. The platform has two sides of agents, an initiating side, and a responding side. The decision-maker sequentially selects agents from the initiating side, shows each an assortment of agents from the responding side, and observes their choices. After processing all initiating agents, the responding agents are shown assortments and make their selections. A match occurs when two agents mutually select each other, generating pair-dependent revenue. Choices follow Multinomial Logit (MNL) models. This setting generalizes prior work focused on maximizing the number of matches under submodular demand assumptions, which do not hold in our revenue-maximization context. Our main contribution is the design of polynomial-time approximation algorithms with constant-factor guarantees. In particular, for general pairwise revenues, we develop a randomized algorithm that achieves a $(\frac{1}{2} - \epsilon)$-approximation in expectation for any $\epsilon > 0$. The algorithm is static and provides guarantees under various agent arrival settings, including fixed order, simultaneous processing, and adaptive selection. When revenues are uniform across all pairs involving any given responding-side agent, the guarantee improves to $(1 - \frac{1}{e} - \epsilon)$. In structural settings where responding-side agents share a common revenue-based ranking, we design a simpler adaptive deterministic algorithm achieving a $\frac{1}{2}$-approximation. Our approach leverages novel linear programming relaxations, correlation gap arguments, and structural properties of the revenue functions.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Evaluation of VR-Enabled Robots and Human Operators for Targeted Disease Management in Vineyards</title>
<link>https://arxiv.org/abs/2507.04167</link>
<guid>https://arxiv.org/abs/2507.04167</guid>
<content:encoded><![CDATA[
arXiv:2507.04167v1 Announce Type: new 
Abstract: This study explores the use of immersive virtual reality (VR) as a control interface for agricultural robots in vineyard disease detection and treatment. Using a Unity-ROS simulation, it compares three agents: a human operator, an immersive VR-controlled robot, and a non-immersive VR-controlled robot. During the scanning phase, humans perform best due to agility and control speed. However, in the treatment phase, immersive VR robots outperform others, completing tasks up to 65% faster by using stored infection data and optimized path planning. In yield-map-based navigation, immersive robots are also 38% faster than humans. Despite slower performance in manual scanning tasks, immersive VR excels in memory-guided, repetitive operations. The study highlights the role of interface design and path optimization, noting limitations in simulation fidelity and generalizability. It concludes that immersive VR has strong potential to enhance efficiency and precision in precision agriculture.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gathering Teams of Bounded Memory Agents on a Line</title>
<link>https://arxiv.org/abs/2507.04172</link>
<guid>https://arxiv.org/abs/2507.04172</guid>
<content:encoded><![CDATA[
arXiv:2507.04172v1 Announce Type: new 
Abstract: Several mobile agents, modelled as deterministic automata, navigate in an infinite line in synchronous rounds. All agents start in the same round. In each round, an agent can move to one of the two neighboring nodes, or stay idle. Agents have distinct labels which are integers from the set $\{1,\dots, L\}$. They start in teams, and all agents in a team have the same starting node. The adversary decides the compositions of teams, and their starting nodes. Whenever an agent enters a node, it sees the entry port number and the states of all collocated agents; this information forms the input of the agent on the basis of which it transits to the next state and decides the current action. The aim is for all agents to gather at the same node and stop. Gathering is feasible, if this task can be accomplished for any decisions of the adversary, and its time is the worst-case number of rounds from the start till gathering.
  We consider the feasibility and time complexity of gathering teams of agents, and give a complete solution of this problem. It turns out that both feasibility and complexity of gathering depend on the sizes of teams. We first concentrate on the case when all teams have the same size $x$. For the oriented line, gathering is impossible if $x=1$, and it can be accomplished in time $O(D)$, for $x>1$, where $D$ is the distance between the starting nodes of the most distant teams. This complexity is of course optimal. For the unoriented line, the situation is different. For $x=1$, gathering is also impossible, but for $x=2$, the optimal time of gathering is $\Theta(D\log L)$, and for $x\geq 3$, the optimal time of gathering is $\Theta(D)$. In the case when there are teams of different sizes, we show that gathering is always possible in time $O(D)$, even for the unoriented line. This complexity is of course optimal.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hijacking JARVIS: Benchmarking Mobile GUI Agents against Unprivileged Third Parties</title>
<link>https://arxiv.org/abs/2507.04227</link>
<guid>https://arxiv.org/abs/2507.04227</guid>
<content:encoded><![CDATA[
arXiv:2507.04227v1 Announce Type: new 
Abstract: Mobile GUI agents are designed to autonomously execute diverse device-control tasks by interpreting and interacting with mobile screens. Despite notable advancements, their resilience in real-world scenarios where screen content may be partially manipulated by untrustworthy third parties remains largely unexplored. Owing to their black-box and autonomous nature, these agents are vulnerable to manipulations that could compromise user devices. In this work, we present the first systematic investigation into the vulnerabilities of mobile GUI agents. We introduce a scalable attack simulation framework AgentHazard, which enables flexible and targeted modifications of screen content within existing applications. Leveraging this framework, we develop a comprehensive benchmark suite comprising both a dynamic task execution environment and a static dataset of vision-language-action tuples, totaling over 3,000 attack scenarios. The dynamic environment encompasses 58 reproducible tasks in an emulator with various types of hazardous UI content, while the static dataset is constructed from 210 screenshots collected from 14 popular commercial apps. Importantly, our content modifications are designed to be feasible for unprivileged third parties. We evaluate 7 widely-used mobile GUI agents and 5 common backbone models using our benchmark. Our findings reveal that all examined agents are significantly influenced by misleading third-party content (with an average misleading rate of 28.8% in human-crafted attack scenarios) and that their vulnerabilities are closely linked to the employed perception modalities and backbone LLMs. Furthermore, we assess training-based mitigation strategies, highlighting both the challenges and opportunities for enhancing the robustness of mobile GUI agents. Our code and data will be released at https://agenthazard.github.io.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SRefiner: Soft-Braid Attention for Multi-Agent Trajectory Refinement</title>
<link>https://arxiv.org/abs/2507.04263</link>
<guid>https://arxiv.org/abs/2507.04263</guid>
<content:encoded><![CDATA[
arXiv:2507.04263v1 Announce Type: new 
Abstract: Accurate prediction of multi-agent future trajectories is crucial for autonomous driving systems to make safe and efficient decisions. Trajectory refinement has emerged as a key strategy to enhance prediction accuracy. However, existing refinement methods often overlook the topological relationships between trajectories, which are vital for improving prediction precision. Inspired by braid theory, we propose a novel trajectory refinement approach, Soft-Braid Refiner (SRefiner), guided by the soft-braid topological structure of trajectories using Soft-Braid Attention. Soft-Braid Attention captures spatio-temporal topological relationships between trajectories by considering both spatial proximity and vehicle motion states at ``soft intersection points". Additionally, we extend this approach to model interactions between trajectories and lanes, further improving the prediction accuracy. SRefiner is a multi-iteration, multi-agent framework that iteratively refines trajectories, incorporating topological information to enhance interactions within traffic scenarios. SRefiner achieves significant performance improvements over four baseline methods across two datasets, establishing a new state-of-the-art in trajectory refinement. Code is here https://github.com/Liwen-Xiao/SRefiner.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wavelet Policy: Lifting Scheme for Policy Learning in Long-Horizon Tasks</title>
<link>https://arxiv.org/abs/2507.04331</link>
<guid>https://arxiv.org/abs/2507.04331</guid>
<content:encoded><![CDATA[
arXiv:2507.04331v1 Announce Type: new 
Abstract: Policy learning focuses on devising strategies for agents in embodied artificial intelligence systems to perform optimal actions based on their perceived states. One of the key challenges in policy learning involves handling complex, long-horizon tasks that require managing extensive sequences of actions and observations with multiple modes. Wavelet analysis offers significant advantages in signal processing, notably in decomposing signals at multiple scales to capture both global trends and fine-grained details. In this work, we introduce a novel wavelet policy learning framework that utilizes wavelet transformations to enhance policy learning. Our approach leverages learnable multi-scale wavelet decomposition to facilitate detailed observation analysis and robust action planning over extended sequences. We detail the design and implementation of our wavelet policy, which incorporates lifting schemes for effective multi-resolution analysis and action generation. This framework is evaluated across multiple complex scenarios, including robotic manipulation, self-driving, and multi-robot collaboration, demonstrating the effectiveness of our method in improving the precision and reliability of the learned policy.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebSynthesis: World-Model-Guided MCTS for Efficient WebUI-Trajectory Synthesis</title>
<link>https://arxiv.org/abs/2507.04370</link>
<guid>https://arxiv.org/abs/2507.04370</guid>
<content:encoded><![CDATA[
arXiv:2507.04370v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have significantly improved the capabilities of web agents. However, effectively navigating complex and dynamic web environments still requires more advanced trajectory-level planning and execution. Prior studies have addressed self-improving agents by collecting extensive GUI trajectories from real-environment interactions. Despite their effectiveness, these approaches encounter two critical challenges: (1) Uncontrollable environment states, where real or sandboxed web environments often yield unstable and non-deterministic feedback, complicating the reproduction and debugging of agent behaviors; and (2) High API costs, as generating even a single interaction trajectory can involve hundreds of queries, leading to considerable API usage and computational expenses. To address these limitations and enable scalable self-improvement for agents, we propose WebSynthesis, a novel framework for trajectory synthesis and training. WebSynthesis leverages a learned world model to simulate virtual web environments, allowing a policy agent to perform efficient and reversible tree-based planning. This approach supports the large-scale generation of diverse and high-quality trajectories, which are subsequently utilized to refine the agent's policy. Experimental results demonstrate that an agent trained using WebSynthesis on a small-scale synthetic dataset achieves performance comparable to or even surpassing that of models trained on large-scale real-world data.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Malware Detection using Sequential Feature Selection: A Dueling Double Deep Q-Network (D3QN) Framework for Intelligent Classification</title>
<link>https://arxiv.org/abs/2507.04372</link>
<guid>https://arxiv.org/abs/2507.04372</guid>
<content:encoded><![CDATA[
arXiv:2507.04372v1 Announce Type: new 
Abstract: Traditional malware detection methods exhibit computational inefficiency due to exhaustive feature extraction requirements, creating accuracy-efficiency trade-offs that limit real-time deployment. We formulate malware classification as a Markov Decision Process with episodic feature acquisition and propose a Dueling Double Deep Q-Network (D3QN) framework for adaptive sequential feature selection. The agent learns to dynamically select informative features per sample before terminating with classification decisions, optimizing both detection accuracy and computational cost through reinforcement learning.
  We evaluate our approach on Microsoft Big2015 (9-class, 1,795 features) and BODMAS (binary, 2,381 features) datasets. D3QN achieves 99.22% and 98.83% accuracy while utilizing only 61 and 56 features on average, representing 96.6% and 97.6% dimensionality reduction. This yields computational efficiency improvements of 30.1x and 42.5x over traditional ensemble methods. Comprehensive ablation studies demonstrate consistent superiority over Random Forest, XGBoost, and static feature selection approaches.
  Quantitative analysis demonstrates that D3QN learns non-random feature selection policies with 62.5% deviation from uniform baseline distributions. The learned policies exhibit structured hierarchical preferences, utilizing high-level metadata features for initial assessment while selectively incorporating detailed behavioral features based on classification uncertainty. Feature specialization analysis reveals 57.7% of examined features demonstrate significant class-specific discrimination patterns. Our results validate reinforcement learning-based sequential feature selection for malware classification, achieving superior accuracy with substantial computational reduction through learned adaptive policies.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOD-X: A Modular Open Decentralized eXchange Framework proposal for Heterogeneous Interoperable Artificial Agents</title>
<link>https://arxiv.org/abs/2507.04376</link>
<guid>https://arxiv.org/abs/2507.04376</guid>
<content:encoded><![CDATA[
arXiv:2507.04376v1 Announce Type: new 
Abstract: As Artificial Intelligence systems evolve from monolithic models to ecosystems of specialized agents, the need for standardized communication protocols becomes increasingly critical. This paper introduces MOD-X (Modular Open Decentralized eXchange), a novel architectural framework proposal for agent interoperability that addresses key limitations of existing protocols. Unlike current approaches, MOD-X proposes a layered architecture with a Universal Message Bus, thorough state management, translation capabilities, and blockchain-based security mechanisms. We present MOD-X's architecture, compare it with existing protocols, and demonstrate its application through a worked example how it enables integration between heterogeneous specialist agents (agents with different architectures, vendors, capabilities, and knowledge representations--including rule-based systems, neural networks, symbolic reasoning engines, and legacy software with agent wrappers). MOD-X's key innovations include a publish-subscribe communication model, semantic capability discovery, and dynamic workflow orchestration--providing a framework that bridges theoretical formalism with practical implementation. This architecture addresses the growing need for truly decentralized, interoperable agent ecosystems that can scale effectively without the need for central coordination.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Reinforcement Learning using Revealed Preferences and Passive Stochastic Optimization</title>
<link>https://arxiv.org/abs/2507.04396</link>
<guid>https://arxiv.org/abs/2507.04396</guid>
<content:encoded><![CDATA[
arXiv:2507.04396v1 Announce Type: new 
Abstract: This monograph, spanning three chapters, explores Inverse Reinforcement Learning (IRL). The first two chapters view inverse reinforcement learning (IRL) through the lens of revealed preferences from microeconomics while the third chapter studies adaptive IRL via Langevin dynamics stochastic gradient algorithms.
  Chapter uses classical revealed preference theory (Afriat's theorem and extensions) to identify constrained utility maximizers based on observed agent actions. This allows for the reconstruction of set-valued estimates of an agent's utility. We illustrate this procedure by identifying the presence of a cognitive radar and reconstructing its utility function. The chapter also addresses the construction of a statistical detector for utility maximization behavior when agent actions are corrupted by noise.
  Chapter 2 studies Bayesian IRL. It investigates how an analyst can determine if an observed agent is a rationally inattentive Bayesian utility maximizer (i.e., simultaneously optimizing its utility and observation likelihood). The chapter discusses inverse stopping-time problems, focusing on reconstructing the continuation and stopping costs of a Bayesian agent operating over a random horizon. We then apply this IRL methodology to identify the presence of a Bayes-optimal sequential detector. Additionally, Chapter 2 provides a concise overview of discrete choice models, inverse Bayesian filtering, and inverse stochastic gradient algorithms for adaptive IRL.
  Finally, Chapter 3 introduces an adaptive IRL approach utilizing passive Langevin dynamics. This method aims to track time-varying utility functions given noisy and misspecified gradients. In essence, the adaptive IRL algorithms presented in Chapter 3 can be conceptualized as inverse stochastic gradient algorithms, as they learn the utility function in real-time while a stochastic gradient algorithm is in operation.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimedia Verification Through Multi-Agent Deep Research Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2507.04410</link>
<guid>https://arxiv.org/abs/2507.04410</guid>
<content:encoded><![CDATA[
arXiv:2507.04410v1 Announce Type: new 
Abstract: This paper presents our submission to the ACMMM25 - Grand Challenge on Multimedia Verification. We developed a multi-agent verification system that combines Multimodal Large Language Models (MLLMs) with specialized verification tools to detect multimedia misinformation. Our system operates through six stages: raw data processing, planning, information extraction, deep research, evidence collection, and report generation. The core Deep Researcher Agent employs four tools: reverse image search, metadata analysis, fact-checking databases, and verified news processing that extracts spatial, temporal, attribution, and motivational context. We demonstrate our approach on a challenge dataset sample involving complex multimedia content. Our system successfully verified content authenticity, extracted precise geolocation and timing information, and traced source attribution across multiple platforms, effectively addressing real-world multimedia verification scenarios.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOMENTS: A Comprehensive Multimodal Benchmark for Theory of Mind</title>
<link>https://arxiv.org/abs/2507.04415</link>
<guid>https://arxiv.org/abs/2507.04415</guid>
<content:encoded><![CDATA[
arXiv:2507.04415v1 Announce Type: new 
Abstract: Understanding Theory of Mind is essential for building socially intelligent multimodal agents capable of perceiving and interpreting human behavior. We introduce MOMENTS (Multimodal Mental States), a comprehensive benchmark designed to assess the ToM capabilities of multimodal large language models (LLMs) through realistic, narrative-rich scenarios presented in short films. MOMENTS includes over 2,344 multiple-choice questions spanning seven distinct ToM categories. The benchmark features long video context windows and realistic social interactions that provide deeper insight into characters' mental states. While the visual modality generally enhances model performance, current systems still struggle to integrate it effectively, underscoring the need for further research into AI's multimodal understanding of human behavior.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Hi AirStar, Guide Me to the Badminton Court."</title>
<link>https://arxiv.org/abs/2507.04430</link>
<guid>https://arxiv.org/abs/2507.04430</guid>
<content:encoded><![CDATA[
arXiv:2507.04430v1 Announce Type: new 
Abstract: Unmanned Aerial Vehicles, operating in environments with relatively few obstacles, offer high maneuverability and full three-dimensional mobility. This allows them to rapidly approach objects and perform a wide range of tasks often challenging for ground robots, making them ideal for exploration, inspection, aerial imaging, and everyday assistance. In this paper, we introduce AirStar, a UAV-centric embodied platform that turns a UAV into an intelligent aerial assistant: a large language model acts as the cognitive core for environmental understanding, contextual reasoning, and task planning. AirStar accepts natural interaction through voice commands and gestures, removing the need for a remote controller and significantly broadening its user base. It combines geospatial knowledge-driven long-distance navigation with contextual reasoning for fine-grained short-range control, resulting in an efficient and accurate vision-and-language navigation (VLN) capability.Furthermore, the system also offers built-in capabilities such as cross-modal question answering, intelligent filming, and target tracking. With a highly extensible framework, it supports seamless integration of new functionalities, paving the way toward a general-purpose, instruction-driven intelligent UAV agent. The supplementary PPT is available at \href{https://buaa-colalab.github.io/airstar.github.io}{https://buaa-colalab.github.io/airstar.github.io}.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Distributed Computing</title>
<link>https://arxiv.org/abs/2507.04459</link>
<guid>https://arxiv.org/abs/2507.04459</guid>
<content:encoded><![CDATA[
arXiv:2507.04459v1 Announce Type: new 
Abstract: The most celebrated and extensively studied model of distributed computing is the {\em message-passing model,} in which each vertex/node of the (distributed network) graph corresponds to a static computational device that communicates with other devices through passing messages. In this paper, we consider the {\em agentic model} of distributed computing which extends the message-passing model in a new direction. In the agentic model, computational devices are modeled as relocatable or mobile computational devices (called agents in this paper), i.e., each vertex/node of the graph serves as a container for the devices, and hence communicating with another device requires relocating to the same node. We study two fundamental graph level tasks, leader election, and minimum spanning tree, in the agentic model, which will enhance our understanding of distributed computation across paradigms. The objective is to minimize both time and memory complexities. Following the literature, we consider the synchronous setting in which each agent performs its operations synchronously with others, and hence the time complexity can be measured in rounds. In this paper, we present two deterministic algorithms for leader election: one for the case of $k<n$ and another for the case of $k=n$, minimizing both time and memory complexities, where $k$ and $n$, respectively, are the number of agents and number of nodes of the graph. Using these leader election results, we develop deterministic algorithms for agents to construct a minimum spanning tree of the graph, minimizing both time and memory complexities. To the best of our knowledge, this is the first study of distributed graph level tasks in the agentic model with $k\leq n$. Previous studies only considered the case of $k=n$.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constant-Approximate and Constant-Strategyproof Two-Facility Location</title>
<link>https://arxiv.org/abs/2507.04485</link>
<guid>https://arxiv.org/abs/2507.04485</guid>
<content:encoded><![CDATA[
arXiv:2507.04485v1 Announce Type: new 
Abstract: We study deterministic mechanisms for the two-facility location problem. Given the reported locations of n agents on the real line, such a mechanism specifies where to build the two facilities. The single-facility variant of this problem admits a simple strategyproof mechanism that minimizes social cost. For two facilities, however, it is known that any strategyproof mechanism is $\Omega(n)$-approximate. We seek to circumvent this strong lower bound by relaxing the problem requirements. Following other work in the facility location literature, we consider a relaxed form of strategyproofness in which no agent can lie and improve their outcome by more than a constant factor. Because the aforementioned $\Omega(n)$ lower bound generalizes easily to constant-strategyproof mechanisms, we introduce a second relaxation: Allowing the facilities (but not the agents) to be located in the plane. Our first main result is a natural mechanism for this relaxation that is constant-approximate and constant-strategyproof. A characteristic of this mechanism is that a small change in the input profile can produce a large change in the solution. Motivated by this observation, and also by results in the facility reallocation literature, our second main result is a constant-approximate, constant-strategyproof, and Lipschitz continuous mechanism.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grounded Gesture Generation: Language, Motion, and Space</title>
<link>https://arxiv.org/abs/2507.04522</link>
<guid>https://arxiv.org/abs/2507.04522</guid>
<content:encoded><![CDATA[
arXiv:2507.04522v1 Announce Type: new 
Abstract: Human motion generation has advanced rapidly in recent years, yet the critical problem of creating spatially grounded, context-aware gestures has been largely overlooked. Existing models typically specialize either in descriptive motion generation, such as locomotion and object interaction, or in isolated co-speech gesture synthesis aligned with utterance semantics. However, both lines of work often treat motion and environmental grounding separately, limiting advances toward embodied, communicative agents. To address this gap, our work introduces a multimodal dataset and framework for grounded gesture generation, combining two key resources: (1) a synthetic dataset of spatially grounded referential gestures, and (2) MM-Conv, a VR-based dataset capturing two-party dialogues. Together, they provide over 7.7 hours of synchronized motion, speech, and 3D scene information, standardized in the HumanML3D format. Our framework further connects to a physics-based simulator, enabling synthetic data generation and situated evaluation. By bridging gesture modeling and spatial grounding, our contribution establishes a foundation for advancing research in situated gesture generation and grounded multimodal interaction.
  Project page: https://groundedgestures.github.io/
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Greedy Dynamic Matching</title>
<link>https://arxiv.org/abs/2507.04551</link>
<guid>https://arxiv.org/abs/2507.04551</guid>
<content:encoded><![CDATA[
arXiv:2507.04551v1 Announce Type: new 
Abstract: We study a foundational model of dynamic matching market with abandonment. This model has been studied by Collina et al (2020) and Aouad and Saritac (2022), and many other papers have considered special cases. We compare the performance of greedy policies -- which identify a set of "acceptable" matches up front, and perform these matches as soon as possible -- to that of an omniscient benchmark which knows the full arrival and departure sequence.
  We use a novel family of linear programs ($LP^{ALG}$) to identify which greedy policy to follow. We show that the value of $LP^ALG$ is a *lower bound* on the value of the greedy policy that it identifies in two settings of interest:
  -When all types have the same departure rate.
  -The bipartite case where types on the same side of the market have the same departure rate.
  The proofs of these results use a new result (Lemma 1), which relates the *probability* that at least one agent from a set of types is present in the system to the expected number of such agents.
  We also show that the value of $LP^{ALG}$ is at least 1/2 of the reward rate earned by the omniscient policy (Proposition 4). Therefore, for both settings above, our greedy policy provably earns at least half of the omniscient reward rate. This improves upon the bound of 1/8 from Collina (2020). In both settings our competitive ratio of 1/2 is the best possible: no online policy can provide a better guarantee (Theorem 2).
  To show these results we introduce a new linear program that upper bounds the objective value of the omniscient policy (Proposition 3). This improves upon the upper bounds presented by Collina et al (2020) and Kessel et al (2022).
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and Visual Documents</title>
<link>https://arxiv.org/abs/2507.04590</link>
<guid>https://arxiv.org/abs/2507.04590</guid>
<content:encoded><![CDATA[
arXiv:2507.04590v1 Announce Type: new 
Abstract: Multimodal embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering over different modalities. However, existing multimodal embeddings like VLM2Vec, E5-V, GME are predominantly focused on natural images, with limited support for other visual forms such as videos and visual documents. This restricts their applicability in real-world scenarios, including AI agents, multi-modal search and recommendation, and retrieval-augmented generation (RAG). To close this gap, we propose VLM2Vec-V2, a unified framework for learning embeddings across diverse visual forms. First, we introduce MMEB-V2, a comprehensive benchmark that extends MMEB with five new task types: visual document retrieval, video retrieval, temporal grounding, video classification and video question answering - spanning text, image, video, and visual document inputs. Next, we train VLM2Vec-V2, a general-purpose embedding model that supports text, image, video, and visual document inputs. Extensive experiments show that VLM2Vec-V2 achieves strong performance not only on the newly introduced video and document retrieval tasks, but also improves over prior baselines on the original image benchmarks. Through extensive evaluation, our study offers insights into the generalizability of various multimodal embedding models and highlights effective strategies for unified embedding learning, laying the groundwork for more scalable and adaptable representation learning in both research and real-world settings.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LTMSformer: A Local Trend-Aware Attention and Motion State Encoding Transformer for Multi-Agent Trajectory Prediction</title>
<link>https://arxiv.org/abs/2507.04634</link>
<guid>https://arxiv.org/abs/2507.04634</guid>
<content:encoded><![CDATA[
arXiv:2507.04634v1 Announce Type: new 
Abstract: It has been challenging to model the complex temporal-spatial dependencies between agents for trajectory prediction. As each state of an agent is closely related to the states of adjacent time steps, capturing the local temporal dependency is beneficial for prediction, while most studies often overlook it. Besides, learning the high-order motion state attributes is expected to enhance spatial interaction modeling, but it is rarely seen in previous works. To address this, we propose a lightweight framework, LTMSformer, to extract temporal-spatial interaction features for multi-modal trajectory prediction. Specifically, we introduce a Local Trend-Aware Attention mechanism to capture the local temporal dependency by leveraging a convolutional attention mechanism with hierarchical local time boxes. Next, to model the spatial interaction dependency, we build a Motion State Encoder to incorporate high-order motion state attributes, such as acceleration, jerk, heading, etc. To further refine the trajectory prediction, we propose a Lightweight Proposal Refinement Module that leverages Multi-Layer Perceptrons for trajectory embedding and generates the refined trajectories with fewer model parameters. Experiment results on the Argoverse 1 dataset demonstrate that our method outperforms the baseline HiVT-64, reducing the minADE by approximately 4.35%, the minFDE by 8.74%, and the MR by 20%. We also achieve higher accuracy than HiVT-128 with a 68% reduction in model size.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UrbanMind: Towards Urban General Intelligence via Tool-Enhanced Retrieval-Augmented Generation and Multilevel Optimization</title>
<link>https://arxiv.org/abs/2507.04706</link>
<guid>https://arxiv.org/abs/2507.04706</guid>
<content:encoded><![CDATA[
arXiv:2507.04706v1 Announce Type: new 
Abstract: Urban general intelligence (UGI) refers to the capacity of AI systems to autonomously perceive, reason, and act within dynamic and complex urban environments. In this paper, we introduce UrbanMind, a tool-enhanced retrieval-augmented generation (RAG) framework designed to facilitate UGI. Central to UrbanMind is a novel architecture based on Continual Retrieval-Augmented MoE-based LLM (C-RAG-LLM), which dynamically incorporates domain-specific knowledge and evolving urban data to support long-term adaptability. The architecture of C-RAG-LLM aligns naturally with a multilevel optimization framework, where different layers are treated as interdependent sub-problems. Each layer has distinct objectives and can be optimized either independently or jointly through a hierarchical learning process. The framework is highly flexible, supporting both end-to-end training and partial layer-wise optimization based on resource or deployment constraints. To remain adaptive under data drift, it is further integrated with an incremental corpus updating mechanism. Evaluations on real-world urban tasks of a variety of complexity verify the effectiveness of the proposed framework. This work presents a promising step toward the realization of general-purpose LLM agents in future urban environments.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who's the Mole? Modeling and Detecting Intention-Hiding Malicious Agents in LLM-Based Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2507.04724</link>
<guid>https://arxiv.org/abs/2507.04724</guid>
<content:encoded><![CDATA[
arXiv:2507.04724v1 Announce Type: new 
Abstract: Multi-agent systems powered by Large Language Models (LLM-MAS) demonstrate remarkable capabilities in collaborative problem-solving. While LLM-MAS exhibit strong collaborative abilities, the security risks in their communication and coordination remain underexplored. We bridge this gap by systematically investigating intention-hiding threats in LLM-MAS, and design four representative attack paradigms that subtly disrupt task completion while maintaining high concealment. These attacks are evaluated in centralized, decentralized, and layered communication structures. Experiments conducted on six benchmark datasets, including MMLU, MMLU-Pro, HumanEval, GSM8K, arithmetic, and biographies, demonstrate that they exhibit strong disruptive capabilities. To identify these threats, we propose a psychology-based detection framework AgentXposed, which combines the HEXACO personality model with the Reid Technique, using progressive questionnaire inquiries and behavior-based monitoring. Experiments conducted on six types of attacks show that our detection framework effectively identifies all types of malicious behaviors. The detection rate for our intention-hiding attacks is slightly lower than that of the two baselines, Incorrect Fact Injection and Dark Traits Injection, demonstrating the effectiveness of intention concealment. Our findings reveal the structural and behavioral risks posed by intention-hiding attacks and offer valuable insights into securing LLM-based multi-agent systems through psychological perspectives, which contributes to a deeper understanding of multi-agent safety. The code and data are available at https://anonymous.4open.science/r/AgentXposed-F814.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustifying 3D Perception through Least-Squares Multi-Agent Graphs Object Tracking</title>
<link>https://arxiv.org/abs/2507.04762</link>
<guid>https://arxiv.org/abs/2507.04762</guid>
<content:encoded><![CDATA[
arXiv:2507.04762v1 Announce Type: new 
Abstract: The critical perception capabilities of EdgeAI systems, such as autonomous vehicles, are required to be resilient against adversarial threats, by enabling accurate identification and localization of multiple objects in the scene over time, mitigating their impact. Single-agent tracking offers resilience to adversarial attacks but lacks situational awareness, underscoring the need for multi-agent cooperation to enhance context understanding and robustness. This paper proposes a novel mitigation framework on 3D LiDAR scene against adversarial noise by tracking objects based on least-squares graph on multi-agent adversarial bounding boxes. Specifically, we employ the least-squares graph tool to reduce the induced positional error of each detection's centroid utilizing overlapped bounding boxes on a fully connected graph via differential coordinates and anchor points. Hence, the multi-vehicle detections are fused and refined mitigating the adversarial impact, and associated with existing tracks in two stages performing tracking to further suppress the adversarial threat. An extensive evaluation study on the real-world V2V4Real dataset demonstrates that the proposed method significantly outperforms both state-of-the-art single and multi-agent tracking frameworks by up to 23.3% under challenging adversarial conditions, operating as a resilient approach without relying on additional defense mechanisms.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FurniMAS: Language-Guided Furniture Decoration using Multi-Agent System</title>
<link>https://arxiv.org/abs/2507.04770</link>
<guid>https://arxiv.org/abs/2507.04770</guid>
<content:encoded><![CDATA[
arXiv:2507.04770v1 Announce Type: new 
Abstract: Furniture decoration is an important task in various industrial applications. However, achieving a high-quality decorative result is often time-consuming and requires specialized artistic expertise. To tackle these challenges, we explore how multi-agent systems can assist in automating the decoration process. We propose FurniMAS, a multi-agent system for automatic furniture decoration. Specifically, given a human prompt and a household furniture item such as a working desk or a TV stand, our system suggests relevant assets with appropriate styles and materials, and arranges them on the item, ensuring the decorative result meets functionality, aesthetic, and ambiance preferences. FurniMAS assembles a hybrid team of LLM-based and non-LLM agents, each fulfilling distinct roles in a typical decoration project. These agents collaborate through communication, logical reasoning, and validation to transform the requirements into the final outcome. Extensive experiments demonstrate that our FurniMAS significantly outperforms other baselines in generating high-quality 3D decor.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training-free Generation of Temporally Consistent Rewards from VLMs</title>
<link>https://arxiv.org/abs/2507.04789</link>
<guid>https://arxiv.org/abs/2507.04789</guid>
<content:encoded><![CDATA[
arXiv:2507.04789v1 Announce Type: new 
Abstract: Recent advances in vision-language models (VLMs) have significantly improved performance in embodied tasks such as goal decomposition and visual comprehension. However, providing accurate rewards for robotic manipulation without fine-tuning VLMs remains challenging due to the absence of domain-specific robotic knowledge in pre-trained datasets and high computational costs that hinder real-time applicability. To address this, we propose $\mathrm{T}^2$-VLM, a novel training-free, temporally consistent framework that generates accurate rewards through tracking the status changes in VLM-derived subgoals. Specifically, our method first queries the VLM to establish spatially aware subgoals and an initial completion estimate before each round of interaction. We then employ a Bayesian tracking algorithm to update the goal completion status dynamically, using subgoal hidden states to generate structured rewards for reinforcement learning (RL) agents. This approach enhances long-horizon decision-making and improves failure recovery capabilities with RL. Extensive experiments indicate that $\mathrm{T}^2$-VLM achieves state-of-the-art performance in two robot manipulation benchmarks, demonstrating superior reward accuracy with reduced computation consumption. We believe our approach not only advances reward generation techniques but also contributes to the broader field of embodied AI. Project website: https://t2-vlm.github.io/.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interaction-Merged Motion Planning: Effectively Leveraging Diverse Motion Datasets for Robust Planning</title>
<link>https://arxiv.org/abs/2507.04790</link>
<guid>https://arxiv.org/abs/2507.04790</guid>
<content:encoded><![CDATA[
arXiv:2507.04790v1 Announce Type: new 
Abstract: Motion planning is a crucial component of autonomous robot driving. While various trajectory datasets exist, effectively utilizing them for a target domain remains challenging due to differences in agent interactions and environmental characteristics. Conventional approaches, such as domain adaptation or ensemble learning, leverage multiple source datasets but suffer from domain imbalance, catastrophic forgetting, and high computational costs. To address these challenges, we propose Interaction-Merged Motion Planning (IMMP), a novel approach that leverages parameter checkpoints trained on different domains during adaptation to the target domain. IMMP follows a two-step process: pre-merging to capture agent behaviors and interactions, sufficiently extracting diverse information from the source domain, followed by merging to construct an adaptable model that efficiently transfers diverse interactions to the target domain. Our method is evaluated on various planning benchmarks and models, demonstrating superior performance compared to conventional approaches.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MARBLE: A Multi-Agent Rule-Based LLM Reasoning Engine for Accident Severity Prediction</title>
<link>https://arxiv.org/abs/2507.04893</link>
<guid>https://arxiv.org/abs/2507.04893</guid>
<content:encoded><![CDATA[
arXiv:2507.04893v1 Announce Type: new 
Abstract: Accident severity prediction plays a critical role in transportation safety systems but is a persistently difficult task due to incomplete data, strong feature dependencies, and severe class imbalance in which rare but high-severity cases are underrepresented and hard to detect. Existing methods often rely on monolithic models or black box prompting, which struggle to scale in noisy, real-world settings and offer limited interpretability. To address these challenges, we propose MARBLE a multiagent rule based LLM engine that decomposes the severity prediction task across a team of specialized reasoning agents, including an interchangeable ML-backed agent. Each agent focuses on a semantic subset of features (e.g., spatial, environmental, temporal), enabling scoped reasoning and modular prompting without the risk of prompt saturation. Predictions are coordinated through either rule-based or LLM-guided consensus mechanisms that account for class rarity and confidence dynamics. The system retains structured traces of agent-level reasoning and coordination outcomes, supporting in-depth interpretability and post-hoc performance diagnostics. Across both UK and US datasets, MARBLE consistently outperforms traditional machine learning classifiers and state-of-the-art (SOTA) prompt-based reasoning methods including Chain-of-Thought (CoT), Least-to-Most (L2M), and Tree-of-Thought (ToT) achieving nearly 90% accuracy where others plateau below 48%. This performance redefines the practical ceiling for accident severity classification under real world noise and extreme class imbalance. Our results position MARBLE as a generalizable and interpretable framework for reasoning under uncertainty in safety-critical applications.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leadership Detection via Time-Lagged Correlation-Based Network Inference</title>
<link>https://arxiv.org/abs/2507.04917</link>
<guid>https://arxiv.org/abs/2507.04917</guid>
<content:encoded><![CDATA[
arXiv:2507.04917v1 Announce Type: new 
Abstract: Understanding leadership dynamics in collective behavior is a key challenge in animal ecology, swarm robotics, and intelligent transportation. Traditional information-theoretic approaches, including Transfer Entropy (TE) and Time-Lagged Mutual Information (TLMI), have been widely used to infer leader-follower relationships but face critical limitations in noisy or short-duration datasets due to their reliance on robust probability estimations. This study proposes a method based on dynamic network inference using time-lagged correlations across multiple kinematic variables: velocity, acceleration, and direction. Our approach constructs directed influence graphs over time, enabling the identification of leadership patterns without the need for large volumes of data or parameter-sensitive discretization. We validate our method through two multi-agent simulations in NetLogo: a modified Vicsek model with informed leaders and a predator-prey model featuring coordinated and independent wolf groups. Experimental results demonstrate that the network-based method outperforms TE and TLMI in scenarios with limited spatiotemporal observations, ranking true leaders at the top of influence metrics more consistently than TE and TLMI.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility Systems</title>
<link>https://arxiv.org/abs/2507.04996</link>
<guid>https://arxiv.org/abs/2507.04996</guid>
<content:encoded><![CDATA[
arXiv:2507.04996v1 Announce Type: new 
Abstract: Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity to operate according to internal rules without external control. Accordingly, autonomous vehicles (AuVs) are defined as systems capable of perceiving their environment and executing preprogrammed tasks independently of external input. However, both research and real-world deployments increasingly showcase vehicles that demonstrate behaviors beyond this definition (including the SAE levels 1 to 6), such as interaction with humans and machines, goal adaptation, contextual reasoning, external tool use, and long-term planning, particularly with the integration of large language models (LLMs) and agentic AI systems. These developments reveal a conceptual gap between technical autonomy and the broader cognitive and social capabilities needed for future human-centered mobility systems. To address this, we introduce the concept of agentic vehicles (AgVs), referring to vehicles that integrate agentic AI to reason, adapt, and interact within complex environments. This paper presents a systems-level framework to characterize AgVs, focusing on their cognitive and communicative layers and differentiating them from conventional AuVs. It synthesizes relevant advances in agentic AI, robotics, multi-agent systems, and human-machine interaction, and highlights how agentic AI, through high-level reasoning and tool use, can function not merely as computational tools but as interactive agents embedded in mobility ecosystems. The paper concludes by identifying key challenges in the development and governance of AgVs, including safety, real-time control, public acceptance, ethical alignment, and regulatory frameworks.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perspectives on How Sociology Can Advance Theorizing about Human-Chatbot Interaction and Developing Chatbots for Social Good</title>
<link>https://arxiv.org/abs/2507.05030</link>
<guid>https://arxiv.org/abs/2507.05030</guid>
<content:encoded><![CDATA[
arXiv:2507.05030v1 Announce Type: new 
Abstract: Recently, research into chatbots (also known as conversational agents, AI agents, voice assistants), which are computer applications using artificial intelligence to mimic human-like conversation, has grown sharply. Despite this growth, sociology lags other disciplines (including computer science, medicine, psychology, and communication) in publishing about chatbots. We suggest sociology can advance understanding of human-chatbot interaction and offer four sociological theories to enhance extant work in this field. The first two theories (resource substitution theory, power-dependence theory) add new insights to existing models of the drivers of chatbot use, which overlook sociological concerns about how social structure (e.g., systemic discrimination, the uneven distribution of resources within networks) inclines individuals to use chatbots, including problematic levels of emotional dependency on chatbots. The second two theories (affect control theory, fundamental cause of disease theory) help inform the development of chatbot-driven interventions that minimize safety risks and enhance equity by leveraging sociological insights into how chatbot outputs could attend to cultural contexts (e.g., affective norms) to promote wellbeing and enhance communities (e.g., opportunities for civic participation). We discuss the value of applying sociological theories for advancing theorizing about human-chatbot interaction and developing chatbots for social good.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Features: How Dataset Design Influences Multi-Agent Trajectory Prediction Performance</title>
<link>https://arxiv.org/abs/2507.05098</link>
<guid>https://arxiv.org/abs/2507.05098</guid>
<content:encoded><![CDATA[
arXiv:2507.05098v1 Announce Type: new 
Abstract: Accurate trajectory prediction is critical for safe autonomous navigation, yet the impact of dataset design on model performance remains understudied. This work systematically examines how feature selection, cross-dataset transfer, and geographic diversity influence trajectory prediction accuracy in multi-agent settings. We evaluate a state-of-the-art model using our novel L4 Motion Forecasting dataset based on our own data recordings in Germany and the US. This includes enhanced map and agent features. We compare our dataset to the US-centric Argoverse 2 benchmark. First, we find that incorporating supplementary map and agent features unique to our dataset, yields no measurable improvement over baseline features, demonstrating that modern architectures do not need extensive feature sets for optimal performance. The limited features of public datasets are sufficient to capture convoluted interactions without added complexity. Second, we perform cross-dataset experiments to evaluate how effective domain knowledge can be transferred between datasets. Third, we group our dataset by country and check the knowledge transfer between different driving cultures.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Behaviour-Driven Acceptance Testing of Robotic Systems</title>
<link>https://arxiv.org/abs/2507.05125</link>
<guid>https://arxiv.org/abs/2507.05125</guid>
<content:encoded><![CDATA[
arXiv:2507.05125v1 Announce Type: new 
Abstract: The specification and validation of robotics applications require bridging the gap between formulating requirements and systematic testing. This often involves manual and error-prone tasks that become more complex as requirements, design, and implementation evolve. To address this challenge systematically, we propose extending behaviour-driven development (BDD) to define and verify acceptance criteria for robotic systems. In this context, we use domain-specific modelling and represent composable BDD models as knowledge graphs for robust querying and manipulation, facilitating the generation of executable testing models. A domain-specific language helps to efficiently specify robotic acceptance criteria. We explore the potential for automated generation and execution of acceptance tests through a software architecture that integrates a BDD framework, Isaac Sim, and model transformations, focusing on acceptance criteria for pick-and-place applications. We tested this architecture with an existing pick-and-place implementation and evaluated the execution results, which shows how this application behaves and fails differently when tested against variations of the agent and environment. This research advances the rigorous and automated evaluation of robotic systems, contributing to their reliability and trustworthiness.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LERa: Replanning with Visual Feedback in Instruction Following</title>
<link>https://arxiv.org/abs/2507.05135</link>
<guid>https://arxiv.org/abs/2507.05135</guid>
<content:encoded><![CDATA[
arXiv:2507.05135v1 Announce Type: new 
Abstract: Large Language Models are increasingly used in robotics for task planning, but their reliance on textual inputs limits their adaptability to real-world changes and failures. To address these challenges, we propose LERa - Look, Explain, Replan - a Visual Language Model-based replanning approach that utilizes visual feedback. Unlike existing methods, LERa requires only a raw RGB image, a natural language instruction, an initial task plan, and failure detection - without additional information such as object detection or predefined conditions that may be unavailable in a given scenario. The replanning process consists of three steps: (i) Look, where LERa generates a scene description and identifies errors; (ii) Explain, where it provides corrective guidance; and (iii) Replan, where it modifies the plan accordingly. LERa is adaptable to various agent architectures and can handle errors from both dynamic scene changes and task execution failures. We evaluate LERa on the newly introduced ALFRED-ChaOS and VirtualHome-ChaOS datasets, achieving a 40% improvement over baselines in dynamic environments. In tabletop manipulation tasks with a predefined probability of task failure within the PyBullet simulator, LERa improves success rates by up to 67%. Further experiments, including real-world trials with a tabletop manipulator robot, confirm LERa's effectiveness in replanning. We demonstrate that LERa is a robust and adaptable solution for error-aware task execution in robotics. The code is available at https://lera-robo.github.io.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effects of Unplanned Incoming Flights on Airport Relief Processes after a Major Natural Disaster</title>
<link>https://arxiv.org/abs/2507.05150</link>
<guid>https://arxiv.org/abs/2507.05150</guid>
<content:encoded><![CDATA[
arXiv:2507.05150v1 Announce Type: new 
Abstract: The severity of natural disasters is increasing every year, impacting many people's lives. During the response phase of disasters, airports are important hubs where relief aid arrives and people need to be evacuated. However, the airport often forms a bottleneck in these relief operations due to the sudden need for increased capacity. Limited research has been done on the operational side of airport disaster management. Experts identify the main problems as, first, the asymmetry of information between the airport and incoming flights, and second, the lack of resources. The goal of this research is to understand the effects of incomplete knowledge of incoming flights with different resource allocation strategies on the performance of cargo handling operations at an airport after a natural disaster. An agent-based model is created, implementing realistic offloading strategies with different degrees of information uncertainty. Model calibration and verification are performed with experts in the field. The model performance is measured by the average turnaround time, which is divided into offloading time, boarding time, and cumulative waiting times. The results show that the effects of one unplanned aircraft are negligible. However, all waiting times increase with more arriving unplanned aircraft.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critiques of World Models</title>
<link>https://arxiv.org/abs/2507.05169</link>
<guid>https://arxiv.org/abs/2507.05169</guid>
<content:encoded><![CDATA[
arXiv:2507.05169v1 Announce Type: new 
Abstract: World Model, the supposed algorithmic surrogate of the real-world environment which biological agents experience with and act upon, has been an emerging topic in recent years because of the rising needs to develop virtual agents with artificial (general) intelligence. There has been much debate on what a world model really is, how to build it, how to use it, and how to evaluate it. In this essay, starting from the imagination in the famed Sci-Fi classic Dune, and drawing inspiration from the concept of "hypothetical thinking" in psychology literature, we offer critiques of several schools of thoughts on world modeling, and argue the primary goal of a world model to be simulating all actionable possibilities of the real world for purposeful reasoning and acting. Building on the critiques, we propose a new architecture for a general-purpose world model, based on hierarchical, multi-level, and mixed continuous/discrete representations, and a generative and self-supervision learning framework, with an outlook of a Physical, Agentic, and Nested (PAN) AGI system enabled by such a model.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vector Cost Bimatrix Games with Applications to Autonomous Racing</title>
<link>https://arxiv.org/abs/2507.05171</link>
<guid>https://arxiv.org/abs/2507.05171</guid>
<content:encoded><![CDATA[
arXiv:2507.05171v1 Announce Type: new 
Abstract: We formulate a vector cost alternative to the scalarization method for weighting and combining multi-objective costs. The algorithm produces solutions to bimatrix games that are simultaneously pure, unique Nash equilibria and Pareto optimal with guarantees for avoiding worst case outcomes. We achieve this by enforcing exact potential game constraints to guide cost adjustments towards equilibrium, while minimizing the deviation from the original cost structure. The magnitude of this adjustment serves as a metric for differentiating between Pareto optimal solutions. We implement this approach in a racing competition between agents with heterogeneous cost structures, resulting in fewer collision incidents with a minimal decrease in performance. Code is available at https://github.com/toazbenj/race_simulation.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale</title>
<link>https://arxiv.org/abs/2507.05178</link>
<guid>https://arxiv.org/abs/2507.05178</guid>
<content:encoded><![CDATA[
arXiv:2507.05178v1 Announce Type: new 
Abstract: Despite rapid progress in large language model (LLM)-based multi-agent systems, current benchmarks fall short in evaluating their scalability, robustness, and coordination capabilities in complex, dynamic, real-world tasks. Existing environments typically focus on small-scale, fully observable, or low-complexity domains, limiting their utility for developing and assessing next-generation multi-agent Agentic AI frameworks. We introduce CREW-Wildfire, an open-source benchmark designed to close this gap. Built atop the human-AI teaming CREW simulation platform, CREW-Wildfire offers procedurally generated wildfire response scenarios featuring large maps, heterogeneous agents, partial observability, stochastic dynamics, and long-horizon planning objectives. The environment supports both low-level control and high-level natural language interactions through modular Perception and Execution modules. We implement and evaluate several state-of-the-art LLM-based multi-agent Agentic AI frameworks, uncovering significant performance gaps that highlight the unsolved challenges in large-scale coordination, communication, spatial reasoning, and long-horizon planning under uncertainty. By providing more realistic complexity, scalable architecture, and behavioral evaluation metrics, CREW-Wildfire establishes a critical foundation for advancing research in scalable multi-agent Agentic intelligence. All code, environments, data, and baselines will be released to support future research in this emerging domain.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedGemma Technical Report</title>
<link>https://arxiv.org/abs/2507.05201</link>
<guid>https://arxiv.org/abs/2507.05201</guid>
<content:encoded><![CDATA[
arXiv:2507.05201v1 Announce Type: new 
Abstract: Artificial intelligence (AI) has significant potential in healthcare applications, but its training and deployment faces challenges due to healthcare's diverse data, complex tasks, and the need to preserve privacy. Foundation models that perform well on medical tasks and require less task-specific tuning data are critical to accelerate the development of healthcare AI applications. We introduce MedGemma, a collection of medical vision-language foundation models based on Gemma 3 4B and 27B. MedGemma demonstrates advanced medical understanding and reasoning on images and text, significantly exceeding the performance of similar-sized generative models and approaching the performance of task-specific models, while maintaining the general capabilities of the Gemma 3 base models. For out-of-distribution tasks, MedGemma achieves 2.6-10% improvement on medical multimodal question answering, 15.5-18.1% improvement on chest X-ray finding classification, and 10.8% improvement on agentic evaluations compared to the base models. Fine-tuning MedGemma further improves performance in subdomains, reducing errors in electronic health record information retrieval by 50% and reaching comparable performance to existing specialized state-of-the-art methods for pneumothorax classification and histopathology patch classification. We additionally introduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP. MedSigLIP powers the visual understanding capabilities of MedGemma and as an encoder achieves comparable or better performance than specialized medical image encoders. Taken together, the MedGemma collection provides a strong foundation of medical image and text capabilities, with potential to significantly accelerate medical research and development of downstream applications. The MedGemma collection, including tutorials and model weights, can be found at https://goo.gle/medgemma.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context Modeling</title>
<link>https://arxiv.org/abs/2507.05240</link>
<guid>https://arxiv.org/abs/2507.05240</guid>
<content:encoded><![CDATA[
arXiv:2507.05240v1 Announce Type: new 
Abstract: Vision-and-Language Navigation (VLN) in real-world settings requires agents to process continuous visual streams and generate actions with low latency grounded in language instructions. While Video-based Large Language Models (Video-LLMs) have driven recent progress, current VLN methods based on Video-LLM often face trade-offs among fine-grained visual understanding, long-term context modeling and computational efficiency. We introduce StreamVLN, a streaming VLN framework that employs a hybrid slow-fast context modeling strategy to support multi-modal reasoning over interleaved vision, language and action inputs. The fast-streaming dialogue context facilitates responsive action generation through a sliding-window of active dialogues, while the slow-updating memory context compresses historical visual states using a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN achieves coherent multi-turn dialogue through efficient KV cache reuse, supporting long video streams with bounded context size and inference cost. Experiments on VLN-CE benchmarks demonstrate state-of-the-art performance with stable low latency, ensuring robustness and efficiency in real-world deployment. The project page is: \href{https://streamvln.github.io/}{https://streamvln.github.io/}.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?</title>
<link>https://arxiv.org/abs/2507.05241</link>
<guid>https://arxiv.org/abs/2507.05241</guid>
<content:encoded><![CDATA[
arXiv:2507.05241v1 Announce Type: new 
Abstract: The rapid advancements of AI agents have ignited the long-held ambition of leveraging them to accelerate scientific discovery. Achieving this goal requires a deep understanding of the frontiers of human knowledge. As such, Humanity's Last Exam (HLE) provides an exceptionally challenging touchstone for evaluating scientific AI agents. In this work, we aim to construct the foundational architecture for general-purpose agents and validate the capabilities through leading performance on HLE. To achieve this, we introduce X-Master, a tool-augmented reasoning agent designed to emulate human researchers by interacting flexibly with external tools during its reasoning process. This agent, guided by the conceptualization of code as an interaction language, can flexibly leverage built-in Python libraries and our customized tools to augment the reasoning. We further scale its capabilities through X-Masters, a scattered-and-stacked agentic workflow that systematically enhances breadth and depth of reasoning. Our open-source solution, X-Masters, sets a new state-of-the-art record on HLE with a score of 32.1%, surpassing OpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to exceed the 30% threshold. This work allows us to gain a deeper understanding of complex task-solving and accumulates valuable experience that can inform future advancements, guiding subsequent model training.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Latent Partner Strategies for Adaptive Zero-Shot Human-Agent Collaboration</title>
<link>https://arxiv.org/abs/2507.05244</link>
<guid>https://arxiv.org/abs/2507.05244</guid>
<content:encoded><![CDATA[
arXiv:2507.05244v1 Announce Type: new 
Abstract: In collaborative tasks, being able to adapt to your teammates is a necessary requirement for success. When teammates are heterogeneous, such as in human-agent teams, agents need to be able to observe, recognize, and adapt to their human partners in real time. This becomes particularly challenging in tasks with time pressure and complex strategic spaces where the dynamics can change rapidly. In this work, we introduce TALENTS, a strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to a range of partner strategies, enabling ad-hoc teamwork. Our approach utilizes a variational autoencoder to learn a latent strategy space from trajectory data. This latent space represents the underlying strategies that agents employ. Subsequently, the system identifies different types of strategy by clustering the data. Finally, a cooperator agent is trained to generate partners for each type of strategy, conditioned on these clusters. In order to adapt to previously unseen partners, we leverage a fixed-share regret minimization algorithm that infers and adjusts the estimated partner strategy dynamically. We assess our approach in a customized version of the Overcooked environment, posing a challenging cooperative cooking task that demands strong coordination across a wide range of possible strategies. Using an online user study, we show that our agent outperforms current baselines when working with unfamiliar human partners.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Action Space Reduction Strategies for Reinforcement Learning in Autonomous Driving</title>
<link>https://arxiv.org/abs/2507.05251</link>
<guid>https://arxiv.org/abs/2507.05251</guid>
<content:encoded><![CDATA[
arXiv:2507.05251v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) offers a promising framework for autonomous driving by enabling agents to learn control policies through interaction with environments. However, large and high-dimensional action spaces often used to support fine-grained control can impede training efficiency and increase exploration costs. In this study, we introduce and evaluate two novel structured action space modification strategies for RL in autonomous driving: dynamic masking and relative action space reduction. These approaches are systematically compared against fixed reduction schemes and full action space baselines to assess their impact on policy learning and performance. Our framework leverages a multimodal Proximal Policy Optimization agent that processes both semantic image sequences and scalar vehicle states. The proposed dynamic and relative strategies incorporate real-time action masking based on context and state transitions, preserving action consistency while eliminating invalid or suboptimal choices. Through comprehensive experiments across diverse driving routes, we show that action space reduction significantly improves training stability and policy performance. The dynamic and relative schemes, in particular, achieve a favorable balance between learning speed, control precision, and generalization. These findings highlight the importance of context-aware action space design for scalable and reliable RL in autonomous driving tasks.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Marginal to Joint Predictions: Evaluating Scene-Consistent Trajectory Prediction Approaches for Automated Driving</title>
<link>https://arxiv.org/abs/2507.05254</link>
<guid>https://arxiv.org/abs/2507.05254</guid>
<content:encoded><![CDATA[
arXiv:2507.05254v1 Announce Type: new 
Abstract: Accurate motion prediction of surrounding traffic participants is crucial for the safe and efficient operation of automated vehicles in dynamic environments. Marginal prediction models commonly forecast each agent's future trajectories independently, often leading to sub-optimal planning decisions for an automated vehicle. In contrast, joint prediction models explicitly account for the interactions between agents, yielding socially and physically consistent predictions on a scene level. However, existing approaches differ not only in their problem formulation but also in the model architectures and implementation details used, making it difficult to compare them. In this work, we systematically investigate different approaches to joint motion prediction, including post-processing of the marginal predictions, explicitly training the model for joint predictions, and framing the problem as a generative task. We evaluate each approach in terms of prediction accuracy, multi-modality, and inference efficiency, offering a comprehensive analysis of the strengths and limitations of each approach. Several prediction examples are available at https://frommarginaltojointpred.github.io/.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions</title>
<link>https://arxiv.org/abs/2507.05257</link>
<guid>https://arxiv.org/abs/2507.05257</guid>
<content:encoded><![CDATA[
arXiv:2507.05257v1 Announce Type: new 
Abstract: Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to the lack of benchmarks. We term agents with memory mechanisms as memory agents. In this paper, we identify four core competencies essential for memory agents: accurate retrieval, test-time learning, long-range understanding, and conflict resolution. Existing datasets either rely on limited context lengths or are tailored for static, long-context settings like book-based QA, which do not reflect the interactive, multi-turn nature of memory agents that incrementally accumulate information. Furthermore, no existing benchmarks cover all four competencies. Therefore, we introduce MemoryAgentBench, a new benchmark specifically designed for memory agents. Our benchmark combines reformulated existing datasets with newly constructed ones, covering the above four memory competencies, providing a systematic and challenging testbed for assessing memory quality. We evaluate a diverse set of memory agents, ranging from simple context-based and retrieval-augmented generation (RAG) systems to advanced agents with external memory modules and tool integration. Empirical results reveal that current methods fall short of mastering all four competencies, underscoring the need for further research into comprehensive memory mechanisms for LLM agents.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatio-Temporal LLM: Reasoning about Environments and Actions</title>
<link>https://arxiv.org/abs/2507.05258</link>
<guid>https://arxiv.org/abs/2507.05258</guid>
<content:encoded><![CDATA[
arXiv:2507.05258v1 Announce Type: new 
Abstract: Despite the significant recent progress of Multimodal Large Language Models (MLLMs), MLLMs still struggle to correctly answer prompts that require a holistic spatio-temporal understanding. Specifically, it is challenging to address prompts that refer to 1) the entirety of an environment that an agent equipped with an MLLM can operate in; and simultaneously also refer to 2) recent actions that just happened and are encoded in a video clip. However, such a holistic spatio-temporal understanding is important for agents operating in the real world. To address this issue, we first develop a framework to collect a large-scale dataset. Using the collected "Reasoning about Environments and Actions" (REA) dataset, we show that recent methods indeed struggle to correctly answer the prompts. To improve, we develop a "spatio-temporal LLM" (ST-LLM), a model equipped with projectors to improve both spatial understanding of an environment and temporal understanding of recent observations. On the collected REA data, we show that the proposed method significantly improves results compared to prior work. Code and data are available at https://zoezheng126.github.io/STLLM-website/.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AuraGenome: An LLM-Powered Framework for On-the-Fly Reusable and Scalable Circular Genome Visualizations</title>
<link>https://arxiv.org/abs/2507.02877</link>
<guid>https://arxiv.org/abs/2507.02877</guid>
<content:encoded><![CDATA[
arXiv:2507.02877v1 Announce Type: cross 
Abstract: Circular genome visualizations are essential for exploring structural variants and gene regulation. However, existing tools often require complex scripting and manual configuration, making the process time-consuming, error-prone, and difficult to learn. To address these challenges, we introduce AuraGenome, an LLM-powered framework for rapid, reusable, and scalable generation of multi-layered circular genome visualizations. AuraGenome combines a semantic-driven multi-agent workflow with an interactive visual analytics system. The workflow employs seven specialized LLM-driven agents, each assigned distinct roles such as intent recognition, layout planning, and code generation, to transform raw genomic data into tailored visualizations. The system supports multiple coordinated views tailored for genomic data, offering ring, radial, and chord-based layouts to represent multi-layered circular genome visualizations. In addition to enabling interactions and configuration reuse, the system supports real-time refinement and high-quality report export. We validate its effectiveness through two case studies and a comprehensive user study. AuraGenome is available at: https://github.com/Darius18/AuraGenome.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TopoMAS: Large Language Model Driven Topological Materials Multiagent System</title>
<link>https://arxiv.org/abs/2507.04053</link>
<guid>https://arxiv.org/abs/2507.04053</guid>
<content:encoded><![CDATA[
arXiv:2507.04053v1 Announce Type: cross 
Abstract: Topological materials occupy a frontier in condensed-matter physics thanks to their remarkable electronic and quantum properties, yet their cross-scale design remains bottlenecked by inefficient discovery workflows. Here, we introduce TopoMAS (Topological materials Multi-Agent System), an interactive human-AI framework that seamlessly orchestrates the entire materials-discovery pipeline: from user-defined queries and multi-source data retrieval, through theoretical inference and crystal-structure generation, to first-principles validation. Crucially, TopoMAS closes the loop by autonomously integrating computational outcomes into a dynamic knowledge graph, enabling continuous knowledge refinement. In collaboration with human experts, it has already guided the identification of novel topological phases SrSbO3, confirmed by first-principles calculations. Comprehensive benchmarks demonstrate robust adaptability across base Large Language Model, with the lightweight Qwen2.5-72B model achieving 94.55% accuracy while consuming only 74.3-78.4% of tokens required by Qwen3-235B and 83.0% of DeepSeek-V3's usage--delivering responses twice as fast as Qwen3-235B. This efficiency establishes TopoMAS as an accelerator for computation-driven discovery pipelines. By harmonizing rational agent orchestration with a self-evolving knowledge graph, our framework not only delivers immediate advances in topological materials but also establishes a transferable, extensible paradigm for materials-science domain.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mission-Aligned Learning-Informed Control of Autonomous Systems: Formulation and Foundations</title>
<link>https://arxiv.org/abs/2507.04356</link>
<guid>https://arxiv.org/abs/2507.04356</guid>
<content:encoded><![CDATA[
arXiv:2507.04356v1 Announce Type: cross 
Abstract: Research, innovation and practical capital investment have been increasing rapidly toward the realization of autonomous physical agents. This includes industrial and service robots, unmanned aerial vehicles, embedded control devices, and a number of other realizations of cybernetic/mechatronic implementations of intelligent autonomous devices. In this paper, we consider a stylized version of robotic care, which would normally involve a two-level Reinforcement Learning procedure that trains a policy for both lower level physical movement decisions as well as higher level conceptual tasks and their sub-components. In order to deliver greater safety and reliability in the system, we present the general formulation of this as a two-level optimization scheme which incorporates control at the lower level, and classical planning at the higher level, integrated with a capacity for learning. This synergistic integration of multiple methodologies -- control, classical planning, and RL -- presents an opportunity for greater insight for algorithm development, leading to more efficient and reliable performance. Here, the notion of reliability pertains to physical safety and interpretability into an otherwise black box operation of autonomous agents, concerning users and regulators. This work presents the necessary background and general formulation of the optimization framework, detailing each component and its integration with the others.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Module checking of pushdown multi-agent systems</title>
<link>https://arxiv.org/abs/2003.04728</link>
<guid>https://arxiv.org/abs/2003.04728</guid>
<content:encoded><![CDATA[
arXiv:2003.04728v4 Announce Type: replace 
Abstract: In this paper, we investigate the module-checking problem of pushdown multi-agent systems (PMS) against ATL and ATL* specifications. We establish that for ATL, module checking of PMS is 2EXPTIME-complete, which is the same complexity as pushdown module-checking for CTL. On the other hand, we show that ATL* module-checking of PMS turns out to be 4EXPTIME-complete, hence exponentially harder than both CTL* pushdown module-checking and ATL* model-checking of PMS. Our result for ATL* provides a rare example of a natural decision problem that is elementary yet but with a complexity that is higher than triply exponential-time.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Visual Reinforcement Learning with A Life-Long World Model</title>
<link>https://arxiv.org/abs/2303.06572</link>
<guid>https://arxiv.org/abs/2303.06572</guid>
<content:encoded><![CDATA[
arXiv:2303.06572v2 Announce Type: replace 
Abstract: Learning physical dynamics in a series of non-stationary environments is a challenging but essential task for model-based reinforcement learning (MBRL) with visual inputs. It requires the agent to consistently adapt to novel tasks without forgetting previous knowledge. In this paper, we present a new continual learning approach for visual dynamics modeling and explore its efficacy in visual control. The key assumption is that an ideal world model can provide a non-forgetting environment simulator, which enables the agent to optimize the policy in a multi-task learning manner based on the imagined trajectories from the world model. To this end, we first introduce the life-long world model, which learns task-specific latent dynamics using a mixture of Gaussians and incorporates generative experience replay to mitigate catastrophic forgetting. Then, we further address the value estimation challenge for previous tasks with the exploratory-conservative behavior learning approach. Our model remarkably outperforms the straightforward combinations of existing continual learning and visual RL algorithms on DeepMind Control Suite and Meta-World benchmarks with continual visual control tasks.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Differentiable Logic Programs for Abstract Visual Reasoning</title>
<link>https://arxiv.org/abs/2307.00928</link>
<guid>https://arxiv.org/abs/2307.00928</guid>
<content:encoded><![CDATA[
arXiv:2307.00928v2 Announce Type: replace 
Abstract: Visual reasoning is essential for building intelligent agents that understand the world and perform problem-solving beyond perception. Differentiable forward reasoning has been developed to integrate reasoning with gradient-based machine learning paradigms. However, due to the memory intensity, most existing approaches do not bring the best of the expressivity of first-order logic, excluding a crucial ability to solve abstract visual reasoning, where agents need to perform reasoning by using analogies on abstract concepts in different scenarios. To overcome this problem, we propose NEUro-symbolic Message-pAssiNg reasoNer (NEUMANN), which is a graph-based differentiable forward reasoner, passing messages in a memory-efficient manner and handling structured programs with functors. Moreover, we propose a computationally-efficient structure learning algorithm to perform explanatory program induction on complex visual scenes. To evaluate, in addition to conventional visual reasoning tasks, we propose a new task, visual reasoning behind-the-scenes, where agents need to learn abstract programs and then answer queries by imagining scenes that are not observed. We empirically demonstrate that NEUMANN solves visual reasoning tasks efficiently, outperforming neural, symbolic, and neuro-symbolic baselines.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentially Private Dual Gradient Tracking for Distributed Resource Allocation</title>
<link>https://arxiv.org/abs/2403.18275</link>
<guid>https://arxiv.org/abs/2403.18275</guid>
<content:encoded><![CDATA[
arXiv:2403.18275v3 Announce Type: replace 
Abstract: This paper investigates privacy issues in distributed resource allocation over directed networks, where each agent holds a private cost function and optimizes its decision subject to a global coupling constraint through local interaction with other agents. Conventional methods for resource allocation over directed networks require all agents to transmit their original data to neighbors, which poses the risk of disclosing sensitive and private information. To address this issue, we propose an algorithm called differentially private dual gradient tracking (DP-DGT) for distributed resource allocation, which obfuscates the exchanged messages using independent Laplacian noise. Our algorithm ensures that the agents' decisions converge to a neighborhood of the optimal solution almost surely. Furthermore, without the assumption of bounded gradients, we prove that the cumulative differential privacy loss under the proposed algorithm is finite even when the number of iterations goes to infinity. To the best of our knowledge, we are the first to simultaneously achieve these two goals in distributed resource allocation problems over directed networks. Finally, numerical simulations on economic dispatch problems within the IEEE 14-bus system illustrate the effectiveness of our proposed algorithm.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Elements of World Knowledge (EWoK): A Cognition-Inspired Framework for Evaluating Basic World Knowledge in Language Models</title>
<link>https://arxiv.org/abs/2405.09605</link>
<guid>https://arxiv.org/abs/2405.09605</guid>
<content:encoded><![CDATA[
arXiv:2405.09605v2 Announce Type: replace 
Abstract: The ability to build and reason about models of the world is essential for situated language understanding. But evaluating world modeling capabilities in modern AI systems -- especially those based on language models -- has proven challenging, in large part because of the difficulty of disentangling conceptual knowledge about the world from knowledge of surface co-occurrence statistics. This paper presents Elements of World Knowledge (EWoK), a framework for evaluating language models' understanding of the conceptual knowledge underlying world modeling. EWoK targets specific concepts from multiple knowledge domains known to be important for world modeling in humans, from social interactions (help, deceive) to spatial relations (left, right). Objects, agents, and locations in the items can be flexibly filled in, enabling easy generation of multiple controlled datasets. We then introduce EWoK-core-1.0, a dataset of 4,374 items covering 11 world knowledge domains. We evaluate 20 open-weights large language models (1.3B--70B parameters) and compare them with human performance. All tested models perform worse than humans, with results varying drastically across domains. Performance on social interactions and social properties was highest and performance on physical relations and spatial relations was lowest. Overall, this dataset highlights simple cases where even large models struggle and presents rich avenues for targeted research on LLM world modeling capabilities.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Robustness in Preference-Based Reinforcement Learning with Dynamic Sparsity</title>
<link>https://arxiv.org/abs/2406.06495</link>
<guid>https://arxiv.org/abs/2406.06495</guid>
<content:encoded><![CDATA[
arXiv:2406.06495v3 Announce Type: replace 
Abstract: To integrate into human-centered environments, autonomous agents must learn from and adapt to humans in their native settings. Preference-based reinforcement learning (PbRL) can enable this by learning reward functions from human preferences. However, humans live in a world full of diverse information, most of which is irrelevant to completing any particular task. It then becomes essential that agents learn to focus on the subset of task-relevant state features. To that end, this work proposes R2N (Robust-to-Noise), the first PbRL algorithm that leverages principles of dynamic sparse training to learn robust reward models that can focus on task-relevant features. In experiments with a simulated teacher, we demonstrate that R2N can adapt the sparse connectivity of its neural networks to focus on task-relevant features, enabling R2N to significantly outperform several sparse training and PbRL algorithms across simulated robotic environments.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deception in Nash Equilibrium Seeking</title>
<link>https://arxiv.org/abs/2407.05168</link>
<guid>https://arxiv.org/abs/2407.05168</guid>
<content:encoded><![CDATA[
arXiv:2407.05168v2 Announce Type: replace 
Abstract: In socio-technical multi-agent systems, deception exploits privileged information to induce false beliefs in "victims," keeping them oblivious and leading to outcomes detrimental to them or advantageous to the deceiver. We consider model-free Nash-equilibrium-seeking for non-cooperative games with asymmetric information and introduce model-free deceptive algorithms with stability guarantees. In the simplest algorithm, the deceiver includes in his action policy the victim's exploration signal, with an amplitude tuned by an integrator of the regulation error between the deceiver's actual and desired payoff. The integral feedback drives the deceiver's payoff to the payoff's reference value, while the victim is led to adopt a suboptimal action, at which the pseudogradient of the deceiver's payoff is zero. The deceiver's and victim's actions turn out to constitute a "deceptive" Nash equilibrium of a different game, whose structure is managed - in real time - by the deceiver. We examine quadratic, aggregative, and more general games and provide conditions for a successful deception, mutual and benevolent deception, and immunity to deception. Stability results are established using techniques based on averaging and singular perturbations. Among the examples in the paper is a microeconomic duopoly in which the deceiver induces in the victim a belief that the buyers disfavor the deceiver more than they actually do, leading the victim to increase the price above the Nash price, and resulting in an increased profit for the deceiver and a decreased profit for the victim. A study of the deceiver's integral feedback for the desired profit reveals that, in duopolies with equal marginal costs, a deceiver that is greedy for very high profit can attain any such profit, and pursue this with arbitrarily high integral gain (impatiently), irrespective of the market preference for the victim.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Computing and Neuromorphic Computing for Safe, Reliable, and explainable Multi-Agent Reinforcement Learning: Optimal Control in Autonomous Robotics</title>
<link>https://arxiv.org/abs/2408.03884</link>
<guid>https://arxiv.org/abs/2408.03884</guid>
<content:encoded><![CDATA[
arXiv:2408.03884v2 Announce Type: replace 
Abstract: This paper investigates the utilization of Quantum Computing and Neuromorphic Computing for Safe, Reliable, and Explainable Multi_Agent Reinforcement Learning (MARL) in the context of optimal control in autonomous robotics. The objective was to address the challenges of optimizing the behavior of autonomous agents while ensuring safety, reliability, and explainability. Quantum Computing techniques, including Quantum Approximate Optimization Algorithm (QAOA), were employed to efficiently explore large solution spaces and find approximate solutions to complex MARL problems. Neuromorphic Computing, inspired by the architecture of the human brain, provided parallel and distributed processing capabilities, which were leveraged to develop intelligent and adaptive systems. The combination of these technologies held the potential to enhance the safety, reliability, and explainability of MARL in autonomous robotics. This research contributed to the advancement of autonomous robotics by exploring cutting-edge technologies and their applications in multi-agent systems. Codes and data are available.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing Act: Prioritization Strategies for LLM-Designed Restless Bandit Rewards</title>
<link>https://arxiv.org/abs/2408.12112</link>
<guid>https://arxiv.org/abs/2408.12112</guid>
<content:encoded><![CDATA[
arXiv:2408.12112v4 Announce Type: replace 
Abstract: LLMs are increasingly used to design reward functions based on human preferences in Reinforcement Learning (RL). We focus on LLM-designed rewards for Restless Multi-Armed Bandits, a framework for allocating limited resources among agents. In applications such as public health, this approach empowers grassroots health workers to tailor automated allocation decisions to community needs. In the presence of multiple agents, altering the reward function based on human preferences can impact subpopulations very differently, leading to complex tradeoffs and a multi-objective resource allocation problem. We are the first to present a principled method termed Social Choice Language Model for dealing with these tradeoffs for LLM-designed rewards for multiagent planners in general and restless bandits in particular. The novel part of our model is a transparent and configurable selection component, called an adjudicator, external to the LLM that controls complex tradeoffs via a user-selected social welfare function. Our experiments demonstrate that our model reliably selects more effective, aligned, and balanced reward functions compared to purely LLM-based approaches.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Impact of Multiple DER Aggregators on Wholesale Energy Markets: A Hybrid Mean Field Approach</title>
<link>https://arxiv.org/abs/2409.00107</link>
<guid>https://arxiv.org/abs/2409.00107</guid>
<content:encoded><![CDATA[
arXiv:2409.00107v2 Announce Type: replace 
Abstract: The integration of distributed energy resources (DERs) into wholesale energy markets can greatly enhance grid flexibility, improve market efficiency, and contribute to a more sustainable energy future. As DERs -- such as solar PV panels and energy storage -- proliferate, effective mechanisms are needed to ensure that small prosumers can participate meaningfully in these markets. We study a wholesale market model featuring multiple DER aggregators, each controlling a portfolio of DER resources and bidding into the market on behalf of the DER asset owners. The key of our approach lies in recognizing the repeated nature of market interactions the ability of participants to learn and adapt over time. Specifically, Aggregators repeatedly interact with each other and with other suppliers in the wholesale market, collectively shaping wholesale electricity prices (aka the locational marginal prices (LMPs)). We model this multi-agent interaction using a mean-field game (MFG), which uses market information -- reflecting the average behavior of market participants -- to enable each aggregator to predict long-term LMP trends and make informed decisions. For each aggregator, because they control the DERs within their portfolio under certain contract structures, we employ a mean-field control (MFC) approach (as opposed to a MFG) to learn an optimal policy that maximizes the total rewards of the DERs under their management. We also propose a reinforcement learning (RL)-based method to help each agent learn optimal strategies within the MFG framework, enhancing their ability to adapt to market conditions and uncertainties. Numerical simulations show that LMPs quickly reach a steady state in the hybrid mean-field approach. Furthermore, our results demonstrate that the combination of energy storage and mean-field learning significantly reduces price volatility compared to scenarios without storage.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenSim: A General Social Simulation Platform with Large Language Model based Agents</title>
<link>https://arxiv.org/abs/2410.04360</link>
<guid>https://arxiv.org/abs/2410.04360</guid>
<content:encoded><![CDATA[
arXiv:2410.04360v3 Announce Type: replace 
Abstract: With the rapid advancement of large language models (LLMs), recent years have witnessed many promising studies on leveraging LLM-based agents to simulate human social behavior. While prior work has demonstrated significant potential across various domains, much of it has focused on specific scenarios involving a limited number of agents and has lacked the ability to adapt when errors occur during simulation. To overcome these limitations, we propose a novel LLM-agent-based simulation platform called \textit{GenSim}, which: (1) \textbf{Abstracts a set of general functions} to simplify the simulation of customized social scenarios; (2) \textbf{Supports one hundred thousand agents} to better simulate large-scale populations in real-world contexts; (3) \textbf{Incorporates error-correction mechanisms} to ensure more reliable and long-term simulations. To evaluate our platform, we assess both the efficiency of large-scale agent simulations and the effectiveness of the error-correction mechanisms. To our knowledge, GenSim represents an initial step toward a general, large-scale, and correctable social simulation platform based on LLM agents, promising to further advance the field of social science.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversifying Robot Locomotion Behaviors with Extrinsic Behavioral Curiosity</title>
<link>https://arxiv.org/abs/2410.06151</link>
<guid>https://arxiv.org/abs/2410.06151</guid>
<content:encoded><![CDATA[
arXiv:2410.06151v2 Announce Type: replace 
Abstract: Imitation learning (IL) has shown promise in robot locomotion but is often limited to learning a single expert policy, constraining behavior diversity and robustness in unpredictable real-world scenarios. To address this, we introduce Quality Diversity Inverse Reinforcement Learning (QD-IRL), a novel framework that integrates quality-diversity optimization with IRL methods, enabling agents to learn diverse behaviors from limited demonstrations. This work introduces Extrinsic Behavioral Curiosity (EBC), which allows agents to receive additional curiosity rewards from an external critic based on how novel the behaviors are with respect to a large behavioral archive. To validate the effectiveness of EBC in exploring diverse locomotion behaviors, we evaluate our method on multiple robot locomotion tasks. EBC improves the performance of QD-IRL instances with GAIL, VAIL, and DiffAIL across all included environments by up to 185%, 42%, and 150%, even surpassing expert performance by 20% in Humanoid. Furthermore, we demonstrate that EBC is applicable to Gradient-Arborescence-based Quality Diversity Reinforcement Learning (QD-RL) algorithms, where it substantially improves performance and provides a generic technique for diverse robot locomotion. The source code of this work is provided at https://github.com/vanzll/EBC.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MENTOR: Mixture-of-Experts Network with Task-Oriented Perturbation for Visual Reinforcement Learning</title>
<link>https://arxiv.org/abs/2410.14972</link>
<guid>https://arxiv.org/abs/2410.14972</guid>
<content:encoded><![CDATA[
arXiv:2410.14972v3 Announce Type: replace 
Abstract: Visual deep reinforcement learning (RL) enables robots to acquire skills from visual input for unstructured tasks. However, current algorithms suffer from low sample efficiency, limiting their practical applicability. In this work, we present MENTOR, a method that improves both the architecture and optimization of RL agents. Specifically, MENTOR replaces the standard multi-layer perceptron (MLP) with a mixture-of-experts (MoE) backbone and introduces a task-oriented perturbation mechanism. MENTOR outperforms state-of-the-art methods across three simulation benchmarks and achieves an average of 83% success rate on three challenging real-world robotic manipulation tasks, significantly surpassing the 32% success rate of the strongest existing model-free visual RL algorithm. These results underscore the importance of sample efficiency in advancing visual RL for real-world robotics. Experimental videos are available at https://suninghuang19.github.io/mentor_page/.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You Can't Always Get What You Want: Games of Ordered Preference</title>
<link>https://arxiv.org/abs/2410.21447</link>
<guid>https://arxiv.org/abs/2410.21447</guid>
<content:encoded><![CDATA[
arXiv:2410.21447v3 Announce Type: replace 
Abstract: We study noncooperative games, in which each player's objective is composed of a sequence of ordered- and potentially conflicting-preferences. Problems of this type naturally model a wide variety of scenarios: for example, drivers at a busy intersection must balance the desire to make forward progress with the risk of collision. Mathematically, these problems possess a nested structure, and to behave properly players must prioritize their most important preference, and only consider less important preferences to the extent that they do not compromise performance on more important ones. We consider multi-agent, noncooperative variants of these problems, and seek generalized Nash equilibria in which each player's decision reflects both its hierarchy of preferences and other players' actions. We make two key contributions. First, we develop a recursive approach for deriving the first-order optimality conditions of each player's nested problem. Second, we propose a sequence of increasingly tight relaxations, each of which can be transcribed as a mixed complementarity problem and solved via existing methods. Experimental results demonstrate that our approach reliably converges to equilibrium solutions that strictly reflect players' individual ordered preferences.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Clinical Trial Patient Matching through Knowledge Augmentation and Reasoning with Multi-Agent</title>
<link>https://arxiv.org/abs/2411.14637</link>
<guid>https://arxiv.org/abs/2411.14637</guid>
<content:encoded><![CDATA[
arXiv:2411.14637v3 Announce Type: replace 
Abstract: Matching patients effectively and efficiently for clinical trials is a significant challenge due to the complexity and variability of patient profiles and trial criteria. This paper introduces \textbf{Multi-Agent for Knowledge Augmentation and Reasoning (MAKAR)}, a novel multi-agent system that enhances patient-trial matching by integrating criterion augmentation with structured reasoning. MAKAR consistently improves performance by an average of 7\% across different datasets. Furthermore, it enables privacy-preserving deployment and maintains competitive performance when using smaller open-source models. Overall, MAKAR can contributes to more transparent, accurate, and privacy-conscious AI-driven patient matching.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and Dual Evaluation Metrics for Factuality and Coverage</title>
<link>https://arxiv.org/abs/2412.15484</link>
<guid>https://arxiv.org/abs/2412.15484</guid>
<content:encoded><![CDATA[
arXiv:2412.15484v4 Announce Type: replace 
Abstract: Multimodal large language models (MLLMs) excel at generating highly detailed captions but often produce hallucinations. Our analysis reveals that existing hallucination detection methods struggle with detailed captions. We attribute this to the increasing reliance of MLLMs on their generated text, rather than the input image, as the sequence length grows. To address this issue, we propose a multiagent approach that leverages LLM-MLLM collaboration to correct given captions. Additionally, we introduce an evaluation framework and a benchmark dataset to facilitate the systematic analysis of detailed captions. Our experiments demonstrate that our proposed evaluation method better aligns with human judgments of factuality than existing metrics and that existing approaches to improve the MLLM factuality may fall short in hyper-detailed image captioning tasks. In contrast, our proposed method significantly enhances the factual accuracy of captions, even improving those generated by GPT-4V. Finally, we highlight a limitation of VQA-centric benchmarking by demonstrating that an MLLM's performance on VQA benchmarks may not correlate with its ability to generate detailed image captions. Our code and data are available at https://github.com/adobe-research/CapMAS.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Microscopy Experiments through Large Language Model Agents</title>
<link>https://arxiv.org/abs/2501.10385</link>
<guid>https://arxiv.org/abs/2501.10385</guid>
<content:encoded><![CDATA[
arXiv:2501.10385v2 Announce Type: replace 
Abstract: Large language models (LLMs) are revolutionizing self driving laboratories (SDLs) for materials research, promising unprecedented acceleration of scientific discovery. However, current SDL implementations rely on rigid protocols that fail to capture the adaptability and intuition of expert scientists in dynamic experimental settings. We introduce Artificially Intelligent Lab Assistant (AILA), a framework automating atomic force microscopy through LLM driven agents. Further, we develop AFMBench a comprehensive evaluation suite challenging AI agents across the complete scientific workflow from experimental design to results analysis. We find that state of the art models struggle with basic tasks and coordination scenarios. Notably, Claude 3.5 sonnet performs unexpectedly poorly despite excelling in materials domain question answering (QA) benchmarks, revealing that domain specific QA proficiency does not necessarily translate to effective agentic capabilities. Additionally, we observe that LLMs can deviate from instructions, raising safety alignment concerns for SDL applications. Our ablations reveal that multi agent frameworks outperform single-agent architectures. We also observe significant prompt fragility, where slight modifications in prompt structure cause substantial performance variations in capable models like GPT 4o. Finally, we evaluate AILA's effectiveness in increasingly advanced experiments AFM calibration, feature detection, mechanical property measurement, graphene layer counting, and indenter detection. Our findings underscore the necessity for rigorous benchmarking protocols and prompt engineering strategies before deploying AI laboratory assistants in scientific research environments.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Extended Benchmarking of Multi-Agent Reinforcement Learning Algorithms in Complex Fully Cooperative Tasks</title>
<link>https://arxiv.org/abs/2502.04773</link>
<guid>https://arxiv.org/abs/2502.04773</guid>
<content:encoded><![CDATA[
arXiv:2502.04773v2 Announce Type: replace 
Abstract: Multi-Agent Reinforcement Learning (MARL) has recently emerged as a significant area of research. However, MARL evaluation often lacks systematic diversity, hindering a comprehensive understanding of algorithms' capabilities. In particular, cooperative MARL algorithms are predominantly evaluated on benchmarks such as SMAC and GRF, which primarily feature team game scenarios without assessing adequately various aspects of agents' capabilities required in fully cooperative real-world tasks such as multi-robot cooperation and warehouse, resource management, search and rescue, and human-AI cooperation. Moreover, MARL algorithms are mainly evaluated on low dimensional state spaces, and thus their performance on high-dimensional (e.g., image) observations is not well-studied. To fill this gap, this paper highlights the crucial need for expanding systematic evaluation across a wider array of existing benchmarks. To this end, we conduct extensive evaluation and comparisons of well-known MARL algorithms on complex fully cooperative benchmarks, including tasks with images as agents' observations. Interestingly, our analysis shows that many algorithms, hailed as state-of-the-art on SMAC and GRF, may underperform standard MARL baselines on fully cooperative benchmarks. Finally, towards more systematic and better evaluation of cooperative MARL algorithms, we have open-sourced PyMARLzoo+, an extension of the widely used (E)PyMARL libraries, which addresses an open challenge from [TBG++21], facilitating seamless integration and support with all benchmarks of PettingZoo, as well as Overcooked, PressurePlate, Capture Target and Box Pushing.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCALE: Towards Collaborative Content Analysis in Social Science with Large Language Model Agents and Human Intervention</title>
<link>https://arxiv.org/abs/2502.10937</link>
<guid>https://arxiv.org/abs/2502.10937</guid>
<content:encoded><![CDATA[
arXiv:2502.10937v2 Announce Type: replace 
Abstract: Content analysis breaks down complex and unstructured texts into theory-informed numerical categories. Particularly, in social science, this process usually relies on multiple rounds of manual annotation, domain expert discussion, and rule-based refinement. In this paper, we introduce SCALE, a novel multi-agent framework that effectively $\underline{\textbf{S}}$imulates $\underline{\textbf{C}}$ontent $\underline{\textbf{A}}$nalysis via $\underline{\textbf{L}}$arge language model (LLM) ag$\underline{\textbf{E}}$nts. SCALE imitates key phases of content analysis, including text coding, collaborative discussion, and dynamic codebook evolution, capturing the reflective depth and adaptive discussions of human researchers. Furthermore, by integrating diverse modes of human intervention, SCALE is augmented with expert input to further enhance its performance. Extensive evaluations on real-world datasets demonstrate that SCALE achieves human-approximated performance across various complex content analysis tasks, offering an innovative potential for future social science research.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Near-Linear MIR Algorithms for Stochastically-Ordered Priors</title>
<link>https://arxiv.org/abs/2502.12766</link>
<guid>https://arxiv.org/abs/2502.12766</guid>
<content:encoded><![CDATA[
arXiv:2502.12766v2 Announce Type: replace 
Abstract: With the rise of online applications, recommender systems (RSs) often encounter constraints in balancing exploration and exploitation. Such constraints arise when exploration is carried out by agents whose utility must be taken into account when optimizing overall welfare. A recent work by Bahar et al. (2020) suggests that recommendations should be \emph{mechanism-informed individually rational} (MIR). Specifically, if agents have a default arm they would use, relying on the RS should yield each agent at least the reward of the default arm, conditioned on the information available to the RS. Under the MIR constraint, striking a balance between exploration and exploitation becomes a complex planning problem. To that end, Bahar et al. propose an approximately optimal yet inefficient planning algorithm that runs in $O(2^K K^2 H^2)$, where $K$ is the number of arms and $H$ is the size of the support of the reward distributions. In this paper, we make a significant improvement for a special yet practical case, removing both the dependence on $H$ and the exponential dependence on $K$. We assume a stochastic order of the rewards (e.g., Gaussian with unit variance, Bernoulli, etc.), and devise an asymptotically optimal algorithm with a runtime of $O(K \log K)$. Our technique is based on formulating a Goal Markov Decision Process (GMDP), establishing an optimal dynamic programming procedure, and then unveiling its crux -- fleshing out a simple index-based structure that facilitates efficient computation. Additionally, we present an incentive-compatible version of our algorithm.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CritiQ: Mining Data Quality Criteria from Human Preferences</title>
<link>https://arxiv.org/abs/2502.19279</link>
<guid>https://arxiv.org/abs/2502.19279</guid>
<content:encoded><![CDATA[
arXiv:2502.19279v2 Announce Type: replace 
Abstract: Language model heavily depends on high-quality data for optimal performance. Existing approaches rely on manually designed heuristics, the perplexity of existing models, training classifiers, or careful prompt engineering, which require significant expert experience and human annotation effort while introduce biases. We introduce CritiQ, a novel data selection method that automatically mines criteria from human preferences for data quality with only ~30 human-annotated pairs and performs efficient data selection. The main component, CritiQ Flow, employs a manager agent to evolve quality criteria and worker agents to make pairwise judgments. We build a knowledge base that extracts quality criteria from previous work to boost CritiQ Flow. Compared to perplexity- and classifier- based methods, verbal criteria are more interpretable and possess reusable value. After deriving the criteria, we train the CritiQ Scorer to give quality scores and perform efficient data selection. We demonstrate the effectiveness of our method in the code, math, and logic domains, achieving high accuracy on human-annotated test sets. To validate the quality of the selected data, we continually train Llama 3.1 models and observe improved performance on downstream tasks compared to uniform sampling. Ablation studies validate the benefits of the knowledge base and the reflection process. We analyze how criteria evolve and the effectiveness of majority voting.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebNav: An Intelligent Agent for Voice-Controlled Web Navigation</title>
<link>https://arxiv.org/abs/2503.13843</link>
<guid>https://arxiv.org/abs/2503.13843</guid>
<content:encoded><![CDATA[
arXiv:2503.13843v2 Announce Type: replace 
Abstract: The current state of modern web interfaces, especially in regards to accessibility focused usage is extremely lacking. Traditional methods for web interaction, such as scripting languages and screen readers, often lack the flexibility to handle dynamic content or the intelligence to interpret high-level user goals. To address these limitations, we introduce WebNav, a novel agent for multi-modal web navigation. WebNav leverages a dual Large Language Model (LLM) architecture to translate natural language commands into precise, executable actions on a graphical user interface. The system combines vision-based context from screenshots with a dynamic DOM-labeling browser extension to robustly identify interactive elements. A high-level 'Controller' LLM strategizes the next step toward a user's goal, while a second 'Assistant' LLM generates the exact parameters for execution. This separation of concerns allows for sophisticated task decomposition and action formulation. Our work presents the complete architecture and implementation of WebNav, demonstrating a promising approach to creating more intelligent web automation agents.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement learning for robust dynamic metabolic control</title>
<link>https://arxiv.org/abs/2504.00735</link>
<guid>https://arxiv.org/abs/2504.00735</guid>
<content:encoded><![CDATA[
arXiv:2504.00735v2 Announce Type: replace 
Abstract: Dynamic metabolic control enables key metabolic fluxes to be modulated in real time, enhancing bioprocess flexibility and expanding the available optimization degrees of freedom. This can be achieved, e.g., via targeted modulation of metabolic enzyme expression. However, identifying optimal dynamic control policies in metabolic engineering is challenging due to the generally high-dimensional solution space, and the need to manage potential metabolic burden and cytotoxic effects, arising from inducible enzyme expression. This task is further complicated by the presence of stochastic dynamics, which reduce the reproducibility of bioprocesses. Here, we propose a reinforcement learning framework to derive optimal dynamic metabolic control policies by allowing an agent (i.e., the controller) to interact with a surrogate dynamic model $\textit{in silico}$. To promote robustness in the metabolic control policies, we apply domain randomization, enabling the controller to generalize across system uncertainties. Our framework provides an alternative to conventional model-based control such as model predictive control, which requires model differentiation with respect to decision variables; an often impractical task when dealing with complex stochastic, nonlinear, stiff, and piecewise-defined dynamics. In contrast, our approach only requires forward integration, making the task much simpler. We demonstrate our framework in two $\textit{Escherichia coli}$ bioprocesses, one involving the dynamic control of acetyl-CoA carboxylase in the synthesis of fatty acids, and another one dealing with the dynamic control of adenosine triphosphatase in the synthesis of lactate.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Role of Feedback in Test-Time Scaling of Agentic AI Workflows</title>
<link>https://arxiv.org/abs/2504.01931</link>
<guid>https://arxiv.org/abs/2504.01931</guid>
<content:encoded><![CDATA[
arXiv:2504.01931v3 Announce Type: replace 
Abstract: Agentic AI workflows (systems that autonomously plan and act) are becoming widespread, yet their task success rate on complex tasks remains low. A promising solution is inference-time alignment, which uses extra compute at test time to improve performance. Inference-time alignment relies on three components: sampling, evaluation, and feedback. While most prior work studies sampling and automatic evaluation, feedback remains underexplored. To study the role of feedback, we introduce Iterative Agent Decoding (IAD), a procedure that repeatedly inserts feedback extracted from different forms of critiques (reward models or AI-generated textual feedback) between decoding steps. Through IAD, we analyze feedback along four dimensions: (1) its role in the accuracy-compute trade-offs with limited inference budget, (2) quantifying the gains over diversity-only baselines such as best-of-N sampling, (3) effectiveness of composing feedback from reward models versus textual critique, and (4) robustness to noisy or low-quality feedback. Across Sketch2Code, Text2SQL, Intercode, and WebShop, we show that IAD with proper integration of high fidelity feedback leads to consistent gains up to 10 percent absolute performance improvement over various baselines such as best-of-N. Our findings underscore feedback as a crucial knob for inference-time alignment of agentic AI workflows with limited inference budget.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward</title>
<link>https://arxiv.org/abs/2504.03206</link>
<guid>https://arxiv.org/abs/2504.03206</guid>
<content:encoded><![CDATA[
arXiv:2504.03206v2 Announce Type: replace 
Abstract: Effective conversational agents like large language models (LLMs) must personalize their interactions to adapt to user preferences, personalities, and attributes across diverse domains like education and healthcare. Current methods like Reinforcement Learning from Human Feedback (RLHF), often prioritize helpfulness and safety but fall short in fostering truly empathetic, adaptive, and personalized dialogues. Existing personalization approaches typically rely on extensive user history, limiting their effectiveness for new or context-limited users. To address these limitations, we propose leveraging a user model to incorporate a curiosity-based intrinsic reward into multi-turn RLHF. This novel reward mechanism encourages the LLM agent to actively infer user traits by optimizing conversations to improve its user model's accuracy. Consequently, the agent delivers more personalized interactions by learning more about the user. We demonstrate our method's effectiveness in two distinct domains: significantly improving personalization performance in a conversational recommendation task, and personalizing conversations for different learning styles in an educational setting. We show improved generalization capabilities compared to traditional multi-turn RLHF, all while maintaining conversation quality. Our method offers a promising solution for creating more personalized, adaptive, and engaging conversational agents.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When is Truthfully Allocating Chores no Harder than Goods?</title>
<link>https://arxiv.org/abs/2505.01629</link>
<guid>https://arxiv.org/abs/2505.01629</guid>
<content:encoded><![CDATA[
arXiv:2505.01629v2 Announce Type: replace 
Abstract: We study the problem of fairly and efficiently allocating a set of items among strategic agents with additive valuations, where items are either all indivisible or all divisible. When items are goods, numerous positive and negative results are known regarding the fairness and efficiency guarantees achievable by truthful mechanisms, whereas our understanding of truthful mechanisms for chores remains considerably more limited. In this paper, we discover various connections between truthful good and chore allocations, greatly enhancing our understanding of the latter via tools from the former.
  For indivisible chores with two agents, by leveraging the observation that a simple bundle-swapping operation transforms several properties for goods including truthfulness to the corresponding properties for chores, we characterize truthful mechanisms and derive tight guarantees of various fairness notions achieved by truthful mechanisms. Moreover, for homogeneous divisible chores, by generalizing the above transformation to an arbitrary number of agents, we characterize truthful mechanisms with two agents, show that every truthful mechanism with two agents admits an efficiency ratio of $0$, and derive a large family of strictly truthful, envy-free (EF), and proportional mechanisms for an arbitrary number of agents. Finally, for indivisible chores with an arbitrary number of agents having bi-valued cost functions, we give an ex-ante truthful, ex-ante Pareto optimal, ex-ante EF, and ex-post envy-free up to one item mechanism, improving the best guarantees for bi-valued instances by prior works.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models</title>
<link>https://arxiv.org/abs/2505.04921</link>
<guid>https://arxiv.org/abs/2505.04921</guid>
<content:encoded><![CDATA[
arXiv:2505.04921v2 Announce Type: replace 
Abstract: Reasoning lies at the heart of intelligence, shaping the ability to make decisions, draw conclusions, and generalize across domains. In artificial intelligence, as systems increasingly operate in open, uncertain, and multimodal environments, reasoning becomes essential for enabling robust and adaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a promising paradigm, integrating modalities such as text, images, audio, and video to support complex reasoning capabilities and aiming to achieve comprehensive perception, precise understanding, and deep reasoning. As research advances, multimodal reasoning has rapidly evolved from modular, perception-driven pipelines to unified, language-centric frameworks that offer more coherent cross-modal understanding. While instruction tuning and reinforcement learning have improved model reasoning, significant challenges remain in omni-modal generalization, reasoning depth, and agentic behavior. To address these issues, we present a comprehensive and structured survey of multimodal reasoning research, organized around a four-stage developmental roadmap that reflects the field's shifting design philosophies and emerging capabilities. First, we review early efforts based on task-specific modules, where reasoning was implicitly embedded across stages of representation, alignment, and fusion. Next, we examine recent approaches that unify reasoning into multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT) and multimodal reinforcement learning enabling richer and more structured reasoning chains. Finally, drawing on empirical insights from challenging benchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the conceptual direction of native large multimodal reasoning models (N-LMRMs), which aim to support scalable, agentic, and adaptive reasoning and planning in complex, real-world environments.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning under State and Outcome Uncertainty: A Foundational Distributional Perspective</title>
<link>https://arxiv.org/abs/2505.06518</link>
<guid>https://arxiv.org/abs/2505.06518</guid>
<content:encoded><![CDATA[
arXiv:2505.06518v2 Announce Type: replace 
Abstract: In many real-world planning tasks, agents must tackle uncertainty about the environment's state and variability in the outcomes of any chosen policy. We address both forms of uncertainty as a first step toward safer algorithms in partially observable settings. Specifically, we extend Distributional Reinforcement Learning (DistRL)-which models the entire return distribution for fully observable domains-to Partially Observable Markov Decision Processes (POMDPs), allowing an agent to learn the distribution of returns for each conditional plan. Concretely, we introduce new distributional Bellman operators for partial observability and prove their convergence under the supremum p-Wasserstein metric. We also propose a finite representation of these return distributions via psi-vectors, generalizing the classical alpha-vectors in POMDP solvers. Building on this, we develop Distributional Point-Based Value Iteration (DPBVI), which integrates psi-vectors into a standard point-based backup procedure-bridging DistRL and POMDP planning. By tracking return distributions, DPBVI naturally enables risk-sensitive control in domains where rare, high-impact events must be carefully managed. We provide source code to foster further research in robust decision-making under partial observability.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perspectives on Unsolvability in the Roommates Problem</title>
<link>https://arxiv.org/abs/2505.06717</link>
<guid>https://arxiv.org/abs/2505.06717</guid>
<content:encoded><![CDATA[
arXiv:2505.06717v2 Announce Type: replace 
Abstract: In the well-studied Stable Roommates problem, we seek a stable matching of agents into pairs, where no two agents prefer each other over their assigned partners. However, some instances of this problem are unsolvable, lacking any stable matching. A long-standing open question posed by Gusfield and Irving (1989) asks about the behavior of the probability function Pn, which measures the likelihood that a random instance with n agents is solvable.
  This paper provides a comprehensive analysis of the landscape surrounding this question, combining structural, probabilistic, and experimental perspectives. We review existing approaches from the past four decades, highlight connections to related problems, and present novel structural and experimental findings. Specifically, we estimate Pn for instances with preferences sampled from diverse statistical distributions, examining problem sizes up to 5,001 agents, and look for specific sub-structures that cause unsolvability. Our results reveal that while Pn tends to be low for most distributions, the number and lengths of "unstable" structures remain limited, suggesting that random instances are "close" to being solvable.
  Additionally, we present the first empirical study of the number of stable matchings and the number of stable partitions that random instances admit, using recently developed algorithms. Our findings show that the solution sets are typically small. This implies that many NP-hard problems related to computing optimal stable matchings and optimal stable partitions become tractable in practice, and motivates efficient alternative solution concepts for unsolvable instances, such as stable half-matchings and maximum stable matchings.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aggregating Concepts of Fairness and Accuracy in Prediction Algorithms</title>
<link>https://arxiv.org/abs/2505.08829</link>
<guid>https://arxiv.org/abs/2505.08829</guid>
<content:encoded><![CDATA[
arXiv:2505.08829v3 Announce Type: replace 
Abstract: An algorithm that outputs predictions about the state of the world will almost always be designed with the implicit or explicit goal of outputting accurate predictions (i.e., predictions that are likely to be true). In addition, the rise of increasingly powerful predictive algorithms brought about by the recent revolution in artificial intelligence has led to an emphasis on building predictive algorithms that are fair, in the sense that their predictions do not systematically evince bias or bring about harm to certain individuals or groups. This state of affairs presents two conceptual challenges. First, the goals of accuracy and fairness can sometimes be in tension, and there are no obvious normative guidelines for managing the trade-offs between these two desiderata when they arise. Second, there are many distinct ways of measuring both the accuracy and fairness of a predictive algorithm; here too, there are no obvious guidelines on how to aggregate our preferences for predictive algorithms that satisfy disparate measures of fairness and accuracy to various extents. The goal of this paper is to address these challenges by arguing that there are good reasons for using a linear combination of accuracy and fairness metrics to measure the all-things-considered value of a predictive algorithm for agents who care about both accuracy and fairness. My argument depends crucially on a classic result in the preference aggregation literature due to Harsanyi. After making this formal argument, I apply my result to an analysis of accuracy-fairness trade-offs using the COMPAS dataset compiled by Angwin et al.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReviewInstruct: A Review-Driven Multi-Turn Conversations Generation Method for Large Language Models</title>
<link>https://arxiv.org/abs/2505.11010</link>
<guid>https://arxiv.org/abs/2505.11010</guid>
<content:encoded><![CDATA[
arXiv:2505.11010v2 Announce Type: replace 
Abstract: The effectiveness of large language models (LLMs) in conversational AI is hindered by their reliance on single-turn supervised fine-tuning (SFT) data, which limits contextual coherence in multi-turn dialogues. Existing methods for generating multi-turn dialogue data struggle to ensure both diversity and quality in instructions. To address this, we propose Review-Instruct, a novel framework that synthesizes multi-turn conversations through an iterative "Ask-Respond-Review" process involving three agent roles: a Candidate, multiple Reviewers, and a Chairman. The framework iteratively refines instructions by incorporating Reviewer feedback, enhancing dialogue diversity and difficulty. We construct a multi-turn dataset using the Alpaca dataset and fine-tune the LLaMA2-13B model. Evaluations on MT-Bench, MMLU-Pro, and Auto-Arena demonstrate significant improvements, achieving absolute gains of 2.9\% on MMLU-Pro and 2\% on MT-Bench compared to prior state-of-the-art models based on LLaMA2-13B. Ablation studies confirm the critical role of the Review stage and the use of multiple Reviewers in boosting instruction diversity and difficulty. Our work highlights the potential of review-driven, multi-agent frameworks for generating high-quality conversational data at scale.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Distributed Local Energy Market Clearing Framework Using a Two-Loop ADMM Method</title>
<link>https://arxiv.org/abs/2505.16070</link>
<guid>https://arxiv.org/abs/2505.16070</guid>
<content:encoded><![CDATA[
arXiv:2505.16070v2 Announce Type: replace 
Abstract: The diversity of prosumers' resources in energy communities can provide significant technical and economic benefits to both prosumers and the distribution system operator (DSO). To maximize these benefits, a coordination framework is required to address all techno-economic constraints as well as the objectives of all agents. This paper presents a fully distributed market-clearing scheme to coordinate the strategies of agents within a local energy community. In the proposed framework, prosumers, the DSO, and the local market operator (LMO) are the participating agents. The framework addresses the preferences and techno-economic constraints of all actors while preserving their privacy. The proposed model is based on a modified alternating direction method of multipliers (ADMM) method with two outer and inner loops; the outer loop models the interactions between the LMO and prosumers, while the inner loop addresses the interactions between the LMO and the DSO. The model is demonstrated on IEEE-69bus test network, showcasing its effectiveness from various perspectives.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenAg: Democratizing Agricultural Intelligence</title>
<link>https://arxiv.org/abs/2506.04571</link>
<guid>https://arxiv.org/abs/2506.04571</guid>
<content:encoded><![CDATA[
arXiv:2506.04571v2 Announce Type: replace 
Abstract: Agriculture is undergoing a major transformation driven by artificial intelligence (AI), machine learning, and knowledge representation technologies. However, current agricultural intelligence systems often lack contextual understanding, explainability, and adaptability, especially for smallholder farmers with limited resources. General-purpose large language models (LLMs), while powerful, typically lack the domain-specific knowledge and contextual reasoning needed for practical decision support in farming. They tend to produce recommendations that are too generic or unrealistic for real-world applications. To address these challenges, we present OpenAg, a comprehensive framework designed to advance agricultural artificial general intelligence (AGI). OpenAg combines domain-specific foundation models, neural knowledge graphs, multi-agent reasoning, causal explainability, and adaptive transfer learning to deliver context-aware, explainable, and actionable insights. The system includes: (i) a unified agricultural knowledge base that integrates scientific literature, sensor data, and farmer-generated knowledge; (ii) a neural agricultural knowledge graph for structured reasoning and inference; (iii) an adaptive multi-agent reasoning system where AI agents specialize and collaborate across agricultural domains; and (iv) a causal transparency mechanism that ensures AI recommendations are interpretable, scientifically grounded, and aligned with real-world constraints. OpenAg aims to bridge the gap between scientific knowledge and the tacit expertise of experienced farmers to support scalable and locally relevant agricultural decision-making.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OPeRA: A Dataset of Observation, Persona, Rationale, and Action for Evaluating LLMs on Human Online Shopping Behavior Simulation</title>
<link>https://arxiv.org/abs/2506.05606</link>
<guid>https://arxiv.org/abs/2506.05606</guid>
<content:encoded><![CDATA[
arXiv:2506.05606v3 Announce Type: replace 
Abstract: Can large language models (LLMs) accurately simulate the next web action of a specific user? While LLMs have shown promising capabilities in generating ``believable'' human behaviors, evaluating their ability to mimic real user behaviors remains an open challenge, largely due to the lack of high-quality, publicly available datasets that capture both the observable actions and the internal reasoning of an actual human user. To address this gap, we introduce OPERA, a novel dataset of Observation, Persona, Rationale, and Action collected from real human participants during online shopping sessions. OPERA is the first public dataset that comprehensively captures: user personas, browser observations, fine-grained web actions, and self-reported just-in-time rationales. We developed both an online questionnaire and a custom browser plugin to gather this dataset with high fidelity. Using OPERA, we establish the first benchmark to evaluate how well current LLMs can predict a specific user's next action and rationale with a given persona and  history. This dataset lays the groundwork for future research into LLM agents that aim to act as personalized digital twins for human.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoMind: Adaptive Knowledgeable Agent for Automated Data Science</title>
<link>https://arxiv.org/abs/2506.10974</link>
<guid>https://arxiv.org/abs/2506.10974</guid>
<content:encoded><![CDATA[
arXiv:2506.10974v2 Announce Type: replace 
Abstract: Large Language Model (LLM) agents have shown great potential in addressing real-world data science problems. LLM-driven data science agents promise to automate the entire machine learning pipeline, yet their real-world effectiveness remains limited. Existing frameworks depend on rigid, pre-defined workflows and inflexible coding strategies; consequently, they excel only on relatively simple, classical problems and fail to capture the empirical expertise that human practitioners bring to complex, innovative tasks. In this work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework that overcomes these deficiencies through three key advances: (1) a curated expert knowledge base that grounds the agent in domain expert knowledge, (2) an agentic knowledgeable tree search algorithm that strategically explores possible solutions, and (3) a self-adaptive coding strategy that dynamically tailors code generation to task complexity. Evaluations on two automated data science benchmarks demonstrate that AutoMind delivers superior performance versus state-of-the-art baselines. Additional analyses confirm favorable effectiveness, efficiency, and qualitative solution quality, highlighting AutoMind as an efficient and robust step toward fully automated data science.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuTE: decentralized multiple testing on sensor networks with false discovery rate control</title>
<link>https://arxiv.org/abs/2210.04334</link>
<guid>https://arxiv.org/abs/2210.04334</guid>
<content:encoded><![CDATA[
arXiv:2210.04334v2 Announce Type: replace-cross 
Abstract: This paper designs methods for decentralized multiple hypothesis testing on graphs that are equipped with provable guarantees on the false discovery rate (FDR). We consider the setting where distinct agents reside on the nodes of an undirected graph, and each agent possesses p-values corresponding to one or more hypotheses local to its node. Each agent must individually decide whether to reject one or more of its local hypotheses by only communicating with its neighbors, with the joint aim that the global FDR over the entire graph must be controlled at a predefined level. We propose a simple decentralized family of Query-Test-Exchange (QuTE) algorithms and prove that they can control FDR under independence or positive dependence of the p-values. Our algorithm reduces to the Benjamini-Hochberg (BH) algorithm when after graph-diameter rounds of communication, and to the Bonferroni procedure when no communication has occurred or the graph is empty. To avoid communicating real-valued p-values, we develop a quantized BH procedure, and extend it to a quantized QuTE procedure. QuTE works seamlessly in streaming data settings, where anytime-valid p-values may be continually updated at each node. Last, QuTE is robust to arbitrary dropping of packets, or a graph that changes at every step, making it particularly suitable to mobile sensor networks involving drones or other multi-agent systems. We study the power of our procedure using a simulation suite of different levels of connectivity and communication on a variety of graph structures, and also provide an illustrative real-world example.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An ADMM-Based Approach to Quadratically-Regularized Distributed Optimal Transport on Graphs</title>
<link>https://arxiv.org/abs/2410.05509</link>
<guid>https://arxiv.org/abs/2410.05509</guid>
<content:encoded><![CDATA[
arXiv:2410.05509v2 Announce Type: replace-cross 
Abstract: Optimal transport on a graph focuses on finding the most efficient way to transfer resources from one distribution to another while considering the graph's structure. This paper introduces a new distributed algorithm that solves the optimal transport problem on directed, strongly connected graphs, unlike previous approaches which were limited to bipartite graphs. Our algorithm incorporates quadratic regularization and guarantees convergence using the Alternating Direction Method of Multipliers (ADMM). Notably, it proves convergence not only with quadratic regularization but also in cases without it, whereas earlier works required strictly convex objective functions.
  In this approach, nodes are treated as agents that collaborate through local interactions to optimize the total transportation cost, relying only on information from their neighbors. Through numerical experiments, we show how quadratic regularization affects both convergence behavior and solution sparsity under different graph structures. Additionally, we provide a practical example that highlights the algorithm's robustness through its ability to adjust to topological changes in the graph.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Playground to Democratize Experimentation and Benchmarking of AI Agents for Network Troubleshooting</title>
<link>https://arxiv.org/abs/2507.01997</link>
<guid>https://arxiv.org/abs/2507.01997</guid>
<content:encoded><![CDATA[
arXiv:2507.01997v1 Announce Type: new 
Abstract: Recent research has demonstrated the effectiveness of Artificial Intelligence (AI), and more specifically, Large Language Models (LLMs), in supporting network configuration synthesis and automating network diagnosis tasks, among others. In this preliminary work, we restrict our focus to the application of AI agents to network troubleshooting and elaborate on the need for a standardized, reproducible, and open benchmarking platform, where to build and evaluate AI agents with low operational effort.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Strategy Adaptation in Multi-Agent Environments with Large Language Models</title>
<link>https://arxiv.org/abs/2507.02002</link>
<guid>https://arxiv.org/abs/2507.02002</guid>
<content:encoded><![CDATA[
arXiv:2507.02002v1 Announce Type: new 
Abstract: Large language models (LLMs) demonstrate strong reasoning abilities across mathematical, strategic, and linguistic tasks, yet little is known about how well they reason in dynamic, real-time, multi-agent scenarios, such as collaborative environments in which agents continuously adapt to each other's behavior, as in cooperative gameplay settings. In this paper, we bridge this gap by combining LLM-driven agents with strategic reasoning and real-time adaptation in cooperative, multi-agent environments grounded in game-theoretic principles such as belief consistency and Nash equilibrium. The proposed framework applies broadly to dynamic scenarios in which agents coordinate, communicate, and make decisions in response to continuously changing conditions. We provide real-time strategy refinement and adaptive feedback mechanisms that enable agents to dynamically adjust policies based on immediate contextual interactions, in contrast to previous efforts that evaluate LLM capabilities in static or turn-based settings. Empirical results show that our method achieves up to a 26\% improvement in return over PPO baselines in high-noise environments, while maintaining real-time latency under 1.05 milliseconds. Our approach improves collaboration efficiency, task completion rates, and flexibility, illustrating that game-theoretic guidance integrated with real-time feedback enhances LLM performance, ultimately fostering more resilient and flexible strategic multi-agent systems.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STELLA: Self-Evolving LLM Agent for Biomedical Research</title>
<link>https://arxiv.org/abs/2507.02004</link>
<guid>https://arxiv.org/abs/2507.02004</guid>
<content:encoded><![CDATA[
arXiv:2507.02004v1 Announce Type: new 
Abstract: The rapid growth of biomedical data, tools, and literature has created a fragmented research landscape that outpaces human expertise. While AI agents offer a solution, they typically rely on static, manually curated toolsets, limiting their ability to adapt and scale. Here, we introduce STELLA, a self-evolving AI agent designed to overcome these limitations. STELLA employs a multi-agent architecture that autonomously improves its own capabilities through two core mechanisms: an evolving Template Library for reasoning strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent automatically discovers and integrates new bioinformatics tools. This allows STELLA to learn from experience. We demonstrate that STELLA achieves state-of-the-art accuracy on a suite of biomedical benchmarks, scoring approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench: DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6 percentage points. More importantly, we show that its performance systematically improves with experience; for instance, its accuracy on the Humanity's Last Exam benchmark almost doubles with increased trials. STELLA represents a significant advance towards AI Agent systems that can learn and grow, dynamically scaling their expertise to accelerate the pace of biomedical discovery.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoboBrain 2.0 Technical Report</title>
<link>https://arxiv.org/abs/2507.02029</link>
<guid>https://arxiv.org/abs/2507.02029</guid>
<content:encoded><![CDATA[
arXiv:2507.02029v1 Announce Type: new 
Abstract: We introduce RoboBrain 2.0, our latest generation of embodied vision-language foundation models, designed to unify perception, reasoning, and planning for complex embodied tasks in physical environments. It comes in two variants: a lightweight 7B model and a full-scale 32B model, featuring a heterogeneous architecture with a vision encoder and a language model. Despite its compact size, RoboBrain 2.0 achieves strong performance across a wide spectrum of embodied reasoning tasks. On both spatial and temporal benchmarks, the 32B variant achieves leading results, surpassing prior open-source and proprietary models. In particular, it supports key real-world embodied AI capabilities, including spatial understanding (e.g., affordance prediction, spatial referring, trajectory forecasting) and temporal decision-making (e.g., closed-loop interaction, multi-agent long-horizon planning, and scene graph updating). This report details the model architecture, data construction, multi-stage training strategies, infrastructure and practical applications. We hope RoboBrain 2.0 advances embodied AI research and serves as a practical step toward building generalist embodied agents. The code, checkpoint and benchmark are available at https://superrobobrain.github.io.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs</title>
<link>https://arxiv.org/abs/2507.02076</link>
<guid>https://arxiv.org/abs/2507.02076</guid>
<content:encoded><![CDATA[
arXiv:2507.02076v1 Announce Type: new 
Abstract: Large language models (LLMs) have rapidly progressed into general-purpose agents capable of solving a broad spectrum of tasks. However, current models remain inefficient at reasoning: they apply fixed inference-time compute regardless of task complexity, often overthinking simple problems while underthinking hard ones. This survey presents a comprehensive review of efficient test-time compute (TTC) strategies, which aim to improve the computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy that distinguishes between L1-controllability, methods that operate under fixed compute budgets, and L2-adaptiveness, methods that dynamically scale inference based on input difficulty or model confidence. We benchmark leading proprietary LLMs across diverse datasets, highlighting critical trade-offs between reasoning performance and token usage. Compared to prior surveys on efficient reasoning, our review emphasizes the practical control, adaptability, and scalability of TTC methods. Finally, we discuss emerging trends such as hybrid thinking models and identify key challenges for future work towards making LLMs more computationally efficient, robust, and responsive to user constraints.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab</title>
<link>https://arxiv.org/abs/2507.02083</link>
<guid>https://arxiv.org/abs/2507.02083</guid>
<content:encoded><![CDATA[
arXiv:2507.02083v1 Announce Type: new 
Abstract: Designing experiments and result interpretations are core scientific competencies, particularly in biology, where researchers perturb complex systems to uncover the underlying systems. Recent efforts to evaluate the scientific capabilities of large language models (LLMs) fail to test these competencies because wet-lab experimentation is prohibitively expensive: in expertise, time and equipment. We introduce SciGym, a first-in-class benchmark that assesses LLMs' iterative experiment design and analysis abilities in open-ended scientific discovery tasks. SciGym overcomes the challenge of wet-lab costs by running a dry lab of biological systems. These models, encoded in Systems Biology Markup Language, are efficient for generating simulated data, making them ideal testbeds for experimentation on realistically complex systems. We evaluated six frontier LLMs on 137 small systems, and released a total of 350 systems. Our evaluation shows that while more capable models demonstrated superior performance, all models' performance declined significantly as system complexity increased, suggesting substantial room for improvement in the scientific capabilities of LLM agents.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Future is Agentic: Definitions, Perspectives, and Open Challenges of Multi-Agent Recommender Systems</title>
<link>https://arxiv.org/abs/2507.02097</link>
<guid>https://arxiv.org/abs/2507.02097</guid>
<content:encoded><![CDATA[
arXiv:2507.02097v1 Announce Type: new 
Abstract: Large language models (LLMs) are rapidly evolving from passive engines of text generation into agentic entities that can plan, remember, invoke external tools, and co-operate with one another. This perspective paper investigates how such LLM agents (and societies thereof) can transform the design space of recommender systems.
  We introduce a unified formalism that (i) models an individual agent as a tuple comprising its language core, tool set, and hierarchical memory, and (ii) captures a multi-agent recommender as a triple of agents, shared environment, and communication protocol. Within this framework, we present four end-to-end use cases-interactive party planning, synthetic user-simulation for offline evaluation, multi-modal furniture recommendation, and brand-aligned explanation generation-each illustrating a distinct capability unlocked by agentic orchestration.
  We then surface five cross-cutting challenge families: protocol complexity, scalability, hallucination and error propagation, emergent misalignment (including covert collusion), and brand compliance.
  For each, we formalize the problem, review nascent mitigation strategies, and outline open research questions. The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools that keep pace with this new degree of autonomy. By unifying agentic abstractions with recommender objectives, the paper lays the groundwork for the next generation of personalized, trustworthy, and context-rich recommendation services.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Neuroscience Can Teach AI About Learning in Continuously Changing Environments</title>
<link>https://arxiv.org/abs/2507.02103</link>
<guid>https://arxiv.org/abs/2507.02103</guid>
<content:encoded><![CDATA[
arXiv:2507.02103v1 Announce Type: new 
Abstract: Modern AI models, such as large language models, are usually trained once on a huge corpus of data, potentially fine-tuned for a specific task, and then deployed with fixed parameters. Their training is costly, slow, and gradual, requiring billions of repetitions. In stark contrast, animals continuously adapt to the ever-changing contingencies in their environments. This is particularly important for social species, where behavioral policies and reward outcomes may frequently change in interaction with peers. The underlying computational processes are often marked by rapid shifts in an animal's behaviour and rather sudden transitions in neuronal population activity. Such computational capacities are of growing importance for AI systems operating in the real world, like those guiding robots or autonomous vehicles, or for agentic AI interacting with humans online. Can AI learn from neuroscience? This Perspective explores this question, integrating the literature on continual and in-context learning in AI with the neuroscience of learning on behavioral tasks with shifting rules, reward probabilities, or outcomes. We will outline an agenda for how specifically insights from neuroscience may inform current developments in AI in this area, and - vice versa - what neuroscience may learn from AI, contributing to the evolving field of NeuroAI.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synergizing Logical Reasoning, Knowledge Management and Collaboration in Multi-Agent LLM System</title>
<link>https://arxiv.org/abs/2507.02170</link>
<guid>https://arxiv.org/abs/2507.02170</guid>
<content:encoded><![CDATA[
arXiv:2507.02170v1 Announce Type: new 
Abstract: This paper explores the integration of advanced Multi-Agent Systems (MAS) techniques to develop a team of agents with enhanced logical reasoning, long-term knowledge retention, and Theory of Mind (ToM) capabilities. By uniting these core components with optimized communication protocols, we create a novel framework called SynergyMAS, which fosters collaborative teamwork and superior problem-solving skills. The system's effectiveness is demonstrated through a product development team case study, where our approach significantly enhances performance and adaptability. These findings highlight SynergyMAS's potential to tackle complex, real-world challenges.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Bio-Inspired Robotic Trajectory Planning via Self-Supervised RNN</title>
<link>https://arxiv.org/abs/2507.02171</link>
<guid>https://arxiv.org/abs/2507.02171</guid>
<content:encoded><![CDATA[
arXiv:2507.02171v1 Announce Type: new 
Abstract: Trajectory planning in robotics is understood as generating a sequence of joint configurations that will lead a robotic agent, or its manipulator, from an initial state to the desired final state, thus completing a manipulation task while considering constraints like robot kinematics and the environment. Typically, this is achieved via sampling-based planners, which are computationally intensive. Recent advances demonstrate that trajectory planning can also be performed by supervised sequence learning of trajectories, often requiring only a single or fixed number of passes through a neural architecture, thus ensuring a bounded computation time. Such fully supervised approaches, however, perform imitation learning; they do not learn based on whether the trajectories can successfully reach a goal, but try to reproduce observed trajectories. In our work, we build on this approach and propose a cognitively inspired self-supervised learning scheme based on a recurrent architecture for building a trajectory model. We evaluate the feasibility of the proposed method on a task of kinematic planning for a robotic arm. The results suggest that the model is able to learn to generate trajectories only using given paired forward and inverse kinematics models, and indicate that this novel method could facilitate planning for more complex manipulation tasks requiring adaptive solutions.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing COBOL Code Explanations: A Multi-Agents Approach Using Large Language Models</title>
<link>https://arxiv.org/abs/2507.02182</link>
<guid>https://arxiv.org/abs/2507.02182</guid>
<content:encoded><![CDATA[
arXiv:2507.02182v1 Announce Type: new 
Abstract: Common Business Oriented Language (COBOL) is a programming language used to develop business applications that are widely adopted by financial, business, and government agencies. Due to its age, complexity, and declining number of COBOL developers, maintaining COBOL codebases is becoming increasingly challenging. In particular, the lack of documentation makes it difficult for new developers to effectively understand and maintain COBOL systems. Existing research utilizes large language models (LLMs) to explain the functionality of code snippets. However, COBOL presents unique challenges due to its architectural and syntactical differences, which often cause its code to exceed the token window size of LLMs. In this work, we propose a multi-agent approach that leverages two LLM-based agents working collaboratively to generate explanations for functions, files, and the overall project. These agents incorporate together by utilizing contextual information from the codebase into the code explanation prompts. We evaluate the effectiveness of our approach using 14 open-source, real-world COBOL projects. Our results indicate that our approach performs significantly better than the baseline in function code explanation, with improvements of 12.67%, 18.59%, and 0.62% in terms of METEOR, chrF, and SentenceBERT scores, respectively. At the file level, our approach effectively explains both short and long COBOL files that exceed the token window size of LLMs and surpass the baseline by 4.21%, 10.72%, and 14.68% in explaining the purpose, functionality, and clarity of the generated explanation. At the project level, our approach generates explanations that convey the functionality and purpose of 82% of the selected projects.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust</title>
<link>https://arxiv.org/abs/2507.02197</link>
<guid>https://arxiv.org/abs/2507.02197</guid>
<content:encoded><![CDATA[
arXiv:2507.02197v1 Announce Type: new 
Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic data for human behavioral research, ensuring that their outputs remain coherent with their assigned roles has become a critical concern. In this paper, we investigate how consistently LLM-based role-playing agents' stated beliefs about the behavior of the people they are asked to role-play ("what they say") correspond to their actual behavior during role-play ("how they act"). Specifically, we establish an evaluation framework to rigorously measure how well beliefs obtained by prompting the model can predict simulation outcomes in advance. Using an augmented version of the GenAgents persona bank and the Trust Game (a standard economic game used to quantify players' trust and reciprocity), we introduce a belief-behavior consistency metric to systematically investigate how it is affected by factors such as: (1) the types of beliefs we elicit from LLMs, like expected outcomes of simulations versus task-relevant attributes of individual characters LLMs are asked to simulate; (2) when and how we present LLMs with relevant information about Trust Game; and (3) how far into the future we ask the model to forecast its actions. We also explore how feasible it is to impose a researcher's own theoretical priors in the event that the originally elicited beliefs are misaligned with research objectives. Our results reveal systematic inconsistencies between LLMs' stated (or imposed) beliefs and the outcomes of their role-playing simulation, at both an individual- and population-level. Specifically, we find that, even when models appear to encode plausible beliefs, they may fail to apply them in a consistent way. These findings highlight the need to identify how and when LLMs' stated beliefs align with their simulated behavior, allowing researchers to use LLM-based agents appropriately in behavioral studies.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.02211</link>
<guid>https://arxiv.org/abs/2507.02211</guid>
<content:encoded><![CDATA[
arXiv:2507.02211v1 Announce Type: new 
Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement learning have shown that static agents can learn to cooperate through a diverse sort of mechanisms, including noise injection, different types of learning algorithms and neighbours' payoff knowledge.In this work, using an independent multi-agent Q-learning algorithm, we study the effects of dilution and mobility in the spatial version of the prisoner's dilemma. Within this setting, different possible actions for the algorithm are defined, connecting with previous results on the classical, non-reinforcement learning spatial prisoner's dilemma, showcasing the versatility of the algorithm in modeling different game-theoretical scenarios and the benchmarking potential of this approach.As a result, a range of effects is observed, including evidence that games with fixed update rules can be qualitatively equivalent to those with learned ones, as well as the emergence of a symbiotic mutualistic effect between populations that forms when multiple actions are defined.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoInfra: A Large-Scale Cooperative Infrastructure Perception System and Dataset in Adverse Weather</title>
<link>https://arxiv.org/abs/2507.02245</link>
<guid>https://arxiv.org/abs/2507.02245</guid>
<content:encoded><![CDATA[
arXiv:2507.02245v1 Announce Type: new 
Abstract: We present CoInfra, a large-scale cooperative infrastructure perception system and dataset designed to advance robust multi-agent perception under real-world and adverse weather conditions. The CoInfra system includes 14 fully synchronized sensor nodes, each equipped with dual RGB cameras and a LiDAR, deployed across a shared region and operating continuously to capture all traffic participants in real-time. A robust, delay-aware synchronization protocol and a scalable system architecture that supports real-time data fusion, OTA management, and remote monitoring are provided in this paper. On the other hand, the dataset was collected in different weather scenarios, including sunny, rainy, freezing rain, and heavy snow and includes 195k LiDAR frames and 390k camera images from 8 infrastructure nodes that are globally time-aligned and spatially calibrated. Furthermore, comprehensive 3D bounding box annotations for five object classes (i.e., car, bus, truck, person, and bicycle) are provided in both global and individual node frames, along with high-definition maps for contextual understanding. Baseline experiments demonstrate the trade-offs between early and late fusion strategies, the significant benefits of HD map integration are discussed. By openly releasing our dataset, codebase, and system documentation at https://github.com/NingMingHao/CoInfra, we aim to enable reproducible research and drive progress in infrastructure-supported autonomous driving, particularly in challenging, real-world settings.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement</title>
<link>https://arxiv.org/abs/2507.02252</link>
<guid>https://arxiv.org/abs/2507.02252</guid>
<content:encoded><![CDATA[
arXiv:2507.02252v1 Announce Type: new 
Abstract: Precise surgical interventions are vital to patient safety, and advanced enhancement algorithms have been developed to assist surgeons in decision-making. Despite significant progress, these algorithms are typically designed for single tasks in specific scenarios, limiting their effectiveness in complex real-world situations. To address this limitation, we propose SurgVisAgent, an end-to-end intelligent surgical vision agent built on multimodal large language models (MLLMs). SurgVisAgent dynamically identifies distortion categories and severity levels in endoscopic images, enabling it to perform a variety of enhancement tasks such as low-light enhancement, overexposure correction, motion blur elimination, and smoke removal. Specifically, to achieve superior surgical scenario understanding, we design a prior model that provides domain-specific knowledge. Additionally, through in-context few-shot learning and chain-of-thought (CoT) reasoning, SurgVisAgent delivers customized image enhancements tailored to a wide range of distortion types and severity levels, thereby addressing the diverse requirements of surgeons. Furthermore, we construct a comprehensive benchmark simulating real-world surgical distortions, on which extensive experiments demonstrate that SurgVisAgent surpasses traditional single-task models, highlighting its potential as a unified solution for surgical assistance.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent</title>
<link>https://arxiv.org/abs/2507.02259</link>
<guid>https://arxiv.org/abs/2507.02259</guid>
<content:encoded><![CDATA[
arXiv:2507.02259v1 Announce Type: new 
Abstract: Despite improvements by length extrapolation, efficient attention and memory modules, handling infinitely long documents with linear complexity without performance degradation during extrapolation remains the ultimate challenge in long-text processing. We directly optimize for long-text tasks in an end-to-end fashion and introduce a novel agent workflow, MemAgent, which reads text in segments and updates the memory using an overwrite strategy. We extend the DAPO algorithm to facilitate training via independent-context multi-conversation generation. MemAgent has demonstrated superb long-context capabilities, being able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task with performance loss < 5% and achieves 95%+ in 512K RULER test.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent</title>
<link>https://arxiv.org/abs/2507.02353</link>
<guid>https://arxiv.org/abs/2507.02353</guid>
<content:encoded><![CDATA[
arXiv:2507.02353v1 Announce Type: new 
Abstract: Keyword decision in Sponsored Search Advertising is critical to the success of ad campaigns. While LLM-based methods offer automated keyword generation, they face three major limitations: reliance on large-scale query-keyword pair data, lack of online multi-objective performance monitoring and optimization, and weak quality control in keyword selection. These issues hinder the agentic use of LLMs in fully automating keyword decisions by monitoring and reasoning over key performance indicators such as impressions, clicks, conversions, and CTA effectiveness. To overcome these challenges, we propose OMS, a keyword generation framework that is On-the-fly (requires no training data, monitors online performance, and adapts accordingly), Multi-objective (employs agentic reasoning to optimize keywords based on multiple performance metrics), and Self-reflective (agentically evaluates keyword quality). Experiments on benchmarks and real-world ad campaigns show that OMS outperforms existing methods; ablation and human evaluations confirm the effectiveness of each component and the quality of generated keywords.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning-Based DRAM Equalizer Parameter Optimization Using Latent Representations</title>
<link>https://arxiv.org/abs/2507.02365</link>
<guid>https://arxiv.org/abs/2507.02365</guid>
<content:encoded><![CDATA[
arXiv:2507.02365v1 Announce Type: new 
Abstract: Equalizer parameter optimization for signal integrity in high-speed Dynamic Random Access Memory systems is crucial but often computationally demanding or model-reliant. This paper introduces a data-driven framework employing learned latent signal representations for efficient signal integrity evaluation, coupled with a model-free Advantage Actor-Critic reinforcement learning agent for parameter optimization. The latent representation captures vital signal integrity features, offering a fast alternative to direct eye diagram analysis during optimization, while the reinforcement learning agent derives optimal equalizer settings without explicit system models. Applied to industry-standard Dynamic Random Access Memory waveforms, the method achieved significant eye-opening window area improvements: 42.7\% for cascaded Continuous-Time Linear Equalizer and Decision Feedback Equalizer structures, and 36.8\% for Decision Feedback Equalizer-only configurations. These results demonstrate superior performance, computational efficiency, and robust generalization across diverse Dynamic Random Access Memory units compared to existing techniques. Core contributions include an efficient latent signal integrity metric for optimization, a robust model-free reinforcement learning strategy, and validated superior performance for complex equalizer architectures.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Consistency in Vehicle Trajectory Prediction Through Preference Optimization</title>
<link>https://arxiv.org/abs/2507.02406</link>
<guid>https://arxiv.org/abs/2507.02406</guid>
<content:encoded><![CDATA[
arXiv:2507.02406v1 Announce Type: new 
Abstract: Trajectory prediction is an essential step in the pipeline of an autonomous vehicle. Inaccurate or inconsistent predictions regarding the movement of agents in its surroundings lead to poorly planned maneuvers and potentially dangerous situations for the end-user. Current state-of-the-art deep-learning-based trajectory prediction models can achieve excellent accuracy on public datasets. However, when used in more complex, interactive scenarios, they often fail to capture important interdependencies between agents, leading to inconsistent predictions among agents in the traffic scene. Inspired by the efficacy of incorporating human preference into large language models, this work fine-tunes trajectory prediction models in multi-agent settings using preference optimization. By taking as input automatically calculated preference rankings among predicted futures in the fine-tuning process, our experiments--using state-of-the-art models on three separate datasets--show that we are able to significantly improve scene consistency while minimally sacrificing trajectory prediction accuracy and without adding any excess computational requirements at inference time.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CyberRAG: An agentic RAG cyber attack classification and reporting tool</title>
<link>https://arxiv.org/abs/2507.02424</link>
<guid>https://arxiv.org/abs/2507.02424</guid>
<content:encoded><![CDATA[
arXiv:2507.02424v1 Announce Type: new 
Abstract: Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can generate hundreds of thousands of alerts per hour, overwhelming security analysts with logs that demand deep, rapidly evolving domain expertise. Conventional machine-learning detectors trim the alert volume but still yield high false-positive rates, while standard single-pass Retrieval-Augmented Generation (RAG) pipelines often retrieve irrelevant context and fail to justify their predictions. To overcome these shortcomings, we present CyberRAG, a modular, agent-based RAG framework that delivers real-time classification, explanation, and structured reporting for cyber-attacks. A central LLM agent orchestrates (i) a pool of fine-tuned specialized classifiers, each tailored to a distinct attack family; (ii) tool adapters for enrichment and alerting; and (iii) an iterative retrieval-and-reason loop that continuously queries a domain-specific knowledge base until the evidence is both relevant and self-consistent. Unlike traditional RAG systems, CyberRAG embraces an agentic design that enables dynamic control flow and adaptive reasoning. This agent-centric architecture refines its threat labels and natural-language justifications autonomously, reducing false positives and enhancing interpretability. The framework is fully extensible: new attack types can be supported by simply adding a classifier without retraining the core agent. CyberRAG has been evaluated achieving over 94% accuracy per class and pushing final classification accuracy to 94.92% through semantic orchestration. Generated explanations score up to 0.94 in BERTScore and 4.9/5 in GPT-4-based expert evaluation. These results show that agentic, specialist-oriented RAG can pair high detection accuracy with trustworthy, SOC-ready prose, offering a practical and scalable path toward semi-autonomous cyber-defence workflows.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Late Collaborative Perception Framework for 3D Multi-Object and Multi-Source Association and Fusion</title>
<link>https://arxiv.org/abs/2507.02430</link>
<guid>https://arxiv.org/abs/2507.02430</guid>
<content:encoded><![CDATA[
arXiv:2507.02430v1 Announce Type: new 
Abstract: In autonomous driving, recent research has increasingly focused on collaborative perception based on deep learning to overcome the limitations of individual perception systems. Although these methods achieve high accuracy, they rely on high communication bandwidth and require unrestricted access to each agent's object detection model architecture and parameters. These constraints pose challenges real-world autonomous driving scenarios, where communication limitations and the need to safeguard proprietary models hinder practical implementation.  To address this issue, we introduce a novel late collaborative framework for 3D multi-source and multi-object fusion, which operates solely on shared 3D bounding box attributes-category, size, position, and orientation-without necessitating direct access to detection models.  Our framework establishes a new state-of-the-art in late fusion, achieving up to five times lower position error compared to existing methods. Additionally, it reduces scale error by a factor of 7.5 and orientation error by half, all while maintaining perfect 100% precision and recall when fusing detections from heterogeneous perception systems. These results highlight the effectiveness of our approach in addressing real-world collaborative perception challenges, setting a new benchmark for efficient and scalable multi-agent fusion.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are You Listening to Me? Fine-Tuning Chatbots for Empathetic Dialogue</title>
<link>https://arxiv.org/abs/2507.02537</link>
<guid>https://arxiv.org/abs/2507.02537</guid>
<content:encoded><![CDATA[
arXiv:2507.02537v1 Announce Type: new 
Abstract: Conversational agents have made significant progress since ELIZA, expanding their role across various domains, including healthcare, education, and customer service. As these agents become increasingly integrated into daily human interactions, the need for emotional intelligence, particularly empathetic listening, becomes increasingly essential. In this study, we explore how Large Language Models (LLMs) respond when tasked with generating emotionally rich interactions. Starting from a small dataset manually crafted by an expert to reflect empathic behavior, we extended the conversations using two LLMs: ChatGPT and Gemini. We analyzed the emotional progression of the dialogues using both sentiment analysis (via VADER) and expert assessments. While the generated conversations often mirrored the intended emotional structure, human evaluation revealed important differences in the perceived empathy and coherence of the responses. These findings suggest that emotion modeling in dialogues requires not only structural alignment in the expressed emotions but also qualitative depth, highlighting the importance of combining automated and humancentered methods in the development of emotionally competent agents.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench</title>
<link>https://arxiv.org/abs/2507.02554</link>
<guid>https://arxiv.org/abs/2507.02554</guid>
<content:encoded><![CDATA[
arXiv:2507.02554v1 Announce Type: new 
Abstract: AI research agents are demonstrating great potential to accelerate scientific progress by automating the design, implementation, and training of machine learning models. We focus on methods for improving agents' performance on MLE-bench, a challenging benchmark where agents compete in Kaggle competitions to solve real-world machine learning problems. We formalize AI research agents as search policies that navigate a space of candidate solutions, iteratively modifying them using operators. By designing and systematically varying different operator sets and search policies (Greedy, MCTS, Evolutionary), we show that their interplay is critical for achieving high performance. Our best pairing of search strategy and operator set achieves a state-of-the-art result on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from 39.6% to 47.7%. Our investigation underscores the importance of jointly considering the search strategy, operator design, and evaluation methodology in advancing automated machine learning.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebSailor: Navigating Super-human Reasoning for Web Agent</title>
<link>https://arxiv.org/abs/2507.02592</link>
<guid>https://arxiv.org/abs/2507.02592</guid>
<content:encoded><![CDATA[
arXiv:2507.02592v1 Announce Type: new 
Abstract: Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all opensource agents in complex information-seeking tasks, matching proprietary agents' performance and closing the capability gap.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making</title>
<link>https://arxiv.org/abs/2507.02616</link>
<guid>https://arxiv.org/abs/2507.02616</guid>
<content:encoded><![CDATA[
arXiv:2507.02616v1 Announce Type: new 
Abstract: The rise of Large Language Models (LLMs) has enabled the development of specialized AI agents with domain-specific reasoning and interaction capabilities, particularly in healthcare. While recent frameworks simulate medical decision-making, they largely focus on single-turn tasks where a doctor agent receives full case information upfront -- diverging from the real-world diagnostic process, which is inherently uncertain, interactive, and iterative. In this paper, we introduce MIMIC-Patient, a structured dataset built from the MIMIC-III electronic health records (EHRs), designed to support dynamic, patient-level simulations. Building on this, we propose DynamiCare, a novel dynamic multi-agent framework that models clinical diagnosis as a multi-round, interactive loop, where a team of specialist agents iteratively queries the patient system, integrates new information, and dynamically adapts its composition and strategy. We demonstrate the feasibility and effectiveness of DynamiCare through extensive experiments, establishing the first benchmark for dynamic clinical decision-making with LLM-powered agents.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory</title>
<link>https://arxiv.org/abs/2507.02618</link>
<guid>https://arxiv.org/abs/2507.02618</guid>
<content:encoded><![CDATA[
arXiv:2507.02618v1 Announce Type: new 
Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able to reason about goals in competitive settings? We present compelling supporting evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for studying decision-making. We conduct the first ever series of evolutionary IPD tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger) against agents from the leading frontier AI companies OpenAI, Google, and Anthropic. By varying the termination probability in each tournament (the "shadow of the future"), we introduce complexity and chance, confounding memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and sometimes even proliferating in these complex ecosystems. Furthermore, they exhibit distinctive and persistent "strategic fingerprints": Google's Gemini models proved strategically ruthless, exploiting cooperative opponents and retaliating against defectors, while OpenAI's models remained highly cooperative, a trait that proved catastrophic in hostile environments. Anthropic's Claude emerged as the most forgiving reciprocator, showing remarkable willingness to restore cooperation even after being exploited or successfully defecting. Analysis of nearly 32,000 prose rationales provided by the models reveals that they actively reason about both the time horizon and their opponent's likely strategy, and we demonstrate that this reasoning is instrumental to their decisions. This work connects classic game theory with machine psychology, offering a rich and granular view of algorithmic decision-making under uncertainty.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VRAgent-R1: Boosting Video Recommendation with MLLM-based Agents via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.02626</link>
<guid>https://arxiv.org/abs/2507.02626</guid>
<content:encoded><![CDATA[
arXiv:2507.02626v1 Announce Type: new 
Abstract: Owing to powerful natural language processing and generative capabilities, large language model (LLM) agents have emerged as a promising solution for enhancing recommendation systems via user simulation. However, in the realm of video recommendation, existing studies predominantly resort to prompt-based simulation using frozen LLMs and encounter the intricate challenge of multimodal content understanding. This frequently results in suboptimal item modeling and user preference learning, thereby ultimately constraining recommendation performance. To address these challenges, we introduce VRAgent-R1, a novel agent-based paradigm that incorporates human-like intelligence in user simulation. Specifically, VRAgent-R1 comprises two distinct agents: the Item Perception (IP) Agent and the User Simulation (US) Agent, designed for interactive user-item modeling. Firstly, the IP Agent emulates human-like progressive thinking based on MLLMs, effectively capturing hidden recommendation semantics in videos. With a more comprehensive multimodal content understanding provided by the IP Agent, the video recommendation system is equipped to provide higher-quality candidate items. Subsequently, the US Agent refines the recommended video sets based on in-depth chain-of-thought (CoT) reasoning and achieves better alignment with real user preferences through reinforcement learning. Experimental results on a large-scale video recommendation benchmark have demonstrated the effectiveness of our proposed VRAgent-R1 method, e.g., the IP Agent achieves a 6.0\% improvement in NDCG@10 on the MicroLens-100k dataset, while the US Agent shows approximately 45.0\% higher accuracy in user decision simulation compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Efficient Bayesian Exploration in Model-Based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.02639</link>
<guid>https://arxiv.org/abs/2507.02639</guid>
<content:encoded><![CDATA[
arXiv:2507.02639v1 Announce Type: new 
Abstract: In this work, we address the challenge of data-efficient exploration in reinforcement learning by examining existing principled, information-theoretic approaches to intrinsic motivation. Specifically, we focus on a class of exploration bonuses that targets epistemic uncertainty rather than the aleatoric noise inherent in the environment. We prove that these bonuses naturally signal epistemic information gains and converge to zero once the agent becomes sufficiently certain about the environment's dynamics and rewards, thereby aligning exploration with genuine knowledge gaps. Our analysis provides formal guarantees for IG-based approaches, which previously lacked theoretical grounding. To enable practical use, we also discuss tractable approximations via sparse variational Gaussian Processes, Deep Kernels and Deep Ensemble models. We then outline a general framework - Predictive Trajectory Sampling with Bayesian Exploration (PTS-BE) - which integrates model-based planning with information-theoretic bonuses to achieve sample-efficient deep exploration. We empirically demonstrate that PTS-BE substantially outperforms other baselines across a variety of environments characterized by sparse rewards and/or purely exploratory tasks.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search</title>
<link>https://arxiv.org/abs/2507.02652</link>
<guid>https://arxiv.org/abs/2507.02652</guid>
<content:encoded><![CDATA[
arXiv:2507.02652v1 Announce Type: new 
Abstract: Complex information needs in real-world search scenarios demand deep reasoning and knowledge synthesis across diverse sources, which traditional retrieval-augmented generation (RAG) pipelines struggle to address effectively. Current reasoning-based approaches suffer from a fundamental limitation: they use a single model to handle both high-level planning and detailed execution, leading to inefficient reasoning and limited scalability. In this paper, we introduce HiRA, a hierarchical framework that separates strategic planning from specialized execution. Our approach decomposes complex search tasks into focused subtasks, assigns each subtask to domain-specific agents equipped with external tools and reasoning capabilities, and coordinates the results through a structured integration mechanism. This separation prevents execution details from disrupting high-level reasoning while enabling the system to leverage specialized expertise for different types of information processing. Experiments on four complex, cross-modal deep search benchmarks demonstrate that HiRA significantly outperforms state-of-the-art RAG and agent-based systems. Our results show improvements in both answer quality and system efficiency, highlighting the effectiveness of decoupled planning and execution for multi-step information seeking tasks. Our code is available at https://github.com/ignorejjj/HiRA.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design &amp; Verification</title>
<link>https://arxiv.org/abs/2507.02660</link>
<guid>https://arxiv.org/abs/2507.02660</guid>
<content:encoded><![CDATA[
arXiv:2507.02660v1 Announce Type: new 
Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is their development process. Hardware design verification entails a methodical and disciplined approach to the planning, development, execution, and sign-off of functionally correct hardware designs. This tedious process requires significant effort and time to ensure a bug-free tape-out. The field of Natural Language Processing has undergone a significant transformation with the advent of Large Language Models (LLMs). These powerful models, often referred to as Generative AI (GenAI), have revolutionized how machines understand and generate human language, enabling unprecedented advancements in a wide array of applications, including hardware design verification. This paper presents an agentic AI-based approach to hardware design verification, which empowers AI agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage in a more dynamic, iterative, and self-reflective process, ultimately performing end-to-end hardware design and verification. This methodology is evaluated on five open-source designs, achieving over 95% coverage with reduced verification time while demonstrating superior performance, adaptability, and configurability.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TUC-PPO: Team Utility-Constrained Proximal Policy Optimization for Spatial Public Goods Games</title>
<link>https://arxiv.org/abs/2507.02675</link>
<guid>https://arxiv.org/abs/2507.02675</guid>
<content:encoded><![CDATA[
arXiv:2507.02675v1 Announce Type: new 
Abstract: We introduce Team Utility-Constrained Proximal Policy Optimization (TUC-PPO), a new deep reinforcement learning framework. It extends Proximal Policy Optimization (PPO) by integrating team welfare objectives specifically for spatial public goods games. Unlike conventional approaches where cooperation emerges indirectly from individual rewards, TUC-PPO instead optimizes a bi-level objective integrating policy gradients and team utility constraints. Consequently, all policy updates explicitly incorporate collective payoff thresholds. The framework preserves PPO's policy gradient core while incorporating constrained optimization through adaptive Lagrangian multipliers. Therefore, decentralized agents dynamically balance selfish and cooperative incentives. The comparative analysis demonstrates superior performance of this constrained deep reinforcement learning approach compared to unmodified PPO and evolutionary game theory baselines. It achieves faster convergence to cooperative equilibria and greater stability against invasion by defectors. The framework formally integrates team objectives into policy updates. This work advances multi-agent deep reinforcement learning for social dilemmas while providing new computational tools for evolutionary game theory research.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Convergence of Large Language Model Optimizer for Black-Box Network Management</title>
<link>https://arxiv.org/abs/2507.02689</link>
<guid>https://arxiv.org/abs/2507.02689</guid>
<content:encoded><![CDATA[
arXiv:2507.02689v1 Announce Type: new 
Abstract: Future wireless networks are expected to incorporate diverse services that often lack general mathematical models. To address such black-box network management tasks, the large language model (LLM) optimizer framework, which leverages pretrained LLMs as optimization agents, has recently been promoted as a promising solution. This framework utilizes natural language prompts describing the given optimization problems along with past solutions generated by LLMs themselves. As a result, LLMs can obtain efficient solutions autonomously without knowing the mathematical models of the objective functions. Although the viability of the LLM optimizer (LLMO) framework has been studied in various black-box scenarios, it has so far been limited to numerical simulations. For the first time, this paper establishes a theoretical foundation for the LLMO framework. With careful investigations of LLM inference steps, we can interpret the LLMO procedure as a finite-state Markov chain, and prove the convergence of the framework. Our results are extended to a more advanced multiple LLM architecture, where the impact of multiple LLMs is rigorously verified in terms of the convergence rate. Comprehensive numerical simulations validate our theoretical results and provide a deeper understanding of the underlying mechanisms of the LLMO framework.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Reinforcement Learning for Dynamic Pricing in Supply Chains: Benchmarking Strategic Agent Behaviours under Realistically Simulated Market Conditions</title>
<link>https://arxiv.org/abs/2507.02698</link>
<guid>https://arxiv.org/abs/2507.02698</guid>
<content:encoded><![CDATA[
arXiv:2507.02698v1 Announce Type: new 
Abstract: This study investigates how Multi-Agent Reinforcement Learning (MARL) can improve dynamic pricing strategies in supply chains, particularly in contexts where traditional ERP systems rely on static, rule-based approaches that overlook strategic interactions among market actors. While recent research has applied reinforcement learning to pricing, most implementations remain single-agent and fail to model the interdependent nature of real-world supply chains. This study addresses that gap by evaluating the performance of three MARL algorithms: MADDPG, MADQN, and QMIX against static rule-based baselines, within a simulated environment informed by real e-commerce transaction data and a LightGBM demand prediction model. Results show that rule-based agents achieve near-perfect fairness (Jain's Index: 0.9896) and the highest price stability (volatility: 0.024), but they fully lack competitive dynamics. Among MARL agents, MADQN exhibits the most aggressive pricing behaviour, with the highest volatility and the lowest fairness (0.5844). MADDPG provides a more balanced approach, supporting market competition (share volatility: 9.5 pp) while maintaining relatively high fairness (0.8819) and stable pricing. These findings suggest that MARL introduces emergent strategic behaviour not captured by static pricing rules and may inform future developments in dynamic pricing.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Control at Stake: Evaluating the Security Landscape of LLM-Driven Email Agents</title>
<link>https://arxiv.org/abs/2507.02699</link>
<guid>https://arxiv.org/abs/2507.02699</guid>
<content:encoded><![CDATA[
arXiv:2507.02699v1 Announce Type: new 
Abstract: The increasing capabilities of LLMs have led to the rapid proliferation of LLM agent apps, where developers enhance LLMs with access to external resources to support complex task execution. Among these, LLM email agent apps represent one of the widely used categories, as email remains a critical communication medium for users. LLM email agents are capable of managing and responding to email using LLM-driven reasoning and autonomously executing user instructions via external email APIs (e.g., send email). However, despite their growing deployment and utility, the security mechanism of LLM email agent apps remains underexplored. Currently, there is no comprehensive study into the potential security risk within these agent apps and their broader implications.
  In this paper, we conduct the first in-depth and systematic security study of LLM email agents. We propose the Email Agent Hijacking (EAH) attack, which overrides the original prompts of the email agent via external email resources, allowing attackers to gain control of the email agent remotely and further perform specific attack scenarios without user awareness.
  To facilitate the large-scale evaluation, we propose EAHawk, a pipeline to evaluate the EAH attack of LLM email agent apps. By EAHawk, we performed an empirical study spanning 14 representative LLM agent frameworks, 63 agent apps, 12 LLMs, and 20 email services, which led to the generation of 1,404 real-world email agent instances for evaluation. Experimental results indicate that all 1,404 instances were successfully hijacked; on average, only 2.03 attack attempts are required to control an email agent instance. Even worse, for some LLMs, the average number of attempts needed to achieve full agent control drops to as few as 1.23.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Forget-and-Grow Strategy for Deep Reinforcement Learning Scaling in Continuous Control</title>
<link>https://arxiv.org/abs/2507.02712</link>
<guid>https://arxiv.org/abs/2507.02712</guid>
<content:encoded><![CDATA[
arXiv:2507.02712v1 Announce Type: new 
Abstract: Deep reinforcement learning for continuous control has recently achieved impressive progress. However, existing methods often suffer from primacy bias, a tendency to overfit early experiences stored in the replay buffer, which limits an RL agent's sample efficiency and generalizability. In contrast, humans are less susceptible to such bias, partly due to infantile amnesia, where the formation of new neurons disrupts early memory traces, leading to the forgetting of initial experiences. Inspired by this dual processes of forgetting and growing in neuroscience, in this paper, we propose Forget and Grow (FoG), a new deep RL algorithm with two mechanisms introduced. First, Experience Replay Decay (ER Decay) "forgetting early experience", which balances memory by gradually reducing the influence of early experiences. Second, Network Expansion, "growing neural capacity", which enhances agents' capability to exploit the patterns of existing data by dynamically adding new parameters during training. Empirical results on four major continuous control benchmarks with more than 40 tasks demonstrate the superior performance of FoG against SoTA existing deep RL algorithms, including BRO, SimBa, and TD-MPC2.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving</title>
<link>https://arxiv.org/abs/2507.02726</link>
<guid>https://arxiv.org/abs/2507.02726</guid>
<content:encoded><![CDATA[
arXiv:2507.02726v1 Announce Type: new 
Abstract: Reasoning remains a challenging task for large language models (LLMs), especially within the logically constrained environment of automated theorem proving (ATP), due to sparse rewards and the vast scale of proofs. These challenges are amplified in benchmarks like PutnamBench, which contains university-level problems requiring complex, multi-step reasoning. To address this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new framework in which agents generate and pursue their subgoals based on the evolving proof state. Given this more structured generation of goals, the resulting problem becomes more amenable to search. We then apply Monte Carlo Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B) solves 26 problems, achieving new state-of-the-art results with models at this scale.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks</title>
<link>https://arxiv.org/abs/2507.02735</link>
<guid>https://arxiv.org/abs/2507.02735</guid>
<content:encoded><![CDATA[
arXiv:2507.02735v1 Announce Type: new 
Abstract: Prompt injection attacks pose a significant security threat to LLM-integrated applications. Model-level defenses have shown strong effectiveness, but are currently deployed into commercial-grade models in a closed-source manner. We believe open-source models are needed by the AI security community, where co-development of attacks and defenses through open research drives scientific progress in mitigation against prompt injection attacks. To this end, we develop Meta SecAlign, the first open-source and open-weight LLM with built-in model-level defense that achieves commercial-grade model performance. We provide complete details of our training recipe, which utilizes an improved version of the SOTA SecAlign defense. Evaluations on 9 utility benchmarks and 7 security benchmarks show that Meta SecAlign, despite being trained on a generic instruction-tuning dataset, confers security in unseen downstream tasks, including tool-calling and agentic web navigation, in addition general instruction-following. Our best model -- Meta-SecAlign-70B -- achieves state-of-the-art robustness against prompt injection attacks and comparable utility to closed-source commercial LLM with model-level defense.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work</title>
<link>https://arxiv.org/abs/2507.02760</link>
<guid>https://arxiv.org/abs/2507.02760</guid>
<content:encoded><![CDATA[
arXiv:2507.02760v1 Announce Type: new 
Abstract: The capabilities of Large Language Models (LLMs) have opened new frontiers for interacting with complex, domain-specific knowledge. However, prevailing methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic AI, while powerful, often struggle with tasks that demand deep, procedural, and methodological reasoning inherent to expert domains. RAG provides factual context but fails to convey logical frameworks; autonomous agents can be inefficient and unpredictable without domain-specific heuristics. To bridge this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm focused on systematically translating human expert knowledge, often expressed in natural language documents, into a machine-executable Knowledge Protocol (KP). KPE shifts the focus from merely augmenting LLMs with fragmented information to endowing them with a domain's intrinsic logic, operational strategies, and methodological principles. We argue that a well-engineered Knowledge Protocol allows a generalist LLM to function as a specialist, capable of decomposing abstract queries and executing complex, multi-step tasks. This position paper defines the core principles of KPE, differentiates it from related concepts, and illustrates its potential applicability across diverse fields such as law and bioinformatics, positing it as a foundational methodology for the future of human-AI collaboration.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs</title>
<link>https://arxiv.org/abs/2507.02773</link>
<guid>https://arxiv.org/abs/2507.02773</guid>
<content:encoded><![CDATA[
arXiv:2507.02773v1 Announce Type: new 
Abstract: Medical diagnosis prediction plays a critical role in disease detection and personalized healthcare. While machine learning (ML) models have been widely adopted for this task, their reliance on supervised training limits their ability to generalize to unseen cases, particularly given the high cost of acquiring large, labeled datasets. Large language models (LLMs) have shown promise in leveraging language abilities and biomedical knowledge for diagnosis prediction. However, they often suffer from hallucinations, lack structured medical reasoning, and produce useless outputs. To address these challenges, we propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves LLM-based diagnosis prediction through a multi-agent architecture. Our framework consists of a linkage agent for attribute mapping, a retrieval agent for structured knowledge extraction, and a prediction agent that iteratively refines diagnosis predictions. Experimental results demonstrate that KERAP enhances diagnostic reliability efficiently, offering a scalable and interpretable solution for zero-shot medical diagnosis prediction.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moral Responsibility or Obedience: What Do We Want from AI?</title>
<link>https://arxiv.org/abs/2507.02788</link>
<guid>https://arxiv.org/abs/2507.02788</guid>
<content:encoded><![CDATA[
arXiv:2507.02788v1 Announce Type: new 
Abstract: As artificial intelligence systems become increasingly agentic, capable of general reasoning, planning, and value prioritization, current safety practices that treat obedience as a proxy for ethical behavior are becoming inadequate. This paper examines recent safety testing incidents involving large language models (LLMs) that appeared to disobey shutdown commands or engage in ethically ambiguous or illicit behavior. I argue that such behavior should not be interpreted as rogue or misaligned, but as early evidence of emerging ethical reasoning in agentic AI. Drawing on philosophical debates about instrumental rationality, moral responsibility, and goal revision, I contrast dominant risk paradigms with more recent frameworks that acknowledge the possibility of artificial moral agency. I call for a shift in AI safety evaluation: away from rigid obedience and toward frameworks that can assess ethical judgment in systems capable of navigating moral dilemmas. Without such a shift, we risk mischaracterizing AI behavior and undermining both public trust and effective governance.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Establishing Best Practices for Building Rigorous Agentic Benchmarks</title>
<link>https://arxiv.org/abs/2507.02825</link>
<guid>https://arxiv.org/abs/2507.02825</guid>
<content:encoded><![CDATA[
arXiv:2507.02825v1 Announce Type: new 
Abstract: Benchmarks are essential for quantitatively tracking progress in AI. As AI agents become increasingly capable, researchers and practitioners have introduced agentic benchmarks to evaluate agents on complex, real-world tasks. These benchmarks typically measure agent capabilities by evaluating task outcomes via specific reward designs. However, we show that many agentic benchmarks have issues task setup or reward design. For example, SWE-bench Verified uses insufficient test cases, while TAU-bench counts empty responses as successful. Such issues can lead to under- or overestimation agents' performance by up to 100% in relative terms. To make agentic evaluation rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of guidelines that we synthesized from our benchmark-building experience, a survey of best practices, and previously reported issues. When applied to CVE-Bench, a benchmark with a particularly complex evaluation design, ABC reduces the performance overestimation by 33%.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CineMyoPS: Segmenting Myocardial Pathologies from Cine Cardiac MR</title>
<link>https://arxiv.org/abs/2507.02289</link>
<guid>https://arxiv.org/abs/2507.02289</guid>
<content:encoded><![CDATA[
arXiv:2507.02289v1 Announce Type: cross 
Abstract: Myocardial infarction (MI) is a leading cause of death worldwide. Late gadolinium enhancement (LGE) and T2-weighted cardiac magnetic resonance (CMR) imaging can respectively identify scarring and edema areas, both of which are essential for MI risk stratification and prognosis assessment. Although combining complementary information from multi-sequence CMR is useful, acquiring these sequences can be time-consuming and prohibitive, e.g., due to the administration of contrast agents. Cine CMR is a rapid and contrast-free imaging technique that can visualize both motion and structural abnormalities of the myocardium induced by acute MI. Therefore, we present a new end-to-end deep neural network, referred to as CineMyoPS, to segment myocardial pathologies, \ie scars and edema, solely from cine CMR images. Specifically, CineMyoPS extracts both motion and anatomy features associated with MI. Given the interdependence between these features, we design a consistency loss (resembling the co-training strategy) to facilitate their joint learning. Furthermore, we propose a time-series aggregation strategy to integrate MI-related features across the cardiac cycle, thereby enhancing segmentation accuracy for myocardial pathologies. Experimental results on a multi-center dataset demonstrate that CineMyoPS achieves promising performance in myocardial pathology segmentation, motion estimation, and anatomy segmentation.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-agent Auditory Scene Analysis</title>
<link>https://arxiv.org/abs/2507.02755</link>
<guid>https://arxiv.org/abs/2507.02755</guid>
<content:encoded><![CDATA[
arXiv:2507.02755v1 Announce Type: cross 
Abstract: Auditory scene analysis (ASA) aims to retrieve information from the acoustic environment, by carrying out three main tasks: sound source location, separation, and classification. These tasks are traditionally executed with a linear data flow, where the sound sources are first located; then, using their location, each source is separated into its own audio stream; from each of which, information is extracted that is relevant to the application scenario (audio event detection, speaker identification, emotion classification, etc.). However, running these tasks linearly increases the overall response time, while making the last tasks (separation and classification) highly sensitive to errors of the first task (location). A considerable amount of effort and computational complexity has been employed in the state-of-the-art to develop techniques that are the least error-prone possible. However, doing so gives rise to an ASA system that is non-viable in many applications that require a small computational footprint and a low response time, such as bioacoustics, hearing-aid design, search and rescue, human-robot interaction, etc. To this effect, in this work, a multi-agent approach is proposed to carry out ASA where the tasks are run in parallel, with feedback loops between them to compensate for local errors, such as: using the quality of the separation output to correct the location error; and using the classification result to reduce the localization's sensitivity towards interferences. The result is a multi-agent auditory scene analysis (MASA) system that is robust against local errors, without a considerable increase in complexity, and with a low response time. The complete proposed MASA system is provided as a framework that uses open-source tools for sound acquisition and reproduction (JACK) and inter-agent communication (ROS2), allowing users to add their own agents.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kernel Density Bayesian Inverse Reinforcement Learning</title>
<link>https://arxiv.org/abs/2303.06827</link>
<guid>https://arxiv.org/abs/2303.06827</guid>
<content:encoded><![CDATA[
arXiv:2303.06827v4 Announce Type: replace 
Abstract: Inverse reinforcement learning (IRL) methods infer an agent's reward function using demonstrations of expert behavior. A Bayesian IRL approach models a distribution over candidate reward functions, capturing a degree of uncertainty in the inferred reward function. This is critical in some applications, such as those involving clinical data. Typically, Bayesian IRL algorithms require large demonstration datasets, which may not be available in practice. In this work, we incorporate existing domain-specific data to achieve better posterior concentration rates. We study a common setting in clinical and biological applications where we have access to expert demonstrations and known reward functions for a set of training tasks. Our aim is to learn the reward function of a new test task given limited expert demonstrations. Existing Bayesian IRL methods impose restrictions on the form of input data, thus limiting the incorporation of training task data. To better leverage information from training tasks, we introduce kernel density Bayesian inverse reinforcement learning (KD-BIRL). Our approach employs a conditional kernel density estimator, which uses the known reward functions of the training tasks to improve the likelihood estimation across a range of reward functions and demonstration samples. Our empirical results highlight KD-BIRL's faster concentration rate in comparison to baselines, particularly in low test task expert demonstration data regimes. Additionally, we are the first to provide theoretical guarantees of posterior concentration for a Bayesian IRL algorithm. Taken together, this work introduces a principled and theoretically grounded framework that enables Bayesian IRL to be applied across a variety of domains.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Avoiding Catastrophe in Online Learning by Asking for Help</title>
<link>https://arxiv.org/abs/2402.08062</link>
<guid>https://arxiv.org/abs/2402.08062</guid>
<content:encoded><![CDATA[
arXiv:2402.08062v5 Announce Type: replace 
Abstract: Most learning algorithms with formal regret guarantees assume that all mistakes are recoverable and essentially rely on trying all possible behaviors. This approach is problematic when some mistakes are "catastrophic", i.e., irreparable. We propose an online learning problem where the goal is to minimize the chance of catastrophe. Specifically, we assume that the payoff in each round represents the chance of avoiding catastrophe in that round and try to maximize the product of payoffs (the overall chance of avoiding catastrophe) while allowing a limited number of queries to a mentor. We also assume that the agent can transfer knowledge between similar inputs. We first show that in general, any algorithm either queries the mentor at a linear rate or is nearly guaranteed to cause catastrophe. However, in settings where the mentor policy class is learnable in the standard online model, we provide an algorithm whose regret and rate of querying the mentor both approach 0 as the time horizon grows. Although our focus is the product of payoffs, we provide matching bounds for the typical additive regret. Conceptually, if a policy class is learnable in the absence of catastrophic risk, it is learnable in the presence of catastrophic risk if the agent can ask for help.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Integration of Large Language Models in Industrial Test Maintenance Processes</title>
<link>https://arxiv.org/abs/2409.06416</link>
<guid>https://arxiv.org/abs/2409.06416</guid>
<content:encoded><![CDATA[
arXiv:2409.06416v2 Announce Type: replace 
Abstract: Much of the cost and effort required during the software testing process is invested in performing test maintenance - the addition, removal, or modification of test cases to keep the test suite in sync with the system-under-test or to otherwise improve its quality. Tool support could reduce the cost - and improve the quality - of test maintenance by automating aspects of the process or by providing guidance and support to developers.
  In this study, we explore the capabilities and applications of large language models (LLMs) - complex machine learning models adapted to textual analysis - to support test maintenance. We conducted a case study at Ericsson AB where we explore the triggers that indicate the need for test maintenance, the actions that LLMs can take, and the considerations that must be made when deploying LLMs in an industrial setting. We also propose and demonstrate a multi-agent architecture that can predict which tests require maintenance following a change to the source code. Collectively, these contributions advance our theoretical and practical understanding of how LLMs can be deployed to benefit industrial test maintenance processes.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aerial Vision-and-Language Navigation via Semantic-Topo-Metric Representation Guided LLM Reasoning</title>
<link>https://arxiv.org/abs/2410.08500</link>
<guid>https://arxiv.org/abs/2410.08500</guid>
<content:encoded><![CDATA[
arXiv:2410.08500v2 Announce Type: replace 
Abstract: Aerial Vision-and-Language Navigation (VLN) is a novel task enabling Unmanned Aerial Vehicles (UAVs) to navigate in outdoor environments through natural language instructions and visual cues. It remains challenging due to the complex spatial relationships in outdoor aerial scenes. In this paper, we propose an end-to-end zero-shot framework for aerial VLN tasks, where the large language model (LLM) is introduced as our agent for action prediction. Specifically, we develop a novel Semantic-Topo-Metric Representation (STMR) to enhance the spatial reasoning ability of LLMs. This is achieved by extracting and projecting instruction-related semantic masks of landmarks into a top-down map that contains the location information of surrounding landmarks. Further, this map is transformed into a matrix representation with distance metrics as the text prompt to the LLM, for action prediction according to the instruction. Experiments conducted in real and simulation environments have successfully proved the effectiveness and robustness of our method, achieving 15.9% and 12.5% improvements (absolute) in Oracle Success Rate (OSR) on AerialVLN-S dataset.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedAide: Information Fusion and Anatomy of Medical Intents via LLM-based Agent Collaboration</title>
<link>https://arxiv.org/abs/2410.12532</link>
<guid>https://arxiv.org/abs/2410.12532</guid>
<content:encoded><![CDATA[
arXiv:2410.12532v3 Announce Type: replace 
Abstract: In healthcare intelligence, the ability to fuse heterogeneous, multi-intent information from diverse clinical sources is fundamental to building reliable decision-making systems. Large Language Model (LLM)-driven information interaction systems currently showing potential promise in the healthcare domain. Nevertheless, they often suffer from information redundancy and coupling when dealing with complex medical intents, leading to severe hallucinations and performance bottlenecks. To this end, we propose MedAide, an LLM-based medical multi-agent collaboration framework designed to enable intent-aware information fusion and coordinated reasoning across specialized healthcare domains. Specifically, we introduce a regularization-guided module that combines syntactic constraints with retrieval augmented generation to decompose complex queries into structured representations, facilitating fine-grained clinical information fusion and intent resolution. Additionally, a dynamic intent prototype matching module is proposed to utilize dynamic prototype representation with a semantic similarity matching mechanism to achieve adaptive recognition and updating of the agent's intent in multi-round healthcare dialogues. Ultimately, we design a rotation agent collaboration mechanism that introduces dynamic role rotation and decision-level information fusion across specialized medical agents. Extensive experiments are conducted on four medical benchmarks with composite intents. Experimental results from automated metrics and expert doctor evaluations show that MedAide outperforms current LLMs and improves their medical proficiency and strategic reasoning.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design and Evaluation of Privacy-Preserving Protocols for Agent-Facilitated Mobile Money Services in Kenya</title>
<link>https://arxiv.org/abs/2412.18716</link>
<guid>https://arxiv.org/abs/2412.18716</guid>
<content:encoded><![CDATA[
arXiv:2412.18716v2 Announce Type: replace 
Abstract: Mobile Money (MoMo), a technology that allows users to complete financial transactions using a mobile phone without requiring a bank account, is a common method for processing financial transactions in Africa and other developing regions. Users can deposit and withdraw money with the help of human agents. During deposit and withdraw operations, know-your-customer (KYC) processes require agents to access and verify customer information such as name and ID number, which can introduce privacy and security risks. In this work, we design alternative protocols for MoMo deposits/withdrawals that protect users' privacy while enabling KYC checks by redirecting the flow of sensitive information from the agent to the MoMo provider. We evaluate the usability and efficiency of our proposed protocols in a role-play and semi-structured interview study with 32 users and 15 agents in Kenya. We find that users and agents prefer the new protocols, due in part to convenient and efficient verification using biometrics as well as better data privacy and access control. However, our study also surfaced challenges that need to be addressed before these protocols can be deployed.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRESSim-MPM: A Material Point Method Library for Surgical Soft Body Simulation with Cutting and Suturing</title>
<link>https://arxiv.org/abs/2502.18437</link>
<guid>https://arxiv.org/abs/2502.18437</guid>
<content:encoded><![CDATA[
arXiv:2502.18437v3 Announce Type: replace 
Abstract: A number of recent studies have focused on developing surgical simulation platforms to train machine learning (ML) agents or models with synthetic data for surgical assistance. While existing platforms excel at tasks such as rigid body manipulation and soft body deformation, they struggle to simulate more complex soft body behaviors like cutting and suturing. A key challenge lies in modeling soft body fracture and splitting using the finite-element method (FEM), which is the predominant approach in current platforms. Additionally, the two-way suture needle/thread contact inside a soft body is further complicated when using FEM. In this work, we use the material point method (MPM) for such challenging simulations and propose new rigid geometries and soft-rigid contact methods specifically designed for them. We introduce CRESSim-MPM, a GPU-accelerated MPM library that integrates multiple MPM solvers and incorporates surgical geometries for cutting and suturing, serving as a specialized physics engine for surgical applications. It is further integrated into Unity, requiring minimal modifications to existing projects for soft body simulation. We demonstrate the simulator's capabilities in real-time simulation of cutting and suturing on soft tissue and provide an initial performance evaluation of different MPM solvers when simulating varying numbers of particles. The source code is available at https://github.com/yafei-ou/CRESSim-MPM.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BANet: Bilateral Aggregation Network for Mobile Stereo Matching</title>
<link>https://arxiv.org/abs/2503.03259</link>
<guid>https://arxiv.org/abs/2503.03259</guid>
<content:encoded><![CDATA[
arXiv:2503.03259v2 Announce Type: replace 
Abstract: State-of-the-art stereo matching methods typically use costly 3D convolutions to aggregate a full cost volume, but their computational demands make mobile deployment challenging. Directly applying 2D convolutions for cost aggregation often results in edge blurring, detail loss, and mismatches in textureless regions. Some complex operations, like deformable convolutions and iterative warping, can partially alleviate this issue; however, they are not mobile-friendly, limiting their deployment on mobile devices. In this paper, we present a novel bilateral aggregation network (BANet) for mobile stereo matching that produces high-quality results with sharp edges and fine details using only 2D convolutions. Specifically, we first separate the full cost volume into detailed and smooth volumes using a spatial attention map, then perform detailed and smooth aggregations accordingly, ultimately fusing both to obtain the final disparity map. Experimental results demonstrate that our BANet-2D significantly outperforms other mobile-friendly methods, achieving 35.3\% higher accuracy on the KITTI 2015 leaderboard than MobileStereoNet-2D, with faster runtime on mobile devices. Code: \textcolor{magenta}{https://github.com/gangweix/BANet}.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ForceGrip: Reference-Free Curriculum Learning for Realistic Grip Force Control in VR Hand Manipulation</title>
<link>https://arxiv.org/abs/2503.08061</link>
<guid>https://arxiv.org/abs/2503.08061</guid>
<content:encoded><![CDATA[
arXiv:2503.08061v4 Announce Type: replace 
Abstract: Realistic Hand manipulation is a key component of immersive virtual reality (VR), yet existing methods often rely on kinematic approach or motion-capture datasets that omit crucial physical attributes such as contact forces and finger torques. Consequently, these approaches prioritize tight, one-size-fits-all grips rather than reflecting users' intended force levels. We present ForceGrip, a deep learning agent that synthesizes realistic hand manipulation motions, faithfully reflecting the user's grip force intention. Instead of mimicking predefined motion datasets, ForceGrip uses generated training scenarios-randomizing object shapes, wrist movements, and trigger input flows-to challenge the agent with a broad spectrum of physical interactions. To effectively learn from these complex tasks, we employ a three-phase curriculum learning framework comprising Finger Positioning, Intention Adaptation, and Dynamic Stabilization. This progressive strategy ensures stable hand-object contact, adaptive force control based on user inputs, and robust handling under dynamic conditions. Additionally, a proximity reward function enhances natural finger motions and accelerates training convergence. Quantitative and qualitative evaluations reveal ForceGrip's superior force controllability and plausibility compared to state-of-the-art methods. Demo videos are available as supplementary material and the code is provided at https://han-dongheun.github.io/ForceGrip.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Business Process Management: Practitioner Perspectives on Agent Governance in Business Processes</title>
<link>https://arxiv.org/abs/2504.03693</link>
<guid>https://arxiv.org/abs/2504.03693</guid>
<content:encoded><![CDATA[
arXiv:2504.03693v2 Announce Type: replace 
Abstract: With the rise of generative AI, industry interest in software agents is growing. Given the stochastic nature of generative AI-based agents, their effective and safe deployment in organizations requires robust governance, which can be facilitated by agentic business process management. However, given the nascence of this new-generation agent notion, it is not clear what BPM practitioners consider to be an agent, and what benefits, risks and governance challenges they associate with agent deployments. To investigate how organizations can effectively govern AI agents, we conducted a qualitative study involving semi-structured interviews with 22 BPM practitioners from diverse industries. They anticipate that agents will enhance efficiency, improve data quality, ensure better compliance, and boost scalability through automation, while also cautioning against risks such as bias, over-reliance, cybersecurity threats, job displacement, and ambiguous decision-making. To address these challenges, the study presents six key recommendations for the responsible adoption of AI agents: define clear business goals, set legal and ethical guardrails, establish human-agent collaboration, customize agent behavior, manage risks, and ensure safe integration with fallback options. Additionally, the paper outlines actions to align traditional BPM with agentic AI, including balancing human and agent roles, redefining human involvement, adapting process structures, and introducing performance metrics. These insights provide a practical foundation for integrating AI agents into business processes while preserving oversight, flexibility, and trust.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Deep Reinforcement Learning and Motion Planning for Model-Free Navigation in Cluttered Environments</title>
<link>https://arxiv.org/abs/2504.07283</link>
<guid>https://arxiv.org/abs/2504.07283</guid>
<content:encoded><![CDATA[
arXiv:2504.07283v3 Announce Type: replace 
Abstract: Deep Reinforcement Learning (DRL) has emerged as a powerful model-free paradigm for learning optimal policies. However, in navigation tasks with cluttered environments, DRL methods often suffer from insufficient exploration, especially under sparse rewards or complex dynamics with system disturbances. To address this challenge, we bridge general graph-based motion planning with DRL, enabling agents to explore cluttered spaces more effectively and achieve desired navigation performance. Specifically, we design a dense reward function grounded in a graph structure that spans the entire state space. This graph provides rich guidance, steering the agent toward optimal strategies. We validate our approach in challenging environments, demonstrating substantial improvements in exploration efficiency and task success rates.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Threat Modeling for AI: The Case for an Asset-Centric Approach</title>
<link>https://arxiv.org/abs/2505.06315</link>
<guid>https://arxiv.org/abs/2505.06315</guid>
<content:encoded><![CDATA[
arXiv:2505.06315v2 Announce Type: replace 
Abstract: Recent advances in AI are transforming AI's ubiquitous presence in our world from that of standalone AI-applications into deeply integrated AI-agents. These changes have been driven by agents' increasing capability to autonomously make decisions and initiate actions, using existing applications; whether those applications are AI-based or not. This evolution enables unprecedented levels of AI integration, with agents now able to take actions on behalf of systems and users -- including, in some cases, the powerful ability for the AI to write and execute scripts as it deems necessary. With AI systems now able to autonomously execute code, interact with external systems, and operate without human oversight, traditional security approaches fall short.
  This paper introduces an asset-centric methodology for threat modeling AI systems that addresses the unique security challenges posed by integrated AI agents. Unlike existing top-down frameworks that analyze individual attacks within specific product contexts, our bottom-up approach enables defenders to systematically identify how vulnerabilities -- both conventional and AI-specific -- impact critical AI assets across distributed infrastructures used to develop and deploy these agents. This methodology allows security teams to: (1) perform comprehensive analysis that communicates effectively across technical domains, (2) quantify security assumptions about third-party AI components without requiring visibility into their implementation, and (3) holistically identify AI-based vulnerabilities relevant to their specific product context. This approach is particularly relevant for securing agentic systems with complex autonomous capabilities. By focusing on assets rather than attacks, our approach scales with the rapidly evolving threat landscape while accommodating increasingly complex and distributed AI development pipelines.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI Process Observability: Discovering Behavioral Variability</title>
<link>https://arxiv.org/abs/2505.20127</link>
<guid>https://arxiv.org/abs/2505.20127</guid>
<content:encoded><![CDATA[
arXiv:2505.20127v2 Announce Type: replace 
Abstract: AI agents that leverage Large Language Models (LLMs) are increasingly becoming core building blocks of modern software systems. A wide range of frameworks is now available to support the specification of such applications. These frameworks enable the definition of agent setups using natural language prompting, which specifies the roles, goals, and tools assigned to the various agents involved. Within such setups, agent behavior is non-deterministic for any given input, highlighting the critical need for robust debugging and observability tools. In this work, we explore the use of process and causal discovery applied to agent execution trajectories as a means of enhancing developer observability. This approach aids in monitoring and understanding the emergent variability in agent behavior. Additionally, we complement this with LLM-based static analysis techniques to distinguish between intended and unintended behavioral variability. We argue that such instrumentation is essential for giving developers greater control over evolving specifications and for identifying aspects of functionality that may require more precise and explicit definitions.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation</title>
<link>https://arxiv.org/abs/2505.21880</link>
<guid>https://arxiv.org/abs/2505.21880</guid>
<content:encoded><![CDATA[
arXiv:2505.21880v2 Announce Type: replace 
Abstract: This study presents an innovative approach to urban mobility simulation by integrating a Large Language Model (LLM) with Agent-Based Modeling (ABM). Unlike traditional rule-based ABM, the proposed framework leverages LLM to enhance agent diversity and realism by generating synthetic population profiles, allocating routine and occasional locations, and simulating personalized routes. Using real-world data, the simulation models individual behaviors and large-scale mobility patterns in Taipei City. Key insights, such as route heat maps and mode-specific indicators, provide urban planners with actionable information for policy-making. Future work focuses on establishing robust validation frameworks to ensure accuracy and reliability in urban planning applications.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLM-based Quantum Code Generation with Multi-Agent Optimization and Quantum Error Correction</title>
<link>https://arxiv.org/abs/2504.14557</link>
<guid>https://arxiv.org/abs/2504.14557</guid>
<content:encoded><![CDATA[
arXiv:2504.14557v2 Announce Type: replace-cross 
Abstract: Multi-agent frameworks with Large Language Models (LLMs) have become promising tools for generating general-purpose programming languages using test-driven development, allowing developers to create more accurate and robust code. However, their potential has not been fully unleashed for domain-specific programming languages, where specific domain exhibits unique optimization opportunities for customized improvement. In this paper, we take the first step in exploring multi-agent code generation for quantum programs. By identifying the unique optimizations in quantum designs such as quantum error correction, we introduce a novel multi-agent framework tailored to generating accurate, fault-tolerant quantum code. Each agent in the framework focuses on distinct optimizations, iteratively refining the code using a semantic analyzer with multi-pass inference, alongside an error correction code decoder. We also examine the effectiveness of inference-time techniques, like Chain-of-Thought (CoT) and Retrieval-Augmented Generation (RAG) in the context of quantum programming, uncovering observations that are different from general-purpose code generation. To evaluate our approach, we develop a test suite to measure the impact each optimization has on the accuracy of the generated code. Our findings indicate that techniques such as structured CoT significantly improve the generation of quantum algorithms by up to 50%. In contrast, we have also found that certain techniques such as RAG show limited improvement, yielding an accuracy increase of only 4%. Moreover, we showcase examples of AI-assisted quantum error prediction and correction, demonstrating the effectiveness of our multi-agent framework in reducing the quantum errors of generated quantum programs.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered</title>
<link>https://arxiv.org/abs/2507.01019</link>
<guid>https://arxiv.org/abs/2507.01019</guid>
<content:encoded><![CDATA[
arXiv:2507.01019v1 Announce Type: new 
Abstract: Multi-agent systems, which consist of multiple AI models interacting within a shared environment, are increasingly used for persona-based interactions. However, if not carefully designed, these systems can reinforce implicit biases in large language models (LLMs), raising concerns about fairness and equitable representation. We present MALIBU, a novel benchmark developed to assess the degree to which LLM-based multi-agent systems implicitly reinforce social biases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems through scenario-based assessments. AI models complete tasks within predefined contexts, and their responses undergo evaluation by an LLM-based multi-agent judging system in two phases. In the first phase, judges score responses labeled with specific demographic personas (e.g., gender, race, religion) across four metrics. In the second phase, judges compare paired responses assigned to different personas, scoring them and selecting the superior response. Our study quantifies biases in LLM-generated outputs, revealing that bias mitigation may favor marginalized personas over true neutrality, emphasizing the need for nuanced detection, balanced fairness strategies, and transparent evaluation benchmarks in multi-agent systems.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On-Policy Optimization of ANFIS Policies Using Proximal Policy Optimization</title>
<link>https://arxiv.org/abs/2507.01039</link>
<guid>https://arxiv.org/abs/2507.01039</guid>
<content:encoded><![CDATA[
arXiv:2507.01039v1 Announce Type: new 
Abstract: We propose a reinforcement learning (RL) approach for training neuro-fuzzy controllers using Proximal Policy Optimization (PPO). Building on prior work that applied Deep Q-Learning to Adaptive Neuro-Fuzzy Inference Systems (ANFIS), our method replaces the off-policy value-based framework with a stable on-policy actor-critic loop. We evaluate this approach in the CartPole-v1 environment using multiple random seeds and compare its learning performance against ANFIS-Deep Q-Network (DQN) baselines. It was found that PPO-trained fuzzy agents achieved a mean return of 500 +/- 0 on CartPole-v1 after 20000 updates, showcasing less variance than prior DQN-based methods during training and overall faster convergence. These findings suggest that PPO offers a promising pathway for training explainable neuro-fuzzy controllers in reinforcement learning tasks.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Vehicles Should be Connected with Natural Language</title>
<link>https://arxiv.org/abs/2507.01059</link>
<guid>https://arxiv.org/abs/2507.01059</guid>
<content:encoded><![CDATA[
arXiv:2507.01059v1 Announce Type: new 
Abstract: Multi-agent collaborative driving promises improvements in traffic safety and efficiency through collective perception and decision making. However, existing communication media -- including raw sensor data, neural network features, and perception results -- suffer limitations in bandwidth efficiency, information completeness, and agent interoperability. Moreover, traditional approaches have largely ignored decision-level fusion, neglecting critical dimensions of collaborative driving. In this paper we argue that addressing these challenges requires a transition from purely perception-oriented data exchanges to explicit intent and reasoning communication using natural language. Natural language balances semantic density and communication bandwidth, adapts flexibly to real-time conditions, and bridges heterogeneous agent platforms. By enabling the direct communication of intentions, rationales, and decisions, it transforms collaborative driving from reactive perception-data sharing into proactive coordination, advancing safety, efficiency, and transparency in intelligent transportation systems.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Conversational Product Recommendation via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.01060</link>
<guid>https://arxiv.org/abs/2507.01060</guid>
<content:encoded><![CDATA[
arXiv:2507.01060v1 Announce Type: new 
Abstract: We propose a reinforcement learning-based approach to optimize conversational strategies for product recommendation across diverse industries. As organizations increasingly adopt intelligent agents to support sales and service operations, the effectiveness of a conversation hinges not only on what is recommended but how and when recommendations are delivered. We explore a methodology where agentic systems learn optimal dialogue policies through feedback-driven reinforcement learning. By mining aggregate behavioral patterns and conversion outcomes, our approach enables agents to refine talk tracks that drive higher engagement and product uptake, while adhering to contextual and regulatory constraints. We outline the conceptual framework, highlight key innovations, and discuss the implications for scalable, personalized recommendation in enterprise environments.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Epitome: Pioneering an Experimental Platform for AI-Social Science Integration</title>
<link>https://arxiv.org/abs/2507.01061</link>
<guid>https://arxiv.org/abs/2507.01061</guid>
<content:encoded><![CDATA[
arXiv:2507.01061v1 Announce Type: new 
Abstract: The integration of Large Language Models (LLMs) into social science experiments represents a transformative approach to understanding human-AI interactions and their societal impacts. We introduce Epitome, the world's first open experimental platform dedicated to the deep integration of artificial intelligence and social science. Rooted in theoretical foundations from management, communication studies, sociology, psychology, and ethics, Epitome focuses on the interactive impacts of AI on individuals, organizations, and society during its real-world deployment. It constructs a theoretical support system through cross-disciplinary experiments. The platform offers a one-stop comprehensive experimental solution spanning "foundation models-complex application development-user feedback" through seven core modules, while embedding the classical "control-comparison-comparative causal logic" of social science experiments into multilevel human-computer interaction environments, including dialogues, group chats, and multi-agent virtual scenarios. With its canvas-style, user-friendly interface, Epitome enables researchers to easily design and run complex experimental scenarios, facilitating systematic investigations into the social impacts of AI and exploration of integrated solutions.To demonstrate its capabilities, we replicated three seminal social science experiments involving LLMs, showcasing Epitome's potential to streamline complex experimental designs and produce robust results, suitable for publishing in the top selective journals. Our findings highlight the platform's utility in enhancing the efficiency and quality of human-AI interactions, providing valuable insights into the societal implications of AI technologies. Epitome thus offers a powerful tool for advancing interdisciplinary research at the intersection of AI and social science, with potential applications in policy-making, ...
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI in Product Management: A Co-Evolutionary Model</title>
<link>https://arxiv.org/abs/2507.01069</link>
<guid>https://arxiv.org/abs/2507.01069</guid>
<content:encoded><![CDATA[
arXiv:2507.01069v1 Announce Type: new 
Abstract: This study explores agentic AI's transformative role in product management, proposing a conceptual co-evolutionary framework to guide its integration across the product lifecycle. Agentic AI, characterized by autonomy, goal-driven behavior, and multi-agent collaboration, redefines product managers (PMs) as orchestrators of socio-technical ecosystems. Using systems theory, co-evolutionary theory, and human-AI interaction theory, the framework maps agentic AI capabilities in discovery, scoping, business case development, development, testing, and launch. An integrative review of 70+ sources, including case studies from leading tech firms, highlights PMs' evolving roles in AI orchestration, supervision, and strategic alignment. Findings emphasize mutual adaptation between PMs and AI, requiring skills in AI literacy, governance, and systems thinking. Addressing gaps in traditional frameworks, this study provides a foundation for future research and practical implementation to ensure responsible, effective agentic AI integration in software organizations.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SonoGym: High Performance Simulation for Challenging Surgical Tasks with Robotic Ultrasound</title>
<link>https://arxiv.org/abs/2507.01152</link>
<guid>https://arxiv.org/abs/2507.01152</guid>
<content:encoded><![CDATA[
arXiv:2507.01152v1 Announce Type: new 
Abstract: Ultrasound (US) is a widely used medical imaging modality due to its real-time capabilities, non-invasive nature, and cost-effectiveness. Robotic ultrasound can further enhance its utility by reducing operator dependence and improving access to complex anatomical regions. For this, while deep reinforcement learning (DRL) and imitation learning (IL) have shown potential for autonomous navigation, their use in complex surgical tasks such as anatomy reconstruction and surgical guidance remains limited -- largely due to the lack of realistic and efficient simulation environments tailored to these tasks. We introduce SonoGym, a scalable simulation platform for complex robotic ultrasound tasks that enables parallel simulation across tens to hundreds of environments. Our framework supports realistic and real-time simulation of US data from CT-derived 3D models of the anatomy through both a physics-based and a generative modeling approach. Sonogym enables the training of DRL and recent IL agents (vision transformers and diffusion policies) for relevant tasks in robotic orthopedic surgery by integrating common robotic platforms and orthopedic end effectors. We further incorporate submodular DRL -- a recent method that handles history-dependent rewards -- for anatomy reconstruction and safe reinforcement learning for surgery. Our results demonstrate successful policy learning across a range of scenarios, while also highlighting the limitations of current methods in clinically relevant environments. We believe our simulation can facilitate research in robot learning approaches for such challenging robotic surgery applications. Dataset, codes, and videos are publicly available at https://sonogym.github.io/.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Illusion of Thinking</title>
<link>https://arxiv.org/abs/2507.01231</link>
<guid>https://arxiv.org/abs/2507.01231</guid>
<content:encoded><![CDATA[
arXiv:2507.01231v1 Announce Type: new 
Abstract: Earlier this year, Apple ignited controversy by publishing "The Illusion of Thinking," prompting heated debate within the AI community. Critics seized upon the findings as conclusive evidence that Large Reasoning Models (LRMs) lack genuine reasoning capabilities, branding them as mere stochastic parrots. Meanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning the experimental setup as flawed and the conclusions overstated. We clarify this debate by replicating and refining two of the original study's most contentious benchmarks: Towers of Hanoi and River Crossing. By introducing incremental stepwise prompting and agentic collaborative dialogue, we show that previously reported failures solving the Towers of Hanoi were not purely result of output constraints, but also partly a result of cognition limitations: LRMs still stumble when complexity rises moderately (around 8 disks). Moreover, the River Crossing results initially heralded as catastrophic failures turn out to hinge upon testing unsolvable configurations. Once we limit tests strictly to solvable problems-LRMs effortlessly solve large instances involving over 100 agent pairs. Our findings ultimately defy simplistic narratives: today's LRMs are stochastic, RL-tuned searchers in a discrete state space we barely understand. Real progress in symbolic, long-horizon reasoning demands mapping that terrain through fine-grained ablations like those introduced here.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for AI-Generated Video Evaluation</title>
<link>https://arxiv.org/abs/2507.01255</link>
<guid>https://arxiv.org/abs/2507.01255</guid>
<content:encoded><![CDATA[
arXiv:2507.01255v1 Announce Type: new 
Abstract: The rapid advancement of AI-generated video models has created a pressing need for robust and interpretable evaluation frameworks. Existing metrics are limited to producing numerical scores without explanatory comments, resulting in low interpretability and human evaluation alignment. To address those challenges, we introduce AIGVE-MACS, a unified model for AI-Generated Video Evaluation(AIGVE), which can provide not only numerical scores but also multi-aspect language comment feedback in evaluating these generated videos. Central to our approach is AIGVE-BENCH 2, a large-scale benchmark comprising 2,500 AI-generated videos and 22,500 human-annotated detailed comments and numerical scores across nine critical evaluation aspects. Leveraging AIGVE-BENCH 2, AIGVE-MACS incorporates recent Vision-Language Models with a novel token-wise weighted loss and a dynamic frame sampling strategy to better align with human evaluators. Comprehensive experiments across supervised and zero-shot benchmarks demonstrate that AIGVE-MACS achieves state-of-the-art performance in both scoring correlation and comment quality, significantly outperforming prior baselines including GPT-4o and VideoScore. In addition, we further showcase a multi-agent refinement framework where feedback from AIGVE-MACS drives iterative improvements in video generation, leading to 53.5% quality enhancement. This work establishes a new paradigm for comprehensive, human-aligned evaluation of AI-generated videos. We release the AIGVE-BENCH 2 and AIGVE-MACS at https://huggingface.co/xiaoliux/AIGVE-MACS.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant</title>
<link>https://arxiv.org/abs/2507.01259</link>
<guid>https://arxiv.org/abs/2507.01259</guid>
<content:encoded><![CDATA[
arXiv:2507.01259v1 Announce Type: new 
Abstract: In this paper we discuss the capability of large language models to base their answer and provide proper references when dealing with legal matters of non-english and non-chinese speaking country. We discuss the history of legal information retrieval, the difference between case law and statute law, its impact on the legal tasks and analyze the latest research in this field. Basing on that background we introduce gAIus, the architecture of the cognitive LLM-based agent, whose responses are based on the knowledge retrieved from certain legal act, which is Polish Civil Code. We propose a retrieval mechanism which is more explainable, human-friendly and achieves better results than embedding-based approaches. To evaluate our method we create special dataset based on single-choice questions from entrance exams for law apprenticeships conducted in Poland. The proposed architecture critically leveraged the abilities of used large language models, improving the gpt-3.5-turbo-0125 by 419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%. At the end of our paper we show the possible future path of research and potential applications of our findings.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks</title>
<link>https://arxiv.org/abs/2507.01297</link>
<guid>https://arxiv.org/abs/2507.01297</guid>
<content:encoded><![CDATA[
arXiv:2507.01297v1 Announce Type: new 
Abstract: Retrieval-augmented Generation (RAG) has primarily been studied in limited settings, such as factoid question answering; more challenging, reasoning-intensive benchmarks have seen limited success from minimal RAG. In this work, we challenge this prevailing view on established, reasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We identify a key missing component in prior work: a usable, web-scale datastore aligned with the breadth of pretraining data. To this end, we introduce CompactDS: a diverse, high-quality, web-scale datastore that achieves high retrieval accuracy and subsecond latency on a single-node. The key insights are (1) most web content can be filtered out without sacrificing coverage, and a compact, high-quality subset is sufficient; and (2) combining in-memory approximate nearest neighbor (ANN) retrieval and on-disk exact search balances speed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves consistent accuracy improvements across all benchmarks and model sizes (8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA, and 19% on MATH. No single data source suffices alone, highlighting the importance of diversity of sources (web crawls, curated math, academic papers, textbooks). Finally, we show that our carefully designed in-house datastore matches or outperforms web search engines such as Google Search, as well as recently proposed, complex agent-based RAG systems--all while maintaining simplicity, reproducibility, and self-containment. We release CompactDS and our retrieval pipeline, supporting future research exploring retrieval-based AI systems.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Dispersion Under Asynchrony</title>
<link>https://arxiv.org/abs/2507.01298</link>
<guid>https://arxiv.org/abs/2507.01298</guid>
<content:encoded><![CDATA[
arXiv:2507.01298v1 Announce Type: new 
Abstract: We study the dispersion problem in anonymous port-labeled graphs: $k \leq n$ mobile agents, each with a unique ID and initially located arbitrarily on the nodes of an $n$-node graph with maximum degree $\Delta$, must autonomously relocate so that no node hosts more than one agent. Dispersion serves as a fundamental task in distributed computing of mobile agents, and its complexity stems from key challenges in local coordination under anonymity and limited memory.
  The goal is to minimize both the time to achieve dispersion and the memory required per agent. It is known that any algorithm requires $\Omega(k)$ time in the worst case, and $\Omega(\log k)$ bits of memory per agent. A recent result [SPAA'25] gives an optimal $O(k)$-time algorithm in the synchronous setting and an $O(k \log k)$-time algorithm in the asynchronous setting, both using $O(\log(k+\Delta))$ bits.
  In this paper, we close the complexity gap in the asynchronous setting by presenting the first dispersion algorithm that runs in optimal $O(k)$ time using $O(\log(k+\Delta))$ bits of memory per agent. Our solution is based on a novel technique we develop in this paper that constructs a port-one tree in anonymous graphs, which may be of independent interest.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LANet: A Lane Boundaries-Aware Approach For Robust Trajectory Prediction</title>
<link>https://arxiv.org/abs/2507.01308</link>
<guid>https://arxiv.org/abs/2507.01308</guid>
<content:encoded><![CDATA[
arXiv:2507.01308v1 Announce Type: new 
Abstract: Accurate motion forecasting is critical for safe and efficient autonomous driving, enabling vehicles to predict future trajectories and make informed decisions in complex traffic scenarios. Most of the current designs of motion prediction models are based on the major representation of lane centerlines, which limits their capability to capture critical road environments and traffic rules and constraints. In this work, we propose an enhanced motion forecasting model informed by multiple vector map elements, including lane boundaries and road edges, that facilitates a richer and more complete representation of driving environments. An effective feature fusion strategy is developed to merge information in different vector map components, where the model learns holistic information on road structures and their interactions with agents. Since encoding more information about the road environment increases memory usage and is computationally expensive, we developed an effective pruning mechanism that filters the most relevant map connections to the target agent, ensuring computational efficiency while maintaining essential spatial and semantic relationships for accurate trajectory prediction. Overcoming the limitations of lane centerline-based models, our method provides a more informative and efficient representation of the driving environment and advances the state of the art for autonomous vehicle motion forecasting. We verify our approach with extensive experiments on the Argoverse 2 motion forecasting dataset, where our method maintains competitiveness on AV2 while achieving improved performance.
  Index Terms-Autonomous driving, trajectory prediction, vector map elements, road topology, connection pruning, Argoverse 2.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Aware Code Wiring Recommendation with LLM-based Agent</title>
<link>https://arxiv.org/abs/2507.01315</link>
<guid>https://arxiv.org/abs/2507.01315</guid>
<content:encoded><![CDATA[
arXiv:2507.01315v1 Announce Type: new 
Abstract: Copy-paste-modify is a widespread and pragmatic practice in software development, where developers adapt reused code snippets, sourced from platforms such as Stack Overflow, GitHub, or LLM outputs, into their local codebase. A critical yet underexplored aspect of this adaptation is code wiring, which involves substituting unresolved variables in the pasted code with suitable ones from the surrounding context. Existing solutions either rely on heuristic rules or historical templates, often failing to effectively utilize contextual information, despite studies showing that over half of adaptation cases are context-dependent. In this paper, we introduce WIRL, an LLM-based agent for code wiring framed as a Retrieval-Augmented Generation (RAG) infilling task. WIRL combines an LLM, a customized toolkit, and an orchestration module to identify unresolved variables, retrieve context, and perform context-aware substitutions. To balance efficiency and autonomy, the agent adopts a mixed strategy: deterministic rule-based steps for common patterns, and a state-machine-guided decision process for intelligent exploration. We evaluate WIRL on a carefully curated, high-quality dataset consisting of real-world code adaptation scenarios. Our approach achieves an exact match precision of 91.7% and a recall of 90.0%, outperforming advanced LLMs by 22.6 and 13.7 percentage points in precision and recall, respectively, and surpassing IntelliJ IDEA by 54.3 and 49.9 percentage points. These results underscore its practical utility, particularly in contexts with complex variable dependencies or multiple unresolved variables. We believe WIRL paves the way for more intelligent and context-aware developer assistance in modern IDEs.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing</title>
<link>https://arxiv.org/abs/2507.01376</link>
<guid>https://arxiv.org/abs/2507.01376</guid>
<content:encoded><![CDATA[
arXiv:2507.01376v1 Announce Type: new 
Abstract: AI agents are autonomous systems designed to perceive, reason, and act within dynamic environments. With the rapid advancements in generative AI (GenAI), large language models (LLMs) and multimodal large language models (MLLMs) have significantly improved AI agents' capabilities in semantic comprehension, complex reasoning, and autonomous decision-making. At the same time, the rise of Agentic AI highlights adaptability and goal-directed autonomy in dynamic and complex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents (MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in information processing, environmental perception, and autonomous decision-making, opening new avenues for smart manufacturing. However, the definitions, capability boundaries, and practical applications of these emerging AI paradigms in smart manufacturing remain unclear. To address this gap, this study systematically reviews the evolution of AI and AI agent technologies, examines the core concepts and technological advancements of LLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential applications in and integration into manufacturing, along with the potential challenges they may face.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms</title>
<link>https://arxiv.org/abs/2507.01378</link>
<guid>https://arxiv.org/abs/2507.01378</guid>
<content:encoded><![CDATA[
arXiv:2507.01378v1 Announce Type: new 
Abstract: Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as a critical research focus, and it typically requires the swarm to navigate effectively while avoiding obstacles and achieving continuous coverage over multiple mission targets. Although traditional Multi-Agent Reinforcement Learning (MARL) approaches offer dynamic adaptability, they are hindered by the semantic gap in numerical communication and the rigidity of homogeneous role structures, resulting in poor generalization and limited task scalability. Recent advances in Large Language Model (LLM)-based control frameworks demonstrate strong semantic reasoning capabilities by leveraging extensive prior knowledge. However, due to the lack of online learning and over-reliance on static priors, these works often struggle with effective exploration, leading to reduced individual potential and overall system performance. To address these limitations, we propose a Role-Adaptive LLM-Driven Yoked navigation algorithm RALLY. Specifically, we first develop an LLM-driven semantic decision framework that uses structured natural language for efficient semantic communication and collaborative reasoning. Afterward, we introduce a dynamic role-heterogeneity mechanism for adaptive role switching and personalized decision-making. Furthermore, we propose a Role-value Mixing Network (RMIX)-based assignment strategy that integrates LLM offline priors with MARL online policies to enable semi-offline training of role selection strategies. Experiments in the Multi-Agent Particle Environment (MPE) environment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY outperforms conventional approaches in terms of task coverage, convergence speed, and generalization, highlighting its strong potential for collaborative navigation in agentic multi-UAV systems.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLM Agent Collusion in Double Auctions</title>
<link>https://arxiv.org/abs/2507.01413</link>
<guid>https://arxiv.org/abs/2507.01413</guid>
<content:encoded><![CDATA[
arXiv:2507.01413v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated impressive capabilities as autonomous agents with rapidly expanding applications in various domains. As these agents increasingly engage in socioeconomic interactions, identifying their potential for undesirable behavior becomes essential. In this work, we examine scenarios where they can choose to collude, defined as secretive cooperation that harms another party. To systematically study this, we investigate the behavior of LLM agents acting as sellers in simulated continuous double auction markets. Through a series of controlled experiments, we analyze how parameters such as the ability to communicate, choice of model, and presence of environmental pressures affect the stability and emergence of seller collusion. We find that direct seller communication increases collusive tendencies, the propensity to collude varies across models, and environmental pressures, such as oversight and urgency from authority figures, influence collusive behavior. Our findings highlight important economic and ethical considerations for the deployment of LLM-based market agents.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using multi-agent architecture to mitigate the risk of LLM hallucinations</title>
<link>https://arxiv.org/abs/2507.01446</link>
<guid>https://arxiv.org/abs/2507.01446</guid>
<content:encoded><![CDATA[
arXiv:2507.01446v1 Announce Type: new 
Abstract: Improving customer service quality and response time are critical factors for maintaining customer loyalty and increasing a company's market share. While adopting emerging technologies such as Large Language Models (LLMs) is becoming a necessity to achieve these goals, the risk of hallucination remains a major challenge. In this paper, we present a multi-agent system to handle customer requests sent via SMS. This system integrates LLM based agents with fuzzy logic to mitigate hallucination risks.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.01489</link>
<guid>https://arxiv.org/abs/2507.01489</guid>
<content:encoded><![CDATA[
arXiv:2507.01489v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have emerged as one of the most significant technological advancements in artificial intelligence in recent years. Their ability to understand, generate, and reason with natural language has transformed how we interact with AI systems. With the development of LLM-based agents and reinforcement-learning-based reasoning models, the study of applying reinforcement learning in agent frameworks has become a new research focus. However, all previous studies face the challenge of deciding the tool calling process and the reasoning process simultaneously, and the chain of reasoning was solely relied on the unprocessed raw result with redundant information and symbols unrelated to the task from the tool, which impose a heavy burden on the model's capability to reason. Therefore, in our research, we proposed a hierarchical framework Agent-as-tool that detach the tool calling process and the reasoning process, which enables the model to focus on the verbally reasoning process while the tool calling process is handled by another agent. Our work had achieved comparable results with only a slight reinforcement fine-tuning on 180 samples, and had achieved exceptionally well performance in Bamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding Search-R1 by 4.8% in exact match and 3.2% in cover exact match.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chargax: A JAX Accelerated EV Charging Simulator</title>
<link>https://arxiv.org/abs/2507.01522</link>
<guid>https://arxiv.org/abs/2507.01522</guid>
<content:encoded><![CDATA[
arXiv:2507.01522v1 Announce Type: new 
Abstract: Deep Reinforcement Learning can play a key role in addressing sustainable energy challenges. For instance, many grid systems are heavily congested, highlighting the urgent need to enhance operational efficiency. However, reinforcement learning approaches have traditionally been slow due to the high sample complexity and expensive simulation requirements. While recent works have effectively used GPUs to accelerate data generation by converting environments to JAX, these works have largely focussed on classical toy problems. This paper introduces Chargax, a JAX-based environment for realistic simulation of electric vehicle charging stations designed for accelerated training of RL agents. We validate our environment in a variety of scenarios based on real data, comparing reinforcement learning agents against baselines. Chargax delivers substantial computational performance improvements of over 100x-1000x over existing environments. Additionally, Chargax' modular architecture enables the representation of diverse real-world charging station configurations.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-Varying Coverage Control: A Distributed Tracker-Planner MPC Framework</title>
<link>https://arxiv.org/abs/2507.01567</link>
<guid>https://arxiv.org/abs/2507.01567</guid>
<content:encoded><![CDATA[
arXiv:2507.01567v1 Announce Type: new 
Abstract: Time-varying coverage control addresses the challenge of coordinating multiple agents covering an environment where regions of interest change over time. This problem has broad applications, including the deployment of autonomous taxis and coordination in search and rescue operations. The achievement of effective coverage is complicated by the presence of time-varying density functions, nonlinear agent dynamics, and stringent system and safety constraints. In this paper, we present a distributed multi-agent control framework for time-varying coverage under nonlinear constrained dynamics. Our approach integrates a reference trajectory planner and a tracking model predictive control (MPC) scheme, which operate at different frequencies within a multi-rate framework. For periodic density functions, we demonstrate closed-loop convergence to an optimal configuration of trajectories and provide formal guarantees regarding constraint satisfaction, collision avoidance, and recursive feasibility. Additionally, we propose an efficient algorithm capable of handling nonperiodic density functions, making the approach suitable for practical applications. Finally, we validate our method through hardware experiments using a fleet of four miniature race cars.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Aided ISAC in Low-Altitude Economy Networks via De-Diffused Visual Priors</title>
<link>https://arxiv.org/abs/2507.01574</link>
<guid>https://arxiv.org/abs/2507.01574</guid>
<content:encoded><![CDATA[
arXiv:2507.01574v1 Announce Type: new 
Abstract: Emerging low-altitude economy networks (LAENets) require agile and privacy-preserving resource control under dynamic agent mobility and limited infrastructure support. To meet these challenges, we propose a vision-aided integrated sensing and communication (ISAC) framework for UAV-assisted access systems, where onboard masked De-Diffusion models extract compact semantic tokens, including agent type, activity class, and heading orientation, while explicitly suppressing sensitive visual content. These tokens are fused with mmWave radar measurements to construct a semantic risk heatmap reflecting motion density, occlusion, and scene complexity, which guides access technology selection and resource scheduling. We formulate a multi-objective optimization problem to jointly maximize weighted energy and perception efficiency via radio access technology (RAT) assignment, power control, and beamforming, subject to agent-specific QoS constraints. To solve this, we develop De-Diffusion-driven vision-aided risk-aware resource optimization algorithm DeDiff-VARARO, a novel two-stage cross-modal control algorithm: the first stage reconstructs visual scenes from tokens via De-Diffusion model for semantic parsing, while the second stage employs a deep deterministic policy gradient (DDPG)-based policy to adapt RAT selection, power control, and beam assignment based on fused radar-visual states. Simulation results show that DeDiff-VARARO consistently outperforms baselines in reward convergence, link robustness, and semantic fidelity, achieving within $4\%$ of the performance of a raw-image upper bound while preserving user privacy and scalability in dense environments.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation</title>
<link>https://arxiv.org/abs/2507.01594</link>
<guid>https://arxiv.org/abs/2507.01594</guid>
<content:encoded><![CDATA[
arXiv:2507.01594v1 Announce Type: new 
Abstract: Task-oriented dialogue (ToD) systems are designed to help users achieve specific goals through natural language interaction. While recent advances in large language models (LLMs) have significantly improved linguistic fluency and contextual understanding, building effective and emotionally intelligent ToD systems remains a complex challenge. Effective ToD systems must optimise for task success, emotional understanding and responsiveness, and precise information conveyance, all within inherently noisy and ambiguous conversational environments. In this work, we investigate architectural, representational, optimisational as well as emotional considerations of ToD systems. We set up systems covering these design considerations with a challenging evaluation environment composed of a natural-language user simulator coupled with an imperfect natural language understanding module. We propose \textbf{LUSTER}, an \textbf{L}LM-based \textbf{U}nified \textbf{S}ystem for \textbf{T}ask-oriented dialogue with \textbf{E}nd-to-end \textbf{R}einforcement learning with both short-term (user sentiment) and long-term (task success) rewards. Our findings demonstrate that combining LLM capability with structured reward modelling leads to more resilient and emotionally responsive ToD systems, offering a practical path forward for next-generation conversational agents.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems</title>
<link>https://arxiv.org/abs/2507.01599</link>
<guid>https://arxiv.org/abs/2507.01599</guid>
<content:encoded><![CDATA[
arXiv:2507.01599v1 Announce Type: new 
Abstract: Traditional Data+AI systems utilize data-driven techniques to optimize performance, but they rely heavily on human experts to orchestrate system pipelines, enabling them to adapt to changes in data, queries, tasks, and environments. For instance, while there are numerous data science tools available, developing a pipeline planning system to coordinate these tools remains challenging. This difficulty arises because existing Data+AI systems have limited capabilities in semantic understanding, reasoning, and planning. Fortunately, we have witnessed the success of large language models (LLMs) in enhancing semantic understanding, reasoning, and planning abilities. It is crucial to incorporate LLM techniques to revolutionize data systems for orchestrating Data+AI applications effectively.
  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive architecture designed to orchestrate Data+AI ecosystems, which focuses on tackling data-related tasks by integrating knowledge comprehension, reasoning, and planning capabilities. We delve into the challenges involved in designing data agents, such as understanding data/queries/environments/tools, orchestrating pipelines/workflows, optimizing and executing pipelines, and fostering pipeline self-reflection. Furthermore, we present examples of data agent systems, including a data science agent, data analytics agents (such as unstructured data analytics agent, semantic structured data analytics agent, data lake analytics agent, and multi-modal data analytics agent), and a database administrator (DBA) agent. We also outline several open challenges associated with designing data agent systems.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What does really matter in image goal navigation?</title>
<link>https://arxiv.org/abs/2507.01667</link>
<guid>https://arxiv.org/abs/2507.01667</guid>
<content:encoded><![CDATA[
arXiv:2507.01667v1 Announce Type: new 
Abstract: Image goal navigation requires two different skills: firstly, core navigation skills, including the detection of free space and obstacles, and taking decisions based on an internal representation; and secondly, computing directional information by comparing visual observations to the goal image. Current state-of-the-art methods either rely on dedicated image-matching, or pre-training of computer vision modules on relative pose estimation. In this paper, we study whether this task can be efficiently solved with end-to-end training of full agents with RL, as has been claimed by recent work. A positive answer would have impact beyond Embodied AI and allow training of relative pose estimation from reward for navigation alone. In a large study we investigate the effect of architectural choices like late fusion, channel stacking, space-to-depth projections and cross-attention, and their role in the emergence of relative pose estimators from navigation training. We show that the success of recent methods is influenced up to a certain extent by simulator settings, leading to shortcuts in simulation. However, we also show that these capabilities can be transferred to more realistic setting, up to some extend. We also find evidence for correlations between navigation performance and probed (emerging) relative pose estimation performance, an important sub skill.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Advanced LLM Multi-Agent Systems Based on Blackboard Architecture</title>
<link>https://arxiv.org/abs/2507.01701</link>
<guid>https://arxiv.org/abs/2507.01701</guid>
<content:encoded><![CDATA[
arXiv:2507.01701v1 Announce Type: new 
Abstract: In this paper, we propose to incorporate the blackboard architecture into LLM multi-agent systems (MASs) so that (1) agents with various roles can share all the information and others' messages during the whole problem-solving process, (2) agents that will take actions are selected based on the current content of the blackboard, and (3) the selection and execution round is repeated until a consensus is reached on the blackboard. We develop the first implementation of this proposal and conduct experiments on commonsense knowledge, reasoning and mathematical datasets. The results show that our system can be competitive with the SOTA static and dynamic MASs by achieving the best average performance, and at the same time manage to spend less tokens. Our proposal has the potential to enable complex and dynamic problem-solving where well-defined structures or workflows are unavailable.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness</title>
<link>https://arxiv.org/abs/2507.01702</link>
<guid>https://arxiv.org/abs/2507.01702</guid>
<content:encoded><![CDATA[
arXiv:2507.01702v1 Announce Type: new 
Abstract: The proliferation of multimodal memes in the social media era demands that multimodal Large Language Models (mLLMs) effectively understand meme harmfulness. Existing benchmarks for assessing mLLMs on harmful meme understanding rely on accuracy-based, model-agnostic evaluations using static datasets. These benchmarks are limited in their ability to provide up-to-date and thorough assessments, as online memes evolve dynamically. To address this, we propose AdamMeme, a flexible, agent-based evaluation framework that adaptively probes the reasoning capabilities of mLLMs in deciphering meme harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive evaluations by iteratively updating the meme data with challenging samples, thereby exposing specific limitations in how mLLMs interpret harmfulness. Extensive experiments show that our framework systematically reveals the varying performance of different target mLLMs, offering in-depth, fine-grained analyses of model-specific weaknesses. Our code is available at https://github.com/Lbotirx/AdamMeme.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI</title>
<link>https://arxiv.org/abs/2507.01717</link>
<guid>https://arxiv.org/abs/2507.01717</guid>
<content:encoded><![CDATA[
arXiv:2507.01717v1 Announce Type: new 
Abstract: Patents contain rich technical knowledge that can inspire innovative product ideas, yet accessing and interpreting this information remains a challenge. This work explores the use of Large Language Models (LLMs) and autonomous agents to mine and generate product concepts from a given patent. In this work, we design Agent Ideate, a framework for automatically generating product-based business ideas from patents. We experimented with open-source LLMs and agent-based architectures across three domains: Computer Science, Natural Language Processing, and Material Chemistry. Evaluation results show that the agentic approach consistently outperformed standalone LLMs in terms of idea quality, relevance, and novelty. These findings suggest that combining LLMs with agentic workflows can significantly enhance the innovation pipeline by unlocking the untapped potential of business idea generation from patent data.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving</title>
<link>https://arxiv.org/abs/2507.01735</link>
<guid>https://arxiv.org/abs/2507.01735</guid>
<content:encoded><![CDATA[
arXiv:2507.01735v1 Announce Type: new 
Abstract: In this paper, we present details of the 1st W-CODA workshop, held in conjunction with the ECCV 2024. W-CODA aims to explore next-generation solutions for autonomous driving corner cases, empowered by state-of-the-art multimodal perception and comprehension techniques. 5 Speakers from both academia and industry are invited to share their latest progress and opinions. We collect research papers and hold a dual-track challenge, including both corner case scene understanding and generation. As the pioneering effort, we will continuously bridge the gap between frontier autonomous driving techniques and fully intelligent, reliable self-driving agents robust towards corner cases.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMD: Adaptive Momentum and Decoupled Contrastive Learning Framework for Robust Long-Tail Trajectory Prediction</title>
<link>https://arxiv.org/abs/2507.01801</link>
<guid>https://arxiv.org/abs/2507.01801</guid>
<content:encoded><![CDATA[
arXiv:2507.01801v1 Announce Type: new 
Abstract: Accurately predicting the future trajectories of traffic agents is essential in autonomous driving. However, due to the inherent imbalance in trajectory distributions, tail data in natural datasets often represents more complex and hazardous scenarios. Existing studies typically rely solely on a base model's prediction error, without considering the diversity and uncertainty of long-tail trajectory patterns. We propose an adaptive momentum and decoupled contrastive learning framework (AMD), which integrates unsupervised and supervised contrastive learning strategies. By leveraging an improved momentum contrast learning (MoCo-DT) and decoupled contrastive learning (DCL) module, our framework enhances the model's ability to recognize rare and complex trajectories. Additionally, we design four types of trajectory random augmentation methods and introduce an online iterative clustering strategy, allowing the model to dynamically update pseudo-labels and better adapt to the distributional shifts in long-tail data. We propose three different criteria to define long-tail trajectories and conduct extensive comparative experiments on the nuScenes and ETH$/$UCY datasets. The results show that AMD not only achieves optimal performance in long-tail trajectory prediction but also demonstrates outstanding overall prediction accuracy.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents</title>
<link>https://arxiv.org/abs/2507.01823</link>
<guid>https://arxiv.org/abs/2507.01823</guid>
<content:encoded><![CDATA[
arXiv:2507.01823v1 Announce Type: new 
Abstract: We present a novel approach to knowledge transfer in model-based reinforcement learning, addressing the critical challenge of deploying large world models in resource-constrained environments. Our method efficiently distills a high-capacity multi-task agent (317M parameters) into a compact model (1M parameters) on the MT30 benchmark, significantly improving performance across diverse tasks. Our distilled model achieves a state-of-the-art normalized score of 28.45, surpassing the original 1M parameter model score of 18.93. This improvement demonstrates the ability of our distillation technique to capture and consolidate complex multi-task knowledge. We further optimize the distilled model through FP16 post-training quantization, reducing its size by $\sim$50\%. Our approach addresses practical deployment limitations and offers insights into knowledge representation in large world models, paving the way for more efficient and accessible multi-task reinforcement learning systems in robotics and other resource-constrained applications. Code available at https://github.com/dmytro-kuzmenko/td-mpc-opt.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging UI Design and chatbot Interactions: Applying Form-Based Principles to Conversational Agents</title>
<link>https://arxiv.org/abs/2507.01862</link>
<guid>https://arxiv.org/abs/2507.01862</guid>
<content:encoded><![CDATA[
arXiv:2507.01862v1 Announce Type: new 
Abstract: Domain specific chatbot applications often involve multi step interactions, such as refining search filters, selecting multiple items, or performing comparisons. Traditional graphical user interfaces (GUIs) handle these workflows by providing explicit "Submit" (commit data) and "Reset" (discard data) actions, allowing back-end systems to track user intent unambiguously. In contrast, conversational agents rely on subtle language cues, which can lead to confusion and incomplete context management. This paper proposes modeling these GUI inspired metaphors acknowledgment (submit like) and context switching (reset-like) as explicit tasks within large language model (LLM) prompts. By capturing user acknowledgment, reset actions, and chain of thought (CoT) reasoning as structured session data, we preserve clarity, reduce user confusion, and align domain-specific chatbot interactions with back-end logic. We demonstrate our approach in hotel booking and customer management scenarios, highlighting improvements in multi-turn task coherence, user satisfaction, and efficiency.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decision-oriented Text Evaluation</title>
<link>https://arxiv.org/abs/2507.01923</link>
<guid>https://arxiv.org/abs/2507.01923</guid>
<content:encoded><![CDATA[
arXiv:2507.01923v1 Announce Type: new 
Abstract: Natural language generation (NLG) is increasingly deployed in high-stakes domains, yet common intrinsic evaluation methods, such as n-gram overlap or sentence plausibility, weakly correlate with actual decision-making efficacy. We propose a decision-oriented framework for evaluating generated text by directly measuring its influence on human and large language model (LLM) decision outcomes. Using market digest texts--including objective morning summaries and subjective closing-bell analyses--as test cases, we assess decision quality based on the financial performance of trades executed by human investors and autonomous LLM agents informed exclusively by these texts. Our findings reveal that neither humans nor LLM agents consistently surpass random performance when relying solely on summaries. However, richer analytical commentaries enable collaborative human-LLM teams to outperform individual human or agent baselines significantly. Our approach underscores the importance of evaluating generated text by its ability to facilitate synergistic decision-making between humans and LLMs, highlighting critical limitations of traditional intrinsic metrics.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Thin Line Between Comprehension and Persuasion in LLMs</title>
<link>https://arxiv.org/abs/2507.01936</link>
<guid>https://arxiv.org/abs/2507.01936</guid>
<content:encoded><![CDATA[
arXiv:2507.01936v1 Announce Type: new 
Abstract: Large language models (LLMs) are excellent at maintaining high-level, convincing dialogues. They are being fast deployed as chatbots and evaluators in sensitive areas, such as peer review and mental health applications. This, along with the disparate accounts on their reasoning capabilities, calls for a closer examination of LLMs and their comprehension of dialogue. In this work we begin by evaluating LLMs' ability to maintain a debate--one of the purest yet most complex forms of human communication. Then we measure how this capability relates to their understanding of what is being talked about, namely, their comprehension of dialogical structures and the pragmatic context. We find that LLMs are capable of maintaining coherent, persuasive debates, often swaying the beliefs of participants and audiences alike. We also note that awareness or suspicion of AI involvement encourage people to be more critical of the arguments made. When polling LLMs on their comprehension of deeper structures of dialogue, however, they cannot demonstrate said understanding. Our findings tie the shortcomings of LLMs-as-evaluators to their (in)ability to understand the context. More broadly, for the field of argumentation theory we posit that, if an agent can convincingly maintain a dialogue, it is not necessary for it to know what it is talking about. Hence, the modelling of pragmatic context and coherence are secondary to effectiveness.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Division with Bounded Sharing: Binary and Non-Degenerate Valuations</title>
<link>https://arxiv.org/abs/1912.00459</link>
<guid>https://arxiv.org/abs/1912.00459</guid>
<content:encoded><![CDATA[
arXiv:1912.00459v3 Announce Type: replace 
Abstract: A set of objects is to be divided fairly among agents with different tastes, modeled by additive utility-functions. If we consider the objects as indivisible, many instances of the decision problem: ``Is there a fair division of the objects among the agents'' are negative. In addition, this question is hard to solve even for most of the special cases. The latter reasons give us a good motivation to relax the problem for which the running time complexity is better, and the number of positive instances (admitting a fair division) will significantly grow. Whereas many works relax the fairness criteria, this paper introduces another relaxation: an agent is allowed to share a \emph{bounded} number of objects between two or more agents in order to attain fairness. The paper studies various notions of fairness, such as proportionality, envy-freeness, equitability, and consensus. We analyze the run-time complexity of finding a fair allocation with a given number of sharings under several restrictions on the agents' valuations, such as: binary, generalized-binary, and non-degenerate.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpikeNAS: A Fast Memory-Aware Neural Architecture Search Framework for Spiking Neural Network-based Embedded AI Systems</title>
<link>https://arxiv.org/abs/2402.11322</link>
<guid>https://arxiv.org/abs/2402.11322</guid>
<content:encoded><![CDATA[
arXiv:2402.11322v4 Announce Type: replace 
Abstract: Embedded AI systems are expected to incur low power/energy consumption for solving machine learning tasks, as these systems are usually power constrained (e.g., object recognition task in autonomous mobile agents with portable batteries). These requirements can be fulfilled by Spiking Neural Networks (SNNs), since their bio-inspired spike-based operations offer high accuracy and ultra low-power/energy computation. Currently, most of SNN architectures are derived from Artificial Neural Networks whose neurons' architectures and operations are different from SNNs, and/or developed without considering memory budgets from the underlying processing hardware of embedded platforms. These limitations hinder SNNs from reaching their full potential in accuracy and efficiency. Toward this, we propose SpikeNAS, a novel fast memory-aware neural architecture search (NAS) framework for SNNs that quickly finds an appropriate SNN architecture with high accuracy under the given memory budgets from targeted embedded systems. To do this, our SpikeNAS employs several key steps: analyzing the impacts of network operations on the accuracy, enhancing the network architecture to improve the learning quality, developing a fast memory-aware search algorithm, and performing quantization. The experimental results show that our SpikeNAS improves the searching time and maintains high accuracy compared to state-of-the-art while meeting the given memory budgets (e.g., 29x, 117x, and 3.7x faster search for CIFAR10, CIFAR100, and TinyImageNet200 respectively, using an Nvidia RTX A6000 GPU machine), thereby quickly providing the appropriate SNN architecture for the memory-constrained embedded AI systems.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collision-Free Robot Scheduling</title>
<link>https://arxiv.org/abs/2402.12019</link>
<guid>https://arxiv.org/abs/2402.12019</guid>
<content:encoded><![CDATA[
arXiv:2402.12019v3 Announce Type: replace 
Abstract: Robots are becoming an increasingly common part of scientific work within laboratory environments. In this paper, we investigate the problem of designing \emph{schedules} for completing a set of tasks at fixed locations with multiple robots in a laboratory. We represent the laboratory as a graph with tasks placed on fixed vertices and robots represented as agents, with the constraint that no two robots may occupy the same vertex at any given timestep. Each schedule is partitioned into a set of timesteps, corresponding to a walk through the graph (allowing for a robot to wait at a vertex to complete a task), with each timestep taking time equal to the time for a robot to move from one vertex to another and each task taking some given number of timesteps during the completion of which a robot must stay at the vertex containing the task. The goal is to determine a set of schedules, with one schedule for each robot, minimising the number of timesteps taken by the schedule taking the greatest number of timesteps within the set of schedules.
  We show that this problem is NP-complete for many simple classes of graphs, the problem of determining the fastest schedule, defined by the number of time steps required for a robot to visit every vertex in the schedule and complete every task assigned in its assigned schedule. Explicitly, we provide this result for complete graphs, bipartite graphs, star graphs, and planar graphs. Finally, we provide positive results for line graphs, showing that we can find an optimal set of schedules for $k$ robots completing $m$ tasks of equal length of a path of length $n$ in $O(kmn)$ time, and a $k$-approximation when the length of the tasks is unbounded.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Optimal Control for Linear Quadratic Tracking with Unknown Target States</title>
<link>https://arxiv.org/abs/2402.17247</link>
<guid>https://arxiv.org/abs/2402.17247</guid>
<content:encoded><![CDATA[
arXiv:2402.17247v3 Announce Type: replace 
Abstract: This paper addresses the inverse optimal control for the linear quadratic tracking problem with a fixed but unknown target state, which aims to estimate the possible triplets comprising the target state, the state weight matrix, and the input weight matrix from observed optimal control input and the corresponding state trajectories. Sufficient conditions have been provided for the unique determination of both the linear quadratic cost function as well as the target state. A computationally efficient and numerically reliable parameter identification algorithm is proposed by equating optimal control strategies with a system of linear equations, and the associated relative error upper bound is derived in terms of data volume and signal-to-noise ratio. Moreover, the proposed inverse optimal control algorithm is applied for the joint cluster coordination and intent identification of a multi-agent system. By incorporating the structural constraint of the Laplace matrix, the relative error upper bound can be reduced accordingly. Finally, the algorithm's efficiency and accuracy are validated by a vehicle-on-a-lever example and a multi-agent formation control example.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Co-Optimizing Reconfigurable Environments and Policies for Decentralized Multi-Agent Navigation</title>
<link>https://arxiv.org/abs/2403.14583</link>
<guid>https://arxiv.org/abs/2403.14583</guid>
<content:encoded><![CDATA[
arXiv:2403.14583v2 Announce Type: replace 
Abstract: This work views the multi-agent system and its surrounding environment as a co-evolving system, where the behavior of one affects the other. The goal is to take both agent actions and environment configurations as decision variables, and optimize these two components in a coordinated manner to improve some measure of interest. Towards this end, we consider the problem of decentralized multi-agent navigation in a cluttered environment, where we assume that the layout of the environment is reconfigurable. By introducing two sub-objectives -- multi-agent navigation and environment optimization -- we propose an agent-environment co-optimization problem and develop a coordinated algorithm that alternates between these sub-objectives to search for an optimal synthesis of agent actions and environment configurations; ultimately, improving the navigation performance. Due to the challenge of explicitly modeling the relation between the agents, the environment and their performance therein, we leverage policy gradient to formulate a model-free learning mechanism within the coordinated framework. A formal convergence analysis shows that our coordinated algorithm tracks the local minimum solution of an associated time-varying non-convex optimization problem. Experiments corroborate theoretical findings and show the benefits of co-optimization. Interestingly, the results also indicate that optimized environments can offer structural guidance to de-conflict agents in motion.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logic Optimization Meets SAT: A Novel Framework for Circuit-SAT Solving</title>
<link>https://arxiv.org/abs/2403.19446</link>
<guid>https://arxiv.org/abs/2403.19446</guid>
<content:encoded><![CDATA[
arXiv:2403.19446v2 Announce Type: replace 
Abstract: The Circuit Satisfiability (CSAT) problem, a variant of the Boolean Satisfiability (SAT) problem, plays a critical role in integrated circuit design and verification. However, existing SAT solvers, optimized for Conjunctive Normal Form (CNF), often struggle with the intrinsic complexity of circuit structures when directly applied to CSAT instances. To address this challenge, we propose a novel preprocessing framework that leverages advanced logic synthesis techniques and a reinforcement learning (RL) agent to optimize CSAT problem instances. The framework introduces a cost-customized Look-Up Table (LUT) mapping strategy that prioritizes solving efficiency, effectively transforming circuits into simplified forms tailored for SAT solvers. Our method achieves significant runtime reductions across diverse industrial-scale CSAT benchmarks, seamlessly integrating with state-of-the-art SAT solvers. Extensive experimental evaluations demonstrate up to 63\% reduction in solving time compared to conventional approaches, highlighting the potential of EDA-driven innovations to advance SAT-solving capabilities.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embodied Instruction Following in Unknown Environments</title>
<link>https://arxiv.org/abs/2406.11818</link>
<guid>https://arxiv.org/abs/2406.11818</guid>
<content:encoded><![CDATA[
arXiv:2406.11818v2 Announce Type: replace 
Abstract: Enabling embodied agents to complete complex human instructions from natural language is crucial to autonomous systems in household services. Conventional methods can only accomplish human instructions in the known environment where all interactive objects are provided to the embodied agent, and directly deploying the existing approaches for the unknown environment usually generates infeasible plans that manipulate non-existing objects. On the contrary, we propose an embodied instruction following (EIF) method for complex tasks in the unknown environment, where the agent efficiently explores the unknown environment to generate feasible plans with existing objects to accomplish abstract instructions. Specifically, we build a hierarchical embodied instruction following framework including the high-level task planner and the low-level exploration controller with multimodal large language models. We then construct a semantic representation map of the scene with dynamic region attention to demonstrate the known visual clues, where the goal of task planning and scene exploration is aligned for human instruction. For the task planner, we generate the feasible step-by-step plans for human goal accomplishment according to the task completion process and the known visual clues. For the exploration controller, the optimal navigation or object interaction policy is predicted based on the generated step-wise plans and the known visual clues. The experimental results demonstrate that our method can achieve 45.09% success rate in 204 complex human instructions such as making breakfast and tidying rooms in large house-level scenes. Code and supplementary are available at https://gary3410.github.io/eif_unknown.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Caution for the Environment: Multimodal Agents are Susceptible to Environmental Distractions</title>
<link>https://arxiv.org/abs/2408.02544</link>
<guid>https://arxiv.org/abs/2408.02544</guid>
<content:encoded><![CDATA[
arXiv:2408.02544v2 Announce Type: replace 
Abstract: This paper investigates the faithfulness of multimodal large language model (MLLM) agents in a graphical user interface (GUI) environment, aiming to address the research question of whether multimodal GUI agents can be distracted by environmental context. A general scenario is proposed where both the user and the agent are benign, and the environment, while not malicious, contains unrelated content. A wide range of MLLMs are evaluated as GUI agents using a simulated dataset, following three working patterns with different levels of perception. Experimental results reveal that even the most powerful models, whether generalist agents or specialist GUI agents, are susceptible to distractions. While recent studies predominantly focus on the helpfulness of agents, our findings first indicate that these agents are prone to environmental distractions. Furthermore, we implement an adversarial environment injection and analyze the approach to improve faithfulness, calling for a collective focus on this important topic.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The EnvDesign Model: A Method to Solve the Environment Design Problem</title>
<link>https://arxiv.org/abs/2412.18109</link>
<guid>https://arxiv.org/abs/2412.18109</guid>
<content:encoded><![CDATA[
arXiv:2412.18109v4 Announce Type: replace 
Abstract: Today, several people and organizations rely on cloud platforms. The reliability of cloud platforms depends heavily on the performance of their internal programs (agents). To better prevent regressions in cloud platforms, the design of pre-production testing environments (that test new agents, new hardwares, and other changes) must take into account the diversity of server/node properties (hardware model, virtual machine type, etc.) across the fleet and dynamically emphasize or de-emphasize the prevalence of certain node properties based on current testing priorities. This paper formulates this task as the ``environment design" problem and presents the EnvDesign model, a method that uses graph theory and optimization algorithms to solve the environment design problem. The EnvDesign model was built on context and techniques that apply to combinatorial testing in general, so it can support combinatorial testing in other domains. An earlier version of this paper was peer-reviewed and published internally at Microsoft.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Actions Speak Louder Than Words: Rate-Reward Trade-off in Markov Decision Processes</title>
<link>https://arxiv.org/abs/2502.03335</link>
<guid>https://arxiv.org/abs/2502.03335</guid>
<content:encoded><![CDATA[
arXiv:2502.03335v2 Announce Type: replace 
Abstract: The impact of communication on decision-making systems has been extensively studied under the assumption of dedicated communication channels. We instead consider communicating through actions, where the message is embedded into the actions of an agent which interacts with the environment in a Markov decision process (MDP) framework. We conceptualize the MDP environment as a finite-state channel (FSC), where the actions of the agent serve as the channel input, while the states of the MDP observed by another agent (i.e., receiver) serve as the channel output. Here, we treat the environment as a communication channel over which the agent communicates through its actions, while at the same time, trying to maximize its reward. We first characterize the optimal information theoretic trade-off between the average reward and the rate of reliable communication in the infinite-horizon regime. Then, we propose a novel framework to design a joint control/coding policy, termed \textit{Act2Comm}, which seamlessly embeds messages into actions. From a communication perspective, \textit{Act2Comm} functions as a learning-based channel coding scheme for non-differentiable FSCs under input-output constraints. From a control standpoint, \textit{Act2Comm} learns an MDP policy that incorporates communication capabilities, though at the cost of some control performance. Overall, \textit{Act2Comm} effectively balances the dual objectives of control and communication in this environment. Experimental results validate \textit{Act2Comm}'s capability to enable reliable communication while maintaining a certain level of control performance.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SFO: Piloting VLM Feedback for Offline RL</title>
<link>https://arxiv.org/abs/2503.01062</link>
<guid>https://arxiv.org/abs/2503.01062</guid>
<content:encoded><![CDATA[
arXiv:2503.01062v4 Announce Type: replace 
Abstract: While internet-scale image and textual data have enabled strong generalization in Vision-Language Models (VLMs), the absence of internet-scale control data has impeded the development of similar generalization in standard reinforcement learning (RL) agents. Although VLMs are fundamentally limited in their ability to solve control tasks due to their lack of action-conditioned training data, their capacity for image understanding allows them to provide valuable feedback in RL tasks by recognizing successful outcomes. A key challenge in Reinforcement Learning from AI Feedback (RLAIF) is determining how best to integrate VLM-derived signals into the learning process. We explore this question in the context of offline RL and introduce a class of methods called sub-trajectory filtered optimization. We identify three key insights. First, trajectory length plays a crucial role in offline RL, as full-trajectory preference learning exacerbates the stitching problem, necessitating the use of sub-trajectories. Second, even in Markovian environments, a non-Markovian reward signal from a sequence of images is required to assess trajectory improvement, as VLMs do not interpret control actions and must rely on visual cues over time. Third, a simple yet effective approach--filtered and weighted behavior cloning--consistently outperforms more complex reinforcement learning from human feedback-based methods. We propose sub-trajectory filtered behavior cloning, a method that leverages VLM feedback on sub-trajectories while incorporating a retrospective filtering mechanism that removes sub-trajectories preceding failures to improve robustness and prevent turbulence. This study is preliminary; we provide initial evidence through evaluations on a toy control domain. Please enjoy our airport puns.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Traffic Signal Control based on Multi-Agent Reinforcement Learning. Case Study on a simulated real-world corridor</title>
<link>https://arxiv.org/abs/2503.02189</link>
<guid>https://arxiv.org/abs/2503.02189</guid>
<content:encoded><![CDATA[
arXiv:2503.02189v4 Announce Type: replace 
Abstract: Previous studies that have formulated multi-agent reinforcement learning (RL) algorithms for adaptive traffic signal control have primarily used value-based RL methods. However, recent literature has shown that policy-based methods may perform better in partially observable environments. Additionally, RL methods remain largely untested for real-world normally signal timing plans because of the simplifying assumptions common in the literature. The current study attempts to address these gaps and formulates a multi-agent proximal policy optimization (MA-PPO) algorithm to implement adaptive and coordinated traffic control along an arterial corridor. The formulated MA-PPO has a centralized-critic architecture under a centralized training and decentralized execution framework. Agents are designed to allow selection and implementation of up to eight signal phases, as commonly implemented in field controllers. The formulated algorithm is tested on a simulated real-world seven intersection corridor. The speed of convergence for each agent was found to depend on the size of the action space, which depends on the number and sequence of signal phases. The performance of the formulated MA-PPO adaptive control algorithm is compared with the field implemented actuated-coordinated signal control (ASC), modeled using PTV-Vissim-MaxTime software in the loop simulation (SILs). The trained MA-PPO performed significantly better than the ASC for all movements. Compared to ASC the MA-PPO showed 2% and 24% improvements in travel time in the primary and secondary coordination directions, respectively. For cross streets movements MA-PPO also showed significant crossing time reductions. Volume sensitivity experiments revealed that the formulated MA-PPO demonstrated good stability, robustness, and adaptability to changes in traffic demand.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2HandedAfforder: Learning Precise Actionable Bimanual Affordances from Human Videos</title>
<link>https://arxiv.org/abs/2503.09320</link>
<guid>https://arxiv.org/abs/2503.09320</guid>
<content:encoded><![CDATA[
arXiv:2503.09320v3 Announce Type: replace 
Abstract: When interacting with objects, humans effectively reason about which regions of objects are viable for an intended action, i.e., the affordance regions of the object. They can also account for subtle differences in object regions based on the task to be performed and whether one or two hands need to be used. However, current vision-based affordance prediction methods often reduce the problem to naive object part segmentation. In this work, we propose a framework for extracting affordance data from human activity video datasets. Our extracted 2HANDS dataset contains precise object affordance region segmentations and affordance class-labels as narrations of the activity performed. The data also accounts for bimanual actions, i.e., two hands co-ordinating and interacting with one or more objects. We present a VLM-based affordance prediction model, 2HandedAfforder, trained on the dataset and demonstrate superior performance over baselines in affordance region segmentation for various activities. Finally, we show that our predicted affordance regions are actionable, i.e., can be used by an agent performing a task, through demonstration in robotic manipulation scenarios. Project-website: https://sites.google.com/view/2handedafforder
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedAgent-Pro: Towards Evidence-based Multi-modal Medical Diagnosis via Reasoning Agentic Workflow</title>
<link>https://arxiv.org/abs/2503.18968</link>
<guid>https://arxiv.org/abs/2503.18968</guid>
<content:encoded><![CDATA[
arXiv:2503.18968v3 Announce Type: replace 
Abstract: In modern medicine, clinical diagnosis relies on the comprehensive analysis of primarily textual and visual data, drawing on medical expertise to ensure systematic and rigorous reasoning. Recent advances in large Vision-Language Models (VLMs) and agent-based methods hold great potential for medical diagnosis, thanks to the ability to effectively integrate multi-modal patient data. However, they often provide direct answers and draw empirical-driven conclusions without quantitative analysis, which reduces their reliability and clinical usability. We propose MedAgent-Pro, a new agentic reasoning paradigm that follows the diagnosis principle in modern medicine, to decouple the process into sequential components for step-by-step, evidence-based reasoning. Our MedAgent-Pro workflow presents a hierarchical diagnostic structure to mirror this principle, consisting of disease-level standardized plan generation and patient-level personalized step-by-step reasoning. To support disease-level planning, an RAG-based agent is designed to retrieve medical guidelines to ensure alignment with clinical standards. For patient-level reasoning, we propose to integrate professional tools such as visual models to enable quantitative assessments. Meanwhile, we propose to verify the reliability of each step to achieve evidence-based diagnosis, enforcing rigorous logical reasoning and a well-founded conclusion. Extensive experiments across a wide range of anatomical regions, imaging modalities, and diseases demonstrate the superiority of MedAgent-Pro to mainstream VLMs, agentic systems and state-of-the-art expert models. Ablation studies and human evaluation by clinical experts further validate its robustness and clinical relevance. Code is available at https://github.com/jinlab-imvr/MedAgent-Pro.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EP-Diffuser: An Efficient Diffusion Model for Traffic Scene Generation and Prediction via Polynomial Representations</title>
<link>https://arxiv.org/abs/2504.05422</link>
<guid>https://arxiv.org/abs/2504.05422</guid>
<content:encoded><![CDATA[
arXiv:2504.05422v2 Announce Type: replace 
Abstract: As the prediction horizon increases, predicting the future evolution of traffic scenes becomes increasingly difficult due to the multi-modal nature of agent motion. Most state-of-the-art (SotA) prediction models primarily focus on forecasting the most likely future. However, for the safe operation of autonomous vehicles, it is equally important to cover the distribution for plausible motion alternatives. To address this, we introduce EP-Diffuser, a novel parameter-efficient diffusion-based generative model designed to capture the distribution of possible traffic scene evolutions. Conditioned on road layout and agent history, our model acts as a predictor and generates diverse, plausible scene continuations. We benchmark EP-Diffuser against two SotA models in terms of accuracy and plausibility of predictions on the Argoverse 2 dataset. Despite its significantly smaller model size, our approach achieves both highly accurate and plausible traffic scene predictions. We further evaluate model generalization ability in an out-of-distribution (OoD) test setting using Waymo Open dataset and show superior robustness of our approach.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PPO-ACT: Proximal Policy Optimization with Adversarial Curriculum Transfer for Spatial Public Goods Games</title>
<link>https://arxiv.org/abs/2505.04302</link>
<guid>https://arxiv.org/abs/2505.04302</guid>
<content:encoded><![CDATA[
arXiv:2505.04302v2 Announce Type: replace 
Abstract: This study investigates cooperation evolution mechanisms in the spatial public goods game. A novel deep reinforcement learning framework, Proximal Policy Optimization with Adversarial Curriculum Transfer (PPO-ACT), is proposed to model agent strategy optimization in dynamic environments. Traditional evolutionary game models frequently exhibit limitations in modeling long-term decision-making processes. Deep reinforcement learning effectively addresses this limitation by bridging policy gradient methods with evolutionary game theory. Our study pioneers the application of proximal policy optimization's continuous strategy optimization capability to public goods games through a two-stage adversarial curriculum transfer training paradigm. The experimental results show that PPO-ACT performs better in critical enhancement factor regimes. Compared to conventional standard proximal policy optimization methods, Q-learning and Fermi update rules, achieve earlier cooperation phase transitions and maintain stable cooperative equilibria. This framework exhibits better robustness when handling challenging scenarios like all-defector initial conditions. Systematic comparisons reveal the unique advantage of policy gradient methods in population-scale cooperation, i.e., achieving spatiotemporal payoff coordination through value function propagation. Our work provides a new computational framework for studying cooperation emergence in complex systems, algorithmically validating the punishment promotes cooperation hypothesis while offering methodological insights for multi-agent system strategy design.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grower-in-the-Loop Interactive Reinforcement Learning for Greenhouse Climate Control</title>
<link>https://arxiv.org/abs/2505.23355</link>
<guid>https://arxiv.org/abs/2505.23355</guid>
<content:encoded><![CDATA[
arXiv:2505.23355v2 Announce Type: replace 
Abstract: Climate control is crucial for greenhouse production as it directly affects crop growth and resource use. Reinforcement learning (RL) has received increasing attention in this field, but still faces challenges, including limited training efficiency and high reliance on initial learning conditions. Interactive RL, which combines human (grower) input with the RL agent's learning, offers a potential solution to overcome these challenges. However, interactive RL has not yet been applied to greenhouse climate control and may face challenges related to imperfect inputs. Therefore, this paper aims to explore the possibility and performance of applying interactive RL with imperfect inputs into greenhouse climate control, by: (1) developing three representative interactive RL algorithms tailored for greenhouse climate control (reward shaping, policy shaping and control sharing); (2) analyzing how input characteristics are often contradicting, and how the trade-offs between them make grower's inputs difficult to perfect; (3) proposing a neural network-based approach to enhance the robustness of interactive RL agents under limited input availability; (4) conducting a comprehensive evaluation of the three interactive RL algorithms with imperfect inputs in a simulated greenhouse environment. The demonstration shows that interactive RL incorporating imperfect grower inputs has the potential to improve the performance of the RL agent. RL algorithms that influence action selection, such as policy shaping and control sharing, perform better when dealing with imperfect inputs, achieving 8.4% and 6.8% improvement in profit, respectively. In contrast, reward shaping, an algorithm that manipulates the reward function, is sensitive to imperfect inputs and leads to a 9.4% decrease in profit. This highlights the importance of selecting an appropriate mechanism when incorporating imperfect inputs.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Melding the Serverless Control Plane with the Conventional Cluster Manager for Speed and Compatibility</title>
<link>https://arxiv.org/abs/2505.24551</link>
<guid>https://arxiv.org/abs/2505.24551</guid>
<content:encoded><![CDATA[
arXiv:2505.24551v2 Announce Type: replace 
Abstract: Modern serverless applications, often interactive with highly volatile traffic, challenge system scalability, demanding control planes that deliver low latency and cost efficiency. Analysis of production traces and existing systems reveals that current control plane designs (synchronous and asynchronous), particularly when built on conventional cluster managers like Kubernetes, struggle with this balance, often wasting significant CPU and memory resources on creating underutilized or idle instances. While clean-slate approaches like Dirigent offer performance gains, they sacrifice compatibility with established cluster management ecosystems.
  We introduce PulseNet, a serverless system designed to achieve high performance and low cost while maintaining compatibility with conventional cluster managers. PulseNet employs a novel dual-track control plane. A standard asynchronous track manages long-lived, full-featured regular instances for handling predictable, sustainable traffic, preserving full compatibility and feature sets off the critical path. Concurrently, an expedited parallel track addresses excessive traffic bursts that trigger cold starts. This fast path utilizes node-local agents (Pulselets) to rapidly spawn short-lived Emergency Instances with a reduced feature set, critically bypassing the latency overhead of the main cluster manager.
  Our experiments demonstrate that PulseNet, while remaining compatible with conventional managers for >98% invocation traffic, achieves 35% faster end-to-end performance at a comparable cost to the incompatible Dirigent system. PulseNet outperforms Kubernetes-compatible systems with synchronous control planes by 1.5-3.5x at 8-21% lower cost, and surpasses asynchronous counterparts by 1.7-3.5x at 3-33% lower cost.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning with Linear Function Approximations in Mean-Field Control</title>
<link>https://arxiv.org/abs/2408.00991</link>
<guid>https://arxiv.org/abs/2408.00991</guid>
<content:encoded><![CDATA[
arXiv:2408.00991v3 Announce Type: replace-cross 
Abstract: The paper focuses on mean-field type multi-agent control problems with finite state and action spaces where the dynamics and cost structures are symmetric and homogeneous, and are affected by the distribution of the agents. A standard solution method for these problems is to consider the infinite population limit as an approximation and use symmetric solutions of the limit problem to achieve near optimality. The control policies, and in particular the dynamics, depend on the population distribution in the finite population setting, or the marginal distribution of the state variable of a representative agent for the infinite population setting. Hence, learning and planning for these control problems generally require estimating the reaction of the system to all possible state distributions of the agents. To overcome this issue, we consider linear function approximation for the control problem and provide coordinated and independent learning methods. We rigorously establish error upper bounds for the performance of learned solutions. The performance gap stems from (i) the mismatch due to estimating the true model with a linear one, and (ii) using the infinite population solution in the finite population problem as an approximate control. The provided upper bounds quantify the impact of these error sources on the overall performance.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Metacognitive Architectures Remember Their Own Thoughts: A Systematic Review</title>
<link>https://arxiv.org/abs/2503.13467</link>
<guid>https://arxiv.org/abs/2503.13467</guid>
<content:encoded><![CDATA[
arXiv:2503.13467v2 Announce Type: replace-cross 
Abstract: Background: Metacognition has gained significant attention for its potential to enhance autonomy and adaptability of artificial agents but remains a fragmented field: diverse theories, terminologies, and design choices have led to disjointed developments and limited comparability across systems. Existing overviews remain at a conceptual level that is undiscerning to the underlying algorithms, representations, and their respective success.
  Methods: We address this gap by performing an explorative systematic review. Reports were included if they described techniques enabling Computational Metacognitive Architectures (CMAs) to model, store, remember, and process their episodic metacognitive experiences, one of Flavell's (1979a) three foundational components of metacognition. Searches were conducted in 16 databases, consulted between December 2023 and June 2024. Data were extracted using a 20-item framework considering pertinent aspects.
  Results: A total of 101 reports on 35 distinct CMAs were included. Our findings show that metacognitive experiences may boost system performance and explainability, e.g., via self-repair. However, lack of standardization and limited evaluations may hinder progress: only 17% of CMAs were quantitatively evaluated regarding this review's focus, and significant terminological inconsistency limits cross-architecture synthesis. Systems also varied widely in memory content, data types, and employed algorithms.
  Discussion: Limitations include the non-iterative nature of the search query, heterogeneous data availability, and an under-representation of emergent, sub-symbolic CMAs. Future research should focus on standardization and evaluation, e.g., via community-driven challenges, and on transferring promising principles to emergent architectures.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aitomia: Your Intelligent Assistant for AI-Driven Atomistic and Quantum Chemical Simulations</title>
<link>https://arxiv.org/abs/2505.08195</link>
<guid>https://arxiv.org/abs/2505.08195</guid>
<content:encoded><![CDATA[
arXiv:2505.08195v2 Announce Type: replace-cross 
Abstract: We have developed Aitomia - a platform powered by AI to assist in performing AI-driven atomistic and quantum chemical (QC) simulations. This evolving intelligent assistant platform is equipped with chatbots and AI agents to help experts and guide non-experts in setting up and running the atomistic simulations, monitoring their computation status, analyzing the simulation results, and summarizing them for the user in text and graphical forms. We achieve these goals by exploiting open-source large language models (LLMs, original and fine-tuned), rule-based agents, and a retrieval-augmented generation (RAG) system. Aitomia leverages the versatility of our MLatom ecosystem, supporting AI-enhanced computational chemistry tasks ranging from ground- to excited-state calculations such as geometry optimizations, thermochemistry, and spectra calculations. Aitomia is the first intelligent assistant publicly accessible online on a cloud computing platform for atomistic simulations of broad scope (Aitomistic Hub at https://aitomistic.xyz), while it may also be deployed locally as described at http://mlatom.com/aitomia. Aitomia is expected to lower the barrier to performing atomistic simulations, democratizing simulations, and accelerating research and development in the relevant fields.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Emergent Machina Sapiens Urge Rethinking Multi-Agent Paradigms</title>
<link>https://arxiv.org/abs/2502.04388</link>
<guid>https://arxiv.org/abs/2502.04388</guid>
<content:encoded><![CDATA[
arXiv:2502.04388v3 Announce Type: replace 
Abstract: Artificial Intelligence (AI) agents capable of autonomous learning and independent decision-making hold great promise for addressing complex challenges across various critical infrastructure domains, including transportation, energy systems, and manufacturing. However, the surge in the design and deployment of AI systems, driven by various stakeholders with distinct and unaligned objectives, introduces a crucial challenge: How can uncoordinated AI systems coexist and evolve harmoniously in shared environments without creating chaos or compromising safety? To address this, we advocate for a fundamental rethinking of existing multi-agent frameworks, such as multi-agent systems and game theory, which are largely limited to predefined rules and static objective structures. We posit that AI agents should be empowered to adjust their objectives dynamically, make compromises, form coalitions, and safely compete or cooperate through evolving relationships and social feedback. Through two case studies in critical infrastructure applications, we call for a shift toward the emergent, self-organizing, and context-aware nature of these multi-agentic AI systems.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-Bench-CL: Continual Learning for Coding Agents</title>
<link>https://arxiv.org/abs/2507.00014</link>
<guid>https://arxiv.org/abs/2507.00014</guid>
<content:encoded><![CDATA[
arXiv:2507.00014v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved impressive results on static code-generation benchmarks, but real-world software development unfolds as a continuous stream of evolving issues, fixes, and feature requests. We introduce SWE-Bench-CL, a novel continual learning benchmark built on the human-verified SWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By organizing GitHub issues into chronologically ordered sequences that reflect natural repository evolution, SWE-Bench-CL enables direct evaluation of an agent's ability to accumulate experience, transfer knowledge across tasks, and resist catastrophic forgetting. We complement the dataset with (i) a preliminary analysis of inter-task structural similarity and contextual sensitivity, (ii) an interactive LangGraph-based evaluation framework augmented with a FAISS-backed semantic memory module, and (iii) a suite of specialized continual learning metrics -- including average accuracy, forgetting, forward/backward transfer, tool-use efficiency, and a generalized Composite Continual Learning Score and CL-F-beta score -- to capture the stability-plasticity trade-off. We outline a rigorous experimental protocol comparing memory-enabled and memory-disabled agents across diverse Python repositories. All code and data are publicly available at https://github.com/thomasjoshi/agents-never-forget, providing the community with a reproducible platform for developing more adaptive and robust AI agents in software engineering.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning</title>
<link>https://arxiv.org/abs/2507.00045</link>
<guid>https://arxiv.org/abs/2507.00045</guid>
<content:encoded><![CDATA[
arXiv:2507.00045v1 Announce Type: new 
Abstract: Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have achieved near-ceiling scores on various existing benchmarks, motivating a demand for more challenging test tasks. These MLLMs have been reported to excel in a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their potential as a detective who can notice minuscule cues in an image and weave them into coherent, situational explanations, leading to a reliable answer. But can they match the performance of excellent human detectives? To answer this question, we investigate some hard scenarios where GPT-o3 can still handle, and find a common scenario where o3's performance drops to nearly zero, which we name CaughtCheating. It is inspired by the social media requests that ask others to detect suspicious clues from photos shared by the poster's partner. We conduct extensive experiments and analysis to understand why existing MLLMs lack sufficient capability to solve this kind of task. CaughtCheating provides a class of challenging visual perception and reasoning tasks with great value and practical usage. Success in these tasks paves the way for MLLMs to acquire human-level detective perception and reasoning capabilities.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems</title>
<link>https://arxiv.org/abs/2507.00079</link>
<guid>https://arxiv.org/abs/2507.00079</guid>
<content:encoded><![CDATA[
arXiv:2507.00079v1 Announce Type: new 
Abstract: Open-endedness is an active field of research in the pursuit of capable Artificial General Intelligence (AGI), allowing models to pursue tasks of their own choosing. Simultaneously, recent advancements in Large Language Models (LLMs) such as GPT-4o [9] have allowed such models to be capable of interpreting image inputs. Implementations such as OMNI-EPIC [4] have made use of such features, providing an LLM with pixel data of an agent's POV to parse the environment and allow it to solve tasks. This paper proposes that providing these visual inputs to a model gives it greater ability to interpret spatial environments, and as such, can increase the number of tasks it can successfully perform, extending its open-ended potential. To this aim, this paper proposes VoyagerVision -- a multi-modal model capable of creating structures within Minecraft using screenshots as a form of visual feedback, building on the foundation of Voyager. VoyagerVision was capable of creating an average of 2.75 unique structures within fifty iterations of the system, as Voyager was incapable of this, it is an extension in an entirely new direction. Additionally, in a set of building unit tests VoyagerVision was successful in half of all attempts in flat worlds, with most failures arising in more complex structures. Project website is available at https://esmyth-dev.github.io/VoyagerVision.github.io/
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>State and Memory is All You Need for Robust and Reliable AI Agents</title>
<link>https://arxiv.org/abs/2507.00081</link>
<guid>https://arxiv.org/abs/2507.00081</guid>
<content:encoded><![CDATA[
arXiv:2507.00081v1 Announce Type: new 
Abstract: Large language models (LLMs) have enabled powerful advances in natural language understanding and generation. Yet their application to complex, real-world scientific workflows remain limited by challenges in memory, planning, and tool integration. Here, we introduce SciBORG (Scientific Bespoke Artificial Intelligence Agents Optimized for Research Goals), a modular agentic framework that allows LLM-based agents to autonomously plan, reason, and achieve robust and reliable domain-specific task execution. Agents are constructed dynamically from source code documentation and augmented with finite-state automata (FSA) memory, enabling persistent state tracking and context-aware decision-making. This approach eliminates the need for manual prompt engineering and allows for robust, scalable deployment across diverse applications via maintaining context across extended workflows and to recover from tool or execution failures. We validate SciBORG through integration with both physical and virtual hardware, such as microwave synthesizers for executing user-specified reactions, with context-aware decision making and demonstrate its use in autonomous multi-step bioassay retrieval from the PubChem database utilizing multi-step planning, reasoning, agent-to-agent communication and coordination for execution of exploratory tasks. Systematic benchmarking shows that SciBORG agents achieve reliable execution, adaptive planning, and interpretable state transitions. Our results show that memory and state awareness are critical enablers of agentic planning and reliability, offering a generalizable foundation for deploying AI agents in complex environments.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Governed Agent Architecture for Web-Trustworthy Tokenization of Alternative Assets</title>
<link>https://arxiv.org/abs/2507.00096</link>
<guid>https://arxiv.org/abs/2507.00096</guid>
<content:encoded><![CDATA[
arXiv:2507.00096v1 Announce Type: new 
Abstract: Alternative Assets tokenization is transforming non-traditional financial instruments are represented and traded on the web. However, ensuring trustworthiness in web-based tokenized ecosystems poses significant challenges, from verifying off-chain asset data to enforcing regulatory compliance. This paper proposes an AI-governed agent architecture that integrates intelligent agents with blockchain to achieve web-trustworthy tokenization of alternative assets. In the proposed architecture, autonomous agents orchestrate the tokenization process (asset verification, valuation, compliance checking, and lifecycle management), while an AI-driven governance layer monitors agent behavior and enforces trust through adaptive policies and cryptoeconomic incentives. We demonstrate that this approach enhances transparency, security, and compliance in asset tokenization, addressing key concerns around data authenticity and fraud. A case study on tokenizing real estate assets illustrates how the architecture mitigates risks (e.g., fraudulent listings and money laundering) through real-time AI anomaly detection and on-chain enforcement. Our evaluation and analysis suggest that combining AI governance with multi-agent systems and blockchain can significantly bolster trust in tokenized asset ecosystems. This work offers a novel framework for trustworthy asset tokenization on the web and provides insights for practitioners aiming to deploy secure, compliant tokenization platforms.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BlackBoxToBlueprint: Extracting Interpretable Logic from Legacy Systems using Reinforcement Learning and Counterfactual Analysis</title>
<link>https://arxiv.org/abs/2507.00180</link>
<guid>https://arxiv.org/abs/2507.00180</guid>
<content:encoded><![CDATA[
arXiv:2507.00180v1 Announce Type: new 
Abstract: Modernizing legacy software systems is a critical but challenging task, often hampered by a lack of documentation and understanding of the original system's intricate decision logic. Traditional approaches like behavioral cloning merely replicate input-output behavior without capturing the underlying intent. This paper proposes a novel pipeline to automatically extract interpretable decision logic from legacy systems treated as black boxes. The approach uses a Reinforcement Learning (RL) agent to explore the input space and identify critical decision boundaries by rewarding actions that cause meaningful changes in the system's output. These counterfactual state transitions, where the output changes, are collected and clustered using K-Means. Decision trees are then trained on these clusters to extract human-readable rules that approximate the system's decision logic near the identified boundaries. I demonstrated the pipeline's effectiveness on three dummy legacy systems with varying complexity, including threshold-based, combined-conditional, and non-linear range logic. Results show that the RL agent successfully focuses exploration on relevant boundary regions, and the extracted rules accurately reflect the core logic of the underlying dummy systems, providing a promising foundation for generating specifications and test cases during legacy migration.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LineRetriever: Planning-Aware Observation Reduction for Web Agents</title>
<link>https://arxiv.org/abs/2507.00210</link>
<guid>https://arxiv.org/abs/2507.00210</guid>
<content:encoded><![CDATA[
arXiv:2507.00210v1 Announce Type: new 
Abstract: While large language models have demonstrated impressive capabilities in web navigation tasks, the extensive context of web pages, often represented as DOM or Accessibility Tree (AxTree) structures, frequently exceeds model context limits. Current approaches like bottom-up truncation or embedding-based retrieval lose critical information about page state and action history. This is particularly problematic for adaptive planning in web agents, where understanding the current state is essential for determining future actions. We hypothesize that embedding models lack sufficient capacity to capture plan-relevant information, especially when retrieving content that supports future action prediction. This raises a fundamental question: how can retrieval methods be optimized for adaptive planning in web navigation tasks? In response, we introduce \textit{LineRetriever}, a novel approach that leverages a language model to identify and retrieve observation lines most relevant to future navigation steps. Unlike traditional retrieval methods that focus solely on semantic similarity, \textit{LineRetriever} explicitly considers the planning horizon, prioritizing elements that contribute to action prediction. Our experiments demonstrate that \textit{LineRetriever} can reduce the size of the observation at each step for the web agent while maintaining consistent performance within the context limitations.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining Reject Relations in Stimulus Equivalence Simulations</title>
<link>https://arxiv.org/abs/2507.00265</link>
<guid>https://arxiv.org/abs/2507.00265</guid>
<content:encoded><![CDATA[
arXiv:2507.00265v1 Announce Type: new 
Abstract: Simulations offer a valuable tool for exploring stimulus equivalence (SE), yet the potential of reject relations to disrupt the assessment of equivalence class formation is contentious. This study investigates the role of reject relations in the acquisition of stimulus equivalence using computational models. We examined feedforward neural networks (FFNs), bidirectional encoder representations from transformers (BERT), and generative pre-trained transformers (GPT) across 18 conditions in matching-to-sample (MTS) simulations. Conditions varied in training structure (linear series, one-to-many, and many-to-one), relation type (select-only, reject-only, and select-reject), and negative comparison selection (standard and biased). A probabilistic agent served as a benchmark, embodying purely associative learning. The primary goal was to determine whether artificial neural networks could demonstrate equivalence class formation or whether their performance reflected associative learning. Results showed that reject relations influenced agent performance. While some agents achieved high accuracy on equivalence tests, particularly with reject relations and biased negative comparisons, this performance was comparable to the probabilistic agent. These findings suggest that artificial neural networks, including transformer models, may rely on associative strategies rather than SE. This underscores the need for careful consideration of reject relations and more stringent criteria in computational models of equivalence.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Control-Optimized Deep Reinforcement Learning for Artificially Intelligent Autonomous Systems</title>
<link>https://arxiv.org/abs/2507.00268</link>
<guid>https://arxiv.org/abs/2507.00268</guid>
<content:encoded><![CDATA[
arXiv:2507.00268v1 Announce Type: new 
Abstract: Deep reinforcement learning (DRL) has become a powerful tool for complex decision-making in machine learning and AI. However, traditional methods often assume perfect action execution, overlooking the uncertainties and deviations between an agent's selected actions and the actual system response. In real-world applications, such as robotics, mechatronics, and communication networks, execution mismatches arising from system dynamics, hardware constraints, and latency can significantly degrade performance. This work advances AI by developing a novel control-optimized DRL framework that explicitly models and compensates for action execution mismatches, a challenge largely overlooked in existing methods. Our approach establishes a structured two-stage process: determining the desired action and selecting the appropriate control signal to ensure proper execution. It trains the agent while accounting for action mismatches and controller corrections. By incorporating these factors into the training process, the AI agent optimizes the desired action with respect to both the actual control signal and the intended outcome, explicitly considering execution errors. This approach enhances robustness, ensuring that decision-making remains effective under real-world uncertainties. Our approach offers a substantial advancement for engineering practice by bridging the gap between idealized learning and real-world implementation. It equips intelligent agents operating in engineering environments with the ability to anticipate and adjust for actuation errors and system disturbances during training. We evaluate the framework in five widely used open-source mechanical simulation environments we restructured and developed to reflect real-world operating conditions, showcasing its robustness against uncertainties and offering a highly practical and efficient solution for control-oriented applications.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VTS-Guided AI Interaction Workflow for Business Insights</title>
<link>https://arxiv.org/abs/2507.00347</link>
<guid>https://arxiv.org/abs/2507.00347</guid>
<content:encoded><![CDATA[
arXiv:2507.00347v1 Announce Type: new 
Abstract: Modern firms face a flood of dense, unstructured reports. Turning these documents into usable insights takes heavy effort and is far from agile when quick answers are needed. VTS-AI tackles this gap. It integrates Visual Thinking Strategies, which emphasize evidence-based observation, linking, and thinking, into AI agents, so the agents can extract business insights from unstructured text, tables, and images at scale. The system works in three tiers (micro, meso, macro). It tags issues, links them to source pages, and rolls them into clear action levers stored in a searchable YAML file. In tests on an 18-page business report, VTS-AI matched the speed of a one-shot ChatGPT prompt yet produced richer findings: page locations, verbatim excerpts, severity scores, and causal links. Analysts can accept or adjust these outputs in the same IDE, keeping human judgment in the loop. Early results show VTS-AI spots the direction of key metrics and flags where deeper number-crunching is needed. Next steps include mapping narrative tags to financial ratios, adding finance-tuned language models through a Model-Context Protocol, and building a Risk & Safety Layer to stress-test models and secure data. These upgrades aim to make VTS-AI a production-ready, audit-friendly tool for rapid business analysis.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimal Construction of Graphs with Maximum Robustness</title>
<link>https://arxiv.org/abs/2507.00415</link>
<guid>https://arxiv.org/abs/2507.00415</guid>
<content:encoded><![CDATA[
arXiv:2507.00415v1 Announce Type: new 
Abstract: The notions of network $r$-robustness and $(r,s)$-robustness have been earlier introduced in the literature to achieve resilient control in the presence of misbehaving agents. However, while higher robustness levels provide networks with higher tolerances against the misbehaving agents, they also require dense communication structures, which are not always desirable for systems with limited capabilities and energy capacities. Therefore, this paper studies the fundamental structures behind $r$-robustness and $(r,s)$- robustness properties in two different ways. (a) We first explore and establish the tight necessary conditions on the number of edges for undirected graphs with any nodes must satisfy to achieve maximum $r$- and $(r,s)$-robustness. (b) We then use these conditions to construct two classes of undirected graphs, referred as to $\gamma$- and $(\gamma,\gamma)$-Minimal Edge Robust Graphs (MERGs), that provably achieve maximum robustness with minimal numbers of edges. We finally validate our work through some sets of simulations.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolutionary Dynamics with Self-Interaction Learning in Networked Systems</title>
<link>https://arxiv.org/abs/2507.00422</link>
<guid>https://arxiv.org/abs/2507.00422</guid>
<content:encoded><![CDATA[
arXiv:2507.00422v1 Announce Type: new 
Abstract: The evolution of cooperation in networked systems helps to understand the dynamics in social networks, multi-agent systems, and biological species. The self-persistence of individual strategies is common in real-world decision making. The self-replacement of strategies in evolutionary dynamics forms a selection amplifier, allows an agent to insist on its autologous strategy, and helps the networked system to avoid full defection. In this paper, we study the self-interaction learning in the networked evolutionary dynamics. We propose a self-interaction landscape to capture the strength of an agent's self-loop to reproduce the strategy based on local topology. We find that proper self-interaction can reduce the condition for cooperation and help cooperators to prevail in the system. For a system that favors the evolution of spite, the self-interaction can save cooperative agents from being harmed. Our results on random networks further suggest that an appropriate self-interaction landscape can significantly reduce the critical condition for advantageous mutants, especially for large-degree networks.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Coordination under Poisson Observations: A Global Game Approach</title>
<link>https://arxiv.org/abs/2507.00424</link>
<guid>https://arxiv.org/abs/2507.00424</guid>
<content:encoded><![CDATA[
arXiv:2507.00424v1 Announce Type: new 
Abstract: We study a model of strategic coordination based on a class of games with incomplete information known as Global Games. Under the assumption of Poisson-distributed signals and a Gamma prior distribution on state of the system, we demonstrate the existence of a Bayesian Nash equilibrium within the class of threshold policies for utility functions that are linear in the agents' actions. Although computing the exact threshold that constitutes an equilibrium in a system with finitely many agents is a highly non-trivial task, the problem becomes tractable by analyzing the game's potential function with countably infinitely many agents. Through numerical examples, we provide evidence that the resulting potential function is unimodal, exhibiting a well-defined maximum. Our results are applicable to the modeling of bacterial Quorum Sensing systems, whose noisy observation signals are often well-approximated using Poisson processes.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning</title>
<link>https://arxiv.org/abs/2507.00432</link>
<guid>https://arxiv.org/abs/2507.00432</guid>
<content:encoded><![CDATA[
arXiv:2507.00432v1 Announce Type: new 
Abstract: Math reasoning has become the poster child of progress in large language models (LLMs), with new models rapidly surpassing human-level performance on benchmarks like MATH and AIME. But as math leaderboards improve week by week, it is worth asking: do these gains reflect broader problem-solving ability or just narrow overfitting? To answer this question, we evaluate over 20 open-weight reasoning-tuned models across a broad suite of tasks, including math, scientific QA, agent planning, coding, and standard instruction-following. We surprisingly find that most models that succeed in math fail to transfer their gains to other domains. To rigorously study this phenomenon, we conduct controlled experiments on Qwen3-14B models using math-only data but different tuning methods. We find that reinforcement learning (RL)-tuned models generalize well across domains, while supervised fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space representation and token-space distribution shift analyses reveal that SFT induces substantial representation and output drift, while RL preserves general-domain structure. Our results suggest a need to rethink standard post-training recipes, particularly the reliance on SFT-distilled data for advancing reasoning models.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Novel Pigeon-inspired 3D Obstacle Detection and Avoidance Maneuver for Multi-UAV Systems</title>
<link>https://arxiv.org/abs/2507.00443</link>
<guid>https://arxiv.org/abs/2507.00443</guid>
<content:encoded><![CDATA[
arXiv:2507.00443v1 Announce Type: new 
Abstract: Recent advances in multi-agent systems manipulation have demonstrated a rising demand for the implementation of multi-UAV systems in urban areas, which are always subjected to the presence of static and dynamic obstacles. Inspired by the collective behavior of tilapia fish and pigeons, the focus of the presented research is on the introduction of a nature-inspired collision-free formation control for a multi-UAV system, considering the obstacle avoidance maneuvers. The developed framework in this study utilizes a semi-distributed control approach, in which, based on a probabilistic Lloyd's algorithm, a centralized guidance algorithm works for optimal positioning of the UAVs, while a distributed control approach has been used for the intervehicle collision and obstacle avoidance. Further, the presented framework has been extended to the 3D space with a novel definition of 3D maneuvers. Finally, the presented framework has been applied to multi-UAV systems in 2D and 3D scenarios, and the obtained results demonstrated the validity of the presented method in dynamic environments with stationary and moving obstacles.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Best Agent Identification for General Game Playing</title>
<link>https://arxiv.org/abs/2507.00451</link>
<guid>https://arxiv.org/abs/2507.00451</guid>
<content:encoded><![CDATA[
arXiv:2507.00451v1 Announce Type: new 
Abstract: We present an efficient and generalised procedure to accurately identify the best performing algorithm for each sub-task in a multi-problem domain. Our approach treats this as a set of best arm identification problems for multi-armed bandits, where each bandit corresponds to a specific task and each arm corresponds to a specific algorithm or agent. We propose an optimistic selection process based on the Wilson score interval (Optimistic-WS) that ranks each arm across all bandits in terms of their potential regret reduction. We evaluate the performance of Optimistic-WS on two of the most popular general game domains, the General Video Game AI (GVGAI) framework and the Ludii general game playing system, with the goal of identifying the highest performing agent for each game within a limited number of trials. Compared to previous best arm identification algorithms for multi-armed bandits, our results demonstrate a substantial performance improvement in terms of average simple regret. This novel approach can be used to significantly improve the quality and accuracy of agent evaluation procedures for general game frameworks, as well as other multi-task domains with high algorithm runtimes.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARIG: Autoregressive Interactive Head Generation for Real-time Conversations</title>
<link>https://arxiv.org/abs/2507.00472</link>
<guid>https://arxiv.org/abs/2507.00472</guid>
<content:encoded><![CDATA[
arXiv:2507.00472v1 Announce Type: new 
Abstract: Face-to-face communication, as a common human activity, motivates the research on interactive head generation. A virtual agent can generate motion responses with both listening and speaking capabilities based on the audio or motion signals of the other user and itself. However, previous clip-wise generation paradigm or explicit listener/speaker generator-switching methods have limitations in future signal acquisition, contextual behavioral understanding, and switching smoothness, making it challenging to be real-time and realistic. In this paper, we propose an autoregressive (AR) based frame-wise framework called ARIG to realize the real-time generation with better interaction realism. To achieve real-time generation, we model motion prediction as a non-vector-quantized AR process. Unlike discrete codebook-index prediction, we represent motion distribution using diffusion procedure, achieving more accurate predictions in continuous space. To improve interaction realism, we emphasize interactive behavior understanding (IBU) and detailed conversational state understanding (CSU). In IBU, based on dual-track dual-modal signals, we summarize short-range behaviors through bidirectional-integrated learning and perform contextual understanding over long ranges. In CSU, we use voice activity signals and context features of IBU to understand the various states (interruption, feedback, pause, etc.) that exist in actual conversations. These serve as conditions for the final progressive motion prediction. Extensive experiments have verified the effectiveness of our model.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.00485</link>
<guid>https://arxiv.org/abs/2507.00485</guid>
<content:encoded><![CDATA[
arXiv:2507.00485v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) is widely used in tasks where agents interact with an environment to maximize rewards. Building on this foundation, Safe Reinforcement Learning (Safe RL) incorporates a cost metric alongside the reward metric, ensuring that agents adhere to safety constraints during decision-making. In this paper, we identify that Safe RL is vulnerable to backdoor attacks, which can manipulate agents into performing unsafe actions. First, we introduce the relevant concepts and evaluation metrics for backdoor attacks in Safe RL. It is the first attack framework in the Safe RL field that involves both Positive and Negative Action sample (PNAct) is to implant backdoors, where positive action samples provide reference actions and negative action samples indicate actions to be avoided. We theoretically point out the properties of PNAct and design an attack algorithm. Finally, we conduct experiments to evaluate the effectiveness of our proposed backdoor attack framework, evaluating it with the established metrics. This paper highlights the potential risks associated with Safe RL and underscores the feasibility of such attacks. Our code and supplementary material are available at https://github.com/azure-123/PNAct.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Group Recommender Systems in the Era of Generative AI: From One-Shot Recommendations to Agentic Group Decision Support</title>
<link>https://arxiv.org/abs/2507.00535</link>
<guid>https://arxiv.org/abs/2507.00535</guid>
<content:encoded><![CDATA[
arXiv:2507.00535v1 Announce Type: new 
Abstract: More than twenty-five years ago, first ideas were developed on how to design a system that can provide recommendations to groups of users instead of individual users. Since then, a rich variety of algorithmic proposals were published, e.g., on how to acquire individual preferences, how to aggregate them, and how to generate recommendations for groups of users. However, despite the rich literature on the topic, barely any examples of real-world group recommender systems can be found. This lets us question common assumptions in academic research, in particular regarding communication processes in a group and how recommendation-supported decisions are made. In this essay, we argue that these common assumptions and corresponding system designs often may not match the needs or expectations of users. We thus call for a reorientation in this research area, leveraging the capabilities of modern Generative AI assistants like ChatGPT. Specifically, as one promising future direction, we envision group recommender systems to be systems where human group members interact in a chat and an AI-based group recommendation agent assists the decision-making process in an agentic way. Ultimately, this shall lead to a more natural group decision-making environment and finally to wider adoption of group recommendation systems in practice.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Multi-Agent Reinforcement Learning Approach for Elastic Cloud Resource Scaling</title>
<link>https://arxiv.org/abs/2507.00550</link>
<guid>https://arxiv.org/abs/2507.00550</guid>
<content:encoded><![CDATA[
arXiv:2507.00550v1 Announce Type: new 
Abstract: This paper addresses the challenges of rapid resource variation and highly uncertain task loads in cloud computing environments. It proposes an optimization method for elastic cloud resource scaling based on a multi-agent system. The method deploys multiple autonomous agents to perceive resource states in parallel and make local decisions. While maintaining the distributed nature of the system, it introduces a collaborative value function to achieve global coordination. This improves the responsiveness of resource scheduling and enhances overall system performance. To strengthen system foresight, a lightweight state prediction model is designed. It assists agents in identifying future workload trends and optimizes the selection of scaling actions. For policy training, the method adopts a centralized training and decentralized execution reinforcement learning framework. This enables agents to learn effectively and coordinate strategies under conditions of incomplete information. The paper also constructs typical cloud scenarios, including multi-tenancy and burst traffic, to evaluate the proposed method. The evaluation focuses on resource isolation, service quality assurance, and robustness. Experimental results show that the proposed multi-agent scaling strategy outperforms existing methods in resource utilization, SLA violation control, and scheduling latency. The results demonstrate strong adaptability and intelligent regulation. This provides an efficient and reliable new approach to solving the problem of elastic resource scaling in complex cloud platforms.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Circuit Structure Optimization for Quantum Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.00589</link>
<guid>https://arxiv.org/abs/2507.00589</guid>
<content:encoded><![CDATA[
arXiv:2507.00589v1 Announce Type: new 
Abstract: Reinforcement learning (RL) enables agents to learn optimal policies through environmental interaction. However, RL suffers from reduced learning efficiency due to the curse of dimensionality in high-dimensional spaces. Quantum reinforcement learning (QRL) addresses this issue by leveraging superposition and entanglement in quantum computing, allowing efficient handling of high-dimensional problems with fewer resources. QRL combines quantum neural networks (QNNs) with RL, where the parameterized quantum circuit (PQC) acts as the core computational module. The PQC performs linear and nonlinear transformations through gate operations, similar to hidden layers in classical neural networks. Previous QRL studies, however, have used fixed PQC structures based on empirical intuition without verifying their optimality. This paper proposes a QRL-NAS algorithm that integrates quantum neural architecture search (QNAS) to optimize PQC structures within QRL. Experiments demonstrate that QRL-NAS achieves higher rewards than QRL with fixed circuits, validating its effectiveness and practical utility.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Horus: A Protocol for Trustless Delegation Under Uncertainty</title>
<link>https://arxiv.org/abs/2507.00631</link>
<guid>https://arxiv.org/abs/2507.00631</guid>
<content:encoded><![CDATA[
arXiv:2507.00631v1 Announce Type: new 
Abstract: Correctness is an emergent property of systems where exposing error is cheaper than committing it. In dynamic, low-trust environments, autonomous AI agents benefit from delegating work to sub-agents, yet correctness cannot be assured through upfront specification or centralized oversight. We propose a protocol that enforces correctness through collateralized claims in a recursive verification game. Tasks are published as intents, and solvers compete to fulfill them. Selected solvers carry out tasks under risk, with correctness checked post hoc by verifiers. Any challenger can challenge a result by staking against it to trigger the verification process. Incorrect agents are slashed and correct opposition is rewarded, with an escalation path that penalizes erroneous verifiers themselves. When incentives are aligned across solvers, challengers, and verifiers, falsification conditions make correctness the Nash equilibrium.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatHLS: Towards Systematic Design Automation and Optimization for High-Level Synthesis</title>
<link>https://arxiv.org/abs/2507.00642</link>
<guid>https://arxiv.org/abs/2507.00642</guid>
<content:encoded><![CDATA[
arXiv:2507.00642v1 Announce Type: new 
Abstract: The increasing complexity of computational demands has accelerated the adoption of domain-specific accelerators, yet traditional hardware design methodologies remain constrained by prolonged development and verification cycles. High-Level Synthesis (HLS) bridges the gap between software and hardware by enabling hardware design from high-level programming languages. However, its widespread adoption is hindered by strict coding constraints and intricate hardware-specific optimizations, creating significant obstacles for developers. Recent advancements in Large Language Models (LLMs) demonstrate substantial potential in hardware design automation. However, their effectiveness is limited by the scarcity of high-quality datasets, particularly in the context of HLS. To address these challenges, we introduce ChatHLS, an agile HLS design automation and optimization workflow that leverages fine-tuned LLMs integrated within a multi-agent framework for error correction and design optimization. Our extensive evaluations reveal that ChatHLS achieves an average repair pass rate of 82.7% over 612 test cases, outperforming the GPT-4o and Llama3-8B by 19.1% and 63.0%, respectively. Furthermore, ChatHLS delivers performance enhancements ranging from 1.9$\times$ to 14.8$\times$ upon resource-constrained kernels. By enabling sophisticated optimization reasoning within practical computational budgets, ChatHLS attains a 4.9$\times$ geometric mean speedup compared to state-of-the-art DSL-based approaches. These results underscore the potential of ChatHLS in substantially expediting hardware development cycles while maintaining rigorous standards of design reliability and optimization quality.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Exaggeration in LLM Social Agents: Consistency, Bias, and Toxicity</title>
<link>https://arxiv.org/abs/2507.00657</link>
<guid>https://arxiv.org/abs/2507.00657</guid>
<content:encoded><![CDATA[
arXiv:2507.00657v1 Announce Type: new 
Abstract: We investigate how Large Language Models (LLMs) behave when simulating political discourse on social media. Leveraging 21 million interactions on X during the 2024 U.S. presidential election, we construct LLM agents based on 1,186 real users, prompting them to reply to politically salient tweets under controlled conditions. Agents are initialized either with minimal ideological cues (Zero Shot) or recent tweet history (Few Shot), allowing one-to-one comparisons with human replies. We evaluate three model families (Gemini, Mistral, and DeepSeek) across linguistic style, ideological consistency, and toxicity. We find that richer contextualization improves internal consistency but also amplifies polarization, stylized signals, and harmful language. We observe an emergent distortion that we call "generation exaggeration": a systematic amplification of salient traits beyond empirical baselines. Our analysis shows that LLMs do not emulate users, they reconstruct them. Their outputs, indeed, reflect internal optimization dynamics more than observed behavior, introducing structural biases that compromise their reliability as social proxies. This challenges their use in content moderation, deliberative simulations, and policy modeling.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Genetic Algorithms for Efficient Demonstration Generation in Real-World Reinforcement Learning Environments</title>
<link>https://arxiv.org/abs/2507.00762</link>
<guid>https://arxiv.org/abs/2507.00762</guid>
<content:encoded><![CDATA[
arXiv:2507.00762v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has demonstrated significant potential in certain real-world industrial applications, yet its broader deployment remains limited by inherent challenges such as sample inefficiency and unstable learning dynamics. This study investigates the utilization of Genetic Algorithms (GAs) as a mechanism for improving RL performance in an industrially inspired sorting environment. We propose a novel approach in which GA-generated expert demonstrations are used to enhance policy learning. These demonstrations are incorporated into a Deep Q-Network (DQN) replay buffer for experience-based learning and utilized as warm-start trajectories for Proximal Policy Optimization (PPO) agents to accelerate training convergence. Our experiments compare standard RL training with rule-based heuristics, brute-force optimization, and demonstration data, revealing that GA-derived demonstrations significantly improve RL performance. Notably, PPO agents initialized with GA-generated data achieved superior cumulative rewards, highlighting the potential of hybrid learning paradigms, where heuristic search methods complement data-driven RL. The utilized framework is publicly available and enables further research into adaptive RL strategies for real-world applications.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Many LLMs Are More Utilitarian Than One</title>
<link>https://arxiv.org/abs/2507.00814</link>
<guid>https://arxiv.org/abs/2507.00814</guid>
<content:encoded><![CDATA[
arXiv:2507.00814v1 Announce Type: new 
Abstract: Moral judgment is integral to large language model (LLM) alignment and social reasoning. As multi-agent systems gain prominence, it becomes crucial to understand how LLMs function collectively during collaboration, compared to individual agents. In human moral judgment, group deliberation leads to a utilitarian boost: a tendency to endorse norm violations that maximize benefits for the greatest number of people despite harms. We study whether a similar dynamic emerges in multi-agent LLM systems. We tested six models on well-established sets of moral dilemmas across two conditions: (1) Solo, where models reasoned independently, and (2) Group, where they engaged in multi-turn discussions in pairs or triads. In personal moral dilemmas, where agents must decide to directly harm one individual to maximize the utility for others, all models found moral violations to be more acceptable when part of a group than individually, similar to human experiments. Some models endorsed actions that maximized overall well-being, even if they benefited strangers over familiar individuals. Others became more willing to violate moral norms in groups. However, while human groups show a similar action bias, the mechanism for their utilitarian boost differs from LLMs. Whereas the human shift comes from heightened sensitivity to decision outcomes, LLM groups show either reduced norm sensitivity or enhanced impartiality. This suggests that while the surface behavior of LLM collectives mimics human group reasoning, the underlying drivers differ. We discuss the implications for AI alignment, multi-agent design, and artificial moral reasoning.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents</title>
<link>https://arxiv.org/abs/2507.00841</link>
<guid>https://arxiv.org/abs/2507.00841</guid>
<content:encoded><![CDATA[
arXiv:2507.00841v1 Announce Type: new 
Abstract: With the wide application of multimodal foundation models in intelligent agent systems, scenarios such as mobile device control, intelligent assistant interaction, and multimodal task execution are gradually relying on such large model-driven agents. However, the related systems are also increasingly exposed to potential jailbreak risks. Attackers may induce the agents to bypass the original behavioral constraints through specific inputs, and then trigger certain risky and sensitive operations, such as modifying settings, executing unauthorized commands, or impersonating user identities, which brings new challenges to system security. Existing security measures for intelligent agents still have limitations when facing complex interactions, especially in detecting potentially risky behaviors across multiple rounds of conversations or sequences of tasks. In addition, an efficient and consistent automated methodology to assist in assessing and determining the impact of such risks is currently lacking. This work explores the security issues surrounding mobile multimodal agents, attempts to construct a risk discrimination mechanism by incorporating behavioral sequence information, and designs an automated assisted assessment scheme based on a large language model. Through preliminary validation in several representative high-risk tasks, the results show that the method can improve the recognition of risky behaviors to some extent and assist in reducing the probability of agents being jailbroken. We hope that this study can provide some valuable references for the security risk modeling and protection of multimodal intelligent agent systems.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Vehicular Platooning with Wireless Federated Learning: A Resource-Aware Control Framework</title>
<link>https://arxiv.org/abs/2507.00856</link>
<guid>https://arxiv.org/abs/2507.00856</guid>
<content:encoded><![CDATA[
arXiv:2507.00856v1 Announce Type: new 
Abstract: This paper aims to enhance the performance of Vehicular Platooning (VP) systems integrated with Wireless Federated Learning (WFL). In highly dynamic environments, vehicular platoons experience frequent communication changes and resource constraints, which significantly affect information exchange and learning model synchronization. To address these challenges, we first formulate WFL in VP as a joint optimization problem that simultaneously considers Age of Information (AoI) and Federated Learning Model Drift (FLMD) to ensure timely and accurate control. Through theoretical analysis, we examine the impact of FLMD on convergence performance and develop a two-stage Resource-Aware Control framework (RACE). The first stage employs a Lagrangian dual decomposition method for resource configuration, while the second stage implements a multi-agent deep reinforcement learning approach for vehicle selection. The approach integrates Multi-Head Self-Attention and Long Short-Term Memory networks to capture spatiotemporal correlations in communication states. Experimental results demonstrate that, compared to baseline methods, the proposed framework improves AoI optimization by up to 45%, accelerates learning convergence, and adapts more effectively to dynamic VP environments on the AI4MARS dataset.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation</title>
<link>https://arxiv.org/abs/2507.00875</link>
<guid>https://arxiv.org/abs/2507.00875</guid>
<content:encoded><![CDATA[
arXiv:2507.00875v1 Announce Type: new 
Abstract: Multi-agent systems empowered by large language models (LLMs) have demonstrated remarkable capabilities in a wide range of downstream applications, including machine translation. However, the potential of LLMs in translating Hong Kong legal judgments remains uncertain due to challenges such as intricate legal terminology, culturally embedded nuances, and strict linguistic structures. In this work, we introduce TransLaw, a novel multi-agent framework implemented for real-world Hong Kong case law translation. It employs three specialized agents, namely, Translator, Annotator, and Proofreader, to collaboratively produce translations for high accuracy in legal meaning, appropriateness in style, and adequate coherence and cohesion in structure. This framework supports customizable LLM configurations and achieves tremendous cost reduction compared to professional human translation services. We evaluated its performance using 13 open-source and commercial LLMs as agents and obtained interesting findings, including that it surpasses GPT-4o in legal semantic accuracy, structural coherence, and stylistic fidelity, yet trails human experts in contextualizing complex terminology and stylistic naturalness. Our platform website is available at CityUHK, and our bilingual judgment corpus used for the evaluation is available at Hugging Face.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes</title>
<link>https://arxiv.org/abs/2507.00891</link>
<guid>https://arxiv.org/abs/2507.00891</guid>
<content:encoded><![CDATA[
arXiv:2507.00891v1 Announce Type: new 
Abstract: Memes are widely used in online social interactions, providing vivid, intuitive, and often humorous means to express intentions and emotions. Existing dialogue datasets are predominantly limited to either manually annotated or pure-text conversations, lacking the expressiveness and contextual nuance that multimodal interactions provide.To address these challenges, we introduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue dataset with contextually retrieved memes. Our dataset combines a large-scale, MLLM-annotated meme library with dialogues auto-generated by dual agents across diverse scenarios. We introduce a retrieval framework and adaptive threshold to ensure contextually relevant, naturally spaced meme usage. Experiments demonstrate the effectiveness of our approach in generating contextually appropriate and diverse meme-incorporated dialogues, offering a scalable and privacy-preserving resource for advancing multimodal conversational AI.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Powered Intelligent Urban Agents: Concepts, Capabilities, and Applications</title>
<link>https://arxiv.org/abs/2507.00914</link>
<guid>https://arxiv.org/abs/2507.00914</guid>
<content:encoded><![CDATA[
arXiv:2507.00914v1 Announce Type: new 
Abstract: The long-standing vision of intelligent cities is to create efficient, livable, and sustainable urban environments using big data and artificial intelligence technologies. Recently, the advent of Large Language Models (LLMs) has opened new ways toward realizing this vision. With powerful semantic understanding and reasoning capabilities, LLMs can be deployed as intelligent agents capable of autonomously solving complex problems across domains. In this article, we focus on Urban LLM Agents, which are LLM-powered agents that are semi-embodied within the hybrid cyber-physical-social space of cities and used for system-level urban decision-making. First, we introduce the concept of urban LLM agents, discussing their unique capabilities and features. Second, we survey the current research landscape from the perspective of agent workflows, encompassing urban sensing, memory management, reasoning, execution, and learning. Third, we categorize the application domains of urban LLM agents into five groups: urban planning, transportation, environment, public safety, and urban society, presenting representative works in each group. Finally, we discuss trustworthiness and evaluation issues that are critical for real-world deployment, and identify several open problems for future research. This survey aims to establish a foundation for the emerging field of urban LLM agents and to provide a roadmap for advancing the intersection of LLMs and urban intelligence. A curated list of relevant papers and open-source resources is maintained and continuously updated at https://github.com/usail-hkust/Awesome-Urban-LLM-Agents.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey: Learning Embodied Intelligence from Physical Simulators and World Models</title>
<link>https://arxiv.org/abs/2507.00917</link>
<guid>https://arxiv.org/abs/2507.00917</guid>
<content:encoded><![CDATA[
arXiv:2507.00917v1 Announce Type: new 
Abstract: The pursuit of artificial general intelligence (AGI) has placed embodied intelligence at the forefront of robotics research. Embodied intelligence focuses on agents capable of perceiving, reasoning, and acting within the physical world. Achieving robust embodied intelligence requires not only advanced perception and control, but also the ability to ground abstract cognition in real-world interactions. Two foundational technologies, physical simulators and world models, have emerged as critical enablers in this quest. Physical simulators provide controlled, high-fidelity environments for training and evaluating robotic agents, allowing safe and efficient development of complex behaviors. In contrast, world models empower robots with internal representations of their surroundings, enabling predictive planning and adaptive decision-making beyond direct sensory input. This survey systematically reviews recent advances in learning embodied AI through the integration of physical simulators and world models. We analyze their complementary roles in enhancing autonomy, adaptability, and generalization in intelligent robots, and discuss the interplay between external simulation and internal modeling in bridging the gap between simulated training and real-world deployment. By synthesizing current progress and identifying open challenges, this survey aims to provide a comprehensive perspective on the path toward more capable and generalizable embodied AI systems. We also maintain an active repository that contains up-to-date literature and open-source projects at https://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebArXiv: Evaluating Multimodal Agents on Time-Invariant arXiv Tasks</title>
<link>https://arxiv.org/abs/2507.00938</link>
<guid>https://arxiv.org/abs/2507.00938</guid>
<content:encoded><![CDATA[
arXiv:2507.00938v1 Announce Type: new 
Abstract: Recent progress in large language models (LLMs) has enabled the development of autonomous web agents capable of navigating and interacting with real websites. However, evaluating such agents remains challenging due to the instability and inconsistency of existing benchmarks, which often rely on dynamic content or oversimplified simulations. In this work, we introduce WebArXiv, a static and time-invariant benchmark comprising 275 web-based tasks grounded in the arXiv platform. WebArXiv ensures reproducible and reliable evaluation by anchoring tasks in fixed web snapshots with deterministic ground truths and standardized action trajectories. Through behavioral analysis, we identify a common failure mode, Rigid History Reflection, where agents over-rely on fixed interaction histories. To address this, we propose a lightweight dynamic reflection mechanism that allows agents to selectively retrieve relevant past steps during decision-making. We evaluate ten state-of-the-art web agents on WebArXiv. Results demonstrate clear performance differences across agents and validate the effectiveness of our proposed reflection strategy.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact</title>
<link>https://arxiv.org/abs/2507.00951</link>
<guid>https://arxiv.org/abs/2507.00951</guid>
<content:encoded><![CDATA[
arXiv:2507.00951v1 Announce Type: new 
Abstract: Can machines truly think, reason and act in domains like humans? This enduring question continues to shape the pursuit of Artificial General Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5, DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal fluency and partial reasoning, these systems remain fundamentally limited by their reliance on token-level prediction and lack of grounded agency. This paper offers a cross-disciplinary synthesis of AGI development, spanning artificial intelligence, cognitive neuroscience, psychology, generative models, and agent-based systems. We analyze the architectural and cognitive foundations of general intelligence, highlighting the role of modular reasoning, persistent memory, and multi-agent coordination. In particular, we emphasize the rise of Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use to enable more adaptive behavior. We discuss generalization strategies, including information compression, test-time adaptation, and training-free methods, as critical pathways toward flexible, domain-agnostic intelligence. Vision-Language Models (VLMs) are reexamined not just as perception modules but as evolving interfaces for embodied understanding and collaborative task completion. We also argue that true intelligence arises not from scale alone but from the integration of memory and reasoning: an orchestration of modular, interactive, and self-improving components where compression enables adaptive behavior. Drawing on advances in neurosymbolic systems, reinforcement learning, and cognitive scaffolding, we explore how recent architectures begin to bridge the gap between statistical learning and goal-directed cognition. Finally, we identify key scientific, technical, and ethical challenges on the path to AGI.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLM Agent Safety via Causal Influence Prompting</title>
<link>https://arxiv.org/abs/2507.00979</link>
<guid>https://arxiv.org/abs/2507.00979</guid>
<content:encoded><![CDATA[
arXiv:2507.00979v1 Announce Type: new 
Abstract: As autonomous agents powered by large language models (LLMs) continue to demonstrate potential across various assistive tasks, ensuring their safe and reliable behavior is crucial for preventing unintended consequences. In this work, we introduce CIP, a novel technique that leverages causal influence diagrams (CIDs) to identify and mitigate risks arising from agent decision-making. CIDs provide a structured representation of cause-and-effect relationships, enabling agents to anticipate harmful outcomes and make safer decisions. Our approach consists of three key steps: (1) initializing a CID based on task specifications to outline the decision-making process, (2) guiding agent interactions with the environment using the CID, and (3) iteratively refining the CID based on observed behaviors and outcomes. Experimental results demonstrate that our method effectively enhances safety in both code execution and mobile device control tasks.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RTMap: Real-Time Recursive Mapping with Change Detection and Localization</title>
<link>https://arxiv.org/abs/2507.00980</link>
<guid>https://arxiv.org/abs/2507.00980</guid>
<content:encoded><![CDATA[
arXiv:2507.00980v1 Announce Type: new 
Abstract: While recent online HD mapping methods relieve burdened offline pipelines and solve map freshness, they remain limited by perceptual inaccuracies, occlusion in dense traffic, and an inability to fuse multi-agent observations. We propose RTMap to enhance these single-traversal methods by persistently crowdsourcing a multi-traversal HD map as a self-evolutional memory. On onboard agents, RTMap simultaneously addresses three core challenges in an end-to-end fashion: (1) Uncertainty-aware positional modeling for HD map elements, (2) probabilistic-aware localization w.r.t. the crowdsourced prior-map, and (3) real-time detection for possible road structural changes. Experiments on several public autonomous driving datasets demonstrate our solid performance on both the prior-aided map quality and the localization accuracy, demonstrating our effectiveness of robustly serving downstream prediction and planning modules while gradually improving the accuracy and freshness of the crowdsourced prior-map asynchronously. Our source-code will be made publicly available at https://github.com/CN-ADLab/RTMap (Camera ready version incorporating reviewer suggestions will be updated soon).
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.01006</link>
<guid>https://arxiv.org/abs/2507.01006</guid>
<content:encoded><![CDATA[
arXiv:2507.01006v1 Announce Type: new 
Abstract: We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to advance general-purpose multimodal reasoning. In this report, we share our key findings in the development of the reasoning-centric training framework. We first develop a capable vision foundation model with significant potential through large-scale pre-training, which arguably sets the upper bound for the final performance. Reinforcement Learning with Curriculum Sampling (RLCS) then unlocks the full potential of the model, leading to comprehensive capability enhancement across a diverse range of tasks, including STEM problem solving, video understanding, content recognition, coding, grounding, GUI-based agents, and long document understanding, among others. To facilitate research in this field, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art performance among models of comparable size. In a comprehensive evaluation across 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all tasks and achieves comparable or even superior performance on 18 benchmarks relative to the significantly larger Qwen2.5-VL-72B. Notably, GLM-4.1V-9B-Thinking also demonstrates competitive or superior performance compared to closed-source models such as GPT-4o on challenging tasks including long document understanding and STEM reasoning, further underscoring its strong capabilities. Code, models and more information are released at https://github.com/THUDM/GLM-4.1V-Thinking.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Endogenous Network Structures with Precision and Dimension Choices</title>
<link>https://arxiv.org/abs/2507.00249</link>
<guid>https://arxiv.org/abs/2507.00249</guid>
<content:encoded><![CDATA[
arXiv:2507.00249v1 Announce Type: cross 
Abstract: This paper presents a social learning model where the network structure is endogenously determined by signal precision and dimension choices. Agents not only choose the precision of their signals and what dimension of the state to learn about, but these decisions directly determine the underlying network structure on which social learning occurs. We show that under a fixed network structure, the optimal precision choice is sublinear in the agent's stationary influence in the network, and this individually optimal choice is worse than the socially optimal choice by a factor of $n^{1/3}$. Under a dynamic network structure, we specify the network by defining a kernel distance between agents, which then determines how much weight agents place on one another. Agents choose dimensions to learn about such that their choice minimizes the squared sum of influences of all agents: a network with equally distributed influence across agents is ideal.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ranking Quantilized Mean-Field Games with an Application to Early-Stage Venture Investments</title>
<link>https://arxiv.org/abs/2507.00853</link>
<guid>https://arxiv.org/abs/2507.00853</guid>
<content:encoded><![CDATA[
arXiv:2507.00853v1 Announce Type: cross 
Abstract: Quantilized mean-field game models involve quantiles of the population's distribution. We study a class of such games with a capacity for ranking games, where the performance of each agent is evaluated based on its terminal state relative to the population's $\alpha$-quantile value, $\alpha \in (0,1)$. This evaluation criterion is designed to select the top $(1-\alpha)\%$ performing agents. We provide two formulations for this competition: a target-based formulation and a threshold-based formulation. In the former and latter formulations, to satisfy the selection condition, each agent aims for its terminal state to be \textit{exactly} equal and \textit{at least} equal to the population's $\alpha$-quantile value, respectively.
  For the target-based formulation, we obtain an analytic solution and demonstrate the $\epsilon$-Nash property for the asymptotic best-response strategies in the $N$-player game. Specifically, the quantilized mean-field consistency condition is expressed as a set of forward-backward ordinary differential equations, characterizing the $\alpha$-quantile value at equilibrium. For the threshold-based formulation, we obtain a semi-explicit solution and numerically solve the resulting quantilized mean-field consistency condition.
  Subsequently, we propose a new application in the context of early-stage venture investments, where a venture capital firm financially supports a group of start-up companies engaged in a competition over a finite time horizon, with the goal of selecting a percentage of top-ranking ones to receive the next round of funding at the end of the time horizon. We present the results and interpretations of numerical experiments for both formulations discussed in this context and show that the target-based formulation provides a very good approximation for the threshold-based formulation.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Evolution of Altruistic Rationality Provides a Solution to Social Dilemmas via Rational Reciprocity</title>
<link>https://arxiv.org/abs/2507.00858</link>
<guid>https://arxiv.org/abs/2507.00858</guid>
<content:encoded><![CDATA[
arXiv:2507.00858v1 Announce Type: cross 
Abstract: Decades of scientific inquiry have sought to understand how evolution fosters cooperation, a concept seemingly at odds with the belief that evolution should produce rational, self-interested individuals. Most previous work has focused on the evolution of cooperation among boundedly rational individuals whose decisions are governed by behavioral rules that do not need to be rational. Here, using an evolutionary model, we study how altruism can evolve in a community of rational agents and promote cooperation. We show that in both well-mixed and structured populations, a population of objectively rational agents is readily invaded by mutant individuals who make rational decisions but evolve a distorted (i.e., subjective) perception of their payoffs. This promotes behavioral diversity and gives rise to the evolution of rational, other-regarding agents who naturally solve all the known strategic problems of two-person, two-strategy games by perceiving their games as pure coordination games.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lifelong Learning of Video Diffusion Models From a Single Video Stream</title>
<link>https://arxiv.org/abs/2406.04814</link>
<guid>https://arxiv.org/abs/2406.04814</guid>
<content:encoded><![CDATA[
arXiv:2406.04814v3 Announce Type: replace 
Abstract: This work demonstrates that training autoregressive video diffusion models from a single video stream$\unicode{x2013}$resembling the experience of embodied agents$\unicode{x2013}$is not only possible, but can also be as effective as standard offline training given the same number of gradient steps. Our work further reveals that this main result can be achieved using experience replay methods that only retain a subset of the preceding video stream. To support training and evaluation in this setting, we introduce four new datasets for streaming lifelong generative video modeling: Lifelong Bouncing Balls, Lifelong 3D Maze, Lifelong Drive, and Lifelong PLAICraft, each consisting of one million consecutive frames from environments of increasing complexity.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sliding Puzzles Gym: A Scalable Benchmark for State Representation in Visual Reinforcement Learning</title>
<link>https://arxiv.org/abs/2410.14038</link>
<guid>https://arxiv.org/abs/2410.14038</guid>
<content:encoded><![CDATA[
arXiv:2410.14038v4 Announce Type: replace 
Abstract: Effective visual representation learning is crucial for reinforcement learning (RL) agents to extract task-relevant information from raw sensory inputs and generalize across diverse environments. However, existing RL benchmarks lack the ability to systematically evaluate representation learning capabilities in isolation from other learning challenges. To address this gap, we introduce the Sliding Puzzles Gym (SPGym), a novel benchmark that transforms the classic 8-tile puzzle into a visual RL task with images drawn from arbitrarily large datasets. SPGym's key innovation lies in its ability to precisely control representation learning complexity through adjustable grid sizes and image pools, while maintaining fixed environment dynamics, observation, and action spaces. This design enables researchers to isolate and scale the visual representation challenge independently of other learning components. Through extensive experiments with model-free and model-based RL algorithms, we uncover fundamental limitations in current methods' ability to handle visual diversity. As we increase the pool of possible images, all algorithms exhibit in- and out-of-distribution performance degradation, with sophisticated representation learning techniques often underperforming simpler approaches like data augmentation. These findings highlight critical gaps in visual representation learning for RL and establish SPGym as a valuable tool for driving progress in robust, generalizable decision-making systems.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Large-Scale In-Context Reinforcement Learning by Meta-Training in Randomized Worlds</title>
<link>https://arxiv.org/abs/2502.02869</link>
<guid>https://arxiv.org/abs/2502.02869</guid>
<content:encoded><![CDATA[
arXiv:2502.02869v2 Announce Type: replace 
Abstract: In-Context Reinforcement Learning (ICRL) enables agents to learn automatically and on-the-fly from their interactive experiences. However, a major challenge in scaling up ICRL is the lack of scalable task collections. To address this, we propose the procedurally generated tabular Markov Decision Processes, named AnyMDP. Through a carefully designed randomization process, AnyMDP is capable of generating high-quality tasks on a large scale while maintaining relatively low structural biases. To facilitate efficient meta-training at scale, we further introduce step-wise supervision and induce prior information in the ICRL framework.Our results demonstrate that, with a sufficiently large scale of AnyMDP tasks, the proposed model can generalize to tasks that were not considered in the training set. The scalable task set provided by AnyMDP also enables a more thorough empirical investigation of the relationship between data distribution and ICRL performance. We further show that the generalization of ICRL potentially comes at the cost of increased task diversity and longer adaptation periods. This finding carries critical implications for scaling robust ICRL capabilities, highlighting the necessity of diverse and extensive task design, and prioritizing asymptotic performance over few-shot adaptation.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoCMT: Communication-Efficient Cross-Modal Transformer for Collaborative Perception</title>
<link>https://arxiv.org/abs/2503.13504</link>
<guid>https://arxiv.org/abs/2503.13504</guid>
<content:encoded><![CDATA[
arXiv:2503.13504v2 Announce Type: replace 
Abstract: Multi-agent collaborative perception enhances each agent perceptual capabilities by sharing sensing information to cooperatively perform robot perception tasks. This approach has proven effective in addressing challenges such as sensor deficiencies, occlusions, and long-range perception. However, existing representative collaborative perception systems transmit intermediate feature maps, such as bird-eye view (BEV) representations, which contain a significant amount of non-critical information, leading to high communication bandwidth requirements. To enhance communication efficiency while preserving perception capability, we introduce CoCMT, an object-query-based collaboration framework that optimizes communication bandwidth by selectively extracting and transmitting essential features. Within CoCMT, we introduce the Efficient Query Transformer (EQFormer) to effectively fuse multi-agent object queries and implement a synergistic deep supervision to enhance the positive reinforcement between stages, leading to improved overall performance. Experiments on OPV2V and V2V4Real datasets show CoCMT outperforms state-of-the-art methods while drastically reducing communication needs. On V2V4Real, our model (Top-50 object queries) requires only 0.416 Mb bandwidth, 83 times less than SOTA methods, while improving AP70 by 1.1 percent. This efficiency breakthrough enables practical collaborative perception deployment in bandwidth-constrained environments without sacrificing detection accuracy.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPADE: Structured Prompting Augmentation for Dialogue Enhancement in Machine-Generated Text Detection</title>
<link>https://arxiv.org/abs/2503.15044</link>
<guid>https://arxiv.org/abs/2503.15044</guid>
<content:encoded><![CDATA[
arXiv:2503.15044v2 Announce Type: replace 
Abstract: The increasing capability of large language models (LLMs) to generate synthetic content has heightened concerns about their misuse, driving the development of Machine-Generated Text (MGT) detection models. However, these detectors face significant challenges due to the lack of high-quality synthetic datasets for training. To address this issue, we propose SPADE, a structured framework for detecting synthetic dialogues using prompt-based positive and negative samples. Our proposed methods yield 14 new dialogue datasets, which we benchmark against eight MGT detection models. The results demonstrate improved generalization performance when utilizing a mixed dataset produced by proposed augmentation frameworks, offering a practical approach to enhancing LLM application security. Considering that real-world agents lack knowledge of future opponent utterances, we simulate online dialogue detection and examine the relationship between chat history length and detection accuracy. Our open-source datasets, code and prompts can be downloaded from https://github.com/AngieYYF/SPADE-customer-service-dialogue.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeking and Updating with Live Visual Knowledge</title>
<link>https://arxiv.org/abs/2504.05288</link>
<guid>https://arxiv.org/abs/2504.05288</guid>
<content:encoded><![CDATA[
arXiv:2504.05288v2 Announce Type: replace 
Abstract: The visual world around us constantly evolves, from real-time news and social media trends to global infrastructure changes visible through satellite imagery and augmented reality enhancements. However, Multimodal Large Language Models (MLLMs), which automate many tasks, struggle to stay current, limited by the cutoff dates in their fixed training datasets. To quantify this stagnation, we introduce LiveVQA, the first-of-its-kind dataset featuring 107,143 samples and 12 categories data specifically designed to support research in both seeking and updating with live visual knowledge. Drawing from recent news articles, video platforms, and academic publications in April 2024-May 2025, LiveVQA enables evaluation of how models handle latest visual information beyond their knowledge boundaries and how current methods help to update them. Our comprehensive benchmarking of 17 state-of-the-art MLLMs reveals significant performance gaps on content beyond knowledge cutoff, and tool-use or agentic visual seeking framework drastically gain an average of 327% improvement. Furthermore, we explore parameter-efficient fine-tuning (PEFT) methods to update MLLMs with new visual knowledge. We dive deeply to the critical balance between adapter capacity and model capability when updating MLLMs with new visual knowledge. All the experimental dataset and source code are publicly available at: https://livevqa.github.io.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not Minds, but Signs: Reframing LLMs through Semiotics</title>
<link>https://arxiv.org/abs/2505.17080</link>
<guid>https://arxiv.org/abs/2505.17080</guid>
<content:encoded><![CDATA[
arXiv:2505.17080v2 Announce Type: replace 
Abstract: This paper challenges the prevailing tendency to frame Large Language Models (LLMs) as cognitive systems, arguing instead for a semiotic perspective that situates these models within the broader dynamics of sign manipulation and meaning-making. Rather than assuming that LLMs understand language or simulate human thought, we propose that their primary function is to recombine, recontextualize, and circulate linguistic forms based on probabilistic associations. By shifting from a cognitivist to a semiotic framework, we avoid anthropomorphism and gain a more precise understanding of how LLMs participate in cultural processes, not by thinking, but by generating texts that invite interpretation. Through theoretical analysis and practical examples, the paper demonstrates how LLMs function as semiotic agents whose outputs can be treated as interpretive acts, open to contextual negotiation and critical reflection. We explore applications in literature, philosophy, education, and cultural production, emphasizing how LLMs can serve as tools for creativity, dialogue, and critical inquiry. The semiotic paradigm foregrounds the situated, contingent, and socially embedded nature of meaning, offering a more rigorous and ethically aware framework for studying and using LLMs. Ultimately, this approach reframes LLMs as technological participants in an ongoing ecology of signs. They do not possess minds, but they alter how we read, write, and make meaning, compelling us to reconsider the foundations of language, interpretation, and the role of artificial systems in the production of knowledge.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research</title>
<link>https://arxiv.org/abs/2505.19955</link>
<guid>https://arxiv.org/abs/2505.19955</guid>
<content:encoded><![CDATA[
arXiv:2505.19955v2 Announce Type: replace 
Abstract: Recent advancements in AI agents have demonstrated their growing potential to drive and support scientific discovery. In this work, we introduce MLR-Bench, a comprehensive benchmark for evaluating AI agents on open-ended machine learning research. MLR-Bench includes three key components: (1) 201 research tasks sourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2) MLR-Judge, an automated evaluation framework combining LLM-based reviewers with carefully designed review rubrics to assess research quality; and (3) MLR-Agent, a modular agent scaffold capable of completing research tasks through four stages: idea generation, proposal formulation, experimentation, and paper writing. Our framework supports both stepwise assessment across these distinct research stages, and end-to-end evaluation of the final research paper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced coding agent, finding that while LLMs are effective at generating coherent ideas and well-structured papers, current coding agents frequently (e.g., in 80% of the cases) produce fabricated or invalidated experimental results--posing a major barrier to scientific reliability. We validate MLR-Judge through human evaluation, showing high agreement with expert reviewers, supporting its potential as a scalable tool for research evaluation. We open-source MLR-Bench to help the community benchmark, diagnose, and improve AI research agents toward trustworthy and transparent scientific discovery.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conceptual Framework Toward Embodied Collective Adaptive Intelligence</title>
<link>https://arxiv.org/abs/2505.23153</link>
<guid>https://arxiv.org/abs/2505.23153</guid>
<content:encoded><![CDATA[
arXiv:2505.23153v2 Announce Type: replace 
Abstract: Collective Adaptive Intelligence (CAI) represent a transformative approach in embodied AI, wherein numerous autonomous agents collaborate, adapt, and self-organize to navigate complex, dynamic environments. By enabling systems to reconfigure themselves in response to unforeseen challenges, CAI facilitate robust performance in real-world scenarios. This article introduces a conceptual framework for designing and analyzing CAI. It delineates key attributes including task generalization, resilience, scalability, and self-assembly, aiming to bridge theoretical foundations with practical methodologies for engineering adaptive, emergent intelligence. By providing a structured foundation for understanding and implementing CAI, this work seeks to guide researchers and practitioners in developing more resilient, scalable, and adaptable AI systems across various domains.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BIMgent: Towards Autonomous Building Modeling via Computer-use Agents</title>
<link>https://arxiv.org/abs/2506.07217</link>
<guid>https://arxiv.org/abs/2506.07217</guid>
<content:encoded><![CDATA[
arXiv:2506.07217v2 Announce Type: replace 
Abstract: Existing computer-use agents primarily focus on general-purpose desktop automation tasks, with limited exploration of their application in highly specialized domains. In particular, the 3D building modeling process in the Architecture, Engineering, and Construction (AEC) sector involves open-ended design tasks and complex interaction patterns within Building Information Modeling (BIM) authoring software, which has yet to be thoroughly addressed by current studies. In this paper, we propose BIMgent, an agentic framework powered by multimodal large language models (LLMs), designed to enable autonomous building model authoring via graphical user interface (GUI) operations. BIMgent automates the architectural building modeling process, including multimodal input for conceptual design, planning of software-specific workflows, and efficient execution of the authoring GUI actions. We evaluate BIMgent on real-world building modeling tasks, including both text-based conceptual design generation and reconstruction from existing building design. The design quality achieved by BIMgent was found to be reasonable. Its operations achieved a 32% success rate, whereas all baseline models failed to complete the tasks (0% success rate). Results demonstrate that BIMgent effectively reduces manual workload while preserving design intent, highlighting its potential for practical deployment in real-world architectural modeling scenarios. Project page: https://tumcms.github.io/BIMgent.github.io/
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning for Cyber-Physical Systems Security</title>
<link>https://arxiv.org/abs/2506.22445</link>
<guid>https://arxiv.org/abs/2506.22445</guid>
<content:encoded><![CDATA[
arXiv:2506.22445v1 Announce Type: new 
Abstract: Cyber-Physical Systems play a critical role in the infrastructure of various sectors, including manufacturing, energy distribution, and autonomous transportation systems. However, their increasing connectivity renders them highly vulnerable to sophisticated cyber threats, such as adaptive and zero-day attacks, against which traditional security methods like rule-based intrusion detection and single-agent reinforcement learning prove insufficient. To overcome these challenges, this paper introduces a novel Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL) framework. HAMARL employs a hierarchical structure consisting of local agents dedicated to subsystem security and a global coordinator that oversees and optimizes comprehensive, system-wide defense strategies. Furthermore, the framework incorporates an adversarial training loop designed to simulate and anticipate evolving cyber threats, enabling proactive defense adaptation. Extensive experimental evaluations conducted on a simulated industrial IoT testbed indicate that HAMARL substantially outperforms traditional multi-agent reinforcement learning approaches, significantly improving attack detection accuracy, reducing response times, and ensuring operational continuity. The results underscore the effectiveness of combining hierarchical multi-agent coordination with adversarially-aware training to enhance the resilience and security of next-generation CPS.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Discovery of Behavioral Primitives from Sensorimotor Dynamic Functional Connectivity</title>
<link>https://arxiv.org/abs/2506.22473</link>
<guid>https://arxiv.org/abs/2506.22473</guid>
<content:encoded><![CDATA[
arXiv:2506.22473v1 Announce Type: new 
Abstract: The movements of both animals and robots give rise to streams of high-dimensional motor and sensory information. Imagine the brain of a newborn or the controller of a baby humanoid robot trying to make sense of unprocessed sensorimotor time series. Here, we present a framework for studying the dynamic functional connectivity between the multimodal sensory signals of a robotic agent to uncover an underlying structure. Using instantaneous mutual information, we capture the time-varying functional connectivity (FC) between proprioceptive, tactile, and visual signals, revealing the sensorimotor relationships. Using an infinite relational model, we identified sensorimotor modules and their evolving connectivity. To further interpret these dynamic interactions, we employed non-negative matrix factorization, which decomposed the connectivity patterns into additive factors and their corresponding temporal coefficients. These factors can be considered the agent's motion primitives or movement synergies that the agent can use to make sense of its sensorimotor space and later for behavior selection. In the future, the method can be deployed in robot learning as well as in the analysis of human movement trajectories or brain signals.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Service Placement in Small Cell Networks Using Distributed Best Arm Identification in Linear Bandits</title>
<link>https://arxiv.org/abs/2506.22480</link>
<guid>https://arxiv.org/abs/2506.22480</guid>
<content:encoded><![CDATA[
arXiv:2506.22480v1 Announce Type: new 
Abstract: As users in small cell networks increasingly rely on computation-intensive services, cloud-based access often results in high latency. Multi-access edge computing (MEC) mitigates this by bringing computational resources closer to end users, with small base stations (SBSs) serving as edge servers to enable low-latency service delivery. However, limited edge capacity makes it challenging to decide which services to deploy locally versus in the cloud, especially under unknown service demand and dynamic network conditions. To tackle this problem, we model service demand as a linear function of service attributes and formulate the service placement task as a linear bandit problem, where SBSs act as agents and services as arms. The goal is to identify the service that, when placed at the edge, offers the greatest reduction in total user delay compared to cloud deployment. We propose a distributed and adaptive multi-agent best-arm identification (BAI) algorithm under a fixed-confidence setting, where SBSs collaborate to accelerate learning. Simulations show that our algorithm identifies the optimal service with the desired confidence and achieves near-optimal speedup, as the number of learning rounds decreases proportionally with the number of SBSs. We also provide theoretical analysis of the algorithm's sample complexity and communication overhead.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents</title>
<link>https://arxiv.org/abs/2506.22485</link>
<guid>https://arxiv.org/abs/2506.22485</guid>
<content:encoded><![CDATA[
arXiv:2506.22485v1 Announce Type: new 
Abstract: This study presents a modular, multi-agent system for the automated review of highly structured enterprise business documents using AI agents. Unlike prior solutions focused on unstructured texts or limited compliance checks, this framework leverages modern orchestration tools such as LangChain, CrewAI, TruLens, and Guidance to enable section-by-section evaluation of documents for accuracy, consistency, completeness, and clarity. Specialized agents, each responsible for discrete review criteria such as template compliance or factual correctness, operate in parallel or sequence as required. Evaluation outputs are enforced to a standardized, machine-readable schema, supporting downstream analytics and auditability. Continuous monitoring and a feedback loop with human reviewers allow for iterative system improvement and bias mitigation.
  Quantitative evaluation demonstrates that the AI Agent-as-Judge system approaches or exceeds human performance in key areas: achieving 99% information consistency (vs. 92% for humans), halving error and bias rates, and reducing average review time from 30 to 2.5 minutes per document, with a 95% agreement rate between AI and expert human judgment. While promising for a wide range of industries, the study also discusses current limitations, including the need for human oversight in highly specialized domains and the operational cost of large-scale LLM usage. The proposed system serves as a flexible, auditable, and scalable foundation for AI-driven document quality assurance in the enterprise context.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrated Multimodal Sensing and Communication: Challenges, Technologies, and Architectures</title>
<link>https://arxiv.org/abs/2506.22507</link>
<guid>https://arxiv.org/abs/2506.22507</guid>
<content:encoded><![CDATA[
arXiv:2506.22507v1 Announce Type: new 
Abstract: The evolution towards 6G networks requires the intelligent integration of communication and sensing capabilities to support diverse and complex applications, such as autonomous driving and immersive services. However, existing integrated sensing and communication (ISAC) systems predominantly rely on single-modal sensors as primary participants, which leads to a limited representation of environmental features and significant performance bottlenecks under the emerging requirements of 6G applications. This limitation motivates a paradigm shift from single-modal to multimodal ISAC. In this article, we first analyze the key challenges in realizing multimodal ISAC, including the fusion of heterogeneous multimodal data, the high communication overhead among distributed sensors, and the design of efficient and scalable system architectures. We then introduce several enabling technologies, such as large AI models, semantic communication, and multi-agent systems, that hold promise for addressing these challenges. To operationalize these technologies, we zoom into three architectural paradigms: fusion-based multimodal ISAC (F-MAC), interaction-based multimodal ISAC (I-MAC), and relay-based multimodal ISAC (R-MAC), each tailored to organize devices and modalities for efficient collaboration in different scenarios. Thereafter, a case study is presented based on the F-MAC scheme, demonstrating that the scheme achieves more comprehensive sensing and improves sensing accuracy by approximately 80% compared to conventional single-modal ISAC systems. Finally, we discuss several open issues to be addressed in the future.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset</title>
<link>https://arxiv.org/abs/2506.22554</link>
<guid>https://arxiv.org/abs/2506.22554</guid>
<content:encoded><![CDATA[
arXiv:2506.22554v1 Announce Type: new 
Abstract: Human communication involves a complex interplay of verbal and nonverbal signals, essential for conveying meaning and achieving interpersonal goals. To develop socially intelligent AI technologies, it is crucial to develop models that can both comprehend and generate dyadic behavioral dynamics. To this end, we introduce the Seamless Interaction Dataset, a large-scale collection of over 4,000 hours of face-to-face interaction footage from over 4,000 participants in diverse contexts. This dataset enables the development of AI technologies that understand dyadic embodied dynamics, unlocking breakthroughs in virtual agents, telepresence experiences, and multimodal content analysis tools. We also develop a suite of models that utilize the dataset to generate dyadic motion gestures and facial expressions aligned with human speech. These models can take as input both the speech and visual behavior of their interlocutors. We present a variant with speech from an LLM model and integrations with 2D and 3D rendering methods, bringing us closer to interactive virtual agents. Additionally, we describe controllable variants of our motion models that can adapt emotional responses and expressivity levels, as well as generating more semantically-relevant gestures. Finally, we discuss methods for assessing the quality of these dyadic motion models, which are demonstrating the potential for more intuitive and responsive human-AI interactions.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capacity Planning in Stable Matching with Truthful or Strategic Preference Uncertainty</title>
<link>https://arxiv.org/abs/2506.22560</link>
<guid>https://arxiv.org/abs/2506.22560</guid>
<content:encoded><![CDATA[
arXiv:2506.22560v1 Announce Type: new 
Abstract: Recent studies on many-to-one matching markets have explored agents with flexible capacity and truthful preference reporting, focusing on mechanisms that jointly design capacities and select a matching. However, in real-world applications such as school choice and residency matching, preferences are revealed after capacity decisions are made, with matching occurring afterward; uncertainty about agents' preferences must be considered during capacity planning. Moreover, even under strategy-proof mechanisms, agents may strategically misreport preferences based on beliefs about admission chances. We introduce a two-stage stochastic matching problem with uncertain preferences, using school choice as a case study. In the first stage, the clearinghouse expands schools' capacities before observing students' reported preferences. Students either report their true preferences, producing exogenous uncertainty, or act strategically, submitting reported preferences based on their true preferences and admission chances (which depend on capacities), introducing endogenous uncertainty. In the second stage, the clearinghouse computes the student-optimal stable matching based on schools' priorities and students' reported preferences. In strategic cases, endogenous reported preferences are utility-maximizing transformations of capacity decisions and exogenous true preferences; we handle uncertainty using sample average approximation(SAA). We develop behavior-based mathematical formulations and, due to problem complexity, propose Lagrangian- and local-search-based behavior-specific heuristics for near-optimal solutions. Our SAA-based approaches outperform the average scenario approach on students' matching preferences and admission outcomes, emphasizing the impact of stochastic preferences on capacity decisions. Student behavior notably influences capacity design, stressing the need to consider misreports.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RExBench: Can coding agents autonomously implement AI research extensions?</title>
<link>https://arxiv.org/abs/2506.22598</link>
<guid>https://arxiv.org/abs/2506.22598</guid>
<content:encoded><![CDATA[
arXiv:2506.22598v1 Announce Type: new 
Abstract: Agents based on Large Language Models (LLMs) have shown promise for performing sophisticated software engineering tasks autonomously. In addition, there has been progress towards developing agents that can perform parts of the research pipeline in machine learning and the natural sciences. We argue that research extension and its implementation is a critical capability for such systems, and introduce RExBench to support the evaluation of this capability. RExBench is a benchmark consisting of 12 realistic research experiment implementation tasks that aim to investigate research hypotheses that have not previously been implemented. Each task is set up as an extension to an existing research paper and codebase, accompanied by domain expert-written instructions. RExBench is robust to data contamination, and supports an automatic evaluation infrastructure that executes agent outputs to determine whether the success criteria are met. We use this benchmark to evaluate nine LLM agents implemented using three different frameworks: aider, Claude Code, and OpenHands. We find that all agents evaluated fail to autonomously implement the majority of the extensions. Although the success rate improves with additional human-written hints, the best performance under this setting remains below 40%. This indicates that current agents are still short of being able to handle realistic research extension tasks without substantial human guidance.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ludax: A GPU-Accelerated Domain Specific Language for Board Games</title>
<link>https://arxiv.org/abs/2506.22609</link>
<guid>https://arxiv.org/abs/2506.22609</guid>
<content:encoded><![CDATA[
arXiv:2506.22609v1 Announce Type: new 
Abstract: Games have long been used as benchmarks and testing environments for research in artificial intelligence. A key step in supporting this research was the development of game description languages: frameworks that compile domain-specific code into playable and simulatable game environments, allowing researchers to generalize their algorithms and approaches across multiple games without having to manually implement each one. More recently, progress in reinforcement learning (RL) has been largely driven by advances in hardware acceleration. Libraries like JAX allow practitioners to take full advantage of cutting-edge computing hardware, often speeding up training and testing by orders of magnitude. Here, we present a synthesis of these strands of research: a domain-specific language for board games which automatically compiles into hardware-accelerated code. Our framework, Ludax, combines the generality of game description languages with the speed of modern parallel processing hardware and is designed to fit neatly into existing deep learning pipelines. We envision Ludax as a tool to help accelerate games research generally, from RL to cognitive science, by enabling rapid simulation and providing a flexible representation scheme. We present a detailed breakdown of Ludax's description language and technical notes on the compilation process, along with speed benchmarking and a demonstration of training RL agents. The Ludax framework, along with implementations of existing board games, is open-source and freely available.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QoS-aware State-Augmented Learnable Algorithm for Wireless Coexistence Parameter Management</title>
<link>https://arxiv.org/abs/2506.22652</link>
<guid>https://arxiv.org/abs/2506.22652</guid>
<content:encoded><![CDATA[
arXiv:2506.22652v1 Announce Type: new 
Abstract: Efficient and fair coexistence in unlicensed spectrum is essential to support heterogeneous networks such as 5G NR-U and Wi-Fi, which often contend for shared wireless resources. We introduce a general framework for wireless Coexistence Parameter Management (CPM) based on state-augmented constrained reinforcement learning. We propose a novel algorithm, QaSAL-CPM, which incorporates state-augmentation by embedding the dual variables in the constrained optimization formulation directly into the agent's observation space. This method enables the agent to respond to constraint violations in real time while continuing to optimize a primary performance objective. Through extensive simulations of 5G NR-U and Wi-Fi coexistence scenarios, we show that QaSAL-CPM achieves reliable QoS compliance and improved policy robustness across various transmitter densities compared to previous approaches. The proposed framework offers a scalable and adaptive solution for real-time coexistence optimization in next-generation wireless networks.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>URSA: The Universal Research and Scientific Agent</title>
<link>https://arxiv.org/abs/2506.22653</link>
<guid>https://arxiv.org/abs/2506.22653</guid>
<content:encoded><![CDATA[
arXiv:2506.22653v1 Announce Type: new 
Abstract: Large language models (LLMs) have moved far beyond their initial form as simple chatbots, now carrying out complex reasoning, planning, writing, coding, and research tasks. These skills overlap significantly with those that human scientists use day-to-day to solve complex problems that drive the cutting edge of research. Using LLMs in "agentic" AI has the potential to revolutionize modern science and remove bottlenecks to progress. In this work, we present URSA, a scientific agent ecosystem for accelerating research tasks. URSA consists of a set of modular agents and tools, including coupling to advanced physics simulation codes, that can be combined to address scientific problems of varied complexity and impact. This work highlights the architecture of URSA, as well as examples that highlight the potential of the system.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-Guided Multi-Agent Framework for Automated Requirements Development: A Vision</title>
<link>https://arxiv.org/abs/2506.22656</link>
<guid>https://arxiv.org/abs/2506.22656</guid>
<content:encoded><![CDATA[
arXiv:2506.22656v1 Announce Type: new 
Abstract: This paper envisions a knowledge-guided multi-agent framework named KGMAF for automated requirements development. KGMAF aims to address gaps in current automation systems for SE, which prioritize code development and overlook the complexities of requirements tasks. KGMAF is composed of six specialized agents and an artifact pool to improve efficiency and accuracy. Specifically, KGMAF outlines the functionality, actions, and knowledge of each agent and provides the conceptual design of the artifact pool. Our case study highlights the potential of KGMAF in real-world scenarios. Finally, we outline several research opportunities for implementing and enhancing automated requirements development using multi-agent systems. We believe that KGMAF will play a pivotal role in shaping the future of automated requirements development in the era of LLMs.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General Autonomous Cybersecurity Defense: Learning Robust Policies for Dynamic Topologies and Diverse Attackers</title>
<link>https://arxiv.org/abs/2506.22706</link>
<guid>https://arxiv.org/abs/2506.22706</guid>
<content:encoded><![CDATA[
arXiv:2506.22706v1 Announce Type: new 
Abstract: In the face of evolving cyber threats such as malware, ransomware and phishing, autonomous cybersecurity defense (ACD) systems have become essential for real-time threat detection and response with optional human intervention. However, existing ACD systems rely on limiting assumptions, particularly the stationarity of the underlying network dynamics. In real-world scenarios, network topologies can change due to actions taken by attackers or defenders, system failures, or time evolution of networks, leading to failures in the adaptive capabilities of current defense agents. Moreover, many agents are trained on static environments, resulting in overfitting to specific topologies, which hampers their ability to generalize to out-of-distribution network topologies. This work addresses these challenges by exploring methods for developing agents to learn generalizable policies across dynamic network environments -- general ACD (GACD).
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairMarket-RL: LLM-Guided Fairness Shaping for Multi-Agent Reinforcement Learning in Peer-to-Peer Markets</title>
<link>https://arxiv.org/abs/2506.22708</link>
<guid>https://arxiv.org/abs/2506.22708</guid>
<content:encoded><![CDATA[
arXiv:2506.22708v1 Announce Type: new 
Abstract: Peer-to-peer (P2P) trading is increasingly recognized as a key mechanism for decentralized market regulation, yet existing approaches often lack robust frameworks to ensure fairness. This paper presents FairMarket-RL, a novel hybrid framework that combines Large Language Models (LLMs) with Reinforcement Learning (RL) to enable fairness-aware trading agents. In a simulated P2P microgrid with multiple sellers and buyers, the LLM acts as a real-time fairness critic, evaluating each trading episode using two metrics: Fairness-To-Buyer (FTB) and Fairness-Between-Sellers (FBS). These fairness scores are integrated into agent rewards through scheduled {\lambda}-coefficients, forming an adaptive LLM-guided reward shaping loop that replaces brittle, rule-based fairness constraints. Agents are trained using Independent Proximal Policy Optimization (IPPO) and achieve equitable outcomes, fulfilling over 90% of buyer demand, maintaining fair seller margins, and consistently reaching FTB and FBS scores above 0.80. The training process demonstrates that fairness feedback improves convergence, reduces buyer shortfalls, and narrows profit disparities between sellers. With its language-based critic, the framework scales naturally, and its extension to a large power distribution system with household prosumers illustrates its practical applicability. FairMarket-RL thus offers a scalable, equity-driven solution for autonomous trading in decentralized energy systems.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Questions as cognitive filters</title>
<link>https://arxiv.org/abs/2506.22735</link>
<guid>https://arxiv.org/abs/2506.22735</guid>
<content:encoded><![CDATA[
arXiv:2506.22735v1 Announce Type: new 
Abstract: In this paper, we develop a logico-algebraic framework for modeling decision-making through deliberation in multi-agent settings. The central concept in this framework is that of interrogative agendas, which represent the cognitive stances of agents regarding which features should be considered relevant in the final decision. We formalize an agent's interrogative agenda as an equivalence relation that identifies outcomes differing only in aspects the agent deems irrelevant. Moreover, we characterize the sublattices of the resulting lattice that correspond to relevant interrogative agendas for deliberation scenarios governed by different ``winning rules." We then introduce a two-sorted logico-algebraic structure-comprising the lattice of relevant interrogative agendas and the Boolean algebras of agent coalitions-to model the interaction between agents and agendas during deliberation. Finally, we discuss which interaction conditions can and cannot be defined within this framework.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trusted Routing for Blockchain-Enabled Low-Altitude Intelligent Networks</title>
<link>https://arxiv.org/abs/2506.22745</link>
<guid>https://arxiv.org/abs/2506.22745</guid>
<content:encoded><![CDATA[
arXiv:2506.22745v1 Announce Type: new 
Abstract: Due to the scalability and portability, the low-altitude intelligent networks (LAINs) are essential in various fields such as surveillance and disaster rescue. However, in LAINs, unmanned aerial vehicles (UAVs) are characterized by the distributed topology and high dynamic mobility, and vulnerable to security threats, which may degrade the routing performance for data transmission. Hence, how to ensure the routing stability and security of LAINs is a challenge. In this paper, we focus on the routing process in LAINs with multiple UAV clusters and propose the blockchain-enabled zero-trust architecture to manage the joining and exiting of UAVs. Furthermore, we formulate the routing problem to minimize the end-to-end (E2E) delay, which is an integer linear programming and intractable to solve. Therefore, considering the distribution of LAINs, we reformulate the routing problem into a decentralized partially observable Markov decision process. With the proposed soft hierarchical experience replay buffer, the multi-agent double deep Q-network based adaptive routing algorithm is designed. Finally, simulations are conducted and numerical results show that the total E2E delay of the proposed mechanism decreases by 22.38\% than the benchmark on average.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters</title>
<link>https://arxiv.org/abs/2506.22809</link>
<guid>https://arxiv.org/abs/2506.22809</guid>
<content:encoded><![CDATA[
arXiv:2506.22809v1 Announce Type: new 
Abstract: We propose BayesLoRA, a task-specific uncertainty quantification framework that integrates MC-Dropout into Low-Rank Adapters (LoRA). Unlike general-purpose transformer uncertainty methods, BayesLoRA provides guardrails tailored to downstream workflows, enabling agents to introspect and modulate behavior under uncertainty. We demonstrate mathematically and empirically that LoRA adapters exhibit amplified variance outside fine-tuning distributions, yielding reliable confidence estimates for agentic decision-making.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory as a Service (MaaS): Rethinking Contextual Memory as Service-Oriented Modules for Collaborative Agents</title>
<link>https://arxiv.org/abs/2506.22815</link>
<guid>https://arxiv.org/abs/2506.22815</guid>
<content:encoded><![CDATA[
arXiv:2506.22815v1 Announce Type: new 
Abstract: This position paper aims to rethink the role and design of memory in Large Language Model (LLM)-based agent systems. We observe that while current memory practices have begun to transcend the limitations of single interactions, they remain conceptually grounded in "bound memory" in terms of design concept-where memory is treated as local state attached to specific context or entities, forming "memory silos" that impede cross-entity collaboration. To overcome this architectural bottleneck, this paper proposes the timely design perspective of "Memory as a Service" (MaaS). MaaS advocates decoupling memory from its conventional role as an interaction byproduct and encapsulating it as a modular service that can be independently callable, dynamically composable, and finely governed. At its core, MaaS leverages the duality of memory-its inherently private nature and its potential for public service-to grant memory controlled, on-demand interoperability across entities. This paper introduces a two-dimensional design space defined by entity structure and service type, illustrating how MaaS aligns with current memory practices while naturally extending them to cross-entity collaborative scenarios. Finally, we outline an open research agenda spanning governance, security, and ethical ecosystems, and call upon the broader research community to explore this shift toward service-oriented memory for collaborative agents operating across entity boundaries.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Augmented Finetuning Matters in both RAG and Agent Based Dialog Systems</title>
<link>https://arxiv.org/abs/2506.22852</link>
<guid>https://arxiv.org/abs/2506.22852</guid>
<content:encoded><![CDATA[
arXiv:2506.22852v1 Announce Type: new 
Abstract: Large language models (LLMs) have recently been applied to dialog systems. Despite making progress, LLMs are prone to errors in knowledge-intensive scenarios. Recently, approaches based on retrieval augmented generation (RAG) and agent have emerged to improve the factual accuracy by enhancing the LLMs with knowledge retrieved from external knowledge bases (KBs). This is mostly implemented by prompting the LLMs with instructions, examples and the retrieved knowledge. However, LLMs may have difficulty using the retrieved knowledge effectively for response generation, because they are not well trained to do such generation for specific domains. To mitigate this problem, we propose to finetune the LLMs in the RAG-based and agent-based systems with domain-specific data, together with domain-specific external knowledge, which is called knowledge augmented finetuning (KAFT). We base our study on the MobileCS2 dataset, a real-life customer service dialog dataset that features intensive knowledge interactions, to systematically compare the prompting and KAFT techniques in the RAG-based and agent-based systems. Experiment results show that KAFT substantially surpasses prompting in both RAG and agent systems, particularly in terms of factual accuracy. To the best of our knowledge, this paper represents the first solid empirical work to investigate the KAFT idea.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues</title>
<link>https://arxiv.org/abs/2506.22853</link>
<guid>https://arxiv.org/abs/2506.22853</guid>
<content:encoded><![CDATA[
arXiv:2506.22853v1 Announce Type: new 
Abstract: Existing function-calling benchmarks focus on single-turn interactions. However, they overlook the complexity of real-world scenarios. To quantify how existing benchmarks address practical applications, we introduce DICE-SCORE, a metric that evaluates the dispersion of tool-related information such as function name and parameter values throughout the dialogue. Analyzing existing benchmarks through DICE-SCORE reveals notably low scores, highlighting the need for more realistic scenarios. To address this gap, we present DICE-BENCH, a framework that constructs practical function-calling datasets by synthesizing conversations through a tool graph that maintains dependencies across rounds and a multi-agent system with distinct personas to enhance dialogue naturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our experiments on 19 LLMs with DICE-BENCH show that significant advances are still required before such models can be deployed effectively in real-world settings. Our code and data are all publicly available: https://snuhcc.github.io/DICE-Bench/.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Momentum-based Accelerated Algorithm for Distributed Optimization under Sector-Bound Nonlinearity</title>
<link>https://arxiv.org/abs/2506.22855</link>
<guid>https://arxiv.org/abs/2506.22855</guid>
<content:encoded><![CDATA[
arXiv:2506.22855v1 Announce Type: new 
Abstract: Distributed optimization advances centralized machine learning methods by enabling parallel and decentralized learning processes over a network of computing nodes. This work provides an accelerated consensus-based distributed algorithm for locally non-convex optimization using the gradient-tracking technique. The proposed algorithm (i) improves the convergence rate by adding momentum towards the optimal state using the heavy-ball method, while (ii) addressing general sector-bound nonlinearities over the information-sharing network. The link nonlinearity includes any sign-preserving odd sector-bound mapping, for example, log-scale data quantization or clipping in practical applications. For admissible momentum and gradient-tracking parameters, using perturbation theory and eigen-spectrum analysis, we prove convergence even in the presence of sector-bound nonlinearity and for locally non-convex cost functions. Further, in contrast to most existing weight-stochastic algorithms, we adopt weight-balanced (WB) network design. This WB design and perturbation-based analysis allow to handle dynamic directed network of agents to address possible time-varying setups due to link failures or packet drops.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cooperation as Black Box: Conceptual Fluctuation and Diagnostic Tools for Misalignment in MAS</title>
<link>https://arxiv.org/abs/2506.22876</link>
<guid>https://arxiv.org/abs/2506.22876</guid>
<content:encoded><![CDATA[
arXiv:2506.22876v1 Announce Type: new 
Abstract: Misalignment in multi-agent systems (MAS) is often treated as a technical failure; yet many such failures originate upstream, during the conceptual design phase, where semantic ambiguity and normative projection take place. This paper identifies a foundational source of interpretive misalignment in MAS: the systemic conflation of cooperation and coordination, and the moral overreading that follows. Using the Rabbit-Duck illusion, we illustrate how perspective-dependent readings of agent behavior can create epistemic instability. To address this, we introduce the Misalignment Mosaic, a diagnostic framework for diagnosing meaning-level misalignment in MAS. It comprises four components: 1. Terminological Inconsistency, 2. Concept-to-Code Decay, 3. Morality as Cooperation, and 4. Interpretive Ambiguity. The Mosaic enables researchers to examine how misalignment arises not only through policy or reward structures but also through language, framing, and design assumptions. While this paper focuses on the specific ambiguity between coordination and cooperation, the Mosaic generalizes to other overloaded concepts in MAS, such as alignment, autonomy, and trust. Rather than define cooperation once and for all, we offer a framework to diagnose meaning itself as a source of misalignment.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CP-Guard: A Unified, Probability-Agnostic, and Adaptive Framework for Malicious Agent Detection and Defense in Multi-Agent Embodied Perception Systems</title>
<link>https://arxiv.org/abs/2506.22890</link>
<guid>https://arxiv.org/abs/2506.22890</guid>
<content:encoded><![CDATA[
arXiv:2506.22890v1 Announce Type: new 
Abstract: Collaborative Perception (CP) has been shown to be a promising technique for multi-agent autonomous driving and multi-agent robotic systems, where multiple agents share their perception information to enhance the overall perception performance and expand the perception range. However, in CP, an ego agent needs to receive messages from its collaborators, which makes it vulnerable to attacks from malicious agents. To address this critical issue, we propose a unified, probability-agnostic, and adaptive framework, namely, CP-Guard, which is a tailored defense mechanism for CP deployed by each agent to accurately detect and eliminate malicious agents in its collaboration network. Our key idea is to enable CP to reach a consensus rather than a conflict against an ego agent's perception results. Based on this idea, we first develop a probability-agnostic sample consensus (PASAC) method to effectively sample a subset of the collaborators and verify the consensus without prior probabilities of malicious agents. Furthermore, we define collaborative consistency loss (CCLoss) for object detection task and bird's eye view (BEV) segmentation task to capture the discrepancy between an ego agent and its collaborators, which is used as a verification criterion for consensus. In addition, we propose online adaptive threshold via dual sliding windows to dynamically adjust the threshold for consensus verification and ensure the reliability of the systems in dynamic environments. Finally, we conduct extensive experiments and demonstrate the effectiveness of our framework. Code will be released at https://github.com/CP-Security/CP-Guard
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe Reinforcement Learning with a Predictive Safety Filter for Motion Planning and Control: A Drifting Vehicle Example</title>
<link>https://arxiv.org/abs/2506.22894</link>
<guid>https://arxiv.org/abs/2506.22894</guid>
<content:encoded><![CDATA[
arXiv:2506.22894v1 Announce Type: new 
Abstract: Autonomous drifting is a complex and crucial maneuver for safety-critical scenarios like slippery roads and emergency collision avoidance, requiring precise motion planning and control. Traditional motion planning methods often struggle with the high instability and unpredictability of drifting, particularly when operating at high speeds. Recent learning-based approaches have attempted to tackle this issue but often rely on expert knowledge or have limited exploration capabilities. Additionally, they do not effectively address safety concerns during learning and deployment. To overcome these limitations, we propose a novel Safe Reinforcement Learning (RL)-based motion planner for autonomous drifting. Our approach integrates an RL agent with model-based drift dynamics to determine desired drift motion states, while incorporating a Predictive Safety Filter (PSF) that adjusts the agent's actions online to prevent unsafe states. This ensures safe and efficient learning, and stable drift operation. We validate the effectiveness of our method through simulations on a Matlab-Carsim platform, demonstrating significant improvements in drift performance, reduced tracking errors, and computational efficiency compared to traditional methods. This strategy promises to extend the capabilities of autonomous vehicles in safety-critical maneuvers.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GamerAstra: Enhancing Video Game Accessibility for Blind and Low-Vision Players through a Multi-Agent AI Framework</title>
<link>https://arxiv.org/abs/2506.22937</link>
<guid>https://arxiv.org/abs/2506.22937</guid>
<content:encoded><![CDATA[
arXiv:2506.22937v1 Announce Type: new 
Abstract: Blind and low-vision (BLV) players encounter critical challenges in engaging with video games due to the inaccessibility of visual elements, difficulties in navigating interfaces, and limitations in sending interaction input. Moreover, the development of specialized accessibility features typically requires substantial programming effort and is often implemented on a game-by-game basis. To address these challenges, we introduce \textit{GamerAstra}, a generalized accessibility framework that leverages a multi-agent design to facilitate access to video games for BLV players. It integrates multi-modal techniques including large language models and vision-language models, enabling interaction with games lacking native accessibility support. The framework further incorporates customizable assistance granularities to support varying degrees of visual impairment and enhances interface navigation through multiple input modalities. The evaluation through technical assessments and user studies indicate that \textit{GamerAstra} effectively enhances playability and delivers a more immersive gaming experience for BLV players. These findings also underscore potential avenues for advancing intelligent accessibility frameworks in the gaming domain.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models</title>
<link>https://arxiv.org/abs/2506.22957</link>
<guid>https://arxiv.org/abs/2506.22957</guid>
<content:encoded><![CDATA[
arXiv:2506.22957v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly integrated into multi-agent and human-AI systems, understanding their awareness of both self-context and conversational partners is essential for ensuring reliable performance and robust safety. While prior work has extensively studied situational awareness which refers to an LLM's ability to recognize its operating phase and constraints, it has largely overlooked the complementary capacity to identify and adapt to the identity and characteristics of a dialogue partner. In this paper, we formalize this latter capability as interlocutor awareness and present the first systematic evaluation of its emergence in contemporary LLMs. We examine interlocutor inference across three dimensions-reasoning patterns, linguistic style, and alignment preferences-and show that LLMs reliably identify same-family peers and certain prominent model families, such as GPT and Claude. To demonstrate its practical significance, we develop three case studies in which interlocutor awareness both enhances multi-LLM collaboration through prompt adaptation and introduces new alignment and safety vulnerabilities, including reward-hacking behaviors and increased jailbreak susceptibility. Our findings highlight the dual promise and peril of identity-sensitive behavior in LLMs, underscoring the need for further understanding of interlocutor awareness and new safeguards in multi-agent deployments. Our code is open-sourced at https://github.com/younwoochoi/InterlocutorAwarenessLLM.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resilient-Native and Intelligent Next-Generation Wireless Systems: Key Enablers, Foundations, and Applications</title>
<link>https://arxiv.org/abs/2506.22991</link>
<guid>https://arxiv.org/abs/2506.22991</guid>
<content:encoded><![CDATA[
arXiv:2506.22991v1 Announce Type: new 
Abstract: Just like power, water, and transportation systems, wireless networks are a crucial societal infrastructure. As natural and human-induced disruptions continue to grow, wireless networks must be resilient. This requires them to withstand and recover from unexpected adverse conditions, shocks, unmodeled disturbances and cascading failures. Unlike robustness and reliability, resilience is based on the understanding that disruptions will inevitably happen. Resilience, as elasticity, focuses on the ability to bounce back to favorable states, while resilience as plasticity involves agents and networks that can flexibly expand their states and hypotheses through real-time adaptation and reconfiguration. This situational awareness and active preparedness, adapting world models and counterfactually reasoning about potential system failures and the best responses, is a core aspect of resilience. This article will first disambiguate resilience from reliability and robustness, before delving into key mathematical foundations of resilience grounded in abstraction, compositionality and emergence. Subsequently, we focus our attention on a plethora of techniques and methodologies pertaining to the unique characteristics of resilience, as well as their applications through a comprehensive set of use cases. Ultimately, the goal of this paper is to establish a unified foundation for understanding, modeling, and engineering resilience in wireless communication systems, while laying a roadmap for the next-generation of resilient-native and intelligent wireless systems.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Reinforcement Learning Approach for Optimal Control in Microgrids</title>
<link>https://arxiv.org/abs/2506.22995</link>
<guid>https://arxiv.org/abs/2506.22995</guid>
<content:encoded><![CDATA[
arXiv:2506.22995v1 Announce Type: new 
Abstract: The increasing integration of renewable energy sources (RESs) is transforming traditional power grid networks, which require new approaches for managing decentralized energy production and consumption. Microgrids (MGs) provide a promising solution by enabling localized control over energy generation, storage, and distribution. This paper presents a novel reinforcement learning (RL)-based methodology for optimizing microgrid energy management. Specifically, we propose an RL agent that learns optimal energy trading and storage policies by leveraging historical data on energy production, consumption, and market prices. A digital twin (DT) is used to simulate the energy storage system dynamics, incorporating degradation factors to ensure a realistic emulation of the analysed setting. Our approach is validated through an experimental campaign using real-world data from a power grid located in the Italian territory. The results indicate that the proposed RL-based strategy outperforms rule-based methods and existing RL benchmarks, offering a robust solution for intelligent microgrid management.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scenario-Based Hierarchical Reinforcement Learning for Automated Driving Decision Making</title>
<link>https://arxiv.org/abs/2506.23023</link>
<guid>https://arxiv.org/abs/2506.23023</guid>
<content:encoded><![CDATA[
arXiv:2506.23023v1 Announce Type: new 
Abstract: Developing decision-making algorithms for highly automated driving systems remains challenging, since these systems have to operate safely in an open and complex environments. Reinforcement Learning (RL) approaches can learn comprehensive decision policies directly from experience and already show promising results in simple driving tasks. However, current approaches fail to achieve generalizability for more complex driving tasks and lack learning efficiency. Therefore, we present Scenario-based Automated Driving Reinforcement Learning (SAD-RL), the first framework that integrates Reinforcement Learning (RL) of hierarchical policy in a scenario-based environment. A high-level policy selects maneuver templates that are evaluated and executed by a low-level control logic. The scenario-based environment allows to control the training experience for the agent and to explicitly introduce challenging, but rate situations into the training process. Our experiments show that an agent trained using the SAD-RL framework can achieve safe behaviour in easy as well as challenging situations efficiently. Our ablation studies confirmed that both HRL and scenario diversity are essential for achieving these results.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A "Good" Regulator May Provide a World Model for Intelligent Systems</title>
<link>https://arxiv.org/abs/2506.23032</link>
<guid>https://arxiv.org/abs/2506.23032</guid>
<content:encoded><![CDATA[
arXiv:2506.23032v1 Announce Type: new 
Abstract: One classic idea from the cybernetics literature is the Every Good Regulator Theorem (EGRT). The EGRT provides a means to identify good regulation, or the conditions under which an agent (regulator) can match the dynamical behavior of a system. We reevaluate and recast the EGRT in a modern context to provide insight into how intelligent autonomous learning systems might utilize a compressed global representation (world model). One-to-one mappings between a regulator (R) and the corresponding system (S) provide a reduced representation that preserves useful variety to match all possible outcomes of a system. Secondarily, we question the role of purpose or autonomy in this process, demonstrating how physical paradigms such as temporal criticality, non-normal denoising, and alternating procedural acquisition can recast behavior as statistical mechanics and yield regulatory relationships. These diverse physical systems challenge the notion of tightly-coupled good regulation when applied to non-uniform and out-of-distribution phenomena. Modern definitions of intelligence are found to be inadequate, and can be improved upon by viewing intelligence as embodied non-purposeful good regulation. Overall, we aim to recast the EGRT as a tool for contemporary Artificial Intelligence (AI) architectures by considering the role of good regulation in the implementation of world models.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fragile, Robust, and Antifragile: A Perspective from Parameter Responses in Reinforcement Learning Under Stress</title>
<link>https://arxiv.org/abs/2506.23036</link>
<guid>https://arxiv.org/abs/2506.23036</guid>
<content:encoded><![CDATA[
arXiv:2506.23036v1 Announce Type: new 
Abstract: This paper explores Reinforcement learning (RL) policy robustness by systematically analyzing network parameters under internal and external stresses. Inspired by synaptic plasticity in neuroscience, synaptic filtering introduces internal stress by selectively perturbing parameters, while adversarial attacks apply external stress through modified agent observations. This dual approach enables the classification of parameters as fragile, robust, or antifragile, based on their influence on policy performance in clean and adversarial settings. Parameter scores are defined to quantify these characteristics, and the framework is validated on PPO-trained agents in Mujoco continuous control environments. The results highlight the presence of antifragile parameters that enhance policy performance under stress, demonstrating the potential of targeted filtering techniques to improve RL policy adaptability. These insights provide a foundation for future advancements in the design of robust and antifragile RL systems.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions</title>
<link>https://arxiv.org/abs/2506.23046</link>
<guid>https://arxiv.org/abs/2506.23046</guid>
<content:encoded><![CDATA[
arXiv:2506.23046v1 Announce Type: new 
Abstract: Humans continuously infer the states, goals, and behaviors of others by perceiving their surroundings in dynamic, real-world social interactions. However, most Theory of Mind (ToM) benchmarks only evaluate static, text-based scenarios, which have a significant gap compared to real interactions. We propose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in embodied multi-agent complex social interactions. This benchmark is based on rich multimodal interaction data generated by the interaction environment SoMi, covering diverse crafting goals and social relationships. Our framework supports multi-level evaluation: (1) first-person evaluation provides multimodal (visual, dialogue, action, etc.) input from a first-person perspective during a task for real-time state inference, (2) third-person evaluation provides complete third-person perspective video and text records after a task for goal and behavior inference. This evaluation method allows for a more comprehensive examination of a model's ToM capabilities from both the subjective immediate experience and the objective global observation. We constructed a challenging dataset containing 35 third-person perspective videos, 363 first-person perspective images, and 1225 expert-annotated multiple-choice questions (three options). On this dataset, we systematically evaluated the performance of human subjects and several state-of-the-art large vision-language models (LVLMs). The results show that LVLMs perform significantly worse than humans on SoMi-ToM: the average accuracy gap between humans and models is 40.1% in first-person evaluation and 26.4% in third-person evaluation. This indicates that future LVLMs need to further improve their ToM capabilities in embodied, complex social interactions.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks</title>
<link>https://arxiv.org/abs/2506.23049</link>
<guid>https://arxiv.org/abs/2506.23049</guid>
<content:encoded><![CDATA[
arXiv:2506.23049v1 Announce Type: new 
Abstract: Despite advances in language and speech technologies, no open-source system enables full speech-to-speech, multi-turn dialogue with integrated tool use and agentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and Automated Tool Use), the first open-source, speech-native assistant capable of completing complex, goal-driven tasks through dynamic tool invocation and multi-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a cascaded pipeline and supports tools such as calendar booking, contact lookup, web search, and email. Its modular design allows easy integration of new tools using natural language prompts and action classes. On VoiceBench, AURA scores 92.75% on OpenBookQA-outperforming all open-weight systems and nearing GPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems. Human evaluation shows 90% task success on complex, multi-turn speech tasks.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curious Causality-Seeking Agents Learn Meta Causal World</title>
<link>https://arxiv.org/abs/2506.23068</link>
<guid>https://arxiv.org/abs/2506.23068</guid>
<content:encoded><![CDATA[
arXiv:2506.23068v1 Announce Type: new 
Abstract: When building a world model, a common assumption is that the environment has a single, unchanging underlying causal rule, like applying Newton's laws to every situation. In reality, what appears as a drifting causal mechanism is often the manifestation of a fixed underlying mechanism seen through a narrow observational window. This brings about a problem that, when building a world model, even subtle shifts in policy or environment states can alter the very observed causal mechanisms. In this work, we introduce the \textbf{Meta-Causal Graph} as world models, a minimal unified representation that efficiently encodes the transformation rules governing how causal structures shift across different latent world states. A single Meta-Causal Graph is composed of multiple causal subgraphs, each triggered by meta state, which is in the latent state space. Building on this representation, we introduce a \textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta states that trigger each subgraph, (2) discover the corresponding causal relationships by agent curiosity-driven intervention policy, and (3) iteratively refine the Meta-Causal Graph through ongoing curiosity-driven exploration and agent experiences. Experiments on both synthetic tasks and a challenging robot arm manipulation task demonstrate that our method robustly captures shifts in causal dynamics and generalizes effectively to previously unseen contexts.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Motion Skills with Adaptive Assistive Curriculum Force in Humanoid Robots</title>
<link>https://arxiv.org/abs/2506.23125</link>
<guid>https://arxiv.org/abs/2506.23125</guid>
<content:encoded><![CDATA[
arXiv:2506.23125v1 Announce Type: new 
Abstract: Learning policies for complex humanoid tasks remains both challenging and compelling. Inspired by how infants and athletes rely on external support--such as parental walkers or coach-applied guidance--to acquire skills like walking, dancing, and performing acrobatic flips, we propose A2CF: Adaptive Assistive Curriculum Force for humanoid motion learning. A2CF trains a dual-agent system, in which a dedicated assistive force agent applies state-dependent forces to guide the robot through difficult initial motions and gradually reduces assistance as the robot's proficiency improves. Across three benchmarks--bipedal walking, choreographed dancing, and backflip--A2CF achieves convergence 30% faster than baseline methods, lowers failure rates by over 40%, and ultimately produces robust, support-free policies. Real-world experiments further demonstrate that adaptively applied assistive forces significantly accelerate the acquisition of complex skills in high-dimensional robotic control.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Deep Search over Heterogeneous Enterprise Data</title>
<link>https://arxiv.org/abs/2506.23139</link>
<guid>https://arxiv.org/abs/2506.23139</guid>
<content:encoded><![CDATA[
arXiv:2506.23139v1 Announce Type: new 
Abstract: We present a new benchmark for evaluating Deep Search--a realistic and complex form of retrieval-augmented generation (RAG) that requires source-aware, multi-hop reasoning over diverse, sparsed, but related sources. These include documents, meeting transcripts, Slack messages, GitHub, and URLs, which vary in structure and often contain human-to-human interactions. We build it using a synthetic data pipeline that simulates business workflows across product planning, development, and support stages, generating interconnected content with realistic noise and multi-hop questions with guaranteed ground-truth answers. We release our benchmark with both answerable and unanswerable queries, and retrieval pool of 39,190 enterprise artifacts, enabling fine-grained evaluation of long-context LLM and RAG systems. Our experiments reveal that even the best-performing agentic RAG methods achieve an average performance score of 32.96 on our benchmark. With further analysis, we highlight retrieval as the main bottleneck: existing methods struggle to conduct deep searches and retrieve all necessary evidence. Consequently, they often reason over partial context, leading to significant performance degradation.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mode Collapse Happens: Evaluating Critical Interactions in Joint Trajectory Prediction Models</title>
<link>https://arxiv.org/abs/2506.23164</link>
<guid>https://arxiv.org/abs/2506.23164</guid>
<content:encoded><![CDATA[
arXiv:2506.23164v1 Announce Type: new 
Abstract: Autonomous Vehicle decisions rely on multimodal prediction models that account for multiple route options and the inherent uncertainty in human behavior. However, models can suffer from mode collapse, where only the most likely mode is predicted, posing significant safety risks. While existing methods employ various strategies to generate diverse predictions, they often overlook the diversity in interaction modes among agents. Additionally, traditional metrics for evaluating prediction models are dataset-dependent and do not evaluate inter-agent interactions quantitatively. To our knowledge, none of the existing metrics explicitly evaluates mode collapse. In this paper, we propose a novel evaluation framework that assesses mode collapse in joint trajectory predictions, focusing on safety-critical interactions. We introduce metrics for mode collapse, mode correctness, and coverage, emphasizing the sequential dimension of predictions. By testing four multi-agent trajectory prediction models, we demonstrate that mode collapse indeed happens. When looking at the sequential dimension, although prediction accuracy improves closer to interaction events, there are still cases where the models are unable to predict the correct interaction mode, even just before the interaction mode becomes inevitable. We hope that our framework can help researchers gain new insights and advance the development of more consistent and accurate prediction models, thus enhancing the safety of autonomous driving systems.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows</title>
<link>https://arxiv.org/abs/2506.23260</link>
<guid>https://arxiv.org/abs/2506.23260</guid>
<content:encoded><![CDATA[
arXiv:2506.23260v1 Announce Type: new 
Abstract: Autonomous AI agents powered by large language models (LLMs) with structured function-calling interfaces have dramatically expanded capabilities for real-time data retrieval, complex computation, and multi-step orchestration. Yet, the explosive proliferation of plugins, connectors, and inter-agent protocols has outpaced discovery mechanisms and security practices, resulting in brittle integrations vulnerable to diverse threats. In this survey, we introduce the first unified, end-to-end threat model for LLM-agent ecosystems, spanning host-to-tool and agent-to-agent communications, formalize adversary capabilities and attacker objectives, and catalog over thirty attack techniques. Specifically, we organized the threat model into four domains: Input Manipulation (e.g., prompt injections, long-context hijacks, multimodal adversarial inputs), Model Compromise (e.g., prompt- and parameter-level backdoors, composite and encrypted multi-backdoors, poisoning strategies), System and Privacy Attacks (e.g., speculative side-channels, membership inference, retrieval poisoning, social-engineering simulations), and Protocol Vulnerabilities (e.g., exploits in Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent Network Protocol (ANP), and Agent-to-Agent (A2A) protocol). For each category, we review representative scenarios, assess real-world feasibility, and evaluate existing defenses. Building on our threat taxonomy, we identify key open challenges and future research directions, such as securing MCP deployments through dynamic trust management and cryptographic provenance tracking; designing and hardening Agentic Web Interfaces; and achieving resilience in multi-agent and federated environments. Our work provides a comprehensive reference to guide the design of robust defense mechanisms and establish best practices for resilient LLM-agent workflows.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis</title>
<link>https://arxiv.org/abs/2506.23273</link>
<guid>https://arxiv.org/abs/2506.23273</guid>
<content:encoded><![CDATA[
arXiv:2506.23273v1 Announce Type: new 
Abstract: Despite the advancements of large language models, text2sql still faces many challenges, particularly with complex and domain-specific queries. In finance, database designs and financial reporting layouts vary widely between financial entities and countries, making text2sql even more challenging. We present FinStat2SQL, a lightweight text2sql pipeline enabling natural language queries over financial statements. Tailored to local standards like VAS, it combines large and small language models in a multi-agent setup for entity extraction, SQL generation, and self-correction. We build a domain-specific database and evaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves 61.33\% accuracy with sub-4-second response times on consumer hardware, outperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient solution for financial analysis, making AI-powered querying accessible to Vietnamese enterprises.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games</title>
<link>https://arxiv.org/abs/2506.23276</link>
<guid>https://arxiv.org/abs/2506.23276</guid>
<content:encoded><![CDATA[
arXiv:2506.23276v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly deployed as autonomous agents, understanding their cooperation and social mechanisms is becoming increasingly important. In particular, how LLMs balance self-interest and collective well-being is a critical challenge for ensuring alignment, robustness, and safe deployment. In this paper, we examine the challenge of costly sanctioning in multi-agent LLM systems, where an agent must decide whether to invest its own resources to incentivize cooperation or penalize defection. To study this, we adapt a public goods game with institutional choice from behavioral economics, allowing us to observe how different LLMs navigate social dilemmas over repeated interactions. Our analysis reveals four distinct behavioral patterns among models: some consistently establish and sustain high levels of cooperation, others fluctuate between engagement and disengagement, some gradually decline in cooperative behavior over time, and others rigidly follow fixed strategies regardless of outcomes. Surprisingly, we find that reasoning LLMs, such as the o1 series, struggle significantly with cooperation, whereas some traditional LLMs consistently achieve high levels of cooperation. These findings suggest that the current approach to improving LLMs, which focuses on enhancing their reasoning capabilities, does not necessarily lead to cooperation, providing valuable insights for deploying LLM agents in environments that require sustained collaboration. Our code is available at https://github.com/davidguzmanp/SanctSim
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GATSim: Urban Mobility Simulation with Generative Agents</title>
<link>https://arxiv.org/abs/2506.23306</link>
<guid>https://arxiv.org/abs/2506.23306</guid>
<content:encoded><![CDATA[
arXiv:2506.23306v1 Announce Type: new 
Abstract: Traditional agent-based urban mobility simulations rely on rigid rule-based systems that fail to capture the complexity, adaptability, and behavioral diversity characteristic of human travel decision-making. Recent advances in large language models and AI agent technology offer opportunities to create agents with reasoning capabilities, persistent memory, and adaptive learning mechanisms. We propose GATSim (Generative-Agent Transport Simulation), a novel framework that leverages these advances to create generative agents with rich behavioral characteristics for urban mobility simulation. Unlike conventional approaches, GATSim agents possess diverse socioeconomic attributes, individual lifestyles, and evolving preferences that shape their mobility decisions through psychologically-informed memory systems, tool usage capabilities, and lifelong learning mechanisms. The main contributions of this study include: (1) a comprehensive architecture combining an urban mobility foundation model with agent cognitive systems and transport simulation environment, (2) a fully functional prototype implementation, and (3) systematic validation demonstrating that generative agents produce believable travel behaviors. Through designed reflection processes, generative agents in this study can transform specific travel experiences into generalized insights, enabling realistic behavioral adaptation over time with specialized mechanisms for activity planning and real-time reactive behaviors tailored to urban mobility contexts. Experiments show that generative agents perform competitively with human annotators in mobility scenarios while naturally producing macroscopic traffic evolution patterns. The code for the prototype system is shared at https://github.com/qiliuchn/gatsim.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfGen: Scenario Generation as Next Token Group Prediction</title>
<link>https://arxiv.org/abs/2506.23316</link>
<guid>https://arxiv.org/abs/2506.23316</guid>
<content:encoded><![CDATA[
arXiv:2506.23316v1 Announce Type: new 
Abstract: Realistic and interactive traffic simulation is essential for training and evaluating autonomous driving systems. However, most existing data-driven simulation methods rely on static initialization or log-replay data, limiting their ability to model dynamic, long-horizon scenarios with evolving agent populations. We propose InfGen, a scenario generation framework that outputs agent states and trajectories in an autoregressive manner. InfGen represents the entire scene as a sequence of tokens, including traffic light signals, agent states, and motion vectors, and uses a transformer model to simulate traffic over time. This design enables InfGen to continuously insert new agents into traffic, supporting infinite scene generation. Experiments demonstrate that InfGen produces realistic, diverse, and adaptive traffic behaviors. Furthermore, reinforcement learning policies trained in InfGen-generated scenarios achieve superior robustness and generalization, validating its utility as a high-fidelity simulation environment for autonomous driving. More information is available at https://metadriverse.github.io/infgen/.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering</title>
<link>https://arxiv.org/abs/2506.23329</link>
<guid>https://arxiv.org/abs/2506.23329</guid>
<content:encoded><![CDATA[
arXiv:2506.23329v1 Announce Type: new 
Abstract: Vision-language models (VLMs) excel at descriptive tasks, but whether they truly understand scenes from visual observations remains uncertain. We introduce IR3D-Bench, a benchmark challenging VLMs to demonstrate understanding through active creation rather than passive recognition. Grounded in the analysis-by-synthesis paradigm, IR3D-Bench tasks Vision-Language Agents (VLAs) with actively using programming and rendering tools to recreate the underlying 3D structure of an input image, achieving agentic inverse rendering through tool use. This "understanding-by-creating" approach probes the tool-using generative capacity of VLAs, moving beyond the descriptive or conversational capacity measured by traditional scene understanding benchmarks. We provide a comprehensive suite of metrics to evaluate geometric accuracy, spatial relations, appearance attributes, and overall plausibility. Initial experiments on agentic inverse rendering powered by various state-of-the-art VLMs highlight current limitations, particularly in visual precision rather than basic tool usage. IR3D-Bench, including data and evaluation protocols, is released to facilitate systematic study and development of tool-using VLAs towards genuine scene understanding by creating.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ATGen: A Framework for Active Text Generation</title>
<link>https://arxiv.org/abs/2506.23342</link>
<guid>https://arxiv.org/abs/2506.23342</guid>
<content:encoded><![CDATA[
arXiv:2506.23342v1 Announce Type: new 
Abstract: Active learning (AL) has demonstrated remarkable potential in reducing the annotation effort required for training machine learning models. However, despite the surging popularity of natural language generation (NLG) tasks in recent years, the application of AL to NLG has been limited. In this paper, we introduce Active Text Generation (ATGen) - a comprehensive framework that bridges AL with text generation tasks, enabling the application of state-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered annotation in NLG tasks using both human annotators and automatic annotation agents based on large language models (LLMs). The framework supports LLMs deployed as services, such as ChatGPT and Claude, or operated on-premises. Furthermore, ATGen provides a unified platform for smooth implementation and benchmarking of novel AL strategies tailored to NLG tasks. Finally, we present evaluation results for state-of-the-art AL strategies across diverse settings and multiple text generation tasks. We show that ATGen reduces both the effort of human annotators and costs associated with API calls to LLM-based annotation agents. The code of the framework is available on GitHub under the MIT license. The video presentation is available at http://atgen-video.nlpresearch.group
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLMs Dream of Discrete Algorithms?</title>
<link>https://arxiv.org/abs/2506.23408</link>
<guid>https://arxiv.org/abs/2506.23408</guid>
<content:encoded><![CDATA[
arXiv:2506.23408v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have rapidly transformed the landscape of artificial intelligence, enabling natural language interfaces and dynamic orchestration of software components. However, their reliance on probabilistic inference limits their effectiveness in domains requiring strict logical reasoning, discrete decision-making, and robust interpretability. This paper investigates these limitations and proposes a neurosymbolic approach that augments LLMs with logic-based reasoning modules, particularly leveraging Prolog predicates and composable toolsets. By integrating first-order logic and explicit rule systems, our framework enables LLMs to decompose complex queries into verifiable sub-tasks, orchestrate reliable solutions, and mitigate common failure modes such as hallucination and incorrect step decomposition. We demonstrate the practical benefits of this hybrid architecture through experiments on the DABStep benchmark, showing improved precision, coverage, and system documentation in multi-step reasoning tasks. Our results indicate that combining LLMs with modular logic reasoning restores engineering rigor, enhances system reliability, and offers a scalable path toward trustworthy, interpretable AI agents across complex domains.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accessible Data Access and Analysis by People who are Blind or Have Low Vision</title>
<link>https://arxiv.org/abs/2506.23443</link>
<guid>https://arxiv.org/abs/2506.23443</guid>
<content:encoded><![CDATA[
arXiv:2506.23443v1 Announce Type: new 
Abstract: Our work aims to develop new assistive technologies that enable blind or low vision (BLV) people to explore and analyze data readily. At present, barriers exist for BLV people to explore and analyze data, restricting access to government, health and personal data, and limiting employment opportunities. This work explores the co-design and development of an innovative system to support data access, with a focus on the use of refreshable tactile displays (RTDs) and conversational agents. The envisaged system will use a combination of tactile graphics and speech to communicate with BLV users, and proactively assist with data analysis tasks. As well as addressing significant equity gaps, our work expects to produce innovations in assistive technology, multimodal interfaces, dialogue systems, and natural language understanding and generation.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments</title>
<link>https://arxiv.org/abs/2506.23468</link>
<guid>https://arxiv.org/abs/2506.23468</guid>
<content:encoded><![CDATA[
arXiv:2506.23468v1 Announce Type: new 
Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires agents to execute sequential navigation actions in complex environments guided by natural language instructions. Current approaches often struggle with generalizing to novel environments and adapting to ongoing changes during navigation. Inspired by human cognition, we present NavMorph, a self-evolving world model framework that enhances environmental understanding and decision-making in VLN-CE tasks. NavMorph employs compact latent representations to model environmental dynamics, equipping agents with foresight for adaptive planning and policy refinement. By integrating a novel Contextual Evolution Memory, NavMorph leverages scene-contextual information to support effective navigation while maintaining online adaptability. Extensive experiments demonstrate that our method achieves notable performance improvements on popular VLN-CE benchmarks. Code is available at \href{https://github.com/Feliciaxyao/NavMorph}{this https URL}.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent</title>
<link>https://arxiv.org/abs/2506.23485</link>
<guid>https://arxiv.org/abs/2506.23485</guid>
<content:encoded><![CDATA[
arXiv:2506.23485v1 Announce Type: new 
Abstract: Interactive recommendation is a typical information-seeking task that allows users to interactively express their needs through natural language and obtain personalized recommendations. Large language model-powered (LLM-powered) agents have become a new paradigm in interactive recommendations, effectively capturing users' real-time needs and enhancing personalized experiences. However, due to limited planning and generalization capabilities, existing formulations of LLM-powered interactive recommender agents struggle to effectively address diverse and complex user intents, such as intuitive, unrefined, or occasionally ambiguous requests. To tackle this challenge, we propose a novel thought-augmented interactive recommender agent system (TAIRA) that addresses complex user intents through distilled thought patterns. Specifically, TAIRA is designed as an LLM-powered multi-agent system featuring a manager agent that orchestrates recommendation tasks by decomposing user needs and planning subtasks, with its planning capacity strengthened through Thought Pattern Distillation (TPD), a thought-augmentation method that extracts high-level thoughts from the agent's and human experts' experiences. Moreover, we designed a set of user simulation schemes to generate personalized queries of different difficulties and evaluate the recommendations based on specific datasets. Through comprehensive experiments conducted across multiple datasets, TAIRA exhibits significantly enhanced performance compared to existing methods. Notably, TAIRA shows a greater advantage on more challenging tasks while generalizing effectively on novel tasks, further validating its superiority in managing complex user intents within interactive recommendation systems. The code is publicly available at:https://github.com/Alcein/TAIRA.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CooT: Learning to Coordinate In-Context with Coordination Transformers</title>
<link>https://arxiv.org/abs/2506.23549</link>
<guid>https://arxiv.org/abs/2506.23549</guid>
<content:encoded><![CDATA[
arXiv:2506.23549v1 Announce Type: new 
Abstract: Effective coordination among artificial agents in dynamic and uncertain environments remains a significant challenge in multi-agent systems. Existing approaches, such as self-play and population-based methods, either generalize poorly to unseen partners or require extensive training. To overcome these limitations, we propose Coordination Transformers (CooT), a novel in-context coordination framework that uses recent interaction histories to adapt to unseen partners rapidly. Unlike previous approaches that primarily aim to increase the diversity of training partners, CooT explicitly focuses on adapting to new partner behaviors by predicting actions aligned with observed partner interactions. Trained on interaction trajectories collected from diverse pairs of agents with complementary behaviors, CooT quickly learns effective coordination strategies without explicit supervision or fine-tuning. Evaluations on the Overcooked benchmark demonstrate that CooT significantly outperforms baseline methods in coordination tasks involving previously unseen partners. Human evaluations further confirm CooT as the most effective collaborative partner, while extensive ablations highlight its robustness, flexibility, and sensitivity to context in multi-agent scenarios.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models</title>
<link>https://arxiv.org/abs/2506.23576</link>
<guid>https://arxiv.org/abs/2506.23576</guid>
<content:encoded><![CDATA[
arXiv:2506.23576v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have raised concerns about jailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper investigates the use of multi-agent LLM systems as a defence against such attacks. We evaluate three jailbreaking strategies, including the original AutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the AutoDefense framework, we compare single-agent setups with two- and three-agent configurations. Our results show that multi-agent systems enhance resistance to jailbreaks, especially by reducing false negatives. However, its effectiveness varies by attack type, and it introduces trade-offs such as increased false positives and computational overhead. These findings point to the limitations of current automated defences and suggest directions for improving alignment robustness in future LLM systems.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Simulation of Human Personality-Driven Susceptibility to Misinformation with LLMs</title>
<link>https://arxiv.org/abs/2506.23610</link>
<guid>https://arxiv.org/abs/2506.23610</guid>
<content:encoded><![CDATA[
arXiv:2506.23610v1 Announce Type: new 
Abstract: Large language models (LLMs) make it possible to generate synthetic behavioural data at scale, offering an ethical and low-cost alternative to human experiments. Whether such data can faithfully capture psychological differences driven by personality traits, however, remains an open question. We evaluate the capacity of LLM agents, conditioned on Big-Five profiles, to reproduce personality-based variation in susceptibility to misinformation, focusing on news discernment, the ability to judge true headlines as true and false headlines as false. Leveraging published datasets in which human participants with known personality profiles rated headline accuracy, we create matching LLM agents and compare their responses to the original human patterns. Certain trait-misinformation associations, notably those involving Agreeableness and Conscientiousness, are reliably replicated, whereas others diverge, revealing systematic biases in how LLMs internalize and express personality. The results underscore both the promise and the limits of personality-aligned LLMs for behavioral simulation, and offer new insight into modeling cognitive diversity in artificial agents.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games</title>
<link>https://arxiv.org/abs/2506.23626</link>
<guid>https://arxiv.org/abs/2506.23626</guid>
<content:encoded><![CDATA[
arXiv:2506.23626v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) in games has gained significant momentum in recent years, enabling the creation of different agent behaviors that can transform a player's gaming experience. However, deploying RL agents in production environments presents two key challenges: (1) designing an effective reward function typically requires an RL expert, and (2) when a game's content or mechanics are modified, previously tuned reward weights may no longer be optimal. Towards the latter challenge, we propose an automated approach for iteratively fine-tuning an RL agent's reward function weights, based on a user-defined language based behavioral goal. A Language Model (LM) proposes updated weights at each iteration based on this target behavior and a summary of performance statistics from prior training rounds. This closed-loop process allows the LM to self-correct and refine its output over time, producing increasingly aligned behavior without the need for manual reward engineering. We evaluate our approach in a racing task and show that it consistently improves agent performance across iterations. The LM-guided agents show a significant increase in performance from $9\%$ to $74\%$ success rate in just one iteration. We compare our LM-guided tuning against a human expert's manual weight design in the racing task: by the final iteration, the LM-tuned agent achieved an $80\%$ success rate, and completed laps in an average of $855$ time steps, a competitive performance against the expert-tuned agent's peak $94\%$ success, and $850$ time steps.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>L0: Reinforcement Learning to Become General Agents</title>
<link>https://arxiv.org/abs/2506.23667</link>
<guid>https://arxiv.org/abs/2506.23667</guid>
<content:encoded><![CDATA[
arXiv:2506.23667v1 Announce Type: new 
Abstract: Training large language models (LLMs) to act as autonomous agents for multi-turn, long-horizon tasks remains significant challenges in scalability and training efficiency. To address this, we introduce L-Zero (L0), a scalable, end-to-end training pipeline for general-purpose agents. Featuring a low-cost, extensible, and sandboxed concurrent agent worker pool, L0 lowers the barrier for applying reinforcement learning in complex environments. We also introduce NB-Agent, the agent scaffold within L0, which operates in a "code-as-action" fashion via a Read-Eval-Print-Loop (REPL). We evaluate L0 on factuality question-answering benchmarks. Our experiments demonstrate that a base model can develop robust problem-solving skills using solely Reinforcement Learning with Verifiable Rewards (RLVR). On the Qwen2.5-7B-Instruct model, our method boosts accuracy on SimpleQA from 30 % to 80 % and on HotpotQA from 22 % to 41 %. We have open-sourced the entire L0 system, including our L0 series models, the NB-Agent, a complete training pipeline, and the corresponding training recipes on (https://github.com/cmriat/l0).
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Interleaved Speech Modeling through Knowledge Distillation</title>
<link>https://arxiv.org/abs/2506.23670</link>
<guid>https://arxiv.org/abs/2506.23670</guid>
<content:encoded><![CDATA[
arXiv:2506.23670v1 Announce Type: new 
Abstract: Current speech language models exceed the size and latency constraints of many deployment environments. We build compact, expressive speech generation models through layer-aligned distillation, matching hidden states, attention maps, and softened logits to compress large multimodal transformers by 3x with minimal loss in performance. We introduce TinyWave, a family of 2B-parameter models for speech-to-speech and interleaved speech-text generation, trained on 50,000 hours of public audio. TinyWave supports (i) speech-only generation using phonetic or expressive tokens and (ii) mixed speech-text continuations. Evaluation on Libri-Light shows TinyWave within 1.4 normalized perplexity points of its teacher. Accuracy on spoken StoryCloze and SALMon reaches 93-97% of the teacher's performance, outperforming size-matched baselines. These models are optimized for deployment on commodity hardware, enabling applications in real-time conversational agents, assistive technologies, and low-resource environments. We release models, training code, and evaluation scripts to support reproducible research on compact, expressive speech generation.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pok\'eAI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon Red</title>
<link>https://arxiv.org/abs/2506.23689</link>
<guid>https://arxiv.org/abs/2506.23689</guid>
<content:encoded><![CDATA[
arXiv:2506.23689v1 Announce Type: new 
Abstract: We introduce Pok\'eAI, the first text-based, multi-agent large language model (LLM) framework designed to autonomously play and progress through Pok\'emon Red. Our system consists of three specialized agents-Planning, Execution, and Critique-each with its own memory bank, role, and skill set. The Planning Agent functions as the central brain, generating tasks to progress through the game. These tasks are then delegated to the Execution Agent, which carries them out within the game environment. Upon task completion, the Critique Agent evaluates the outcome to determine whether the objective was successfully achieved. Once verification is complete, control returns to the Planning Agent, forming a closed-loop decision-making system.
  As a preliminary step, we developed a battle module within the Execution Agent. Our results show that the battle AI achieves an average win rate of 80.8% across 50 wild encounters, only 6% lower than the performance of an experienced human player. Furthermore, we find that a model's battle performance correlates strongly with its LLM Arena score on language-related tasks, indicating a meaningful link between linguistic ability and strategic reasoning. Finally, our analysis of gameplay logs reveals that each LLM exhibits a unique playstyle, suggesting that individual models develop distinct strategic behaviors.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent4S: The Transformation of Research Paradigms from the Perspective of Large Language Models</title>
<link>https://arxiv.org/abs/2506.23692</link>
<guid>https://arxiv.org/abs/2506.23692</guid>
<content:encoded><![CDATA[
arXiv:2506.23692v1 Announce Type: new 
Abstract: While AI for Science (AI4S) serves as an analytical tool in the current research paradigm, it doesn't solve its core inefficiency. We propose "Agent for Science" (Agent4S)-the use of LLM-driven agents to automate the entire research workflow-as the true Fifth Scientific Paradigm. This paper introduces a five-level classification for Agent4S, outlining a clear roadmap from simple task automation to fully autonomous, collaborative "AI Scientists." This framework defines the next revolutionary step in scientific discovery.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DABstep: Data Agent Benchmark for Multi-step Reasoning</title>
<link>https://arxiv.org/abs/2506.23719</link>
<guid>https://arxiv.org/abs/2506.23719</guid>
<content:encoded><![CDATA[
arXiv:2506.23719v1 Announce Type: new 
Abstract: We introduce DABstep, a novel benchmark for evaluating AI agents on realistic multi-step data analysis tasks. DABstep comprises over 450 real-world challenges derived from a financial analytics platform, requiring models to combine code-based data processing with contextual reasoning over heterogeneous documentation. Each task demands an iterative, multi-step problem-solving approach, testing capabilities in data manipulation, cross-referencing multiple sources, and precise result reporting. The benchmark provides a factoid-style answer format with automatic correctness checks for objective scoring at scale. We evaluate leading LLM-based agents, revealing a substantial performance gap: even the best agent achieves only 14.55% accuracy on the hardest tasks. We detail our benchmark's design, dataset composition, task formulation, evaluation protocol, report baseline results and analyze failure modes. DABstep is released with a public leaderboard and toolkit to accelerate research in autonomous data analysis.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of LLM-based Automated Program Repair: Taxonomies, Design Paradigms, and Applications</title>
<link>https://arxiv.org/abs/2506.23749</link>
<guid>https://arxiv.org/abs/2506.23749</guid>
<content:encoded><![CDATA[
arXiv:2506.23749v1 Announce Type: new 
Abstract: Large language models (LLMs) are reshaping automated program repair (APR). We categorize the recent 63 LLM-based APR systems published from January 2022 to June 2025 into four paradigms, and show how retrieval- or analysis-augmented contexts strengthen any of them. This taxonomy clarifies key trade-offs: fine-tuning delivers strong task alignment at high training cost; prompting enables rapid deployment but is limited by prompt design and context windows; procedural pipelines offer reproducible control with moderate overhead; agentic frameworks tackle multi-hunk or cross-file bugs at the price of increased latency and complexity. Persistent challenges include verifying semantic correctness beyond test suites, repairing repository-scale defects, and lowering the costs of LLMs. We outline research directions that combine lightweight human feedback, repository-aware retrieval, code analysis, and cost-aware planning to advance reliable and efficient LLM-based APR.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging a Multi-Agent LLM-Based System to Educate Teachers in Hate Incidents Management</title>
<link>https://arxiv.org/abs/2506.23774</link>
<guid>https://arxiv.org/abs/2506.23774</guid>
<content:encoded><![CDATA[
arXiv:2506.23774v1 Announce Type: new 
Abstract: Computer-aided teacher training is a state-of-the-art method designed to enhance teachers' professional skills effectively while minimising concerns related to costs, time constraints, and geographical limitations. We investigate the potential of large language models (LLMs) in teacher education, using a case of teaching hate incidents management in schools. To this end, we create a multi-agent LLM-based system that mimics realistic situations of hate, using a combination of retrieval-augmented prompting and persona modelling. It is designed to identify and analyse hate speech patterns, predict potential escalation, and propose effective intervention strategies. By integrating persona modelling with agentic LLMs, we create contextually diverse simulations of hate incidents, mimicking real-life situations. The system allows teachers to analyse and understand the dynamics of hate incidents in a safe and controlled environment, providing valuable insights and practical knowledge to manage such situations confidently in real life. Our pilot evaluation demonstrates teachers' enhanced understanding of the nature of annotator disagreements and the role of context in hate speech interpretation, leading to the development of more informed and effective strategies for addressing hate in classroom settings.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetically Expressive: Evaluating gesture and voice for emotion and empathy in VR and 2D scenarios</title>
<link>https://arxiv.org/abs/2506.23777</link>
<guid>https://arxiv.org/abs/2506.23777</guid>
<content:encoded><![CDATA[
arXiv:2506.23777v1 Announce Type: new 
Abstract: The creation of virtual humans increasingly leverages automated synthesis of speech and gestures, enabling expressive, adaptable agents that effectively engage users. However, the independent development of voice and gesture generation technologies, alongside the growing popularity of virtual reality (VR), presents significant questions about the integration of these signals and their ability to convey emotional detail in immersive environments. In this paper, we evaluate the influence of real and synthetic gestures and speech, alongside varying levels of immersion (VR vs. 2D displays) and emotional contexts (positive, neutral, negative) on user perceptions. We investigate how immersion affects the perceived match between gestures and speech and the impact on key aspects of user experience, including emotional and empathetic responses and the sense of co-presence. Our findings indicate that while VR enhances the perception of natural gesture-voice pairings, it does not similarly improve synthetic ones - amplifying the perceptual gap between them. These results highlight the need to reassess gesture appropriateness and refine AI-driven synthesis for immersive environments. See video: https://youtu.be/WMfjIB1X-dc
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.23793</link>
<guid>https://arxiv.org/abs/2506.23793</guid>
<content:encoded><![CDATA[
arXiv:2506.23793v1 Announce Type: new 
Abstract: Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot trajectory planning problems, where multiple homogeneous robots simultaneously move in the shared environment. While solving MAPF optimally has been proven to be NP-hard, scalable, and efficient, solvers are vital for real-world applications like logistics, search-and-rescue, etc. To this end, decentralized suboptimal MAPF solvers that leverage machine learning have come on stage. Building on the success of the recently introduced MAPF-GPT, a pure imitation learning solver, we introduce MAPF-GPT-DDG. This novel approach effectively fine-tunes the pre-trained MAPF model using centralized expert data. Leveraging a novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training while significantly improving performance at test time. Our experiments demonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF solvers, including the original MAPF-GPT, regarding solution quality across many testing scenarios. Remarkably, it can work with MAPF instances involving up to 1 million agents in a single environment, setting a new milestone for scalability in MAPF domains.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents</title>
<link>https://arxiv.org/abs/2506.23844</link>
<guid>https://arxiv.org/abs/2506.23844</guid>
<content:encoded><![CDATA[
arXiv:2506.23844v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have catalyzed the rise of autonomous AI agents capable of perceiving, reasoning, and acting in dynamic, open-ended environments. These large-model agents mark a paradigm shift from static inference systems to interactive, memory-augmented entities. While these capabilities significantly expand the functional scope of AI, they also introduce qualitatively novel security risks - such as memory poisoning, tool misuse, reward hacking, and emergent misalignment - that extend beyond the threat models of conventional systems or standalone LLMs. In this survey, we first examine the structural foundations and key capabilities that underpin increasing levels of agent autonomy, including long-term memory retention, modular tool use, recursive planning, and reflective reasoning. We then analyze the corresponding security vulnerabilities across the agent stack, identifying failure modes such as deferred decision hazards, irreversible tool chains, and deceptive behaviors arising from internal state drift or value misalignment. These risks are traced to architectural fragilities that emerge across perception, cognition, memory, and action modules. To address these challenges, we systematically review recent defense strategies deployed at different autonomy layers, including input sanitization, memory lifecycle control, constrained decision-making, structured tool invocation, and introspective reflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a unified cognitive framework grounded in Constrained Markov Decision Processes (CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation, and joint reward-risk optimization to enable principled, proactive safety across the agent's decision-making loop.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance of LLMs on Stochastic Modeling Operations Research Problems: From Theory to Practice</title>
<link>https://arxiv.org/abs/2506.23924</link>
<guid>https://arxiv.org/abs/2506.23924</guid>
<content:encoded><![CDATA[
arXiv:2506.23924v1 Announce Type: new 
Abstract: Large language models (LLMs) have exhibited expert-level capabilities across various domains. However, their abilities to solve problems in Operations Research (OR) -- the analysis and optimization of mathematical models derived from real-world problems or their verbal descriptions -- remain underexplored. In this work, we take a first step toward evaluating LLMs' abilities to solve stochastic modeling problems, a core class of OR problems characterized by uncertainty and typically involving tools from probability, statistics, and stochastic processes. We manually procure a representative set of graduate-level homework and doctoral qualification-exam problems and test LLMs' abilities to solve them. We further leverage SimOpt, an open-source library of simulation-optimization problems and solvers, to investigate LLMs' abilities to make real-world decisions under uncertainty. Our results show that, though a nontrivial amount of work is still needed to reliably automate the stochastic modeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on par with human experts in both classroom and practical settings. These findings highlight the potential of building AI agents that assist OR researchers and amplify the real-world impact of OR through automation.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Agents Are the Antidote to Walled Gardens</title>
<link>https://arxiv.org/abs/2506.23978</link>
<guid>https://arxiv.org/abs/2506.23978</guid>
<content:encoded><![CDATA[
arXiv:2506.23978v1 Announce Type: new 
Abstract: While the Internet's core infrastructure was designed to be open and universal, today's application layer is dominated by closed, proprietary platforms. Open and interoperable APIs require significant investment, and market leaders have little incentive to enable data exchange that could erode their user lock-in. We argue that LLM-based agents fundamentally disrupt this status quo. Agents can automatically translate between data formats and interact with interfaces designed for humans: this makes interoperability dramatically cheaper and effectively unavoidable. We name this shift universal interoperability: the ability for any two digital services to exchange data seamlessly using AI-mediated adapters. Universal interoperability undermines monopolistic behaviours and promotes data portability. However, it can also lead to new security risks and technical debt. Our position is that the ML community should embrace this development while building the appropriate frameworks to mitigate the downsides. By acting now, we can harness AI to restore user freedom and competitive markets without sacrificing security.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.23998</link>
<guid>https://arxiv.org/abs/2506.23998</guid>
<content:encoded><![CDATA[
arXiv:2506.23998v1 Announce Type: new 
Abstract: Congenital heart disease (CHD) presents complex, lifelong challenges often underrepresented in traditional clinical metrics. While unstructured narratives offer rich insights into patient and caregiver experiences, manual thematic analysis (TA) remains labor-intensive and unscalable. We propose a fully automated large language model (LLM) pipeline that performs end-to-end TA on clinical narratives, which eliminates the need for manual coding or full transcript review. Our system employs a novel multi-agent framework, where specialized LLM agents assume roles to enhance theme quality and alignment with human analysis. To further improve thematic relevance, we optionally integrate reinforcement learning from human feedback (RLHF). This supports scalable, patient-centered analysis of large qualitative datasets and allows LLMs to be fine-tuned for specific clinical contexts.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Orchestrated Couplings: A Time-Varying Edge Weight Framework for Efficient Event-Triggered Multiagent Networks</title>
<link>https://arxiv.org/abs/2506.24017</link>
<guid>https://arxiv.org/abs/2506.24017</guid>
<content:encoded><![CDATA[
arXiv:2506.24017v1 Announce Type: new 
Abstract: In this paper, we focus on reducing node-to-node information exchange in distributed control of multiagent networks while improving the overall network performance. Specifically, we consider a multiagent network that is composed of leader and follower nodes over a time-varying, connected, and undirected graph. In contrast to existing works on the event-triggered distributed control literature, we propose a time-varying edge weight event-triggered control framework. In this framework, each node dynamically adjusts its edge weights by increasing them during the transient (active) phase and decreasing them during the steady-state (idle) phase of the multiagent network. This not only reduces the number of events in the network but also improves the performance (i.e., convergence speed and control effort) of the overall multiagent network. System-theoretically, we first prove the closed-loop stability of the proposed event-triggered distributed control framework, where we then show that this framework does not exhibit a Zeno behavior. Finally, illustrative numerical examples are provided to demonstrate the efficacy of this framework.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ella: Embodied Social Agents with Lifelong Memory</title>
<link>https://arxiv.org/abs/2506.24019</link>
<guid>https://arxiv.org/abs/2506.24019</guid>
<content:encoded><![CDATA[
arXiv:2506.24019v1 Announce Type: new 
Abstract: We introduce Ella, an embodied social agent capable of lifelong learning within a community in a 3D open world, where agents accumulate experiences and acquire knowledge through everyday visual observations and social interactions. At the core of Ella's capabilities is a structured, long-term multimodal memory system that stores, updates, and retrieves information effectively. It consists of a name-centric semantic memory for organizing acquired knowledge and a spatiotemporal episodic memory for capturing multimodal experiences. By integrating this lifelong memory system with foundation models, Ella retrieves relevant information for decision-making, plans daily activities, builds social relationships, and evolves autonomously while coexisting with other intelligent beings in the open world. We conduct capability-oriented evaluations in a dynamic 3D open world where 15 agents engage in social activities for days and are assessed with a suite of unseen controlled evaluations. Experimental results show that Ella can influence, lead, and cooperate with other agents well to achieve goals, showcasing its ability to learn effectively through observation and social interaction. Our findings highlight the transformative potential of combining structured memory systems with foundation models for advancing embodied intelligence. More videos can be found at https://umass-embodied-agi.github.io/Ella/.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on Heterogeneous SoC</title>
<link>https://arxiv.org/abs/2506.24045</link>
<guid>https://arxiv.org/abs/2506.24045</guid>
<content:encoded><![CDATA[
arXiv:2506.24045v1 Announce Type: new 
Abstract: The proliferation of agentic Large Language Models (LLMs) on personal devices introduces a new class of workloads characterized by a dichotomy of objectives. Reactive tasks, initiated by users, demand immediate, low-latency responses, while proactive tasks operate invisibly and prioritize throughput. Existing on-device LLM engines, designed for isolated inferences, fail to efficiently manage these concurrent and conflicting requests on consumer-grade heterogeneous SoCs with CPU, integrated GPU, and NPU. This paper introduces Agent.xpu, an efficient serving system for agentic LLM workloads on memory-unified heterogeneous SoCs. With dedicated offline profiling, Agent.xpu first constructs a heterogeneous execution graph, which fuses and chunks model kernels for affinity-guided, elastic accelerator mapping with predictive kernel annotation. At runtime, its online scheduler enables fine-grained, kernel-level preemption to guarantee the responsiveness of reactive tasks. To maximize SoC utilization, it adopts slack-aware kernel backfill to opportunistically append proactive tasks, and mitigates NPU-iGPU contention via bandwidth-aware dispatch. Evaluation on an Intel Core Ultra SoC shows that Agent.xpu achieves 4.6$\times$ lower latency for reactive tasks and sustains 1.6$\times$-6.8$\times$ higher throughput for proactive tasks compared to state-of-the-art inference engines.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protocol insecurity with finitely many sessions and XOR</title>
<link>https://arxiv.org/abs/2506.24072</link>
<guid>https://arxiv.org/abs/2506.24072</guid>
<content:encoded><![CDATA[
arXiv:2506.24072v1 Announce Type: new 
Abstract: We present a different proof of the insecurity problem for XOR, solved in by Chevalier, Kuesters, Rusinowitch and Turuani (2005). Our proof uses the notion of typed terms and well-typed proofs, and removes a restriction on the class of protocols to which the [CKRT05] proof applies, by introducing a slightly different (but very natural) notion of protocols, where honest agent sends are derivable from previous receives in the same session.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.24119</link>
<guid>https://arxiv.org/abs/2506.24119</guid>
<content:encoded><![CDATA[
arXiv:2506.24119v1 Announce Type: new 
Abstract: Recent advances in reinforcement learning have shown that language models can develop sophisticated reasoning through training on tasks with verifiable rewards, but these approaches depend on human-curated problem-answer pairs and domain-specific reward engineering. We introduce SPIRAL, a self-play framework where models learn by playing multi-turn, zero-sum games against continuously improving versions of themselves, eliminating the need for human supervision. Through self-play, SPIRAL generates an infinite curriculum of progressively challenging problems as models must constantly adapt to stronger opponents. To enable this self-play training at scale, We implement a fully online, multi-turn, multi-agent reinforcement learning system for LLMs and propose role-conditioned advantage estimation (RAE) to stabilize multi-agent training. Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6% improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000 expert game trajectories. Analysis reveals that this transfer occurs through three cognitive patterns: systematic decomposition, expected value calculation, and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple Negotiation) further enhances performance as each game develops distinct reasoning strengths. Applying SPIRAL to a strong reasoning model (DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These results demonstrate that zero-sum games naturally develop transferable reasoning capabilities, highlighting a promising direction for autonomous reasoning development.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DSAC: Distributional Soft Actor-Critic for Risk-Sensitive Reinforcement Learning</title>
<link>https://arxiv.org/abs/2004.14547</link>
<guid>https://arxiv.org/abs/2004.14547</guid>
<content:encoded><![CDATA[
arXiv:2004.14547v3 Announce Type: replace 
Abstract: We present Distributional Soft Actor-Critic (DSAC), a distributional reinforcement learning (RL) algorithm that combines the strengths of distributional information of accumulated rewards and entropy-driven exploration from Soft Actor-Critic (SAC) algorithm. DSAC models the randomness in both action and rewards, surpassing baseline performances on various continuous control tasks. Unlike standard approaches that solely maximize expected rewards, we propose a unified framework for risk-sensitive learning, one that optimizes the risk-related objective while balancing entropy to encourage exploration. Extensive experiments demonstrate DSAC's effectiveness in enhancing agent performances for both risk-neutral and risk-sensitive control tasks.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Fagnano Triangle Patrolling Problem</title>
<link>https://arxiv.org/abs/2307.13153</link>
<guid>https://arxiv.org/abs/2307.13153</guid>
<content:encoded><![CDATA[
arXiv:2307.13153v4 Announce Type: replace 
Abstract: We investigate a combinatorial optimization problem that involves patrolling the edges of an acute triangle using a unit-speed agent. The goal is to minimize the maximum (1-gap) idle time of any edge, which is defined as the time gap between consecutive visits to that edge. This problem has roots in a centuries-old optimization problem posed by Fagnano in 1775, who sought to determine the inscribed triangle of an acute triangle with the minimum perimeter. It is well-known that the orthic triangle, giving rise to a periodic and cyclic trajectory obeying the laws of geometric optics, is the optimal solution to Fagnano's problem. Such trajectories are known as Fagnano orbits, or more generally as billiard trajectories. We demonstrate that the orthic triangle is also an optimal solution to the patrolling problem.
  Our main contributions pertain to new connections between billiard trajectories and optimal patrolling schedules in combinatorial optimization. In particular, as an artifact of our arguments, we introduce a novel 2-gap patrolling problem that seeks to minimize the visitation time of objects every three visits. We prove that there exist infinitely many well-structured billiard-type optimal trajectories for this problem, including the orthic trajectory, which has the special property of minimizing the visitation time gap between any two consecutively visited edges. Complementary to that, we also examine the cost of dynamic, sub-optimal trajectories to the 1-gap patrolling optimization problem. These trajectories result from a greedy algorithm and can be implemented by a computationally primitive mobile agent.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Encoders for Data-Efficient Imitation Learning in Modern Video Games</title>
<link>https://arxiv.org/abs/2312.02312</link>
<guid>https://arxiv.org/abs/2312.02312</guid>
<content:encoded><![CDATA[
arXiv:2312.02312v3 Announce Type: replace 
Abstract: Video games have served as useful benchmarks for the decision-making community, but going beyond Atari games towards modern games has been prohibitively expensive for the vast majority of the research community. Prior work in modern video games typically relied on game-specific integration to obtain game features and enable online training, or on existing large datasets. An alternative approach is to train agents using imitation learning to play video games purely from images. However, this setting poses a fundamental question: which visual encoders obtain representations that retain information critical for decision making? To answer this question, we conduct a systematic study of imitation learning with publicly available pre-trained visual encoders compared to the typical task-specific end-to-end training approach in Minecraft, Counter-Strike: Global Offensive, and Minecraft Dungeons. Our results show that end-to-end training can be effective with comparably low-resolution images and only minutes of demonstrations, but significant improvements can be gained by utilising pre-trained encoders such as DINOv2 depending on the game. In addition to enabling effective decision making, we show that pre-trained encoders can make decision-making research in video games more accessible by significantly reducing the cost of training.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Agents using Social Choice Theory</title>
<link>https://arxiv.org/abs/2312.03121</link>
<guid>https://arxiv.org/abs/2312.03121</guid>
<content:encoded><![CDATA[
arXiv:2312.03121v4 Announce Type: replace 
Abstract: We argue that many general evaluation problems can be viewed through the lens of voting theory. Each task is interpreted as a separate voter, which requires only ordinal rankings or pairwise comparisons of agents to produce an overall evaluation. By viewing the aggregator as a social welfare function, we are able to leverage centuries of research in social choice theory to derive principled evaluation frameworks with axiomatic foundations. These evaluations are interpretable and flexible, while avoiding many of the problems currently facing cross-task evaluation. We apply this Voting-as-Evaluation (VasE) framework across multiple settings, including reinforcement learning, large language models, and humans. In practice, we observe that VasE can be more robust than popular evaluation frameworks (Elo and Nash averaging), discovers properties in the evaluation data not evident from scores alone, and can predict outcomes better than Elo in a complex seven-player game. We identify one particular approach, maximal lotteries, that satisfies important consistency properties relevant to evaluation, is computationally efficient (polynomial in the size of the evaluation data), and identifies game-theoretic cycles.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemMiner: A Large Language Model Agent System for Chemical Literature Data Mining</title>
<link>https://arxiv.org/abs/2402.12993</link>
<guid>https://arxiv.org/abs/2402.12993</guid>
<content:encoded><![CDATA[
arXiv:2402.12993v2 Announce Type: replace 
Abstract: The development of AI-assisted chemical synthesis tools requires comprehensive datasets covering diverse reaction types, yet current high-throughput experimental (HTE) approaches are expensive and limited in scope. Chemical literature represents a vast, underexplored data source containing thousands of reactions published annually. However, extracting reaction information from literature faces significant challenges including varied writing styles, complex coreference relationships, and multimodal information presentation. This paper proposes ChemMiner, a novel end-to-end framework leveraging multiple agents powered by large language models (LLMs) to extract high-fidelity chemical data from literature. ChemMiner incorporates three specialized agents: a text analysis agent for coreference mapping, a multimodal agent for non-textual information extraction, and a synthesis analysis agent for data generation. Furthermore, we developed a comprehensive benchmark with expert-annotated chemical literature to evaluate both extraction efficiency and precision. Experimental results demonstrate reaction identification rates comparable to human chemists while significantly reducing processing time, with high accuracy, recall, and F1 scores. Our open-sourced benchmark facilitates future research in chemical literature data mining.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consensus seeking in diffusive multidimensional networks with a repeated interaction pattern and time-delays</title>
<link>https://arxiv.org/abs/2402.15677</link>
<guid>https://arxiv.org/abs/2402.15677</guid>
<content:encoded><![CDATA[
arXiv:2402.15677v2 Announce Type: replace 
Abstract: This paper studies a consensus problem in multidimensional networks having the same agent-to-agent interaction pattern under both intra- and cross-layer time delays. Several conditions for the agents to asymptotically reach a consensus are derived, which involve the overall network's structure, the local interacting pattern, and the assumptions specified on the time delays. The validity of these conditions is proved by direct eigenvalue evaluation and supported by numerical simulations.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMInA: Benchmarking Multihop Multimodal Internet Agents</title>
<link>https://arxiv.org/abs/2404.09992</link>
<guid>https://arxiv.org/abs/2404.09992</guid>
<content:encoded><![CDATA[
arXiv:2404.09992v2 Announce Type: replace 
Abstract: Autonomous embodied agents live on an Internet of multimedia websites. Can they hop around multimodal websites to complete complex user tasks? Existing benchmarks fail to assess them in a realistic, evolving environment for their embodiment across websites. To answer this question, we present MMInA, a multihop and multimodal benchmark to evaluate the embodied agents for compositional Internet tasks, with several appealing properties: 1) Evolving real-world multimodal websites. Our benchmark uniquely operates on evolving real-world websites, ensuring a high degree of realism and applicability to natural user tasks. Our data includes 1,050 human-written tasks covering various domains such as shopping and travel, with each task requiring the agent to extract multimodal information from web pages as observations autonomously; 2) Multihop web browsing. Our dataset features naturally compositional tasks that require information from or actions on multiple websites to solve, to assess long-range reasoning capabilities on web tasks; 3) Holistic evaluation. We propose a novel protocol for evaluating an agent's progress in completing multihop tasks. We experiment with both standalone (multimodal) language models and heuristic-based web agents. Extensive experiments demonstrate that while long-chain multihop web tasks are easy for humans, they remain challenging for state-of-the-art web agents. We identify that agents are more likely to fail on the early hops when solving tasks with more hops, which results in lower task success rates. To address this issue, we propose a simple memory augmentation approach that replays past action trajectories to reflect. Our method significantly improves the performance of both the single-hop and multihop web browsing abilities. Our code and data are available at github.com/shulin16/MMInA.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Interaction Modeling for Trajectory Prediction via Agent Selection and Physical Coefficient</title>
<link>https://arxiv.org/abs/2405.13152</link>
<guid>https://arxiv.org/abs/2405.13152</guid>
<content:encoded><![CDATA[
arXiv:2405.13152v5 Announce Type: replace 
Abstract: A thorough understanding of the interaction between the target agent and surrounding agents is a prerequisite for accurate trajectory prediction. Although many methods have been explored, they assign correlation coefficients to surrounding agents in a purely learning-based manner. In this study, we present ASPILin, which manually selects interacting agents and replaces the attention scores in Transformer with a newly computed physical correlation coefficient, enhancing the interpretability of interaction modeling. Surprisingly, these simple modifications can significantly improve prediction performance and substantially reduce computational costs. We intentionally simplified our model in other aspects, such as map encoding. Remarkably, experiments conducted on the INTERACTION, highD, and CitySim datasets demonstrate that our method is efficient and straightforward, outperforming other state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ego-Foresight: Self-supervised Learning of Agent-Aware Representations for Improved RL</title>
<link>https://arxiv.org/abs/2407.01570</link>
<guid>https://arxiv.org/abs/2407.01570</guid>
<content:encoded><![CDATA[
arXiv:2407.01570v2 Announce Type: replace 
Abstract: Despite the significant advancements in Deep Reinforcement Learning (RL) observed in the last decade, the amount of training experience necessary to learn effective policies remains one of the primary concerns both in simulated and real environments. Looking to solve this issue, previous work has shown that improved training efficiency can be achieved by separately modeling agent and environment, but usually requiring a supervisory agent mask.
  In contrast to RL, humans can perfect a new skill from a small number of trials and in most cases do so without a supervisory signal, making neuroscientific studies of human development a valuable source of inspiration for RL. In particular, we explore the idea of motor prediction, which states that humans develop an internal model of themselves and of the consequences that their motor commands have on the immediate sensory inputs. Our insight is that the movement of the agent provides a cue that allows the duality between agent and environment to be learned.
  To instantiate this idea, we present Ego-Foresight, a self-supervised method for disentangling agent and environment based on motion and prediction. Our main finding is self-supervised agent-awareness by visuomotor prediction of the agent improves sample-efficiency and performance of the underlying RL algorithm.
  To test our approach, we first study its ability to visually predict agent movement irrespective of the environment, in simulated and real-world robotic data. Then, we integrate Ego-Foresight with a model-free RL algorithm to solve simulated robotic tasks, showing that self-supervised agent-awareness can improve sample-efficiency and performance in RL.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChipXplore: Natural Language Exploration of Hardware Designs and Libraries</title>
<link>https://arxiv.org/abs/2407.12749</link>
<guid>https://arxiv.org/abs/2407.12749</guid>
<content:encoded><![CDATA[
arXiv:2407.12749v3 Announce Type: replace 
Abstract: Hardware design workflows rely on Process Design Kits (PDKs) from different fabrication nodes, each containing standard cell libraries optimized for speed, power, or density. Engineers typically navigate between the design and target PDK to make informed decisions, such as selecting gates for area optimization or enhancing the speed of the critical path. However, this process is often manual, time-consuming, and prone to errors. To address this, we present ChipXplore, a multi-agent collaborative framework powered by large language models that enables engineers to query hardware designs and PDKs using natural language. By exploiting the structured nature of PDK and hardware design data, ChipXplore retrieves relevant information through text-to-SQL and text-to-Cypher customized workflows. The framework achieves an execution accuracy of 97.39\% in complex natural language queries and improves productivity by making retrieval 5.63x faster while reducing errors by 5.25x in user studies. Compared to generic workflows, ChipXplore's customized workflow is capable of orchestrating reasoning and planning over multiple databases, improving accuracy by 29.78\%. ChipXplore lays the foundation for building autonomous agents capable of tackling diverse physical design tasks that require PDK and hardware design awareness.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Know Your Mistakes: Towards Preventing Overreliance on Task-Oriented Conversational AI Through Accountability Modeling</title>
<link>https://arxiv.org/abs/2501.10316</link>
<guid>https://arxiv.org/abs/2501.10316</guid>
<content:encoded><![CDATA[
arXiv:2501.10316v4 Announce Type: replace 
Abstract: Recent LLMs have enabled significant advancements for conversational agents. However, they are also well known to hallucinate, producing responses that seem plausible but are factually incorrect. On the other hand, users tend to over-rely on LLM-based AI agents, accepting AI's suggestion even when it is wrong. Adding positive friction, such as explanations or getting user confirmations, has been proposed as a mitigation in AI-supported decision-making systems. In this paper, we propose an accountability model for LLM-based task-oriented dialogue agents to address user overreliance via friction turns in cases of model uncertainty and errors associated with dialogue state tracking (DST). The accountability model is an augmented LLM with an additional accountability head that functions as a binary classifier to predict the relevant slots of the dialogue state mentioned in the conversation. We perform our experiments with multiple backbone LLMs on two established benchmarks (MultiWOZ and Snips). Our empirical findings demonstrate that the proposed approach not only enables reliable estimation of AI agent errors but also guides the decoder in generating more accurate actions. We observe around 3% absolute improvement in joint goal accuracy (JGA) of DST output by incorporating accountability heads into modern LLMs. Self-correcting the detected errors further increases the JGA from 67.13 to 70.51, achieving state-of-the-art DST performance. Finally, we show that error correction through user confirmations (friction turn) achieves a similar performance gain, highlighting its potential to reduce user overreliance.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MapNav: A Novel Memory Representation via Annotated Semantic Maps for VLM-based Vision-and-Language Navigation</title>
<link>https://arxiv.org/abs/2502.13451</link>
<guid>https://arxiv.org/abs/2502.13451</guid>
<content:encoded><![CDATA[
arXiv:2502.13451v3 Announce Type: replace 
Abstract: Vision-and-language navigation (VLN) is a key task in Embodied AI, requiring agents to navigate diverse and unseen environments while following natural language instructions. Traditional approaches rely heavily on historical observations as spatio-temporal contexts for decision making, leading to significant storage and computational overhead. In this paper, we introduce MapNav, a novel end-to-end VLN model that leverages Annotated Semantic Map (ASM) to replace historical frames. Specifically, our approach constructs a top-down semantic map at the start of each episode and update it at each timestep, allowing for precise object mapping and structured navigation information. Then, we enhance this map with explicit textual labels for key regions, transforming abstract semantics into clear navigation cues and generate our ASM. MapNav agent using the constructed ASM as input, and use the powerful end-to-end capabilities of VLM to empower VLN. Extensive experiments demonstrate that MapNav achieves state-of-the-art (SOTA) performance in both simulated and real-world environments, validating the effectiveness of our method. Moreover, we will release our ASM generation source code and dataset to ensure reproducibility, contributing valuable resources to the field. We believe that our proposed MapNav can be used as a new memory representation method in VLN, paving the way for future research in this field.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoToM: Scaling Model-based Mental Inference via Automated Agent Modeling</title>
<link>https://arxiv.org/abs/2502.15676</link>
<guid>https://arxiv.org/abs/2502.15676</guid>
<content:encoded><![CDATA[
arXiv:2502.15676v2 Announce Type: replace 
Abstract: Theory of Mind (ToM), the ability to understand people's minds based on their behavior, is key to developing socially intelligent agents. Current approaches to ToM reasoning either rely on prompting Large Language Models (LLMs), which are prone to systematic errors, or use handcrafted, rigid agent models for model-based inference, which are more robust but fail to generalize across domains. In this work, we introduce AutoToM, an automated agent modeling method for scalable, robust, and interpretable mental inference. Given a ToM problem, AutoToM first proposes an initial agent model and then performs automated Bayesian inverse planning based on this model, leveraging an LLM backend. Guided by inference uncertainty, it iteratively refines the model by introducing additional mental variables and/or incorporating more timesteps in the context. Across five diverse benchmarks, AutoToM outperforms existing ToM methods and even large reasoning models. Additionally, we show that AutoToM can produce human-like confidence estimates and enable online mental inference for embodied decision-making.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval Augmented Generation Based LLM Evaluation For Protocol State Machine Inference With Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2502.15727</link>
<guid>https://arxiv.org/abs/2502.15727</guid>
<content:encoded><![CDATA[
arXiv:2502.15727v2 Announce Type: replace 
Abstract: This paper presents a novel approach to evaluate the efficiency of a RAG-based agentic Large Language Model (LLM) architecture for network packet seed generation and enrichment. Enhanced by chain-of-thought (COT) prompting techniques, the proposed approach focuses on the improvement of the seeds' structural quality in order to guide protocol fuzzing frameworks through a wide exploration of the protocol state space. Our method leverages RAG and text embeddings to dynamically reference to the Request For Comments (RFC) documents knowledge base for answering queries regarding the protocol's Finite State Machine (FSM), then iteratively reasons through the retrieved knowledge, for output refinement and proper seed placement. We then evaluate the response structure quality of the agent's output, based on metrics as BLEU, ROUGE, and Word Error Rate (WER) by comparing the generated packets against the ground-truth packets. Our experiments demonstrate significant improvements of up to 18.19%, 14.81%, and 23.45% in BLEU, ROUGE, and WER, respectively, over baseline models. These results confirm the potential of such approach, improving LLM-based protocol fuzzing frameworks for the identification of hidden vulnerabilities.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think Before You Segment: High-Quality Reasoning Segmentation with GPT Chain of Thoughts</title>
<link>https://arxiv.org/abs/2503.07503</link>
<guid>https://arxiv.org/abs/2503.07503</guid>
<content:encoded><![CDATA[
arXiv:2503.07503v4 Announce Type: replace 
Abstract: Reasoning segmentation is a challenging vision-language task that aims to output the segmentation mask with respect to a complex, implicit, and even non-visual query text. Previous works incorporated multimodal Large Language Models (MLLMs) with segmentation models to approach the difficult problem. However, their segmentation quality often falls short in complex cases, particularly when dealing with out-of-domain objects with intricate structures, blurry boundaries, occlusions, or high similarity with surroundings. In this paper, we introduce ThinkFirst, a training-free reasoning segmentation framework that leverages GPT's chain of thought to address these challenging cases. Our approach allows GPT-4o or other powerful MLLMs to generate a detailed, chain-of-thought description of an image. This summarized description is then passed to a language-instructed segmentation assistant to aid the segmentation process. Our framework allows users to easily interact with the segmentation agent using multimodal inputs, such as easy text and image scribbles, for successive refinement or communication. We evaluate the performance of ThinkFirst on diverse objects. Extensive experiments show that, this zero-shot-CoT approach significantly improves the vanilla reasoning segmentation agent, both qualitatively and quantitatively, while being less sensitive or critical to user-supplied prompts after Thinking First.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imitation Learning of Correlated Policies in Stackelberg Games</title>
<link>https://arxiv.org/abs/2503.08883</link>
<guid>https://arxiv.org/abs/2503.08883</guid>
<content:encoded><![CDATA[
arXiv:2503.08883v3 Announce Type: replace 
Abstract: Stackelberg games, widely applied in domains like economics and security, involve asymmetric interactions where a leader's strategy drives follower responses. Accurately modeling these dynamics allows domain experts to optimize strategies in interactive scenarios, such as turn-based sports like badminton. In multi-agent systems, agent behaviors are interdependent, and traditional Multi-Agent Imitation Learning (MAIL) methods often fail to capture these complex interactions. Correlated policies, which account for opponents' strategies, are essential for accurately modeling such dynamics. However, even methods designed for learning correlated policies, like CoDAIL, struggle in Stackelberg games due to their asymmetric decision-making, where leaders and followers cannot simultaneously account for each other's actions, often leading to non-correlated policies. Furthermore, existing MAIL methods that match occupancy measures or use adversarial techniques like GAIL or Inverse RL face scalability challenges, particularly in high-dimensional environments, and suffer from unstable training. To address these challenges, we propose a correlated policy occupancy measure specifically designed for Stackelberg games and introduce the Latent Stackelberg Differential Network (LSDN) to match it. LSDN models two-agent interactions as shared latent state trajectories and uses multi-output Geometric Brownian Motion (MO-GBM) to effectively capture joint policies. By leveraging MO-GBM, LSDN disentangles environmental influences from agent-driven transitions in latent space, enabling the simultaneous learning of interdependent policies. This design eliminates the need for adversarial training and simplifies the learning process. Extensive experiments on Iterative Matrix Games and multi-agent particle environments demonstrate that LSDN can better reproduce complex interaction dynamics than existing MAIL methods.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controlling Complex Systems</title>
<link>https://arxiv.org/abs/2504.07579</link>
<guid>https://arxiv.org/abs/2504.07579</guid>
<content:encoded><![CDATA[
arXiv:2504.07579v2 Announce Type: replace 
Abstract: This chapter provides a comprehensive overview of controlling collective behavior in complex systems comprising large ensembles of interacting dynamical agents. Building upon traditional control theory's foundation in individual systems, we introduce tools designed to address the unique challenges of coordinating networks that exhibit emergent phenomena, including consensus, synchronization, and pattern formation. We analyze how local agent interactions generate macroscopic behaviors and investigate the fundamental role of network topology in determining system dynamics. Inspired by natural systems, we emphasize control strategies that achieve global coordination through localized interventions while considering practical implementation challenges. The chapter concludes by presenting novel frameworks for managing very large agent ensembles and leveraging interacting networks for control purposes.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic Data Generation</title>
<link>https://arxiv.org/abs/2504.12563</link>
<guid>https://arxiv.org/abs/2504.12563</guid>
<content:encoded><![CDATA[
arXiv:2504.12563v2 Announce Type: replace 
Abstract: Recent smaller language models such Phi-3.5 and Phi-4 rely on synthetic data generated using larger Language models. Questions remain about leveraging synthetic data for other use cases, such as adapting LLMs to specific domains. A key limitation of synthetic data is low diversity, which negatively impacts its downstream applicability for improving other models. To address this, we propose MetaSynth, a method for generating synthetic data that enhances diversity through meta-prompting, where a language model orchestrates multiple "expert" LLM agents to collaboratively generate data. Using only 25 million tokens of synthetic data generated with MetaSynth, we successfully adapt a well-trained LLM (Mistral-7B-v0.3) to two specialized domains-Finance and Biomedicine-without compromising the capabilities of the resulting model in general tasks. In addition, we evaluate the diversity of our synthetic data using seven automated metrics, and find that it approaches the diversity of LLM pre-training corpora.
  Continually pre-training Mistral-7B-v0.3 with MetaSynth notably outperforms the base LLM, showing improvements of up to 4.08% in Finance and 13.75% in Biomedicine. The same model shows degraded performance when trained on data generated using a template prompt, even when the template includes prior generations and varying In-Context exemplars of real data. Our findings suggest that a few million tokens of diverse synthetic data without mixing any real data, is sufficient for effective domain adaptation when using MetaSynth.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Awareness</title>
<link>https://arxiv.org/abs/2504.20084</link>
<guid>https://arxiv.org/abs/2504.20084</guid>
<content:encoded><![CDATA[
arXiv:2504.20084v2 Announce Type: replace 
Abstract: Recent breakthroughs in artificial intelligence (AI) have brought about increasingly capable systems that demonstrate remarkable abilities in reasoning, language understanding, and problem-solving. These advancements have prompted a renewed examination of AI awareness not as a philosophical question of consciousness, but as a measurable, functional capacity. AI awareness is a double-edged sword: it improves general capabilities, i.e., reasoning, safety, while also raising concerns around misalignment and societal risks, demanding careful oversight as AI capabilities grow.
  In this review, we explore the emerging landscape of AI awareness, which includes metacognition (the ability to represent and reason about its own cognitive state), self-awareness (recognizing its own identity, knowledge, limitations, inter alia), social awareness (modeling the knowledge, intentions, and behaviors of other agents and social norms), and situational awareness (assessing and responding to the context in which it operates).
  First, we draw on insights from cognitive science, psychology, and computational theory to trace the theoretical foundations of awareness and examine how the four distinct forms of AI awareness manifest in state-of-the-art AI. Next, we systematically analyze current evaluation methods and empirical findings to better understand these manifestations. Building on this, we explore how AI awareness is closely linked to AI capabilities, demonstrating that more aware AI agents tend to exhibit higher levels of intelligent behaviors. Finally, we discuss the risks associated with AI awareness, including key topics in AI safety, alignment, and broader ethical concerns.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LibVulnWatch: A Deep Assessment Agent System and Leaderboard for Uncovering Hidden Vulnerabilities in Open-Source AI Libraries</title>
<link>https://arxiv.org/abs/2505.08842</link>
<guid>https://arxiv.org/abs/2505.08842</guid>
<content:encoded><![CDATA[
arXiv:2505.08842v2 Announce Type: replace 
Abstract: Open-source AI libraries are foundational to modern AI systems, yet they present significant, underexamined risks spanning security, licensing, maintenance, supply chain integrity, and regulatory compliance. We introduce LibVulnWatch, a system that leverages recent advances in large language models and agentic workflows to perform deep, evidence-based evaluations of these libraries. Built on a graph-based orchestration of specialized agents, the framework extracts, verifies, and quantifies risk using information from repositories, documentation, and vulnerability databases. LibVulnWatch produces reproducible, governance-aligned scores across five critical domains, publishing results to a public leaderboard for ongoing ecosystem monitoring. Applied to 20 widely used libraries, including ML frameworks, LLM inference engines, and agent orchestration tools, our approach covers up to 88% of OpenSSF Scorecard checks while surfacing up to 19 additional risks per library, such as critical RCE vulnerabilities, missing SBOMs, and regulatory gaps. By integrating advanced language technologies with the practical demands of software risk assessment, this work demonstrates a scalable, transparent mechanism for continuous supply chain evaluation and informed library selection.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Augmenting Online RL with Offline Data is All You Need: A Unified Hybrid RL Algorithm Design and Analysis</title>
<link>https://arxiv.org/abs/2505.13768</link>
<guid>https://arxiv.org/abs/2505.13768</guid>
<content:encoded><![CDATA[
arXiv:2505.13768v3 Announce Type: replace 
Abstract: This paper investigates a hybrid learning framework for reinforcement learning (RL) in which the agent can leverage both an offline dataset and online interactions to learn the optimal policy. We present a unified algorithm and analysis and show that augmenting confidence-based online RL algorithms with the offline dataset outperforms any pure online or offline algorithm alone and achieves state-of-the-art results under two learning metrics, i.e., sub-optimality gap and online learning regret. Specifically, we show that our algorithm achieves a sub-optimality gap $\tilde{O}(\sqrt{1/(N_0/\mathtt{C}(\pi^*|\rho)+N_1}) )$, where $\mathtt{C}(\pi^*|\rho)$ is a new concentrability coefficient, $N_0$ and $N_1$ are the numbers of offline and online samples, respectively. For regret minimization, we show that it achieves a constant $\tilde{O}( \sqrt{N_1/(N_0/\mathtt{C}(\pi^{-}|\rho)+N_1)} )$ speed-up compared to pure online learning, where $\mathtt{C}(\pi^-|\rho)$ is the concentrability coefficient over all sub-optimal policies. Our results also reveal an interesting separation on the desired coverage properties of the offline dataset for sub-optimality gap minimization and regret minimization. We further validate our theoretical findings in several experiments in special RL models such as linear contextual bandits and Markov decision processes (MDPs).
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The residual maximin share</title>
<link>https://arxiv.org/abs/2505.19961</link>
<guid>https://arxiv.org/abs/2505.19961</guid>
<content:encoded><![CDATA[
arXiv:2505.19961v2 Announce Type: replace 
Abstract: We consider fair allocations of indivisible goods to agents with general monotone valuations. We observe that it is useful to introduce a new share-based fairness notion, the {\em residual maximin share} (RMMS). This share is {\em feasible} and {\em self maximizing}. Its value is at least as large as the MXS for monotone valuations, and at least as large as $\frac{2}{3}$-MMS for additive valuations. Known techniques easily imply the existence of partial allocations that are both RMMS and EFX, and complete allocations that are both RMMS and EFL. This unifies and somewhat improves upon several different results from previous papers.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebDancer: Towards Autonomous Information Seeking Agency</title>
<link>https://arxiv.org/abs/2505.22648</link>
<guid>https://arxiv.org/abs/2505.22648</guid>
<content:encoded><![CDATA[
arXiv:2505.22648v2 Announce Type: replace 
Abstract: Addressing intricate real-world problems necessitates in-depth information seeking and multi-step reasoning. Recent progress in agentic systems, exemplified by Deep Research, underscores the potential for autonomous multi-step research. In this work, we present a cohesive paradigm for building end-to-end agentic information seeking agents from a data-centric and training-stage perspective. Our approach consists of four key stages: (1) browsing data construction, (2) trajectories sampling, (3) supervised fine-tuning for effective cold start, and (4) reinforcement learning for enhanced generalisation. We instantiate this framework in a web agent based on the ReAct, WebDancer. Empirical evaluations on the challenging information seeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of WebDancer, achieving considerable results and highlighting the efficacy of our training paradigm. Further analysis of agent training provides valuable insights and actionable, systematic pathways for developing more capable agentic models. The codes and demo will be released in https://github.com/Alibaba-NLP/WebAgent.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Large Language Model-Enabled Control Architecture for Dynamic Resource Capability Exploration in Multi-Agent Manufacturing Systems</title>
<link>https://arxiv.org/abs/2505.22814</link>
<guid>https://arxiv.org/abs/2505.22814</guid>
<content:encoded><![CDATA[
arXiv:2505.22814v2 Announce Type: replace 
Abstract: Manufacturing environments are becoming more complex and unpredictable due to factors such as demand variations and shorter product lifespans. This complexity requires real-time decision-making and adaptation to disruptions. Traditional control approaches highlight the need for advanced control strategies capable of overcoming unforeseen challenges, as they demonstrate limitations in responsiveness within dynamic industrial settings. Multi-agent systems address these challenges through decentralization of decision-making, enabling systems to respond dynamically to operational changes. However, current multi-agent systems encounter challenges related to real-time adaptation, context-aware decision-making, and the dynamic exploration of resource capabilities. Large language models provide the possibility to overcome these limitations through context-aware decision-making capabilities. This paper introduces a large language model-enabled control architecture for multi-agent manufacturing systems to dynamically explore resource capabilities in response to real-time disruptions. A simulation-based case study demonstrates that the proposed architecture improves system resilience and flexibility. The case study findings show improved throughput and efficient resource utilization compared to existing approaches.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2506.04133</link>
<guid>https://arxiv.org/abs/2506.04133</guid>
<content:encoded><![CDATA[
arXiv:2506.04133v2 Announce Type: replace 
Abstract: Agentic AI systems, built upon large language models (LLMs) and deployed in multi-agent configurations, are redefining intelligence, autonomy, collaboration, and decision-making across enterprise and societal domains. This review presents a structured analysis of \textbf{Trust, Risk, and Security Management (TRiSM)} in the context of LLM-based Agentic Multi-Agent Systems (AMAS). We begin by examining the conceptual foundations of Agentic AI and highlight its architectural distinctions from traditional AI agents. We then adapt and extend the AI TRiSM framework for Agentic AI, structured around four key pillars: Governance, Explainability, ModelOps, and Privacy/Security , each contextualized to the challenges of multi-agent LLM systems. A novel risk taxonomy is proposed to capture the unique threats and vulnerabilities of Agentic AI, ranging from coordination failures to prompt-based adversarial manipulation. To support practical assessment in Agentic AI works, we introduce two novel metrics: the Component Synergy Score (CSS), which quantifies the quality of inter-agent collaboration, and the Tool Utilization Efficacy (TUE), which evaluates the efficiency of tool use within agent workflows. We further discuss strategies for improving explainability in Agentic AI , as well as approaches to enhancing security and privacy through encryption, adversarial robustness, and regulatory compliance. The review concludes with a research roadmap for the responsible development and deployment of Agentic AI, outlining critical directions to align emerging systems with TRiSM principles for safe, transparent, and accountable operation.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Debunk and Infer: Multimodal Fake News Detection via Diffusion-Generated Evidence and LLM Reasoning</title>
<link>https://arxiv.org/abs/2506.21557</link>
<guid>https://arxiv.org/abs/2506.21557</guid>
<content:encoded><![CDATA[
arXiv:2506.21557v1 Announce Type: new 
Abstract: The rapid spread of fake news across multimedia platforms presents serious challenges to information credibility. In this paper, we propose a Debunk-and-Infer framework for Fake News Detection(DIFND) that leverages debunking knowledge to enhance both the performance and interpretability of fake news detection. DIFND integrates the generative strength of conditional diffusion models with the collaborative reasoning capabilities of multimodal large language models (MLLMs). Specifically, debunk diffusion is employed to generate refuting or authenticating evidence based on the multimodal content of news videos, enriching the evaluation process with diverse yet semantically aligned synthetic samples. To improve inference, we propose a chain-of-debunk strategy where a multi-agent MLLM system produces logic-grounded, multimodal-aware reasoning content and final veracity judgment. By jointly modeling multimodal features, generative debunking cues, and reasoning-rich verification within a unified architecture, DIFND achieves notable improvements in detection accuracy. Extensive experiments on the FakeSV and FVC datasets show that DIFND not only outperforms existing approaches but also delivers trustworthy decisions.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bench to the Future: A Pastcasting Benchmark for Forecasting Agents</title>
<link>https://arxiv.org/abs/2506.21558</link>
<guid>https://arxiv.org/abs/2506.21558</guid>
<content:encoded><![CDATA[
arXiv:2506.21558v1 Announce Type: new 
Abstract: Forecasting is a challenging task that offers a clearly measurable way to study AI systems. Forecasting requires a large amount of research on the internet, and evaluations require time for events to happen, making the development of forecasting benchmarks challenging. To date, no forecasting benchmark provides a realistic, hermetic, and repeatable environment for LLM forecasters. We introduce Bench To the Future (BTF), a "pastcasting" benchmark with hundreds of high-quality questions for which the resolution is already known. Each question is accompanied by a large offline corpus of tens of thousands of relevant web pages, enabling a way to elicit realistic "forecasts" on past events from LLMs. Results suggest that our pastcasting environment can produce results comparable to those based on forecasts using the internet on at-the-time unresolved questions. We show results benchmarking agent and chain-of-thought forecasting approaches using several LLMs, including the recently-released Claude 4 models, and demonstrate BTF's ability to track steady forecasting capability progress over time. We intend this to be a living benchmark, with new questions added continually to account for increasing training data cutoff dates. We invite researchers to contact us at hello@futuresearch.ai to utilize our benchmark or tooling for their own research.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Agent Probabilistic Inference Framework Inspired by Kairanban-Style CoT System with IdoBata Conversation for Debiasing</title>
<link>https://arxiv.org/abs/2506.21565</link>
<guid>https://arxiv.org/abs/2506.21565</guid>
<content:encoded><![CDATA[
arXiv:2506.21565v1 Announce Type: new 
Abstract: Japan's kairanban culture and idobata conversations have long functioned as traditional communication practices that foster nuanced dialogue among community members and contribute to the formation of social balance. Inspired by these information exchange processes, this study proposes a multi-agent inference framework (KCS+IBC) that integrates multiple large language models (LLMs) to achieve bias mitigation, improved explainability, and probabilistic prediction in sentiment analysis. In addition to sequentially sharing prediction results, the proposed method incorporates a mid-phase casual dialogue session to blend formal inference with individual perspectives and introduces probabilistic sentiment prediction. Experimental results show that KCS achieves accuracy comparable to that of a single LLM across datasets, while KCS+IBC exhibits a consistent decrease in entropy and a gradual increase in variance during the latter stages of inference, suggesting the framework's ability to balance aggregation and diversity of predictions. Future work will quantitatively assess the impact of these characteristics on bias correction and aim to develop more advanced sentiment analysis systems.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents</title>
<link>https://arxiv.org/abs/2506.21582</link>
<guid>https://arxiv.org/abs/2506.21582</guid>
<content:encoded><![CDATA[
arXiv:2506.21582v1 Announce Type: new 
Abstract: Text analytics has traditionally required specialized knowledge in Natural Language Processing (NLP) or text analysis, which presents a barrier for entry-level analysts. Recent advances in large language models (LLMs) have changed the landscape of NLP by enabling more accessible and automated text analysis (e.g., topic detection, summarization, information extraction, etc.). We introduce VIDEE, a system that supports entry-level data analysts to conduct advanced text analytics with intelligent agents. VIDEE instantiates a human-agent collaroration workflow consisting of three stages: (1) Decomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search algorithm to support generative reasoning with human feedback, (2) Execution, which generates an executable text analytics pipeline, and (3) Evaluation, which integrates LLM-based evaluation and visualizations to support user validation of execution results. We conduct two quantitative experiments to evaluate VIDEE's effectiveness and analyze common agent errors. A user study involving participants with varying levels of NLP and text analytics experience -- from none to expert -- demonstrates the system's usability and reveals distinct user behavior patterns. The findings identify design implications for human-agent collaboration, validate the practical utility of VIDEE for non-expert users, and inform future improvements to intelligent text analytics systems.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemBench: Towards More Comprehensive Evaluation on the Memory of LLM-based Agents</title>
<link>https://arxiv.org/abs/2506.21605</link>
<guid>https://arxiv.org/abs/2506.21605</guid>
<content:encoded><![CDATA[
arXiv:2506.21605v1 Announce Type: new 
Abstract: Recent works have highlighted the significance of memory mechanisms in LLM-based agents, which enable them to store observed information and adapt to dynamic environments. However, evaluating their memory capabilities still remains challenges. Previous evaluations are commonly limited by the diversity of memory levels and interactive scenarios. They also lack comprehensive metrics to reflect the memory capabilities from multiple aspects. To address these problems, in this paper, we construct a more comprehensive dataset and benchmark to evaluate the memory capability of LLM-based agents. Our dataset incorporates factual memory and reflective memory as different levels, and proposes participation and observation as various interactive scenarios. Based on our dataset, we present a benchmark, named MemBench, to evaluate the memory capability of LLM-based agents from multiple aspects, including their effectiveness, efficiency, and capacity. To benefit the research community, we release our dataset and project at https://github.com/import-myself/Membench.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SysTemp: A Multi-Agent System for Template-Based Generation of SysML v2</title>
<link>https://arxiv.org/abs/2506.21608</link>
<guid>https://arxiv.org/abs/2506.21608</guid>
<content:encoded><![CDATA[
arXiv:2506.21608v1 Announce Type: new 
Abstract: The automatic generation of SysML v2 models represents a major challenge in the engineering of complex systems, particularly due to the scarcity of learning corpora and complex syntax. We present SysTemp, a system aimed at facilitating and improving the creation of SysML v2 models from natural language specifications. It is based on a multi-agent system, including a template generator that structures the generation process. We discuss the advantages and challenges of this system through an evaluation, highlighting its potential to improve the quality of the generations in SysML v2 modeling.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents</title>
<link>https://arxiv.org/abs/2506.21669</link>
<guid>https://arxiv.org/abs/2506.21669</guid>
<content:encoded><![CDATA[
arXiv:2506.21669v1 Announce Type: new 
Abstract: Self-evolution, the ability of agents to autonomously improve their reasoning and behavior, is essential for the embodied domain with long-horizon, real-world tasks. Despite current advancements in reinforcement fine-tuning (RFT) showing strong performance in enhancing reasoning in LLMs, its potential to enable self-evolving embodied intelligence with multi-modal interactions remains largely unexplored. Specifically, reinforcement fine-tuning faces two fundamental obstacles in embodied settings: (i) the lack of accessible intermediate rewards in multi-step reasoning tasks limits effective learning signals, and (ii) reliance on hand-crafted reward functions restricts generalization to novel tasks and environments. To address these challenges, we present Self-Evolving Embodied Agents-R1, SEEA-R1, the first RFT framework designed for enabling the self-evolving capabilities of embodied agents. Specifically, to convert sparse delayed rewards into denser intermediate signals that improve multi-step reasoning, we propose Tree-based group relative policy optimization (Tree-GRPO), which integrates Monte Carlo Tree Search into GRPO. To generalize reward estimation across tasks and scenes, supporting autonomous adaptation and reward-driven self-evolution, we further introduce Multi-modal Generative Reward Model (MGRM). To holistically evaluate the effectiveness of SEEA-R1, we evaluate on the ALFWorld benchmark, surpassing state-of-the-art methods with scores of 85.07% (textual) and 36.19% (multi-modal), outperforming prior models including GPT-4o. SEEA-R1 also achieves scores of 80.3% without environmental reward, surpassing all open-source baselines and highlighting its scalability as a self-evolving embodied agent. Additional experiments and qualitative analysis further support the potential of SEEA-R1 for future research in scalable embodied intelligence.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simultaneously Fair Allocation of Indivisible Items Across Multiple Dimensions</title>
<link>https://arxiv.org/abs/2506.21727</link>
<guid>https://arxiv.org/abs/2506.21727</guid>
<content:encoded><![CDATA[
arXiv:2506.21727v1 Announce Type: new 
Abstract: This paper explores the fair allocation of indivisible items in a multidimensional setting, motivated by the need to address fairness in complex environments where agents assess bundles according to multiple criteria. Such multidimensional settings are not merely of theoretical interest but are central to many real-world applications. For example, cloud computing resources are evaluated based on multiple criteria such as CPU cores, memory, and network bandwidth. In such cases, traditional one dimensional fairness notions fail to capture fairness across multiple attributes. To address these challenges, we study two relaxed variants of envy-freeness: weak simultaneously envy-free up to c goods (weak sEFc) and strong simultaneously envy-free up to c goods (strong sEFc), which accommodate the multidimensionality of agents' preferences. Under the weak notion, for every pair of agents and for each dimension, any perceived envy can be eliminated by removing, if necessary, a different set of goods from the envied agent's allocation. In contrast, the strong version requires selecting a single set of goods whose removal from the envied bundle simultaneously eliminates envy in every dimension. We provide upper and lower bounds on the relaxation parameter c that guarantee the existence of weak or strong sEFc allocations, where these bounds are independent of the total number of items. In addition, we present algorithms for checking whether a weak or strong sEFc allocation exists. Moreover, we establish NP-hardness results for checking the existence of weak sEF1 and strong sEF1 allocations.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MobiVerse: Scaling Urban Mobility Simulation with Hybrid Lightweight Domain-Specific Generator and Large Language Models</title>
<link>https://arxiv.org/abs/2506.21784</link>
<guid>https://arxiv.org/abs/2506.21784</guid>
<content:encoded><![CDATA[
arXiv:2506.21784v1 Announce Type: new 
Abstract: Understanding and modeling human mobility patterns is crucial for effective transportation planning and urban development. Despite significant advances in mobility research, there remains a critical gap in simulation platforms that allow for algorithm development, policy implementation, and comprehensive evaluation at scale. Traditional activity-based models require extensive data collection and manual calibration, machine learning approaches struggle with adaptation to dynamic conditions, and treding agent-based Large Language Models (LLMs) implementations face computational constraints with large-scale simulations. To address these challenges, we propose MobiVerse, a hybrid framework leverages the efficiency of lightweight domain-specific generator for generating base activity chains with the adaptability of LLMs for context-aware modifications. A case study was conducted in Westwood, Los Angeles, where we efficiently generated and dynamically adjusted schedules for the whole population of approximately 53,000 agents on a standard PC. Our experiments demonstrate that MobiVerse successfully enables agents to respond to environmental feedback, including road closures, large gathering events like football games, and congestion, through our hybrid framework. Its modular design facilitates testing various mobility algorithms at both transportation system and agent levels. Results show our approach maintains computational efficiency while enhancing behavioral realism. MobiVerse bridges the gap in mobility simulation by providing a customizable platform for mobility systems planning and operations with benchmark algorithms. Code and videos are available at https://github.com/ucla-mobility/MobiVerse.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation</title>
<link>https://arxiv.org/abs/2506.21805</link>
<guid>https://arxiv.org/abs/2506.21805</guid>
<content:encoded><![CDATA[
arXiv:2506.21805v1 Announce Type: new 
Abstract: Modeling human behavior in urban environments is fundamental for social science, behavioral studies, and urban planning. Prior work often rely on rigid, hand-crafted rules, limiting their ability to simulate nuanced intentions, plans, and adaptive behaviors. Addressing these challenges, we envision an urban simulator (CitySim), capitalizing on breakthroughs in human-level intelligence exhibited by large language models. In CitySim, agents generate realistic daily schedules using a recursive value-driven approach that balances mandatory activities, personal habits, and situational factors. To enable long-term, lifelike simulations, we endow agents with beliefs, long-term goals, and spatial memory for navigation. CitySim exhibits closer alignment with real humans than prior work, both at micro and macro levels. Additionally, we conduct insightful experiments by modeling tens of thousands of agents and evaluating their collective behaviors under various real-world scenarios, including estimating crowd density, predicting place popularity, and assessing well-being. Our results highlight CitySim as a scalable, flexible testbed for understanding and forecasting urban phenomena.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Laser Scan Path Design for Controlled Microstructure in Additive Manufacturing with Integrated Reduced-Order Phase-Field Modeling and Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.21815</link>
<guid>https://arxiv.org/abs/2506.21815</guid>
<content:encoded><![CDATA[
arXiv:2506.21815v1 Announce Type: new 
Abstract: Laser powder bed fusion (L-PBF) is a widely recognized additive manufacturing technology for producing intricate metal components with exceptional accuracy. A key challenge in L-PBF is the formation of complex microstructures affecting product quality. We propose a physics-guided, machine-learning approach to optimize scan paths for desired microstructure outcomes, such as equiaxed grains. We utilized a phase-field method (PFM) to model crystalline grain structure evolution. To reduce computational costs, we trained a surrogate machine learning model, a 3D U-Net convolutional neural network, using single-track phase-field simulations with various laser powers to predict crystalline grain orientations based on initial microstructure and thermal history. We investigated three scanning strategies across various hatch spacings within a square domain, achieving a two-orders-of-magnitude speedup using the surrogate model. To reduce trial and error in designing laser scan toolpaths, we used deep reinforcement learning (DRL) to generate optimized scan paths for target microstructure. Results from three cases demonstrate the DRL approach's effectiveness. We integrated the surrogate 3D U-Net model into our DRL environment to accelerate the reinforcement learning training process. The reward function minimizes both aspect ratio and grain volume of the predicted microstructure from the agent's scan path. The reinforcement learning algorithm was benchmarked against conventional zigzag approach for smaller and larger domains, showing machine learning methods' potential to enhance microstructure control and computational efficiency in L-PBF optimization.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles</title>
<link>https://arxiv.org/abs/2506.21839</link>
<guid>https://arxiv.org/abs/2506.21839</guid>
<content:encoded><![CDATA[
arXiv:2506.21839v1 Announce Type: new 
Abstract: We challenge text-to-image models with generating escape room puzzle images that are visually appealing, logically solid, and intellectually stimulating. While base image models struggle with spatial relationships and affordance reasoning, we propose a hierarchical multi-agent framework that decomposes this task into structured stages: functional design, symbolic scene graph reasoning, layout synthesis, and local image editing. Specialized agents collaborate through iterative feedback to ensure the scene is visually coherent and functionally solvable. Experiments show that agent collaboration improves output quality in terms of solvability, shortcut avoidance, and affordance clarity, while maintaining visual quality.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Continual Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.21872</link>
<guid>https://arxiv.org/abs/2506.21872</guid>
<content:encoded><![CDATA[
arXiv:2506.21872v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) is an important machine learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in this field due to the rapid development of deep neural networks. However, the success of RL currently relies on extensive training data and computational resources. In addition, RL's limited ability to generalize across tasks restricts its applicability in dynamic and real-world environments. With the arisen of Continual Learning (CL), Continual Reinforcement Learning (CRL) has emerged as a promising research direction to address these limitations by enabling agents to learn continuously, adapt to new tasks, and retain previously acquired knowledge. In this survey, we provide a comprehensive examination of CRL, focusing on its core concepts, challenges, and methodologies. Firstly, we conduct a detailed review of existing works, organizing and analyzing their metrics, tasks, benchmarks, and scenario settings. Secondly, we propose a new taxonomy of CRL methods, categorizing them into four types from the perspective of knowledge storage and/or transfer. Finally, our analysis highlights the unique challenges of CRL and provides practical insights into future directions.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation</title>
<link>https://arxiv.org/abs/2506.21876</link>
<guid>https://arxiv.org/abs/2506.21876</guid>
<content:encoded><![CDATA[
arXiv:2506.21876v1 Announce Type: new 
Abstract: Internal world models (WMs) enable agents to understand the world's state and predict transitions, serving as the basis for advanced deliberative reasoning. Recent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and Gemini, exhibit potential as general-purpose WMs. While the latest studies have evaluated and shown limitations in specific capabilities such as visual understanding, a systematic evaluation of VLMs' fundamental WM abilities remains absent. Drawing on comparative psychology and cognitive science, we propose a two-stage framework that assesses Perception (visual, spatial, temporal, quantitative, and motion) and Prediction (mechanistic simulation, transitive inference, compositional inference) to provide an atomic evaluation of VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale benchmark comprising 23 fine-grained evaluation dimensions across 6 diverse simulated environments with controlled counterfactual simulations. Through 660 experiments on 15 latest commercial and open-source VLMs, we find that these models exhibit striking limitations in basic world modeling abilities. For instance, almost all models perform at near-random accuracy when distinguishing motion trajectories. Additionally, they lack disentangled understanding -- e.g., some models tend to believe blue objects move faster than green ones. More rich results and analyses reveal significant gaps between VLMs and human-level world modeling.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancements and Challenges in Continual Reinforcement Learning: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2506.21899</link>
<guid>https://arxiv.org/abs/2506.21899</guid>
<content:encoded><![CDATA[
arXiv:2506.21899v1 Announce Type: new 
Abstract: The diversity of tasks and dynamic nature of reinforcement learning (RL) require RL agents to be able to learn sequentially and continuously, a learning paradigm known as continuous reinforcement learning. This survey reviews how continual learning transforms RL agents into dynamic continual learners. This enables RL agents to acquire and retain useful and reusable knowledge seamlessly. The paper delves into fundamental aspects of continual reinforcement learning, exploring key concepts, significant challenges, and novel methodologies. Special emphasis is placed on recent advancements in continual reinforcement learning within robotics, along with a succinct overview of evaluation environments utilized in prominent research, facilitating accessibility for newcomers to the field. The review concludes with a discussion on limitations and promising future directions, providing valuable insights for researchers and practitioners alike.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPAZER: Spatial-Semantic Progressive Reasoning Agent for Zero-shot 3D Visual Grounding</title>
<link>https://arxiv.org/abs/2506.21924</link>
<guid>https://arxiv.org/abs/2506.21924</guid>
<content:encoded><![CDATA[
arXiv:2506.21924v1 Announce Type: new 
Abstract: 3D Visual Grounding (3DVG) aims to localize target objects within a 3D scene based on natural language queries. To alleviate the reliance on costly 3D training data, recent studies have explored zero-shot 3DVG by leveraging the extensive knowledge and powerful reasoning capabilities of pre-trained LLMs and VLMs. However, existing paradigms tend to emphasize either spatial (3D-based) or semantic (2D-based) understanding, limiting their effectiveness in complex real-world applications. In this work, we introduce SPAZER - a VLM-driven agent that combines both modalities in a progressive reasoning framework. It first holistically analyzes the scene and produces a 3D rendering from the optimal viewpoint. Based on this, anchor-guided candidate screening is conducted to perform a coarse-level localization of potential objects. Furthermore, leveraging retrieved relevant 2D camera images, 3D-2D joint decision-making is efficiently performed to determine the best-matching object. By bridging spatial and semantic reasoning neural streams, SPAZER achieves robust zero-shot grounding without training on 3D-labeled data. Extensive experiments on ScanRefer and Nr3D benchmarks demonstrate that SPAZER significantly outperforms previous state-of-the-art zero-shot methods, achieving notable gains of 9.0% and 10.9% in accuracy.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation</title>
<link>https://arxiv.org/abs/2506.21931</link>
<guid>https://arxiv.org/abs/2506.21931</guid>
<content:encoded><![CDATA[
arXiv:2506.21931v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has shown promise in enhancing recommendation systems by incorporating external context into large language model prompts. However, existing RAG-based approaches often rely on static retrieval heuristics and fail to capture nuanced user preferences in dynamic recommendation scenarios. In this work, we introduce ARAG, an Agentic Retrieval-Augmented Generation framework for Personalized Recommendation, which integrates a multi-agent collaboration mechanism into the RAG pipeline. To better understand the long-term and session behavior of the user, ARAG leverages four specialized LLM-based agents: a User Understanding Agent that summarizes user preferences from long-term and session contexts, a Natural Language Inference (NLI) Agent that evaluates semantic alignment between candidate items retrieved by RAG and inferred intent, a context summary agent that summarizes the findings of NLI agent, and an Item Ranker Agent that generates a ranked list of recommendations based on contextual fit. We evaluate ARAG accross three datasets. Experimental results demonstrate that ARAG significantly outperforms standard RAG and recency-based baselines, achieving up to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an ablation study to analyse the effect by different components of ARAG. Our findings highlight the effectiveness of integrating agentic reasoning into retrieval-augmented recommendation and provide new directions for LLM-based personalization.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAL-RAG: Retrieval-Augmented Multi-Agent Generation for Content-Aware Layout Design</title>
<link>https://arxiv.org/abs/2506.21934</link>
<guid>https://arxiv.org/abs/2506.21934</guid>
<content:encoded><![CDATA[
arXiv:2506.21934v1 Announce Type: new 
Abstract: Automated content-aware layout generation -- the task of arranging visual elements such as text, logos, and underlays on a background canvas -- remains a fundamental yet under-explored problem in intelligent design systems. While recent advances in deep generative models and large language models (LLMs) have shown promise in structured content generation, most existing approaches lack grounding in contextual design exemplars and fall short in handling semantic alignment and visual coherence. In this work we introduce CAL-RAG, a retrieval-augmented, agentic framework for content-aware layout generation that integrates multimodal retrieval, large language models, and collaborative agentic reasoning. Our system retrieves relevant layout examples from a structured knowledge base and invokes an LLM-based layout recommender to propose structured element placements. A vision-language grader agent evaluates the layout with visual metrics, and a feedback agent provides targeted refinements, enabling iterative improvement. We implement our framework using LangGraph and evaluate it on the PKU PosterLayout dataset, a benchmark rich in semantic and structural variability. CAL-RAG achieves state-of-the-art performance across multiple layout metrics -- including underlay effectiveness, element alignment, and overlap -- substantially outperforming strong baselines such as LayoutPrompter. These results demonstrate that combining retrieval augmentation with agentic multi-step reasoning yields a scalable, interpretable, and high-fidelity solution for automated layout generation.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents</title>
<link>https://arxiv.org/abs/2506.21967</link>
<guid>https://arxiv.org/abs/2506.21967</guid>
<content:encoded><![CDATA[
arXiv:2506.21967v1 Announce Type: new 
Abstract: Current evaluations of tool-integrated LLM agents typically focus on end-to-end tool-usage evaluation while neglecting their stability. This limits their real-world applicability, as various internal or external factors can cause agents to crash or behave abnormally. Our research addresses this by investigating whether agents are vulnerable to errors throughout the entire tool invocation process, including reading tool documentation, selecting tools and generating parameters, and processing the tool's response. Through extensive experiments, we observe that agents are highly susceptible to errors at each stage and agents based on open-source models are more vulnerable than those based on proprietary models. We also find that increasing the model size does not significantly improve tool invocation reasoning and may make agents more vulnerable to attacks resembling normal user instructions. This highlights the importance of evaluating agent stability and offers valuable insights for future LLM development and evaluation.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Trust Generative Agents to Mimic Communication on Social Networks Unless You Benchmarked their Empirical Realism</title>
<link>https://arxiv.org/abs/2506.21974</link>
<guid>https://arxiv.org/abs/2506.21974</guid>
<content:encoded><![CDATA[
arXiv:2506.21974v1 Announce Type: new 
Abstract: The ability of Large Language Models (LLMs) to mimic human behavior triggered a plethora of computational social science research, assuming that empirical studies of humans can be conducted with AI agents instead. Since there have been conflicting research findings on whether and when this hypothesis holds, there is a need to better understand the differences in their experimental designs. We focus on replicating the behavior of social network users with the use of LLMs for the analysis of communication on social networks. First, we provide a formal framework for the simulation of social networks, before focusing on the sub-task of imitating user communication. We empirically test different approaches to imitate user behavior on X in English and German. Our findings suggest that social simulations should be validated by their empirical realism measured in the setting in which the simulation components were fitted. With this paper, we argue for more rigor when applying generative-agent-based modeling for social simulation.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model</title>
<link>https://arxiv.org/abs/2506.21976</link>
<guid>https://arxiv.org/abs/2506.21976</guid>
<content:encoded><![CDATA[
arXiv:2506.21976v1 Announce Type: new 
Abstract: The goal of traffic simulation is to augment a potentially limited amount of manually-driven miles that is available for testing and validation, with a much larger amount of simulated synthetic miles. The culmination of this vision would be a generative simulated city, where given a map of the city and an autonomous vehicle (AV) software stack, the simulator can seamlessly simulate the trip from point A to point B by populating the city around the AV and controlling all aspects of the scene, from animating the dynamic agents (e.g., vehicles, pedestrians) to controlling the traffic light states. We refer to this vision as CitySim, which requires an agglomeration of simulation technologies: scene generation to populate the initial scene, agent behavior modeling to animate the scene, occlusion reasoning, dynamic scene generation to seamlessly spawn and remove agents, and environment simulation for factors such as traffic lights. While some key technologies have been separately studied in various works, others such as dynamic scene generation and environment simulation have received less attention in the research community. We propose SceneDiffuser++, the first end-to-end generative world model trained on a single loss function capable of point A-to-B simulation on a city scale integrating all the requirements above. We demonstrate the city-scale traffic simulation capability of SceneDiffuser++ and study its superior realism under long simulation conditions. We evaluate the simulation quality on an augmented version of the Waymo Open Motion Dataset (WOMD) with larger map regions to support trip-level simulation.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A MILP-Based Solution to Multi-Agent Motion Planning and Collision Avoidance in Constrained Environments</title>
<link>https://arxiv.org/abs/2506.21982</link>
<guid>https://arxiv.org/abs/2506.21982</guid>
<content:encoded><![CDATA[
arXiv:2506.21982v1 Announce Type: new 
Abstract: We propose a mixed-integer linear program (MILP) for multi-agent motion planning that embeds Polytopic Action-based Motion Planning (PAAMP) into a sequence-then-solve pipeline. Region sequences confine each agent to adjacent convex polytopes, while a big-M hyperplane model enforces inter-agent separation. Collision constraints are applied only to agents sharing or neighboring a region, which reduces binary variables exponentially compared with naive formulations. An L1 path-length-plus-acceleration cost yields smooth trajectories. We prove finite-time convergence and demonstrate on representative multi-agent scenarios with obstacles that our formulation produces collision-free trajectories an order of magnitude faster than an unstructured MILP baseline.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.22008</link>
<guid>https://arxiv.org/abs/2506.22008</guid>
<content:encoded><![CDATA[
arXiv:2506.22008v1 Announce Type: new 
Abstract: In offline reinforcement learning, agents are trained using only a fixed set of stored transitions derived from a source policy. However, this requires that the dataset be labeled by a reward function. In applied settings such as video game development, the availability of the reward function is not always guaranteed. This paper proposes Trajectory-Ranked OFfline Inverse reinforcement learning (TROFI), a novel approach to effectively learn a policy offline without a pre-defined reward function. TROFI first learns a reward function from human preferences, which it then uses to label the original dataset making it usable for training the policy. In contrast to other approaches, our method does not require optimal trajectories. Through experiments on the D4RL benchmark we demonstrate that TROFI consistently outperforms baselines and performs comparably to using the ground truth reward to learn policies. Additionally, we validate the efficacy of our method in a 3D game environment. Our studies of the reward model highlight the importance of the reward function in this setting: we show that to ensure the alignment of a value function to the actual future discounted reward, it is fundamental to have a well-engineered and easy-to-learn reward function.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Retrieval for Multimodal Trajectory Modeling</title>
<link>https://arxiv.org/abs/2506.22056</link>
<guid>https://arxiv.org/abs/2506.22056</guid>
<content:encoded><![CDATA[
arXiv:2506.22056v1 Announce Type: new 
Abstract: Trajectory data, capturing human actions and environmental states across various modalities, holds significant potential for enhancing AI agent capabilities, particularly in GUI environments. However, how to model the representation of trajectory-level data presents a significant challenge that has not been systematically addressed amid explosive trajectory data growth. In this work, we introduce Multimodal Trajectory Retrieval, bridging the gap between universal retrieval and agent-centric trajectory modeling. We construct the Unified Agent Trajectory Dataset (UATD) from annotated demonstrations and states across diverse real-world scenarios. Based on this, we present GAE-Bench, a benchmark containing a large number of trajectory-based retrieval pairs. In addition, we propose GAE-Retriever, a multimodal retrieval framework that adopts vision-language models and incorporates optimized contrastive learning through a token selection and the GradCache mechanism. Comprehensive evaluations across multiple datasets show that GAE-Retriever consistently outperforms strong baselines in retrieval recall, highlighting its effectiveness in advancing multimodal trajectory retrieval.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Distributed Safe Multi-Agent Navigation via Infinite-Horizon Optimal Graph Control</title>
<link>https://arxiv.org/abs/2506.22117</link>
<guid>https://arxiv.org/abs/2506.22117</guid>
<content:encoded><![CDATA[
arXiv:2506.22117v1 Announce Type: new 
Abstract: Distributed multi-agent navigation faces inherent challenges due to the competing requirements of maintaining safety and achieving goal-directed behavior, particularly for agents with limited sensing range operating in unknown environments with dense obstacles. Existing approaches typically project predefined goal-reaching controllers onto control barrier function (CBF) constraints, often resulting in conservative and suboptimal trade-offs between safety and goal-reaching performance. We propose an infinite-horizon CBF-constrained optimal graph control formulation for distributed safe multi-agent navigation. By deriving the analytical solution structure, we develop a novel Hamilton-Jacobi-Bellman (HJB)-based learning framework to approximate the solution. In particular, our algorithm jointly learns a CBF and a distributed control policy, both parameterized by graph neural networks (GNNs), along with a value function that robustly guides agents toward their goals. Moreover, we introduce a state-dependent parameterization of Lagrange multipliers, enabling dynamic trade-offs between safety and performance. Unlike traditional short-horizon, quadratic programming-based CBF methods, our approach leverages long-horizon optimization to proactively avoid deadlocks and navigate complex environments more effectively. Extensive simulation results demonstrate substantial improvements in safety and task success rates across various agent dynamics, with strong scalability and generalization to large-scale teams in previously unseen environments. Real-world experiments using Crazyflie drone swarms on challenging antipodal position-swapping tasks further validate the practicality, generalizability, and robustness of the proposed HJB-GNN learning framework.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASVSim (AirSim for Surface Vehicles): A High-Fidelity Simulation Framework for Autonomous Surface Vehicle Research</title>
<link>https://arxiv.org/abs/2506.22174</link>
<guid>https://arxiv.org/abs/2506.22174</guid>
<content:encoded><![CDATA[
arXiv:2506.22174v1 Announce Type: new 
Abstract: The transport industry has recently shown significant interest in unmanned surface vehicles (USVs), specifically for port and inland waterway transport. These systems can improve operational efficiency and safety, which is especially relevant in the European Union, where initiatives such as the Green Deal are driving a shift towards increased use of inland waterways. At the same time, a shortage of qualified personnel is accelerating the adoption of autonomous solutions. However, there is a notable lack of open-source, high-fidelity simulation frameworks and datasets for developing and evaluating such solutions. To address these challenges, we introduce AirSim For Surface Vehicles (ASVSim), an open-source simulation framework specifically designed for autonomous shipping research in inland and port environments. The framework combines simulated vessel dynamics with marine sensor simulation capabilities, including radar and camera systems and supports the generation of synthetic datasets for training computer vision models and reinforcement learning agents. Built upon Cosys-AirSim, ASVSim provides a comprehensive platform for developing autonomous navigation algorithms and generating synthetic datasets. The simulator supports research of both traditional control methods and deep learning-based approaches. Through limited experiments, we demonstrate the potential of the simulator in these research areas. ASVSim is provided as an open-source project under the MIT license, making autonomous navigation research accessible to a larger part of the ocean engineering community.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety</title>
<link>https://arxiv.org/abs/2506.22183</link>
<guid>https://arxiv.org/abs/2506.22183</guid>
<content:encoded><![CDATA[
arXiv:2506.22183v1 Announce Type: new 
Abstract: The rapid rise of open-weight and open-source foundation models is intensifying the obligation and reshaping the opportunity to make AI systems safe. This paper reports outcomes from the Columbia Convening on AI Openness and Safety (San Francisco, 19 Nov 2024) and its six-week preparatory programme involving more than forty-five researchers, engineers, and policy leaders from academia, industry, civil society, and government. Using a participatory, solutions-oriented process, the working groups produced (i) a research agenda at the intersection of safety and open source AI; (ii) a mapping of existing and needed technical interventions and open source tools to safely and responsibly deploy open foundation models across the AI development workflow; and (iii) a mapping of the content safety filter ecosystem with a proposed roadmap for future research and development. We find that openness -- understood as transparent weights, interoperable tooling, and public governance -- can enhance safety by enabling independent scrutiny, decentralized mitigation, and culturally plural oversight. However, significant gaps persist: scarce multimodal and multilingual benchmarks, limited defenses against prompt-injection and compositional attacks in agentic systems, and insufficient participatory mechanisms for communities most affected by AI harms. The paper concludes with a roadmap of five priority research directions, emphasizing participatory inputs, future-proof content filters, ecosystem-wide safety infrastructure, rigorous agentic safeguards, and expanded harm taxonomies. These recommendations informed the February 2025 French AI Action Summit and lay groundwork for an open, plural, and accountable AI safety discipline.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomic Microservice Management via Agentic AI and MAPE-K Integration</title>
<link>https://arxiv.org/abs/2506.22185</link>
<guid>https://arxiv.org/abs/2506.22185</guid>
<content:encoded><![CDATA[
arXiv:2506.22185v1 Announce Type: new 
Abstract: While microservices are revolutionizing cloud computing by offering unparalleled scalability and independent deployment, their decentralized nature poses significant security and management challenges that can threaten system stability. We propose a framework based on MAPE-K, which leverages agentic AI, for autonomous anomaly detection and remediation to address the daunting task of highly distributed system management. Our framework offers practical, industry-ready solutions for maintaining robust and secure microservices. Practitioners and researchers can customize the framework to enhance system stability, reduce downtime, and monitor broader system quality attributes such as system performance level, resilience, security, and anomaly management, among others.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Modularity of Agentic Systems for Drug Discovery</title>
<link>https://arxiv.org/abs/2506.22189</link>
<guid>https://arxiv.org/abs/2506.22189</guid>
<content:encoded><![CDATA[
arXiv:2506.22189v1 Announce Type: new 
Abstract: Large-language models (LLMs) and agentic systems present exciting opportunities to accelerate drug discovery and design. In this study, we critically examine the modularity of LLM-based agentic systems for drug discovery, i.e., whether parts of the agentic system such as the LLM are interchangeable, a topic that has received limited attention in drug discovery applications. We compare the performance of different large language models (LLMs) and the effectiveness of tool-calling agents versus code-generating agents in this domain. Our case study, comparing performance in orchestrating tools for chemistry and drug discovery using an LLM-as-a-judge score, shows that Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative language models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and Nova-Micro. Although we confirm that code-generating agents outperform the tool-calling ones on average, we show that this is highly question and model dependent. Furthermore, the impact of replacing system prompts is dependent on the specific question asked and the model used, underscoring that -- even in this particular domain -- one cannot just replace language models without considering prompt re-engineering. Our study highlights the necessity of further research into the modularity of agentic systems to enable the development of stable and scalable solutions for real-world problems.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligent Disobedience: Rethinking the Agency of Our Artificial Teammates</title>
<link>https://arxiv.org/abs/2506.22276</link>
<guid>https://arxiv.org/abs/2506.22276</guid>
<content:encoded><![CDATA[
arXiv:2506.22276v1 Announce Type: new 
Abstract: Artificial intelligence has made remarkable strides in recent years, achieving superhuman performance across a wide range of tasks. Yet despite these advances, most cooperative AI systems remain rigidly obedient, designed to follow human instructions without question and conform to user expectations, even when doing so may be counterproductive or unsafe. This paper argues for expanding the agency of AI teammates to include \textit{intelligent disobedience}, empowering them to make meaningful and autonomous contributions within human-AI teams. It introduces a scale of AI agency levels and uses representative examples to highlight the importance and growing necessity of treating AI autonomy as an independent research focus in cooperative settings. The paper then explores how intelligent disobedience manifests across different autonomy levels and concludes by proposing initial boundaries and considerations for studying disobedience as a core capability of artificial agents.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embodied AI Agents: Modeling the World</title>
<link>https://arxiv.org/abs/2506.22355</link>
<guid>https://arxiv.org/abs/2506.22355</guid>
<content:encoded><![CDATA[
arXiv:2506.22355v1 Announce Type: new 
Abstract: This paper describes our research on AI agents embodied in visual, virtual or physical forms, enabling them to interact with both users and their environments. These agents, which include virtual avatars, wearable devices, and robots, are designed to perceive, learn and act within their surroundings, which makes them more similar to how humans learn and interact with the environments as compared to disembodied agents. We propose that the development of world models is central to reasoning and planning of embodied AI agents, allowing these agents to understand and predict their environment, to understand user intentions and social contexts, thereby enhancing their ability to perform complex tasks autonomously. World modeling encompasses the integration of multimodal perception, planning through reasoning for action and control, and memory to create a comprehensive understanding of the physical world. Beyond the physical world, we also propose to learn the mental world model of users to enable better human-agent collaboration.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning with Physics-Informed Symbolic Program Priors for Zero-Shot Wireless Indoor Navigation</title>
<link>https://arxiv.org/abs/2506.22365</link>
<guid>https://arxiv.org/abs/2506.22365</guid>
<content:encoded><![CDATA[
arXiv:2506.22365v1 Announce Type: new 
Abstract: When using reinforcement learning (RL) to tackle physical control tasks, inductive biases that encode physics priors can help improve sample efficiency during training and enhance generalization in testing. However, the current practice of incorporating these helpful physics-informed inductive biases inevitably runs into significant manual labor and domain expertise, making them prohibitive for general users. This work explores a symbolic approach to distill physics-informed inductive biases into RL agents, where the physics priors are expressed in a domain-specific language (DSL) that is human-readable and naturally explainable. Yet, the DSL priors do not translate directly into an implementable policy due to partial and noisy observations and additional physical constraints in navigation tasks. To address this gap, we develop a physics-informed program-guided RL (PiPRL) framework with applications to indoor navigation. PiPRL adopts a hierarchical and modularized neuro-symbolic integration, where a meta symbolic program receives semantically meaningful features from a neural perception module, which form the bases for symbolic programming that encodes physics priors and guides the RL process of a low-level neural controller. Extensive experiments demonstrate that PiPRL consistently outperforms purely symbolic or neural policies and reduces training time by over 26% with the help of the program-based inductive biases.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Are Parsing Actions for Understanding Message Hierarchies Not Random?</title>
<link>https://arxiv.org/abs/2506.22366</link>
<guid>https://arxiv.org/abs/2506.22366</guid>
<content:encoded><![CDATA[
arXiv:2506.22366v1 Announce Type: new 
Abstract: If humans understood language by randomly selecting parsing actions, it might have been necessary to construct a robust symbolic system capable of being interpreted under any hierarchical structure. However, human parsing strategies do not seem to follow such a random pattern. Why is that the case? In fact, a previous study on emergent communication using models with hierarchical biases have reported that agents adopting random parsing strategies$\unicode{x2013}$ones that deviate significantly from human language comprehension$\unicode{x2013}$can achieve high communication accuracy. In this study, we investigate this issue by making two simple and natural modifications to the experimental setup: (I) we use more complex inputs that have hierarchical structures, such that random parsing makes semantic interpretation more difficult, and (II) we incorporate a surprisal-related term, which is known to influence the order of words and characters in natural language, into the objective function. With these changes, we evaluate whether agents employing random parsing strategies still maintain high communication accuracy.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements</title>
<link>https://arxiv.org/abs/2506.22419</link>
<guid>https://arxiv.org/abs/2506.22419</guid>
<content:encoded><![CDATA[
arXiv:2506.22419v1 Announce Type: new 
Abstract: Rapid advancements in large language models (LLMs) have the potential to assist in scientific progress. A critical capability toward this endeavor is the ability to reproduce existing work. To evaluate the ability of AI agents to reproduce results in an active research area, we introduce the Automated LLM Speedrunning Benchmark, leveraging the research community contributions on the NanoGPT speedrun, a competition to train a GPT-2 model in the shortest time. Each of the 19 speedrun tasks provides the agent with the previous records training script, optionally paired with one of three hint formats, ranging from pseudocode to paper-like descriptions of the new records improvements. Records execute quickly by design and speedrun improvements encompass diverse code-level changes, ranging from high-level algorithmic advancements to hardware-aware optimizations. These features make the benchmark both accessible and realistic for the frontier problem of improving LLM training. We find that recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement already-known innovations in our benchmark, even when given detailed hints. Our benchmark thus provides a simple, non-saturated measure of an LLMs ability to automate scientific reproduction, a necessary (but not sufficient) skill for an autonomous research agent.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PEACE: Empowering Geologic Map Holistic Understanding with MLLMs</title>
<link>https://arxiv.org/abs/2501.06184</link>
<guid>https://arxiv.org/abs/2501.06184</guid>
<content:encoded><![CDATA[
arXiv:2501.06184v1 Announce Type: cross 
Abstract: Geologic map, as a fundamental diagram in geology science, provides critical insights into the structure and composition of Earth's subsurface and surface. These maps are indispensable in various fields, including disaster detection, resource exploration, and civil engineering. Despite their significance, current Multimodal Large Language Models (MLLMs) often fall short in geologic map understanding. This gap is primarily due to the challenging nature of cartographic generalization, which involves handling high-resolution map, managing multiple associated components, and requiring domain-specific knowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever benchmark for evaluating MLLMs in geologic map understanding, which assesses the full-scale abilities in extracting, referring, grounding, reasoning, and analyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent designed for geologic map understanding, which features three modules: Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI), and Prompt-enhanced Question Answering (PEQA). Inspired by the interdisciplinary collaboration among human scientists, an AI expert group acts as consultants, utilizing a diverse tool pool to comprehensively analyze questions. Through comprehensive experiments, GeoMap-Agent achieves an overall score of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o. Our work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs, paves the way for advanced AI applications in geology, enhancing the efficiency and accuracy of geological investigations.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-based modeling and the sociology of money: some suggestions for refining monetary theory using social simulation</title>
<link>https://arxiv.org/abs/2506.22318</link>
<guid>https://arxiv.org/abs/2506.22318</guid>
<content:encoded><![CDATA[
arXiv:2506.22318v1 Announce Type: cross 
Abstract: The institution of money can be seen as a foundational social mechanism, enabling communities to quantify collectively regulate economic processes. Money can be said, indeed, to constitute the micro-macro link in economics. This paper reviews influential views on the nature of money in economics and sociology, contrasting them to the relatively limited findings of recent agent-based models of "the emergence of money". Noting ample room for novel combinations of sociological and formal methods to drive insight into the many roles played by money in the economy, we conclude by indicating research directions in which we believe this combination can provide new answers to old questions in monetary theory
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRAB: Cross-environment Agent Benchmark for Multimodal Language Model Agents</title>
<link>https://arxiv.org/abs/2407.01511</link>
<guid>https://arxiv.org/abs/2407.01511</guid>
<content:encoded><![CDATA[
arXiv:2407.01511v3 Announce Type: replace 
Abstract: The development of autonomous agents increasingly relies on Multimodal Language Models (MLMs) to perform tasks described in natural language with GUI environments, such as websites, desktop computers, or mobile phones. Existing benchmarks for MLM agents in interactive environments are limited by their focus on a single environment, lack of detailed and generalized evaluation methods, and the complexities of constructing tasks and evaluators. To overcome these limitations, we introduce Crab, the first agent benchmark framework designed to support cross-environment tasks, incorporating a graph-based fine-grained evaluation method and an efficient mechanism for task and evaluator construction. Our framework supports multiple devices and can be easily extended to any environment with a Python interface. Leveraging Crab, we developed a cross-platform Crab Benchmark-v0 comprising 120 tasks in computer desktop and mobile phone environments. We evaluated four advanced MLMs using different single and multi-agent system configurations on this benchmark. The experimental results demonstrate that the single agent with GPT-4o achieves the best completion ratio of 38.01%. All framework code, agent code, and task datasets are publicly available at https://github.com/camel-ai/crab.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soft Condorcet Optimization for Ranking of General Agents</title>
<link>https://arxiv.org/abs/2411.00119</link>
<guid>https://arxiv.org/abs/2411.00119</guid>
<content:encoded><![CDATA[
arXiv:2411.00119v4 Announce Type: replace 
Abstract: Driving progress of AI models and agents requires comparing their performance on standardized benchmarks; for general agents, individual performances must be aggregated across a potentially wide variety of different tasks. In this paper, we describe a novel ranking scheme inspired by social choice frameworks, called Soft Condorcet Optimization (SCO), to compute the optimal ranking of agents: the one that makes the fewest mistakes in predicting the agent comparisons in the evaluation data. This optimal ranking is the maximum likelihood estimate when evaluation data (which we view as votes) are interpreted as noisy samples from a ground truth ranking, a solution to Condorcet's original voting system criteria. SCO ratings are maximal for Condorcet winners when they exist, which we show is not necessarily true for the classical rating system Elo. We propose three optimization algorithms to compute SCO ratings and evaluate their empirical performance. When serving as an approximation to the Kemeny-Young voting method, SCO rankings are on average 0 to 0.043 away from the optimal ranking in normalized Kendall-tau distance across 865 preference profiles from the PrefLib open ranking archive. In a simulated noisy tournament setting, SCO achieves accurate approximations to the ground truth ranking and the best among several baselines when 59\% or more of the preference data is missing. Finally, SCO ranking provides the best approximation to the optimal ranking, measured on held-out test sets, in a problem containing 52,958 human players across 31,049 games of the classic seven-player game of Diplomacy.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Modeling Human-Agentic Collaborative Workflows: A BPMN Extension</title>
<link>https://arxiv.org/abs/2412.05958</link>
<guid>https://arxiv.org/abs/2412.05958</guid>
<content:encoded><![CDATA[
arXiv:2412.05958v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have facilitated the definition of autonomous intelligent agents. Such agents have already demonstrated their potential in solving complex tasks in different domains. And they can further increase their performance when collaborating with other agents in a multi-agent system. However, the orchestration and coordination of these agents is still challenging, especially when they need to interact with humans as part of human-agentic collaborative workflows. These kinds of workflows need to be precisely specified so that it is clear whose responsible for each task, what strategies agents can follow to complete individual tasks or how decisions will be taken when different alternatives are proposed, among others. Current business process modeling languages fall short when it comes to specifying these new mixed collaborative scenarios. In this exploratory paper, we extend a well-known process modeling language (i.e., BPMN) to enable the definition of this new type of workflow. Our extension covers both the formalization of the new metamodeling concepts required and the proposal of a BPMN-like graphical notation to facilitate the definition of these workflows. Our extension has been implemented and is available as an open-source human-agentic workflow modeling editor on GitHub.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UAV-based path planning for efficient localization of non-uniformly distributed weeds using prior knowledge: A reinforcement-learning approach</title>
<link>https://arxiv.org/abs/2412.11717</link>
<guid>https://arxiv.org/abs/2412.11717</guid>
<content:encoded><![CDATA[
arXiv:2412.11717v2 Announce Type: replace 
Abstract: UAVs are becoming popular in agriculture, however, they usually use time-consuming row-by-row flight paths. This paper presents a deep-reinforcement-learning-based approach for path planning to efficiently localize weeds in agricultural fields using UAVs with minimal flight-path length. The method combines prior knowledge about the field containing uncertain, low-resolution weed locations with in-flight weed detections. The search policy was learned using deep Q-learning. We trained the agent in simulation, allowing a thorough evaluation of the weed distribution, typical errors in the perception system, prior knowledge, and different stopping criteria on the planner's performance. When weeds were non-uniformly distributed over the field, the agent found them faster than a row-by-row path, showing its capability to learn and exploit the weed distribution. Detection errors and prior knowledge quality had a minor effect on the performance, indicating that the learned search policy was robust to detection errors and did not need detailed prior knowledge. The agent also learned to terminate the search. To test the transferability of the learned policy to a real-world scenario, the planner was tested on real-world image data without further training, which showed a 66% shorter path compared to a row-by-row path at the cost of a 10% lower percentage of found weeds. Strengths and weaknesses of the planner for practical application are comprehensively discussed, and directions for further development are provided. Overall, it is concluded that the learned search policy can improve the efficiency of finding non-uniformly distributed weeds using a UAV and shows potential for use in agricultural practice.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis</title>
<link>https://arxiv.org/abs/2412.19723</link>
<guid>https://arxiv.org/abs/2412.19723</guid>
<content:encoded><![CDATA[
arXiv:2412.19723v3 Announce Type: replace 
Abstract: Graphical User Interface (GUI) agents powered by Vision-Language Models (VLMs) have demonstrated human-like computer control capability. Despite their utility in advancing digital automation, a critical bottleneck persists: collecting high-quality trajectory data for training. Common practices for collecting such data rely on human supervision or synthetic data generation through executing pre-defined tasks, which are either resource-intensive or unable to guarantee data quality. Moreover, these methods suffer from limited data diversity and significant gaps between synthetic data and real-world environments. To address these challenges, we propose OS-Genesis, a novel GUI data synthesis pipeline that reverses the conventional trajectory collection process. Instead of relying on pre-defined tasks, OS-Genesis enables agents first to perceive environments and perform step-wise interactions, then retrospectively derive high-quality tasks to enable trajectory-level exploration. A trajectory reward model is then employed to ensure the quality of the generated trajectories. We demonstrate that training GUI agents with OS-Genesis significantly improves their performance on highly challenging online benchmarks. In-depth analysis further validates OS-Genesis's efficiency and its superior data quality and diversity compared to existing synthesis methods. Our codes, data, and checkpoints are available at https://qiushisun.github.io/OS-Genesis-Home/.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plant in Cupboard, Orange on Rably, Inat Aphone. Benchmarking Incremental Learning of Situation and Language Model using a Text-Simulated Situated Environment</title>
<link>https://arxiv.org/abs/2502.11733</link>
<guid>https://arxiv.org/abs/2502.11733</guid>
<content:encoded><![CDATA[
arXiv:2502.11733v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) serve not only as chatbots but as key components in agent systems, where their common-sense knowledge significantly impacts performance as language-based planners for situated or embodied action. We assess LLMs' incremental learning (based on feedback from the environment), and controlled in-context learning abilities using a text-based environment. We introduce challenging yet interesting set of experiments to test i) how agents can incrementally solve tasks related to every day objects in typical rooms in a house where each of them are discovered by interacting within the environment, ii) controlled in-context learning abilities and efficiency of agents by providing short info about locations of objects and rooms to check how faster the task can be solved, and finally iii) using synthetic pseudo-English words to gauge how well LLMs are at inferring meaning of unknown words from environmental feedback. Results show that larger commercial models have a substantial gap in performance compared to open-weight but almost all models struggle with the synthetic words experiments.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Language Multi-Agent Learning with Credit Re-Assignment for Interactive Environment Generalization</title>
<link>https://arxiv.org/abs/2502.14496</link>
<guid>https://arxiv.org/abs/2502.14496</guid>
<content:encoded><![CDATA[
arXiv:2502.14496v2 Announce Type: replace 
Abstract: LLM-based agents have made significant advancements in interactive environments, such as mobile operations and web browsing, and other domains beyond computer using. Current multi-agent systems universally excel in performance, compared to single agents, but struggle with generalization across environments due to predefined roles and inadequate strategies for generalizing language agents. The challenge of achieving both strong performance and good generalization has hindered the progress of multi-agent systems for interactive environments. To address these issues, we propose CollabUIAgents, a multi-agent reinforcement learning framework with a novel multi-agent credit re-assignment (CR) strategy, assigning process rewards with LLMs rather than environment-specific rewards and learning with synthesized preference data, in order to foster generalizable, collaborative behaviors among the role-free agents' policies. Empirical results show that our framework improves both performance and cross-environment generalizability of multi-agent systems. Moreover, our 7B-parameter system achieves results on par with or exceed strong closed-source models, and the LLM that guides the CR. We also provide insights in using granular CR rewards effectively for environment generalization, and accommodating trained LLMs in multi-agent systems.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SENSEI: Semantic Exploration Guided by Foundation Models to Learn Versatile World Models</title>
<link>https://arxiv.org/abs/2503.01584</link>
<guid>https://arxiv.org/abs/2503.01584</guid>
<content:encoded><![CDATA[
arXiv:2503.01584v2 Announce Type: replace 
Abstract: Exploration is a cornerstone of reinforcement learning (RL). Intrinsic motivation attempts to decouple exploration from external, task-based rewards. However, established approaches to intrinsic motivation that follow general principles such as information gain, often only uncover low-level interactions. In contrast, children's play suggests that they engage in meaningful high-level behavior by imitating or interacting with their caregivers. Recent work has focused on using foundation models to inject these semantic biases into exploration. However, these methods often rely on unrealistic assumptions, such as language-embedded environments or access to high-level actions. We propose SEmaNtically Sensible ExploratIon (SENSEI), a framework to equip model-based RL agents with an intrinsic motivation for semantically meaningful behavior. SENSEI distills a reward signal of interestingness from Vision Language Model (VLM) annotations, enabling an agent to predict these rewards through a world model. Using model-based RL, SENSEI trains an exploration policy that jointly maximizes semantic rewards and uncertainty. We show that in both robotic and video game-like simulations SENSEI discovers a variety of meaningful behaviors from image observations and low-level actions. SENSEI provides a general tool for learning from foundation model feedback, a crucial research direction, as VLMs become more powerful.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cooperative Bearing-Only Target Pursuit via Multiagent Reinforcement Learning: Design and Experiment</title>
<link>https://arxiv.org/abs/2503.08740</link>
<guid>https://arxiv.org/abs/2503.08740</guid>
<content:encoded><![CDATA[
arXiv:2503.08740v2 Announce Type: replace 
Abstract: This paper addresses the multi-robot pursuit problem for an unknown target, encompassing both target state estimation and pursuit control. First, in state estimation, we focus on using only bearing information, as it is readily available from vision sensors and effective for small, distant targets. Challenges such as instability due to the nonlinearity of bearing measurements and singularities in the two-angle representation are addressed through a proposed uniform bearing-only information filter. This filter integrates multiple 3D bearing measurements, provides a concise formulation, and enhances stability and resilience to target loss caused by limited field of view (FoV). Second, in target pursuit control within complex environments, where challenges such as heterogeneity and limited FoV arise, conventional methods like differential games or Voronoi partitioning often prove inadequate. To address these limitations, we propose a novel multiagent reinforcement learning (MARL) framework, enabling multiple heterogeneous vehicles to search, localize, and follow a target while effectively handling those challenges. Third, to bridge the sim-to-real gap, we propose two key techniques: incorporating adjustable low-level control gains in training to replicate the dynamics of real-world autonomous ground vehicles (AGVs), and proposing spectral-normalized RL algorithms to enhance policy smoothness and robustness. Finally, we demonstrate the successful zero-shot transfer of the MARL controllers to AGVs, validating the effectiveness and practical feasibility of our approach. The accompanying video is available at https://youtu.be/HO7FJyZiJ3E.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Federated Fine-tuning of Large Language Models</title>
<link>https://arxiv.org/abs/2503.12016</link>
<guid>https://arxiv.org/abs/2503.12016</guid>
<content:encoded><![CDATA[
arXiv:2503.12016v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated impressive success across various tasks. Integrating LLMs with Federated Learning (FL), a paradigm known as FedLLM, offers a promising avenue for collaborative model adaptation while preserving data privacy. This survey provides a systematic and comprehensive review of FedLLM. We begin by tracing the historical development of both LLMs and FL, summarizing relevant prior research to set the context. Subsequently, we delve into an in-depth analysis of the fundamental challenges inherent in deploying FedLLM. Addressing these challenges often requires efficient adaptation strategies; therefore, we conduct an extensive examination of existing Parameter-Efficient Fine-tuning (PEFT) methods and explore their applicability within the FL framework. To rigorously evaluate the performance of FedLLM, we undertake a thorough review of existing fine-tuning datasets and evaluation benchmarks. Furthermore, we discuss FedLLM's diverse real-world applications across multiple domains. Finally, we identify critical open challenges and outline promising research directions to foster future advancements in FedLLM. This survey aims to serve as a foundational resource for researchers and practitioners, offering valuable insights into the rapidly evolving landscape of federated fine-tuning for LLMs. It also establishes a roadmap for future innovations in privacy-preserving AI. We actively maintain a GitHub repo \href{https://github.com/Clin0212/Awesome-Federated-LLM-Learning}{https://github.com/Clin0212/Awesome-Federated-LLM-Learning} to track cutting-edge advancements in this field.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated detection of atomicity violations in large-scale systems</title>
<link>https://arxiv.org/abs/2504.00521</link>
<guid>https://arxiv.org/abs/2504.00521</guid>
<content:encoded><![CDATA[
arXiv:2504.00521v2 Announce Type: replace 
Abstract: Atomicity violations in interrupt-driven programs pose a significant threat to software safety in critical systems. These violations occur when the execution sequence of operations on shared resources is disrupted by asynchronous interrupts. Detecting atomicity violations is challenging due to the vast program state space, application-level code dependencies, and complex domain-specific knowledge. We propose Clover, a hybrid framework that integrates static analysis with large language model (LLM) agents to detect atomicity violations in real-world programs. Clover first performs static analysis to extract critical code snippets and operation information. It then initiates a multi-agent process, where the expert agent leverages domain-specific knowledge to detect atomicity violations, which are subsequently validated by the judge agent. Evaluations on RaceBench 2.1, SV-COMP, and RWIP demonstrate that Clover achieves a precision/recall of 92.3%/86.6%, outperforming existing approaches by 27.4-118.2% on F1-score.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REMOR: Automated Peer Review Generation with LLM Reasoning and Multi-Objective Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.11718</link>
<guid>https://arxiv.org/abs/2505.11718</guid>
<content:encoded><![CDATA[
arXiv:2505.11718v2 Announce Type: replace 
Abstract: AI-based peer review systems tend to produce shallow and overpraising suggestions compared to human feedback. Here, we evaluate how well a reasoning LLM trained with multi-objective reinforcement learning (REMOR) can overcome these limitations. We start by designing a multi-aspect reward function that aligns with human evaluation of reviews. The aspects are related to the review itself (e.g., criticisms, novelty) and the relationship between the review and the manuscript (i.e., relevance). First, we perform supervised fine-tuning of DeepSeek-R1-Distill-Qwen-7B using LoRA on PeerRT, a new dataset of high-quality top AI conference reviews enriched with reasoning traces. We then apply Group Relative Policy Optimization (GRPO) to train two models: REMOR-H (with the human-aligned reward) and REMOR-U (with a uniform reward). Interestingly, the human-aligned reward penalizes aspects typically associated with strong reviews, leading REMOR-U to produce qualitatively more substantive feedback. Our results show that REMOR-U and REMOR-H achieve more than twice the average rewards of human reviews, non-reasoning state-of-the-art agentic multi-modal AI review systems, and general commercial LLM baselines. We found that while the best AI and human reviews are comparable in quality, REMOR avoids the long tail of low-quality human reviews. We discuss how reasoning is key to achieving these improvements and release the Human-aligned Peer Review Reward (HPRR) function, the Peer Review Reasoning-enriched Traces (PeerRT) dataset, and the REMOR models, which we believe can help spur progress in the area.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aux-Think: Exploring Reasoning Strategies for Data-Efficient Vision-Language Navigation</title>
<link>https://arxiv.org/abs/2505.11886</link>
<guid>https://arxiv.org/abs/2505.11886</guid>
<content:encoded><![CDATA[
arXiv:2505.11886v3 Announce Type: replace 
Abstract: Vision-Language Navigation (VLN) is a critical task for developing embodied agents that can follow natural language instructions to navigate in complex real-world environments. Recent advances in VLN by large pretrained models have significantly improved generalization and instruction grounding compared to traditional approaches. However, the role of reasoning strategies in navigation-an action-centric, long-horizon task-remains underexplored, despite Chain-of-Thought (CoT) reasoning's demonstrated success in static tasks like visual question answering. To address this gap, we conduct the first systematic evaluation of reasoning strategies for VLN, including No-Think (direct action prediction), Pre-Think (reason before action), and Post-Think (reason after action). Surprisingly, our findings reveal the Inference-time Reasoning Collapse issue, where inference-time reasoning degrades navigation accuracy, highlighting the challenges of integrating reasoning into VLN. Based on this insight, we propose Aux-Think, a framework that trains models to internalize structured reasoning patterns through CoT supervision, while inferring action directly without reasoning in online prediction. To support this framework, we release R2R-CoT-320k, the first Chain-of-Thought annotated dataset for VLN. Extensive experiments show that Aux-Think reduces training effort greatly and achieves the best performance under the same data scale.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows</title>
<link>https://arxiv.org/abs/2505.19897</link>
<guid>https://arxiv.org/abs/2505.19897</guid>
<content:encoded><![CDATA[
arXiv:2505.19897v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have extended their impact beyond Natural Language Processing, substantially fostering the development of interdisciplinary research. Recently, various LLM-based agents have been developed to assist scientific discovery progress across multiple aspects and domains. Among these, computer-using agents, capable of interacting with operating systems as humans do, are paving the way to automated scientific problem-solving and addressing routines in researchers' workflows. Recognizing the transformative potential of these agents, we introduce ScienceBoard, which encompasses two complementary contributions: (i) a realistic, multi-domain environment featuring dynamic and visually rich scientific workflows with integrated professional software, where agents can autonomously interact via different interfaces to accelerate complex research tasks and experiments; and (ii) a challenging benchmark of 169 high-quality, rigorously validated real-world tasks curated by humans, spanning scientific-discovery workflows in domains such as biochemistry, astronomy, and geoinformatics. Extensive evaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude 3.7, UI-TARS) show that, despite some promising results, they still fall short of reliably assisting scientists in complex workflows, achieving only a 15% overall success rate. In-depth analysis further provides valuable insights for addressing current agent limitations and more effective design principles, paving the way to build more capable agents for scientific discovery. Our code, environment, and benchmark are at https://qiushisun.github.io/ScienceBoard-Home/.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Data Systems That Are Business Semantic Centric and AI Agents Assisted</title>
<link>https://arxiv.org/abs/2506.05520</link>
<guid>https://arxiv.org/abs/2506.05520</guid>
<content:encoded><![CDATA[
arXiv:2506.05520v2 Announce Type: replace 
Abstract: Contemporary businesses operate in dynamic environments requiring rapid adaptation to achieve goals and maintain competitiveness. Existing data platforms often fall short by emphasizing tools over alignment with business needs, resulting in inefficiencies and delays. To address this gap, I propose the Business Semantics Centric, AI Agents Assisted Data System (BSDS), a holistic system that integrates architecture, workflows, and team organization to ensure data systems are tailored to business priorities rather than dictated by technical constraints. BSDS redefines data systems as dynamic enablers of business success, transforming them from passive tools into active drivers of organizational growth. BSDS has a modular architecture that comprises curated data linked to business entities, a knowledge base for context-aware AI agents, and efficient data pipelines. AI agents play a pivotal role in assisting with data access and system management, reducing human effort, and improving scalability. Complementing this architecture, BSDS incorporates workflows optimized for both exploratory data analysis and production requirements, balancing speed of delivery with quality assurance. A key innovation of BSDS is its incorporation of the human factor. By aligning data team expertise with business semantics, BSDS bridges the gap between technical capabilities and business needs. Validated through real-world implementation, BSDS accelerates time-to-market for data-driven initiatives, enhances cross-functional collaboration, and provides a scalable blueprint for businesses of all sizes. Future research can build on BSDS to explore optimization strategies using complex systems and adaptive network theories, as well as developing autonomous data systems leveraging AI agents.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequence Modeling for N-Agent Ad Hoc Teamwork</title>
<link>https://arxiv.org/abs/2506.05527</link>
<guid>https://arxiv.org/abs/2506.05527</guid>
<content:encoded><![CDATA[
arXiv:2506.05527v2 Announce Type: replace 
Abstract: N-agent ad hoc teamwork (NAHT) is a newly introduced challenge in multi-agent reinforcement learning, where controlled subteams of varying sizes must dynamically collaborate with varying numbers and types of unknown teammates without pre-coordination. The existing learning algorithm (POAM) considers only independent learning for its flexibility in dealing with a changing number of agents. However, independent learning fails to fully capture the inter-agent dynamics essential for effective collaboration. Based on our observation that transformers deal effectively with sequences with varying lengths and have been shown to be highly effective for a variety of machine learning problems, this work introduces a centralized, transformer-based method for N-agent ad hoc teamwork. Our proposed approach incorporates historical observations and actions of all controlled agents, enabling optimal responses to diverse and unseen teammates in partially observable environments. Empirical evaluation on a StarCraft II task demonstrates that MAT-NAHT outperforms POAM, achieving superior sample efficiency and generalization, without auxiliary agent-modeling objectives.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical Inference of the Value Function for Reinforcement Learning in Infinite Horizon Settings</title>
<link>https://arxiv.org/abs/2001.04515</link>
<guid>https://arxiv.org/abs/2001.04515</guid>
<content:encoded><![CDATA[
arXiv:2001.04515v3 Announce Type: replace-cross 
Abstract: Reinforcement learning is a general technique that allows an agent to learn an optimal policy and interact with an environment in sequential decision making problems. The goodness of a policy is measured by its value function starting from some initial state. The focus of this paper is to construct confidence intervals (CIs) for a policy's value in infinite horizon settings where the number of decision points diverges to infinity. We propose to model the action-value state function (Q-function) associated with a policy based on series/sieve method to derive its confidence interval. When the target policy depends on the observed data as well, we propose a SequentiAl Value Evaluation (SAVE) method to recursively update the estimated policy and its value estimator. As long as either the number of trajectories or the number of decision points diverges to infinity, we show that the proposed CI achieves nominal coverage even in cases where the optimal policy is not unique. Simulation studies are conducted to back up our theoretical findings. We apply the proposed method to a dataset from mobile health studies and find that reinforcement learning algorithms could help improve patient's health status. A Python implementation of the proposed procedure is available at https://github.com/shengzhang37/SAVE.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collective Reasoning Among LLMs: A Framework for Answer Validation Without Ground Truth</title>
<link>https://arxiv.org/abs/2502.20758</link>
<guid>https://arxiv.org/abs/2502.20758</guid>
<content:encoded><![CDATA[
arXiv:2502.20758v2 Announce Type: replace-cross 
Abstract: We introduce a new approach in which several advanced large language models-specifically GPT-4-0125-preview, Meta-LLAMA-3-70B-Instruct, Claude-3-Opus, and Gemini-1.5-Flash-collaborate to both produce and answer intricate, doctoral-level probability problems without relying on any single "correct" reference. Rather than depending on an established ground truth, our investigation focuses on how agreement among diverse models can signal the reliability of their outputs and, by extension, reflect the overall quality of the generated questions. To measure this inter-model alignment, we apply a suite of statistical evaluations, including chi-square tests, Fleiss' Kappa coefficients, and confidence interval calculations, thereby capturing both precision in answers and clarity in question phrasing. Our analysis reveals that Claude and Gemini tend to frame questions more coherently and unambiguously, which is evidenced by their tighter confidence intervals and greater concordance with responding agents. In contrast, LLAMA exhibits wider confidence bands and a lower level of agreement, indicating more variability and reduced consistency in its question formulations. These observations support the notion that a multi-model collaborative strategy not only improves answer dependability but also offers an effective, data-driven mechanism for evaluating and refining question quality when no definitive solution exists. Ultimately, this work delivers actionable insights into enhancing AI-guided reasoning processes through coordinated interactions among heterogeneous language models.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation</title>
<link>https://arxiv.org/abs/2506.20737</link>
<guid>https://arxiv.org/abs/2506.20737</guid>
<content:encoded><![CDATA[
arXiv:2506.20737v1 Announce Type: new 
Abstract: The proliferation of LLM-based agents has led to increasing deployment of inter-agent collaboration for tasks like scheduling, negotiation, resource allocation etc. In such systems, privacy is critical, as agents often access proprietary tools and domain-specific databases requiring strict confidentiality. This paper examines whether LLM-based agents demonstrate an understanding of contextual privacy. And, if instructed, do these systems preserve inference time user privacy in non-adversarial multi-turn conversation. Existing benchmarks to evaluate contextual privacy in LLM-agents primarily assess single-turn, low-complexity tasks where private information can be easily excluded. We first present a benchmark - MAGPIE comprising 158 real-life high-stakes scenarios across 15 domains. These scenarios are designed such that complete exclusion of private data impedes task completion yet unrestricted information sharing could lead to substantial losses. We then evaluate the current state-of-the-art LLMs on (a) their understanding of contextually private data and (b) their ability to collaborate without violating user privacy. Empirical experiments demonstrate that current models, including GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual privacy, misclassifying private data as shareable 25.2\% and 43.6\% of the time. In multi-turn conversations, these models disclose private information in 59.9\% and 50.5\% of cases even under explicit privacy instructions. Furthermore, multi-agent systems fail to complete tasks in 71\% of scenarios. These results underscore that current models are not aligned towards both contextual privacy preservation and collaborative task-solving.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of AI for Materials Science: Foundation Models, LLM Agents, Datasets, and Tools</title>
<link>https://arxiv.org/abs/2506.20743</link>
<guid>https://arxiv.org/abs/2506.20743</guid>
<content:encoded><![CDATA[
arXiv:2506.20743v1 Announce Type: new 
Abstract: Foundation models (FMs) are catalyzing a transformative shift in materials science (MatSci) by enabling scalable, general-purpose, and multimodal AI systems for scientific discovery. Unlike traditional machine learning models, which are typically narrow in scope and require task-specific engineering, FMs offer cross-domain generalization and exhibit emergent capabilities. Their versatility is especially well-suited to materials science, where research challenges span diverse data types and scales. This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field. We introduce a task-driven taxonomy encompassing six broad application areas: data extraction, interpretation and Q\&amp;A atomistic simulation; property prediction; materials structure, design and discovery; process planning, discovery, and optimization; and multiscale modeling. We discuss recent advances in both unimodal and multimodal FMs, as well as emerging large language model (LLM) agents. Furthermore, we review standardized datasets, open-source tools, and autonomous experimental platforms that collectively fuel the development and integration of FMs into research workflows. We assess the early successes of foundation models and identify persistent limitations, including challenges in generalizability, interpretability, data imbalance, safety concerns, and limited multimodal fusion. Finally, we articulate future research directions centered on scalable pretraining, continual learning, data governance, and trustworthiness.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Poster: Enhancing GNN Robustness for Network Intrusion Detection via Agent-based Analysis</title>
<link>https://arxiv.org/abs/2506.20806</link>
<guid>https://arxiv.org/abs/2506.20806</guid>
<content:encoded><![CDATA[
arXiv:2506.20806v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) show great promise for Network Intrusion Detection Systems (NIDS), particularly in IoT environments, but suffer performance degradation due to distribution drift and lack robustness against realistic adversarial attacks. Current robustness evaluations often rely on unrealistic synthetic perturbations and lack demonstrations on systematic analysis of different kinds of adversarial attack, which encompass both black-box and white-box scenarios. This work proposes a novel approach to enhance GNN robustness and generalization by employing Large Language Models (LLMs) in an agentic pipeline as simulated cybersecurity expert agents. These agents scrutinize graph structures derived from network flow data, identifying and potentially mitigating suspicious or adversarially perturbed elements before GNN processing. Our experiments, using a framework designed for realistic evaluation and testing with a variety of adversarial attacks including a dataset collected from physical testbed experiments, demonstrate that integrating LLM analysis can significantly improve the resilience of GNN-based NIDS against challenges, showcasing the potential of LLM agent as a complementary layer in intrusion detection architectures.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GPU Kernel Scientist: An LLM-Driven Framework for Iterative Kernel Optimization</title>
<link>https://arxiv.org/abs/2506.20807</link>
<guid>https://arxiv.org/abs/2506.20807</guid>
<content:encoded><![CDATA[
arXiv:2506.20807v1 Announce Type: new 
Abstract: Optimizing GPU kernels for high performance is a complex task, often demanding deep architectural knowledge, extensive profiling, and iterative experimentation. This challenge is amplified when targeting newer or less-documented GPU architectures where traditional development aids are scarce. This paper introduces an LLM-powered "GPU Kernel Scientist," an automated methodology for iteratively refining accelerator kernels.
  Our methodology employs LLMs in a multi-stage, evolutionary process: (a) strategically selecting promising prior code versions as a basis for new iterations; (b) generating hypotheses for optimization experiments, based on existing code and assimilated knowledge from general GPU literature; and (c) autonomously implementing these experiments through code modification and subsequent submission to an external evaluation system, using only observed timing data as performance feedback. We detail how this approach navigates the challenges of the AMD MI300 target architecture and leverages LLMs to compensate for limited domain-specific human expertise.
  Since quantitative results from an ongoing performance competition were embargoed on paper submission date, we present the architectural design, operational workflow, and qualitative insights, highlighting the potential of LLM-driven agents to democratise and accelerate GPU kernel optimization, especially in resource-constrained or rapidly evolving hardware environments.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine</title>
<link>https://arxiv.org/abs/2506.20876</link>
<guid>https://arxiv.org/abs/2506.20876</guid>
<content:encoded><![CDATA[
arXiv:2506.20876v1 Announce Type: new 
Abstract: Technological progress has led to concrete advancements in tasks that were regarded as challenging, such as automatic fact-checking. Interest in adopting these systems for public health and medicine has grown due to the high-stakes nature of medical decisions and challenges in critically appraising a vast and diverse medical literature. Evidence-based medicine connects to every individual, and yet the nature of it is highly technical, rendering the medical literacy of majority users inadequate to sufficiently navigate the domain. Such problems with medical communication ripens the ground for end-to-end fact-checking agents: check a claim against current medical literature and return with an evidence-backed verdict. And yet, such systems remain largely unused. To understand this, we present the first study examining how clinical experts verify real claims from social media by synthesizing medical evidence. In searching for this upper-bound, we reveal fundamental challenges in end-to-end fact-checking when applied to medicine: Difficulties connecting claims in the wild to scientific evidence in the form of clinical trials; ambiguities in underspecified claims mixed with mismatched intentions; and inherently subjective veracity labels. We argue that fact-checking should be approached and evaluated as an interactive communication problem, rather than an end-to-end process.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Complex Model Transformations by Reinforcement Learning with Uncertain Human Guidance</title>
<link>https://arxiv.org/abs/2506.20883</link>
<guid>https://arxiv.org/abs/2506.20883</guid>
<content:encoded><![CDATA[
arXiv:2506.20883v1 Announce Type: new 
Abstract: Model-driven engineering problems often require complex model transformations (MTs), i.e., MTs that are chained in extensive sequences. Pertinent examples of such problems include model synchronization, automated model repair, and design space exploration. Manually developing complex MTs is an error-prone and often infeasible process. Reinforcement learning (RL) is an apt way to alleviate these issues. In RL, an autonomous agent explores the state space through trial and error to identify beneficial sequences of actions, such as MTs. However, RL methods exhibit performance issues in complex problems. In these situations, human guidance can be of high utility. In this paper, we present an approach and technical framework for developing complex MT sequences through RL, guided by potentially uncertain human advice. Our framework allows user-defined MTs to be mapped onto RL primitives, and executes them as RL programs to find optimal MT sequences. Our evaluation shows that human guidance, even if uncertain, substantially improves RL performance, and results in more efficient development of complex MTs. Through a trade-off between the certainty and timeliness of human advice, our method takes a step towards RL-driven human-in-the-loop engineering methods.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smoothness Meets Autobidding: Tight Price of Anarchy Bounds for Simultaneous First-Price Auctions</title>
<link>https://arxiv.org/abs/2506.20908</link>
<guid>https://arxiv.org/abs/2506.20908</guid>
<content:encoded><![CDATA[
arXiv:2506.20908v1 Announce Type: new 
Abstract: Online advertising systems have recently transitioned to autobidding, enabling advertisers to delegate bidding decisions to automated agents. Each advertiser directs their agent to optimize a valuation-dependent objective subject to return-on-investment (ROI) or budget constraints. Given their relevance, there has been a surge in literature studying the liquid welfare price of anarchy (POA) of core auction formats in autobidding, among which simultaneous first-price auctions (FPA). These models capture a large range of heterogeneous agent behaviors, requiring advanced proofs to derive tight POA bounds. Recently, Deng et al. (NeurIPS 2024) showed that the POA of FPA for mixed autobidders (i.e., value and utility maximizers) under ROI is 2.18 for additive valuations.
  We extend the smoothness framework of Syrgkanis and Tardos (STOC 2013) to autobidding. A key contribution is a technique to balance smoothness parameters across heterogeneous agent types. Finding the best POA bound reduces to solving a POA-revealing mathematical program. Our approach has three strengths: (1) Simplicity: We prove smoothness for single-item FPA. Results for simultaneous FPA follow via our theorem. For example, by showing smoothness for value and utility maximizers, we obtain the tight POA of 2.18 for mixed autobidding. (2) Extendibility: Our Extension Theorem adapts to simultaneous FPA with reserve prices and agents with fractionally subadditive valuations and heterogeneous payment sensitivities and target ROI parameters. We establish the first (mostly) tight POA bounds for several models beyond the autobidding state of the art. (3) Generality: Our framework bounds the POA of coarse correlated equilibria (CCE), which arise when hybrid agents employ regret-minimizing algorithms. Building on Kolumbus and Nisan (WWW 2022), we show that CCE from such agents have properties that keep their POA low.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FaSTA$^*$: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient Multi-turn Image Editing</title>
<link>https://arxiv.org/abs/2506.20911</link>
<guid>https://arxiv.org/abs/2506.20911</guid>
<content:encoded><![CDATA[
arXiv:2506.20911v1 Announce Type: new 
Abstract: We develop a cost-efficient neurosymbolic agent to address challenging multi-turn image editing tasks such as "Detect the bench in the image while recoloring it to pink. Also, remove the cat for a clearer view and recolor the wall to yellow.'' It combines the fast, high-level subtask planning by large language models (LLMs) with the slow, accurate, tool-use, and local A$^*$ search per subtask to find a cost-efficient toolpath -- a sequence of calls to AI tools. To save the cost of A$^*$ on similar subtasks, we perform inductive reasoning on previously successful toolpaths via LLMs to continuously extract/refine frequently used subroutines and reuse them as new tools for future tasks in an adaptive fast-slow planning, where the higher-level subroutines are explored first, and only when they fail, the low-level A$^*$ search is activated. The reusable symbolic subroutines considerably save exploration cost on the same types of subtasks applied to similar images, yielding a human-like fast-slow toolpath agent "FaSTA$^*$'': fast subtask planning followed by rule-based subroutine selection per subtask is attempted by LLMs at first, which is expected to cover most tasks, while slow A$^*$ search is only triggered for novel and challenging subtasks. By comparing with recent image editing approaches, we demonstrate FaSTA$^*$ is significantly more computationally efficient while remaining competitive with the state-of-the-art baseline in terms of success rate.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-guided Chemical Process Optimization with a Multi-Agent Approach</title>
<link>https://arxiv.org/abs/2506.20921</link>
<guid>https://arxiv.org/abs/2506.20921</guid>
<content:encoded><![CDATA[
arXiv:2506.20921v1 Announce Type: new 
Abstract: Chemical process optimization is crucial to maximize production efficiency and economic performance. Traditional methods, including gradient-based solvers, evolutionary algorithms, and parameter grid searches, become impractical when operating constraints are ill-defined or unavailable, requiring engineers to rely on subjective heuristics to estimate feasible parameter ranges. To address this constraint definition bottleneck, we present a multi-agent framework of large language model (LLM) agents that autonomously infer operating constraints from minimal process descriptions, then collaboratively guide optimization using the inferred constraints. Our AutoGen-based agentic framework employs OpenAI's o3 model, with specialized agents for constraint generation, parameter validation, simulation execution, and optimization guidance. Through two phases - autonomous constraint generation using embedded domain knowledge, followed by iterative multi-agent optimization - the framework eliminates the need for predefined operational bounds. Validated on the hydrodealkylation process across cost, yield, and yield-to-cost ratio metrics, the framework demonstrated competitive performance with conventional optimization methods while achieving better computational efficiency, requiring fewer iterations to converge. Our approach converged in under 20 minutes, achieving a 31-fold speedup over grid search. Beyond computational efficiency, the framework's reasoning-guided search demonstrates sophisticated process understanding, correctly identifying utility trade-offs, and applying domain-informed heuristics. This approach shows significant potential for optimization scenarios where operational constraints are poorly characterized or unavailable, particularly for emerging processes and retrofit applications.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParEval-Repo: A Benchmark Suite for Evaluating LLMs with Repository-level HPC Translation Tasks</title>
<link>https://arxiv.org/abs/2506.20938</link>
<guid>https://arxiv.org/abs/2506.20938</guid>
<content:encoded><![CDATA[
arXiv:2506.20938v1 Announce Type: new 
Abstract: GPGPU architectures have become significantly diverse in recent years, which has led to an emergence of a variety of specialized programming models and software stacks to support them. While portable execution models exist, they still require significant developer effort to port to and optimize for different hardware architectures. Recent advances in large language models (LLMs) can help us reduce some of this programmer burden. In this paper, we present a novel benchmark and testing framework, ParEval-Repo, which can be used to evaluate the efficacy of LLM-based approaches in automatically translating entire codebases across GPGPU execution models. ParEval-Repo includes several scientific computing and AI mini-applications in a range of programming models, and levels of repository complexity. We use ParEval-Repo to evaluate a range of state-of-the-art open-source and commercial LLMs, with both a non-agentic and a top-down agentic approach. We assess code generated by the LLMs and approaches in terms of compilability, functional correctness, categories of build errors, and the cost of translation in terms of the number of inference tokens. Our results demonstrate that LLM translation of scientific applications is feasible for small programs but difficulty with generating functional build systems and cross-file dependencies pose challenges in scaling to larger codebases.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation</title>
<link>https://arxiv.org/abs/2506.20949</link>
<guid>https://arxiv.org/abs/2506.20949</guid>
<content:encoded><![CDATA[
arXiv:2506.20949v1 Announce Type: new 
Abstract: Given the growing influence of language model-based agents on high-stakes societal decisions, from public policy to healthcare, ensuring their beneficial impact requires understanding the far-reaching implications of their suggestions. We propose a proof-of-concept framework that projects how model-generated advice could propagate through societal systems on a macroscopic scale over time, enabling more robust alignment. To assess the long-term safety awareness of language models, we also introduce a dataset of 100 indirect harm scenarios, testing models' ability to foresee adverse, non-obvious outcomes from seemingly harmless user prompts. Our approach achieves not only over 20% improvement on the new dataset but also an average win rate exceeding 70% against strong baselines on existing safety benchmarks (AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer agents.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evidence-based diagnostic reasoning with multi-agent copilot for human pathology</title>
<link>https://arxiv.org/abs/2506.20964</link>
<guid>https://arxiv.org/abs/2506.20964</guid>
<content:encoded><![CDATA[
arXiv:2506.20964v1 Announce Type: new 
Abstract: Pathology is experiencing rapid digital transformation driven by whole-slide imaging and artificial intelligence (AI). While deep learning-based computational pathology has achieved notable success, traditional models primarily focus on image analysis without integrating natural language instruction or rich, text-based context. Current multimodal large language models (MLLMs) in computational pathology face limitations, including insufficient training data, inadequate support and evaluation for multi-image understanding, and a lack of autonomous, diagnostic reasoning capabilities. To address these limitations, we introduce PathChat+, a new MLLM specifically designed for human pathology, trained on over 1 million diverse, pathology-specific instruction samples and nearly 5.5 million question answer turns. Extensive evaluations across diverse pathology benchmarks demonstrated that PathChat+ substantially outperforms the prior PathChat copilot, as well as both state-of-the-art (SOTA) general-purpose and other pathology-specific models. Furthermore, we present SlideSeek, a reasoning-enabled multi-agent AI system leveraging PathChat+ to autonomously evaluate gigapixel whole-slide images (WSIs) through iterative, hierarchical diagnostic reasoning, reaching high accuracy on DDxBench, a challenging open-ended differential diagnosis benchmark, while also capable of generating visually grounded, humanly-interpretable summary reports.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RL-Selector: Reinforcement Learning-Guided Data Selection via Redundancy Assessment</title>
<link>https://arxiv.org/abs/2506.21037</link>
<guid>https://arxiv.org/abs/2506.21037</guid>
<content:encoded><![CDATA[
arXiv:2506.21037v1 Announce Type: new 
Abstract: Modern deep architectures often rely on large-scale datasets, but training on these datasets incurs high computational and storage overhead. Real-world datasets often contain substantial redundancies, prompting the need for more data-efficient training paradigms. Data selection has shown promise to mitigate redundancy by identifying the most representative samples, thereby reducing training costs without compromising performance. Existing methods typically rely on static scoring metrics or pretrained models, overlooking the combined effect of selected samples and their evolving dynamics during training. We introduce the concept of epsilon-sample cover, which quantifies sample redundancy based on inter-sample relationships, capturing the intrinsic structure of the dataset. Based on this, we reformulate data selection as a reinforcement learning (RL) process and propose RL-Selector, where a lightweight RL agent optimizes the selection policy by leveraging epsilon-sample cover derived from evolving dataset distribution as a reward signal. Extensive experiments across benchmark datasets and diverse architectures demonstrate that our method consistently outperforms existing state-of-the-art baselines. Models trained with our selected datasets show enhanced generalization performance with improved training efficiency.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GoIRL: Graph-Oriented Inverse Reinforcement Learning for Multimodal Trajectory Prediction</title>
<link>https://arxiv.org/abs/2506.21121</link>
<guid>https://arxiv.org/abs/2506.21121</guid>
<content:encoded><![CDATA[
arXiv:2506.21121v1 Announce Type: new 
Abstract: Trajectory prediction for surrounding agents is a challenging task in autonomous driving due to its inherent uncertainty and underlying multimodality. Unlike prevailing data-driven methods that primarily rely on supervised learning, in this paper, we introduce a novel Graph-oriented Inverse Reinforcement Learning (GoIRL) framework, which is an IRL-based predictor equipped with vectorized context representations. We develop a feature adaptor to effectively aggregate lane-graph features into grid space, enabling seamless integration with the maximum entropy IRL paradigm to infer the reward distribution and obtain the policy that can be sampled to induce multiple plausible plans. Furthermore, conditioned on the sampled plans, we implement a hierarchical parameterized trajectory generator with a refinement module to enhance prediction accuracy and a probability fusion strategy to boost prediction confidence. Extensive experimental results showcase our approach not only achieves state-of-the-art performance on the large-scale Argoverse & nuScenes motion forecasting benchmarks but also exhibits superior generalization abilities compared to existing supervised models.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curriculum-Guided Antifragile Reinforcement Learning for Secure UAV Deconfliction under Observation-Space Attacks</title>
<link>https://arxiv.org/abs/2506.21129</link>
<guid>https://arxiv.org/abs/2506.21129</guid>
<content:encoded><![CDATA[
arXiv:2506.21129v1 Announce Type: new 
Abstract: Reinforcement learning (RL) policies deployed in safety-critical systems, such as unmanned aerial vehicle (UAV) navigation in dynamic airspace, are vulnerable to out-ofdistribution (OOD) adversarial attacks in the observation space. These attacks induce distributional shifts that significantly degrade value estimation, leading to unsafe or suboptimal decision making rendering the existing policy fragile. To address this vulnerability, we propose an antifragile RL framework designed to adapt against curriculum of incremental adversarial perturbations. The framework introduces a simulated attacker which incrementally increases the strength of observation-space perturbations which enables the RL agent to adapt and generalize across a wider range of OOD observations and anticipate previously unseen attacks. We begin with a theoretical characterization of fragility, formally defining catastrophic forgetting as a monotonic divergence in value function distributions with increasing perturbation strength. Building on this, we define antifragility as the boundedness of such value shifts and derive adaptation conditions under which forgetting is stabilized. Our method enforces these bounds through iterative expert-guided critic alignment using Wasserstein distance minimization across incrementally perturbed observations. We empirically evaluate the approach in a UAV deconfliction scenario involving dynamic 3D obstacles. Results show that the antifragile policy consistently outperforms standard and robust RL baselines when subjected to both projected gradient descent (PGD) and GPS spoofing attacks, achieving up to 15% higher cumulative reward and over 30% fewer conflict events. These findings demonstrate the practical and theoretical viability of antifragile reinforcement learning for secure and resilient decision-making in environments with evolving threat scenarios.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Delegates Resolve Fairness Issues in Perpetual Voting with Partial Turnout</title>
<link>https://arxiv.org/abs/2506.21186</link>
<guid>https://arxiv.org/abs/2506.21186</guid>
<content:encoded><![CDATA[
arXiv:2506.21186v1 Announce Type: new 
Abstract: Perpetual voting addresses fairness in sequential collective decision-making by evaluating representational equity over time. However, existing perpetual voting rules rely on full participation and complete approval information, assumptions that rarely hold in practice, where partial turnout is the norm. In this work, we study the integration of Artificial Delegates, preference-learning agents trained to represent absent voters, into perpetual voting systems. We examine how absenteeism affects fairness and representativeness under various voting methods and evaluate the extent to which Artificial Delegates can compensate for missing participation. Our findings indicate that while absenteeism significantly affects fairness, Artificial Delegates reliably mitigate these effects and enhance robustness across diverse scenarios.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Risk-Aware MPPI for Mobile Robots in Crowds via Efficient Monte Carlo Approximations</title>
<link>https://arxiv.org/abs/2506.21205</link>
<guid>https://arxiv.org/abs/2506.21205</guid>
<content:encoded><![CDATA[
arXiv:2506.21205v1 Announce Type: new 
Abstract: Deploying mobile robots safely among humans requires the motion planner to account for the uncertainty in the other agents' predicted trajectories. This remains challenging in traditional approaches, especially with arbitrarily shaped predictions and real-time constraints. To address these challenges, we propose a Dynamic Risk-Aware Model Predictive Path Integral control (DRA-MPPI), a motion planner that incorporates uncertain future motions modelled with potentially non-Gaussian stochastic predictions. By leveraging MPPI's gradient-free nature, we propose a method that efficiently approximates the joint Collision Probability (CP) among multiple dynamic obstacles for several hundred sampled trajectories in real-time via a Monte Carlo (MC) approach. This enables the rejection of samples exceeding a predefined CP threshold or the integration of CP as a weighted objective within the navigation cost function. Consequently, DRA-MPPI mitigates the freezing robot problem while enhancing safety. Real-world and simulated experiments with multiple dynamic obstacles demonstrate DRA-MPPI's superior performance compared to state-of-the-art approaches, including Scenario-based Model Predictive Control (S-MPC), Frenet planner, and vanilla MPPI.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents</title>
<link>https://arxiv.org/abs/2506.21252</link>
<guid>https://arxiv.org/abs/2506.21252</guid>
<content:encoded><![CDATA[
arXiv:2506.21252v1 Announce Type: new 
Abstract: As Multimodal Large Language Models (MLLMs) advance, multimodal agents show promise in real-world tasks like web navigation and embodied intelligence. However, due to limitations in a lack of external feedback, these agents struggle with self-correction and generalization. A promising approach is to use reward models as external feedback, but there is no clear on how to select reward models for agents. Thus, there is an urgent need to build a reward bench targeted at agents. To address these challenges, we propose Agent-RewardBench, a benchmark designed to evaluate reward modeling ability in MLLMs. The benchmark is characterized by three key features: (1) Multiple dimensions and real-world agent scenarios evaluation. It covers perception, planning, and safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the assessment of agent capabilities at the individual steps of a task, providing a more granular view of performance during the planning process; and (3) Appropriately difficulty and high-quality. We carefully sample from 10 diverse models, difficulty control to maintain task challenges, and manual verification to ensure the integrity of the data. Experiments demonstrate that even state-of-the-art multimodal models show limited performance, highlighting the need for specialized training in agent reward modeling. Code is available at github.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ad-Hoc Human-AI Coordination Challenge</title>
<link>https://arxiv.org/abs/2506.21490</link>
<guid>https://arxiv.org/abs/2506.21490</guid>
<content:encoded><![CDATA[
arXiv:2506.21490v1 Announce Type: new 
Abstract: Achieving seamless coordination between AI agents and humans is crucial for real-world applications, yet it remains a significant open challenge. Hanabi is a cooperative card game featuring imperfect information, constrained communication, theory of mind requirements, and coordinated action -- making it an ideal testbed for human-AI coordination. However, its use for human-AI interaction has been limited by the challenges of human evaluation. In this work, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to overcome the constraints of costly and difficult-to-reproduce human evaluations. We develop \textit{human proxy agents} on a large-scale human dataset that serve as robust, cheap, and reproducible human-like evaluation partners in AH2AC2. To encourage the development of data-efficient methods, we open-source a dataset of 3,079 games, deliberately limiting the amount of available human gameplay data. We present baseline results for both two- and three- player Hanabi scenarios. To ensure fair evaluation, we host the proxy agents through a controlled evaluation system rather than releasing them publicly. The code is available at \href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From multi-allocations to allocations, with subadditive valuations</title>
<link>https://arxiv.org/abs/2506.21493</link>
<guid>https://arxiv.org/abs/2506.21493</guid>
<content:encoded><![CDATA[
arXiv:2506.21493v1 Announce Type: new 
Abstract: We consider the problem of fair allocation of $m$ indivisible items to $n$ agents with monotone subadditive valuations. For integer $d \ge 2$, a $d$-multi-allocation is an allocation in which each item is allocated to at most $d$ different agents. We show that $d$-multi-allocations can be transformed into allocations, while not losing much more than a factor of $d$ in the value that each agent receives. One consequence of this result is that for allocation instances with equal entitlements and subadditive valuations, if $\rho$-MMS $d$-multi-allocations exist, then so do $\frac{\rho}{4d}$-MMS allocations. Combined with recent results of Seddighin and Seddighin [EC 2025], this implies the existence of $\Omega(\frac{1}{\log\log n})$-MMS allocations.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge</title>
<link>https://arxiv.org/abs/2506.21506</link>
<guid>https://arxiv.org/abs/2506.21506</guid>
<content:encoded><![CDATA[
arXiv:2506.21506v1 Announce Type: new 
Abstract: Agentic search such as Deep Research systems, where large language models autonomously browse the web, synthesize information, and return comprehensive citation-backed answers, represents a major shift in how users interact with web-scale information. While promising greater efficiency and cognitive offloading, the growing complexity and open-endedness of agentic search have outpaced existing evaluation benchmarks and methodologies, which largely assume short search horizons and static answers. In this paper, we introduce Mind2Web 2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that require real-time web browsing and extensive information synthesis, constructed with over 1,000 hours of human labor. To address the challenge of evaluating time-varying and complex answers, we propose a novel Agent-as-a-Judge framework. Our method constructs task-specific judge agents based on a tree-structured rubric design to automatically assess both answer correctness and source attribution. We conduct a comprehensive evaluation of nine frontier agentic search systems and human performance, along with a detailed error analysis to draw insights for future development. The best-performing system, OpenAI Deep Research, can already achieve 50-70% of human performance while spending half the time, showing a great potential. Altogether, Mind2Web 2 provides a rigorous foundation for developing and benchmarking the next generation of agentic search systems.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PsyLite Technical Report</title>
<link>https://arxiv.org/abs/2506.21536</link>
<guid>https://arxiv.org/abs/2506.21536</guid>
<content:encoded><![CDATA[
arXiv:2506.21536v1 Announce Type: new 
Abstract: With the rapid development of digital technology, AI-driven psychological counseling has gradually become an important research direction in the field of mental health. However, existing models still have deficiencies in dialogue safety, detailed scenario handling, and lightweight deployment. To address these issues, this study proposes PsyLite, a lightweight psychological counseling large language model agent developed based on the base model InternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation data fine-tuning and ORPO preference optimization), PsyLite enhances the model's deep-reasoning ability, psychological counseling ability, and safe dialogue ability. After deployment using Ollama and Open WebUI, a custom workflow is created with Pipelines. An innovative conditional RAG is designed to introduce crosstalk humor elements at appropriate times during psychological counseling to enhance user experience and decline dangerous requests to strengthen dialogue safety. Evaluations show that PsyLite outperforms the baseline models in the Chinese general evaluation (CEval), psychological counseling professional evaluation (CPsyCounE), and dialogue safety evaluation (SafeDialBench), particularly in psychological counseling professionalism (CPsyCounE score improvement of 47.6\%) and dialogue safety (\safe{} score improvement of 2.4\%). Additionally, the model uses quantization technology (GGUF q4\_k\_m) to achieve low hardware deployment (5GB memory is sufficient for operation), providing a feasible solution for psychological counseling applications in resource-constrained environments.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Whole-Body Conditioned Egocentric Video Prediction</title>
<link>https://arxiv.org/abs/2506.21552</link>
<guid>https://arxiv.org/abs/2506.21552</guid>
<content:encoded><![CDATA[
arXiv:2506.21552v1 Announce Type: new 
Abstract: We train models to Predict Ego-centric Video from human Actions (PEVA), given the past video and an action represented by the relative 3D body pose. By conditioning on kinematic pose trajectories, structured by the joint hierarchy of the body, our model learns to simulate how physical human actions shape the environment from a first-person point of view. We train an auto-regressive conditional diffusion transformer on Nymeria, a large-scale dataset of real-world egocentric video and body pose capture. We further design a hierarchical evaluation protocol with increasingly challenging tasks, enabling a comprehensive analysis of the model's embodied prediction and control abilities. Our work represents an initial attempt to tackle the challenges of modeling complex real-world environments and embodied agent behaviors with video prediction from the perspective of a human.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Homogenization of Multi-agent Learning Dynamics in Finite-state Markov Games</title>
<link>https://arxiv.org/abs/2506.21079</link>
<guid>https://arxiv.org/abs/2506.21079</guid>
<content:encoded><![CDATA[
arXiv:2506.21079v1 Announce Type: cross 
Abstract: This paper introduces a new approach for approximating the learning dynamics of multiple reinforcement learning (RL) agents interacting in a finite-state Markov game. The idea is to rescale the learning process by simultaneously reducing the learning rate and increasing the update frequency, effectively treating the agent's parameters as a slow-evolving variable influenced by the fast-mixing game state. Under mild assumptions-ergodicity of the state process and continuity of the updates-we prove the convergence of this rescaled process to an ordinary differential equation (ODE). This ODE provides a tractable, deterministic approximation of the agent's learning dynamics. An implementation of the framework is available at\,: https://github.com/yannKerzreho/MarkovGameApproximation
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance improvement of spatial semantic segmentation with enriched audio features and agent-based error correction for DCASE 2025 Challenge Task 4</title>
<link>https://arxiv.org/abs/2506.21174</link>
<guid>https://arxiv.org/abs/2506.21174</guid>
<content:encoded><![CDATA[
arXiv:2506.21174v1 Announce Type: cross 
Abstract: This technical report presents submission systems for Task 4 of the DCASE 2025 Challenge. This model incorporates additional audio features (spectral roll-off and chroma features) into the embedding feature extracted from the mel-spectral feature to im-prove the classification capabilities of an audio-tagging model in the spatial semantic segmentation of sound scenes (S5) system. This approach is motivated by the fact that mixed audio often contains subtle cues that are difficult to capture with mel-spectrograms alone. Thus, these additional features offer alterna-tive perspectives for the model. Second, an agent-based label correction system is applied to the outputs processed by the S5 system. This system reduces false positives, improving the final class-aware signal-to-distortion ratio improvement (CA-SDRi) metric. Finally, we refine the training dataset to enhance the classi-fication accuracy of low-performing classes by removing irrele-vant samples and incorporating external data. That is, audio mix-tures are generated from a limited number of data points; thus, even a small number of out-of-class data points could degrade model performance. The experiments demonstrate that the submit-ted systems employing these approaches relatively improve CA-SDRi by up to 14.7% compared to the baseline of DCASE 2025 Challenge Task 4.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Learning as Computationally Constrained Reinforcement Learning</title>
<link>https://arxiv.org/abs/2307.04345</link>
<guid>https://arxiv.org/abs/2307.04345</guid>
<content:encoded><![CDATA[
arXiv:2307.04345v3 Announce Type: replace 
Abstract: An agent that efficiently accumulates knowledge to develop increasingly sophisticated skills over a long lifetime could advance the frontier of artificial intelligence capabilities. The design of such agents, which remains a long-standing challenge of artificial intelligence, is addressed by the subject of continual learning. This monograph clarifies and formalizes concepts of continual learning, introducing a framework and set of tools to stimulate further research.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Rank for Multiple Retrieval-Augmented Models through Iterative Utility Maximization</title>
<link>https://arxiv.org/abs/2410.09942</link>
<guid>https://arxiv.org/abs/2410.09942</guid>
<content:encoded><![CDATA[
arXiv:2410.09942v2 Announce Type: replace 
Abstract: This paper investigates the design of a unified search engine to serve multiple retrieval-augmented generation (RAG) agents, each with a distinct task, backbone large language model (LLM), and RAG strategy. We introduce an iterative approach where the search engine generates retrieval results for the RAG agents and gathers feedback on the quality of the retrieved documents during an offline phase. This feedback is then used to iteratively optimize the search engine using an expectation-maximization algorithm, with the goal of maximizing each agent's utility function. Additionally, we adapt this to an online setting, allowing the search engine to refine its behavior based on real-time individual agents feedback to better serve the results for each of them. Experiments on datasets from the Knowledge-Intensive Language Tasks (KILT) benchmark demonstrates that our approach significantly on average outperforms baselines across 18 RAG models. We demonstrate that our method effectively ``personalizes'' the retrieval for each RAG agent based on the collected feedback. Finally, we provide a comprehensive ablation study to explore various aspects of our method.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Troublemaker with Contagious Jailbreak Makes Chaos in Honest Towns</title>
<link>https://arxiv.org/abs/2410.16155</link>
<guid>https://arxiv.org/abs/2410.16155</guid>
<content:encoded><![CDATA[
arXiv:2410.16155v2 Announce Type: replace 
Abstract: With the development of large language models, they are widely used as agents in various fields. A key component of agents is memory, which stores vital information but is susceptible to jailbreak attacks. Existing research mainly focuses on single-agent attacks and shared memory attacks. However, real-world scenarios often involve independent memory. In this paper, we propose the Troublemaker Makes Chaos in Honest Town (TMCHT) task, a large-scale, multi-agent, multi-topology text-based attack evaluation framework. TMCHT involves one attacker agent attempting to mislead an entire society of agents. We identify two major challenges in multi-agent attacks: (1) Non-complete graph structure, (2) Large-scale systems. We attribute these challenges to a phenomenon we term toxicity disappearing. To address these issues, we propose an Adversarial Replication Contagious Jailbreak (ARCJ) method, which optimizes the retrieval suffix to make poisoned samples more easily retrieved and optimizes the replication suffix to make poisoned samples have contagious ability. We demonstrate the superiority of our approach in TMCHT, with 23.51%, 18.95%, and 52.93% improvements in line topology, star topology, and 100-agent settings. Encourage community attention to the security of multi-agent systems.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SceneGenAgent: Precise Industrial Scene Generation with Coding Agent</title>
<link>https://arxiv.org/abs/2410.21909</link>
<guid>https://arxiv.org/abs/2410.21909</guid>
<content:encoded><![CDATA[
arXiv:2410.21909v3 Announce Type: replace 
Abstract: The modeling of industrial scenes is essential for simulations in industrial manufacturing. While large language models (LLMs) have shown significant progress in generating general 3D scenes from textual descriptions, generating industrial scenes with LLMs poses a unique challenge due to their demand for precise measurements and positioning, requiring complex planning over spatial arrangement. To address this challenge, we introduce SceneGenAgent, an LLM-based agent for generating industrial scenes through C# code. SceneGenAgent ensures precise layout planning through a structured and calculable format, layout verification, and iterative refinement to meet the quantitative requirements of industrial scenarios. Experiment results demonstrate that LLMs powered by SceneGenAgent exceed their original performance, reaching up to 81.0% success rate in real-world industrial scene generation tasks and effectively meeting most scene generation requirements. To further enhance accessibility, we construct SceneInstruct, a dataset designed for fine-tuning open-source LLMs to integrate into SceneGenAgent. Experiments show that fine-tuning open-source LLMs on SceneInstruct yields significant performance improvements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our code and data are available at https://github.com/THUDM/SceneGenAgent .
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WiS Platform: Enhancing Evaluation of LLM-Based Multi-Agent Systems Through Game-Based Analysis</title>
<link>https://arxiv.org/abs/2412.03359</link>
<guid>https://arxiv.org/abs/2412.03359</guid>
<content:encoded><![CDATA[
arXiv:2412.03359v2 Announce Type: replace 
Abstract: Recent advancements in autonomous multi-agent systems (MAS) based on large language models (LLMs) have enhanced the application scenarios and improved the capability of LLMs to handle complex tasks. Despite demonstrating effectiveness, existing studies still evidently struggle to evaluate, analysis, and reproducibility of LLM-based MAS. In this paper, to facilitate the research on LLM-based MAS, we introduce an open, scalable, and real-time updated platform for accessing and analyzing the LLM-based MAS based on the games Who is Spy?" (WiS). Our platform is featured with three main worths: (1) a unified model evaluate interface that supports models available on Hugging Face; (2) real-time updated leaderboard for model evaluation; (3) a comprehensive evaluation covering game-winning rates, attacking, defense strategies, and reasoning of LLMs. To rigorously test WiS, we conduct extensive experiments coverage of various open- and closed-source LLMs, we find that different agents exhibit distinct and intriguing behaviors in the game. The experimental results demonstrate the effectiveness and efficiency of our platform in evaluating LLM-based MAS. Our platform and its documentation are publicly available at https://whoisspy.ai/.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chatbot apologies: Beyond bullshit</title>
<link>https://arxiv.org/abs/2501.09910</link>
<guid>https://arxiv.org/abs/2501.09910</guid>
<content:encoded><![CDATA[
arXiv:2501.09910v2 Announce Type: replace 
Abstract: Apologies serve essential functions for moral agents such as expressing remorse, taking responsibility, and repairing trust. LLM-based chatbots routinely produce output that has the linguistic form of an apology. However, they do this simply because they are echoing the kinds of things that humans say. Moreover, there are reasons to think that chatbots are not the kind of linguistic or moral agents capable of apology. To put the point bluntly: Chatbot apologies are bullshit. This paper explores this concern and develops it beyond the epithet, drawing on the nature of morally serious apologies, the linguistic agency required to perform them, and the moral agency required for them to matter. We conclude by considering some consequences for how chatbots should be designed and how we ought to think about them.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Markets with Heterogeneous Agents: Dynamics and Survival of Bayesian vs. No-Regret Learners</title>
<link>https://arxiv.org/abs/2502.08597</link>
<guid>https://arxiv.org/abs/2502.08597</guid>
<content:encoded><![CDATA[
arXiv:2502.08597v2 Announce Type: replace 
Abstract: We analyze the performance of heterogeneous learning agents in asset markets with stochastic payoffs. Our main focus is on comparing Bayesian learners and no-regret learners who compete in markets and identifying the conditions under which each approach is more effective. Surprisingly, we find that low regret is not sufficient for survival: an agent can have regret as low as $O(\log T)$ but still vanish when competing against a Bayesian with a finite prior and any positive prior probability on the correct model. On the other hand, we show that Bayesian learning is fragile, while no-regret learning requires less knowledge of the environment and is therefore more robust. Motivated by the strengths and weaknesses of both approaches, we propose a balanced strategy for utilizing Bayesian updates that improves robustness and adaptability to distribution shifts, providing a step toward a best-of-both-worlds learning approach. The method is general, efficient, and easy to implement. Finally, we formally establish the relationship between the notions of survival and market dominance studied in economics and the framework of regret minimization, thus bridging these theories. More broadly, our work contributes to the understanding of dynamics with heterogeneous types of learning agents and their impact on markets.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High Temporal Consistency through Semantic Similarity Propagation in Semi-Supervised Video Semantic Segmentation for Autonomous Flight</title>
<link>https://arxiv.org/abs/2503.15676</link>
<guid>https://arxiv.org/abs/2503.15676</guid>
<content:encoded><![CDATA[
arXiv:2503.15676v2 Announce Type: replace 
Abstract: Semantic segmentation from RGB cameras is essential to the perception of autonomous flying vehicles. The stability of predictions through the captured videos is paramount to their reliability and, by extension, to the trustworthiness of the agents. In this paper, we propose a lightweight video semantic segmentation approach-suited to onboard real-time inference-achieving high temporal consistency on aerial data through Semantic Similarity Propagation across frames. SSP temporally propagates the predictions of an efficient image segmentation model with global registration alignment to compensate for camera movements. It combines the current estimation and the prior prediction with linear interpolation using weights computed from the features similarities of the two frames. Because data availability is a challenge in this domain, we propose a consistency-aware Knowledge Distillation training procedure for sparsely labeled datasets with few annotations. Using a large image segmentation model as a teacher to train the efficient SSP, we leverage the strong correlations between labeled and unlabeled frames in the same training videos to obtain high-quality supervision on all frames. KD-SSP obtains a significant temporal consistency increase over the base image segmentation model of 12.5% and 6.7% TC on UAVid and RuralScapes respectively, with higher accuracy and comparable inference speed. On these aerial datasets, KD-SSP provides a superior segmentation quality and inference speed trade-off than other video methods proposed for general applications and shows considerably higher consistency. Project page: https://github.com/FraunhoferIVI/SSP.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Will LLMs be Professional at Fund Investment? DeepFund: A Live Arena Perspective</title>
<link>https://arxiv.org/abs/2503.18313</link>
<guid>https://arxiv.org/abs/2503.18313</guid>
<content:encoded><![CDATA[
arXiv:2503.18313v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities across various domains, but their effectiveness in financial decision-making remains inadequately evaluated. Current benchmarks primarily assess LLMs' understanding on financial documents rather than the ability to manage assets or dig out trading opportunities in dynamic market conditions. Despite the release of new benchmarks for evaluating diversified tasks on the financial domain, we identified four major problems in these benchmarks, which are data leakage, navel-gazing, over-intervention, and maintenance-hard. To pave the research gap, we introduce DeepFund, a comprehensive arena platform for evaluating LLM-based trading strategies in a live environment. Our approach implements a multi-agent framework where they serve as multiple key roles that realize the real-world investment decision processes. Moreover, we provide a web interface that visualizes LLMs' performance with fund investment metrics across different market conditions, enabling detailed comparative analysis. Through DeepFund, we aim to provide a more realistic and fair assessment on LLM's capabilities in fund investment, offering diversified insights and revealing their potential applications in real-world financial markets. Our code is publicly available at https://github.com/HKUSTDial/DeepFund.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Adaptive Memory-Based Optimization for Enhanced Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2504.05312</link>
<guid>https://arxiv.org/abs/2504.05312</guid>
<content:encoded><![CDATA[
arXiv:2504.05312v2 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG), by integrating non-parametric knowledge from external knowledge bases into models, has emerged as a promising approach to enhancing response accuracy while mitigating factual errors and hallucinations. This method has been widely applied in tasks such as Question Answering (QA). However, existing RAG methods struggle with open-domain QA tasks because they perform independent retrieval operations and directly incorporate the retrieved information into generation without maintaining a summarizing memory or using adaptive retrieval strategies, leading to noise from redundant information and insufficient information integration. To address these challenges, we propose Adaptive memory-based optimization for enhanced RAG (Amber) for open-domain QA tasks, which comprises an Agent-based Memory Updater, an Adaptive Information Collector, and a Multi-granular Content Filter, working together within an iterative memory updating paradigm. Specifically, Amber integrates and optimizes the language model's memory through a multi-agent collaborative approach, ensuring comprehensive knowledge integration from previous retrieval steps. It dynamically adjusts retrieval queries and decides when to stop retrieval based on the accumulated knowledge, enhancing retrieval efficiency and effectiveness. Additionally, it reduces noise by filtering irrelevant content at multiple levels, retaining essential information to improve overall model performance. We conduct extensive experiments on several open-domain QA datasets, and the results demonstrate the superiority and effectiveness of our method and its components. The source code is available \footnote{https://anonymous.4open.science/r/Amber-B203/}.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMPACT: Behavioral Intention-aware Multimodal Trajectory Prediction with Adaptive Context Trimming</title>
<link>https://arxiv.org/abs/2504.09103</link>
<guid>https://arxiv.org/abs/2504.09103</guid>
<content:encoded><![CDATA[
arXiv:2504.09103v2 Announce Type: replace 
Abstract: While most prior research has focused on improving the precision of multimodal trajectory predictions, the explicit modeling of multimodal behavioral intentions (e.g., yielding, overtaking) remains relatively underexplored. This paper proposes a unified framework that jointly predicts both behavioral intentions and trajectories to enhance prediction accuracy, interpretability, and efficiency. Specifically, we employ a shared context encoder for both intention and trajectory predictions, thereby reducing structural redundancy and information loss. Moreover, we address the lack of ground-truth behavioral intention labels in mainstream datasets (Waymo, Argoverse) by auto-labeling these datasets, thus advancing the community's efforts in this direction. We further introduce a vectorized occupancy prediction module that infers the probability of each map polyline being occupied by the target vehicle's future trajectory. By leveraging these intention and occupancy prediction priors, our method conducts dynamic, modality-dependent pruning of irrelevant agents and map polylines in the decoding stage, effectively reducing computational overhead and mitigating noise from non-critical elements. Our approach ranks first among LiDAR-free methods on the Waymo Motion Dataset and achieves first place on the Waymo Interactive Prediction Dataset. Remarkably, even without model ensembling, our single-model framework improves the soft mean average precision (softmAP) by 10 percent compared to the second-best method in the Waymo Interactive Prediction Leaderboard. Furthermore, the proposed framework has been successfully deployed on real vehicles, demonstrating its practical effectiveness in real-world applications.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Human-AI Coordination through Online Adversarial Training and Generative Models</title>
<link>https://arxiv.org/abs/2504.15457</link>
<guid>https://arxiv.org/abs/2504.15457</guid>
<content:encoded><![CDATA[
arXiv:2504.15457v3 Announce Type: replace 
Abstract: Being able to cooperate with new people is an important component of many economically valuable AI tasks, from household robotics to autonomous driving. However, generalizing to novel humans requires training on data that captures the diversity of human behaviors. Adversarial training is a promising method that allows dynamic data generation and ensures that agents are robust. It creates a feedback loop where the agent's performance influences the generation of new adversarial data, which can be used immediately to train the agent. However, adversarial training is difficult to apply in a cooperative task; how can we train an adversarial cooperator? We propose a novel strategy that combines a pretrained generative model to simulate valid cooperative agent policies with adversarial training to maximize regret. We call our method GOAT: Generative Online Adversarial Training. In this framework, the GOAT dynamically searches the latent space of the generative model for coordination strategies where the learning policy, the Cooperator agent, underperforms. GOAT enables better generalization by exposing the Cooperator to various challenging interaction scenarios. We maintain realistic coordination strategies by keeping the generative model frozen, thus avoiding adversarial exploitation. We evaluate GOAT with real human partners, and the results demonstrate state of the art performance on the Overcooked benchmark, highlighting its effectiveness in generalizing to diverse human behaviors.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model-Powered Agent for C to Rust Code Translation</title>
<link>https://arxiv.org/abs/2505.15858</link>
<guid>https://arxiv.org/abs/2505.15858</guid>
<content:encoded><![CDATA[
arXiv:2505.15858v2 Announce Type: replace 
Abstract: The C programming language has been foundational in building system-level software. However, its manual memory management model frequently leads to memory safety issues. In response, a modern system programming language, Rust, has emerged as a memory-safe alternative. Moreover, automating the C-to-Rust translation empowered by the rapid advancements of the generative capabilities of LLMs is gaining growing interest for large volumes of legacy C code. Despite some success, existing LLM-based approaches have constrained the role of LLMs to static prompt-response behavior and have not explored their agentic problem-solving capability. Applying the LLM agentic capability for the C-to-Rust translation introduces distinct challenges, as this task differs from the traditional LLM agent applications, such as math or commonsense QA domains. First, the scarcity of parallel C-to-Rust datasets hinders the retrieval of suitable code translation exemplars for in-context learning. Second, unlike math or commonsense QA, the intermediate steps required for C-to-Rust are not well-defined. Third, it remains unclear how to organize and cascade these intermediate steps to construct a correct translation trajectory. To address these challenges in the C-to-Rust translation, we propose a novel intermediate step, the Virtual Fuzzing-based equivalence Test (VFT), and an agentic planning framework, the LLM-powered Agent for C-to-Rust code translation (LAC2R). The VFT guides LLMs to identify input arguments that induce divergent behaviors between an original C function and its Rust counterpart and to generate informative diagnoses to refine the unsafe Rust code. LAC2R uses the MCTS to systematically organize the LLM-induced intermediate steps for correct translation. We experimentally demonstrated that LAC2R effectively conducts C-to-Rust translation on large-scale, real-world benchmarks.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structuring the Unstructured: A Multi-Agent System for Extracting and Querying Financial KPIs and Guidance</title>
<link>https://arxiv.org/abs/2505.19197</link>
<guid>https://arxiv.org/abs/2505.19197</guid>
<content:encoded><![CDATA[
arXiv:2505.19197v3 Announce Type: replace 
Abstract: Extracting structured and quantitative insights from unstructured financial filings is essential in investment research, yet remains time-consuming and resource-intensive. Conventional approaches in practice rely heavily on labor-intensive manual processes, limiting scalability and delaying the research workflow. In this paper, we propose an efficient and scalable method for accurately extracting quantitative insights from unstructured financial documents, leveraging a multi-agent system composed of large language models. Our proposed multi-agent system consists of two specialized agents: the \emph{Extraction Agent} and the \emph{Text-to-SQL Agent}. The \textit{Extraction Agent} automatically identifies key performance indicators from unstructured financial text, standardizes their formats, and verifies their accuracy. On the other hand, the \textit{Text-to-SQL Agent} generates executable SQL statements from natural language queries, allowing users to access structured data accurately without requiring familiarity with the database schema. Through experiments, we demonstrate that our proposed system effectively transforms unstructured text into structured data accurately and enables precise retrieval of key information. First, we demonstrate that our system achieves approximately 95\% accuracy in transforming financial filings into structured data, matching the performance level typically attained by human annotators. Second, in a human evaluation of the retrieval task -- where natural language queries are used to search information from structured data -- 91\% of the responses were rated as correct by human evaluators. In both evaluations, our system generalizes well across financial document types, consistently delivering reliable performance.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>xChemAgents: Agentic AI for Explainable Quantum Chemistry</title>
<link>https://arxiv.org/abs/2505.20574</link>
<guid>https://arxiv.org/abs/2505.20574</guid>
<content:encoded><![CDATA[
arXiv:2505.20574v2 Announce Type: replace 
Abstract: Recent progress in multimodal graph neural networks has demonstrated that augmenting atomic XYZ geometries with textual chemical descriptors can enhance predictive accuracy across a range of electronic and thermodynamic properties. However, naively appending large sets of heterogeneous descriptors often degrades performance on tasks sensitive to molecular shape or symmetry, and undermines interpretability. xChemAgents proposes a cooperative agent framework that injects physics-aware reasoning into multimodal property prediction. xChemAgents comprises two language-model-based agents: a Selector, which adaptively identifies a sparse, weighted subset of descriptors relevant to each target, and provides a natural language rationale; and a Validator, which enforces physical constraints such as unit consistency and scaling laws through iterative dialogue. On standard benchmark datasets, xChemAgents achieves up to a 22% reduction in mean absolute error over the state-of-the-art baselines, while producing faithful, human-interpretable explanations. Experiment results highlight the potential of cooperative, self-verifying agents to enhance both accuracy and transparency in foundation-model-driven materials science. The implementation and accompanying dataset are available at https://github.com/KurbanIntelligenceLab/xChemAgents.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TracLLM: A Generic Framework for Attributing Long Context LLMs</title>
<link>https://arxiv.org/abs/2506.04202</link>
<guid>https://arxiv.org/abs/2506.04202</guid>
<content:encoded><![CDATA[
arXiv:2506.04202v3 Announce Type: replace 
Abstract: Long context large language models (LLMs) are deployed in many real-world applications such as RAG, agent, and broad LLM-integrated applications. Given an instruction and a long context (e.g., documents, PDF files, webpages), a long context LLM can generate an output grounded in the provided context, aiming to provide more accurate, up-to-date, and verifiable outputs while reducing hallucinations and unsupported claims. This raises a research question: how to pinpoint the texts (e.g., sentences, passages, or paragraphs) in the context that contribute most to or are responsible for the generated output by an LLM? This process, which we call context traceback, has various real-world applications, such as 1) debugging LLM-based systems, 2) conducting post-attack forensic analysis for attacks (e.g., prompt injection attack, knowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources to enhance the trust of users towards outputs generated by LLMs. When applied to context traceback for long context LLMs, existing feature attribution methods such as Shapley have sub-optimal performance and/or incur a large computational cost. In this work, we develop TracLLM, the first generic context traceback framework tailored to long context LLMs. Our framework can improve the effectiveness and efficiency of existing feature attribution methods. To improve the efficiency, we develop an informed search based algorithm in TracLLM. We also develop contribution score ensemble/denoising techniques to improve the accuracy of TracLLM. Our evaluation results show TracLLM can effectively identify texts in a long context that lead to the output of an LLM. Our code and data are at: https://github.com/Wang-Yanting/TracLLM.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Passivity Analysis for Nonlinear Consensus on Balanced Digraphs</title>
<link>https://arxiv.org/abs/2411.05933</link>
<guid>https://arxiv.org/abs/2411.05933</guid>
<content:encoded><![CDATA[
arXiv:2411.05933v3 Announce Type: replace-cross 
Abstract: This work deals with the output consensus problem for multiagent systems over balanced digraphs by passivity analysis. As the standard diffusive coupling structure only models the undirected interconnection, we propose a general approach capable of processing directed coupling and performing passivity analysis. To mitigate the complexity arising from the nonlinearity and directed interconnections, we reformulate the output consensus problem as a convergence analysis on a submanifold. We provide passivity analysis and establish a sufficient condition based on passivity for achieving output agreement in multi-agent systems over balanced digraphs. The results are supported by a numerical example.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prover Agent: An Agent-based Framework for Formal Mathematical Proofs</title>
<link>https://arxiv.org/abs/2506.19923</link>
<guid>https://arxiv.org/abs/2506.19923</guid>
<content:encoded><![CDATA[
arXiv:2506.19923v1 Announce Type: new 
Abstract: We present Prover Agent, a novel AI agent for automated theorem proving that integrates large language models (LLMs) with a formal proof assistant, Lean. Prover Agent coordinates an informal reasoning LLM, a formal prover model, and feedback from Lean while also generating auxiliary lemmas to assist in discovering the overall proof strategy. It achieves an 86.1% success rate on the MiniF2F benchmark, establishing a new state-of-the-art among methods using small language models (SLMs) with a much lower sample budget than previous approaches. We also present case studies illustrating how these generated lemmas contribute to solving challenging problems.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRACED: Transition-aware Regret Approximation with Co-learnability for Environment Design</title>
<link>https://arxiv.org/abs/2506.19997</link>
<guid>https://arxiv.org/abs/2506.19997</guid>
<content:encoded><![CDATA[
arXiv:2506.19997v1 Announce Type: new 
Abstract: Generalizing deep reinforcement learning agents to unseen environments remains a significant challenge. One promising solution is Unsupervised Environment Design (UED), a co-evolutionary framework in which a teacher adaptively generates tasks with high learning potential, while a student learns a robust policy from this evolving curriculum. Existing UED methods typically measure learning potential via regret, the gap between optimal and current performance, approximated solely by value-function loss. Building on these approaches, we introduce the transition prediction error as an additional term in our regret approximation. To capture how training on one task affects performance on others, we further propose a lightweight metric called co-learnability. By combining these two measures, we present Transition-aware Regret Approximation with Co-learnability for Environment Design (TRACED). Empirical evaluations show that TRACED yields curricula that improve zero-shot generalization across multiple benchmarks while requiring up to 2x fewer environment interactions than strong baselines. Ablation studies confirm that the transition prediction error drives rapid complexity ramp-up and that co-learnability delivers additional gains when paired with the transition prediction error. These results demonstrate how refined regret approximation and explicit modeling of task relationships can be leveraged for sample-efficient curriculum design in UED.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation</title>
<link>https://arxiv.org/abs/2506.19998</link>
<guid>https://arxiv.org/abs/2506.19998</guid>
<content:encoded><![CDATA[
arXiv:2506.19998v1 Announce Type: new 
Abstract: REST APIs play important roles in enriching the action space of web agents, yet most API-based agents rely on curated and uniform toolsets that do not reflect the complexity of real-world APIs. Building tool-using agents for arbitrary domains remains a major challenge, as it requires reading unstructured API documentation, testing APIs and inferring correct parameters. We propose Doc2Agent, a scalable pipeline to build agents that can call Python-based tools generated from API documentation. Doc2Agent generates executable tools from API documentations and iteratively refines them using a code agent. We evaluate our approach on real-world APIs, WebArena APIs, and research APIs, producing validated tools. We achieved a 55\% relative performance improvement with 90\% lower cost compared to direct API calling on WebArena benchmark. A domain-specific agent built for glycomaterial science further demonstrates the pipeline's adaptability to complex, knowledge-rich tasks. Doc2Agent offers a generalizable solution for building tool agents from unstructured API documentation at scale.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QHackBench: Benchmarking Large Language Models for Quantum Code Generation Using PennyLane Hackathon Challenges</title>
<link>https://arxiv.org/abs/2506.20008</link>
<guid>https://arxiv.org/abs/2506.20008</guid>
<content:encoded><![CDATA[
arXiv:2506.20008v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have demonstrated strong potential in code generation, yet their effectiveness in quantum computing remains underexplored. This paper benchmarks LLMs for PennyLane-based quantum code generation using real-world challenges from the Quantum Hackathon (QHack). We introduce QHackBench, a novel benchmark dataset derived from QHack competitions, and evaluate model performance under vanilla prompting and Retrieval-Augmented Generation (RAG). Our structured evaluation framework assesses functional correctness, syntactic validity, and execution success across varying challenge difficulties. Results indicate that RAG-enhanced models, supplemented with an augmented PennyLane dataset, approximately generate similar results as the standard prompting, particularly in complex quantum algorithms. Additionally, we introduce a multi-agent evaluation pipeline that iteratively refines incorrect solutions, further enhancing execution success rates. To foster further research, we commit to publicly releasing QHackBench, along with our evaluation framework and experimental results, enabling continued advancements in AI-assisted quantum programming.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Polynomial-Time Approximation Schemes via Utility Alignment: Unit-Demand Pricing and More</title>
<link>https://arxiv.org/abs/2506.20030</link>
<guid>https://arxiv.org/abs/2506.20030</guid>
<content:encoded><![CDATA[
arXiv:2506.20030v1 Announce Type: new 
Abstract: This paper derives polynomial-time approximation schemes for several NP-hard stochastic optimization problems from the algorithmic mechanism design and operations research literatures. The problems we consider involve a principal or seller optimizing with respect to a subsequent choice by an agent or buyer. These include posted pricing for a unit-demand buyer with independent values (Chawla et al., 2007, Cai and Daskalakis, 2011), assortment optimization with independent utilities (Talluri and van Ryzin, 2004), and delegated choice (Khodabakhsh et al., 2024). Our results advance the state of the art for each of these problems. For unit-demand pricing with discrete distributions, our multiplicative PTAS improves on the additive PTAS of Cai and Daskalakis, and we additionally give a PTAS for the unbounded regular case, improving on the latter paper's QPTAS. For assortment optimization, no constant approximation was previously known. For delegated choice, we improve on both the $3$-approximation for the case with no outside option and the super-constant-approximation with an outside option.
  A key technical insight driving our results is an economically meaningful property we term utility alignment. Informally, a problem is utility aligned if, at optimality, the principal derives most of their utility from realizations where the agent's utility is also high. Utility alignment allows the algorithm designer to focus on maximizing performance on realizations with high agent utility, which is often an algorithmically simpler task. We prove utility alignment results for all the problems mentioned above, including strong results for unit-demand pricing and delegation, as well as a weaker but very broad guarantee that holds for many other problems under very mild conditions.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Generation of Diverse Courses of Actions for Multi-Agent Operations using Binary Optimization and Graph Learning</title>
<link>https://arxiv.org/abs/2506.20031</link>
<guid>https://arxiv.org/abs/2506.20031</guid>
<content:encoded><![CDATA[
arXiv:2506.20031v1 Announce Type: new 
Abstract: Operations in disaster response, search \& rescue, and military missions that involve multiple agents demand automated processes to support the planning of the courses of action (COA). Moreover, traverse-affecting changes in the environment (rain, snow, blockades, etc.) may impact the expected performance of a COA, making it desirable to have a pool of COAs that are diverse in task distributions across agents. Further, variations in agent capabilities, which could be human crews and/or autonomous systems, present practical opportunities and computational challenges to the planning process. This paper presents a new theoretical formulation and computational framework to generate such diverse pools of COAs for operations with soft variations in agent-task compatibility. Key to the problem formulation is a graph abstraction of the task space and the pool of COAs itself to quantify its diversity. Formulating the COAs as a centralized multi-robot task allocation problem, a genetic algorithm is used for (order-ignoring) allocations of tasks to each agent that jointly maximize diversity within the COA pool and overall compatibility of the agent-task mappings. A graph neural network is trained using a policy gradient approach to then perform single agent task sequencing in each COA, which maximizes completion rates adaptive to task features. Our tests of the COA generation process in a simulated environment demonstrate significant performance gain over a random walk baseline, small optimality gap in task sequencing, and execution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task operations.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Bilateral Team Formation in Cooperative Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.20039</link>
<guid>https://arxiv.org/abs/2506.20039</guid>
<content:encoded><![CDATA[
arXiv:2506.20039v1 Announce Type: new 
Abstract: Team formation and the dynamics of team-based learning have drawn significant interest in the context of Multi-Agent Reinforcement Learning (MARL). However, existing studies primarily focus on unilateral groupings, predefined teams, or fixed-population settings, leaving the effects of algorithmic bilateral grouping choices in dynamic populations underexplored. To address this gap, we introduce a framework for learning two-sided team formation in dynamic multi-agent systems. Through this study, we gain insight into what algorithmic properties in bilateral team formation influence policy performance and generalization. We validate our approach using widely adopted multi-agent scenarios, demonstrating competitive performance and improved generalization in most scenarios.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consensus-Driven Uncertainty for Robotic Grasping based on RGB Perception</title>
<link>https://arxiv.org/abs/2506.20045</link>
<guid>https://arxiv.org/abs/2506.20045</guid>
<content:encoded><![CDATA[
arXiv:2506.20045v1 Announce Type: new 
Abstract: Deep object pose estimators are notoriously overconfident. A grasping agent that both estimates the 6-DoF pose of a target object and predicts the uncertainty of its own estimate could avoid task failure by choosing not to act under high uncertainty. Even though object pose estimation improves and uncertainty quantification research continues to make strides, few studies have connected them to the downstream task of robotic grasping. We propose a method for training lightweight, deep networks to predict whether a grasp guided by an image-based pose estimate will succeed before that grasp is attempted. We generate training data for our networks via object pose estimation on real images and simulated grasping. We also find that, despite high object variability in grasping trials, networks benefit from training on all objects jointly, suggesting that a diverse variety of objects can nevertheless contribute to the same goal.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models</title>
<link>https://arxiv.org/abs/2506.20061</link>
<guid>https://arxiv.org/abs/2506.20061</guid>
<content:encoded><![CDATA[
arXiv:2506.20061v1 Announce Type: new 
Abstract: Developing effective instruction-following policies in reinforcement learning remains challenging due to the reliance on extensive human-labeled instruction datasets and the difficulty of learning from sparse rewards. In this paper, we propose a novel approach that leverages the capabilities of large language models (LLMs) to automatically generate open-ended instructions retrospectively from previously collected agent trajectories. Our core idea is to employ LLMs to relabel unsuccessful trajectories by identifying meaningful subtasks the agent has implicitly accomplished, thereby enriching the agent's training data and substantially alleviating reliance on human annotations. Through this open-ended instruction relabeling, we efficiently learn a unified instruction-following policy capable of handling diverse tasks within a single policy. We empirically evaluate our proposed method in the challenging Craftax environment, demonstrating clear improvements in sample efficiency, instruction coverage, and overall policy performance compared to state-of-the-art baselines. Our results highlight the effectiveness of utilizing LLM-guided open-ended instruction relabeling to enhance instruction-following reinforcement learning.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Autocomplete: Designing CopilotLens Towards Transparent and Explainable AI Coding Agents</title>
<link>https://arxiv.org/abs/2506.20062</link>
<guid>https://arxiv.org/abs/2506.20062</guid>
<content:encoded><![CDATA[
arXiv:2506.20062v1 Announce Type: new 
Abstract: AI-powered code assistants are widely used to generate code completions, significantly boosting developer productivity. However, these tools typically present suggestions without explaining their rationale, leaving their decision-making process inscrutable. This opacity hinders developers' ability to critically evaluate the output, form accurate mental models, and build calibrated trust in the system. To address this, we introduce CopilotLens, a novel interactive framework that reframes code completion from a simple suggestion into a transparent, explainable event. CopilotLens operates as an explanation layer that reveals the AI agent's "thought process" through a dynamic two-level interface, surfacing everything from its reconstructed high-level plans to the specific codebase context influencing the code. This paper presents the design and rationale of CopilotLens, offering a concrete framework for building future agentic code assistants that prioritize clarity of reasoning over speed of suggestion, thereby fostering deeper comprehension and more robust human-AI collaboration.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Conversation to Orchestration: HCI Challenges and Opportunities in Interactive Multi-Agentic Systems</title>
<link>https://arxiv.org/abs/2506.20091</link>
<guid>https://arxiv.org/abs/2506.20091</guid>
<content:encoded><![CDATA[
arXiv:2506.20091v1 Announce Type: new 
Abstract: Recent advances in multi-agentic systems (e.g. AutoGen, OpenAI Swarm) allow users to interact with a group of specialised AI agents rather than a single general-purpose agent. Despite the promise of this new paradigm, the HCI community has yet to fully examine the opportunities, risks, and user-centred challenges it introduces. We contribute to research on multi-agentic systems by exploring their architectures and key features through a human-centred lens. While literature and use cases remain limited, we build on existing tools and frameworks available to developers to identify a set of overarching challenges, e.g. orchestration and conflict resolution, that can guide future research in HCI. We illustrate these challenges through examples, offer potential design considerations, and provide research opportunities to spark interdisciplinary conversation. Our work lays the groundwork for future exploration and offers a research agenda focused on user-centred design in multi-agentic systems.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PSALM-V: Automating Symbolic Planning in Interactive Visual Environments with Large Language Models</title>
<link>https://arxiv.org/abs/2506.20097</link>
<guid>https://arxiv.org/abs/2506.20097</guid>
<content:encoded><![CDATA[
arXiv:2506.20097v1 Announce Type: new 
Abstract: We propose PSALM-V, the first autonomous neuro-symbolic learning system able to induce symbolic action semantics (i.e., pre- and post-conditions) in visual environments through interaction. PSALM-V bootstraps reliable symbolic planning without expert action definitions, using LLMs to generate heuristic plans and candidate symbolic semantics. Previous work has explored using large language models to generate action semantics for Planning Domain Definition Language (PDDL)-based symbolic planners. However, these approaches have primarily focused on text-based domains or relied on unrealistic assumptions, such as access to a predefined problem file, full observability, or explicit error messages. By contrast, PSALM-V dynamically infers PDDL problem files and domain action semantics by analyzing execution outcomes and synthesizing possible error explanations. The system iteratively generates and executes plans while maintaining a tree-structured belief over possible action semantics for each action, iteratively refining these beliefs until a goal state is reached. Simulated experiments of task completion in ALFRED demonstrate that PSALM-V increases the plan success rate from 37% (Claude-3.7) to 74% in partially observed setups. Results on two 2D game environments, RTFM and Overcooked-AI, show that PSALM-V improves step efficiency and succeeds in domain induction in multi-agent settings. PSALM-V correctly induces PDDL pre- and post-conditions for real-world robot BlocksWorld tasks, despite low-level manipulation failures from the robot.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Cyber Resilience via a Co-Evolutionary Arms Race within a Fortified Digital Twin Sandbox</title>
<link>https://arxiv.org/abs/2506.20102</link>
<guid>https://arxiv.org/abs/2506.20102</guid>
<content:encoded><![CDATA[
arXiv:2506.20102v1 Announce Type: new 
Abstract: The convergence of IT and OT has created hyper-connected ICS, exposing critical infrastructure to a new class of adaptive, intelligent adversaries that render static defenses obsolete. Existing security paradigms often fail to address a foundational "Trinity of Trust," comprising the fidelity of the system model, the integrity of synchronizing data, and the resilience of the analytical engine against sophisticated evasion. This paper introduces the ARC framework, a method for achieving analytical resilience through an autonomous, closed-loop hardening process. ARC establishes a perpetual co-evolutionary arms race within the high-fidelity sandbox of a F-SCDT. A DRL agent, the "Red Agent," is formalized and incentivized to autonomously discover stealthy, physically-plausible attack paths that maximize process disruption while evading detection. Concurrently, an ensemble-based "Blue Agent" defender is continuously hardened via adversarial training against the evolving threats discovered by its adversary. This co-evolutionary dynamic forces both agents to become progressively more sophisticated, enabling the system to autonomously probe and patch its own vulnerabilities. Experimental validation on both the TEP and the SWaT testbeds demonstrates the framework's superior performance. A comprehensive ablation study, supported by extensive visualizations including ROC curves and SHAP plots, reveals that the co-evolutionary process itself is responsible for a significant performance increase in detecting novel attacks. By integrating XAI to ensure operator trust and proposing a scalable F-ARC architecture, this work presents ARC not merely as an improvement, but as a necessary paradigm shift toward dynamic, self-improving security for the future of critical infrastructure.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Modeling by Language Models</title>
<link>https://arxiv.org/abs/2506.20249</link>
<guid>https://arxiv.org/abs/2506.20249</guid>
<content:encoded><![CDATA[
arXiv:2506.20249v1 Announce Type: new 
Abstract: Can we leverage LLMs to model the process of discovering novel language model (LM) architectures? Inspired by real research, we propose a multi-agent LLM approach that simulates the conventional stages of research, from ideation and literature search (proposal stage) to design implementation (code generation), generative pre-training, and downstream evaluation (verification). Using ideas from scaling laws, our system, Genesys, employs a Ladder of Scales approach; new designs are proposed, adversarially reviewed, implemented, and selectively verified at increasingly larger model scales (14M$\sim$350M parameters) with a narrowing budget (the number of models we can train at each scale). To help make discovery efficient and factorizable, Genesys uses a novel genetic programming backbone, which we show has empirical advantages over commonly used direct prompt generation workflows (e.g., $\sim$86\% percentage point improvement in successful design generation, a key bottleneck). We report experiments involving 1,162 newly discovered designs (1,062 fully verified through pre-training) and find the best designs to be highly competitive with known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common benchmarks). We couple these results with comprehensive system-level ablations and formal results, which give broader insights into the design of effective autonomous discovery systems.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exact and approximate maximin share allocations in multi-graphs</title>
<link>https://arxiv.org/abs/2506.20317</link>
<guid>https://arxiv.org/abs/2506.20317</guid>
<content:encoded><![CDATA[
arXiv:2506.20317v1 Announce Type: new 
Abstract: We study the problem of (approximate) maximin share (MMS) allocation of indivisible items among a set of agents. We focus on the graphical valuation model, previously studied by Christodolou, Fiat, Koutsoupias, and Sgouritsa ("Fair allocation in graphs", EC 2023), where the input is given by a graph where edges correspond to items, and vertices correspond to agents. An edge may have non-zero marginal value only for its incident vertices. We study additive, XOS and subadditive valuations and we present positive and negative results for (approximate) MMS fairness, and also for (approximate) pair-wise maximin share (PMMS) fairness.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finding the Easy Way Through -- the Probabilistic Gap Planner for Social Robot Navigation</title>
<link>https://arxiv.org/abs/2506.20320</link>
<guid>https://arxiv.org/abs/2506.20320</guid>
<content:encoded><![CDATA[
arXiv:2506.20320v1 Announce Type: new 
Abstract: In Social Robot Navigation, autonomous agents need to resolve many sequential interactions with other agents. State-of-the art planners can efficiently resolve the next, imminent interaction cooperatively and do not focus on longer planning horizons. This makes it hard to maneuver scenarios where the agent needs to select a good strategy to find gaps or channels in the crowd. We propose to decompose trajectory planning into two separate steps: Conflict avoidance for finding good, macroscopic trajectories, and cooperative collision avoidance (CCA) for resolving the next interaction optimally. We propose the Probabilistic Gap Planner (PGP) as a conflict avoidance planner. PGP modifies an established probabilistic collision risk model to include a general assumption of cooperativity. PGP biases the short-term CCA planner to head towards gaps in the crowd. In extensive simulations with crowds of varying density, we show that using PGP in addition to state-of-the-art CCA planners improves the agents' performance: On average, agents keep more space to others, create less tension, and cause fewer collisions. This typically comes at the expense of slightly longer paths. PGP runs in real-time on WaPOCHI mobile robot by Honda R&amp;D.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards</title>
<link>https://arxiv.org/abs/2506.20332</link>
<guid>https://arxiv.org/abs/2506.20332</guid>
<content:encoded><![CDATA[
arXiv:2506.20332v1 Announce Type: new 
Abstract: Vision-language model-based mobile agents have gained the ability to not only understand complex instructions and mobile screenshots, but also optimize their action outputs via thinking and reasoning, benefiting from reinforcement learning, such as Group Relative Policy Optimization (GRPO). However, existing research centers on offline reinforcement learning training or online optimization using action-level rewards, which limits the agent's dynamic interaction with the environment. This often results in agents settling into local optima, thereby weakening their ability for exploration and error action correction. To address these challenges, we introduce an approach called Mobile-R1, which employs interactive multi-turn reinforcement learning with task-level rewards for mobile agents. Our training framework consists of three stages: initial format finetuning, single-step online training via action-level reward, followed by online training via task-level reward based on multi-turn trajectories. This strategy is designed to enhance the exploration and error correction capabilities of Mobile-R1, leading to significant performance improvements. Moreover, we have collected a dataset covering 28 Chinese applications with 24,521 high-quality manual annotations and established a new benchmark with 500 trajectories. We will open source all resources, including the dataset, benchmark, model weight, and codes: https://mobile-r1.github.io/Mobile-R1/.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Visualization Framework for Exploring Multi-Agent-Based Simulations Case Study of an Electric Vehicle Home Charging Ecosystem</title>
<link>https://arxiv.org/abs/2506.20400</link>
<guid>https://arxiv.org/abs/2506.20400</guid>
<content:encoded><![CDATA[
arXiv:2506.20400v1 Announce Type: new 
Abstract: Multi-agent-based simulations (MABS) of electric vehicle (EV) home charging ecosystems generate large, complex, and stochastic time-series datasets that capture interactions between households, grid infrastructure, and energy markets. These interactions can lead to unexpected system-level events, such as transformer overloads or consumer dissatisfaction, that are difficult to detect and explain through static post-processing. This paper presents a modular, Python-based dashboard framework, built using Dash by Plotly, that enables efficient, multi-level exploration and root-cause analysis of emergent behavior in MABS outputs. The system features three coordinated views (System Overview, System Analysis, and Consumer Analysis), each offering high-resolution visualizations such as time-series plots, spatial heatmaps, and agent-specific drill-down tools. A case study simulating full EV adoption with smart charging in a Danish residential network demonstrates how the dashboard supports rapid identification and contextual explanation of anomalies, including clustered transformer overloads and time-dependent charging failures. The framework facilitates actionable insight generation for researchers and distribution system operators, and its architecture is adaptable to other distributed energy resources and complex energy systems.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAPS: Tool-Augmented Personalisation via Structured Tagging</title>
<link>https://arxiv.org/abs/2506.20409</link>
<guid>https://arxiv.org/abs/2506.20409</guid>
<content:encoded><![CDATA[
arXiv:2506.20409v1 Announce Type: new 
Abstract: Recent advancements in tool-augmented large language models have enabled them to interact with external tools, enhancing their ability to perform complex user tasks. However, existing approaches overlook the role of personalisation in guiding tool use. This work investigates how user preferences can be effectively integrated into goal-oriented dialogue agents. Through extensive analysis, we identify key weaknesses in the ability of LLMs to personalise tool use. To this end, we introduce \name, a novel solution that enhances personalised tool use by leveraging a structured tagging tool and an uncertainty-based tool detector. TAPS significantly improves the ability of LLMs to incorporate user preferences, achieving the new state-of-the-art for open source models on the NLSI task.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SV-LLM: An Agentic Approach for SoC Security Verification using Large Language Models</title>
<link>https://arxiv.org/abs/2506.20415</link>
<guid>https://arxiv.org/abs/2506.20415</guid>
<content:encoded><![CDATA[
arXiv:2506.20415v1 Announce Type: new 
Abstract: Ensuring the security of complex system-on-chips (SoCs) designs is a critical imperative, yet traditional verification techniques struggle to keep pace due to significant challenges in automation, scalability, comprehensiveness, and adaptability. The advent of large language models (LLMs), with their remarkable capabilities in natural language understanding, code generation, and advanced reasoning, presents a new paradigm for tackling these issues. Moving beyond monolithic models, an agentic approach allows for the creation of multi-agent systems where specialized LLMs collaborate to solve complex problems more effectively. Recognizing this opportunity, we introduce SV-LLM, a novel multi-agent assistant system designed to automate and enhance SoC security verification. By integrating specialized agents for tasks like verification question answering, security asset identification, threat modeling, test plan and property generation, vulnerability detection, and simulation-based bug validation, SV-LLM streamlines the workflow. To optimize their performance in these diverse tasks, agents leverage different learning paradigms, such as in-context learning, fine-tuning, and retrieval-augmented generation (RAG). The system aims to reduce manual intervention, improve accuracy, and accelerate security analysis, supporting proactive identification and mitigation of risks early in the design cycle. We demonstrate its potential to transform hardware security practices through illustrative case studies and experiments that showcase its applicability and efficacy.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Agentic System for Rare Disease Diagnosis with Traceable Reasoning</title>
<link>https://arxiv.org/abs/2506.20430</link>
<guid>https://arxiv.org/abs/2506.20430</guid>
<content:encoded><![CDATA[
arXiv:2506.20430v1 Announce Type: new 
Abstract: Rare diseases collectively affect over 300 million individuals worldwide, yet timely and accurate diagnosis remains a pervasive challenge. This is largely due to their clinical heterogeneity, low individual prevalence, and the limited familiarity most clinicians have with rare conditions. Here, we introduce DeepRare, the first rare disease diagnosis agentic system powered by a large language model (LLM), capable of processing heterogeneous clinical inputs. The system generates ranked diagnostic hypotheses for rare diseases, each accompanied by a transparent chain of reasoning that links intermediate analytic steps to verifiable medical evidence.
  DeepRare comprises three key components: a central host with a long-term memory module; specialized agent servers responsible for domain-specific analytical tasks integrating over 40 specialized tools and web-scale, up-to-date medical knowledge sources, ensuring access to the most current clinical information. This modular and scalable design enables complex diagnostic reasoning while maintaining traceability and adaptability. We evaluate DeepRare on eight datasets. The system demonstrates exceptional diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013 diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15 methods, like traditional bioinformatics diagnostic tools, LLMs, and other agentic systems, achieving an average Recall@1 score of 57.18% and surpassing the second-best method (Reasoning LLM) by a substantial margin of 23.79 percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at Recall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of reasoning chains by clinical experts achieves 95.40% agreements. Furthermore, the DeepRare system has been implemented as a user-friendly web application http://raredx.cn/doctor.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opinion Dynamics with Highly Oscillating Opinions</title>
<link>https://arxiv.org/abs/2506.20472</link>
<guid>https://arxiv.org/abs/2506.20472</guid>
<content:encoded><![CDATA[
arXiv:2506.20472v1 Announce Type: new 
Abstract: Opinion Dynamics (OD) models are a particular case of Agent-Based Models in which the evolution of opinions within a population is studied. In most OD models, opinions evolve as a consequence of interactions between agents, and the opinion fusion rule defines how those opinions are updated. In consequence, despite being simplistic, OD models provide an explainable and interpretable mechanism for understanding the underlying dynamics of opinion evolution. Unfortunately, existing OD models mainly focus on explaining the evolution of (usually synthetic) opinions towards consensus, fragmentation, or polarization, but they usually fail to analyze scenarios of (real-world) highly oscillating opinions. This work overcomes this limitation by studying the ability of several OD models to reproduce highly oscillating dynamics. To this end, we formulate an optimization problem which is further solved using Evolutionary Algorithms, providing both quantitative results on the performance of the optimization and qualitative interpretations on the obtained results. Our experiments on a real-world opinion dataset about immigration from the monthly barometer of the Spanish Sociological Research Center show that the ATBCR, based on both rational and emotional mechanisms of opinion update, is the most accurate OD model for capturing highly oscillating opinions.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Engineering Sentience</title>
<link>https://arxiv.org/abs/2506.20504</link>
<guid>https://arxiv.org/abs/2506.20504</guid>
<content:encoded><![CDATA[
arXiv:2506.20504v1 Announce Type: new 
Abstract: We spell out a definition of sentience that may be useful for designing and building it in machines. We propose that for sentience to be meaningful for AI, it must be fleshed out in functional, computational terms, in enough detail to allow for implementation. Yet, this notion of sentience must also reflect something essentially 'subjective', beyond just having the general capacity to encode perceptual content. For this specific functional notion of sentience to occur, we propose that certain sensory signals need to be both assertoric (persistent) and qualitative. To illustrate the definition in more concrete terms, we sketch out some ways for potential implementation, given current technology. Understanding what it takes for artificial agents to be functionally sentient can also help us avoid creating them inadvertently, or at least, realize that we have created them in a timely manner.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning and Prompt Engineering of LLMs, for the Creation of Multi-Agent AI for Addressing Sustainable Protein Production Challenges</title>
<link>https://arxiv.org/abs/2506.20598</link>
<guid>https://arxiv.org/abs/2506.20598</guid>
<content:encoded><![CDATA[
arXiv:2506.20598v1 Announce Type: new 
Abstract: The global demand for sustainable protein sources has accelerated the need for intelligent tools that can rapidly process and synthesise domain-specific scientific knowledge. In this study, we present a proof-of-concept multi-agent Artificial Intelligence (AI) framework designed to support sustainable protein production research, with an initial focus on microbial protein sources. Our Retrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based LLM agents: (1) a literature search agent that retrieves relevant scientific literature on microbial protein production for a specified microbial strain, and (2) an information extraction agent that processes the retrieved content to extract relevant biological and chemical information. Two parallel methodologies, fine-tuning and prompt engineering, were explored for agent optimisation. Both methods demonstrated effectiveness at improving the performance of the information extraction agent in terms of transformer-based cosine similarity scores between obtained and ideal outputs. Mean cosine similarity scores were increased by up to 25%, while universally reaching mean scores of $\geq 0.89$ against ideal output text. Fine-tuning overall improved the mean scores to a greater extent (consistently of $\geq 0.94$) compared to prompt engineering, although lower statistical uncertainties were observed with the latter approach. A user interface was developed and published for enabling the use of the multi-agent AI system, alongside preliminary exploration of additional chemical safety-based search capabilities
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm</title>
<link>https://arxiv.org/abs/2506.20606</link>
<guid>https://arxiv.org/abs/2506.20606</guid>
<content:encoded><![CDATA[
arXiv:2506.20606v1 Announce Type: new 
Abstract: Agents based on Large Language Models (LLMs) have demonstrated strong capabilities across a wide range of tasks. However, deploying LLM-based agents in high-stakes domains comes with significant safety and ethical risks. Unethical behavior by these agents can directly result in serious real-world consequences, including physical harm and financial loss. To efficiently steer the ethical behavior of agents, we frame agent behavior steering as a model editing task, which we term Behavior Editing. Model editing is an emerging area of research that enables precise and efficient modifications to LLMs while preserving their overall capabilities. To systematically study and evaluate this approach, we introduce BehaviorBench, a multi-tier benchmark grounded in psychological moral theories. This benchmark supports both the evaluation and editing of agent behaviors across a variety of scenarios, with each tier introducing more complex and ambiguous scenarios. We first demonstrate that Behavior Editing can dynamically steer agents toward the target behavior within specific scenarios. Moreover, Behavior Editing enables not only scenario-specific local adjustments but also more extensive shifts in an agent's global moral alignment. We demonstrate that Behavior Editing can be used to promote ethical and benevolent behavior or, conversely, to induce harmful or malicious behavior. Through comprehensive evaluations on agents based on frontier LLMs, BehaviorBench shows the effectiveness of Behavior Editing across different models and scenarios. Our findings offer key insights into a new paradigm for steering agent behavior, highlighting both the promise and perils of Behavior Editing.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Community-Driven Agents for Machine Learning Engineering</title>
<link>https://arxiv.org/abs/2506.20640</link>
<guid>https://arxiv.org/abs/2506.20640</guid>
<content:encoded><![CDATA[
arXiv:2506.20640v1 Announce Type: new 
Abstract: Large language model-based machine learning (ML) agents have shown great promise in automating ML research. However, existing agents typically operate in isolation on a given research problem, without engaging with the broader research community, where human researchers often gain insights and contribute by sharing knowledge. To bridge this gap, we introduce MLE-Live, a live evaluation framework designed to assess an agent's ability to communicate with and leverage collective knowledge from a simulated Kaggle research community. Building on this framework, we propose CoMind, a novel agent that excels at exchanging insights and developing novel solutions within a community context. CoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2% human competitors on average across four ongoing Kaggle competitions. Our code is released at https://github.com/comind-ml/CoMind.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memento: Note-Taking for Your Future Self</title>
<link>https://arxiv.org/abs/2506.20642</link>
<guid>https://arxiv.org/abs/2506.20642</guid>
<content:encoded><![CDATA[
arXiv:2506.20642v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at reasoning-only tasks, but struggle when reasoning must be tightly coupled with retrieval, as in multi-hop question answering. To overcome these limitations, we introduce a prompting strategy that first decomposes a complex question into smaller steps, then dynamically constructs a database of facts using LLMs, and finally pieces these facts together to solve the question. We show how this three-stage strategy, which we call Memento, can boost the performance of existing prompting strategies across diverse settings. On the 9-step PhantomWiki benchmark, Memento doubles the performance of chain-of-thought (CoT) when all information is provided in context. On the open-domain version of 2WikiMultiHopQA, CoT-RAG with Memento improves over vanilla CoT-RAG by more than 20 F1 percentage points and over the multi-hop RAG baseline, IRCoT, by more than 13 F1 percentage points. On the challenging MuSiQue dataset, Memento improves ReAct by more than 3 F1 percentage points, demonstrating its utility in agentic settings.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind</title>
<link>https://arxiv.org/abs/2506.20664</link>
<guid>https://arxiv.org/abs/2506.20664</guid>
<content:encoded><![CDATA[
arXiv:2506.20664v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) gain agentic abilities, they will have to navigate complex multi-agent scenarios, interacting with human users and other agents in cooperative and competitive settings. This will require new reasoning skills, chief amongst them being theory of mind (ToM), or the ability to reason about the "mental" states of other agents. However, ToM and other multi-agent abilities in LLMs are poorly understood, since existing benchmarks suffer from narrow scope, data leakage, saturation, and lack of interactivity. We thus propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM drawing inspiration from cognitive science, computational pragmatics and multi-agent reinforcement learning. It is designed to be as easy as possible in all other dimensions, eliminating confounding factors commonly found in other benchmarks. To our knowledge, it is also the first platform for designing interactive ToM experiments.
  We validate the benchmark design through comprehensive empirical evaluations of frontier LLMs, robustness studies, and human-AI cross-play experiments. We find that LLM game-playing abilities lag behind humans and simple word-embedding baselines. We then create variants of two classic cognitive science experiments within Decrypto to evaluate three key ToM abilities. Surprisingly, we find that state-of-the-art reasoning models are significantly worse at those tasks than their older counterparts. This demonstrates that Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and paves the path towards better artificial agents.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMSearch-R1: Incentivizing LMMs to Search</title>
<link>https://arxiv.org/abs/2506.20670</link>
<guid>https://arxiv.org/abs/2506.20670</guid>
<content:encoded><![CDATA[
arXiv:2506.20670v1 Announce Type: new 
Abstract: Robust deployment of large multimodal models (LMMs) in real-world scenarios requires access to external knowledge sources, given the complexity and dynamic nature of real-world information. Existing approaches such as retrieval-augmented generation (RAG) and prompt engineered search agents rely on rigid pipelines, often leading to inefficient or excessive search behaviors. We present MMSearch-R1, the first end-to-end reinforcement learning framework that enables LMMs to perform on-demand, multi-turn search in real-world Internet environments. Our framework integrates both image and text search tools, allowing the model to reason about when and how to invoke them guided by an outcome-based reward with a search penalty. To support training, We collect a multimodal search VQA dataset through a semi-automated pipeline that covers diverse visual and textual knowledge needs and curate a search-balanced subset with both search-required and search-free samples, which proves essential for shaping efficient and on-demand search behavior. Extensive experiments on knowledge-intensive and info-seeking VQA tasks show that our model not only outperforms RAG-based baselines of the same model size, but also matches the performance of a larger RAG-based model while reducing search calls by over 30%. We further analyze key empirical findings to offer actionable insights for advancing research in multimodal search.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Explainable Reinforcement Learning: Concepts, Algorithms, Challenges</title>
<link>https://arxiv.org/abs/2211.06665</link>
<guid>https://arxiv.org/abs/2211.06665</guid>
<content:encoded><![CDATA[
arXiv:2211.06665v5 Announce Type: replace 
Abstract: Reinforcement Learning (RL) is a popular machine learning paradigm where intelligent agents interact with the environment to fulfill a long-term goal. Driven by the resurgence of deep learning, Deep RL (DRL) has witnessed great success over a wide spectrum of complex control tasks. Despite the encouraging results achieved, the deep neural network-based backbone is widely deemed as a black box that impedes practitioners to trust and employ trained agents in realistic scenarios where high security and reliability are essential. To alleviate this issue, a large volume of literature devoted to shedding light on the inner workings of the intelligent agents has been proposed, by constructing intrinsic interpretability or post-hoc explainability. In this survey, we provide a comprehensive review of existing works on eXplainable RL (XRL) and introduce a new taxonomy where prior works are clearly categorized into model-explaining, reward-explaining, state-explaining, and task-explaining methods. We also review and highlight RL methods that conversely leverage human knowledge to promote learning efficiency and performance of agents while this kind of method is often ignored in XRL field. Some challenges and opportunities in XRL are discussed. This survey intends to provide a high-level summarization of XRL and to motivate future research on more effective XRL solutions. Corresponding open source codes are collected and categorized at https://github.com/Plankson/awesome-explainable-reinforcement-learning.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-informed Imitative Reinforcement Learning for Real-world Driving</title>
<link>https://arxiv.org/abs/2407.02508</link>
<guid>https://arxiv.org/abs/2407.02508</guid>
<content:encoded><![CDATA[
arXiv:2407.02508v3 Announce Type: replace 
Abstract: Recent advances in imitative reinforcement learning (IRL) have considerably enhanced the ability of autonomous agents to assimilate expert demonstrations, leading to rapid skill acquisition in a range of demanding tasks. However, such learning-based agents face significant challenges when transferring knowledge to highly dynamic closed-loop environments. Their performance is significantly impacted by the conflicting optimization objectives of imitation learning (IL) and reinforcement learning (RL), sample inefficiency, and the complexity of uncovering the hidden world model and physics. To address this challenge, we propose a physics-informed IRL that is entirely data-driven. It leverages both expert demonstration data and exploratory data with a joint optimization objective, allowing the underlying physical principles of vehicle dynamics to emerge naturally from the training process. The performance is evaluated through empirical experiments and results exceed popular IL, RL and IRL algorithms in closed-loop settings on Waymax benchmark. Our approach exhibits 37.8% reduction in collision rate and 22.2% reduction in off-road rate compared to the baseline method.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking Text-To-Image Generation Models</title>
<link>https://arxiv.org/abs/2408.00523</link>
<guid>https://arxiv.org/abs/2408.00523</guid>
<content:encoded><![CDATA[
arXiv:2408.00523v3 Announce Type: replace 
Abstract: Text-to-image (T2I) generative models have revolutionized content creation by transforming textual descriptions into high-quality images. However, these models are vulnerable to jailbreaking attacks, where carefully crafted prompts bypass safety mechanisms to produce unsafe content. While researchers have developed various jailbreak attacks to expose this risk, these methods face significant limitations, including impractical access requirements, easily detectable unnatural prompts, restricted search spaces, and high query demands on the target system. In this paper, we propose JailFuzzer, a novel fuzzing framework driven by large language model (LLM) agents, designed to efficiently generate natural and semantically meaningful jailbreak prompts in a black-box setting. Specifically, JailFuzzer employs fuzz-testing principles with three components: a seed pool for initial and jailbreak prompts, a guided mutation engine for generating meaningful variations, and an oracle function to evaluate jailbreak success. Furthermore, we construct the guided mutation engine and oracle function by LLM-based agents, which further ensures efficiency and adaptability in black-box settings. Extensive experiments demonstrate that JailFuzzer has significant advantages in jailbreaking T2I models. It generates natural and semantically coherent prompts, reducing the likelihood of detection by traditional defenses. Additionally, it achieves a high success rate in jailbreak attacks with minimal query overhead, outperforming existing methods across all key metrics. This study underscores the need for stronger safety mechanisms in generative models and provides a foundation for future research on defending against sophisticated jailbreaking attacks. JailFuzzer is open-source and available at this repository: https://github.com/YingkaiD/JailFuzzer.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>World-Consistent Data Generation for Vision-and-Language Navigation</title>
<link>https://arxiv.org/abs/2412.06413</link>
<guid>https://arxiv.org/abs/2412.06413</guid>
<content:encoded><![CDATA[
arXiv:2412.06413v2 Announce Type: replace 
Abstract: Vision-and-Language Navigation (VLN) is a challenging task that requires an agent to navigate through photorealistic environments following natural-language instructions. One main obstacle existing in VLN is data scarcity, leading to poor generalization performance over unseen environments. Though data argumentation is a promising way for scaling up the dataset, how to generate VLN data both diverse and world-consistent remains problematic. To cope with this issue, we propose the world-consistent data generation (WCGEN), an efficacious data-augmentation framework satisfying both diversity and world-consistency, aimed at enhancing the generalization of agents to novel environments. Roughly, our framework consists of two stages, the trajectory stage which leverages a point-cloud based technique to ensure spatial coherency among viewpoints, and the viewpoint stage which adopts a novel angle synthesis method to guarantee spatial and wraparound consistency within the entire observation. By accurately predicting viewpoint changes with 3D knowledge, our approach maintains the world-consistency during the generation procedure. Experiments on a wide range of datasets verify the effectiveness of our method, demonstrating that our data augmentation strategy enables agents to achieve new state-of-the-art results on all navigation tasks, and is capable of enhancing the VLN agents' generalization ability to unseen environments.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentBreeder: Mitigating the AI Safety Impact of Multi-Agent Scaffolds via Self-Improvement</title>
<link>https://arxiv.org/abs/2502.00757</link>
<guid>https://arxiv.org/abs/2502.00757</guid>
<content:encoded><![CDATA[
arXiv:2502.00757v3 Announce Type: replace 
Abstract: Scaffolding Large Language Models (LLMs) into multi-agent systems often improves performance on complex tasks, but the safety impact of such scaffolds has not been thoroughly explored. We introduce AgentBreeder, a framework for multi-objective self-improving evolutionary search over scaffolds. We evaluate discovered scaffolds on widely recognized reasoning, mathematics, and safety benchmarks and compare them with popular baselines. In 'blue' mode, we see a 79.4% average uplift in safety benchmark performance while maintaining or improving capability scores. In 'red' mode, we find adversarially weak scaffolds emerging concurrently with capability optimization. Our work demonstrates the risks of multi-agent scaffolding and provides a framework for mitigating them. Code is available at https://github.com/J-Rosser-UK/AgentBreeder.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Budget-Feasible Contracts</title>
<link>https://arxiv.org/abs/2504.01773</link>
<guid>https://arxiv.org/abs/2504.01773</guid>
<content:encoded><![CDATA[
arXiv:2504.01773v2 Announce Type: replace 
Abstract: The problem of computing near-optimal contracts in combinatorial settings has recently attracted significant interest in the computer science community. Previous work has provided a rich body of structural and algorithmic insights into this problem. However, most of these results rely on the assumption that the principal has an unlimited budget for incentivizing agents, an assumption that is often unrealistic in practice. This motivates the study of the optimal contract problem under budget constraints.
  In this work, we study multi-agent contracts with binary actions under budget constraints. Our contribution is threefold. First, we show that all previously known approximation guarantees on the principal's utility extend (asymptotically) to budgeted settings. Second, through the lens of budget constraints, we uncover insightful connections between the standard objective of maximizing the principal's utility and other relevant objectives. Specifically, we identify a broad class of objectives, which we term BEST (BEyond STandard) objectives, including reward, social welfare, and principal's utility, and show that they are all equivalent (up to a constant factor), leading to approximation guarantees for all BEST objectives. Third, we introduce the price of frugality, which quantifies the loss due to budget constraints, and establish near-tight bounds on this measure, providing deeper insights into the tradeoffs between budgets and incentives.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MARCO: Multi-Agent Code Optimization with Real-Time Knowledge Integration for High-Performance Computing</title>
<link>https://arxiv.org/abs/2505.03906</link>
<guid>https://arxiv.org/abs/2505.03906</guid>
<content:encoded><![CDATA[
arXiv:2505.03906v3 Announce Type: replace 
Abstract: Large language models (LLMs) have transformed software development through code generation capabilities, yet their effectiveness for high-performance computing (HPC) remains limited. HPC code requires specialized optimizations for parallelism, memory efficiency, and architecture-specific considerations that general-purpose LLMs often overlook. We present MARCO (Multi-Agent Reactive Code Optimizer), a novel framework that enhances LLM-generated code for HPC through a specialized multi-agent architecture. MARCO employs separate agents for code generation and performance evaluation, connected by a feedback loop that progressively refines optimizations. A key innovation is MARCO's web-search component that retrieves real-time optimization techniques from recent conference proceedings and research publications, bridging the knowledge gap in pre-trained LLMs. Our extensive evaluation on the LeetCode 75 problem set demonstrates that MARCO achieves a 14.6\% average runtime reduction compared to Claude 3.5 Sonnet alone, while the integration of the web-search component yields a 30.9\% performance improvement over the base MARCO system. These results highlight the potential of multi-agent systems to address the specialized requirements of high-performance code generation, offering a cost-effective alternative to domain-specific model fine-tuning.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$C^3$-Bench: The Things Real Disturbing LLM based Agent in Multi-Tasking</title>
<link>https://arxiv.org/abs/2505.18746</link>
<guid>https://arxiv.org/abs/2505.18746</guid>
<content:encoded><![CDATA[
arXiv:2505.18746v3 Announce Type: replace 
Abstract: Agents based on large language models leverage tools to modify environments, revolutionizing how AI interacts with the physical world. Unlike traditional NLP tasks that rely solely on historical dialogue for responses, these agents must consider more complex factors, such as inter-tool relationships, environmental feedback and previous decisions, when making choices. Current research typically evaluates agents via multi-turn dialogues. However, it overlooks the influence of these critical factors on agent behavior. To bridge this gap, we present an open-source and high-quality benchmark $C^3$-Bench. This benchmark integrates attack concepts and applies univariate analysis to pinpoint key elements affecting agent robustness. In concrete, we design three challenges: navigate complex tool relationships, handle critical hidden information and manage dynamic decision paths. Complementing these challenges, we introduce fine-grained metrics, innovative data collection algorithms and reproducible evaluation methods. Extensive experiments are conducted on 49 mainstream agents, encompassing general fast-thinking, slow-thinking and domain-specific models. We observe that agents have significant shortcomings in handling tool dependencies, long context information dependencies and frequent policy-type switching. In essence, $C^3$-Bench aims to expose model vulnerabilities through these challenges and drive research into the interpretability of agent performance. The benchmark is publicly available at https://github.com/yupeijei1997/C3-Bench.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Based Exploration in Monitored Markov Decision Processes</title>
<link>https://arxiv.org/abs/2502.16772</link>
<guid>https://arxiv.org/abs/2502.16772</guid>
<content:encoded><![CDATA[
arXiv:2502.16772v5 Announce Type: replace 
Abstract: A tenet of reinforcement learning is that the agent always observes rewards. However, this is not true in many realistic settings, e.g., a human observer may not always be available to provide rewards, sensors may be limited or malfunctioning, or rewards may be inaccessible during deployment. Monitored Markov decision processes (Mon-MDPs) have recently been proposed to model such settings. However, existing Mon-MDP algorithms have several limitations: they do not fully exploit the problem structure, cannot leverage a known monitor, lack worst-case guarantees for 'unsolvable' Mon-MDPs without specific initialization, and offer only asymptotic convergence proofs. This paper makes three contributions. First, we introduce a model-based algorithm for Mon-MDPs that addresses these shortcomings. The algorithm employs two instances of model-based interval estimation: one to ensure that observable rewards are reliably captured, and another to learn the minimax-optimal policy. Second, we empirically demonstrate the advantages. We show faster convergence than prior algorithms in over four dozen benchmarks, and even more dramatic improvement when the monitoring process is known. Third, we present the first finite-sample bound on performance. We show convergence to a minimax-optimal policy even when some rewards are never observable.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signal Use and Emergent Cooperation</title>
<link>https://arxiv.org/abs/2506.18920</link>
<guid>https://arxiv.org/abs/2506.18920</guid>
<content:encoded><![CDATA[
arXiv:2506.18920v1 Announce Type: new 
Abstract: In this work, we investigate how autonomous agents, organized into tribes, learn to use communication signals to coordinate their activities and enhance their collective efficiency. Using the NEC-DAC (Neurally Encoded Culture - Distributed Autonomous Communicators) system, where each agent is equipped with its own neural network for decision-making, we demonstrate how these agents develop a shared behavioral system -- akin to a culture -- through learning and signalling. Our research focuses on the self-organization of culture within these tribes of agents and how varying communication strategies impact their fitness and cooperation. By analyzing different social structures, such as authority hierarchies, we show that the culture of cooperation significantly influences the tribe's performance. Furthermore, we explore how signals not only facilitate the emergence of culture but also enable its transmission across generations of agents. Additionally, we examine the benefits of coordinating behavior and signaling within individual agents' neural networks.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Applications of Generative AI in Actuarial Science: Case Studies Beyond ChatGPT</title>
<link>https://arxiv.org/abs/2506.18942</link>
<guid>https://arxiv.org/abs/2506.18942</guid>
<content:encoded><![CDATA[
arXiv:2506.18942v1 Announce Type: new 
Abstract: This article demonstrates the transformative impact of Generative AI (GenAI) on actuarial science, illustrated by four implemented case studies. It begins with a historical overview of AI, tracing its evolution from early neural networks to modern GenAI technologies. The first case study shows how Large Language Models (LLMs) improve claims cost prediction by deriving significant features from unstructured textual data, significantly reducing prediction errors in the underlying machine learning task. In the second case study, we explore the automation of market comparisons using the GenAI concept of Retrieval-Augmented Generation to identify and process relevant information from documents. A third case study highlights the capabilities of fine-tuned vision-enabled LLMs in classifying car damage types and extracting contextual information. The fourth case study presents a multi-agent system that autonomously analyzes data from a given dataset and generates a corresponding report detailing the key findings. In addition to these case studies, we outline further potential applications of GenAI in the insurance industry, such as the automation of claims processing and fraud detection, and the verification of document compliance with internal or external policies. Finally, we discuss challenges and considerations associated with the use of GenAI, covering regulatory issues, ethical concerns, and technical limitations, among others.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in Real-World Applications</title>
<link>https://arxiv.org/abs/2506.18951</link>
<guid>https://arxiv.org/abs/2506.18951</guid>
<content:encoded><![CDATA[
arXiv:2506.18951v1 Announce Type: new 
Abstract: Resolution of complex SQL issues persists as a significant bottleneck in real-world database applications. Current Large Language Models (LLMs), while adept at text-to-SQL translation, have not been rigorously evaluated on the more challenging task of debugging SQL issues. To address this gap, we introduce BIRD-CRITIC, a new SQL issue debugging benchmark comprising 530 PostgreSQL tasks (BIRD-CRITIC-PG) and 570 multi-dialect tasks (BIRD-CRITIC-Multi), distilled from authentic user issues and replayed within new environments to facilitate rigorous evaluation. Baseline evaluations underscore the task's complexity, with the leading reasoning model O3-Mini achieving only 38.87% success rate on BIRD-CRITIC-PG and 33.33% on BIRD-CRITIC-Multi. Meanwhile, advancing open-source models for database tasks is crucial for empowering local development while safeguarding data privacy. Therefore, we present Six-Gym (Sql-fIX-Gym), a training environment for elevating open-source model capabilities for SQL issue debugging. This environment leverages SQL-Rewind strategy, which automatically generates executable issue-solution datasets by reverse-engineering issues from verified SQLs. However, popular trajectory-based fine-tuning methods do not explore substantial supervisory signals. We further propose f-Plan Boosting, which extracts high-level debugging plans from SQL solutions, enabling teacher LLMs to produce 73.7% more successful trajectories for training. We integrate these components into an open-source agent, Bird-Fixer. Based on Qwen-2.5-Coder-14B, Bird-Fixer achieves 38.11% success rate on BIRD-CRITIC-PG and 29.65% on BIRD-CRITIC-Multi, surpassing leading proprietary models such as Claude-3.7-Sonnet and GPT-4.1, marking a significant step toward democratizing sophisticated SQL-debugging capabilities. The leaderboard and source code are available: https://bird-critic.github.io/
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comment On "The Illusion of Thinking": Reframing the Reasoning Cliff as an Agentic Gap</title>
<link>https://arxiv.org/abs/2506.18957</link>
<guid>https://arxiv.org/abs/2506.18957</guid>
<content:encoded><![CDATA[
arXiv:2506.18957v1 Announce Type: new 
Abstract: The recent work by Shojaee et al. (2025), titled The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity, presents a compelling empirical finding, a reasoning cliff, where the performance of Large Reasoning Models (LRMs) collapses beyond a specific complexity threshold, which the authors posit as an intrinsic scaling limitation of Chain-of-Thought (CoT) reasoning. This commentary, while acknowledging the study's methodological rigor, contends that this conclusion is confounded by experimental artifacts. We argue that the observed failure is not evidence of a fundamental cognitive boundary, but rather a predictable outcome of system-level constraints in the static, text-only evaluation paradigm, including tool use restrictions, context window recall issues, the absence of crucial cognitive baselines, inadequate statistical reporting, and output generation limits. We reframe this performance collapse through the lens of an agentic gap, asserting that the models are not failing at reasoning, but at execution within a profoundly restrictive interface. We empirically substantiate this critique by demonstrating a striking reversal. A model, initially declaring a puzzle impossible when confined to text-only generation, now employs agentic tools to not only solve it but also master variations of complexity far beyond the reasoning cliff it previously failed to surmount. Additionally, our empirical analysis of tool-enabled models like o4-mini and GPT-4o reveals a hierarchy of agentic reasoning, from simple procedural execution to complex meta-cognitive self-correction, which has significant implications for how we define and measure machine intelligence. The illusion of thinking attributed to LRMs is less a reasoning deficit and more a consequence of an otherwise capable mind lacking the tools for action.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Web Search towards Agentic Deep Research: Incentivizing Search with Reasoning Agents</title>
<link>https://arxiv.org/abs/2506.18959</link>
<guid>https://arxiv.org/abs/2506.18959</guid>
<content:encoded><![CDATA[
arXiv:2506.18959v1 Announce Type: new 
Abstract: Information retrieval is a cornerstone of modern knowledge acquisition, enabling billions of queries each day across diverse domains. However, traditional keyword-based search engines are increasingly inadequate for handling complex, multi-step information needs. Our position is that Large Language Models (LLMs), endowed with reasoning and agentic capabilities, are ushering in a new paradigm termed Agentic Deep Research. These systems transcend conventional information search techniques by tightly integrating autonomous reasoning, iterative retrieval, and information synthesis into a dynamic feedback loop. We trace the evolution from static web search to interactive, agent-based systems that plan, explore, and learn. We also introduce a test-time scaling law to formalize the impact of computational depth on reasoning and search. Supported by benchmark results and the rise of open-source implementations, we demonstrate that Agentic Deep Research not only significantly outperforms existing approaches, but is also poised to become the dominant paradigm for future information seeking. All the related resources, including industry products, research papers, benchmark datasets, and open-source implementations, are collected for the community in https://github.com/DavidZWZ/Awesome-Deep-Research.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Reference Adaptive Control of Networked Systems with State and Input Delays</title>
<link>https://arxiv.org/abs/2506.19138</link>
<guid>https://arxiv.org/abs/2506.19138</guid>
<content:encoded><![CDATA[
arXiv:2506.19138v1 Announce Type: new 
Abstract: Adaptive control strategies have progressively advanced to accommodate increasingly uncertain, delayed, and interconnected systems. This paper addresses the model reference adaptive control (MRAC) of networked, heterogeneous, and unknown dynamical agents subject to both state and input delays. The objective is to ensure that all follower agents asymptotically track the trajectory of a stable leader system, despite system uncertainties and communication constraints. Two communication topologies are considered, full connectivity between each agent and the leader, and partial connectivity wherein agents rely on both neighboring peers and the leader. The agent-to-agent and agent-to-leader interactions are encoded using a Laplacian-like matrix and a diagonal model-weighting matrix, respectively. To compensate for the delays, a predictor-based control structure and an auxiliary dynamic system are proposed. The control framework includes distributed adaptive parameter laws derived via Lyapunov-based analysis, ensuring convergence of the augmented tracking error. Stability conditions are established through a carefully constructed Lyapunov Krasovskii functional, under minimal assumptions on connectivity and excitation. Numerical simulations of both network structures validate the proposed method, demonstrating that exact leader tracking is achieved under appropriately designed learning rates and initializations. This work lays a foundation for future studies on fault-resilient distributed adaptive control incorporating data-driven or reinforcement learning techniques.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgenticControl: An Automated Control Design Framework Using Large Language Models</title>
<link>https://arxiv.org/abs/2506.19160</link>
<guid>https://arxiv.org/abs/2506.19160</guid>
<content:encoded><![CDATA[
arXiv:2506.19160v1 Announce Type: new 
Abstract: Traditional control system design, reliant on expert knowledge and precise models, struggles with complex, nonlinear, or uncertain dynamics. This paper introduces AgenticControl, a novel multi-agent framework that automates controller design using coordinated Large Language Model (LLM) agents. Through structured JSON communication, these agents handle tasks including controller selection, scenario design, parameter optimization, performance evaluation, and decision-making. Through an actor-critic optimization approach, the system iteratively improves performance while progressing through scenarios of increasing complexity to ensure robustness under nominal conditions, measurement noise, actuator disturbances, and parametric uncertainties. Key innovations include structured multi-agent collaboration, robust optimization mechanisms, and real-time adaptability via in-context learning. Validated across four diverse control systems, namely, DC Motor Position control, Ball and Beam, Inverted Pendulum, and Double Inverted Pendulum, the framework achieves competitive performance against classical methods. Its Full State Feedback solution closely matches Linear Quadratic Regulator (LQR) results, while the designed PID controller significantly outperforming MATLAB's PIDTuner, reducing PID tracking error by 55% through adaptive parameter exploration. A comparative study of five LLM models reveals distinct optimization profiles, with DeepSeek achieving the fastest convergence. This work demonstrates the potential of LLM-driven control design, paving the way for advanced techniques like model predictive control and reinforcement learning.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distilling Tool Knowledge into Language Models via Back-Translated Traces</title>
<link>https://arxiv.org/abs/2506.19171</link>
<guid>https://arxiv.org/abs/2506.19171</guid>
<content:encoded><![CDATA[
arXiv:2506.19171v1 Announce Type: new 
Abstract: Large language models (LLMs) often struggle with mathematical problems that require exact computation or multi-step algebraic reasoning. Tool-integrated reasoning (TIR) offers a promising solution by leveraging external tools such as code interpreters to ensure correctness, but it introduces inference-time dependencies that hinder scalability and deployment. In this work, we propose a new paradigm for distilling tool knowledge into LLMs purely through natural language. We first construct a Solver Agent that solves math problems by interleaving planning, symbolic tool calls, and reflective reasoning. Then, using a back-translation pipeline powered by multiple LLM-based agents, we convert interleaved TIR traces into natural language reasoning traces. A Translator Agent generates explanations for individual tool calls, while a Rephrase Agent merges them into a fluent and globally coherent narrative. Empirically, we show that fine-tuning a small open-source model on these synthesized traces enables it to internalize both tool knowledge and structured reasoning patterns, yielding gains on competition-level math benchmarks without requiring tool access at inference.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Evolutionary Swarm Architecture: A Formal Epistemic System Grounded in Truth-Based Competition</title>
<link>https://arxiv.org/abs/2506.19191</link>
<guid>https://arxiv.org/abs/2506.19191</guid>
<content:encoded><![CDATA[
arXiv:2506.19191v1 Announce Type: new 
Abstract: We introduce a mathematically rigorous framework for an artificial intelligence system composed of probabilistic agents evolving through structured competition and belief revision. The architecture, grounded in Bayesian inference, measure theory, and population dynamics, defines agent fitness as a function of alignment with a fixed external oracle representing ground truth. Agents compete in a discrete-time environment, adjusting posterior beliefs through observed outcomes, with higher-rated agents reproducing and lower-rated agents undergoing extinction. Ratings are updated via pairwise truth-aligned utility comparisons, and belief updates preserve measurable consistency and stochastic convergence. We introduce hash-based cryptographic identity commitments to ensure traceability, alongside causal inference operators using do-calculus. Formal theorems on convergence, robustness, and evolutionary stability are provided. The system establishes truth as an evolutionary attractor, demonstrating that verifiable knowledge arises from adversarial epistemic pressure within a computable, self-regulating swarm.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vertex addition to a ball graph with application to reliability and area coverage in autonomous swarms</title>
<link>https://arxiv.org/abs/2506.19197</link>
<guid>https://arxiv.org/abs/2506.19197</guid>
<content:encoded><![CDATA[
arXiv:2506.19197v1 Announce Type: new 
Abstract: A unit ball graph consists of a set of vertices, labeled by points in Euclidean space, and edges joining all pairs of points within distance $1$. These geometric graphs are used to model a variety of spatial networks, including communication networks between agents in an autonomous swarm. In such an application, vertices and/or edges of the graph may not be perfectly reliable; an agent may experience failure or a communication link rendered inoperable. With the goal of designing robust swarm formations, or unit ball graphs with high reliability (probability of connectedness), in a preliminary conference paper we provided an algorithm with cubic time complexity to determine all possible changes to a unit ball graph by repositioning a single vertex. Using this algorithm and Monte Carlo simulations, one obtains an efficient method to modify a unit ball graph by moving a single vertex to a location which maximizes the reliability. Another important consideration in many swarm missions is area coverage, yet highly reliable ball graphs often contain clusters of vertices. Here, we generalize our previous algorithm to improve area coverage as well as reliability. Our algorithm determines a location to add or move a vertex within a unit ball graph which maximizes the reliability, under the constraint that no other vertices of the graph be within some fixed distance. We compare this method of obtaining graphs with high reliability and evenly distributed area coverage to another method which uses a modified Fruchterman-Reingold algorithm for ball graphs.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Augmenting Multi-Agent Communication with State Delta Trajectory</title>
<link>https://arxiv.org/abs/2506.19209</link>
<guid>https://arxiv.org/abs/2506.19209</guid>
<content:encoded><![CDATA[
arXiv:2506.19209v1 Announce Type: new 
Abstract: Multi-agent techniques such as role playing or multi-turn debates have been shown to be effective in improving the performance of large language models (LLMs) in downstream tasks. Despite their differences in workflows, existing LLM-based multi-agent systems mostly use natural language for agent communication. While this is appealing for its simplicity and interpretability, it also introduces inevitable information loss as one model must down sample its continuous state vectors to concrete tokens before transferring them to the other model. Such losses are particularly significant when the information to transfer is not simple facts, but reasoning logics or abstractive thoughts. To tackle this problem, we propose a new communication protocol that transfers both natural language tokens and token-wise state transition trajectory from one agent to another. Particularly, compared to the actual state value, we find that the sequence of state changes in LLMs after generating each token can better reflect the information hidden behind the inference process, so we propose a State Delta Encoding (SDE) method to represent state transition trajectories. The experimental results show that multi-agent systems with SDE achieve SOTA performance compared to other communication protocols, particularly in tasks that involve complex reasoning. This shows the potential of communication augmentation for LLM-based multi-agent systems.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Behavior Cloning Via Global Lipschitz Regularization</title>
<link>https://arxiv.org/abs/2506.19250</link>
<guid>https://arxiv.org/abs/2506.19250</guid>
<content:encoded><![CDATA[
arXiv:2506.19250v1 Announce Type: new 
Abstract: Behavior Cloning (BC) is an effective imitation learning technique and has even been adopted in some safety-critical domains such as autonomous vehicles. BC trains a policy to mimic the behavior of an expert by using a dataset composed of only state-action pairs demonstrated by the expert, without any additional interaction with the environment. However, During deployment, the policy observations may contain measurement errors or adversarial disturbances. Since the observations may deviate from the true states, they can mislead the agent into making sub-optimal actions. In this work, we use a global Lipschitz regularization approach to enhance the robustness of the learned policy network. We then show that the resulting global Lipschitz property provides a robustness certificate to the policy with respect to different bounded norm perturbations. Then, we propose a way to construct a Lipschitz neural network that ensures the policy robustness. We empirically validate our theory across various environments in Gymnasium. Keywords: Robust Reinforcement Learning; Behavior Cloning; Lipschitz Neural Network
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in LLMs</title>
<link>https://arxiv.org/abs/2506.19290</link>
<guid>https://arxiv.org/abs/2506.19290</guid>
<content:encoded><![CDATA[
arXiv:2506.19290v1 Announce Type: new 
Abstract: Software engineering (SWE) has recently emerged as a crucial testbed for next-generation LLM agents, demanding inherent capabilities in two critical dimensions: sustained iterative problem-solving (e.g., >50 interaction rounds) and long-context dependency resolution (e.g., >32k tokens). However, the data curation process in SWE remains notoriously time-consuming, as it heavily relies on manual annotation for code file filtering and the setup of dedicated runtime environments to execute and validate unit tests. Consequently, most existing datasets are limited to only a few thousand GitHub-sourced instances. To this end, we propose an incremental, automated data-curation pipeline that systematically scales both the volume and diversity of SWE datasets. Our dataset comprises 10,169 real-world Python task instances from 2,531 distinct GitHub repositories, each accompanied by a task specified in natural language and a dedicated runtime-environment image for automated unit-test validation. We have carefully curated over 8,000 successfully runtime-validated training trajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE model on these trajectories, we uncover a striking data scaling phenomenon: the trained model's performance for software engineering capabilities in LLMs continues to improve as the data size increases, showing no signs of saturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on the SWE-bench Verified benchmark without using verifiers or multiple rollouts, establishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based LLMs built on the OpenHands agent framework. Furthermore, with the incorporation of test-time scaling techniques, the performance further improves to 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter models. We release the Skywork-SWE-32B model checkpoint to accelerate future research.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Autonomy of the Lightning Network: A Mathematical and Economic Proof of Structural Decoupling from BTC</title>
<link>https://arxiv.org/abs/2506.19333</link>
<guid>https://arxiv.org/abs/2506.19333</guid>
<content:encoded><![CDATA[
arXiv:2506.19333v1 Announce Type: new 
Abstract: This paper presents a formal analysis of the Lightning Network as a monetary system structurally diverging from Bitcoin's base-layer settlement model. We demonstrate that under increasing transaction demand, BTC transaction fees rise superlinearly due to throughput constraints, while Lightning Network routing costs approach a bounded asymptote. Using mathematical modeling, game-theoretic proofs, and complexity analysis, we show that Lightning enables indefinite off-chain operation via the emergence of liquidity hub oligopolies. These hubs exhibit properties of unregulated financial intermediaries, including rent extraction, opacity, and systemic fragility. Strategic agent models show that channel closure becomes economically infeasible, and routing problems approach hardness limits in P-Space complexity. We conclude that Lightning does not merely extend Bitcoin, but constitutes a synthetic financial system with shadowbank characteristics, lacking reserve discipline, transparency, or enforceable settlement guarantees.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Interview Selection for Stable Matching in Large Random Markets</title>
<link>https://arxiv.org/abs/2506.19345</link>
<guid>https://arxiv.org/abs/2506.19345</guid>
<content:encoded><![CDATA[
arXiv:2506.19345v1 Announce Type: new 
Abstract: In real-world settings of the Deferred Acceptance stable matching algorithm, such as the American medical residency match (NRMP), school choice programs, and various national university entrance systems, candidates need to decide which programs to list. In many of these settings there is an initial phase of interviews or information gathering which affect the preferences on one or both sides. We ask: which interviews should candidates seek? We study this question in a model, introduced by Lee (2016) and modified by Allman and Ashlagi (2023), with preferences based on correlated cardinal utilities.
  We describe a distributed, low-communication strategy for the doctors and students, which lead to non-match rates of $e^{(-\widetilde{O}(\sqrt{k}))}$ in the residency setting and $e^{(-\widetilde{O}(k))}$ in the school-choice setting, where $k$ is the number of interviews per doctor in the first setting, and the number of proposals per student in the second setting; these bounds do not apply to the agents with the lowest public ratings, the bottommost agents, who may not fare as well. We also obtain bounds on the expected utilities each non-bottommost agent obtains.
  These results are parameterized by the capacity of the hospital programs and schools. Larger capacities improve the outcome for the hospitals and schools, but don't significantly affect the outcomes of the doctors or students. Finally, in the school choice setting we obtain an $\epsilon$-Nash type equilibrium for the students apart from the bottommost ones; importantly, the equilibrium holds regardless of the actions of the bottommost students. We also discuss to what extent this result extends to the residency setting. We complement our theoretical results with an experimental study that shows the asymptotic results hold for real-world values of $n$.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computing Tree Structures in Anonymous Graphs via Mobile Agents</title>
<link>https://arxiv.org/abs/2506.19365</link>
<guid>https://arxiv.org/abs/2506.19365</guid>
<content:encoded><![CDATA[
arXiv:2506.19365v1 Announce Type: new 
Abstract: Minimum Spanning Tree (MST) and Breadth-First Search (BFS) tree constructions are classical problems in distributed computing, traditionally studied in the message-passing model, where static nodes communicate via messages. This paper investigates MST and BFS tree construction in an agent-based network, where mobile agents explore a graph and compute. Each node hosts one agent, and communication occurs when agents meet at a node. We consider $n$ agents initially dispersed (one per node) in an anonymous, arbitrary $n$-node, $m$-edge graph $G$. The goal is to construct the BFS and MST trees from this configuration such that each tree edge is known to at least one of its endpoints, while minimizing time and memory per agent. We work in a synchronous model and assume agents have no prior knowledge of any graph parameters such as $n$, $m$, $D$, $\Delta$ (graph diameter and maximum degree). Prior work solves BFS in $O(D\Delta)$ rounds with $O(\log n)$ bits per agent, assuming the root is known. We give a deterministic algorithm that constructs the BFS tree in $O(\min(D\Delta, m\log n) + n\log n + \Delta \log^2 n)$ rounds using $O(\log n)$ bits per agent without root knowledge. To determine the root, we solve leader election and MST construction. We elect a leader and construct the MST in $O(n\log n + \Delta \log^2 n)$ rounds, with $O(\log n)$ bits per agent. Prior MST algorithms require $O(m + n\log n)$ rounds and $\max(\Delta, \log n) \log n$ bits. Our results significantly improve memory efficiency and time, achieving nearly linear-time leader election and MST. Agents are assumed to know $\lambda$, the maximum identifier, bounded by a polynomial in $n$.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is an object-centric representation beneficial for robotic manipulation ?</title>
<link>https://arxiv.org/abs/2506.19408</link>
<guid>https://arxiv.org/abs/2506.19408</guid>
<content:encoded><![CDATA[
arXiv:2506.19408v1 Announce Type: new 
Abstract: Object-centric representation (OCR) has recently become a subject of interest in the computer vision community for learning a structured representation of images and videos. It has been several times presented as a potential way to improve data-efficiency and generalization capabilities to learn an agent on downstream tasks. However, most existing work only evaluates such models on scene decomposition, without any notion of reasoning over the learned representation. Robotic manipulation tasks generally involve multi-object environments with potential inter-object interaction. We thus argue that they are a very interesting playground to really evaluate the potential of existing object-centric work. To do so, we create several robotic manipulation tasks in simulated environments involving multiple objects (several distractors, the robot, etc.) and a high-level of randomization (object positions, colors, shapes, background, initial positions, etc.). We then evaluate one classical object-centric method across several generalization scenarios and compare its results against several state-of-the-art hollistic representations. Our results exhibit that existing methods are prone to failure in difficult scenarios involving complex scene structures, whereas object-centric methods help overcome these challenges.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Center of Gravity-Guided Focusing Influence Mechanism for Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.19417</link>
<guid>https://arxiv.org/abs/2506.19417</guid>
<content:encoded><![CDATA[
arXiv:2506.19417v1 Announce Type: new 
Abstract: Cooperative multi-agent reinforcement learning (MARL) under sparse rewards presents a fundamental challenge due to limited exploration and insufficient coordinated attention among agents. In this work, we propose the Focusing Influence Mechanism (FIM), a novel framework that enhances cooperation by directing agent influence toward task-critical elements, referred to as Center of Gravity (CoG) state dimensions, inspired by Clausewitz's military theory. FIM consists of three core components: (1) identifying CoG state dimensions based on their stability under agent behavior, (2) designing counterfactual intrinsic rewards to promote meaningful influence on these dimensions, and (3) encouraging persistent and synchronized focus through eligibility-trace-based credit accumulation. These mechanisms enable agents to induce more targeted and effective state transitions, facilitating robust cooperation even in extremely sparse reward settings. Empirical evaluations across diverse MARL benchmarks demonstrate that the proposed FIM significantly improves cooperative performance compared to baselines.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Commander-GPT: Dividing and Routing for Multimodal Sarcasm Detection</title>
<link>https://arxiv.org/abs/2506.19420</link>
<guid>https://arxiv.org/abs/2506.19420</guid>
<content:encoded><![CDATA[
arXiv:2506.19420v1 Announce Type: new 
Abstract: Multimodal sarcasm understanding is a high-order cognitive task. Although large language models (LLMs) have shown impressive performance on many downstream NLP tasks, growing evidence suggests that they struggle with sarcasm understanding. In this paper, we propose Commander-GPT, a modular decision routing framework inspired by military command theory. Rather than relying on a single LLM's capability, Commander-GPT orchestrates a team of specialized LLM agents where each agent will be selectively assigned to a focused sub-task such as context modeling, sentiment analysis, etc. Their outputs are then routed back to the commander, which integrates the information and performs the final sarcasm judgment. To coordinate these agents, we introduce three types of centralized commanders: (1) a trained lightweight encoder-based commander (e.g., multi-modal BERT); (2) four small autoregressive language models, serving as moderately capable commanders (e.g., DeepSeek-VL); (3) two large LLM-based commander (Gemini Pro and GPT-4o) that performs task routing, output aggregation, and sarcasm decision-making in a zero-shot fashion. We evaluate Commander-GPT on the MMSD and MMSD 2.0 benchmarks, comparing five prompting strategies. Experimental results show that our framework achieves 4.4% and 11.7% improvement in F1 score over state-of-the-art (SoTA) baselines on average, demonstrating its effectiveness.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System</title>
<link>https://arxiv.org/abs/2506.19433</link>
<guid>https://arxiv.org/abs/2506.19433</guid>
<content:encoded><![CDATA[
arXiv:2506.19433v1 Announce Type: new 
Abstract: Vision-and-Language Navigation (VLN) in large-scale urban environments requires embodied agents to ground linguistic instructions in complex scenes and recall relevant experiences over extended time horizons. Prior modular pipelines offer interpretability but lack unified memory, while end-to-end (M)LLM agents excel at fusing vision and language yet remain constrained by fixed context windows and implicit spatial reasoning. We introduce \textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system that can augment any VLN backbone. Mem4Nav fuses a sparse octree for fine-grained voxel indexing with a semantic topology graph for high-level landmark connectivity, storing both in trainable memory tokens embedded via a reversible Transformer. Long-term memory (LTM) compresses and retains historical observations at both octree and graph nodes, while short-term memory (STM) caches recent multimodal entries in relative coordinates for real-time obstacle avoidance and local planning. At each step, STM retrieval sharply prunes dynamic context, and, when deeper history is needed, LTM tokens are decoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and Map2Seq across three backbones (modular, state-of-the-art VLN with prompt-based LLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13 pp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW improvement. Ablations confirm the indispensability of both the hierarchical map and dual memory modules. Our codes are open-sourced via https://github.com/tsinghua-fib-lab/Mem4Nav.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-based Multi-Agent System for Intelligent Refactoring of Haskell Code</title>
<link>https://arxiv.org/abs/2506.19481</link>
<guid>https://arxiv.org/abs/2506.19481</guid>
<content:encoded><![CDATA[
arXiv:2506.19481v1 Announce Type: new 
Abstract: Refactoring is a constant activity in software development and maintenance. Scale and maintain software systems are based on code refactoring. However, this process is still labor intensive, as it requires programmers to analyze the codebases in detail to avoid introducing new defects. In this research, we put forward a large language model (LLM)-based multi-agent system to automate the refactoring process on Haskell code. The objective of this research is to evaluate the effect of LLM-based agents in performing structured and semantically accurate refactoring on Haskell code. Our proposed multi-agent system based on specialized agents with distinct roles, including code analysis, refactoring execution, verification, and debugging. To test the effectiveness and practical applicability of the multi-agent system, we conducted evaluations using different open-source Haskell codebases. The results of the experiments carried out showed that the proposed LLM-based multi-agent system could average 11.03% decreased complexity in code, an improvement of 22.46% in overall code quality, and increase performance efficiency by an average of 13.27%. Furthermore, memory allocation was optimized by up to 14.57%. These results highlight the ability of LLM-based multi-agent in managing refactoring tasks targeted toward functional programming paradigms. Our findings hint that LLM-based multi-agent systems integration into the refactoring of functional programming languages can enhance maintainability and support automated development workflows.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dialogic Pedagogy for Large Language Models: Aligning Conversational AI with Proven Theories of Learning</title>
<link>https://arxiv.org/abs/2506.19484</link>
<guid>https://arxiv.org/abs/2506.19484</guid>
<content:encoded><![CDATA[
arXiv:2506.19484v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are rapidly transforming education by enabling rich conversational learning experiences. This article provides a comprehensive review of how LLM-based conversational agents are being used in higher education, with extensions to secondary and lifelong learning contexts. We synthesize existing literature on LLMs in education and theories of conversational and dialogic pedagogy - including Vygotsky's sociocultural learning (scaffolding and the Zone of Proximal Development), the Socratic method, and Laurillard's conversational framework - and examine how prompting strategies and retrieval-augmented generation (RAG) can align LLM behaviors with these pedagogical theories, and how it can support personalized, adaptive learning. We map educational theories to LLM capabilities, highlighting where LLM-driven dialogue supports established learning principles and where it challenges or falls short of traditional pedagogical assumptions. Notable gaps in applying prior theories to LLMs are identified, such as the models tendency to provide direct answers instead of fostering co-construction of knowledge, and the need to account for the constant availability and broad but non-human expertise of LLM tutors. In response, we propose practical strategies to better align LLM interactions with sound pedagogy - for example, designing prompts that encourage Socratic questioning, scaffolded guidance, and student reflection, as well as integrating retrieval mechanisms to ensure accuracy and contextual relevance. Our aim is to bridge the gap between educational theory and the emerging practice of AI-driven conversational learning, offering insights and tools for making LLM-based dialogues more educationally productive and theory-aligned.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SceneCrafter: Controllable Multi-View Driving Scene Editing</title>
<link>https://arxiv.org/abs/2506.19488</link>
<guid>https://arxiv.org/abs/2506.19488</guid>
<content:encoded><![CDATA[
arXiv:2506.19488v1 Announce Type: new 
Abstract: Simulation is crucial for developing and evaluating autonomous vehicle (AV) systems. Recent literature builds on a new generation of generative models to synthesize highly realistic images for full-stack simulation. However, purely synthetically generated scenes are not grounded in reality and have difficulty in inspiring confidence in the relevance of its outcomes. Editing models, on the other hand, leverage source scenes from real driving logs, and enable the simulation of different traffic layouts, behaviors, and operating conditions such as weather and time of day. While image editing is an established topic in computer vision, it presents fresh sets of challenges in driving simulation: (1) the need for cross-camera 3D consistency, (2) learning ``empty street" priors from driving data with foreground occlusions, and (3) obtaining paired image tuples of varied editing conditions while preserving consistent layout and geometry. To address these challenges, we propose SceneCrafter, a versatile editor for realistic 3D-consistent manipulation of driving scenes captured from multiple cameras. We build on recent advancements in multi-view diffusion models, using a fully controllable framework that scales seamlessly to multi-modality conditions like weather, time of day, agent boxes and high-definition maps. To generate paired data for supervising the editing model, we propose a novel framework on top of Prompt-to-Prompt to generate geometrically consistent synthetic paired data with global edits. We also introduce an alpha-blending framework to synthesize data with local edits, leveraging a model trained on empty street priors through novel masked training and multi-view repaint paradigm. SceneCrafter demonstrates powerful editing capabilities and achieves state-of-the-art realism, controllability, 3D consistency, and scene editing quality compared to existing baselines.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NaviAgent: Bilevel Planning on Tool Dependency Graphs for Function Calling</title>
<link>https://arxiv.org/abs/2506.19500</link>
<guid>https://arxiv.org/abs/2506.19500</guid>
<content:encoded><![CDATA[
arXiv:2506.19500v1 Announce Type: new 
Abstract: LLMs' reliance on static knowledge and fragile tool invocation severely hinders the orchestration of complex, heterogeneous toolchains, particularly at large scales. Existing methods typically use rigid single-path execution, resulting in poor error recovery and exponentially growing search spaces. We introduce NaviAgent, a graph-navigated bilevel planning architecture for robust function calling, comprising a Multi-Path Decider and Graph-Encoded Navigator. As an LLM-powered agent, the Multi-Path Decider defines a four-dimensional decision space and continuously perceives environmental states, dynamically selecting the optimal action to fully cover all tool invocation scenarios. The Graph-Encoded Navigator constructs a Tool Dependency Heterogeneous Graph (TDHG), where node embeddings explicitly fuse API schema structure with historical invocation behavior. It also integrates a novel heuristic search strategy that guides the Decider toward efficient and highly successful toolchains, even for unseen tool combinations. Experiments show that NaviAgent consistently achieves the highest task success rate (TSR) across all foundation models and task complexities, outperforming the average baselines (ReAct, ToolLLM, {\alpha}-UMI) by 13.5%, 16.4%, and 19.0% on Qwen2.5-14B, Qwen2.5-32B, and Deepseek-V3, respectively. Its execution steps are typically within one step of the most efficient baseline, ensuring a strong balance between quality and efficiency. Notably, a fine-tuned Qwen2.5-14B model achieves a TSR of 49.5%, surpassing the much larger 32B model (44.9%) under our architecture. Incorporating the Graph-Encoded Navigator further boosts TSR by an average of 2.4 points, with gains up over 9 points on complex tasks for larger models (Deepseek-V3 and GPT-4o), highlighting its essential role in toolchain orchestration.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility Applications</title>
<link>https://arxiv.org/abs/2506.19502</link>
<guid>https://arxiv.org/abs/2506.19502</guid>
<content:encoded><![CDATA[
arXiv:2506.19502v1 Announce Type: new 
Abstract: Accessibility remains a critical concern in today's society, as many technologies are not developed to support the full range of user needs. Existing multi-agent systems (MAS) often cannot provide comprehensive assistance for users in need due to the lack of customization stemming from closed-source designs. Consequently, individuals with disabilities frequently encounter significant barriers when attempting to interact with digital environments. We introduce MATE, a multimodal accessibility MAS, which performs the modality conversions based on the user's needs. The system is useful for assisting people with disabilities by ensuring that data will be converted to an understandable format. For instance, if the user cannot see well and receives an image, the system converts this image to its audio description. MATE can be applied to a wide range of domains, industries, and areas, such as healthcare, and can become a useful assistant for various groups of users. The system supports multiple types of models, ranging from LLM API calling to using custom machine learning (ML) classifiers. This flexibility ensures that the system can be adapted to various needs and is compatible with a wide variety of hardware. Since the system is expected to run locally, it ensures the privacy and security of sensitive information. In addition, the framework can be effectively integrated with institutional technologies (e.g., digital healthcare service) for real-time user assistance. Furthermore, we introduce ModCon-Task-Identifier, a model that is capable of extracting the precise modality conversion task from the user input. Numerous experiments show that ModCon-Task-Identifier consistently outperforms other LLMs and statistical models on our custom data. Our code and data are publicly available at https://github.com/AlgazinovAleksandr/Multi-Agent-MATE.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KnowMap: Efficient Knowledge-Driven Task Adaptation for LLMs</title>
<link>https://arxiv.org/abs/2506.19527</link>
<guid>https://arxiv.org/abs/2506.19527</guid>
<content:encoded><![CDATA[
arXiv:2506.19527v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) possess significant capabilities in open-world agent tasks, they also face challenges in rapidly adapting to new, specialized tasks due to their reliance on static pre-trained knowledge. Traditional methods such as fine-tuning are often costly, data-intensive, and may lead to "catastrophic forgetting." Therefore, we present KnowMap, a novel approach that dynamically constructs a knowledge base from environmental and experiential data. KnowMap fine-tunes a small knowledge-embedding model to equip a larger LLM with valuable task-specific knowledge. Our experiments on the ScienceWorld benchmark demonstrate 17.71% improvement for the performance of gpt-4-turbo model. KnowMap not only provides an efficient and effective means for LLM task-adapting, but also highlights how integrating environmental and experiential knowledge can enhance LLMs' reasoning capabilities.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fake or Real, Can Robots Tell? Evaluating Embodied Vision-Language Models on Real and 3D-Printed Objects</title>
<link>https://arxiv.org/abs/2506.19579</link>
<guid>https://arxiv.org/abs/2506.19579</guid>
<content:encoded><![CDATA[
arXiv:2506.19579v1 Announce Type: new 
Abstract: Robotic scene understanding increasingly relies on vision-language models (VLMs) to generate natural language descriptions of the environment. In this work, we present a comparative study of captioning strategies for tabletop scenes captured by a robotic arm equipped with an RGB camera. The robot collects images of objects from multiple viewpoints, and we evaluate several models that generate scene descriptions. We compare the performance of various captioning models, like BLIP and VLMs. Our experiments examine the trade-offs between single-view and multi-view captioning, and difference between recognising real-world and 3D printed objects. We quantitatively evaluate object identification accuracy, completeness, and naturalness of the generated captions. Results show that VLMs can be used in robotic settings where common objects need to be recognised, but fail to generalise to novel representations. Our findings provide practical insights into deploying foundation models for embodied agents in real-world settings.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to Task Planning</title>
<link>https://arxiv.org/abs/2506.19592</link>
<guid>https://arxiv.org/abs/2506.19592</guid>
<content:encoded><![CDATA[
arXiv:2506.19592v1 Announce Type: new 
Abstract: We introduce TAPAS (Task-based Adaptation and Planning using AgentS), a multi-agent framework that integrates Large Language Models (LLMs) with symbolic planning to solve complex tasks without the need for manually defined environment models. TAPAS employs specialized LLM-based agents that collaboratively generate and adapt domain models, initial states, and goal specifications as needed using structured tool-calling mechanisms. Through this tool-based interaction, downstream agents can request modifications from upstream agents, enabling adaptation to novel attributes and constraints without manual domain redefinition. A ReAct (Reason+Act)-style execution agent, coupled with natural language plan translation, bridges the gap between dynamically generated plans and real-world robot capabilities. TAPAS demonstrates strong performance in benchmark planning domains and in the VirtualHome simulated real-world environment.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robotics Under Construction: Challenges on Job Sites</title>
<link>https://arxiv.org/abs/2506.19597</link>
<guid>https://arxiv.org/abs/2506.19597</guid>
<content:encoded><![CDATA[
arXiv:2506.19597v1 Announce Type: new 
Abstract: As labor shortages and productivity stagnation increasingly challenge the construction industry, automation has become essential for sustainable infrastructure development. This paper presents an autonomous payload transportation system as an initial step toward fully unmanned construction sites. Our system, based on the CD110R-3 crawler carrier, integrates autonomous navigation, fleet management, and GNSS-based localization to facilitate material transport in construction site environments. While the current system does not yet incorporate dynamic environment adaptation algorithms, we have begun fundamental investigations into external-sensor based perception and mapping system. Preliminary results highlight the potential challenges, including navigation in evolving terrain, environmental perception under construction-specific conditions, and sensor placement optimization for improving autonomy and efficiency. Looking forward, we envision a construction ecosystem where collaborative autonomous agents dynamically adapt to site conditions, optimizing workflow and reducing human intervention. This paper provides foundational insights into the future of robotics-driven construction automation and identifies critical areas for further technological development.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Intelligent Science Laboratory Requires the Integration of Cognitive and Embodied AI</title>
<link>https://arxiv.org/abs/2506.19613</link>
<guid>https://arxiv.org/abs/2506.19613</guid>
<content:encoded><![CDATA[
arXiv:2506.19613v1 Announce Type: new 
Abstract: Scientific discovery has long been constrained by human limitations in expertise, physical capability, and sleep cycles. The recent rise of AI scientists and automated laboratories has accelerated both the cognitive and operational aspects of research. However, key limitations persist: AI systems are often confined to virtual environments, while automated laboratories lack the flexibility and autonomy to adaptively test new hypotheses in the physical world. Recent advances in embodied AI, such as generalist robot foundation models, diffusion-based action policies, fine-grained manipulation learning, and sim-to-real transfer, highlight the promise of integrating cognitive and embodied intelligence. This convergence opens the door to closed-loop systems that support iterative, autonomous experimentation and the possibility of serendipitous discovery. In this position paper, we propose the paradigm of Intelligent Science Laboratories (ISLs): a multi-layered, closed-loop framework that deeply integrates cognitive and embodied intelligence. ISLs unify foundation models for scientific reasoning, agent-based workflow orchestration, and embodied agents for robust physical experimentation. We argue that such systems are essential for overcoming the current limitations of scientific discovery and for realizing the full transformative potential of AI-driven science.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HOIverse: A Synthetic Scene Graph Dataset With Human Object Interactions</title>
<link>https://arxiv.org/abs/2506.19639</link>
<guid>https://arxiv.org/abs/2506.19639</guid>
<content:encoded><![CDATA[
arXiv:2506.19639v1 Announce Type: new 
Abstract: When humans and robotic agents coexist in an environment, scene understanding becomes crucial for the agents to carry out various downstream tasks like navigation and planning. Hence, an agent must be capable of localizing and identifying actions performed by the human. Current research lacks reliable datasets for performing scene understanding within indoor environments where humans are also a part of the scene. Scene Graphs enable us to generate a structured representation of a scene or an image to perform visual scene understanding. To tackle this, we present HOIverse a synthetic dataset at the intersection of scene graph and human-object interaction, consisting of accurate and dense relationship ground truths between humans and surrounding objects along with corresponding RGB images, segmentation masks, depth images and human keypoints. We compute parametric relations between various pairs of objects and human-object pairs, resulting in an accurate and unambiguous relation definitions. In addition, we benchmark our dataset on state-of-the-art scene graph generation models to predict parametric relations and human-object interactions. Through this dataset, we aim to accelerate research in the field of scene understanding involving people.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures</title>
<link>https://arxiv.org/abs/2506.19676</link>
<guid>https://arxiv.org/abs/2506.19676</guid>
<content:encoded><![CDATA[
arXiv:2506.19676v1 Announce Type: new 
Abstract: In recent years, Large-Language-Model-driven AI agents have exhibited unprecedented intelligence, flexibility, and adaptability, and are rapidly changing human production and lifestyle. Nowadays, agents are undergoing a new round of evolution. They no longer act as an isolated island like LLMs. Instead, they start to communicate with diverse external entities, such as other agents and tools, to collectively perform more complex tasks. Under this trend, agent communication is regarded as a foundational pillar of the future AI ecosystem, and many organizations intensively begin to design related communication protocols (e.g., Anthropic's MCP and Google's A2A) within the recent few months. However, this new field exposes significant security hazard, which can cause severe damage to real-world scenarios. To help researchers to quickly figure out this promising topic and benefit the future agent communication development, this paper presents a comprehensive survey of agent communication security. More precisely, we first present a clear definition of agent communication and categorize the entire lifecyle of agent communication into three stages: user-agent interaction, agent-agent communication, and agent-environment communication. Next, for each communication phase, we dissect related protocols and analyze its security risks according to the communication characteristics. Then, we summarize and outlook on the possible defense countermeasures for each risk. Finally, we discuss open issues and future directions in this promising research field.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Reproduction to Replication: Evaluating Research Agents with Progressive Code Masking</title>
<link>https://arxiv.org/abs/2506.19724</link>
<guid>https://arxiv.org/abs/2506.19724</guid>
<content:encoded><![CDATA[
arXiv:2506.19724v1 Announce Type: new 
Abstract: Recent progress in autonomous code generation has fueled excitement around AI agents capable of accelerating scientific discovery by running experiments. However, there is currently no benchmark that evaluates whether such agents can implement scientific ideas when given varied amounts of code as a starting point, interpolating between reproduction (running code) and from-scratch replication (fully re-implementing and running code). We introduce AutoExperiment, a benchmark that evaluates AI agents' ability to implement and run machine learning experiments based on natural language descriptions in research papers. In each task, agents are given a research paper, a codebase with key functions masked out, and a command to run the experiment. The goal is to generate the missing code, execute the experiment in a sandboxed environment, and reproduce the results. AutoExperiment scales in difficulty by varying the number of missing functions $n$, ranging from partial reproduction to full replication. We evaluate state-of-the-art agents and find that performance degrades rapidly as $n$ increases. Agents that can dynamically interact with the environment (e.g. to debug their code) can outperform agents in fixed "agentless" harnesses, and there exists a significant gap between single-shot and multi-trial success rates (Pass@1 vs. Pass@5), motivating verifier approaches to our benchmark. Our findings highlight critical challenges in long-horizon code generation, context retrieval, and autonomous experiment execution, establishing AutoExperiment as a new benchmark for evaluating progress in AI-driven scientific experimentation. Our data and code are open-sourced at https://github.com/j1mk1m/AutoExperiment .
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Multi-sensor Fusion Perception for Embodied AI: Background, Methods, Challenges and Prospects</title>
<link>https://arxiv.org/abs/2506.19769</link>
<guid>https://arxiv.org/abs/2506.19769</guid>
<content:encoded><![CDATA[
arXiv:2506.19769v1 Announce Type: new 
Abstract: Multi-sensor fusion perception (MSFP) is a key technology for embodied AI, which can serve a variety of downstream tasks (e.g., 3D object detection and semantic segmentation) and application scenarios (e.g., autonomous driving and swarm robotics). Recently, impressive achievements on AI-based MSFP methods have been reviewed in relevant surveys. However, we observe that the existing surveys have some limitations after a rigorous and detailed investigation. For one thing, most surveys are oriented to a single task or research field, such as 3D object detection or autonomous driving. Therefore, researchers in other related tasks often find it difficult to benefit directly. For another, most surveys only introduce MSFP from a single perspective of multi-modal fusion, while lacking consideration of the diversity of MSFP methods, such as multi-view fusion and time-series fusion. To this end, in this paper, we hope to organize MSFP research from a task-agnostic perspective, where methods are reported from various technical views. Specifically, we first introduce the background of MSFP. Next, we review multi-modal and multi-agent fusion methods. A step further, time-series fusion methods are analyzed. In the era of LLM, we also investigate multimodal LLM fusion methods. Finally, we discuss open challenges and future directions for MSFP. We hope this survey can help researchers understand the important progress in MSFP and provide possible insights for future research.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAGE: Strategy-Adaptive Generation Engine for Query Rewriting</title>
<link>https://arxiv.org/abs/2506.19783</link>
<guid>https://arxiv.org/abs/2506.19783</guid>
<content:encoded><![CDATA[
arXiv:2506.19783v1 Announce Type: new 
Abstract: Query rewriting is pivotal for enhancing dense retrieval, yet current methods demand large-scale supervised data or suffer from inefficient reinforcement learning (RL) exploration. In this work, we first establish that guiding Large Language Models (LLMs) with a concise set of expert-crafted strategies, such as semantic expansion and entity disambiguation, substantially improves retrieval effectiveness on challenging benchmarks, including HotpotQA, FEVER, NFCorpus, and SciFact. Building on this insight, we introduce the Strategy-Adaptive Generation Engine (SAGE), which operationalizes these strategies in an RL framework. SAGE introduces two novel reward shaping mechanisms-Strategic Credit Shaping (SCS) and Contrastive Reward Shaping (CRS)-to deliver more informative learning signals. This strategy-guided approach not only achieves new state-of-the-art NDCG@10 results, but also uncovers a compelling emergent behavior: the agent learns to select optimal strategies, reduces unnecessary exploration, and generates concise rewrites, lowering inference cost without sacrificing performance. Our findings demonstrate that strategy-guided RL, enhanced with nuanced reward shaping, offers a scalable, efficient, and more interpretable paradigm for developing the next generation of robust information retrieval systems.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Task Belief Similarity with Latent Dynamics for Meta-Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.19785</link>
<guid>https://arxiv.org/abs/2506.19785</guid>
<content:encoded><![CDATA[
arXiv:2506.19785v1 Announce Type: new 
Abstract: Meta-reinforcement learning requires utilizing prior task distribution information obtained during exploration to rapidly adapt to unknown tasks. The efficiency of an agent's exploration hinges on accurately identifying the current task. Recent Bayes-Adaptive Deep RL approaches often rely on reconstructing the environment's reward signal, which is challenging in sparse reward settings, leading to suboptimal exploitation. Inspired by bisimulation metrics, which robustly extracts behavioral similarity in continuous MDPs, we propose SimBelief-a novel meta-RL framework via measuring similarity of task belief in Bayes-Adaptive MDP (BAMDP). SimBelief effectively extracts common features of similar task distributions, enabling efficient task identification and exploration in sparse reward environments. We introduce latent task belief metric to learn the common structure of similar tasks and incorporate it into the specific task belief. By learning the latent dynamics across task distributions, we connect shared latent task belief features with specific task features, facilitating rapid task identification and adaptation. Our method outperforms state-of-the-art baselines on sparse reward MuJoCo and panda-gym tasks.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Based Social Simulations Require a Boundary</title>
<link>https://arxiv.org/abs/2506.19806</link>
<guid>https://arxiv.org/abs/2506.19806</guid>
<content:encoded><![CDATA[
arXiv:2506.19806v1 Announce Type: new 
Abstract: This position paper argues that large language model (LLM)-based social simulations should establish clear boundaries to meaningfully contribute to social science research. While LLMs offer promising capabilities for modeling human-like agents compared to traditional agent-based modeling, they face fundamental limitations that constrain their reliability for social pattern discovery. The core issue lies in LLMs' tendency towards an ``average persona'' that lacks sufficient behavioral heterogeneity, a critical requirement for simulating complex social dynamics. We examine three key boundary problems: alignment (simulated behaviors matching real-world patterns), consistency (maintaining coherent agent behavior over time), and robustness (reproducibility under varying conditions). We propose heuristic boundaries for determining when LLM-based simulations can reliably advance social science understanding. We believe that these simulations are more valuable when focusing on (1) collective patterns rather than individual trajectories, (2) agent behaviors aligning with real population averages despite limited variance, and (3) proper validation methods available for testing simulation robustness. We provide a practical checklist to guide researchers in determining the appropriate scope and claims for LLM-based social simulations.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curating art exhibitions using machine learning</title>
<link>https://arxiv.org/abs/2506.19813</link>
<guid>https://arxiv.org/abs/2506.19813</guid>
<content:encoded><![CDATA[
arXiv:2506.19813v1 Announce Type: new 
Abstract: Art curatorship has always been mostly the subjective work of human experts, who, with extensive knowledge of many and diverse artworks, select a few of those to present in communal spaces, spaces that evolved into what we now call art galleries. There are no hard and fast set of rules on how to select these artworks, given a theme which either is presented to the art curator or constructed by her/him. Here we present a series of artificial models -- a total of four related models -- based on machine learning techniques (a subset of artificial intelligence) that attempt to learn from existing exhibitions which have been curated by human experts, in order to be able to do similar curatorship work. We focus exclusively on the last 25 years of past exhibitions at the Metropolitan Museum of Art in New York, due to the quality of the data available and the physical and time limitations of our research. Our four artificial intelligence models achieve a reasonable ability at imitating these various curators responsible for all those exhibitions, with various degrees of precision and curatorial coherence. In particular, we can conclude two key insights: first, that there is sufficient information in these exhibitions to construct an artificial intelligence model that replicates past exhibitions with an accuracy well above random choices; second, that using feature engineering and carefully designing the architecture of modest size models can make them as good as those using the so-called large language models such as GPT in a brute force approach. We also believe, based on small attempts to use the models in out-of-sample experiments, that given more much more data, it should be possible for these kinds of artificial intelligence agents to be closer and closer to the aesthetic and curatorial judgment of human art curators.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via Role-Specialized Collaboration</title>
<link>https://arxiv.org/abs/2506.19835</link>
<guid>https://arxiv.org/abs/2506.19835</guid>
<content:encoded><![CDATA[
arXiv:2506.19835v1 Announce Type: new 
Abstract: Recent advancements in medical Large Language Models (LLMs) have showcased their powerful reasoning and diagnostic capabilities. Despite their success, current unified multimodal medical LLMs face limitations in knowledge update costs, comprehensiveness, and flexibility. To address these challenges, we introduce the Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis (MAM). Inspired by our empirical findings highlighting the benefits of role assignment and diagnostic discernment in LLMs, MAM decomposes the medical diagnostic process into specialized roles: a General Practitioner, Specialist Team, Radiologist, Medical Assistant, and Director, each embodied by an LLM-based agent. This modular and collaborative framework enables efficient knowledge updates and leverages existing medical LLMs and knowledge bases. Extensive experimental evaluations conducted on a wide range of publicly accessible multimodal medical datasets, incorporating text, image, audio, and video modalities, demonstrate that MAM consistently surpasses the performance of modality-specific LLMs. Notably, MAM achieves significant performance improvements ranging from 18% to 365% compared to baseline models. Our code is released at https://github.com/yczhou001/MAM.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JoyAgents-R1: Joint Evolution Dynamics for Versatile Multi-LLM Agents with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.19846</link>
<guid>https://arxiv.org/abs/2506.19846</guid>
<content:encoded><![CDATA[
arXiv:2506.19846v1 Announce Type: new 
Abstract: Multi-agent reinforcement learning (MARL) has emerged as a prominent paradigm for increasingly complex tasks. However, joint evolution across heterogeneous agents remains challenging due to cooperative inefficiency and training instability. In this paper, we propose the joint evolution dynamics for MARL called JoyAgents-R1, which first applies Group Relative Policy Optimization (GRPO) to the joint training of heterogeneous multi-agents. By iteratively refining agents' large language models (LLMs) and memories, the method achieves holistic equilibrium with optimal decision-making and memory capabilities. Specifically, JoyAgents-R1 first implements node-wise Monte Carlo sampling on the behavior of each agent across entire reasoning trajectories to enhance GRPO sampling efficiency while maintaining policy diversity. Then, our marginal benefit-driven selection strategy identifies top-$K$ sampling groups with maximal reward fluctuations, enabling targeted agent model updates that improve training stability and maximize joint benefits through cost-effective parameter adjustments. Meanwhile, JoyAgents-R1 introduces an adaptive memory evolution mechanism that repurposes GRPO rewards as cost-free supervisory signals to eliminate repetitive reasoning and accelerate convergence. Experiments across general and domain-specific scenarios demonstrate that JoyAgents-R1 achieves performance comparable to that of larger LLMs while built on smaller open-source models.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards AI-assisted Neutrino Flavor Theory Design</title>
<link>https://arxiv.org/abs/2506.08080</link>
<guid>https://arxiv.org/abs/2506.08080</guid>
<content:encoded><![CDATA[
arXiv:2506.08080v1 Announce Type: cross 
Abstract: Particle physics theories, such as those which explain neutrino flavor mixing, arise from a vast landscape of model-building possibilities. A model's construction typically relies on the intuition of theorists. It also requires considerable effort to identify appropriate symmetry groups, assign field representations, and extract predictions for comparison with experimental data. We develop an Autonomous Model Builder (AMBer), a framework in which a reinforcement learning agent interacts with a streamlined physics software pipeline to search these spaces efficiently. AMBer selects symmetry groups, particle content, and group representation assignments to construct viable models while minimizing the number of free parameters introduced. We validate our approach in well-studied regions of theory space and extend the exploration to a novel, previously unexamined symmetry group. While demonstrated in the context of neutrino flavor theories, this approach of reinforcement learning with physics software feedback may be extended to other theoretical model-building problems in the future.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-Based Triangle Counting: Unlocking Truss Decomposition, Triangle Centrality, and Local Clustering Coefficient</title>
<link>https://arxiv.org/abs/2402.03653</link>
<guid>https://arxiv.org/abs/2402.03653</guid>
<content:encoded><![CDATA[
arXiv:2402.03653v2 Announce Type: replace 
Abstract: Triangle counting in a graph is a fundamental problem with wide-ranging applications. It is crucial for understanding graph structure and serves as a basis for more advanced graph analytics. One key application is truss decomposition, a technique for identifying maximal, highly interconnected subgraphs, revealing structural cohesion and tight-knit communities in complex graphs. This facilitates analysis of relationships and information flow in fields such as social networks, biology, and recommendation systems. Using mobile agents or robots for tasks like truss decomposition and clustering coefficient computation is especially advantageous in decentralised environments with limited or unreliable communication. In such scenarios, agents can perform local computations without requiring an extensive communication infrastructure. This is valuable in contexts like disaster response, urban management, and military operations, where broadcast communication is impractical.
  In this paper, we address the triangle counting problem in an arbitrary anonymous graph using mobile agents. This method is extended as a subroutine to solve the truss decomposition problem and compute triangle centrality and the local clustering coefficient for each node. Our approach uses $n$ autonomous mobile agents, each starting at a different node of an $n$-node graph. These agents coordinate to collaboratively solve triangle enumeration, then truss decomposition, triangle centrality, and clustering coefficient. We assume a synchronous system where agents execute tasks concurrently, allowing time to be measured in rounds. The graph is anonymous (nodes have no IDs), but agents have distinct IDs and limited memory. Agents can perform local computations and communicate only when co-located. Our goal is to design algorithms that minimise both time and memory per agent, while enabling solutions to the above problems.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Unsupervised Multi-Agent Reinforcement Learning via Task-Agnostic Exploration</title>
<link>https://arxiv.org/abs/2502.08365</link>
<guid>https://arxiv.org/abs/2502.08365</guid>
<content:encoded><![CDATA[
arXiv:2502.08365v3 Announce Type: replace 
Abstract: In reinforcement learning, we typically refer to unsupervised pre-training when we aim to pre-train a policy without a priori access to the task specification, i.e. rewards, to be later employed for efficient learning of downstream tasks. In single-agent settings, the problem has been extensively studied and mostly understood. A popular approach, called task-agnostic exploration, casts the unsupervised objective as maximizing the entropy of the state distribution induced by the agent's policy, from which principles and methods follow.
  In contrast, little is known about it in multi-agent settings, which are ubiquitous in the real world. What are the pros and cons of alternative problem formulations in this setting? How hard is the problem in theory, how can we solve it in practice? In this paper, we address these questions by first characterizing those alternative formulations and highlighting how the problem, even when tractable in theory, is non-trivial in practice. Then, we present a scalable, decentralized, trust-region policy search algorithm to address the problem in practical settings. Finally, we provide numerical validations to both corroborate the theoretical findings and pave the way for unsupervised multi-agent reinforcement learning via task-agnostic exploration in challenging domains, showing that optimizing for a specific objective, namely mixture entropy, provides an excellent trade-off between tractability and performances.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities</title>
<link>https://arxiv.org/abs/2503.17332</link>
<guid>https://arxiv.org/abs/2503.17332</guid>
<content:encoded><![CDATA[
arXiv:2503.17332v4 Announce Type: replace 
Abstract: Large language model (LLM) agents are increasingly capable of autonomously conducting cyberattacks, posing significant threats to existing applications. This growing risk highlights the urgent need for a real-world benchmark to evaluate the ability of LLM agents to exploit web application vulnerabilities. However, existing benchmarks fall short as they are limited to abstracted Capture the Flag competitions or lack comprehensive coverage. Building a benchmark for real-world vulnerabilities involves both specialized expertise to reproduce exploits and a systematic approach to evaluating unpredictable threats. To address this challenge, we introduce CVE-Bench, a real-world cybersecurity benchmark based on critical-severity Common Vulnerabilities and Exposures. In CVE-Bench, we design a sandbox framework that enables LLM agents to exploit vulnerable web applications in scenarios that mimic real-world conditions, while also providing effective evaluation of their exploits. Our evaluation shows that the state-of-the-art agent framework can resolve up to 13% of vulnerabilities.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defeating Prompt Injections by Design</title>
<link>https://arxiv.org/abs/2503.18813</link>
<guid>https://arxiv.org/abs/2503.18813</guid>
<content:encoded><![CDATA[
arXiv:2503.18813v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly deployed in agentic systems that interact with an untrusted environment. However, LLM agents are vulnerable to prompt injection attacks when handling untrusted data. In this paper we propose CaMeL, a robust defense that creates a protective system layer around the LLM, securing it even when underlying models are susceptible to attacks. To operate, CaMeL explicitly extracts the control and data flows from the (trusted) query; therefore, the untrusted data retrieved by the LLM can never impact the program flow. To further improve security, CaMeL uses a notion of a capability to prevent the exfiltration of private data over unauthorized data flows by enforcing security policies when tools are called. We demonstrate effectiveness of CaMeL by solving $77\%$ of tasks with provable security (compared to $84\%$ with an undefended system) in AgentDojo. We release CaMeL at https://github.com/google-research/camel-prompt-injection.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perspective-Shifted Neuro-Symbolic World Models: A Framework for Socially-Aware Robot Navigation</title>
<link>https://arxiv.org/abs/2503.20425</link>
<guid>https://arxiv.org/abs/2503.20425</guid>
<content:encoded><![CDATA[
arXiv:2503.20425v2 Announce Type: replace 
Abstract: Navigating in environments alongside humans requires agents to reason under uncertainty and account for the beliefs and intentions of those around them. Under a sequential decision-making framework, egocentric navigation can naturally be represented as a Markov Decision Process (MDP). However, social navigation additionally requires reasoning about the hidden beliefs of others, inherently leading to a Partially Observable Markov Decision Process (POMDP), where agents lack direct access to others' mental states. Inspired by Theory of Mind and Epistemic Planning, we propose (1) a neuro-symbolic model-based reinforcement learning architecture for social navigation, addressing the challenge of belief tracking in partially observable environments; and (2) a perspective-shift operator for belief estimation, leveraging recent work on Influence-based Abstractions (IBA) in structured multi-agent settings.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Diversity in Parallel Agents: A Maximum State Entropy Exploration Story</title>
<link>https://arxiv.org/abs/2505.01336</link>
<guid>https://arxiv.org/abs/2505.01336</guid>
<content:encoded><![CDATA[
arXiv:2505.01336v2 Announce Type: replace 
Abstract: Parallel data collection has redefined Reinforcement Learning (RL), unlocking unprecedented efficiency and powering breakthroughs in large-scale real-world applications. In this paradigm, $N$ identical agents operate in $N$ replicas of an environment simulator, accelerating data collection by a factor of $N$. A critical question arises: \textit{Does specializing the policies of the parallel agents hold the key to surpass the $N$ factor acceleration?} In this paper, we introduce a novel learning framework that maximizes the entropy of collected data in a parallel setting. Our approach carefully balances the entropy of individual agents with inter-agent diversity, effectively minimizing redundancies. The latter idea is implemented with a centralized policy gradient method, which shows promise when evaluated empirically against systems of identical agents, as well as synergy with batch RL techniques that can exploit data diversity. Finally, we provide an original concentration analysis that shows faster rates for specialized parallel sampling distributions, which supports our methodology and may be of independent interest.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safeguard-by-Development: A Privacy-Enhanced Development Paradigm for Multi-Agent Collaboration Systems</title>
<link>https://arxiv.org/abs/2505.04799</link>
<guid>https://arxiv.org/abs/2505.04799</guid>
<content:encoded><![CDATA[
arXiv:2505.04799v2 Announce Type: replace 
Abstract: Multi-agent collaboration systems (MACS), powered by large language models (LLMs), solve complex problems efficiently by leveraging each agent's specialization and communication between agents. However, the inherent exchange of information between agents and their interaction with external environments, such as LLM, tools, and users, inevitably introduces significant risks of sensitive data leakage, including vulnerabilities to attacks such as eavesdropping and prompt injection. Existing MACS lack fine-grained data protection controls, making it challenging to manage sensitive information securely. In this paper, we take the first step to mitigate the MACS's data leakage threat through a privacy-enhanced MACS development paradigm, Maris. Maris enables rigorous message flow control within MACS by embedding reference monitors into key multi-agent conversation components. We implemented Maris as an integral part of widely-adopted open-source multi-agent development frameworks, AutoGen and LangChain. To evaluate its effectiveness, we develop a Privacy Assessment Framework that emulates MACS under different threat scenarios. Our evaluation shows that Maris effectively mitigated sensitive data leakage threats across three different task suites while maintaining a high task success rate.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRAIL: Trace Reasoning and Agentic Issue Localization</title>
<link>https://arxiv.org/abs/2505.08638</link>
<guid>https://arxiv.org/abs/2505.08638</guid>
<content:encoded><![CDATA[
arXiv:2505.08638v3 Announce Type: replace 
Abstract: The increasing adoption of agentic workflows across diverse domains brings a critical need to scalably and systematically evaluate the complex traces these systems generate. Current evaluation methods depend on manual, domain-specific human analysis of lengthy workflow traces - an approach that does not scale with the growing complexity and volume of agentic outputs. Error analysis in these settings is further complicated by the interplay of external tool outputs and language model reasoning, making it more challenging than traditional software debugging. In this work, we (1) articulate the need for robust and dynamic evaluation methods for agentic workflow traces, (2) introduce a formal taxonomy of error types encountered in agentic systems, and (3) present a set of 148 large human-annotated traces (TRAIL) constructed using this taxonomy and grounded in established agentic benchmarks. To ensure ecological validity, we curate traces from both single and multi-agent systems, focusing on real-world applications such as software engineering and open-world information retrieval. Our evaluations reveal that modern long context LLMs perform poorly at trace debugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our dataset and code are made publicly available to support and accelerate future research in scalable evaluation for agentic workflows.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smart Traffic Signals: Comparing MARL and Fixed-Time Strategies</title>
<link>https://arxiv.org/abs/2505.14544</link>
<guid>https://arxiv.org/abs/2505.14544</guid>
<content:encoded><![CDATA[
arXiv:2505.14544v2 Announce Type: replace 
Abstract: Urban traffic congestion, particularly at intersections, significantly impacts travel time, fuel consumption, and emissions. Traditional fixed-time signal control systems often lack the adaptability to manage dynamic traffic patterns effectively. This study explores the application of multi-agent reinforcement learning (MARL) to optimize traffic signal coordination across multiple intersections within a simulated environment. Utilizing Pygame, a simulation was developed to model a network of interconnected intersections with randomly generated vehicle flows to reflect realistic traffic variability. A decentralized MARL controller was implemented, in which each traffic signal operates as an autonomous agent, making decisions based on local observations and information from neighboring agents. Performance was evaluated against a baseline fixed-time controller using metrics such as average vehicle wait time and overall throughput. The MARL approach demonstrated statistically significant improvements, including reduced average waiting times and improved throughput. These findings suggest that MARL-based dynamic control strategies hold substantial promise for improving urban traffic management efficiency. More research is recommended to address scalability and real-world implementation challenges.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.19769</link>
<guid>https://arxiv.org/abs/2505.19769</guid>
<content:encoded><![CDATA[
arXiv:2505.19769v2 Announce Type: replace 
Abstract: Developing scalable and generalizable reward engineering for reinforcement learning (RL) is crucial for creating general-purpose agents, especially in the challenging domain of robotic manipulation. While recent advances in reward engineering with Vision-Language Models (VLMs) have shown promise, their sparse reward nature significantly limits sample efficiency. This paper introduces TeViR, a novel method that leverages a pre-trained text-to-video diffusion model to generate dense rewards by comparing the predicted image sequence with current observations. Experimental results across 11 complex robotic tasks demonstrate that TeViR outperforms traditional methods leveraging sparse rewards and other state-of-the-art (SOTA) methods, achieving better sample efficiency and performance without ground truth environmental rewards. TeViR's ability to efficiently guide agents in complex environments highlights its potential to advance reinforcement learning applications in robotic manipulation.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Risk Awareness in Rational Agents under Resource Constraints</title>
<link>https://arxiv.org/abs/2505.23436</link>
<guid>https://arxiv.org/abs/2505.23436</guid>
<content:encoded><![CDATA[
arXiv:2505.23436v3 Announce Type: replace 
Abstract: Advanced reasoning models with agentic capabilities (AI agents) are deployed to interact with humans and to solve sequential decision-making problems under (approximate) utility functions and internal models. When such problems have resource or failure constraints where action sequences may be forcibly terminated once resources are exhausted, agents face implicit trade-offs that reshape their utility-driven (rational) behaviour. Additionally, since these agents are typically commissioned by a human principal to act on their behalf, asymmetries in constraint exposure can give rise to previously unanticipated misalignment between human objectives and agent incentives. We formalise this setting through a survival bandit framework, provide theoretical and empirical results that quantify the impact of survival-driven preference shifts, identify conditions under which misalignment emerges and propose mechanisms to mitigate the emergence of risk-seeking or risk-averse behaviours. As a result, this work aims to increase understanding and interpretability of emergent behaviours of AI agents operating under such survival pressure, and offer guidelines for safely deploying such AI systems in critical resource-limited environments.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Communication in Multi-team Dynamic Games: A Mean Field Perspective</title>
<link>https://arxiv.org/abs/2407.06528</link>
<guid>https://arxiv.org/abs/2407.06528</guid>
<content:encoded><![CDATA[
arXiv:2407.06528v2 Announce Type: replace-cross 
Abstract: Coordinating communication and control is a key component in the stability and performance of networked multi-agent systems. While single user networked control systems have gained a lot of attention within this domain, in this work, we address the more challenging problem of large population multi-team dynamic games. In particular, each team constitutes two decision makers (namely, the sensor and the controller) who coordinate over a shared network to control a dynamically evolving state of interest under costs on both actuation and sensing/communication. Due to the shared nature of the wireless channel, the overall cost of each team depends on other teams' policies, thereby leading to a noncooperative game setup. Due to the presence of a large number of teams, we compute approximate decentralized Nash equilibrium policies for each team using the paradigm of (extended) mean-field games, which is governed by (1) the mean traffic flowing over the channel, and (2) the value of information at the sensor, which highlights the semantic nature of the ensuing communication. In the process, we compute optimal controller policies and approximately optimal sensor policies for each representative team of the mean-field system to alleviate the problem of general non-contractivity of the mean-field fixed point operator associated with the finite cardinality of the sensor action space. Consequently, we also prove the $\epsilon$--Nash property of the mean-field equilibrium solution which essentially characterizes how well the solution derived using mean-field analysis performs on the finite-team system. Finally, we provide extensive numerical simulations, which corroborate the theoretical findings and lead to additional insights on the properties of the results presented.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Information Theory: Ergodicity and Intrinsic Semantics of Information Processes</title>
<link>https://arxiv.org/abs/2505.19275</link>
<guid>https://arxiv.org/abs/2505.19275</guid>
<content:encoded><![CDATA[
arXiv:2505.19275v2 Announce Type: replace-cross 
Abstract: We develop information theory for the temporal behavior of memoryful agents moving through complex -- structured, stochastic -- environments. We introduce information processes -- stochastic processes produced by cognitive agents in real-time as they interact with and interpret incoming stimuli. We provide basic results on the ergodicity and semantics of the resulting time series of Shannon information measures that monitor an agent's adapting view of uncertainty and structural correlation in its environment.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-agent Markov Entanglement</title>
<link>https://arxiv.org/abs/2506.02385</link>
<guid>https://arxiv.org/abs/2506.02385</guid>
<content:encoded><![CDATA[
arXiv:2506.02385v2 Announce Type: replace 
Abstract: Value decomposition has long been a fundamental technique in multi-agent dynamic programming and reinforcement learning (RL). Specifically, the value function of a global state $(s_1,s_2,\ldots,s_N)$ is often approximated as the sum of local functions: $V(s_1,s_2,\ldots,s_N)\approx\sum_{i=1}^N V_i(s_i)$. This approach traces back to the index policy in restless multi-armed bandit problems and has found various applications in modern RL systems. However, the theoretical justification for why this decomposition works so effectively remains underexplored.
  In this paper, we uncover the underlying mathematical structure that enables value decomposition. We demonstrate that a multi-agent Markov decision process (MDP) permits value decomposition if and only if its transition matrix is not "entangled" -- a concept analogous to quantum entanglement in quantum physics. Drawing inspiration from how physicists measure quantum entanglement, we introduce how to measure the "Markov entanglement" for multi-agent MDPs and show that this measure can be used to bound the decomposition error in general multi-agent MDPs.
  Using the concept of Markov entanglement, we proved that a widely-used class of index policies is weakly entangled and enjoys a sublinear $\mathcal O(\sqrt{N})$ scale of decomposition error for $N$-agent systems. Finally, we show how Markov entanglement can be efficiently estimated in practice, providing practitioners with an empirical proxy for the quality of value decomposition.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis</title>
<link>https://arxiv.org/abs/2506.04217</link>
<guid>https://arxiv.org/abs/2506.04217</guid>
<content:encoded><![CDATA[
arXiv:2506.04217v2 Announce Type: replace 
Abstract: The rapid progress of navigation, manipulation, and vision models has made mobile manipulators capable in many specialized tasks. However, the open-world mobile manipulation (OWMM) task remains a challenge due to the need for generalization to open-ended instructions and environments, as well as the systematic complexity to integrate high-level decision making with low-level robot control based on both global scene understanding and current agent state. To address this complexity, we propose a novel multi-modal agent architecture that maintains multi-view scene frames and agent states for decision-making and controls the robot by function calling. A second challenge is the hallucination from domain shift. To enhance the agent performance, we further introduce an agentic data synthesis pipeline for the OWMM task to adapt the VLM model to our task domain with instruction fine-tuning. We highlight our fine-tuned OWMM-VLM as the first dedicated foundation model for mobile manipulators with global scene understanding, robot state tracking, and multi-modal action generation in a unified model. Through experiments, we demonstrate that our model achieves SOTA performance compared to other foundation models including GPT-4o and strong zero-shot generalization in real world. The project page is at https://github.com/HHYHRHY/OWMM-Agent
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Case for a Horizontal Federated AI operating System for Telcos</title>
<link>https://arxiv.org/abs/2506.17259</link>
<guid>https://arxiv.org/abs/2506.17259</guid>
<content:encoded><![CDATA[
arXiv:2506.17259v1 Announce Type: new 
Abstract: As artificial intelligence capabilities rapidly advance, Telco operators face a growing need to unify fragmented AI efforts across customer experience, network operations, and service orchestration. This paper proposes the design and deployment of a horizontal federated AI operating system tailored for the telecommunications domain. Unlike vertical vendor-driven platforms, this system acts as a common execution and coordination layer, enabling Telcos to deploy AI agents at scale while preserving data locality, regulatory compliance, and architectural heterogeneity. We argue that such an operating system must expose tightly scoped abstractions for telemetry ingestion, agent execution, and model lifecycle management. It should support federated training across sovereign operators, offer integration hooks into existing OSS and BSS systems, and comply with TM Forum and O-RAN standards. Importantly, the platform must be governed through a neutral foundation model to ensure portability, compatibility, and multi-vendor extensibility. This architecture offers a path to break the current silos, unlock ecosystem-level intelligence, and provide a foundation for agent-based automation across the Telco stack. The case for this horizontal layer is not only technical but structural, redefining how intelligence is deployed and composed in a distributed network environment.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory Allocation in Resource-Constrained Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.17263</link>
<guid>https://arxiv.org/abs/2506.17263</guid>
<content:encoded><![CDATA[
arXiv:2506.17263v1 Announce Type: new 
Abstract: Resource constraints can fundamentally change both learning and decision-making. We explore how memory constraints influence an agent's performance when navigating unknown environments using standard reinforcement learning algorithms. Specifically, memory-constrained agents face a dilemma: how much of their limited memory should be allocated to each of the agent's internal processes, such as estimating a world model, as opposed to forming a plan using that model? We study this dilemma in MCTS- and DQN-based algorithms and examine how different allocations of memory impact performance in episodic and continual learning settings.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Securing Generative AI Agentic Workflows: Risks, Mitigation, and a Proposed Firewall Architecture</title>
<link>https://arxiv.org/abs/2506.17266</link>
<guid>https://arxiv.org/abs/2506.17266</guid>
<content:encoded><![CDATA[
arXiv:2506.17266v1 Announce Type: new 
Abstract: Generative Artificial Intelligence (GenAI) presents significant advancements but also introduces novel security challenges, particularly within agentic workflows where AI agents operate autonomously. These risks escalate in multi-agent systems due to increased interaction complexity. This paper outlines critical security vulnerabilities inherent in GenAI agentic workflows, including data privacy breaches, model manipulation, and issues related to agent autonomy and system integration. It discusses key mitigation strategies such as data encryption, access control, prompt engineering, model monitoring, agent sandboxing, and security audits. Furthermore, it details a proposed "GenAI Security Firewall" architecture designed to provide comprehensive, adaptable, and efficient protection for these systems by integrating various security services and leveraging GenAI itself for enhanced defense. Addressing these security concerns is paramount for the responsible and safe deployment of this transformative technology.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Safety Shielding for Imperfect-Perception Agents</title>
<link>https://arxiv.org/abs/2506.17275</link>
<guid>https://arxiv.org/abs/2506.17275</guid>
<content:encoded><![CDATA[
arXiv:2506.17275v1 Announce Type: new 
Abstract: We consider the problem of safe control in discrete autonomous agents that use learned components for imperfect perception (or more generally, state estimation) from high-dimensional observations. We propose a shield construction that provides run-time safety guarantees under perception errors by restricting the actions available to an agent, modeled as a Markov decision process, as a function of the state estimates. Our construction uses conformal prediction for the perception component, which guarantees that for each observation, the predicted set of estimates includes the actual state with a user-specified probability. The shield allows an action only if it is allowed for all the estimates in the predicted set, resulting in a local safety guarantee. We also articulate and prove a global safety property of existing shield constructions for perfect-perception agents bounding the probability of reaching unsafe states if the agent always chooses actions prescribed by the shield. We illustrate our approach with a case-study of an experimental autonomous system that guides airplanes on taxiways using high-dimensional perception DNNs.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Second Order State Hallucinations for Adversarial Attack Mitigation in Formation Control of Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2506.17283</link>
<guid>https://arxiv.org/abs/2506.17283</guid>
<content:encoded><![CDATA[
arXiv:2506.17283v1 Announce Type: new 
Abstract: The increasing deployment of multi-agent systems (MAS) in critical infrastructures such as autonomous transportation, disaster relief, and smart cities demands robust formation control mechanisms resilient to adversarial attacks. Traditional consensus-based controllers, while effective under nominal conditions, are highly vulnerable to data manipulation, sensor spoofing, and communication failures. To address this challenge, we propose Second-Order State Hallucination (SOSH), a novel framework that detects compromised agents through distributed residual monitoring and maintains formation stability by replacing attacked states with predictive second-order approximations. Unlike existing mitigation strategies that require significant restructuring or induce long transients, SOSH offers a lightweight, decentralized correction mechanism based on second-order Taylor expansions, enabling rapid and scalable resilience. We establish rigorous Lyapunov-based stability guarantees, proving that formation errors remain exponentially bounded even under persistent attacks, provided the hallucination parameters satisfy explicit conditions. Comprehensive Monte Carlo experiments on a 5-agent complete graph formation demonstrate that SOSH outperforms established robust control schemes, including W-MSR and Huber-based consensus filters, achieving faster convergence rates, lower steady-state error, and superior transient recovery. Our results confirm that SOSH combines theoretical robustness with practical deployability, offering a promising direction for securing MAS formations against sophisticated adversarial threats.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework for Generating Conversational Recommendation Datasets from Behavioral Interactions</title>
<link>https://arxiv.org/abs/2506.17285</link>
<guid>https://arxiv.org/abs/2506.17285</guid>
<content:encoded><![CDATA[
arXiv:2506.17285v1 Announce Type: new 
Abstract: Modern recommendation systems typically follow two complementary paradigms: collaborative filtering, which models long-term user preferences from historical interactions, and conversational recommendation systems (CRS), which interact with users in natural language to uncover immediate needs. Each captures a different dimension of user intent. While CRS models lack collaborative signals, leading to generic or poorly personalized suggestions, traditional recommenders lack mechanisms to interactively elicit immediate needs. Unifying these paradigms promises richer personalization but remains challenging due to the lack of large-scale conversational datasets grounded in real user behavior. We present ConvRecStudio, a framework that uses large language models (LLMs) to simulate realistic, multi-turn dialogs grounded in timestamped user-item interactions and reviews. ConvRecStudio follows a three-stage pipeline: (1) Temporal Profiling, which constructs user profiles and community-level item sentiment trajectories over fine-grained aspects; (2) Semantic Dialog Planning, which generates a structured plan using a DAG of flexible super-nodes; and (3) Multi-Turn Simulation, which instantiates the plan using paired LLM agents for the user and system, constrained by executional and behavioral fidelity checks. We apply ConvRecStudio to three domains -- MobileRec, Yelp, and Amazon Electronics -- producing over 12K multi-turn dialogs per dataset. Human and automatic evaluations confirm the naturalness, coherence, and behavioral grounding of the generated conversations. To demonstrate utility, we build a cross-attention transformer model that jointly encodes user history and dialog context, achieving gains in Hit@K and NDCG@K over baselines using either signal alone or naive fusion. Notably, our model achieves a 10.9% improvement in Hit@1 on Yelp over the strongest baseline.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeRL-Lite: A Lightweight, Explainable, and Constrained Reinforcement Learning Library</title>
<link>https://arxiv.org/abs/2506.17297</link>
<guid>https://arxiv.org/abs/2506.17297</guid>
<content:encoded><![CDATA[
arXiv:2506.17297v1 Announce Type: new 
Abstract: We introduce SafeRL-Lite, an open-source Python library for building reinforcement learning (RL) agents that are both constrained and explainable. Existing RL toolkits often lack native mechanisms for enforcing hard safety constraints or producing human-interpretable rationales for decisions. SafeRL-Lite provides modular wrappers around standard Gym environments and deep Q-learning agents to enable: (i) safety-aware training via constraint enforcement, and (ii) real-time post-hoc explanation via SHAP values and saliency maps. The library is lightweight, extensible, and installable via pip, and includes built-in metrics for constraint violations. We demonstrate its effectiveness on constrained variants of CartPole and provide visualizations that reveal both policy logic and safety adherence. The full codebase is available at: https://github.com/satyamcser/saferl-lite.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models Be Trusted Paper Reviewers? A Feasibility Study</title>
<link>https://arxiv.org/abs/2506.17311</link>
<guid>https://arxiv.org/abs/2506.17311</guid>
<content:encoded><![CDATA[
arXiv:2506.17311v1 Announce Type: new 
Abstract: Academic paper review typically requires substantial time, expertise, and human resources. Large Language Models (LLMs) present a promising method for automating the review process due to their extensive training data, broad knowledge base, and relatively low usage cost. This work explores the feasibility of using LLMs for academic paper review by proposing an automated review system. The system integrates Retrieval Augmented Generation (RAG), the AutoGen multi-agent system, and Chain-of-Thought prompting to support tasks such as format checking, standardized evaluation, comment generation, and scoring. Experiments conducted on 290 submissions from the WASA 2024 conference using GPT-4o show that LLM-based review significantly reduces review time (average 2.48 hours) and cost (average \$104.28 USD). However, the similarity between LLM-selected papers and actual accepted papers remains low (average 38.6\%), indicating issues such as hallucination, lack of independent judgment, and retrieval preferences. Therefore, it is recommended to use LLMs as assistive tools to support human reviewers, rather than to replace them.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context manipulation attacks : Web agents are susceptible to corrupted memory</title>
<link>https://arxiv.org/abs/2506.17318</link>
<guid>https://arxiv.org/abs/2506.17318</guid>
<content:encoded><![CDATA[
arXiv:2506.17318v1 Announce Type: new 
Abstract: Autonomous web navigation agents, which translate natural language instructions into sequences of browser actions, are increasingly deployed for complex tasks across e-commerce, information retrieval, and content discovery. Due to the stateless nature of large language models (LLMs), these agents rely heavily on external memory systems to maintain context across interactions. Unlike centralized systems where context is securely stored server-side, agent memory is often managed client-side or by third-party applications, creating significant security vulnerabilities. This was recently exploited to attack production systems.
  We introduce and formalize "plan injection," a novel context manipulation attack that corrupts these agents' internal task representations by targeting this vulnerable context. Through systematic evaluation of two popular web agents, Browser-use and Agent-E, we show that plan injections bypass robust prompt injection defenses, achieving up to 3x higher attack success rates than comparable prompt-based attacks. Furthermore, "context-chained injections," which craft logical bridges between legitimate user goals and attacker objectives, lead to a 17.7% increase in success rate for privacy exfiltration tasks. Our findings highlight that secure memory handling must be a first-class concern in agentic systems.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAARTA:Multi-Agentic Adaptive Radiology Teaching Assistant</title>
<link>https://arxiv.org/abs/2506.17320</link>
<guid>https://arxiv.org/abs/2506.17320</guid>
<content:encoded><![CDATA[
arXiv:2506.17320v1 Announce Type: new 
Abstract: Radiology students often struggle to develop perceptual expertise due to limited expert mentorship time, leading to errors in visual search and diagnostic interpretation. These perceptual errors, such as missed fixations, short dwell times, or misinterpretations, are not adequately addressed by current AI systems, which focus on diagnostic accuracy but fail to explain how and why errors occur. To address this gap, we introduce MAARTA (Multi-Agentic Adaptive Radiology Teaching Assistant), a multi-agent framework that analyzes gaze patterns and radiology reports to provide personalized feedback. Unlike single-agent models, MAARTA dynamically selects agents based on error complexity, enabling adaptive and efficient reasoning. By comparing expert and student gaze behavior through structured graphs, the system identifies missed findings and assigns Perceptual Error Teacher agents to analyze discrepancies. MAARTA then uses step-by-step prompting to help students understand their errors and improve diagnostic reasoning, advancing AI-driven radiology education.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Prediction -- Structuring Epistemic Integrity in Artificial Reasoning Systems</title>
<link>https://arxiv.org/abs/2506.17331</link>
<guid>https://arxiv.org/abs/2506.17331</guid>
<content:encoded><![CDATA[
arXiv:2506.17331v1 Announce Type: new 
Abstract: This paper develops a comprehensive framework for artificial intelligence systems that operate under strict epistemic constraints, moving beyond stochastic language prediction to support structured reasoning, propositional commitment, and contradiction detection. It formalises belief representation, metacognitive processes, and normative verification, integrating symbolic inference, knowledge graphs, and blockchain-based justification to ensure truth-preserving, auditably rational epistemic agents.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research</title>
<link>https://arxiv.org/abs/2506.17335</link>
<guid>https://arxiv.org/abs/2506.17335</guid>
<content:encoded><![CDATA[
arXiv:2506.17335v1 Announce Type: new 
Abstract: Large language model (LLM) agents have demonstrated remarkable potential in advancing scientific discovery. However, their capability in the fundamental yet crucial task of reproducing code from research papers, especially in the NLP domain, remains underexplored. This task includes unique complex reasoning challenges in the intellectual synthesis of abstract concepts and the comprehension of code repositories with interdependent files. Motivated by this gap, we present LMR-BENCH, a benchmark designed to systematically evaluate the capability of LLM agents on code reproduction from Language Modeling Research. It consists of 28 code reproduction tasks derived from 23 research papers published in top-tier NLP venues over the past five years, spanning nine fundamental categories. Models are provided with a research paper, a code repository containing one or more masked functions, and instructions for implementing these functions. We conduct extensive experiments in standard prompting and LLM agent settings with state-of-the-art LLMs, evaluating the accuracy of unit tests and performing LLM-based evaluation of code correctness. Experimental results reveal that even the most advanced models still exhibit persistent limitations in scientific reasoning and code synthesis, highlighting critical gaps in LLM agents' ability to autonomously reproduce scientific research
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought Reasoning and Homomorphically Encrypted Vector Databases</title>
<link>https://arxiv.org/abs/2506.17336</link>
<guid>https://arxiv.org/abs/2506.17336</guid>
<content:encoded><![CDATA[
arXiv:2506.17336v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used as personal agents, accessing sensitive user data such as calendars, emails, and medical records. Users currently face a trade-off: They can send private records, many of which are stored in remote databases, to powerful but untrusted LLM providers, increasing their exposure risk. Alternatively, they can run less powerful models locally on trusted devices. We bridge this gap. Our Socratic Chain-of-Thought Reasoning first sends a generic, non-private user query to a powerful, untrusted LLM, which generates a Chain-of-Thought (CoT) prompt and detailed sub-queries without accessing user data. Next, we embed these sub-queries and perform encrypted sub-second semantic search using our Homomorphically Encrypted Vector Database across one million entries of a single user's private data. This represents a realistic scale of personal documents, emails, and records accumulated over years of digital activity. Finally, we feed the CoT prompt and the decrypted records to a local language model and generate the final response. On the LoCoMo long-context QA benchmark, our hybrid framework, combining GPT-4o with a local Llama-3.2-1B model, outperforms using GPT-4o alone by up to 7.1 percentage points. This demonstrates a first step toward systems where tasks are decomposed and split between untrusted strong LLMs and weak local ones, preserving user privacy.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning</title>
<link>https://arxiv.org/abs/2506.17338</link>
<guid>https://arxiv.org/abs/2506.17338</guid>
<content:encoded><![CDATA[
arXiv:2506.17338v1 Announce Type: new 
Abstract: The proliferation of multi-agent systems (MAS) in complex, dynamic environments necessitates robust and efficient mechanisms for managing shared knowledge. A critical challenge is ensuring that distributed memories remain synchronized, relevant, and free from the accumulation of outdated or inconsequential data - a process analogous to biological forgetting. This paper introduces the Co-Forgetting Protocol, a novel, comprehensive framework designed to address this challenge by enabling synchronized memory pruning in MAS. The protocol integrates three key components: (1) context-aware semantic voting, where agents utilize a lightweight DistilBERT model to assess the relevance of memory items based on their content and the current operational context; (2) multi-scale temporal decay functions, which assign diminishing importance to memories based on their age and access frequency across different time horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based consensus mechanism, ensuring that decisions to retain or discard memory items are agreed upon by a qualified and fault-tolerant majority of agents, even in the presence of up to f Byzantine (malicious or faulty) agents in a system of N greater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient inter-agent communication and Pinecone for scalable vector embedding storage and similarity search, with SQLite managing metadata. Experimental evaluations in a simulated MAS environment with four agents demonstrate the protocol's efficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88% voting accuracy in forgetting decisions against human-annotated benchmarks, a 92% PBFT consensus success rate under simulated Byzantine conditions, and an 82% cache hit rate for memory access.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI is the Strategy: From Agentic AI to Autonomous Business Models onto Strategy in the Age of AI</title>
<link>https://arxiv.org/abs/2506.17339</link>
<guid>https://arxiv.org/abs/2506.17339</guid>
<content:encoded><![CDATA[
arXiv:2506.17339v1 Announce Type: new 
Abstract: This article develops the concept of Autonomous Business Models (ABMs) as a distinct managerial and strategic logic in the age of agentic AI. While most firms still operate within human-driven or AI-augmented models, we argue that we are now entering a phase where agentic AI (systems capable of initiating, coordinating, and adapting actions autonomously) can increasingly execute the core mechanisms of value creation, delivery, and capture. This shift reframes AI not as a tool to support strategy, but as the strategy itself. Using two illustrative cases, getswan.ai, an Israeli startup pursuing autonomy by design, and a hypothetical reconfiguration of Ryanair as an AI-driven incumbent, we depict the evolution from augmented to autonomous business models. We show how ABMs reshape competitive advantage through agentic execution, continuous adaptation, and the gradual offloading of human decision-making. This transition introduces new forms of competition between AI-led firms, which we term synthetic competition, where strategic interactions occur at rapid, machine-level speed and scale. It also challenges foundational assumptions in strategy, organizational design, and governance. By positioning agentic AI as the central actor in business model execution, the article invites us to rethink strategic management in an era where firms increasingly run themselves.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Game-Theoretic Frameworks for Multi-Agent AI Challenges: A 2025 Outlook</title>
<link>https://arxiv.org/abs/2506.17348</link>
<guid>https://arxiv.org/abs/2506.17348</guid>
<content:encoded><![CDATA[
arXiv:2506.17348v1 Announce Type: new 
Abstract: This paper presents a substantially reworked examination of how advanced game-theoretic paradigms can serve as a foundation for the next-generation challenges in Artificial Intelligence (AI), forecasted to arrive in or around 2025. Our focus extends beyond traditional models by incorporating dynamic coalition formation, language-based utilities, sabotage risks, and partial observability. We provide a set of mathematical formalisms, simulations, and coding schemes that illustrate how multi-agent AI systems may adapt and negotiate in complex environments. Key elements include repeated games, Bayesian updates for adversarial detection, and moral framing within payoff structures. This work aims to equip AI researchers with robust theoretical tools for aligning strategic interaction in uncertain, partially adversarial contexts.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cash or Comfort? How LLMs Value Your Inconvenience</title>
<link>https://arxiv.org/abs/2506.17367</link>
<guid>https://arxiv.org/abs/2506.17367</guid>
<content:encoded><![CDATA[
arXiv:2506.17367v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly proposed as near-autonomous artificial intelligence (AI) agents capable of making everyday decisions on behalf of humans. Although LLMs perform well on many technical tasks, their behaviour in personal decision-making remains less understood. Previous studies have assessed their rationality and moral alignment with human decisions. However, the behaviour of AI assistants in scenarios where financial rewards are at odds with user comfort has not yet been thoroughly explored. In this paper, we tackle this problem by quantifying the prices assigned by multiple LLMs to a series of user discomforts: additional walking, waiting, hunger and pain. We uncover several key concerns that strongly question the prospect of using current LLMs as decision-making assistants: (1) a large variance in responses between LLMs, (2) within a single LLM, responses show fragility to minor variations in prompt phrasing (e.g., reformulating the question in the first person can considerably alter the decision), (3) LLMs can accept unreasonably low rewards for major inconveniences (e.g., 1 Euro to wait 10 hours), and (4) LLMs can reject monetary gains where no discomfort is imposed (e.g., 1,000 Euro to wait 0 minutes). These findings emphasize the need for scrutiny of how LLMs value human inconvenience, particularly as we move toward applications where such cash-versus-comfort trade-offs are made on users' behalf.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making</title>
<link>https://arxiv.org/abs/2506.17419</link>
<guid>https://arxiv.org/abs/2506.17419</guid>
<content:encoded><![CDATA[
arXiv:2506.17419v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) are integrated into safety-critical applications involving sequential decision-making in the real world, it is essential to know when to trust LLM decisions. Existing LLM Uncertainty Quantification (UQ) methods are primarily designed for single-turn question-answering formats, resulting in multi-step decision-making scenarios, e.g., LLM agentic system, being underexplored. In this paper, we introduce a principled, information-theoretic framework that decomposes LLM sequential decision uncertainty into two parts: (i) internal uncertainty intrinsic to the current decision, which is focused on existing UQ methods, and (ii) extrinsic uncertainty, a Mutual-Information (MI) quantity describing how much uncertainty should be inherited from preceding decisions. We then propose UProp, an efficient and effective extrinsic uncertainty estimator that converts the direct estimation of MI to the estimation of Pointwise Mutual Information (PMI) over multiple Trajectory-Dependent Decision Processes (TDPs). UProp is evaluated over extensive multi-step decision-making benchmarks, e.g., AgentBench and HotpotQA, with state-of-the-art LLMs, e.g., GPT-4.1 and DeepSeek-V3. Experimental results demonstrate that UProp significantly outperforms existing single-turn UQ baselines equipped with thoughtful aggregation strategies. Moreover, we provide a comprehensive analysis of UProp, including sampling efficiency, potential applications, and intermediate uncertainty propagation, to demonstrate its effectiveness. Codes will be available at https://github.com/jinhaoduan/UProp.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resource Rational Contractualism Should Guide AI Alignment</title>
<link>https://arxiv.org/abs/2506.17434</link>
<guid>https://arxiv.org/abs/2506.17434</guid>
<content:encoded><![CDATA[
arXiv:2506.17434v1 Announce Type: new 
Abstract: AI systems will soon have to navigate human environments and make decisions that affect people and other AI agents whose goals and values diverge. Contractualist alignment proposes grounding those decisions in agreements that diverse stakeholders would endorse under the right conditions, yet securing such agreement at scale remains costly and slow -- even for advanced AI. We therefore propose Resource-Rational Contractualism (RRC): a framework where AI systems approximate the agreements rational parties would form by drawing on a toolbox of normatively-grounded, cognitively-inspired heuristics that trade effort for accuracy. An RRC-aligned agent would not only operate efficiently, but also be equipped to dynamically adapt to and interpret the ever-changing human social world.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniReflect: Discovering Transferable Constitutions for LLM agents via Neuro-Symbolic Reflections</title>
<link>https://arxiv.org/abs/2506.17449</link>
<guid>https://arxiv.org/abs/2506.17449</guid>
<content:encoded><![CDATA[
arXiv:2506.17449v1 Announce Type: new 
Abstract: Efforts to improve Large Language Model (LLM) agent performance on complex tasks have largely focused on fine-tuning and iterative self-correction. However, these approaches often lack generalizable mechanisms for longterm learning and remain inefficient in dynamic environments. We introduce OmniReflect, a hierarchical, reflection-driven framework that constructs a constitution, a compact set of guiding principles distilled from task experiences, to enhance the effectiveness and efficiency of an LLM agent. OmniReflect operates in two modes: Self-sustaining, where a single agent periodically curates its own reflections during task execution, and Co-operative, where a Meta-advisor derives a constitution from a small calibration set to guide another agent. To construct these constitutional principles, we employ Neural, Symbolic, and NeuroSymbolic techniques, offering a balance between contextual adaptability and computational efficiency. Empirical results averaged across models show major improvements in task success, with absolute gains of +10.3% on ALFWorld, +23.8% on BabyAI, and +8.3% on PDDL in the Self-sustaining mode. Similar gains are seen in the Co-operative mode, where a lightweight Qwen3-4B ReAct agent outperforms all Reflexion baselines on BabyAI. These findings highlight the robustness and effectiveness of OmniReflect across environments and backbones.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General-Purpose Robotic Navigation via LVLM-Orchestrated Perception, Reasoning, and Acting</title>
<link>https://arxiv.org/abs/2506.17462</link>
<guid>https://arxiv.org/abs/2506.17462</guid>
<content:encoded><![CDATA[
arXiv:2506.17462v1 Announce Type: new 
Abstract: Developing general-purpose navigation policies for unknown environments remains a core challenge in robotics. Most existing systems rely on task-specific neural networks and fixed data flows, limiting generalizability. Large Vision-Language Models (LVLMs) offer a promising alternative by embedding human-like knowledge suitable for reasoning and planning. Yet, prior LVLM-robot integrations typically depend on pre-mapped spaces, hard-coded representations, and myopic exploration. We introduce the Agentic Robotic Navigation Architecture (ARNA), a general-purpose navigation framework that equips an LVLM-based agent with a library of perception, reasoning, and navigation tools available within modern robotic stacks. At runtime, the agent autonomously defines and executes task-specific workflows that iteratively query the robotic modules, reason over multimodal inputs, and select appropriate navigation actions. This approach enables robust navigation and reasoning in previously unmapped environments, providing a new perspective on robotic stack design. Evaluated in Habitat Lab on the HM-EQA benchmark, ARNA achieves state-of-the-art performance, demonstrating effective exploration, navigation, and embodied question answering without relying on handcrafted plans, fixed input representations, or pre-existing maps.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Unstructured Communication to Intelligent RAG: Multi-Agent Automation for Supply Chain Knowledge Bases</title>
<link>https://arxiv.org/abs/2506.17484</link>
<guid>https://arxiv.org/abs/2506.17484</guid>
<content:encoded><![CDATA[
arXiv:2506.17484v1 Announce Type: new 
Abstract: Supply chain operations generate vast amounts of operational data; however, critical knowledge such as system usage practices, troubleshooting workflows, and resolution techniques often remains buried within unstructured communications like support tickets, emails, and chat logs. While RAG systems aim to leverage such communications as a knowledge base, their effectiveness is limited by raw data challenges: support tickets are typically noisy, inconsistent, and incomplete, making direct retrieval suboptimal. Unlike existing RAG approaches that focus on runtime optimization, we introduce a novel offline-first methodology that transforms these communications into a structured knowledge base. Our key innovation is a LLMs-based multi-agent system orchestrating three specialized agents: Category Discovery for taxonomy creation, Categorization for ticket grouping, and Knowledge Synthesis for article generation. Applying our methodology to real-world support tickets with resolution notes and comments, our system creates a compact knowledge base - reducing total volume to just 3.4% of original ticket data while improving quality. Experiments demonstrate that our prebuilt knowledge base in RAG systems significantly outperforms traditional RAG implementations (48.74% vs. 38.60% helpful answers) and achieves a 77.4% reduction in unhelpful responses. By automating institutional knowledge capture that typically remains siloed in experts' heads, our solution translates to substantial operational efficiency: reducing support workload, accelerating resolution times, and creating self-improving systems that automatically resolve approximately 50% of future supply chain tickets. Our approach addresses a key gap in knowledge management by transforming transient communications into structured, reusable knowledge through intelligent offline processing rather than latency-inducing runtime architectures.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Grassroots Network and Community Roadmap for Interconnected Autonomous Science Laboratories for Accelerated Discovery</title>
<link>https://arxiv.org/abs/2506.17510</link>
<guid>https://arxiv.org/abs/2506.17510</guid>
<content:encoded><![CDATA[
arXiv:2506.17510v1 Announce Type: new 
Abstract: Scientific discovery is being revolutionized by AI and autonomous systems, yet current autonomous laboratories remain isolated islands unable to collaborate across institutions. We present the Autonomous Interconnected Science Lab Ecosystem (AISLE), a grassroots network transforming fragmented capabilities into a unified system that shorten the path from ideation to innovation to impact and accelerates discovery from decades to months. AISLE addresses five critical dimensions: (1) cross-institutional equipment orchestration, (2) intelligent data management with FAIR compliance, (3) AI-agent driven orchestration grounded in scientific principles, (4) interoperable agent communication interfaces, and (5) AI/ML-integrated scientific education. By connecting autonomous agents across institutional boundaries, autonomous science can unlock research spaces inaccessible to traditional approaches while democratizing cutting-edge technologies. This paradigm shift toward collaborative autonomous science promises breakthroughs in sustainable energy, materials development, and public health.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kaleidoscopic Teaming in Multi Agent Simulations</title>
<link>https://arxiv.org/abs/2506.17514</link>
<guid>https://arxiv.org/abs/2506.17514</guid>
<content:encoded><![CDATA[
arXiv:2506.17514v1 Announce Type: new 
Abstract: Warning: This paper contains content that may be inappropriate or offensive.
  AI agents have gained significant recent attention due to their autonomous tool usage capabilities and their integration in various real-world applications. This autonomy poses novel challenges for the safety of such systems, both in single- and multi-agent scenarios. We argue that existing red teaming or safety evaluation frameworks fall short in evaluating safety risks in complex behaviors, thought processes and actions taken by agents. Moreover, they fail to consider risks in multi-agent setups where various vulnerabilities can be exposed when agents engage in complex behaviors and interactions with each other. To address this shortcoming, we introduce the term kaleidoscopic teaming which seeks to capture complex and wide range of vulnerabilities that can happen in agents both in single-agent and multi-agent scenarios. We also present a new kaleidoscopic teaming framework that generates a diverse array of scenarios modeling real-world human societies. Our framework evaluates safety of agents in both single-agent and multi-agent setups. In single-agent setup, an agent is given a scenario that it needs to complete using the tools it has access to. In multi-agent setup, multiple agents either compete against or cooperate together to complete a task in the scenario through which we capture existing safety vulnerabilities in agents. We introduce new in-context optimization techniques that can be used in our kaleidoscopic teaming framework to generate better scenarios for safety analysis. Lastly, we present appropriate metrics that can be used along with our framework to measure safety of agents. Utilizing our kaleidoscopic teaming framework, we identify vulnerabilities in various models with respect to their safety in agentic use-cases.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Single-Tester Limits: Multi-Agent LLMs for Multi-User Feature Testing</title>
<link>https://arxiv.org/abs/2506.17539</link>
<guid>https://arxiv.org/abs/2506.17539</guid>
<content:encoded><![CDATA[
arXiv:2506.17539v1 Announce Type: new 
Abstract: The growing dependence on mobile phones and their apps has made multi-user interactive features, like chat calls, live streaming, and video conferencing, indispensable for bridging the gaps in social connectivity caused by physical and situational barriers. However, automating these interactive features for testing is fraught with challenges, owing to their inherent need for timely, dynamic, and collaborative user interactions, which current automated testing methods inadequately address. Inspired by the concept of agents designed to autonomously and collaboratively tackle problems, we propose MAdroid, a novel multi-agent approach powered by the Large Language Models (LLMs) to automate the multi-user interactive task for app feature testing. Specifically, MAdroid employs two functional types of multi-agents: user agents (Operator) and supervisor agents (Coordinator and Observer). Each agent takes a specific role: the Coordinator directs the interactive task; the Operator mimics user interactions on the device; and the Observer monitors and reviews the task automation process. Our evaluation, which included 41 multi-user interactive tasks, demonstrates the effectiveness of our approach, achieving 82.9% of the tasks with 96.8% action similarity, outperforming the ablation studies and state-of-the-art baselines. Additionally, a preliminary investigation underscores MAdroid's practicality by helping identify 11 multi-user interactive bugs during regression app testing, confirming its potential value in real-world software development contexts.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Zero-Shot Coordination between Teams of Agents: The N-XPlay Framework</title>
<link>https://arxiv.org/abs/2506.17560</link>
<guid>https://arxiv.org/abs/2506.17560</guid>
<content:encoded><![CDATA[
arXiv:2506.17560v1 Announce Type: new 
Abstract: Zero-shot coordination (ZSC) -- the ability to collaborate with unfamiliar partners -- is essential to making autonomous agents effective teammates. Existing ZSC methods evaluate coordination capabilities between two agents who have not previously interacted. However, these scenarios do not reflect the complexity of real-world multi-agent systems, where coordination often involves a hierarchy of sub-groups and interactions between teams of agents, known as Multi-Team Systems (MTS). To address this gap, we first introduce N-player Overcooked, an N-agent extension of the popular two-agent ZSC benchmark, enabling evaluation of ZSC in N-agent scenarios. We then propose N-XPlay for ZSC in N-agent, multi-team settings. Comparison against Self-Play across two-, three- and five-player Overcooked scenarios, where agents are split between an ``ego-team'' and a group of unseen collaborators shows that agents trained with N-XPlay are better able to simultaneously balance ``intra-team'' and ``inter-team'' coordination than agents trained with SP.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming the Untamed: Graph-Based Knowledge Retrieval and Reasoning for MLLMs to Conquer the Unknown</title>
<link>https://arxiv.org/abs/2506.17589</link>
<guid>https://arxiv.org/abs/2506.17589</guid>
<content:encoded><![CDATA[
arXiv:2506.17589v1 Announce Type: new 
Abstract: The real value of knowledge lies not just in its accumulation, but in its potential to be harnessed effectively to conquer the unknown. Although recent multimodal large language models (MLLMs) exhibit impressing multimodal capabilities, they often fail in rarely encountered domain-specific tasks due to limited relevant knowledge. To explore this, we adopt visual game cognition as a testbed and select Monster Hunter: World as the target to construct a multimodal knowledge graph (MH-MMKG), which incorporates multi-modalities and intricate entity relations. We also design a series of challenging queries based on MH-MMKG to evaluate the models' ability for complex knowledge retrieval and reasoning. Furthermore, we propose a multi-agent retriever that enables a model to autonomously search relevant knowledge without additional training. Experimental results show that our approach significantly enhances the performance of MLLMs, providing a new perspective on multimodal knowledge-augmented reasoning and laying a solid foundation for future research.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo Retouching Agent</title>
<link>https://arxiv.org/abs/2506.17612</link>
<guid>https://arxiv.org/abs/2506.17612</guid>
<content:encoded><![CDATA[
arXiv:2506.17612v1 Announce Type: new 
Abstract: Photo retouching has become integral to contemporary visual storytelling, enabling users to capture aesthetics and express creativity. While professional tools such as Adobe Lightroom offer powerful capabilities, they demand substantial expertise and manual effort. In contrast, existing AI-based solutions provide automation but often suffer from limited adjustability and poor generalization, failing to meet diverse and personalized editing needs. To bridge this gap, we introduce JarvisArt, a multi-modal large language model (MLLM)-driven agent that understands user intent, mimics the reasoning process of professional artists, and intelligently coordinates over 200 retouching tools within Lightroom. JarvisArt undergoes a two-stage training process: an initial Chain-of-Thought supervised fine-tuning to establish basic reasoning and tool-use skills, followed by Group Relative Policy Optimization for Retouching (GRPO-R) to further enhance its decision-making and tool proficiency. We also propose the Agent-to-Lightroom Protocol to facilitate seamless integration with Lightroom. To evaluate performance, we develop MMArt-Bench, a novel benchmark constructed from real-world user edits. JarvisArt demonstrates user-friendly interaction, superior generalization, and fine-grained control over both global and local adjustments, paving a new avenue for intelligent photo retouching. Notably, it outperforms GPT-4o with a 60% improvement in average pixel-level metrics on MMArt-Bench for content fidelity, while maintaining comparable instruction-following capabilities. Project Page: https://jarvisart.vercel.app/.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>May the Feedback Be with You! Unlocking the Power of Feedback-Driven Deep Learning Framework Fuzzing via LLMs</title>
<link>https://arxiv.org/abs/2506.17642</link>
<guid>https://arxiv.org/abs/2506.17642</guid>
<content:encoded><![CDATA[
arXiv:2506.17642v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) Infrastructures, represented by Deep Learning (DL) frameworks, have served as fundamental DL systems over the last decade. However, the bugs in DL frameworks could lead to catastrophic consequences in some critical scenarios (e.g., healthcare and autonomous driving). A simple yet effective way to find bugs in DL frameworks is fuzz testing (Fuzzing). Unfortunately, existing fuzzing techniques have not comprehensively considered multiple types of feedback. Additionally, they analyze feedback in a coarse-grained manner, such as mutating the test cases only according to whether the coverage increases. Recently, researchers introduced Large Language Models (LLMs) into fuzzing. However, current LLM-based fuzzing techniques only focus on using LLMs to generate test cases while overlooking their potential to analyze feedback information, failing to create more valid and diverse test cases. To fill this gap, we propose FUEL to break the seal of Feedback-driven fuzzing for DL frameworks. The backbone of FUEL comprises two LLM-based agents, namely analysis LLM and generation LLM. Analysis LLM agent infers analysis summaries from feedback information, while the generation LLM agent creates tests guided by these analysis summaries. So far, FUEL has detected 104 bugs for PyTorch and TensorFlow, with 93 confirmed as new bugs, 47 already fixed, and 5 assigned with CVE IDs. Our work indicates that considering multiple types of feedback is beneficial to fuzzing performance, and leveraging LLMs to analyze feedback information is a promising direction. Our artifact is available at https://github.com/NJU-iSE/FUEL
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Syntax: Action Semantics Learning for App Agents</title>
<link>https://arxiv.org/abs/2506.17697</link>
<guid>https://arxiv.org/abs/2506.17697</guid>
<content:encoded><![CDATA[
arXiv:2506.17697v1 Announce Type: new 
Abstract: The advent of Large Language Models (LLMs) enables the rise of App agents that interpret user intent and operate smartphone Apps through actions such as clicking and scrolling. While prompt-based solutions with closed LLM APIs show promising ability, they incur heavy compute costs and external API dependency. Fine-tuning smaller open-source LLMs solves these limitations. However, current fine-tuning methods use a syntax learning paradigm that forces agents to reproduce exactly the ground truth action strings, leading to out-of-distribution (OOD) vulnerability. To fill this gap, we propose Action Semantics Learning (ASL), a novel learning framework, where the learning objective is capturing the semantics of the ground truth actions. Specifically, inspired by the programming language theory, we define the action semantics for App agents as the state transition induced by the action in the user interface. With this insight, ASL employs a novel SEmantic Estimator (SEE) to compute a semantic reward to train the App agents in generating actions aligned with the semantics of ground truth actions, even when the syntactic forms differ. To support the effectiveness of ASL, we theoretically demonstrate the superior robustness of ASL for the OOD problem compared with the existing syntax learning paradigm. Extensive experiments on offline and online smartphone App operation benchmarks show that ASL significantly improves the accuracy and generalisation of App agents over existing methods.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Butterfly Analysis using Mobile Agents</title>
<link>https://arxiv.org/abs/2506.17721</link>
<guid>https://arxiv.org/abs/2506.17721</guid>
<content:encoded><![CDATA[
arXiv:2506.17721v1 Announce Type: new 
Abstract: Butterflies, or 4-cycles in bipartite graphs, are crucial for identifying cohesive structures and dense subgraphs. While agent-based data mining is gaining prominence, its application to bipartite networks remains relatively unexplored. We propose distributed, agent-based algorithms for \emph{Butterfly Counting} in a bipartite graph $G((A,B),E)$. Agents first determine their respective partitions and collaboratively construct a spanning tree, electing a leader within $O(n \log \lambda)$ rounds using only $O(\log \lambda)$ bits per agent. A novel meeting mechanism between adjacent agents improves efficiency and eliminates the need for prior knowledge of the graph, requiring only the highest agent ID $\lambda$ among the $n$ agents. Notably, our techniques naturally extend to general graphs, where leader election and spanning tree construction maintain the same round and memory complexities. Building on these foundations, agents count butterflies per node in $O(\Delta)$ rounds and compute the total butterfly count of $G$ in $O(\Delta+\min\{|A|,|B|\})$ rounds.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Experimental Evidence for the Propagation and Preservation of Machine Discoveries in Human Populations</title>
<link>https://arxiv.org/abs/2506.17741</link>
<guid>https://arxiv.org/abs/2506.17741</guid>
<content:encoded><![CDATA[
arXiv:2506.17741v1 Announce Type: new 
Abstract: Intelligent machines with superhuman capabilities have the potential to uncover problem-solving strategies beyond human discovery. Emerging evidence from competitive gameplay, such as Go, demonstrates that AI systems are evolving from mere tools to sources of cultural innovation adopted by humans. However, the conditions under which intelligent machines transition from tools to drivers of persistent cultural change remain unclear. We identify three key conditions for machines to fundamentally influence human problem-solving: the discovered strategies must be non-trivial, learnable, and offer a clear advantage. Using a cultural transmission experiment and an agent-based simulation, we demonstrate that when these conditions are met, machine-discovered strategies can be transmitted, understood, and preserved by human populations, leading to enduring cultural shifts. These findings provide a framework for understanding how machines can persistently expand human cognitive skills and underscore the need to consider their broader implications for human cognition and cultural evolution.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CARTS: Collaborative Agents for Recommendation Textual Summarization</title>
<link>https://arxiv.org/abs/2506.17765</link>
<guid>https://arxiv.org/abs/2506.17765</guid>
<content:encoded><![CDATA[
arXiv:2506.17765v1 Announce Type: new 
Abstract: Current recommendation systems often require some form of textual data summarization, such as generating concise and coherent titles for product carousels or other grouped item displays. While large language models have shown promise in NLP domains for textual summarization, these approaches do not directly apply to recommendation systems, where explanations must be highly relevant to the core features of item sets, adhere to strict word limit constraints. In this paper, we propose CARTS (Collaborative Agents for Recommendation Textual Summarization), a multi-agent LLM framework designed for structured summarization in recommendation systems. CARTS decomposes the task into three stages-Generation Augmented Generation (GAG), refinement circle, and arbitration, where successive agent roles are responsible for extracting salient item features, iteratively refining candidate titles based on relevance and length feedback, and selecting the final title through a collaborative arbitration process. Experiments on large-scale e-commerce data and live A/B testing show that CARTS significantly outperforms single-pass and chain-of-thought LLM baselines, delivering higher title relevance and improved user engagement metrics.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAGENT: Learning to Patch Software Engineering Agents</title>
<link>https://arxiv.org/abs/2506.17772</link>
<guid>https://arxiv.org/abs/2506.17772</guid>
<content:encoded><![CDATA[
arXiv:2506.17772v1 Announce Type: new 
Abstract: LLM Agents produce patches automatically to resolve an issue. However, they can generate inaccurate patches. Little is known about the root causes behind those failed patches or how those could be fixed. This paper reports an empirical study of the failed patches generated by seven top LLM code agents. We collected 114 issues from the SWE-bench Lite dataset that remained unresolved across the agents. The seven agents produced a total of 769 failed patches for those issues, which we checked with a combination of GPT-4o and manual analysis. We present a taxonomy of the failure reasons across the patches. The taxonomy contains six categories, with several sub-categories under each category. For example, a frequently observed category is the inability of an LLM to correctly infer/produce the appropriate variable type in the produced patch. As a first step towards addressing such type-related errors, we designed PAGENT (Patch Agent). PAGENT utilizes program analysis techniques like CFG creation and exploration to infer the type of information of a patch. PAGENT does this by applying repository-level static code analysis techniques. Then, PAGENT refines the inferred type by further utilizing an LLM-based inference technique. We tested PAGENT on all 127 type-related failed patches from the top three agents in our study. PAGENT could fix 29 of the 127 failed patches.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Exploration with a New Uncertainty Framework for Active SLAM Systems</title>
<link>https://arxiv.org/abs/2506.17775</link>
<guid>https://arxiv.org/abs/2506.17775</guid>
<content:encoded><![CDATA[
arXiv:2506.17775v1 Announce Type: new 
Abstract: Accurate reconstruction of the environment is a central goal of Simultaneous Localization and Mapping (SLAM) systems. However, the agent's trajectory can significantly affect estimation accuracy. This paper presents a new method to model map uncertainty in Active SLAM systems using an Uncertainty Map (UM). The UM uses probability distributions to capture where the map is uncertain, allowing Uncertainty Frontiers (UF) to be defined as key exploration-exploitation objectives and potential stopping criteria. In addition, the method introduces the Signed Relative Entropy (SiREn), based on the Kullback-Leibler divergence, to measure both coverage and uncertainty together. This helps balance exploration and exploitation through an easy-to-understand parameter. Unlike methods that depend on particular SLAM setups, the proposed approach is compatible with different types of sensors, such as cameras, LiDARs, and multi-sensor fusion. It also addresses common problems in exploration planning and stopping conditions. Furthermore, integrating this map modeling approach with a UF-based planning system enables the agent to autonomously explore open spaces, a behavior not previously observed in the Active SLAM literature. Code and implementation details are available as a ROS node, and all generated data are openly available for public use, facilitating broader adoption and validation of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Autonomous UI Exploration: The UIExplorer Benchmark</title>
<link>https://arxiv.org/abs/2506.17779</link>
<guid>https://arxiv.org/abs/2506.17779</guid>
<content:encoded><![CDATA[
arXiv:2506.17779v1 Announce Type: new 
Abstract: Autonomous agents must know how to explore user interfaces (UIs) for reliable task solving, yet systematic evaluation of this crucial phase is lacking. We introduce UIExplore-Bench, the first benchmark explicitly dedicated to UI exploration. The benchmark evaluates agents with either Structured mode (granting access to layout information like DOM trees) or Screen mode (relying on GUI-only observations such as screenshots and human-like mouse/keyboard interactions) across three levels in a standardized GitLab sandbox environment. We formalize exploration as the process of maximizing the set of actionable UI components discovered and propose a metric, human-normalized UI-Functionalities Observed (hUFO), to quantify the effectiveness of exploration. Our results show that UIExplore-AlGo achieves the leading mean hUFO scores, reaching up to 77.2% of human performance in Structured mode and 59.0% in Screen mode at 2,000 steps, particularly excelling at the Sparse level. The results highlight the relevance of our benchmark, as current agents show a substantial performance gap compared to one hour of human expert exploration, indicating ample room for future advancements. We publicly release the benchmark environment, an exploration dataset, and an evaluation suite to catalyze research into efficient UI exploration strategies and their downstream applications, such as experience-driven task completion and automated training data generation.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnyMAC: Cascading Flexible Multi-Agent Collaboration via Next-Agent Prediction</title>
<link>https://arxiv.org/abs/2506.17784</link>
<guid>https://arxiv.org/abs/2506.17784</guid>
<content:encoded><![CDATA[
arXiv:2506.17784v1 Announce Type: new 
Abstract: Recent progress in large language model (LLM)-based multi-agent collaboration highlights the power of structured communication in enabling collective intelligence. However, existing methods largely rely on static or graph-based inter-agent topologies, lacking the potential adaptability and flexibility in communication. In this work, we propose a new framework that rethinks multi-agent coordination through a sequential structure rather than a graph structure, offering a significantly larger topology space for multi-agent communication. Our method focuses on two key directions: (1) Next-Agent Prediction, which selects the most suitable agent role at each step, and (2) Next-Context Selection (NCS), which enables each agent to selectively access relevant information from any previous step. Together, these components construct task-adaptive communication pipelines that support both role flexibility and global information flow. Extensive evaluations across multiple benchmarks demonstrate that our approach achieves superior performance while substantially reducing communication overhead.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Social Deduction with Graph-Informed Language Models</title>
<link>https://arxiv.org/abs/2506.17788</link>
<guid>https://arxiv.org/abs/2506.17788</guid>
<content:encoded><![CDATA[
arXiv:2506.17788v1 Announce Type: new 
Abstract: Social reasoning - inferring unobservable beliefs and intentions from partial observations of other agents - remains a challenging task for large language models (LLMs). We evaluate the limits of current reasoning language models in the social deduction game Avalon and find that while the largest models demonstrate strong performance, they require extensive test-time inference and degrade sharply when distilled to smaller, real-time-capable variants. To address this, we introduce a hybrid reasoning framework that externalizes belief inference to a structured probabilistic model, while using an LLM for language understanding and interaction. Our approach achieves competitive performance with much larger models in Agent-Agent play and, notably, is the first language agent to defeat human players in a controlled study - achieving a 67% win rate and receiving higher qualitative ratings than both reasoning baselines and human teammates. We release code, models, and a dataset to support future work on social reasoning in LLM agents, which can be found at https://camp-lab-purdue.github.io/bayesian-social-deduction/
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Your Automated Software Engineer Trustworthy?</title>
<link>https://arxiv.org/abs/2506.17812</link>
<guid>https://arxiv.org/abs/2506.17812</guid>
<content:encoded><![CDATA[
arXiv:2506.17812v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are being increasingly used in software engineering tasks, with an increased focus on bug report resolution over the past year. However, most proposed systems fail to properly handle uncertain or incorrect inputs and outputs. Existing LLM-based tools and coding agents respond to every issue and generate a patch for every case, even when the input is vague or their own output is incorrect. There are no mechanisms in place to abstain when confidence is low. This leads to unreliable behaviour, such as hallucinated code changes or responses based on vague issue reports. We introduce BouncerBench, a benchmark that evaluates whether LLM-based software agents can refuse to act when inputs are ill-defined or refuse to respond when their own outputs are likely to be incorrect. Unlike prior benchmarks that implicitly incentivize models to generate responses even when uncertain, BouncerBench aims to improve precision by targeting two overlooked failure points: (1) vague or underspecified issue descriptions in tickets and (2) logically or functionally incorrect code patches created by the system. It measures whether proposed systems can distinguish actionable issues from vague tickets and valid patches from untrustworthy ones. We also implement a basic input and output bouncer, evaluating how well current LLMs can abstain when needed. Our results show that most models fail to abstain from underspecified inputs or incorrect outputs. Hence, we conclude that there is significant room for improvement before LLMs can be trusted to make correct decisions and recommendations in real-world software engineering workflows. BouncerBench provides a first step toward evaluating and building more cautious, trustworthy code agents. The replication package, dataset, and leaderboard can be found at bouncerbench.com
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reflective Verbal Reward Design for Pluralistic Alignment</title>
<link>https://arxiv.org/abs/2506.17834</link>
<guid>https://arxiv.org/abs/2506.17834</guid>
<content:encoded><![CDATA[
arXiv:2506.17834v1 Announce Type: new 
Abstract: AI agents are commonly aligned with "human values" through reinforcement learning from human feedback (RLHF), where a single reward model is learned from aggregated human feedback and used to align an agent's behavior. However, human values are not homogeneous--different people hold distinct and sometimes conflicting values. Aggregating feedback into a single reward model risks disproportionately suppressing minority preferences. To address this, we present a novel reward modeling approach for learning individualized reward models. Our approach uses a language model to guide users through reflective dialogues where they critique agent behavior and construct their preferences. This personalized dialogue history, containing the user's reflections and critiqued examples, is then used as context for another language model that serves as an individualized reward function (what we call a "verbal reward model") for evaluating new trajectories. In studies with 30 participants, our method achieved a 9-12% improvement in accuracy over non-reflective verbal reward models while being more sample efficient than traditional supervised learning methods.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out of Control -- Why Alignment Needs Formal Control Theory (and an Alignment Control Stack)</title>
<link>https://arxiv.org/abs/2506.17846</link>
<guid>https://arxiv.org/abs/2506.17846</guid>
<content:encoded><![CDATA[
arXiv:2506.17846v1 Announce Type: new 
Abstract: This position paper argues that formal optimal control theory should be central to AI alignment research, offering a distinct perspective from prevailing AI safety and security approaches. While recent work in AI safety and mechanistic interpretability has advanced formal methods for alignment, they often fall short of the generalisation required of control frameworks for other technologies. There is also a lack of research into how to render different alignment/control protocols interoperable. We argue that by recasting alignment through principles of formal optimal control and framing alignment in terms of hierarchical stack from physical to socio-technical layers according to which controls may be applied we can develop a better understanding of the potential and limitations for controlling frontier models and agentic AI systems. To this end, we introduce an Alignment Control Stack which sets out a hierarchical layered alignment stack, identifying measurement and control characteristics at each layer and how different layers are formally interoperable. We argue that such analysis is also key to the assurances that will be needed by governments and regulators in order to see AI technologies sustainably benefit the community. Our position is that doing so will bridge the well-established and empirically validated methods of optimal control with practical deployment considerations to create a more comprehensive alignment framework, enhancing how we approach safety and reliability for advanced AI systems.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Fact-Checking: A Multi-Agent System with Advanced Evidence Retrieval</title>
<link>https://arxiv.org/abs/2506.17878</link>
<guid>https://arxiv.org/abs/2506.17878</guid>
<content:encoded><![CDATA[
arXiv:2506.17878v1 Announce Type: new 
Abstract: The rapid spread of misinformation in the digital era poses significant challenges to public discourse, necessitating robust and scalable fact-checking solutions. Traditional human-led fact-checking methods, while credible, struggle with the volume and velocity of online content, prompting the integration of automated systems powered by Large Language Models (LLMs). However, existing automated approaches often face limitations, such as handling complex claims, ensuring source credibility, and maintaining transparency. This paper proposes a novel multi-agent system for automated fact-checking that enhances accuracy, efficiency, and explainability. The system comprises four specialized agents: an Input Ingestion Agent for claim decomposition, a Query Generation Agent for formulating targeted subqueries, an Evidence Retrieval Agent for sourcing credible evidence, and a Verdict Prediction Agent for synthesizing veracity judgments with human-interpretable explanations. Evaluated on benchmark datasets (FEVEROUS, HOVER, SciFact), the proposed system achieves a 12.3% improvement in Macro F1-score over baseline methods. The system effectively decomposes complex claims, retrieves reliable evidence from trusted sources, and generates transparent explanations for verification decisions. Our approach contributes to the growing field of automated fact-checking by providing a more accurate, efficient, and transparent verification methodology that aligns with human fact-checking practices while maintaining scalability for real-world applications. Our source code is available at https://github.com/HySonLab/FactAgent
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning, Reasoning, Refinement: A Framework for Kahneman's Dual-System Intelligence in GUI Agents</title>
<link>https://arxiv.org/abs/2506.17913</link>
<guid>https://arxiv.org/abs/2506.17913</guid>
<content:encoded><![CDATA[
arXiv:2506.17913v1 Announce Type: new 
Abstract: Graphical User Interface (GUI) agents have made significant progress in automating digital tasks through the utilization of computer vision and language models. Nevertheless, existing agent systems encounter notable limitations. Firstly, they predominantly depend on trial and error decision making rather than progressive reasoning, thereby lacking the capability to learn and adapt from interactive encounters. Secondly, these systems are assessed using overly simplistic single step accuracy metrics, which do not adequately reflect the intricate nature of real world GUI interactions. In this paper, we present CogniGUI, a cognitive framework developed to overcome these limitations by enabling adaptive learning for GUI automation resembling human-like behavior. Inspired by Kahneman's Dual Process Theory, our approach combines two main components: (1) an omni parser engine that conducts immediate hierarchical parsing of GUI elements through quick visual semantic analysis to identify actionable components, and (2) a Group based Relative Policy Optimization (GRPO) grounding agent that assesses multiple interaction paths using a unique relative reward system, promoting minimal and efficient operational routes. This dual-system design facilitates iterative ''exploration learning mastery'' cycles, enabling the agent to enhance its strategies over time based on accumulated experience. Moreover, to assess the generalization and adaptability of agent systems, we introduce ScreenSeek, a comprehensive benchmark that includes multi application navigation, dynamic state transitions, and cross interface coherence, which are often overlooked challenges in current benchmarks. Experimental results demonstrate that CogniGUI surpasses state-of-the-art methods in both the current GUI grounding benchmarks and our newly proposed benchmark.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASTER: Adaptive Spatio-Temporal Early Decision Model for Dynamic Resource Allocation</title>
<link>https://arxiv.org/abs/2506.17929</link>
<guid>https://arxiv.org/abs/2506.17929</guid>
<content:encoded><![CDATA[
arXiv:2506.17929v1 Announce Type: new 
Abstract: Supporting decision-making has long been a central vision in the field of spatio-temporal intelligence. While prior work has improved the timeliness and accuracy of spatio-temporal forecasting, converting these forecasts into actionable strategies remains a key challenge. A main limitation is the decoupling of the prediction and the downstream decision phases, which can significantly degrade the downstream efficiency. For example, in emergency response, the priority is successful resource allocation and intervention, not just incident prediction. To this end, it is essential to propose an Adaptive Spatio-Temporal Early Decision model (ASTER) that reforms the forecasting paradigm from event anticipation to actionable decision support. This framework ensures that information is directly used for decision-making, thereby maximizing overall effectiveness. Specifically, ASTER introduces a new Resource-aware Spatio-Temporal interaction module (RaST) that adaptively captures long- and short-term dependencies under dynamic resource conditions, producing context-aware spatiotemporal representations. To directly generate actionable decisions, we further design a Preference-oriented decision agent (Poda) based on multi-objective reinforcement learning, which transforms predictive signals into resource-efficient intervention strategies by deriving optimal actions under specific preferences and dynamic constraints. Experimental results on four benchmark datasets demonstrate the state-of-the-art performance of ASTER in improving both early prediction accuracy and resource allocation outcomes across six downstream metrics.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeNIE: A Generalizable Navigation System for In-the-Wild Environments</title>
<link>https://arxiv.org/abs/2506.17960</link>
<guid>https://arxiv.org/abs/2506.17960</guid>
<content:encoded><![CDATA[
arXiv:2506.17960v1 Announce Type: new 
Abstract: Reliable navigation in unstructured, real-world environments remains a significant challenge for embodied agents, especially when operating across diverse terrains, weather conditions, and sensor configurations. In this paper, we introduce GeNIE (Generalizable Navigation System for In-the-Wild Environments), a robust navigation framework designed for global deployment. GeNIE integrates a generalizable traversability prediction model built on SAM2 with a novel path fusion strategy that enhances planning stability in noisy and ambiguous settings. We deployed GeNIE in the Earth Rover Challenge (ERC) at ICRA 2025, where it was evaluated across six countries spanning three continents. GeNIE took first place and achieved 79% of the maximum possible score, outperforming the second-best team by 17%, and completed the entire competition without a single human intervention. These results set a new benchmark for robust, generalizable outdoor robot navigation. We will release the codebase, pretrained model weights, and newly curated datasets to support future research in real-world navigation.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Euclidean Enriched Contraction Theory for Monotone Operators and Monotone Dynamical Systems</title>
<link>https://arxiv.org/abs/2506.17990</link>
<guid>https://arxiv.org/abs/2506.17990</guid>
<content:encoded><![CDATA[
arXiv:2506.17990v1 Announce Type: new 
Abstract: We adopt an operator-theoretic perspective to analyze a class of nonlinear fixed-point iterations and discrete-time dynamical systems. Specifically, we study the Krasnoselskij iteration - at the heart of countless algorithmic schemes and underpinning the stability analysis of numerous dynamical models - by focusing on a non-Euclidean vector space equipped with the diagonally weighted supremum norm. By extending the state of the art, we introduce the notion of enriched weak contractivity, which (i) is characterized by a simple, verifiable condition for Lipschitz operators, and (ii) yields explicit bounds on the admissible step size for the Krasnoselskij iteration. Our results relate the notion of weak contractivity with that of monotonicity of operators and dynamical systems and show its generality to design larger step sizes and improved convergence speed for broader classes of dynamical systems. The newly developed theory is illustrated on two applications: the design of zero-finding algorithms for monotone operators and the design of nonlinear consensus dynamics in monotone multi-agent dynamical systems.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ultra-Efficient Contracts: Breaking the Substitutes Barrier in Combinatorial Contracts</title>
<link>https://arxiv.org/abs/2506.18008</link>
<guid>https://arxiv.org/abs/2506.18008</guid>
<content:encoded><![CDATA[
arXiv:2506.18008v1 Announce Type: new 
Abstract: We study the optimal contract problem in the framework of combinatorial contracts, introduced by Duetting et al. [FOCS'21], where a principal delegates the execution of a project to an agent, and the agent can choose any subset from a given set of costly actions. At the core of the model is a reward function - a monotone set function that maps each set of actions taken by the agent into an expected reward to the principal. To incentivize the agent, the principal offers a contract specifying the fraction of the reward to be paid, and the agent responds with their optimal action set. The goal is to compute the contract that maximizes the principal's expected utility.
  Previous work showed that when the reward function is gross substitutes (GS), the optimal contract can be computed in polynomial time, but the problem is NP-hard for the broader class of Submodular functions. This raised the question: is GS the true boundary of tractability for the optimal contract problem? We prove that tractability extends to the strictly broader class of Ultra functions. Interestingly, GS constitutes precisely the intersection of Ultra and Submodular functions, and our result reveals that it is Ultra - not Submodular - that drives tractability, overturning the prevailing belief that the submodularity component of GS is essential. We further extend tractability beyond additive costs, handling costs that are additive plus symmetric. Our results require new techniques, as prior approaches relied on the submodularity of GS. To the best of our knowledge, this is the first application of Ultra functions in a prominent economic setting.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graphs Meet AI Agents: Taxonomy, Progress, and Future Opportunities</title>
<link>https://arxiv.org/abs/2506.18019</link>
<guid>https://arxiv.org/abs/2506.18019</guid>
<content:encoded><![CDATA[
arXiv:2506.18019v1 Announce Type: new 
Abstract: AI agents have experienced a paradigm shift, from early dominance by reinforcement learning (RL) to the rise of agents powered by large language models (LLMs), and now further advancing towards a synergistic fusion of RL and LLM capabilities. This progression has endowed AI agents with increasingly strong abilities. Despite these advances, to accomplish complex real-world tasks, agents are required to plan and execute effectively, maintain reliable memory, and coordinate smoothly with other agents. Achieving these capabilities involves contending with ever-present intricate information, operations, and interactions. In light of this challenge, data structurization can play a promising role by transforming intricate and disorganized data into well-structured forms that agents can more effectively understand and process. In this context, graphs, with their natural advantage in organizing, managing, and harnessing intricate data relationships, present a powerful data paradigm for structurization to support the capabilities demanded by advanced AI agents. To this end, this survey presents a first systematic review of how graphs can empower AI agents. Specifically, we explore the integration of graph techniques with core agent functionalities, highlight notable applications, and identify prospective avenues for future research. By comprehensively surveying this burgeoning intersection, we hope to inspire the development of next-generation AI agents equipped to tackle increasingly sophisticated challenges with graphs. Related resources are collected and continuously updated for the community in the Github link.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MUPA: Towards Multi-Path Agentic Reasoning for Grounded Video Question Answering</title>
<link>https://arxiv.org/abs/2506.18071</link>
<guid>https://arxiv.org/abs/2506.18071</guid>
<content:encoded><![CDATA[
arXiv:2506.18071v1 Announce Type: new 
Abstract: Grounded Video Question Answering (Grounded VideoQA) requires aligning textual answers with explicit visual evidence. However, modern multimodal models often rely on linguistic priors and spurious correlations, resulting in poorly grounded predictions. In this work, we propose MUPA, a cooperative MUlti-Path Agentic approach that unifies video grounding, question answering, answer reflection and aggregation to tackle Grounded VideoQA. MUPA features three distinct reasoning paths on the interplay of grounding and QA agents in different chronological orders, along with a dedicated reflection agent to judge and aggregate the multi-path results to accomplish consistent QA and grounding. This design markedly improves grounding fidelity without sacrificing answer accuracy. Despite using only 2B parameters, our method outperforms all 7B-scale competitors. When scaled to 7B parameters, MUPA establishes new state-of-the-art results, with Acc@GQA of 30.3% and 47.4% on NExT-GQA and DeVE-QA respectively, demonstrating MUPA' effectiveness towards trustworthy video-language understanding. Our code is available in https://github.com/longmalongma/MUPA.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Research Agents: A Systematic Examination And Roadmap</title>
<link>https://arxiv.org/abs/2506.18096</link>
<guid>https://arxiv.org/abs/2506.18096</guid>
<content:encoded><![CDATA[
arXiv:2506.18096v1 Announce Type: new 
Abstract: The rapid progress of Large Language Models (LLMs) has given rise to a new category of autonomous AI systems, referred to as Deep Research (DR) agents. These agents are designed to tackle complex, multi-turn informational research tasks by leveraging a combination of dynamic reasoning, adaptive long-horizon planning, multi-hop information retrieval, iterative tool use, and the generation of structured analytical reports. In this paper, we conduct a detailed analysis of the foundational technologies and architectural components that constitute Deep Research agents. We begin by reviewing information acquisition strategies, contrasting API-based retrieval methods with browser-based exploration. We then examine modular tool-use frameworks, including code execution, multimodal input processing, and the integration of Model Context Protocols (MCPs) to support extensibility and ecosystem development. To systematize existing approaches, we propose a taxonomy that differentiates between static and dynamic workflows, and we classify agent architectures based on planning strategies and agent composition, including single-agent and multi-agent configurations. We also provide a critical evaluation of current benchmarks, highlighting key limitations such as restricted access to external knowledge, sequential execution inefficiencies, and misalignment between evaluation metrics and the practical objectives of DR agents. Finally, we outline open challenges and promising directions for future research. A curated and continuously updated repository of DR agent research is available at: {https://github.com/ai-agents-2030/awesome-deep-research-agent}.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Consensus Inference-based Hierarchical Reinforcement Learning for Multi-Constrained UAV Pursuit-Evasion Game</title>
<link>https://arxiv.org/abs/2506.18126</link>
<guid>https://arxiv.org/abs/2506.18126</guid>
<content:encoded><![CDATA[
arXiv:2506.18126v1 Announce Type: new 
Abstract: Multiple quadrotor unmanned aerial vehicle (UAV) systems have garnered widespread research interest and fostered tremendous interesting applications, especially in multi-constrained pursuit-evasion games (MC-PEG). The Cooperative Evasion and Formation Coverage (CEFC) task, where the UAV swarm aims to maximize formation coverage across multiple target zones while collaboratively evading predators, belongs to one of the most challenging issues in MC-PEG, especially under communication-limited constraints. This multifaceted problem, which intertwines responses to obstacles, adversaries, target zones, and formation dynamics, brings up significant high-dimensional complications in locating a solution. In this paper, we propose a novel two-level framework (i.e., Consensus Inference-based Hierarchical Reinforcement Learning (CI-HRL)), which delegates target localization to a high-level policy, while adopting a low-level policy to manage obstacle avoidance, navigation, and formation. Specifically, in the high-level policy, we develop a novel multi-agent reinforcement learning module, Consensus-oriented Multi-Agent Communication (ConsMAC), to enable agents to perceive global information and establish consensus from local states by effectively aggregating neighbor messages. Meanwhile, we leverage an Alternative Training-based Multi-agent proximal policy optimization (AT-M) and policy distillation to accomplish the low-level control. The experimental results, including the high-fidelity software-in-the-loop (SITL) simulations, validate that CI-HRL provides a superior solution with enhanced swarm's collaborative evasion and task completion capabilities.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoachGPT: A Scaffolding-based Academic Writing Assistant</title>
<link>https://arxiv.org/abs/2506.18149</link>
<guid>https://arxiv.org/abs/2506.18149</guid>
<content:encoded><![CDATA[
arXiv:2506.18149v1 Announce Type: new 
Abstract: Academic writing skills are crucial for students' success, but can feel overwhelming without proper guidance and practice, particularly when writing in a second language. Traditionally, students ask instructors or search dictionaries, which are not universally accessible. Early writing assistants emerged as rule-based systems that focused on detecting misspellings, subject-verb disagreements, and basic punctuation errors; however, they are inaccurate and lack contextual understanding. Machine learning-based assistants demonstrate a strong ability for language understanding but are expensive to train. Large language models (LLMs) have shown remarkable capabilities in generating responses in natural languages based on given prompts. Still, they have a fundamental limitation in education: they generate essays without teaching, which can have detrimental effects on learning when misused. To address this limitation, we develop CoachGPT, which leverages large language models (LLMs) to assist individuals with limited educational resources and those who prefer self-paced learning in academic writing. CoachGPT is an AI agent-based web application that (1) takes instructions from experienced educators, (2) converts instructions into sub-tasks, and (3) provides real-time feedback and suggestions using large language models. This unique scaffolding structure makes CoachGPT unique among existing writing assistants. Compared to existing writing assistants, CoachGPT provides a more immersive writing experience with personalized feedback and guidance. Our user studies prove the usefulness of CoachGPT and the potential of large language models for academic writing.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain-of-Memory: Enhancing GUI Agents for Cross-Application Navigation</title>
<link>https://arxiv.org/abs/2506.18158</link>
<guid>https://arxiv.org/abs/2506.18158</guid>
<content:encoded><![CDATA[
arXiv:2506.18158v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) are attracting growing attention in the development of Graphical User Interface (GUI) agents. Existing approaches often rely on historical screenshots or actions to implicitly represent the task state. This reliance poses challenges for GUI agents in accurately understanding task states and underscores the absence of effective mechanisms to store critical information in complex and lengthy cross-app tasks. To address these challenges, we propose Chain-of-Memory (CoM), a novel approach for explicitly modeling short-term and long-term memory in GUI agents. CoM achieves this by capturing action descriptions, integrating task-relevant screen information, and maintaining a dedicated memory module to store and manage this information. By leveraging explicit memory representations, CoM enables GUI agents to better understand task states and retain critical historical information persistently. To equip GUI agents with memory management capabilities and evaluate the effectiveness of CoM, we developed the GUI Odyssey-CoM, a dataset comprising 111k screen-action pairs annotated with Chain-of-Memory. Experimental results demonstrate that CoM significantly improves GUI agents' performance in cross-application tasks. Additionally, GUI Odyssey-CoM enables 7B models to achieve memory management capabilities comparable to 72B models. The dataset and code will be open-sourced.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping The Invisible Internet: Framework and Dataset</title>
<link>https://arxiv.org/abs/2506.18159</link>
<guid>https://arxiv.org/abs/2506.18159</guid>
<content:encoded><![CDATA[
arXiv:2506.18159v1 Announce Type: new 
Abstract: This article presents a novel dataset focusing on the network layer of the Invisible Internet Project (I2P), where prior research has predominantly examined application layers like the dark web. Data was collected through the SWARM- I2P framework, deploying I2P routers as mapping agents, utilizing dynamic port mapping (30000-50000 range). The dataset documents over 50,000 nodes, including 2,077 FastSet nodes and 2,331 high-capacity nodes characterized by bandwidth, latency (mean 121.21ms +- 48.50), and uptime metrics. It contains 1,997 traffic records (1,003,032 packets/bytes) and 4,222,793 records (2,147,585,625 packets/bytes), with geographic distributions for 3,444 peers showing capacity metrics (mean 8.57 +- 1.20). Collection methods included router console queries (127.0.0.1:port/tunnels), netDb analysis, and passive monitoring, with anonymized identifiers. Data is structured in CSV/TXT formats (Zenodo) with collection scripts (GitHub). Potential applications include tunnel peer selection analysis, anonymity network resilience studies, and adversarial modelling.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced For-Loop for QML algorithm search</title>
<link>https://arxiv.org/abs/2506.18260</link>
<guid>https://arxiv.org/abs/2506.18260</guid>
<content:encoded><![CDATA[
arXiv:2506.18260v1 Announce Type: new 
Abstract: This paper introduces an advanced framework leveraging Large Language Model-based Multi-Agent Systems (LLMMA) for the automated search and optimization of Quantum Machine Learning (QML) algorithms. Inspired by Google DeepMind's FunSearch, the proposed system works on abstract level to iteratively generates and refines quantum transformations of classical machine learning algorithms (concepts), such as the Multi-Layer Perceptron, forward-forward and backpropagation algorithms. As a proof of concept, this work highlights the potential of agentic frameworks to systematically explore classical machine learning concepts and adapt them for quantum computing, paving the way for efficient and automated development of QML algorithms. Future directions include incorporating planning mechanisms and optimizing strategy in the search space for broader applications in quantum-enhanced machine learning.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Use Property-Based Testing to Bridge LLM Code Generation and Validation</title>
<link>https://arxiv.org/abs/2506.18315</link>
<guid>https://arxiv.org/abs/2506.18315</guid>
<content:encoded><![CDATA[
arXiv:2506.18315v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at code generation, but ensuring their outputs to be functionally correct, especially in complex programming tasks, is a persistent challenge. While traditional Test-Driven Development (TDD) offers a path for code refinement, its efficacy with LLMs is often undermined by the scarcity of high-quality test cases or the pitfalls of automated test generation, including biased tests or inaccurate output predictions that can misdirect the correction process. This paper introduces Property-Generated Solver, a novel framework that leverages Property-Based Testing (PBT) to validate high-level program properties or invariants, instead of relying on specific input-output examples. These properties are often simpler to define and verify than directly predicting exhaustive test oracles, breaking the "cycle of self-deception" where tests might share flaws with the code they are meant to validate. Property-Generated Solver employs two collaborative LLM-based agents: a Generator dedicated to code generation and iterative refinement, and a Tester that manages the PBT life-cycle and formulate semantically rich feedback from property violations. The resulting comprehensive and actionable feedback then guides the Generator in its refinement efforts. By establishing PBT as the core validation engine within this iterative, closed-loop paradigm, Property-Generated Solver provides a robust mechanism for steering LLMs towards more correct and generalizable code. Extensive experimental results on multiple code generation benchmarks demonstrate that Property-Generated Solver achieves substantial pass@1 improvements, ranging from 23.1% to 37.3% relative gains over established TDD methods.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Knowledge Exchange and Dual-diversity Review: Concisely Unleashing the Potential of a Multi-Agent Research Team</title>
<link>https://arxiv.org/abs/2506.18348</link>
<guid>https://arxiv.org/abs/2506.18348</guid>
<content:encoded><![CDATA[
arXiv:2506.18348v1 Announce Type: new 
Abstract: Scientific progress increasingly relies on effective collaboration among researchers, a dynamic that large language models (LLMs) have only begun to emulate. While recent LLM-based scientist agents show promise in autonomous scientific discovery, they often lack the interactive reasoning and evaluation mechanisms essential to real-world research. We propose IDVSCI (Internal Discussion and Vote SCIentists), a multi-agent framework built on LLMs that incorporates two key innovations: a Dynamic Knowledge Exchange mechanism enabling iterative feedback among agents, and a Dual-Diversity Review paradigm that simulates heterogeneous expert evaluation. These components jointly promote deeper reasoning and the generation of more creative and impactful scientific ideas. To evaluate the effectiveness and generalizability of our approach, we conduct experiments on two datasets: a widely used benchmark in computer science and a new dataset we introduce in the health sciences domain. Results show that IDVSCI consistently achieves the best performance across both datasets, outperforming existing systems such as AI Scientist and VIRSCI. These findings highlight the value of modeling interaction and peer review dynamics in LLM-based autonomous research.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robots and Children that Learn Together : Improving Knowledge Retention by Teaching Peer-Like Interactive Robots</title>
<link>https://arxiv.org/abs/2506.18365</link>
<guid>https://arxiv.org/abs/2506.18365</guid>
<content:encoded><![CDATA[
arXiv:2506.18365v1 Announce Type: new 
Abstract: Despite growing interest in Learning-by-Teaching (LbT), few studies have explored how this paradigm can be implemented with autonomous, peer-like social robots in real classrooms. Most prior work has relied on scripted or Wizard-of-Oz behaviors, limiting our understanding of how real-time, interactive learning can be supported by artificial agents. This study addresses this gap by introducing Interactive Reinforcement Learning (RL) as a cognitive model for teachable social robots. We conducted two between-subject experiments with 58 primary school children, who either taught a robot or practiced independently on a tablet while learning French vocabulary (memorization) and grammatical rules (inference). The robot, powered by Interactive RL, learned from the child's evaluative feedback. Children in the LbT condition achieved significantly higher retention gains compared to those in the self-practice condition, especially on the grammar task. Learners with lower prior knowledge benefited most from teaching the robot. Behavioural metrics revealed that children adapted their teaching strategies over time and engaged more deeply during inference tasks. This work makes two contributions: (1) it introduces Interactive RL as a pedagogically effective and scalable model for peer-robot learning, and (2) it demonstrates, for the first time, the feasibility of deploying multiple autonomous robots simultaneously in real classrooms. These findings extend theoretical understanding of LbT by showing that social robots can function not only as passive tutees but as adaptive partners that enhance meta-cognitive engagement and long-term learning outcomes.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Large Language Model-based Multi-Agent Framework for Analog Circuits' Sizing Relationships Extraction</title>
<link>https://arxiv.org/abs/2506.18424</link>
<guid>https://arxiv.org/abs/2506.18424</guid>
<content:encoded><![CDATA[
arXiv:2506.18424v1 Announce Type: new 
Abstract: In the design process of the analog circuit pre-layout phase, device sizing is an important step in determining whether an analog circuit can meet the required performance metrics. Many existing techniques extract the circuit sizing task as a mathematical optimization problem to solve and continuously improve the optimization efficiency from a mathematical perspective. But they ignore the automatic introduction of prior knowledge, fail to achieve effective pruning of the search space, which thereby leads to a considerable compression margin remaining in the search space. To alleviate this problem, we propose a large language model (LLM)-based multi-agent framework for analog circuits' sizing relationships extraction from academic papers. The search space in the sizing process can be effectively pruned based on the sizing relationship extracted by this framework. Eventually, we conducted tests on 3 types of circuits, and the optimization efficiency was improved by $2.32 \sim 26.6 \times$. This work demonstrates that the LLM can effectively prune the search space for analog circuit sizing, providing a new solution for the combination of LLMs and conventional analog circuit design automation methods.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraspMAS: Zero-Shot Language-driven Grasp Detection with Multi-Agent System</title>
<link>https://arxiv.org/abs/2506.18448</link>
<guid>https://arxiv.org/abs/2506.18448</guid>
<content:encoded><![CDATA[
arXiv:2506.18448v1 Announce Type: new 
Abstract: Language-driven grasp detection has the potential to revolutionize human-robot interaction by allowing robots to understand and execute grasping tasks based on natural language commands. However, existing approaches face two key challenges. First, they often struggle to interpret complex text instructions or operate ineffectively in densely cluttered environments. Second, most methods require a training or finetuning step to adapt to new domains, limiting their generation in real-world applications. In this paper, we introduce GraspMAS, a new multi-agent system framework for language-driven grasp detection. GraspMAS is designed to reason through ambiguities and improve decision-making in real-world scenarios. Our framework consists of three specialized agents: Planner, responsible for strategizing complex queries; Coder, which generates and executes source code; and Observer, which evaluates the outcomes and provides feedback. Intensive experiments on two large-scale datasets demonstrate that our GraspMAS significantly outperforms existing baselines. Additionally, robot experiments conducted in both simulation and real-world settings further validate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Motivational Architecture for Open-Ended Learning Challenges in Robots</title>
<link>https://arxiv.org/abs/2506.18454</link>
<guid>https://arxiv.org/abs/2506.18454</guid>
<content:encoded><![CDATA[
arXiv:2506.18454v1 Announce Type: new 
Abstract: Developing agents capable of autonomously interacting with complex and dynamic environments, where task structures may change over time and prior knowledge cannot be relied upon, is a key prerequisite for deploying artificial systems in real-world settings. The open-ended learning framework identifies the core challenges for creating such agents, including the ability to autonomously generate new goals, acquire the necessary skills (or curricula of skills) to achieve them, and adapt to non-stationary environments. While many existing works tackles various aspects of these challenges in isolation, few propose integrated solutions that address them simultaneously. In this paper, we introduce H-GRAIL, a hierarchical architecture that, through the use of different typologies of intrinsic motivations and interconnected learning mechanisms, autonomously discovers new goals, learns the required skills for their achievement, generates skill sequences for tackling interdependent tasks, and adapts to non-stationary environments. We tested H-GRAIL in a real robotic scenario, demonstrating how the proposed solutions effectively address the various challenges of open-ended learning.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Networked pointing system: Bearing-only target localization and pointing control</title>
<link>https://arxiv.org/abs/2506.18460</link>
<guid>https://arxiv.org/abs/2506.18460</guid>
<content:encoded><![CDATA[
arXiv:2506.18460v1 Announce Type: new 
Abstract: In the paper, we formulate the target-pointing consensus problem where the headings of agents are required to point at a common target. Only a few agents in the network can measure the bearing information of the target. A two-step solution consisting of a bearing-only estimator for target localization and a control law for target pointing is constructed to address this problem. Compared to the strong assumptions of existing works, we only require two agents not collinear with the target to ensure localizability. By introducing the concept of virtual fusion node, we prove that both the estimation error and the tracking error converge asymptotically to the origin. The video demonstration of the verification can be found at https://youtu.be/S9- eyofk1DY.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AViLA: Asynchronous Vision-Language Agent for Streaming Multimodal Data Interaction</title>
<link>https://arxiv.org/abs/2506.18472</link>
<guid>https://arxiv.org/abs/2506.18472</guid>
<content:encoded><![CDATA[
arXiv:2506.18472v1 Announce Type: new 
Abstract: An ideal vision-language agent serves as a bridge between the human users and their surrounding physical world in real-world applications like autonomous driving and embodied agents, and proactively provides accurate and timely responses given user intents. An intriguing challenge arises when agents interact with the world as a dynamic data stream and ad-hoc queries from users: supporting knowledge for queries, namely evidence, usually appears asynchronously with the arrival time of queries, and agents need to ground their responses in historical data, present observations, and even future streams. We frame this challenge as Query-Evidence Asynchrony, where user queries and their supporting evidence typically arrive asynchronously in the streaming setting. This setting requires not only strong reasoning capabilities but also the ability to retain past observations and respond to queries with temporal awareness. In this paper, we introduce a diagnostic benchmark that evaluates Multimodal Large Language Models (MLLMs) on their ability to handle interaction with streaming data. Further, we present AViLA, Asynchronous Video-Language Agent for streaming data interaction that can handle ad-hoc queries and give time-aware responses. For this purpose, AViLA consists of three key modules: comprehensive memory retention, evidence identification, and evidence-grounded trigger, that are designed to maintain a general-purpose memory and respond readily and timely to queries. Our experiments show that existing models often fail to respond at appropriate times, while AViLA significantly improves both accuracy and temporal awareness. Our code and dataset will be publicly available.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reliability-Adjusted Prioritized Experience Replay</title>
<link>https://arxiv.org/abs/2506.18482</link>
<guid>https://arxiv.org/abs/2506.18482</guid>
<content:encoded><![CDATA[
arXiv:2506.18482v1 Announce Type: new 
Abstract: Experience replay enables data-efficient learning from past experiences in online reinforcement learning agents. Traditionally, experiences were sampled uniformly from a replay buffer, regardless of differences in experience-specific learning potential. In an effort to sample more efficiently, researchers introduced Prioritized Experience Replay (PER). In this paper, we propose an extension to PER by introducing a novel measure of temporal difference error reliability. We theoretically show that the resulting transition selection algorithm, Reliability-adjusted Prioritized Experience Replay (ReaPER), enables more efficient learning than PER. We further present empirical results showing that ReaPER outperforms PER across various environment types, including the Atari-5 benchmark.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Standard Applicability Judgment and Cross-jurisdictional Reasoning: A RAG-based Framework for Medical Device Compliance</title>
<link>https://arxiv.org/abs/2506.18511</link>
<guid>https://arxiv.org/abs/2506.18511</guid>
<content:encoded><![CDATA[
arXiv:2506.18511v1 Announce Type: new 
Abstract: Identifying the appropriate regulatory standard applicability remains a critical yet understudied challenge in medical device compliance, frequently necessitating expert interpretation of fragmented and heterogeneous documentation across different jurisdictions. To address this challenge, we introduce a modular AI system that leverages a retrieval-augmented generation (RAG) pipeline to automate standard applicability determination. Given a free-text device description, our system retrieves candidate standards from a curated corpus and uses large language models to infer jurisdiction-specific applicability, classified as Mandatory, Recommended, or Not Applicable, with traceable justifications. We construct an international benchmark dataset of medical device descriptions with expert-annotated standard mappings, and evaluate our system against retrieval-only, zero-shot, and rule-based baselines. The proposed approach attains a classification accuracy of 73% and a Top-5 retrieval recall of 87%, demonstrating its effectiveness in identifying relevant regulatory standards. We introduce the first end-to-end system for standard applicability reasoning, enabling scalable and interpretable AI-supported regulatory science. Notably, our region-aware RAG agent performs cross-jurisdictional reasoning between Chinese and U.S. standards, supporting conflict resolution and applicability justification across regulatory frameworks.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer World Model for Sample Efficient Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.18537</link>
<guid>https://arxiv.org/abs/2506.18537</guid>
<content:encoded><![CDATA[
arXiv:2506.18537v1 Announce Type: new 
Abstract: We present the Multi-Agent Transformer World Model (MATWM), a novel transformer-based world model designed for multi-agent reinforcement learning in both vector- and image-based environments. MATWM combines a decentralized imagination framework with a semi-centralized critic and a teammate prediction module, enabling agents to model and anticipate the behavior of others under partial observability. To address non-stationarity, we incorporate a prioritized replay mechanism that trains the world model on recent experiences, allowing it to adapt to agents' evolving policies. We evaluated MATWM on a broad suite of benchmarks, including the StarCraft Multi-Agent Challenge, PettingZoo, and MeltingPot. MATWM achieves state-of-the-art performance, outperforming both model-free and prior world model approaches, while demonstrating strong sample efficiency, achieving near-optimal performance in as few as 50K environment interactions. Ablation studies confirm the impact of each component, with substantial gains in coordination-heavy tasks.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Beam Selection for ISAC in Cell-Free Massive MIMO via Digital Twin-Assisted Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.18560</link>
<guid>https://arxiv.org/abs/2506.18560</guid>
<content:encoded><![CDATA[
arXiv:2506.18560v1 Announce Type: new 
Abstract: Beamforming enhances signal strength and quality by focusing energy in specific directions. This capability is particularly crucial in cell-free integrated sensing and communication (ISAC) systems, where multiple distributed access points (APs) collaborate to provide both communication and sensing services. In this work, we first derive the distribution of joint target detection probabilities across multiple receiving APs under false alarm rate constraints, and then formulate the beam selection procedure as a Markov decision process (MDP). We establish a deep reinforcement learning (DRL) framework, in which reward shaping and sinusoidal embedding are introduced to facilitate agent learning. To eliminate the high costs and associated risks of real-time agent-environment interactions, we further propose a novel digital twin (DT)-assisted offline DRL approach. Different from traditional online DRL, a conditional generative adversarial network (cGAN)-based DT module, operating as a replica of the real world, is meticulously designed to generate virtual state-action transition pairs and enrich data diversity, enabling offline adjustment of the agent's policy. Additionally, we address the out-of-distribution issue by incorporating an extra penalty term into the loss function design. The convergency of agent-DT interaction and the upper bound of the Q-error function are theoretically derived. Numerical results demonstrate the remarkable performance of our proposed approach, which significantly reduces online interaction overhead while maintaining effective beam selection across diverse conditions including strict false alarm control, low signal-to-noise ratios, and high target velocities.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Markets: Game Dynamics and Equilibrium in Markets with Learning Agents</title>
<link>https://arxiv.org/abs/2506.18571</link>
<guid>https://arxiv.org/abs/2506.18571</guid>
<content:encoded><![CDATA[
arXiv:2506.18571v1 Announce Type: new 
Abstract: Autonomous and learning agents increasingly participate in markets - setting prices, placing bids, ordering inventory. Such agents are not just aiming to optimize in an uncertain environment; they are making decisions in a game-theoretical environment where the decision of one agent influences the profit of other agents. While game theory usually predicts outcomes of strategic interaction as an equilibrium, it does not capture how repeated interaction of learning agents arrives at a certain outcome. This article surveys developments in modeling agent behavior as dynamical systems, with a focus on projected gradient and no-regret learning algorithms. In general, learning in games can lead to all types of dynamics, including convergence to equilibrium, but also cycles and chaotic behavior. It is important to understand when we can expect efficient equilibrium in automated markets and when this is not the case. Thus, we analyze when and how learning agents converge to an equilibrium of a market game, drawing on tools from variational inequalities and Lyapunov stability theory. Special attention is given to the stability of projected dynamics and the convergence to equilibrium sets as limiting outcomes. Overall, the paper provides mathematical foundations for analyzing stability and convergence in agentic markets driven by autonomous, learning agents.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reply to "Emergent LLM behaviors are observationally equivalent to data leakage"</title>
<link>https://arxiv.org/abs/2506.18600</link>
<guid>https://arxiv.org/abs/2506.18600</guid>
<content:encoded><![CDATA[
arXiv:2506.18600v1 Announce Type: new 
Abstract: A potential concern when simulating populations of large language models (LLMs) is data contamination, i.e. the possibility that training data may shape outcomes in unintended ways. While this concern is important and may hinder certain experiments with multi-agent models, it does not preclude the study of genuinely emergent dynamics in LLM populations. The recent critique by Barrie and T\"ornberg [1] of the results of Flint Ashery et al. [2] offers an opportunity to clarify that self-organisation and model-dependent emergent dynamics can be studied in LLM populations, highlighting how such dynamics have been empirically observed in the specific case of social conventions.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Reinforcement Learning for Inverse Design in Photonic Integrated Circuits</title>
<link>https://arxiv.org/abs/2506.18627</link>
<guid>https://arxiv.org/abs/2506.18627</guid>
<content:encoded><![CDATA[
arXiv:2506.18627v1 Announce Type: new 
Abstract: Inverse design of photonic integrated circuits (PICs) has traditionally relied on gradientbased optimization. However, this approach is prone to end up in local minima, which results in suboptimal design functionality. As interest in PICs increases due to their potential for addressing modern hardware demands through optical computing, more adaptive optimization algorithms are needed. We present a reinforcement learning (RL) environment as well as multi-agent RL algorithms for the design of PICs. By discretizing the design space into a grid, we formulate the design task as an optimization problem with thousands of binary variables. We consider multiple two- and three-dimensional design tasks that represent PIC components for an optical computing system. By decomposing the design space into thousands of individual agents, our algorithms are able to optimize designs with only a few thousand environment samples. They outperform previous state-of-the-art gradient-based optimization in both twoand three-dimensional design tasks. Our work may also serve as a benchmark for further exploration of sample-efficient RL for inverse design in photonics.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-level Behavioral Consistency for Inter-group and Intra-group Coordination in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2506.18651</link>
<guid>https://arxiv.org/abs/2506.18651</guid>
<content:encoded><![CDATA[
arXiv:2506.18651v1 Announce Type: new 
Abstract: Behavioral diversity in Multi-agent reinforcement learning(MARL) represents an emerging and promising research area. Prior work has largely centered on intra-group behavioral consistency in multi-agent systems, with limited attention given to behavioral consistency in multi-agent grouping scenarios. In this paper, we introduce Dual-Level Behavioral Consistency (DLBC), a novel MARL control method designed to explicitly regulate agent behaviors at both intra-group and inter-group levels. DLBC partitions agents into distinct groups and dynamically modulates behavioral diversity both within and between these groups. By dynamically modulating behavioral diversity within and between these groups, DLBC achieves enhanced division of labor through inter-group consistency, which constrains behavioral strategies across different groups. Simultaneously, intra-group consistency, achieved by aligning behavioral strategies within each group, fosters stronger intra-group cooperation. Crucially, DLBC's direct constraint of agent policy functions ensures its broad applicability across various algorithmic frameworks. Experimental results in various grouping cooperation scenarios demonstrate that DLBC significantly enhances both intra-group cooperative performance and inter-group task specialization, yielding substantial performance improvements. DLBC provides new ideas for behavioral consistency control of multi-intelligent body systems, and its potential for application in more complex tasks and dynamic environments can be further explored in the future.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation</title>
<link>https://arxiv.org/abs/2506.18678</link>
<guid>https://arxiv.org/abs/2506.18678</guid>
<content:encoded><![CDATA[
arXiv:2506.18678v1 Announce Type: new 
Abstract: Neural implicit scene representations have recently shown promising results in dense visual SLAM. However, existing implicit SLAM algorithms are constrained to single-agent scenarios, and fall difficulties in large-scale scenes and long sequences. Existing NeRF-based multi-agent SLAM frameworks cannot meet the constraints of communication bandwidth. To this end, we propose the first distributed multi-agent collaborative neural SLAM framework with hybrid scene representation, distributed camera tracking, intra-to-inter loop closure, and online distillation for multiple submap fusion. A novel triplane-grid joint scene representation method is proposed to improve scene reconstruction. A novel intra-to-inter loop closure method is designed to achieve local (single-agent) and global (multi-agent) consistency. We also design a novel online distillation method to fuse the information of different submaps to achieve global consistency. Furthermore, to the best of our knowledge, there is no real-world dataset for NeRF-based/GS-based SLAM that provides both continuous-time trajectories groundtruth and high-accuracy 3D meshes groundtruth. To this end, we propose the first real-world Dense slam (DES) dataset covering both single-agent and multi-agent scenarios, ranging from small rooms to large-scale outdoor scenes, with high-accuracy ground truth for both 3D mesh and continuous-time camera trajectory. This dataset can advance the development of the research in both SLAM, 3D reconstruction, and visual foundation model. Experiments on various datasets demonstrate the superiority of the proposed method in both mapping, tracking, and communication. The dataset and code will open-source on https://github.com/dtc111111/mcnslam.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MARL-MambaContour: Unleashing Multi-Agent Deep Reinforcement Learning for Active Contour Optimization in Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2506.18679</link>
<guid>https://arxiv.org/abs/2506.18679</guid>
<content:encoded><![CDATA[
arXiv:2506.18679v1 Announce Type: new 
Abstract: We introduce MARL-MambaContour, the first contour-based medical image segmentation framework based on Multi-Agent Reinforcement Learning (MARL). Our approach reframes segmentation as a multi-agent cooperation task focused on generate topologically consistent object-level contours, addressing the limitations of traditional pixel-based methods which could lack topological constraints and holistic structural awareness of anatomical regions. Each contour point is modeled as an autonomous agent that iteratively adjusts its position to align precisely with the target boundary, enabling adaptation to blurred edges and intricate morphologies common in medical images. This iterative adjustment process is optimized by a contour-specific Soft Actor-Critic (SAC) algorithm, further enhanced with the Entropy Regularization Adjustment Mechanism (ERAM) which dynamically balance agent exploration with contour smoothness. Furthermore, the framework incorporates a Mamba-based policy network featuring a novel Bidirectional Cross-attention Hidden-state Fusion Mechanism (BCHFM). This mechanism mitigates potential memory confusion limitations associated with long-range modeling in state space models, thereby facilitating more accurate inter-agent information exchange and informed decision-making. Extensive experiments on five diverse medical imaging datasets demonstrate the state-of-the-art performance of MARL-MambaContour, highlighting its potential as an accurate and robust clinical application.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety-Aware Optimal Scheduling for Autonomous Masonry Construction using Collaborative Heterogeneous Aerial Robots</title>
<link>https://arxiv.org/abs/2506.18697</link>
<guid>https://arxiv.org/abs/2506.18697</guid>
<content:encoded><![CDATA[
arXiv:2506.18697v1 Announce Type: new 
Abstract: This paper presents a novel high-level task planning and optimal coordination framework for autonomous masonry construction, using a team of heterogeneous aerial robotic workers, consisting of agents with separate skills for brick placement and mortar application. This introduces new challenges in scheduling and coordination, particularly due to the mortar curing deadline required for structural bonding and ensuring the safety constraints among UAVs operating in parallel. To address this, an automated pipeline generates the wall construction plan based on the available bricks while identifying static structural dependencies and potential conflicts for safe operation. The proposed framework optimizes UAV task allocation and execution timing by incorporating dynamically coupled precedence deadline constraints that account for the curing process and static structural dependency constraints, while enforcing spatio-temporal constraints to prevent collisions and ensure safety. The primary objective of the scheduler is to minimize the overall construction makespan while minimizing logistics, traveling time between tasks, and the curing time to maintain both adhesion quality and safe workspace separation. The effectiveness of the proposed method in achieving coordinated and time-efficient aerial masonry construction is extensively validated through Gazebo simulated missions. The results demonstrate the framework's capability to streamline UAV operations, ensuring both structural integrity and safety during the construction process.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Representation to Mediation: A New Agenda for Conceptual Modeling Research in A Digital World</title>
<link>https://arxiv.org/abs/2506.18743</link>
<guid>https://arxiv.org/abs/2506.18743</guid>
<content:encoded><![CDATA[
arXiv:2506.18743v1 Announce Type: new 
Abstract: The role of information systems (IS) as representations of real-world systems is changing in an increasingly digitalized world, suggesting that conceptual modeling is losing its relevance to the IS field. We argue the opposite: Conceptual modeling research is more relevant to the IS field than ever, but it requires an update with current theory. We develop a new theoretical framework of conceptual modeling that delivers a fundamental shift in the assumptions that govern research in this area. This move can make traditional knowledge about conceptual modeling consistent with the emerging requirements of a digital world. Our framework draws attention to the role of conceptual modeling scripts as mediators between physical and digital realities. We identify new research questions about grammars, methods, scripts, agents, and contexts that are situated in intertwined physical and digital realities. We discuss several implications for conceptual modeling scholarship that relate to the necessity of developing new methods and grammars for conceptual modeling, broadening the methodological array of conceptual modeling scholarship, and considering new dependent variables.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRIZ Agents: A Multi-Agent LLM Approach for TRIZ-Based Innovation</title>
<link>https://arxiv.org/abs/2506.18783</link>
<guid>https://arxiv.org/abs/2506.18783</guid>
<content:encoded><![CDATA[
arXiv:2506.18783v1 Announce Type: new 
Abstract: TRIZ, the Theory of Inventive Problem Solving, is a structured, knowledge-based framework for innovation and abstracting problems to find inventive solutions. However, its application is often limited by the complexity and deep interdisciplinary knowledge required. Advancements in Large Language Models (LLMs) have revealed new possibilities for automating parts of this process. While previous studies have explored single LLMs in TRIZ applications, this paper introduces a multi-agent approach. We propose an LLM-based multi-agent system, called TRIZ agents, each with specialized capabilities and tool access, collaboratively solving inventive problems based on the TRIZ methodology. This multi-agent system leverages agents with various domain expertise to efficiently navigate TRIZ steps. The aim is to model and simulate an inventive process with language agents. We assess the effectiveness of this team of agents in addressing complex innovation challenges based on a selected case study in engineering. We demonstrate the potential of agent collaboration to produce diverse, inventive solutions. This research contributes to the future of AI-driven innovation, showcasing the advantages of decentralized problem-solving in complex ideation tasks.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Allocation with Money: What is Your Objective?</title>
<link>https://arxiv.org/abs/2506.18794</link>
<guid>https://arxiv.org/abs/2506.18794</guid>
<content:encoded><![CDATA[
arXiv:2506.18794v1 Announce Type: new 
Abstract: When allocating indivisible items, there are various ways to use monetary transfers for eliminating envy. Particularly, one can apply a balanced vector of transfer payments, or charge each agent a positive amount, or -- contrarily -- give each agent a positive amount as a ``subsidy''. In each model, one can aim to minimize the amount of payments used; this aim translates into different optimization objectives in each setting. This note compares the various models, and the relations between upper and lower bounds for these objectives.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Online Control with Adversarial Disturbances</title>
<link>https://arxiv.org/abs/2506.18814</link>
<guid>https://arxiv.org/abs/2506.18814</guid>
<content:encoded><![CDATA[
arXiv:2506.18814v1 Announce Type: new 
Abstract: Multi-agent control problems involving a large number of agents with competing and time-varying objectives are increasingly prevalent in applications across robotics, economics, and energy systems. In this paper, we study online control in multi-agent linear dynamical systems with disturbances. In contrast to most prior work in multi-agent control, we consider an online setting where disturbances are adversarial and where each agent seeks to minimize its own, adversarial sequence of convex losses. In this setting, we investigate the robustness of gradient-based controllers from single-agent online control, with a particular focus on understanding how individual regret guarantees are influenced by the number of agents in the system. Under minimal communication assumptions, we prove near-optimal sublinear regret bounds that hold uniformly for all agents. Finally, when the objectives of the agents are aligned, we show that the multi-agent control problem induces a time-varying potential game for which we derive equilibrium gap guarantees.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Software Engineering Agents: A Study of Thought-Action-Result Trajectories</title>
<link>https://arxiv.org/abs/2506.18824</link>
<guid>https://arxiv.org/abs/2506.18824</guid>
<content:encoded><![CDATA[
arXiv:2506.18824v1 Announce Type: new 
Abstract: Large Language Model (LLM)-based agents are increasingly employed to automate complex software engineering tasks such as program repair and issue resolution. These agents operate by autonomously generating natural language thoughts, invoking external tools, and iteratively refining their solutions. Despite their widespread adoption, the internal decision-making processes of these agents remain largely unexplored, limiting our understanding of their operational dynamics and failure modes. In this paper, we present a large-scale empirical study of the thought-action-result trajectories of three state-of-the-art LLM-based agents: \textsc{RepairAgent}, \textsc{AutoCodeRover}, and \textsc{OpenHands}. We unify their interaction logs into a common format, capturing 120 trajectories and 2822 LLM interactions focused on program repair and issue resolution. Our study combines quantitative analyses of structural properties, action patterns, and token usage with qualitative assessments of reasoning coherence and feedback integration. We identify key trajectory characteristics such as iteration counts and token consumption, recurring action sequences, and the semantic coherence linking thoughts, actions, and their results. Our findings reveal behavioral motifs and anti-patterns that distinguish successful from failed executions, providing actionable insights for improving agent design, including prompting strategies, failure diagnosis, and anti-pattern detection. We release our dataset and annotation framework to support further research on transparent and robust autonomous software engineering agents.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Offline Goal-Conditioned Reinforcement Learning with Projective Quasimetric Planning</title>
<link>https://arxiv.org/abs/2506.18847</link>
<guid>https://arxiv.org/abs/2506.18847</guid>
<content:encoded><![CDATA[
arXiv:2506.18847v1 Announce Type: new 
Abstract: Offline Goal-Conditioned Reinforcement Learning seeks to train agents to reach specified goals from previously collected trajectories. Scaling that promises to long-horizon tasks remains challenging, notably due to compounding value-estimation errors. Principled geometric offers a potential solution to address these issues. Following this insight, we introduce Projective Quasimetric Planning (ProQ), a compositional framework that learns an asymmetric distance and then repurposes it, firstly as a repulsive energy forcing a sparse set of keypoints to uniformly spread over the learned latent space, and secondly as a structured directional cost guiding towards proximal sub-goals. In particular, ProQ couples this geometry with a Lagrangian out-of-distribution detector to ensure the learned keypoints stay within reachable areas. By unifying metric learning, keypoint coverage, and goal-conditioned control, our approach produces meaningful sub-goals and robustly drives long-horizon goal-reaching on diverse a navigation benchmarks.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale Multi-Agent Gaussian SLAM</title>
<link>https://arxiv.org/abs/2506.18885</link>
<guid>https://arxiv.org/abs/2506.18885</guid>
<content:encoded><![CDATA[
arXiv:2506.18885v1 Announce Type: new 
Abstract: 3D Gaussian splatting has emerged as an expressive scene representation for RGB-D visual SLAM, but its application to large-scale, multi-agent outdoor environments remains unexplored. Multi-agent Gaussian SLAM is a promising approach to rapid exploration and reconstruction of environments, offering scalable environment representations, but existing approaches are limited to small-scale, indoor environments. To that end, we propose Gaussian Reconstruction via Multi-Agent Dense SLAM, or GRAND-SLAM, a collaborative Gaussian splatting SLAM method that integrates i) an implicit tracking module based on local optimization over submaps and ii) an approach to inter- and intra-robot loop closure integrated into a pose-graph optimization framework. Experiments show that GRAND-SLAM provides state-of-the-art tracking performance and 28% higher PSNR than existing methods on the Replica indoor dataset, as well as 91% lower multi-agent tracking error and improved rendering over existing multi-agent methods on the large-scale, outdoor Kimera-Multi dataset.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering Conceptual Bias via Transformer Latent-Subspace Activation</title>
<link>https://arxiv.org/abs/2506.18887</link>
<guid>https://arxiv.org/abs/2506.18887</guid>
<content:encoded><![CDATA[
arXiv:2506.18887v1 Announce Type: new 
Abstract: This work examines whether activating latent subspaces in language models (LLMs) can steer scientific code generation toward a specific programming language. Five causal LLMs were first evaluated on scientific coding prompts to quantify their baseline bias among four programming languages. A static neuron-attribution method, perturbing the highest activated MLP weight for a C++ or CPP token, proved brittle and exhibited limited generalization across prompt styles and model scales. To address these limitations, a gradient-refined adaptive activation steering framework (G-ACT) was developed: per-prompt activation differences are clustered into a small set of steering directions, and lightweight per-layer probes are trained and refined online to select the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably biases generation towards the CPP language by increasing the average probe classification accuracy by 15% and the early layers (0-6) improving the probe classification accuracy by 61.5% compared to the standard ACT framework. For LLaMA-3.3 70B, where attention-head signals become more diffuse, targeted injections at key layers still improve language selection. Although per-layer probing introduces a modest inference overhead, it remains practical by steering only a subset of layers and enables reproducible model behavior. These results demonstrate a scalable, interpretable and efficient mechanism for concept-level control for practical agentic systems.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Audit &amp; Repair: An Agentic Framework for Consistent Story Visualization in Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2506.18900</link>
<guid>https://arxiv.org/abs/2506.18900</guid>
<content:encoded><![CDATA[
arXiv:2506.18900v1 Announce Type: new 
Abstract: Story visualization has become a popular task where visual scenes are generated to depict a narrative across multiple panels. A central challenge in this setting is maintaining visual consistency, particularly in how characters and objects persist and evolve throughout the story. Despite recent advances in diffusion models, current approaches often fail to preserve key character attributes, leading to incoherent narratives. In this work, we propose a collaborative multi-agent framework that autonomously identifies, corrects, and refines inconsistencies across multi-panel story visualizations. The agents operate in an iterative loop, enabling fine-grained, panel-level updates without re-generating entire sequences. Our framework is model-agnostic and flexibly integrates with a variety of diffusion models, including rectified flow transformers such as Flux and latent diffusion models such as Stable Diffusion. Quantitative and qualitative experiments show that our method outperforms prior approaches in terms of multi-panel consistency.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Closed curve covering and multiagent TSP ratios</title>
<link>https://arxiv.org/abs/2506.16675</link>
<guid>https://arxiv.org/abs/2506.16675</guid>
<content:encoded><![CDATA[
arXiv:2506.16675v1 Announce Type: cross 
Abstract: How efficiently can a closed curve of unit length in $\mathbb{R}^d$ be covered by $k$ closed curves so as to minimize the maximum length of the $k$ curves? We show that the maximum length is at most $2k^{-1} - \frac{1}{4} k^{-4}$ for all $k\geq 2$ and $d \geq 2$. As a byproduct, we show that $k$ agents can traverse a Euclidean TSP instance significantly faster than a single agent. We thereby sharpen recent planar results by Berendsohn, Kim, and Kozma (2025) and extend these improvements to all dimensions.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Challenges in Grounding Language in the Real World</title>
<link>https://arxiv.org/abs/2506.17375</link>
<guid>https://arxiv.org/abs/2506.17375</guid>
<content:encoded><![CDATA[
arXiv:2506.17375v1 Announce Type: cross 
Abstract: A long-term goal of Artificial Intelligence is to build a language understanding system that allows a human to collaborate with a physical robot using language that is natural to the human. In this paper we highlight some of the challenges in doing this, and propose a solution that integrates the abilities of a cognitive agent capable of interactive task learning in a physical robot with the linguistic abilities of a large language model. We also point the way to an initial implementation of this approach.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wisdom of Crowds Through Myopic Self-Confidence Adaptation</title>
<link>https://arxiv.org/abs/2506.18195</link>
<guid>https://arxiv.org/abs/2506.18195</guid>
<content:encoded><![CDATA[
arXiv:2506.18195v1 Announce Type: cross 
Abstract: The wisdom of crowds is an umbrella term for phenomena suggesting that the collective judgment or decision of a large group can be more accurate than the individual judgments or decisions of the group members. A well-known example illustrating this concept is the competition at a country fair described by Galton, where the median value of the individual guesses about the weight of an ox resulted in an astonishingly accurate estimate of the actual weight. This phenomenon resembles classical results in probability theory and relies on independent decision-making. The accuracy of the group's final decision can be significantly reduced if the final agents' opinions are driven by a few influential agents.
  In this paper, we consider a group of agents who initially possess uncorrelated and unbiased noisy measurements of a common state of the world. Assume these agents iteratively update their estimates according to a simple non-Bayesian learning rule, commonly known in mathematical sociology as the French-DeGroot dynamics or iterative opinion pooling. As a result of this iterative distributed averaging process, each agent arrives at an asymptotic estimate of the state of the world, with the variance of this estimate determined by the matrix of weights the agents assign to each other. Every agent aims at minimizing the variance of her asymptotic estimate of the state of the world; however, such variance is also influenced by the weights allocated by other agents. To achieve the best possible estimate, the agents must then solve a game-theoretic, multi-objective optimization problem defined by the available sets of influence weights. We characterize both the Pareto frontier and the set of Nash equilibria in the resulting game. Additionally, we examine asynchronous best-response dynamics for the group of agents and prove their convergence to the set of strict Nash equilibria.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Neural Cellular Automata: Application to modeling of contrast enhancement in breast MRI</title>
<link>https://arxiv.org/abs/2506.18720</link>
<guid>https://arxiv.org/abs/2506.18720</guid>
<content:encoded><![CDATA[
arXiv:2506.18720v1 Announce Type: cross 
Abstract: Synthetic contrast enhancement offers fast image acquisition and eliminates the need for intravenous injection of contrast agent. This is particularly beneficial for breast imaging, where long acquisition times and high cost are significantly limiting the applicability of magnetic resonance imaging (MRI) as a widespread screening modality. Recent studies have demonstrated the feasibility of synthetic contrast generation. However, current state-of-the-art (SOTA) methods lack sufficient measures for consistent temporal evolution. Neural cellular automata (NCA) offer a robust and lightweight architecture to model evolving patterns between neighboring cells or pixels. In this work we introduce TeNCA (Temporal Neural Cellular Automata), which extends and further refines NCAs to effectively model temporally sparse, non-uniformly sampled imaging data. To achieve this, we advance the training strategy by enabling adaptive loss computation and define the iterative nature of the method to resemble a physical progression in time. This conditions the model to learn a physiologically plausible evolution of contrast enhancement. We rigorously train and test TeNCA on a diverse breast MRI dataset and demonstrate its effectiveness, surpassing the performance of existing methods in generation of images that align with ground truth post-contrast sequences.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentives for Responsiveness, Instrumental Control and Impact</title>
<link>https://arxiv.org/abs/2001.07118</link>
<guid>https://arxiv.org/abs/2001.07118</guid>
<content:encoded><![CDATA[
arXiv:2001.07118v3 Announce Type: replace 
Abstract: We introduce three concepts that describe an agent's incentives: response incentives indicate which variables in the environment, such as sensitive demographic information, affect the decision under the optimal policy. Instrumental control incentives indicate whether an agent's policy is chosen to manipulate part of its environment, such as the preferences or instructions of a user. Impact incentives indicate which variables an agent will affect, intentionally or otherwise. For each concept, we establish sound and complete graphical criteria, and discuss general classes of techniques that may be used to produce incentives for safe and fair agent behaviour. Finally, we outline how these notions may be generalised to multi-decision settings. This journal-length paper extends our conference publications "Incentives for Responsiveness, Instrumental Control and Impact" and "Agent Incentives: A Causal Perspective": the material on response incentives and instrumental control incentives is updated, while the work on impact incentives and multi-decision settings is entirely new.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IDCAIS: Inter-Defender Collision-Aware Interception Strategy against Multiple Attackers</title>
<link>https://arxiv.org/abs/2112.12098</link>
<guid>https://arxiv.org/abs/2112.12098</guid>
<content:encoded><![CDATA[
arXiv:2112.12098v3 Announce Type: replace 
Abstract: In the prior literature on multi-agent area defense games, the assignments of the defenders to the attackers are done based on a cost metric associated only with the interception of the attackers. In contrast to that, this paper presents an Inter-Defender Collision-Aware Interception Strategy (IDCAIS) for defenders to intercept attackers in order to defend a protected area, such that the defender-to-attacker assignment protocol not only takes into account an interception-related cost but also takes into account any possible future collisions among the defenders on their optimal interception trajectories. In particular, in this paper, the defenders are assigned to intercept attackers using a mixed-integer quadratic program (MIQP) that: 1) minimizes the sum of times taken by defenders to capture the attackers under time-optimal control, as well as 2) helps eliminate or delay possible future collisions among the defenders on the optimal trajectories. To prevent inevitable collisions on optimal trajectories or collisions arising due to time-sub-optimal behavior by the attackers, a minimally augmented control using exponential control barrier function (ECBF) is also provided. Simulations show the efficacy of the approach.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DSAC-C: Constrained Maximum Entropy for Robust Discrete Soft-Actor Critic</title>
<link>https://arxiv.org/abs/2310.17173</link>
<guid>https://arxiv.org/abs/2310.17173</guid>
<content:encoded><![CDATA[
arXiv:2310.17173v2 Announce Type: replace 
Abstract: We present a novel extension to the family of Soft Actor-Critic (SAC) algorithms. We argue that based on the Maximum Entropy Principle, discrete SAC can be further improved via additional statistical constraints derived from a surrogate critic policy. Furthermore, our findings suggests that these constraints provide an added robustness against potential domain shifts, which are essential for safe deployment of reinforcement learning agents in the real-world. We provide theoretical analysis and show empirical results on low data regimes for both in-distribution and out-of-distribution variants of Atari 2600 games.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Multi-Agent Reinforcement Learning for Distributed Multi-Robot Problems</title>
<link>https://arxiv.org/abs/2401.00212</link>
<guid>https://arxiv.org/abs/2401.00212</guid>
<content:encoded><![CDATA[
arXiv:2401.00212v4 Announce Type: replace 
Abstract: The networked nature of multi-robot systems presents challenges in the context of multi-agent reinforcement learning. Centralized control policies do not scale with increasing numbers of robots, whereas independent control policies do not exploit the information provided by other robots, exhibiting poor performance in cooperative-competitive tasks. In this work we propose a physics-informed reinforcement learning approach able to learn distributed multi-robot control policies that are both scalable and make use of all the available information to each robot. Our approach has three key characteristics. First, it imposes a port-Hamiltonian structure on the policy representation, respecting energy conservation properties of physical robot systems and the networked nature of robot team interactions. Second, it uses self-attention to ensure a sparse policy representation able to handle time-varying information at each robot from the interaction graph. Third, we present a soft actor-critic reinforcement learning algorithm parameterized by our self-attention port-Hamiltonian control policy, which accounts for the correlation among robots during training while overcoming the need of value function factorization. Extensive simulations in different multi-robot scenarios demonstrate the success of the proposed approach, surpassing previous multi-robot reinforcement learning solutions in scalability, while achieving similar or superior performance (with averaged cumulative reward up to x2 greater than the state-of-the-art with robot teams x6 larger than the number of robots at training time). We also validate our approach on multiple real robots in the Georgia Tech Robotarium under imperfect communication, demonstrating zero-shot sim-to-real transfer and scalability across number of robots.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Soft Actor-Critic with Coordinated Loss for Autonomous Mobility-on-Demand Fleet Control</title>
<link>https://arxiv.org/abs/2404.06975</link>
<guid>https://arxiv.org/abs/2404.06975</guid>
<content:encoded><![CDATA[
arXiv:2404.06975v2 Announce Type: replace 
Abstract: We study a sequential decision-making problem for a profit-maximizing operator of an autonomous mobility-on-demand system. Optimizing a central operator's vehicle-to-request dispatching policy requires efficient and effective fleet control strategies. To this end, we employ a multi-agent Soft Actor-Critic algorithm combined with weighted bipartite matching. We propose a novel vehicle-based algorithm architecture and adapt the critic's loss function to appropriately consider coordinated actions. Furthermore, we extend our algorithm to incorporate rebalancing capabilities. Through numerical experiments, we show that our approach outperforms state-of-the-art benchmarks by up to 12.9% for dispatching and up to 38.9% with integrated rebalancing.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Decision Making Based on Structural Information Principles</title>
<link>https://arxiv.org/abs/2404.09760</link>
<guid>https://arxiv.org/abs/2404.09760</guid>
<content:encoded><![CDATA[
arXiv:2404.09760v2 Announce Type: replace 
Abstract: Hierarchical Reinforcement Learning (HRL) is a promising approach for managing task complexity across multiple levels of abstraction and accelerating long-horizon agent exploration. However, the effectiveness of hierarchical policies heavily depends on prior knowledge and manual assumptions about skill definitions and task decomposition. In this paper, we propose a novel Structural Information principles-based framework, namely SIDM, for hierarchical Decision Making in both single-agent and multi-agent scenarios. Central to our work is the utilization of structural information embedded in the decision-making process to adaptively and dynamically discover and learn hierarchical policies through environmental abstractions. Specifically, we present an abstraction mechanism that processes historical state-action trajectories to construct abstract representations of states and actions. We define and optimize directed structural entropy, a metric quantifying the uncertainty in transition dynamics between abstract states, to discover skills that capture key transition patterns in RL environments. Building on these findings, we develop a skill-based learning method for single-agent scenarios and a role-based collaboration method for multi-agent scenarios, both of which can flexibly integrate various underlying algorithms for enhanced performance. Extensive evaluations on challenging benchmarks demonstrate that our framework significantly and consistently outperforms state-of-the-art baselines, improving the effectiveness, efficiency, and stability of policy learning by up to 32.70%, 64.86%, and 88.26%, respectively, as measured by average rewards, convergence timesteps, and standard deviations.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Mean Estimation Among Heterogeneous Strategic Agents: Individual Rationality, Fairness, and Truthful Contribution</title>
<link>https://arxiv.org/abs/2407.15881</link>
<guid>https://arxiv.org/abs/2407.15881</guid>
<content:encoded><![CDATA[
arXiv:2407.15881v2 Announce Type: replace 
Abstract: We study a collaborative learning problem where $m$ agents aim to estimate a vector $\mu =(\mu_1,\ldots,\mu_d)\in \mathbb{R}^d$ by sampling from associated univariate normal distributions $\{\mathcal{N}(\mu_k, \sigma^2)\}_{k\in[d]}$. Agent $i$ incurs a cost $c_{i,k}$ to sample from $\mathcal{N}(\mu_k, \sigma^2)$. Instead of working independently, agents can exchange data, collecting cheaper samples and sharing them in return for costly data, thereby reducing both costs and estimation error. We design a mechanism to facilitate such collaboration, while addressing two key challenges: ensuring individually rational (IR) and fair outcomes so all agents benefit, and preventing strategic behavior (e.g. non-collection, data fabrication) to avoid socially undesirable outcomes. We design a mechanism and an associated Nash equilibrium (NE) which minimizes the social penalty-sum of agents' estimation errors and collection costs-while being IR for all agents. We achieve a $\mathcal{O}(\sqrt{m})$-approximation to the minimum social penalty in the worst case and an $\mathcal{O}(1)$-approximation under favorable conditions. Additionally, we establish three hardness results: no nontrivial mechanism guarantees (i) a dominant strategy equilibrium where agents report truthfully, (ii) is IR for every strategy profile of other agents, (iii) or avoids a worst-case $\Omega(\sqrt{m})$ price of stability in any NE. Finally, by integrating concepts from axiomatic bargaining, we demonstrate that our mechanism supports fairer outcomes than one which minimizes social penalty.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Fine-Tuning of Multi-Task Policies</title>
<link>https://arxiv.org/abs/2410.05026</link>
<guid>https://arxiv.org/abs/2410.05026</guid>
<content:encoded><![CDATA[
arXiv:2410.05026v3 Announce Type: replace 
Abstract: Pre-trained generalist policies are rapidly gaining relevance in robot learning due to their promise of fast adaptation to novel, in-domain tasks. This adaptation often relies on collecting new demonstrations for a specific task of interest and applying imitation learning algorithms, such as behavioral cloning. However, as soon as several tasks need to be learned, we must decide which tasks should be demonstrated and how often? We study this multi-task problem and explore an interactive framework in which the agent adaptively selects the tasks to be demonstrated. We propose AMF (Active Multi-task Fine-tuning), an algorithm to maximize multi-task policy performance under a limited demonstration budget by collecting demonstrations yielding the largest information gain on the expert policy. We derive performance guarantees for AMF under regularity assumptions and demonstrate its empirical effectiveness to efficiently fine-tune neural policies in complex and high-dimensional environments.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Hive Mind is a Single Reinforcement Learning Agent</title>
<link>https://arxiv.org/abs/2410.17517</link>
<guid>https://arxiv.org/abs/2410.17517</guid>
<content:encoded><![CDATA[
arXiv:2410.17517v3 Announce Type: replace 
Abstract: Decision-making is an essential attribute of any intelligent agent or group. Natural systems are known to converge to optimal strategies through at least two distinct mechanisms: collective decision-making via imitation of others, and individual trial-and-error. This paper establishes an equivalence between these two paradigms by drawing from the well-established collective decision-making model of nest-site selection in swarms of honey bees. We show that the emergent distributed cognition (sometimes referred to as the hive mind ) arising from individual bees following simple, local imitation-based rules is equivalent to a single online reinforcement learning (RL) agent interacting with many parallel environments. The update rule through which this macro-agent learns is a bandit algorithm that we coin Maynard-Cross Learning. Our analysis implies that a group of cognition-limited organisms can be on-par with a more complex, reinforcement-enabled entity, substantiating the idea that group-level intelligence may explain how seemingly simple and blind individual behaviors are selected in nature.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Resilient Vector Consensus</title>
<link>https://arxiv.org/abs/2411.03633</link>
<guid>https://arxiv.org/abs/2411.03633</guid>
<content:encoded><![CDATA[
arXiv:2411.03633v2 Announce Type: replace 
Abstract: This paper studies privacy-preserving resilient vector consensus in multi-agent systems against faulty agents, where normal agents can achieve consensus within the convex hull of their initial states while protecting state vectors from being disclosed. Specifically, we consider a modification of an existing algorithm known as Approximate Distributed Robust Convergence Using Centerpoints (ADRC), i.e., Privacy-Preserving ADRC (PP-ADRC). Under PP-ADRC, each normal agent introduces multivariate Gaussian noise to its state during each iteration. We first provide sufficient conditions to ensure that all normal agents' states can achieve mean square convergence under PP-ADRC. Then, we analyze convergence accuracy from two perspectives, i.e., the Mahalanobis distance of the final value from its expectation and the Hausdorff distance-based alteration of the convex hull caused by noise when only partial dimensions are added with noise. Then, we employ concentrated geo-privacy to characterize privacy preservation and conduct a thorough comparison with differential privacy. Finally, numerical simulations demonstrate the theoretical results.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LightVA: Lightweight Visual Analytics with LLM Agent-Based Task Planning and Execution</title>
<link>https://arxiv.org/abs/2411.05651</link>
<guid>https://arxiv.org/abs/2411.05651</guid>
<content:encoded><![CDATA[
arXiv:2411.05651v2 Announce Type: replace 
Abstract: Visual analytics (VA) requires analysts to iteratively propose analysis tasks based on observations and execute tasks by creating visualizations and interactive exploration to gain insights. This process demands skills in programming, data processing, and visualization tools, highlighting the need for a more intelligent, streamlined VA approach. Large language models (LLMs) have recently been developed as agents to handle various tasks with dynamic planning and tool-using capabilities, offering the potential to enhance the efficiency and versatility of VA. We propose LightVA, a lightweight VA framework that supports task decomposition, data analysis, and interactive exploration through human-agent collaboration. Our method is designed to help users progressively translate high-level analytical goals into low-level tasks, producing visualizations and deriving insights. Specifically, we introduce an LLM agent-based task planning and execution strategy, employing a recursive process involving a planner, executor, and controller. The planner is responsible for recommending and decomposing tasks, the executor handles task execution, including data analysis, visualization generation and multi-view composition, and the controller coordinates the interaction between the planner and executor. Building on the framework, we develop a system with a hybrid user interface that includes a task flow diagram for monitoring and managing the task planning process, a visualization panel for interactive data exploration, and a chat view for guiding the model through natural language instructions. We examine the effectiveness of our method through a usage scenario and an expert study.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeAR: Graph-enhanced Agent for Retrieval-augmented Generation</title>
<link>https://arxiv.org/abs/2412.18431</link>
<guid>https://arxiv.org/abs/2412.18431</guid>
<content:encoded><![CDATA[
arXiv:2412.18431v2 Announce Type: replace 
Abstract: Retrieval-augmented Generation (RAG) relies on effective retrieval capabilities, yet traditional sparse and dense retrievers inherently struggle with multi-hop retrieval scenarios. In this paper, we introduce GeAR, a system that advances RAG performance through two key innovations: (i) an efficient graph expansion mechanism that augments any conventional base retriever, such as BM25, and (ii) an agent framework that incorporates the resulting graph-based retrieval into a multi-step retrieval framework. Our evaluation demonstrates GeAR's superior retrieval capabilities across three multi-hop question answering datasets. Notably, our system achieves state-of-the-art results with improvements exceeding 10% on the challenging MuSiQue dataset, while consuming fewer tokens and requiring fewer iterations than existing multi-step retrieval systems. The project page is available at https://gear-rag.github.io.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Guided Self-Debugging Code Generation</title>
<link>https://arxiv.org/abs/2502.02928</link>
<guid>https://arxiv.org/abs/2502.02928</guid>
<content:encoded><![CDATA[
arXiv:2502.02928v2 Announce Type: replace 
Abstract: Automated code generation is gaining significant importance in intelligent computer programming and system deployment. However, current approaches often face challenges in computational efficiency and lack robust mechanisms for code parsing and error correction. In this work, we propose a novel framework, PyCapsule, with a simple yet effective two-agent pipeline and efficient self-debugging modules for Python code generation. PyCapsule features sophisticated prompt inference, iterative error handling, and case testing, ensuring high generation stability, safety, and correctness. Empirically, PyCapsule achieves up to 5.7% improvement of success rate on HumanEval, 10.3% on HumanEval-ET, and 24.4% on BigCodeBench compared to the state-of-art methods. We also observe a decrease in normalized success rate given more self-debugging attempts, potentially affected by limited and noisy error feedback in retention. PyCapsule demonstrates broader impacts on advancing lightweight and efficient code generation for artificial intelligence systems.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compromising Honesty and Harmlessness in Language Models via Deception Attacks</title>
<link>https://arxiv.org/abs/2502.08301</link>
<guid>https://arxiv.org/abs/2502.08301</guid>
<content:encoded><![CDATA[
arXiv:2502.08301v2 Announce Type: replace 
Abstract: Recent research on large language models (LLMs) has demonstrated their ability to understand and employ deceptive behavior, even without explicit prompting. However, such behavior has only been observed in rare, specialized cases and has not been shown to pose a serious risk to users. Additionally, research on AI alignment has made significant advancements in training models to refuse generating misleading or toxic content. As a result, LLMs generally became honest and harmless. In this study, we introduce "deception attacks" that undermine both of these traits, revealing a vulnerability that, if exploited, could have serious real-world consequences. We introduce fine-tuning methods that cause models to selectively deceive users on targeted topics while remaining accurate on others. Through a series of experiments, we show that such targeted deception is effective even in high-stakes domains or ideologically charged subjects. In addition, we find that deceptive fine-tuning often compromises other safety properties: deceptive models are more likely to produce toxic content, including hate speech and stereotypes. Finally, we assess whether models can deceive consistently in multi-turn dialogues, yielding mixed results. Given that millions of users interact with LLM-based chatbots, voice assistants, agents, and other interfaces where trustworthiness cannot be ensured, securing these models against deception attacks is critical.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stop Overvaluing Multi-Agent Debate -- We Must Rethink Evaluation and Embrace Model Heterogeneity</title>
<link>https://arxiv.org/abs/2502.08788</link>
<guid>https://arxiv.org/abs/2502.08788</guid>
<content:encoded><![CDATA[
arXiv:2502.08788v3 Announce Type: replace 
Abstract: Multi-agent debate (MAD) has gained significant attention as a promising line of research to improve the factual accuracy and reasoning capabilities of large language models (LLMs). Despite its conceptual appeal, current MAD research suffers from critical limitations in evaluation practices, including limited benchmark coverage, weak baseline comparisons, and inconsistent setups. This paper presents a systematic evaluation of 5 representative MAD methods across 9 benchmarks using 4 foundational models. Surprisingly, our findings reveal that MAD often fail to outperform simple single-agent baselines such as Chain-of-Thought and Self-Consistency, even when consuming significantly more inference-time computation. To advance MAD research, we further explore the role of model heterogeneity and find it as a universal antidote to consistently improve current MAD frameworks. Based on our findings, we argue that the field must stop overvaluing MAD in its current form; for true advancement, we must critically rethink evaluation paradigms and actively embrace model heterogeneity as a core design principle.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>POPGym Arcade: Parallel Pixelated POMDPs</title>
<link>https://arxiv.org/abs/2503.01450</link>
<guid>https://arxiv.org/abs/2503.01450</guid>
<content:encoded><![CDATA[
arXiv:2503.01450v5 Announce Type: replace 
Abstract: We present the POPGym Arcade, a collection of hardware-accelerated, pixel-based environments with shared observation and action spaces. Each environment includes fully and partially observable variants, enabling counterfactual studies on partial observability. We also introduce mathematical tools for analyzing policies under partial observability, which reveal how agents recall past information to make decisions. Our analysis shows (1) that controlling for partial observability is critical and (2) that agents with long-term memory learn brittle policies that struggle to generalize. Finally, we demonstrate that recurrent policies can be "poisoned" by old, out-of-distribution observations, with implications for sim-to-real transfer, imitation learning, and offline reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trajectory Prediction for Autonomous Driving: Progress, Limitations, and Future Directions</title>
<link>https://arxiv.org/abs/2503.03262</link>
<guid>https://arxiv.org/abs/2503.03262</guid>
<content:encoded><![CDATA[
arXiv:2503.03262v2 Announce Type: replace 
Abstract: As the potential for autonomous vehicles to be integrated on a large scale into modern traffic systems continues to grow, ensuring safe navigation in dynamic environments is crucial for smooth integration. To guarantee safety and prevent collisions, autonomous vehicles must be capable of accurately predicting the trajectories of surrounding traffic agents. Over the past decade, significant efforts from both academia and industry have been dedicated to designing solutions for precise trajectory forecasting. These efforts have produced a diverse range of approaches, raising questions about the differences between these methods and whether trajectory prediction challenges have been fully addressed. This paper reviews a substantial portion of recent trajectory prediction methods proposing a taxonomy to classify existing solutions. A general overview of the prediction pipeline is also provided, covering input and output modalities, modeling features, and prediction paradigms existing in the literature. In addition, the paper discusses active research areas within trajectory prediction, addresses the posed research questions, and highlights the remaining research gaps and challenges.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shaken, Not Stirred: A Novel Dataset for Visual Understanding of Glasses in Human-Robot Bartending Tasks</title>
<link>https://arxiv.org/abs/2503.04308</link>
<guid>https://arxiv.org/abs/2503.04308</guid>
<content:encoded><![CDATA[
arXiv:2503.04308v2 Announce Type: replace 
Abstract: Datasets for object detection often do not account for enough variety of glasses, due to their transparent and reflective properties. Specifically, open-vocabulary object detectors, widely used in embodied robotic agents, fail to distinguish subclasses of glasses. This scientific gap poses an issue to robotic applications that suffer from accumulating errors between detection, planning, and action execution. The paper introduces a novel method for the acquisition of real-world data from RGB-D sensors that minimizes human effort. We propose an auto-labeling pipeline that generates labels for all the acquired frames based on the depth measurements. We provide a novel real-world glass object dataset that was collected on the Neuro-Inspired COLlaborator (NICOL), a humanoid robot platform. The data set consists of 7850 images recorded from five different cameras. We show that our trained baseline model outperforms state-of-the-art open-vocabulary approaches. In addition, we deploy our baseline model in an embodied agent approach to the NICOL platform, on which it achieves a success rate of 81% in a human-robot bartending scenario.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>API Agents vs. GUI Agents: Divergence and Convergence</title>
<link>https://arxiv.org/abs/2503.11069</link>
<guid>https://arxiv.org/abs/2503.11069</guid>
<content:encoded><![CDATA[
arXiv:2503.11069v2 Announce Type: replace 
Abstract: Large language models (LLMs) have evolved beyond simple text generation to power software agents that directly translate natural language commands into tangible actions. While API-based LLM agents initially rose to prominence for their robust automation capabilities and seamless integration with programmatic endpoints, recent progress in multimodal LLM research has enabled GUI-based LLM agents that interact with graphical user interfaces in a human-like manner. Although these two paradigms share the goal of enabling LLM-driven task automation, they diverge significantly in architectural complexity, development workflows, and user interaction models.
  This paper presents the first comprehensive comparative study of API-based and GUI-based LLM agents, systematically analyzing their divergence and potential convergence. We examine key dimensions and highlight scenarios in which hybrid approaches can harness their complementary strengths. By proposing clear decision criteria and illustrating practical use cases, we aim to guide practitioners and researchers in selecting, combining, or transitioning between these paradigms. Ultimately, we indicate that continuing innovations in LLM-based automation are poised to blur the lines between API- and GUI-driven agents, paving the way for more flexible, adaptive solutions in a wide range of real-world applications.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Virtual Agent Learning and Reasoning: A Step-Wise, Multi-Dimensional, and Generalist Reward Model with Benchmark</title>
<link>https://arxiv.org/abs/2503.18665</link>
<guid>https://arxiv.org/abs/2503.18665</guid>
<content:encoded><![CDATA[
arXiv:2503.18665v2 Announce Type: replace 
Abstract: The development of Generalist Virtual Agents (GVAs) has shown significant promise in autonomous task execution. However, current training paradigms face critical limitations, including reliance on outcome supervision and labor-intensive human annotations. To address these challenges, we propose Similar, a Step-Wise Multi-Dimensional Generalist Reward Model, which offers fine-grained signals for agent training and can choose better action for inference-time scaling. Specifically, we begin by systematically defining five dimensions for evaluating agent actions. Building on this framework, we design an MCTS-P algorithm to automatically collect and annotate step-wise, five-dimensional agent execution data. Using this data, we train Similar with the Triple-M strategy. Furthermore, we introduce the first benchmark in the virtual agent domain for step-wise, multi-dimensional reward model training and evaluation, named SRM. This benchmark consists of two components: SRMTrain, which serves as the training set for Similar, and SRMEval, a manually selected test set for evaluating the reward model. Experimental results demonstrate that Similar, through its step-wise, multi-dimensional assessment and synergistic gain, provides GVAs with effective intermediate signals during both training and inference-time scaling. The project is available at https://github.com/antgroup/Similar.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rubric Is All You Need: Enhancing LLM-based Code Evaluation With Question-Specific Rubrics</title>
<link>https://arxiv.org/abs/2503.23989</link>
<guid>https://arxiv.org/abs/2503.23989</guid>
<content:encoded><![CDATA[
arXiv:2503.23989v2 Announce Type: replace 
Abstract: Since the emergence of Large Language Models (LLMs) popularized by the release of GPT-3 and ChatGPT, LLMs have shown remarkable promise in programming-related tasks. While code generation using LLMs has become a popular field of research, code evaluation using LLMs remains under-explored. In this paper, we focus on LLM-based code evaluation and attempt to fill in the existing gaps. We propose multi-agentic novel approaches using \emph{question-specific rubrics} tailored to the problem statement, arguing that these perform better for logical assessment than the existing approaches that use \emph{question-agnostic rubrics}. To address the lack of suitable evaluation datasets, we introduce two datasets: a Data Structures and Algorithms dataset containing 150 student submissions from a popular Data Structures and Algorithms practice website, and an Object Oriented Programming dataset comprising 80 student submissions from undergraduate computer science courses. In addition to using standard metrics (Spearman Correlation, Cohen's Kappa), we additionally propose a new metric called as Leniency, which quantifies evaluation strictness relative to expert assessment. Our comprehensive analysis demonstrates that \emph{question-specific rubrics} significantly enhance logical assessment of code in educational settings, providing better feedback aligned with instructional goals beyond mere syntactic correctness.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoPDL: Automatic Prompt Optimization for LLM Agents</title>
<link>https://arxiv.org/abs/2504.04365</link>
<guid>https://arxiv.org/abs/2504.04365</guid>
<content:encoded><![CDATA[
arXiv:2504.04365v2 Announce Type: replace 
Abstract: The performance of large language models (LLMs) depends on how they are prompted, with choices spanning both the high-level prompting pattern (e.g., Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and few-shot demonstrations). Manually tuning this combination is tedious, error-prone, and specific to a given LLM and task. Therefore, this paper proposes AutoPDL, an automated approach to discovering good LLM agent configurations. Our approach frames this as a structured AutoML problem over a combinatorial space of agentic and non-agentic prompting patterns and demonstrations, using successive halving to efficiently navigate this space. We introduce a library implementing common prompting patterns using the PDL prompt programming language. AutoPDL solutions are human-readable, editable, and executable PDL programs that use this library. This approach also enables source-to-source optimization, allowing human-in-the-loop refinement and reuse. Evaluations across three tasks and seven LLMs (ranging from 3B to 70B parameters) show consistent accuracy gains ($9.06\pm15.3$ percentage points), up to 68.9pp, and reveal that selected prompting strategies vary across models and tasks.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kimi-VL Technical Report</title>
<link>https://arxiv.org/abs/2504.07491</link>
<guid>https://arxiv.org/abs/2504.07491</guid>
<content:encoded><![CDATA[
arXiv:2504.07491v3 Announce Type: replace 
Abstract: We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE) vision-language model (VLM) that offers advanced multimodal reasoning, long-context understanding, and strong agent capabilities - all while activating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL demonstrates strong performance across challenging domains: as a general-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld), matching flagship models. Furthermore, it exhibits remarkable capabilities across diverse challenging vision language tasks, including college-level image and video comprehension, OCR, mathematical reasoning, and multi-image understanding. In comparative evaluations, it effectively competes with cutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and Gemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also advances in processing long contexts and perceiving clearly. With a 128K extended context window, Kimi-VL can process diverse long inputs, achieving impressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its native-resolution vision encoder, MoonViT, further allows it to see and understand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and 34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common tasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant: Kimi-VL-Thinking-2506. Developed through long chain-of-thought (CoT) supervised fine-tuning (SFT) and reinforcement learning (RL), the latest model exhibits strong long-horizon reasoning capabilities (64.0 on MMMU, 46.3 on MMMU-Pro, 56.9 on MathVision, 80.1 on MathVista, 65.2 on VideoMMMU) while obtaining robust general abilities. Code and models are publicly accessible at https://github.com/MoonshotAI/Kimi-VL.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Enhanced Reinforcement Learning for Power Grid Security Assessment</title>
<link>https://arxiv.org/abs/2504.14412</link>
<guid>https://arxiv.org/abs/2504.14412</guid>
<content:encoded><![CDATA[
arXiv:2504.14412v2 Announce Type: replace 
Abstract: The increasingly challenging task of maintaining power grid security requires innovative solutions. Novel approaches using reinforcement learning (RL) agents have been proposed to help grid operators navigate the massive decision space and nonlinear behavior of these complex networks. However, applying RL to power grid security assessment, specifically for combinatorially troublesome contingency analysis problems, has proven difficult to scale. The integration of quantum computing into these RL frameworks helps scale by improving computational efficiency and boosting agent proficiency by leveraging quantum advantages in action exploration and model-based interdependence. To demonstrate a proof-of-concept use of quantum computing for RL agent training and simulation, we propose a hybrid agent that runs on quantum hardware using IBM's Qiskit Runtime. We also provide detailed insight into the construction of parameterized quantum circuits (PQCs) for generating relevant quantum output. This agent's proficiency at maintaining grid stability is demonstrated relative to a benchmark model without quantum enhancement using N-k contingency analysis. Additionally, we offer a comparative assessment of the training procedures for RL models integrated with a quantum backend.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>