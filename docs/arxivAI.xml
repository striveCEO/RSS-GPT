<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs</link>

<item>
<title>PaperArena: An Evaluation Benchmark for Tool-Augmented Agentic Reasoning on Scientific Literature</title>
<link>https://arxiv.org/abs/2510.10909</link>
<guid>https://arxiv.org/abs/2510.10909</guid>
<content:encoded><![CDATA[

arXiv:2510.10909v2 Announce Type: replace 
Abstract: Understanding and reasoning on the web-scale scientific literature is a crucial touchstone for large language model (LLM) based agents designed to support complex knowledge-intensive tasks. However, existing works are mainly restricted to tool-free tasks within isolated papers, largely due to the lack of a benchmark for cross-paper reasoning and multi-tool orchestration in real research scenarios. In this work, we propose PaperArena, an evaluation benchmark for agents to address real-world research questions that typically require integrating information across multiple papers with the assistance of external tools. Given a research question, agents should integrate diverse formats across multiple papers through reasoning and interacting with appropriate tools, thereby producing a well-grounded answer. To support standardized evaluation, we provide a modular and extensible platform for agent execution, offering tools such as multimodal parsing, context retrieval, and programmatic computation. Experimental results reveal that even the most advanced LLM powering a well-established agent system achieves merely 38.78% average accuracy. On the hard subset, accuracy drops to only 18.47%, highlighting great potential for improvement. We also present several empirical findings, including that all agents tested exhibit inefficient tool usage, often invoking more tools than necessary to solve a task. We invite the community to adopt PaperArena to develop and evaluate more capable agents for scientific discovery. Our code and data are available https://github.com/Melmaphother/PaperArena.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>DecoupleSearch: Decouple Planning and Search via Hierarchical Reward Modeling</title>
<link>https://arxiv.org/abs/2510.21712</link>
<guid>https://arxiv.org/abs/2510.21712</guid>
<content:encoded><![CDATA[
arXiv:2510.21712v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) systems have emerged as a pivotal methodology for enhancing Large Language Models (LLMs) through the dynamic integration of external knowledge. To further improve RAG's flexibility, Agentic RAG introduces autonomous agents into the workflow. However, Agentic RAG faces several challenges: (1) the success of each step depends on both high-quality planning and accurate search, (2) the lack of supervision for intermediate reasoning steps, and (3) the exponentially large candidate space for planning and searching. To address these challenges, we propose DecoupleSearch, a novel framework that decouples planning and search processes using dual value models, enabling independent optimization of plan reasoning and search grounding. Our approach constructs a reasoning tree, where each node represents planning and search steps. We leverage Monte Carlo Tree Search to assess the quality of each step. During inference, Hierarchical Beam Search iteratively refines planning and search candidates with dual value models. Extensive experiments across policy models of varying parameter sizes, demonstrate the effectiveness of our method.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Enhanced Operator Assistance for UNICOS Applications</title>
<link>https://arxiv.org/abs/2510.21717</link>
<guid>https://arxiv.org/abs/2510.21717</guid>
<content:encoded><![CDATA[
arXiv:2510.21717v1 Announce Type: new 
Abstract: This project explores the development of an AI-enhanced operator assistant for UNICOS, CERN's UNified Industrial Control System. While powerful, UNICOS presents a number of challenges, including the cognitive burden of decoding widgets, manual effort required for root cause analysis, and difficulties maintainers face in tracing datapoint elements (DPEs) across a complex codebase. In situations where timely responses are critical, these challenges can increase cognitive load and slow down diagnostics. To address these issues, a multi-agent system was designed and implemented. The solution is supported by a modular architecture comprising a UNICOS-side extension written in CTRL code, a Python-based multi-agent system deployed on a virtual machine, and a vector database storing both operator documentation and widget animation code. Preliminary evaluations suggest that the system is capable of decoding widgets, performing root cause analysis by leveraging live device data and documentation, and tracing DPEs across a complex codebase. Together, these capabilities reduce the manual workload of operators and maintainers, enhance situational awareness in operations, and accelerate responses to alarms and anomalies. Beyond these immediate gains, this work highlights the potential of introducing multi-modal reasoning and retrieval augmented generation (RAG) into the domain of industrial control. Ultimately, this work represents more than a proof of concept: it provides a basis for advancing intelligent operator interfaces at CERN. By combining modular design, extensibility, and practical AI integration, this project not only alleviates current operator pain points but also points toward broader opportunities for assistive AI in accelerator operations.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PREFINE: Personalized Story Generation via Simulated User Critics and User-Specific Rubric Generation</title>
<link>https://arxiv.org/abs/2510.21721</link>
<guid>https://arxiv.org/abs/2510.21721</guid>
<content:encoded><![CDATA[
arXiv:2510.21721v1 Announce Type: new 
Abstract: While recent advances in Large Language Models (LLMs) have improved the quality of creative text generation, significant challenges remain in producing personalized stories that reflect individual user preferences. Conventional approaches rely on explicit feedback or fine-tuning, which presents practical issues regarding user burden, data collection, computational costs, and privacy. In this work, we propose PREFINE (Persona-and-Rubric Guided Critique-and-Refine), a novel framework that extends the Critique-and-Refine paradigm to personalization. PREFINE constructs a pseudo-user agent from a user's interaction history and generates user-specific rubrics (evaluation criteria). By having this agent critique and refine outputs on the user's behalf based on these tailored rubrics, our method achieves personalized generation without requiring parameter updates or direct user feedback. We conducted a comprehensive evaluation on the PerDOC and PerMPST story datasets. We designed three baseline methods and several model variants to verify the contribution of each component of our framework. In automatic evaluations (LLM-as-a-Judge), PREFINE achieved higher win rates and statistically significant scores than the baselines, without compromising general story quality. Analysis of the model variants confirmed that both the pseudo-user agent and the user-specific rubrics are crucial for enhancing personalization performance. Beyond story generation, our approach holds potential for enabling efficient personalization in broader applications, such as dialogue systems, education, and recommendation.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Task Assignment, Sequencing and Multi-agent Path-finding</title>
<link>https://arxiv.org/abs/2510.21738</link>
<guid>https://arxiv.org/abs/2510.21738</guid>
<content:encoded><![CDATA[
arXiv:2510.21738v1 Announce Type: new 
Abstract: In this article, we address the problem of collaborative task assignment, sequencing, and multi-agent pathfinding (TSPF), where a team of agents must visit a set of task locations without collisions while minimizing flowtime. TSPF incorporates agent-task compatibility constraints and ensures that all tasks are completed. We propose a Conflict-Based Search with Task Sequencing (CBS-TS), an optimal and complete algorithm that alternates between finding new task sequences and resolving conflicts in the paths of current sequences. CBS-TS uses a mixed-integer linear program (MILP) to optimize task sequencing and employs Conflict-Based Search (CBS) with Multi-Label A* (MLA*) for collision-free path planning within a search forest. By invoking MILP for the next-best sequence only when needed, CBS-TS efficiently limits the search space, enhancing computational efficiency while maintaining optimality. We compare the performance of our CBS-TS against Conflict-based Steiner Search (CBSS), a baseline method that, with minor modifications, can address the TSPF problem. Experimental results demonstrate that CBS-TS outperforms CBSS in most testing scenarios, achieving higher success rates and consistently optimal solutions, whereas CBSS achieves near-optimal solutions in some cases. The supplementary video is available at https://youtu.be/QT8BYgvefmU.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taxonomy and Trends in Reinforcement Learning for Robotics and Control Systems: A Structured Review</title>
<link>https://arxiv.org/abs/2510.21758</link>
<guid>https://arxiv.org/abs/2510.21758</guid>
<content:encoded><![CDATA[
arXiv:2510.21758v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has become a foundational approach for enabling intelligent robotic behavior in dynamic and uncertain environments. This work presents an in-depth review of RL principles, advanced deep reinforcement learning (DRL) algorithms, and their integration into robotic and control systems. Beginning with the formalism of Markov Decision Processes (MDPs), the study outlines essential elements of the agent-environment interaction and explores core algorithmic strategies including actor-critic methods, value-based learning, and policy gradients. Emphasis is placed on modern DRL techniques such as DDPG, TD3, PPO, and SAC, which have shown promise in solving high-dimensional, continuous control tasks. A structured taxonomy is introduced to categorize RL applications across domains such as locomotion, manipulation, multi-agent coordination, and human-robot interaction, along with training methodologies and deployment readiness levels. The review synthesizes recent research efforts, highlighting technical trends, design patterns, and the growing maturity of RL in real-world robotics. Overall, this work aims to bridge theoretical advances with practical implementations, providing a consolidated perspective on the evolving role of RL in autonomous robotic systems.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Pose Uncertainty: A Differentiable Rendering Cram\'er-Rao Bound</title>
<link>https://arxiv.org/abs/2510.21785</link>
<guid>https://arxiv.org/abs/2510.21785</guid>
<content:encoded><![CDATA[
arXiv:2510.21785v1 Announce Type: new 
Abstract: Pose estimation is essential for many applications within computer vision and robotics. Despite its uses, few works provide rigorous uncertainty quantification for poses under dense or learned models. We derive a closed-form lower bound on the covariance of camera pose estimates by treating a differentiable renderer as a measurement function. Linearizing image formation with respect to a small pose perturbation on the manifold yields a render-aware Cram\'er-Rao bound. Our approach reduces to classical bundle-adjustment uncertainty, ensuring continuity with vision theory. It also naturally extends to multi-agent settings by fusing Fisher information across cameras. Our statistical formulation has downstream applications for tasks such as cooperative perception and novel view synthesis without requiring explicit keypoint correspondences.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting</title>
<link>https://arxiv.org/abs/2510.21817</link>
<guid>https://arxiv.org/abs/2510.21817</guid>
<content:encoded><![CDATA[
arXiv:2510.21817v1 Announce Type: new 
Abstract: Current Vision-Language-Action (VLA) models are often constrained by a rigid, static interaction paradigm, which lacks the ability to see, hear, speak, and act concurrently as well as handle real-time user interruptions dynamically. This hinders seamless embodied collaboration, resulting in an inflexible and unresponsive user experience. To address these limitations, we introduce VITA-E, a novel embodied interaction framework designed for both behavioral concurrency and nearly real-time interruption. The core of our approach is a dual-model architecture where two parallel VLA instances operate as an ``Active Model'' and a ``Standby Model'', allowing the embodied agent to observe its environment, listen to user speech, provide verbal responses, and execute actions, all concurrently and interruptibly, mimicking human-like multitasking capabilities. We further propose a ``model-as-controller'' paradigm, where we fine-tune the VLM to generate special tokens that serve as direct system-level commands, coupling the model's reasoning with the system's behavior. Experiments conducted on a physical humanoid platform demonstrate that VITA-E can reliably handle complex interactive scenarios. Our framework is compatible with various dual-system VLA models, achieving an extremely high success rate on emergency stops and speech interruptions while also successfully performing concurrent speech and action. This represents a significant step towards more natural and capable embodied assistants.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCoPE VLM: Selective Context Processing for Efficient Document Navigation in Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.21850</link>
<guid>https://arxiv.org/abs/2510.21850</guid>
<content:encoded><![CDATA[
arXiv:2510.21850v1 Announce Type: new 
Abstract: Understanding long-context visual information remains a fundamental challenge for vision-language models, particularly in agentic tasks such as GUI control and web navigation. While web pages and GUI environments are inherently structured documents, current VLMs typically neglect decision-oriented document understanding in their training objectives. Existing approaches primarily extend visual embeddings to process long, high-resolution inputs, but these methods are memory-intensive and impractical for locally deployable solutions. To address these issues, we propose SCoPE VLM, a document navigation expert that leverages a novel Chain of Scroll mechanism to selectively and recursively navigate documents, focusing exclusively on relevant segments. We introduce a dedicated data generation pipeline to construct informative Chain of Scroll trajectories and Episodic Group Relative Policy Optimization, a tailored reinforcement learning method to reduce the gap between training and inference. Our method substantially reduces memory usage and effectively models human-like reading behaviors. To the best of our knowledge, SCoPE VLM is the first framework to explicitly model agentic reading patterns in multi-page document question answering, advancing the capabilities of multimodal agents.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIGN: Schema-Induced Games for Naming</title>
<link>https://arxiv.org/abs/2510.21855</link>
<guid>https://arxiv.org/abs/2510.21855</guid>
<content:encoded><![CDATA[
arXiv:2510.21855v1 Announce Type: new 
Abstract: Real-world AI systems are tackling increasingly complex problems, often through interactions among large language model (LLM) agents. When these agents develop inconsistent conventions, coordination can break down. Applications such as collaborative coding and distributed planning therefore require reliable, consistent communication, and scalability is a central concern as systems grow. We introduce Schema-Induced Games for Naming (SIGN), a naming game that examines how lightweight structure can steer convention formation. We compare schema-induced communication to unconstrained natural language and find faster convergence with up to 5.8x higher agreement. These results suggest that minimal structure can act as a simple control knob for efficient multi-agent coordination, pointing toward broader applications beyond the naming game.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing Corner Cases in Autonomous Driving: A World Model-based Approach with Mixture of Experts and LLMs</title>
<link>https://arxiv.org/abs/2510.21867</link>
<guid>https://arxiv.org/abs/2510.21867</guid>
<content:encoded><![CDATA[
arXiv:2510.21867v1 Announce Type: new 
Abstract: Accurate and reliable motion forecasting is essential for the safe deployment of autonomous vehicles (AVs), particularly in rare but safety-critical scenarios known as corner cases. Existing models often underperform in these situations due to an over-representation of common scenes in training data and limited generalization capabilities. To address this limitation, we present WM-MoE, the first world model-based motion forecasting framework that unifies perception, temporal memory, and decision making to address the challenges of high-risk corner-case scenarios. The model constructs a compact scene representation that explains current observations, anticipates future dynamics, and evaluates the outcomes of potential actions. To enhance long-horizon reasoning, we leverage large language models (LLMs) and introduce a lightweight temporal tokenizer that maps agent trajectories and contextual cues into the LLM's feature space without additional training, enriching temporal context and commonsense priors. Furthermore, a mixture-of-experts (MoE) is introduced to decompose complex corner cases into subproblems and allocate capacity across scenario types, and a router assigns scenes to specialized experts that infer agent intent and perform counterfactual rollouts. In addition, we introduce nuScenes-corner, a new benchmark that comprises four real-world corner-case scenarios for rigorous evaluation. Extensive experiments on four benchmark datasets (nuScenes, NGSIM, HighD, and MoCAD) showcase that WM-MoE consistently outperforms state-of-the-art (SOTA) baselines and remains robust under corner-case and data-missing conditions, indicating the promise of world model-based architectures for robust and generalizable motion forecasting in fully AVs.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Literature Survey Automation with an Iterative Workflow</title>
<link>https://arxiv.org/abs/2510.21900</link>
<guid>https://arxiv.org/abs/2510.21900</guid>
<content:encoded><![CDATA[
arXiv:2510.21900v1 Announce Type: new 
Abstract: Automatic literature survey generation has attracted increasing attention, yet most existing systems follow a one-shot paradigm, where a large set of papers is retrieved at once and a static outline is generated before drafting. This design often leads to noisy retrieval, fragmented structures, and context overload, ultimately limiting survey quality. Inspired by the iterative reading process of human researchers, we propose \ours, a framework based on recurrent outline generation, in which a planning agent incrementally retrieves, reads, and updates the outline to ensure both exploration and coherence. To provide faithful paper-level grounding, we design paper cards that distill each paper into its contributions, methods, and findings, and introduce a review-and-refine loop with visualization enhancement to improve textual flow and integrate multimodal elements such as figures and tables. Experiments on both established and emerging topics show that \ours\ substantially outperforms state-of-the-art baselines in content coverage, structural coherence, and citation quality, while producing more accessible and better-organized surveys. To provide a more reliable assessment of such improvements, we further introduce Survey-Arena, a pairwise benchmark that complements absolute scoring and more clearly positions machine-generated surveys relative to human-written ones. The code is available at https://github.com/HancCui/IterSurvey\_Autosurveyv2.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Software Engineering Agents for Embodied Controller Generation : A Study in Minigrid Environments</title>
<link>https://arxiv.org/abs/2510.21902</link>
<guid>https://arxiv.org/abs/2510.21902</guid>
<content:encoded><![CDATA[
arXiv:2510.21902v1 Announce Type: new 
Abstract: Software Engineering Agents (SWE-Agents) have proven effective for traditional software engineering tasks with accessible codebases, but their performance for embodied tasks requiring well-designed information discovery remains unexplored. We present the first extended evaluation of SWE-Agents on controller generation for embodied tasks, adapting Mini-SWE-Agent (MSWEA) to solve 20 diverse embodied tasks from the Minigrid environment. Our experiments compare agent performance across different information access conditions: with and without environment source code access, and with varying capabilities for interactive exploration. We quantify how different information access levels affect SWE-Agent performance for embodied tasks and analyze the relative importance of static code analysis versus dynamic exploration for task solving. This work establishes controller generation for embodied tasks as a crucial evaluation domain for SWE-Agents and provides baseline results for future research in efficient reasoning systems.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TOM-SWE: User Mental Modeling For Software Engineering Agents</title>
<link>https://arxiv.org/abs/2510.21903</link>
<guid>https://arxiv.org/abs/2510.21903</guid>
<content:encoded><![CDATA[
arXiv:2510.21903v1 Announce Type: new 
Abstract: Recent advances in coding agents have made them capable of planning, editing, running, and testing complex code bases. Despite their growing ability in coding tasks, these systems still struggle to infer and track user intent, especially when instructions are underspecified or context-dependent. To bridge this gap, we introduce ToM-SWE, a dual-agent architecture that pairs a primary software-engineering (SWE) agent with a lightweight theory-of-mind (ToM) partner agent dedicated to modeling the user's mental state. The ToM agent infers user goals, constraints, and preferences from instructions and interaction history, maintains a \textbf{persistent memory} of the user, and provides user-related suggestions to the SWE agent. In two software engineering benchmarks (ambiguous SWE-bench and stateful SWE-bench), ToM-SWE improves task success rates and user satisfaction. Notably, on the stateful SWE benchmark, a newly introduced evaluation that provides agents with a user simulator along with previous interaction histories, ToM-SWE achieves a substantially higher task success rate of 59.7\% compared to 18.1\% for OpenHands, a state-of-the-art SWE agent. Furthermore, in a three-week study with professional developers using ToM-SWE in their daily work, participants found it useful 86\% of the time, underscoring the value of stateful user modeling for practical coding agents.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conditional Recall</title>
<link>https://arxiv.org/abs/2510.21904</link>
<guid>https://arxiv.org/abs/2510.21904</guid>
<content:encoded><![CDATA[
arXiv:2510.21904v1 Announce Type: new 
Abstract: In the neon-lit nights of 2026, Johnson \& Johnson unveiled X. A pill, not larger than a snowflake, that promised a tempest of change. This miraculous drug didn't just allow people to cherry-pick memories to erase from their minds, it could also leave a reminder of this erasure in the minds of those who ingested it.
  Amidst the iconic red-bricked walls of Harvard Law, you, with books in one hand and dreams in the other, are on a mission. You are not just another student; you carry the hope of revolutionizing the archaic chambers of the legal world. Each night, as you pore over the tomes of law, you wonder what greatness society can achieve.
  On a cold evening, your phone buzzes. It's Dex, your old college friend turned underground dealer. His message is simple: ``Got X. Special price for you.'' The temptation swirls around you. Would you trade the lessons of the past for a clearer, yet incomplete future? The decision rests in your hands.
  We explore the game theoretic implications of a technology (such as TEEs) that allows agents to commit to forget information and discuss several applications.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparison of Conversational Models and Humans in Answering Technical Questions: the Firefox Case</title>
<link>https://arxiv.org/abs/2510.21933</link>
<guid>https://arxiv.org/abs/2510.21933</guid>
<content:encoded><![CDATA[
arXiv:2510.21933v1 Announce Type: new 
Abstract: The use of Large Language Models (LLMs) to support tasks in software development has steadily increased over recent years. From assisting developers in coding activities to providing conversational agents that answer newcomers' questions. In collaboration with the Mozilla Foundation, this study evaluates the effectiveness of Retrieval-Augmented Generation (RAG) in assisting developers within the Mozilla Firefox project. We conducted an empirical analysis comparing responses from human developers, a standard GPT model, and a GPT model enhanced with RAG, using real queries from Mozilla's developer chat rooms. To ensure a rigorous evaluation, Mozilla experts assessed the responses based on helpfulness, comprehensiveness, and conciseness. The results show that RAG-assisted responses were more comprehensive than human developers (62.50% to 54.17%) and almost as helpful (75.00% to 79.17%), suggesting RAG's potential to enhance developer assistance. However, the RAG responses were not as concise and often verbose. The results show the potential to apply RAG-based tools to Open Source Software (OSS) to minimize the load to core maintainers without losing answer quality. Toning down retrieval mechanisms and making responses even shorter in the future would enhance developer assistance in massive projects like Mozilla Firefox.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>We Need Accountability in Human-AI Agent Relationships</title>
<link>https://arxiv.org/abs/2510.21967</link>
<guid>https://arxiv.org/abs/2510.21967</guid>
<content:encoded><![CDATA[
arXiv:2510.21967v1 Announce Type: new 
Abstract: We argue that accountability mechanisms are needed in human-AI agent relationships to ensure alignment with user and societal interests. We propose a framework according to which AI agents' engagement is conditional on appropriate user behaviour. The framework incorporates design-strategies such as distancing, disengaging, and discouraging.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LightAgent: Mobile Agentic Foundation Models</title>
<link>https://arxiv.org/abs/2510.22009</link>
<guid>https://arxiv.org/abs/2510.22009</guid>
<content:encoded><![CDATA[
arXiv:2510.22009v1 Announce Type: new 
Abstract: With the advancement of multimodal large language models (MLLMs), building GUI agent systems has become an increasingly promising direction-especially for mobile platforms, given their rich app ecosystems and intuitive touch interactions. Yet mobile GUI agents face a critical dilemma: truly on-device models (4B or smaller) lack sufficient performance, while capable models (starting from 7B) are either too large for mobile deployment or prohibitively costly (e.g., cloud-only closed-source MLLMs). To resolve this, we propose LightAgent, a mobile agentic foundation model solution that leverages device-cloud collaboration to tap the cost-efficiency of on-device models and the high capability of cloud models, while avoiding their drawbacks. Specifically, LightAgent enhances Qwen2.5-VL-3B via two-stage SFT->GRPO training on synthetic GUI data for strong decision-making, integrates an efficient long-reasoning mechanism to utilize historical interactions under tight resources, and defaults to on-device execution-only escalating challenging subtasks to the cloud via real-time complexity assessment. Experiments on the online AndroidLab benchmark and diverse apps show LightAgent matches or nears larger models, with a significant reduction in cloud costs.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictive Coding Enhances Meta-RL To Achieve Interpretable Bayes-Optimal Belief Representation Under Partial Observability</title>
<link>https://arxiv.org/abs/2510.22039</link>
<guid>https://arxiv.org/abs/2510.22039</guid>
<content:encoded><![CDATA[
arXiv:2510.22039v1 Announce Type: new 
Abstract: Learning a compact representation of history is critical for planning and generalization in partially observable environments. While meta-reinforcement learning (RL) agents can attain near Bayes-optimal policies, they often fail to learn the compact, interpretable Bayes-optimal belief states. This representational inefficiency potentially limits the agent's adaptability and generalization capacity. Inspired by predictive coding in neuroscience--which suggests that the brain predicts sensory inputs as a neural implementation of Bayesian inference--and by auxiliary predictive objectives in deep RL, we investigate whether integrating self-supervised predictive coding modules into meta-RL can facilitate learning of Bayes-optimal representations. Through state machine simulation, we show that meta-RL with predictive modules consistently generates more interpretable representations that better approximate Bayes-optimal belief states compared to conventional meta-RL across a wide variety of tasks, even when both achieve optimal policies. In challenging tasks requiring active information seeking, only meta-RL with predictive modules successfully learns optimal representations and policies, whereas conventional meta-RL struggles with inadequate representation learning. Finally, we demonstrate that better representation learning leads to improved generalization. Our results strongly suggest the role of predictive learning as a guiding principle for effective representation learning in agents navigating partial observability.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLM-SlideEval: Evaluating VLMs on Structured Comprehension and Perturbation Sensitivity in PPT</title>
<link>https://arxiv.org/abs/2510.22045</link>
<guid>https://arxiv.org/abs/2510.22045</guid>
<content:encoded><![CDATA[
arXiv:2510.22045v1 Announce Type: new 
Abstract: Vision-language models (VLMs) are increasingly used to evaluate multimodal content, including presentation slides, yet their slide-specific understanding remains underexplored {despite their growing role as critics in agentic, model-forward pipelines}. We introduce VLM-SlideEval, an evaluation framework that probes VLMs along three axes: (1) element-level extraction from slide images aligned to ground truth; (2) robustness to controlled perturbations in geometry, style, and text; and (3) higher-level comprehension, such as recovering a deck's narrative order from shuffled slides. Using publicly available decks from Zenodo (https://huggingface.co/datasets/Forceless/Zenodo10K/viewer/default/pptx), we standardize ground-truth element metadata from PowerPoint XML and live renderings into a unified, verifiable schema. Empirically, VLMs underperform on pixel-accurate extraction and show non-trivial agreement, fidelity, and consistency under controlled perturbations, while performing better on single-slide content understanding; however, they do not reliably capture narrative structure across slides. These results highlight the limits of current VLMs for slide evaluation and motivate calibrated, critic-in-the-loop evaluators that drive iterative refinement and selection in agentic pipelines.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Efficient Domain-Specific Artificial Intelligence Models and Agents: Pathways and Paradigms</title>
<link>https://arxiv.org/abs/2510.22052</link>
<guid>https://arxiv.org/abs/2510.22052</guid>
<content:encoded><![CDATA[
arXiv:2510.22052v1 Announce Type: new 
Abstract: The field of artificial intelligence (AI) has taken a tight hold on broad aspects of society, industry, business, and governance in ways that dictate the prosperity and might of the world's economies. The AI market size is projected to grow from 189 billion USD in 2023 to 4.8 trillion USD by 2033. Currently, AI is dominated by large language models that exhibit linguistic and visual intelligence. However, training these models requires a massive amount of data scraped from the web as well as large amounts of energy (50--60 GWh to train GPT-4). Despite these costs, these models often hallucinate, a characteristic that prevents them from being deployed in critical application domains. In contrast, the human brain consumes only 20~W of power. What is needed is the next level of AI evolution in which lightweight domain-specific multimodal models with higher levels of intelligence can reason, plan, and make decisions in dynamic environments with real-time data and prior knowledge, while learning continuously and evolving in ways that enhance future decision-making capability. This will define the next wave of AI, progressing from today's large models, trained with vast amounts of data, to nimble energy-efficient domain-specific agents that can reason and think in a world full of uncertainty. To support such agents, hardware will need to be reimagined to allow energy efficiencies greater than 1000x over the state of the art. Such a vision of future AI systems is developed in this work.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Reinforcement Learning for Real-World Code Repair</title>
<link>https://arxiv.org/abs/2510.22075</link>
<guid>https://arxiv.org/abs/2510.22075</guid>
<content:encoded><![CDATA[
arXiv:2510.22075v1 Announce Type: new 
Abstract: We tackle the challenge of training reliable code-fixing agents in real repositories, where complex builds and shifting dependencies make evaluation unstable. We developed a verifiable pipeline with success defined as post-fix build validation and improved reproducibility across ~1K real issues by pinning dependencies and disabling automatic upgrades. Building on this, we introduced a scalable simplified pipeline for large-scale reinforcement learning (RL). Using this setup, we supervised fine-tuned Qwen3-32B in the full pipeline and applied RL on top of the SFT model in the simplified environment. The SFT model distilled from GPT-4.1 trajectories performs on par while being 56x smaller, and RL added 7-20% absolute gains under matched train-test conditions. "Thinking mode" was on par or worse in our experiments. Both SFT and RL models failed to generalize across environments, highlighting the importance of matching train-test environments for building reliable real-world code-fixing agents.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compositional Bias Control in Large Language Models: Preference Learning Fails, Supervision Succeeds</title>
<link>https://arxiv.org/abs/2510.22084</link>
<guid>https://arxiv.org/abs/2510.22084</guid>
<content:encoded><![CDATA[
arXiv:2510.22084v1 Announce Type: new 
Abstract: Large Language Models (LLMs) still produce gender-stereotyped language even in occupation-neutral contexts that reflect deep societal biases (Rudinger et al., 2018). To address this, prior work has proposed prompting, constrained decoding (Dathathri et al., 2020; Zhou et al., 2024), post-processing, and fine-tuning-based alignment (Rafailov et al., 2023; Ravfogel et al., 2022). However, the comparative efficacy and learning dynamics remain little understood. We report a comparative analysis of six control techniques for bias mitigation: prompt-only, generate-and-filter, DFA-based Ctrl-G decoding, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Iterative Nullspace Projection (INLP). We evaluate each method on a compositional constraint task. This task requires generating sentences that contain at least one agentic and one communal descriptor for each of the twenty Winogender-derived occupations. We quantify trade-offs between control strength and naturalness with evaluations of constraint compliance, lexical diversity, and fluency. Our results reveal key contrasts among the methods: SFT achieves 99.87 +- 0.15% compliance and high lexical diversity, while DPO, despite similar training stability, fails at 4.53 +- 0.82%. Ctrl-G guarantees perfect compliance, but at the cost of severely reduced fluency and diversity. Preference-based learning fundamentally differs: it cannot satisfy compositional constraints, as binary preference signals encode ranking, not logical conjunctions. Only explicit positive supervision enables mitigation of compositional biases; preference-based alignment fails to generalize logical structures, underscoring the limitations of preference learning and the necessity of explicit supervision for fair and fluent controlled generation.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embracing Trustworthy Brain-Agent Collaboration as Paradigm Extension for Intelligent Assistive Technologies</title>
<link>https://arxiv.org/abs/2510.22095</link>
<guid>https://arxiv.org/abs/2510.22095</guid>
<content:encoded><![CDATA[
arXiv:2510.22095v1 Announce Type: new 
Abstract: Brain-Computer Interfaces (BCIs) offer a direct communication pathway between the human brain and external devices, holding significant promise for individuals with severe neurological impairments. However, their widespread adoption is hindered by critical limitations, such as low information transfer rates and extensive user-specific calibration. To overcome these challenges, recent research has explored the integration of Large Language Models (LLMs), extending the focus from simple command decoding to understanding complex cognitive states. Despite these advancements, deploying agentic AI faces technical hurdles and ethical concerns. Due to the lack of comprehensive discussion on this emerging direction, this position paper argues that the field is poised for a paradigm extension from BCI to Brain-Agent Collaboration (BAC). We emphasize reframing agents as active and collaborative partners for intelligent assistance rather than passive brain signal data processors, demanding a focus on ethical data handling, model reliability, and a robust human-agent collaboration framework to ensure these systems are safe, trustworthy, and effective.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Reality: Designing Personal Experiences and Interactive Narratives in AR Theater</title>
<link>https://arxiv.org/abs/2510.22098</link>
<guid>https://arxiv.org/abs/2510.22098</guid>
<content:encoded><![CDATA[
arXiv:2510.22098v1 Announce Type: new 
Abstract: Augmented Reality (AR) technologies are redefining how we perceive and interact with the world by seamlessly integrating digital elements into our physical surroundings. These technologies offer personalized experiences and transform familiar spaces by layering new narratives onto the real world.
  Through increased levels of perceived agency and immersive environments, my work aims to merge the human elements of live theater with the dynamic potential of virtual entities and AI agents. This approach captures the subtlety and magic of storytelling, making theater experiences available anytime and anywhere. The system I am building introduces innovative methods for theatrical production in virtual settings, informed by my research and eight published works. These contributions highlight domain-specific insights that have shaped the design of an immersive AR Theater system.
  My research in building a well-designed AR stage features avatars and interactive elements that allow users to engage with stories at their own pace, granting them full agency over their experience. However, to ensure a smooth and curated experience that aligns with the director or creator's vision, several factors must be considered, especially in open-world settings that depend on natural user movement. This requires the story to be conveyed in a controlled manner, while the interaction remains intuitive and natural for the user.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Streaming Generation for Music Accompaniment</title>
<link>https://arxiv.org/abs/2510.22105</link>
<guid>https://arxiv.org/abs/2510.22105</guid>
<content:encoded><![CDATA[
arXiv:2510.22105v1 Announce Type: new 
Abstract: Music generation models can produce high-fidelity coherent accompaniment given complete audio input, but are limited to editing and loop-based workflows. We study real-time audio-to-audio accompaniment: as a model hears an input audio stream (e.g., a singer singing), it has to also simultaneously generate in real-time a coherent accompanying stream (e.g., a guitar accompaniment). In this work, we propose a model design considering inevitable system delays in practical deployment with two design variables: future visibility $t_f$, the offset between the output playback time and the latest input time used for conditioning, and output chunk duration $k$, the number of frames emitted per call. We train Transformer decoders across a grid of $(t_f,k)$ and show two consistent trade-offs: increasing effective $t_f$ improves coherence by reducing the recency gap, but requires faster inference to stay within the latency budget; increasing $k$ improves throughput but results in degraded accompaniment due to a reduced update rate. Finally, we observe that naive maximum-likelihood streaming training is insufficient for coherent accompaniment where future context is not available, motivating advanced anticipatory and agentic objectives for live jamming.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STAR-RIS-assisted Collaborative Beamforming for Low-altitude Wireless Networks</title>
<link>https://arxiv.org/abs/2510.22108</link>
<guid>https://arxiv.org/abs/2510.22108</guid>
<content:encoded><![CDATA[
arXiv:2510.22108v1 Announce Type: new 
Abstract: While low-altitude wireless networks (LAWNs) based on uncrewed aerial vehicles (UAVs) offer high mobility, flexibility, and coverage for urban communications, they face severe signal attenuation in dense environments due to obstructions. To address this critical issue, we consider introducing collaborative beamforming (CB) of UAVs and omnidirectional reconfigurable beamforming (ORB) of simultaneous transmitting and reflecting reconfigurable intelligent surfaces (STAR-RIS) to enhance the signal quality and directionality. On this basis, we formulate a joint rate and energy optimization problem (JREOP) to maximize the transmission rate of the overall system, while minimizing the energy consumption of the UAV swarm. Due to the non-convex and NP-hard nature of JREOP, we propose a heterogeneous multi-agent collaborative dynamic (HMCD) optimization framework, which has two core components. The first component is a simulated annealing (SA)-based STAR-RIS control method, which dynamically optimizes reflection and transmission coefficients to enhance signal propagation. The second component is an improved multi-agent deep reinforcement learning (MADRL) control method, which incorporates a self-attention evaluation mechanism to capture interactions between UAVs and an adaptive velocity transition mechanism to enhance training stability. Simulation results demonstrate that HMCD outperforms various baselines in terms of convergence speed, average transmission rate, and energy consumption. Further analysis reveals that the average transmission rate of the overall system scales positively with both UAV count and STAR-RIS element numbers.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When UAV Swarm Meets IRS: Collaborative Secure Communications in Low-altitude Wireless Networks</title>
<link>https://arxiv.org/abs/2510.22117</link>
<guid>https://arxiv.org/abs/2510.22117</guid>
<content:encoded><![CDATA[
arXiv:2510.22117v1 Announce Type: new 
Abstract: Low-altitude wireless networks (LAWNs) represent a promising architecture that integrates unmanned aerial vehicles (UAVs) as aerial nodes to provide enhanced coverage, reliability, and throughput for diverse applications. However, these networks face significant security vulnerabilities from both known and potential unknown eavesdroppers, which may threaten data confidentiality and system integrity. To solve this critical issue, we propose a novel secure communication framework for LAWNs where the selected UAVs within a swarm function as a virtual antenna array (VAA), complemented by intelligent reflecting surface (IRS) to create a robust defense against eavesdropping attacks. Specifically, we formulate a multi-objective optimization problem that simultaneously maximizes the secrecy rate while minimizing the maximum sidelobe level and total energy consumption, requiring joint optimization of UAV excitation current weights, flight trajectories, and IRS phase shifts. This problem presents significant difficulties due to the dynamic nature of the system and heterogeneous components. Thus, we first transform the problem into a heterogeneous Markov decision process (MDP). Then, we propose a heterogeneous multi-agent control approach (HMCA) that integrates a dedicated IRS control policy with a multi-agent soft actor-critic framework for UAV control, which enables coordinated operation across heterogeneous network elements. Simulation results show that the proposed HMCA achieves superior performance compared to baseline approaches in terms of secrecy rate improvement, sidelobe suppression, and energy efficiency. Furthermore, we find that the collaborative and passive beamforming synergy between VAA and IRS creates robust security guarantees when the number of UAVs increases.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Continuous Mean Field Games: Deep Reinforcement Learning for Non-Stationary Dynamics</title>
<link>https://arxiv.org/abs/2510.22158</link>
<guid>https://arxiv.org/abs/2510.22158</guid>
<content:encoded><![CDATA[
arXiv:2510.22158v1 Announce Type: new 
Abstract: Mean field games (MFGs) have emerged as a powerful framework for modeling interactions in large-scale multi-agent systems. Despite recent advancements in reinforcement learning (RL) for MFGs, existing methods are typically limited to finite spaces or stationary models, hindering their applicability to real-world problems. This paper introduces a novel deep reinforcement learning (DRL) algorithm specifically designed for non-stationary continuous MFGs. The proposed approach builds upon a Fictitious Play (FP) methodology, leveraging DRL for best-response computation and supervised learning for average policy representation. Furthermore, it learns a representation of the time-dependent population distribution using a Conditional Normalizing Flow. To validate the effectiveness of our method, we evaluate it on three different examples of increasing complexity. By addressing critical limitations in scalability and density approximation, this work represents a significant advancement in applying DRL techniques to complex MFG problems, bringing the field closer to real-world multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CreditXAI: A Multi-Agent System for Explainable Corporate Credit Rating</title>
<link>https://arxiv.org/abs/2510.22222</link>
<guid>https://arxiv.org/abs/2510.22222</guid>
<content:encoded><![CDATA[
arXiv:2510.22222v1 Announce Type: new 
Abstract: In the domain of corporate credit rating, traditional deep learning methods have improved predictive accuracy but still suffer from the inherent 'black-box' problem and limited interpretability. While incorporating non-financial information enriches the data and provides partial interpretability, the models still lack hierarchical reasoning mechanisms, limiting their comprehensive analytical capabilities. To address these challenges, we propose CreditXAI, a Multi-Agent System (MAS) framework that simulates the collaborative decision-making process of professional credit analysts. The framework focuses on business, financial, and governance risk dimensions to generate consistent and interpretable credit assessments. Experimental results demonstrate that multi-agent collaboration improves predictive accuracy by more than 7% over the best single-agent baseline, confirming its significant synergistic advantage in corporate credit risk evaluation. This study provides a new technical pathway to build intelligent and interpretable credit rating models.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CGoT: A Novel Inference Mechanism for Embodied Multi-Agent Systems Using Composable Graphs of Thoughts</title>
<link>https://arxiv.org/abs/2510.22235</link>
<guid>https://arxiv.org/abs/2510.22235</guid>
<content:encoded><![CDATA[
arXiv:2510.22235v1 Announce Type: new 
Abstract: The integration of self-driving cars and service robots is becoming increasingly prevalent across a wide array of fields, playing a crucial and expanding role in both industrial applications and everyday life. In parallel, the rapid advancements in Large Language Models (LLMs) have garnered substantial attention and interest within the research community. This paper introduces a novel vehicle-robot system that leverages the strengths of both autonomous vehicles and service robots. In our proposed system, two autonomous ego-vehicles transports service robots to locations within an office park, where they perform a series of tasks. The study explores the feasibility and potential benefits of incorporating LLMs into this system, with the aim of enhancing operational efficiency and maximizing the potential of the cooperative mechanisms between the vehicles and the robots. This paper proposes a novel inference mechanism which is called CGOT toward this type of system where an agent can carry another agent. Experimental results are presented to validate the performance of the proposed method.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GSAlign: Geometric and Semantic Alignment Network for Aerial-Ground Person Re-Identification</title>
<link>https://arxiv.org/abs/2510.22268</link>
<guid>https://arxiv.org/abs/2510.22268</guid>
<content:encoded><![CDATA[
arXiv:2510.22268v1 Announce Type: new 
Abstract: Aerial-Ground person re-identification (AG-ReID) is an emerging yet challenging task that aims to match pedestrian images captured from drastically different viewpoints, typically from unmanned aerial vehicles (UAVs) and ground-based surveillance cameras. The task poses significant challenges due to extreme viewpoint discrepancies, occlusions, and domain gaps between aerial and ground imagery. While prior works have made progress by learning cross-view representations, they remain limited in handling severe pose variations and spatial misalignment. To address these issues, we propose a Geometric and Semantic Alignment Network (GSAlign) tailored for AG-ReID. GSAlign introduces two key components to jointly tackle geometric distortion and semantic misalignment in aerial-ground matching: a Learnable Thin Plate Spline (LTPS) Module and a Dynamic Alignment Module (DAM). The LTPS module adaptively warps pedestrian features based on a set of learned keypoints, effectively compensating for geometric variations caused by extreme viewpoint changes. In parallel, the DAM estimates visibility-aware representation masks that highlight visible body regions at the semantic level, thereby alleviating the negative impact of occlusions and partial observations in cross-view correspondence. A comprehensive evaluation on CARGO with four matching protocols demonstrates the effectiveness of GSAlign, achieving significant improvements of +18.8\% in mAP and +16.8\% in Rank-1 accuracy over previous state-of-the-art methods on the aerial-ground setting. The code is available at: \textcolor{magenta}{https://github.com/stone96123/GSAlign}.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Agents are Powerful: Black Hole Search in Time-Varying Graphs</title>
<link>https://arxiv.org/abs/2510.22309</link>
<guid>https://arxiv.org/abs/2510.22309</guid>
<content:encoded><![CDATA[
arXiv:2510.22309v1 Announce Type: new 
Abstract: A black hole is a harmful node in a graph that destroys any resource entering it, making its identification a critical task. In the \emph{Black Hole Search (BHS)} problem, a team of agents operates on a graph $G$ with the objective that at least one agent must survive and correctly identify an edge incident to the black hole. Prior work has addressed BHS in arbitrary dynamic graphs under the restrictive \emph{face-to-face} communication, where agents can exchange information only when co-located. This constraint significantly increases the number of agents required to solve the problem. In this work, we strengthen the capabilities of agents in two ways: (i) granting them \emph{global communication}, enabling interaction regardless of location, and (ii) equipping them with \emph{1-hop visibility}, allowing each agent to observe its immediate neighborhood. These enhancements lead to more efficient solutions for the BHS problem in dynamic graphs.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IFS: Information Flow Structure for Multi-agent Ad Hoc System</title>
<link>https://arxiv.org/abs/2510.22320</link>
<guid>https://arxiv.org/abs/2510.22320</guid>
<content:encoded><![CDATA[
arXiv:2510.22320v1 Announce Type: new 
Abstract: Multi-agent ad hoc systems are dynamic collaborative systems in which multiple autonomous agents must cooperate with both known and unknown teammates in open environments, without relying on pre-coordinated strategies. These systems operate under conditions of uncertainty and partial observability, where team composition, agent behaviors, and environmental factors may change during execution. Through an analysis of information flow in such systems, we identify two key limitations in existing research: insufficient information flow and limited information processing capacity. To address these issues, we propose an information flow structure for multi-agent ad hoc systems (IFS), which tackles these challenges from the perspectives of communication and information fusion. Experimental results in StarCraft II demonstrate that IFS significantly improves both information flow and processing capacity, while exhibiting strong generalization capabilities and outperforming baseline methods in complex ad hoc teamwork scenarios.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2510.22344</link>
<guid>https://arxiv.org/abs/2510.22344</guid>
<content:encoded><![CDATA[
arXiv:2510.22344v1 Announce Type: new 
Abstract: While Retrieval-Augmented Generation (RAG) mitigates hallucination and knowledge staleness in Large Language Models (LLMs), existing frameworks often falter on complex, multi-hop queries that require synthesizing information from disparate sources. Current advanced RAG methods, employing iterative or adaptive strategies, lack a robust mechanism to systematically identify and fill evidence gaps, often propagating noise or failing to gather a comprehensive context. We introduce FAIR-RAG, a novel agentic framework that transforms the standard RAG pipeline into a dynamic, evidence-driven reasoning process. At its core is an Iterative Refinement Cycle governed by a module we term Structured Evidence Assessment (SEA). The SEA acts as an analytical gating mechanism: it deconstructs the initial query into a checklist of required findings and audits the aggregated evidence to identify confirmed facts and, critically, explicit informational gaps. These gaps provide a precise signal to an Adaptive Query Refinement agent, which generates new, targeted sub-queries to retrieve missing information. This cycle repeats until the evidence is verified as sufficient, ensuring a comprehensive context for a final, strictly faithful generation. We conducted experiments on challenging multi-hop QA benchmarks, including HotpotQA, 2WikiMultiHopQA, and MusiQue. In a unified experimental setup, FAIR-RAG significantly outperforms strong baselines. On HotpotQA, it achieves an F1-score of 0.453 -- an absolute improvement of 8.3 points over the strongest iterative baseline -- establishing a new state-of-the-art for this class of methods on these benchmarks. Our work demonstrates that a structured, evidence-driven refinement process with explicit gap analysis is crucial for unlocking reliable and accurate reasoning in advanced RAG systems for complex, knowledge-intensive tasks.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BLIP-FusePPO: A Vision-Language Deep Reinforcement Learning Framework for Lane Keeping in Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2510.22370</link>
<guid>https://arxiv.org/abs/2510.22370</guid>
<content:encoded><![CDATA[
arXiv:2510.22370v1 Announce Type: new 
Abstract: In this paper, we propose Bootstrapped Language-Image Pretraining-driven Fused State Representation in Proximal Policy Optimization (BLIP-FusePPO), a novel multimodal reinforcement learning (RL) framework for autonomous lane-keeping (LK), in which semantic embeddings generated by a vision-language model (VLM) are directly fused with geometric states, LiDAR observations, and Proportional-Integral-Derivative-based (PID) control feedback within the agent observation space. The proposed method lets the agent learn driving rules that are aware of their surroundings and easy to understand by combining high-level scene understanding from the VLM with low-level control and spatial signals. Our architecture brings together semantic, geometric, and control-aware representations to make policy learning more robust. A hybrid reward function that includes semantic alignment, LK accuracy, obstacle avoidance, and speed regulation helps learning to be more efficient and generalizable. Our method is different from the approaches that only use semantic models to shape rewards. Instead, it directly embeds semantic features into the state representation. This cuts down on expensive runtime inference and makes sure that semantic guidance is always available. The simulation results show that the proposed model is better at LK stability and adaptability than the best vision-based and multimodal RL baselines in a wide range of difficult driving situations. We make our code publicly available.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PortGPT: Towards Automated Backporting Using Large Language Models</title>
<link>https://arxiv.org/abs/2510.22396</link>
<guid>https://arxiv.org/abs/2510.22396</guid>
<content:encoded><![CDATA[
arXiv:2510.22396v1 Announce Type: new 
Abstract: Patch backporting, the process of migrating mainline security patches to older branches, is an essential task in maintaining popular open-source projects (e.g., Linux kernel). However, manual backporting can be labor-intensive, while existing automated methods, which heavily rely on predefined syntax or semantic rules, often lack agility for complex patches.
  In this paper, we introduce PORTGPT, an LLM-agent for end-to-end automation of patch backporting in real-world scenarios. PORTGPT enhances an LLM with tools to access code on-demand, summarize Git history, and revise patches autonomously based on feedback (e.g., from compilers), hence, simulating human-like reasoning and verification. PORTGPT achieved an 89.15% success rate on existing datasets (1815 cases), and 62.33% on our own dataset of 146 complex cases, both outperforms state-of-the-art of backporting tools. We contributed 9 backported patches from PORTGPT to the Linux kernel community and all patches are now merged.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group size effects and collective misalignment in LLM multi-agent systems</title>
<link>https://arxiv.org/abs/2510.22422</link>
<guid>https://arxiv.org/abs/2510.22422</guid>
<content:encoded><![CDATA[
arXiv:2510.22422v1 Announce Type: new 
Abstract: Multi-agent systems of large language models (LLMs) are rapidly expanding across domains, introducing dynamics not captured by single-agent evaluations. Yet, existing work has mostly contrasted the behavior of a single agent with that of a collective of fixed size, leaving open a central question: how does group size shape dynamics? Here, we move beyond this dichotomy and systematically explore outcomes across the full range of group sizes. We focus on multi-agent misalignment, building on recent evidence that interacting LLMs playing a simple coordination game can generate collective biases absent in individual models. First, we show that collective bias is a deeper phenomenon than previously assessed: interaction can amplify individual biases, introduce new ones, or override model-level preferences. Second, we demonstrate that group size affects the dynamics in a non-linear way, revealing model-dependent dynamical regimes. Finally, we develop a mean-field analytical approach and show that, above a critical population size, simulations converge to deterministic predictions that expose the basins of attraction of competing equilibria. These findings establish group size as a key driver of multi-agent dynamics and highlight the need to consider population-level effects when deploying LLM-based systems at scale.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hollywood Town: Long-Video Generation via Cross-Modal Multi-Agent Orchestration</title>
<link>https://arxiv.org/abs/2510.22431</link>
<guid>https://arxiv.org/abs/2510.22431</guid>
<content:encoded><![CDATA[
arXiv:2510.22431v1 Announce Type: new 
Abstract: Recent advancements in multi-agent systems have demonstrated significant potential for enhancing creative task performance, such as long video generation. This study introduces three innovations to improve multi-agent collaboration. First, we propose OmniAgent, a hierarchical, graph-based multi-agent framework for long video generation that leverages a film-production-inspired architecture to enable modular specialization and scalable inter-agent collaboration. Second, inspired by context engineering, we propose hypergraph nodes that enable temporary group discussions among agents lacking sufficient context, reducing individual memory requirements while ensuring adequate contextual information. Third, we transition from directed acyclic graphs (DAGs) to directed cyclic graphs with limited retries, allowing agents to reflect and refine outputs iteratively, thereby improving earlier stages through feedback from subsequent nodes. These contributions lay the groundwork for developing more robust multi-agent systems in creative tasks.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Egocentric Multimodal Goal Inference for Assistive Wearable Agents</title>
<link>https://arxiv.org/abs/2510.22443</link>
<guid>https://arxiv.org/abs/2510.22443</guid>
<content:encoded><![CDATA[
arXiv:2510.22443v1 Announce Type: new 
Abstract: There has been a surge of interest in assistive wearable agents: agents embodied in wearable form factors (e.g., smart glasses) who take assistive actions toward a user's goal/query (e.g. "Where did I leave my keys?"). In this work, we consider the important complementary problem of inferring that goal from multi-modal contextual observations. Solving this "goal inference" problem holds the promise of eliminating the effort needed to interact with such an agent. This work focuses on creating WAGIBench, a strong benchmark to measure progress in solving this problem using vision-language models (VLMs). Given the limited prior work in this area, we collected a novel dataset comprising 29 hours of multimodal data from 348 participants across 3,477 recordings, featuring ground-truth goals alongside accompanying visual, audio, digital, and longitudinal contextual observations. We validate that human performance exceeds model performance, achieving 93% multiple-choice accuracy compared with 84% for the best-performing VLM. Generative benchmark results that evaluate several families of modern vision-language models show that larger models perform significantly better on the task, yet remain far from practical usefulness, as they produce relevant goals only 55% of the time. Through a modality ablation, we show that models benefit from extra information in relevant modalities with minimal performance degradation from irrelevant modalities.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning "Partner-Aware" Collaborators in Multi-Party Collaboration</title>
<link>https://arxiv.org/abs/2510.22462</link>
<guid>https://arxiv.org/abs/2510.22462</guid>
<content:encoded><![CDATA[
arXiv:2510.22462v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly bring deployed in agentic settings where they act as collaborators with humans. Therefore, it is increasingly important to be able to evaluate their abilities to collaborate effectively in multi-turn, multi-party tasks. In this paper, we build on the AI alignment and safe interruptability literature to offer novel theoretical insights on collaborative behavior between LLM-driven collaborator agents and an intervention agent. Our goal is to learn an ideal partner-aware collaborator that increases the group's common-ground (CG)-alignment on task-relevant propositions-by intelligently collecting information provided in interventions by a partner agent.We show how LLM agents trained using standard RLHF and related approaches are naturally inclined to ignore possibly well-meaning interventions, which makes increasing group common ground non-trivial in this setting. We employ a two-player Modified-Action MDP to examine this suboptimal behavior of standard AI agents, and propose Interruptible Collaborative Roleplayer (ICR)-a novel partner-aware learning algorithm to train CG-optimal collaborators. Experiments on multiple collaborative task environments show that ICR, on average, is more capable of promoting successful CG convergence and exploring more diverse solutions in such tasks.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Local Stackelberg Equilibria from Repeated Interactions with a Learning Agent</title>
<link>https://arxiv.org/abs/2510.22471</link>
<guid>https://arxiv.org/abs/2510.22471</guid>
<content:encoded><![CDATA[
arXiv:2510.22471v1 Announce Type: new 
Abstract: Motivated by the question of how a principal can maximize its utility in repeated interactions with a learning agent, we study repeated games between an principal and an agent employing a mean-based learning algorithm. Prior work has shown that computing or even approximating the global Stackelberg value in similar settings can require an exponential number of rounds in the size of the agent's action space, making it computationally intractable. In contrast, we shift focus to the computation of local Stackelberg equilibria and introduce an algorithm that, within the smoothed analysis framework, constitutes a Polynomial Time Approximation Scheme (PTAS) for finding an epsilon-approximate local Stackelberg equilibrium. Notably, the algorithm's runtime is polynomial in the size of the agent's action space yet exponential in (1/epsilon) - a dependency we prove to be unavoidable.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-GSPO: Communication-Efficient Multi-Agent Systems via Group Sequence Policy Optimization</title>
<link>https://arxiv.org/abs/2510.22477</link>
<guid>https://arxiv.org/abs/2510.22477</guid>
<content:encoded><![CDATA[
arXiv:2510.22477v1 Announce Type: new 
Abstract: To combat the prohibitive communication costs of ``free-for-all" multi-agent systems (MAS), we introduce \textbf{Agent-GSPO}, a framework that directly optimizes for token economy using sequence-level reinforcement learning. Agent-GSPO leverages the stable and memory-efficient Group Sequence Policy Optimization (GSPO) algorithm to train agents on a communication-aware reward that explicitly penalizes verbosity. Across seven reasoning benchmarks, Agent-GSPO not only achieves new state-of-the-art performance but does so with a fraction of the token consumption of existing methods. By fostering emergent strategies like ``strategic silence," our approach provides a practical blueprint for developing scalable and economically viable multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Oversight via Partitioned Human Supervision</title>
<link>https://arxiv.org/abs/2510.22500</link>
<guid>https://arxiv.org/abs/2510.22500</guid>
<content:encoded><![CDATA[
arXiv:2510.22500v1 Announce Type: new 
Abstract: As artificial intelligence (AI) systems approach and surpass expert human performance across a broad range of tasks, obtaining high-quality human supervision for evaluation and training becomes increasingly challenging. Our focus is on tasks that require deep knowledge and skills of multiple domains. Unfortunately, even the best human experts are knowledgeable only in a single narrow area, and will not be able to evaluate the correctness of advanced AI systems on such superhuman tasks. However, based on their narrow expertise, humans may provide a weak signal, i.e., a complementary label indicating an option that is incorrect. For example, a cardiologist could state that "this is not related to cardiology,'' even if they cannot identify the true disease. Based on this weak signal, we propose a scalable oversight framework that enables us to evaluate frontier AI systems without the need to prepare the ground truth. We derive an unbiased estimator of top-1 accuracy from complementary labels and quantify how many complementary labels are needed to match the variance of ordinary labels. We further introduce two estimators to combine scarce ordinary labels with abundant complementary labels. We provide finite-sample deviation guarantees for both complementary-only and the mixed estimators. Empirically, we show that we can evaluate the output of large language models without the ground truth, if we have complementary labels. We further show that we can train an AI system with such weak signals: we show how we can design an agentic AI system automatically that can perform better with this partitioned human supervision. Our code is available at https://github.com/R-Yin-217/Scalable-Oversight-via-Human-Partitioned-Supervision.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Multi-Agent Safety via Tube-Based Tightened Exponential Barrier Functions</title>
<link>https://arxiv.org/abs/2510.22514</link>
<guid>https://arxiv.org/abs/2510.22514</guid>
<content:encoded><![CDATA[
arXiv:2510.22514v1 Announce Type: new 
Abstract: This paper presents a constructive framework for synthesizing provably safe controllers for nonlinear multi-agent systems subject to bounded disturbances. The methodology applies to systems representable in Brunovsky canonical form, accommodating arbitrary-order dynamics in multi-dimensional spaces. The central contribution is a method of constraint tightening that formally couples robust error feedback with nominal trajectory planning. The key insight is that the design of an ancillary feedback law, which confines state errors to a robust positively invariant (RPI) tube, simultaneously provides the exact information needed to ensure the safety of the nominal plan. Specifically, the geometry of the resulting RPI tube is leveraged via its support function to derive state-dependent safety margins. These margins are then used to systematically tighten the high relative-degree exponential control barrier function (eCBF) constraints imposed on the nominal planner. This integrated synthesis guarantees that any nominal trajectory satisfying the tightened constraints corresponds to a provably safe trajectory for the true, disturbed system. We demonstrate the practical utility of this formal synthesis method by implementing the planner within a distributed Model Predictive Control (MPC) scheme, which optimizes performance while inheriting the robust safety guarantees.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open Multimodal Retrieval-Augmented Factual Image Generation</title>
<link>https://arxiv.org/abs/2510.22521</link>
<guid>https://arxiv.org/abs/2510.22521</guid>
<content:encoded><![CDATA[
arXiv:2510.22521v1 Announce Type: new 
Abstract: Large Multimodal Models (LMMs) have achieved remarkable progress in generating photorealistic and prompt-aligned images, but they often produce outputs that contradict verifiable knowledge, especially when prompts involve fine-grained attributes or time-sensitive events. Conventional retrieval-augmented approaches attempt to address this issue by introducing external information, yet they are fundamentally incapable of grounding generation in accurate and evolving knowledge due to their reliance on static sources and shallow evidence integration. To bridge this gap, we introduce ORIG, an agentic open multimodal retrieval-augmented framework for Factual Image Generation (FIG), a new task that requires both visual realism and factual grounding. ORIG iteratively retrieves and filters multimodal evidence from the web and incrementally integrates the refined knowledge into enriched prompts to guide generation. To support systematic evaluation, we build FIG-Eval, a benchmark spanning ten categories across perceptual, compositional, and temporal dimensions. Experiments demonstrate that ORIG substantially improves factual consistency and overall image quality over strong baselines, highlighting the potential of open multimodal retrieval for factual image generation.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finding the Needle in the Crash Stack: Industrial-Scale Crash Root Cause Localization with AutoCrashFL</title>
<link>https://arxiv.org/abs/2510.22530</link>
<guid>https://arxiv.org/abs/2510.22530</guid>
<content:encoded><![CDATA[
arXiv:2510.22530v1 Announce Type: new 
Abstract: Fault Localization (FL) aims to identify root causes of program failures. FL typically targets failures observed from test executions, and as such, often involves dynamic analyses to improve accuracy, such as coverage profiling or mutation testing. However, for large industrial software, measuring coverage for every execution is prohibitively expensive, making the use of such techniques difficult. To address these issues and apply FL in an industrial setting, this paper proposes AutoCrashFL, an LLM agent for the localization of crashes that only requires the crashdump from the Program Under Test (PUT) and access to the repository of the corresponding source code. We evaluate AutoCrashFL against real-world crashes of SAP HANA, an industrial software project consisting of more than 35 million lines of code. Experiments reveal that AutoCrashFL is more effective in localization, as it identified 30% crashes at the top, compared to 17% achieved by the baseline. Through thorough analysis, we find that AutoCrashFL has attractive practical properties: it is relatively more effective for complex bugs, and it can indicate confidence in its results. Overall, these results show the practicality of LLM agent deployment on an industrial scale.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Closed-Loop Personalized Learning Agent Integrating Neural Cognitive Diagnosis, Bounded-Ability Adaptive Testing, and LLM-Driven Feedback</title>
<link>https://arxiv.org/abs/2510.22559</link>
<guid>https://arxiv.org/abs/2510.22559</guid>
<content:encoded><![CDATA[
arXiv:2510.22559v1 Announce Type: new 
Abstract: As information technology advances, education is moving from one-size-fits-all instruction toward personalized learning. However, most methods handle modeling, item selection, and feedback in isolation rather than as a closed loop. This leads to coarse or opaque student models, assumption-bound adaptivity that ignores diagnostic posteriors, and generic, non-actionable feedback. To address these limitations, this paper presents an end-to-end personalized learning agent, EduLoop-Agent, which integrates a Neural Cognitive Diagnosis model (NCD), a Bounded-Ability Estimation Computerized Adaptive Testing strategy (BECAT), and large language models (LLMs). The NCD module provides fine-grained estimates of students' mastery at the knowledge-point level; BECAT dynamically selects subsequent items to maximize relevance and learning efficiency; and LLMs convert diagnostic signals into structured, actionable feedback. Together, these components form a closed-loop framework of ``Diagnosis--Recommendation--Feedback.'' Experiments on the ASSISTments dataset show that the NCD module achieves strong performance on response prediction while yielding interpretable mastery assessments. The adaptive recommendation strategy improves item relevance and personalization, and the LLM-based feedback offers targeted study guidance aligned with identified weaknesses. Overall, the results indicate that the proposed design is effective and practically deployable, providing a feasible pathway to generating individualized learning trajectories in intelligent education.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPIRAL: Self-Play Incremental Racing Algorithm for Learning in Multi-Drone Competitions</title>
<link>https://arxiv.org/abs/2510.22568</link>
<guid>https://arxiv.org/abs/2510.22568</guid>
<content:encoded><![CDATA[
arXiv:2510.22568v1 Announce Type: new 
Abstract: This paper introduces SPIRAL (Self-Play Incremental Racing Algorithm for Learning), a novel approach for training autonomous drones in multi-agent racing competitions. SPIRAL distinctively employs a self-play mechanism to incrementally cultivate complex racing behaviors within a challenging, dynamic environment. Through this self-play core, drones continuously compete against increasingly proficient versions of themselves, naturally escalating the difficulty of competitive interactions. This progressive learning journey guides agents from mastering fundamental flight control to executing sophisticated cooperative multi-drone racing strategies. Our method is designed for versatility, allowing integration with any state-of-the-art Deep Reinforcement Learning (DRL) algorithms within its self-play framework. Simulations demonstrate the significant advantages of SPIRAL and benchmark the performance of various DRL algorithms operating within it. Consequently, we contribute a versatile, scalable, and self-improving learning framework to the field of autonomous drone racing. SPIRAL's capacity to autonomously generate appropriate and escalating challenges through its self-play dynamic offers a promising direction for developing robust and adaptive racing strategies in multi-agent environments. This research opens new avenues for enhancing the performance and reliability of autonomous racing drones in increasingly complex and competitive scenarios.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curriculum-Based Iterative Self-Play for Scalable Multi-Drone Racing</title>
<link>https://arxiv.org/abs/2510.22570</link>
<guid>https://arxiv.org/abs/2510.22570</guid>
<content:encoded><![CDATA[
arXiv:2510.22570v1 Announce Type: new 
Abstract: The coordination of multiple autonomous agents in high-speed, competitive environments represents a significant engineering challenge. This paper presents CRUISE (Curriculum-Based Iterative Self-Play for Scalable Multi-Drone Racing), a reinforcement learning framework designed to solve this challenge in the demanding domain of multi-drone racing. CRUISE overcomes key scalability limitations by synergistically combining a progressive difficulty curriculum with an efficient self-play mechanism to foster robust competitive behaviors. Validated in high-fidelity simulation with realistic quadrotor dynamics, the resulting policies significantly outperform both a standard reinforcement learning baseline and a state-of-the-art game-theoretic planner. CRUISE achieves nearly double the planner's mean racing speed, maintains high success rates, and demonstrates robust scalability as agent density increases. Ablation studies confirm that the curriculum structure is the critical component for this performance leap. By providing a scalable and effective training methodology, CRUISE advances the development of autonomous systems for dynamic, competitive tasks and serves as a blueprint for future real-world deployment.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personal Care Utility (PCU): Building the Health Infrastructure for Everyday Insight and Guidance</title>
<link>https://arxiv.org/abs/2510.22602</link>
<guid>https://arxiv.org/abs/2510.22602</guid>
<content:encoded><![CDATA[
arXiv:2510.22602v1 Announce Type: new 
Abstract: Building on decades of success in digital infrastructure and biomedical innovation, we propose the Personal Care Utility (PCU) - a cybernetic system for lifelong health guidance. PCU is conceived as a global, AI-powered utility that continuously orchestrates multimodal data, knowledge, and services to assist individuals and populations alike. Drawing on multimodal agents, event-centric modeling, and contextual inference, it offers three essential capabilities: (1) trusted health information tailored to the individual, (2) proactive health navigation and behavior guidance, and (3) ongoing interpretation of recovery and treatment response after medical events. Unlike conventional episodic care, PCU functions as an ambient, adaptive companion - observing, interpreting, and guiding health in real time across daily life. By integrating personal sensing, experiential computing, and population-level analytics, PCU promises not only improved outcomes for individuals but also a new substrate for public health and scientific discovery. We describe the architecture, design principles, and implementation challenges of this emerging paradigm.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Everything counts: the managed omnirelevance of speech in 'human - voice agent' interaction</title>
<link>https://arxiv.org/abs/2510.22610</link>
<guid>https://arxiv.org/abs/2510.22610</guid>
<content:encoded><![CDATA[
arXiv:2510.22610v1 Announce Type: new 
Abstract: To this day, turn-taking models determining voice agents' conduct have been examined from a technical point of view, while the interactional constraints or resources they constitute for human conversationalists have not been empirically described. From the detailed analysis of corpora of naturalistic data, we document how, whether in interaction with rule-based robots from a 'pre-LLM era' or with the most recent voice agents, humans' conduct was produced in reference to the ever-present risk that, each time they spoke, their talk may trigger a new uncalled-for contribution from the artificial agent. We argue that this 'omnirelevance of human speech' is a constitutive feature of current human-agent interaction that, due to recent improvements in voice capture technology, weighs on human practices even more today than in the past. Specifically, we document how, in multiparty settings, humans shaped their conduct in such a way as to remain undetected by the machine's sensors.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI Agents</title>
<link>https://arxiv.org/abs/2510.22620</link>
<guid>https://arxiv.org/abs/2510.22620</guid>
<content:encoded><![CDATA[
arXiv:2510.22620v1 Announce Type: new 
Abstract: AI agents powered by large language models (LLMs) are being deployed at scale, yet we lack a systematic understanding of how the choice of backbone LLM affects agent security. The non-deterministic sequential nature of AI agents complicates security modeling, while the integration of traditional software with AI components entangles novel LLM vulnerabilities with conventional security risks. Existing frameworks only partially address these challenges as they either capture specific vulnerabilities only or require modeling of complete agents. To address these limitations, we introduce threat snapshots: a framework that isolates specific states in an agent's execution flow where LLM vulnerabilities manifest, enabling the systematic identification and categorization of security risks that propagate from the LLM to the agent level. We apply this framework to construct the $\operatorname{b}^3$ benchmark, a security benchmark based on 194331 unique crowdsourced adversarial attacks. We then evaluate 31 popular LLMs with it, revealing, among other insights, that enhanced reasoning capabilities improve security, while model size does not correlate with security. We release our benchmark, dataset, and evaluation code to facilitate widespread adoption by LLM providers and practitioners, offering guidance for agent developers and incentivizing model developers to prioritize backbone security improvements.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SwiftSolve: A Self-Iterative, Complexity-Aware Multi-Agent Framework for Competitive Programming</title>
<link>https://arxiv.org/abs/2510.22626</link>
<guid>https://arxiv.org/abs/2510.22626</guid>
<content:encoded><![CDATA[
arXiv:2510.22626v1 Announce Type: new 
Abstract: Correctness alone is insufficient: LLM-generated programs frequently satisfy unit tests while violating contest time or memory budgets. We present SwiftSolve, a complexity-aware multi-agent system for competitive programming that couples algorithmic planning with empirical profiling and complexity-guided repair. We frame competitive programming as a software environment where specialized agents act as programmers, each assuming roles such as planning, coding, profiling, and complexity analysis. A Planner proposes an algorithmic sketch; a deterministic Static Pruner filters high-risk plans; a Coder emits ISO C++17; a Profiler compiles and executes candidates on a fixed input-size schedule to record wall time and peak memory; and a Complexity Analyst fits log-log growth (s, R2) with an LLM fallback to assign a complexity class and dispatch targeted patches to either the Planner or Coder. Agents communicate via typed, versioned JSON; a controller enforces iteration caps and diminishing returns stopping. Evaluated on 26 problems (16 BigO, 10 Codeforces Div. 2) in a POSIX sandbox (2 s / 256-512 MB), SwiftSolve attains pass@1 = 61.54% (16/26) on the first attempt and Solved@<=3 = 80.77% with marginal latency change (mean 11.96 s to 12.66 s per attempt). Aggregate run-level success is 73.08% at 12.40 s mean. Failures are predominantly resource-bound, indicating inefficiency rather than logic errors. Against Claude Opus 4, SwiftSolve improves run-level success (73.1% vs 52.6%) at approximately 2x runtime overhead (12.4 s vs 6.8 s). Beyond correctness (pass@k), we report efficiency metrics (eff@k for runtime and memory, incidence of TLE or MLE, and complexity fit accuracy on BigO), demonstrating that profiling and complexity-guided replanning reduce inefficiency while preserving accuracy.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Environment-aware Motion Matching</title>
<link>https://arxiv.org/abs/2510.22632</link>
<guid>https://arxiv.org/abs/2510.22632</guid>
<content:encoded><![CDATA[
arXiv:2510.22632v1 Announce Type: new 
Abstract: Interactive applications demand believable characters that respond naturally to dynamic environments. Traditional character animation techniques often struggle to handle arbitrary situations, leading to a growing trend of dynamically selecting motion-captured animations based on predefined features. While Motion Matching has proven effective for locomotion by aligning to target trajectories, animating environment interactions and crowd behaviors remains challenging due to the need to consider surrounding elements. Existing approaches often involve manual setup or lack the naturalism of motion capture. Furthermore, in crowd animation, body animation is frequently treated as a separate process from trajectory planning, leading to inconsistencies between body pose and root motion. To address these limitations, we present Environment-aware Motion Matching, a novel real-time system for full-body character animation that dynamically adapts to obstacles and other agents, emphasizing the bidirectional relationship between pose and trajectory. In a preprocessing step, we extract shape, pose, and trajectory features from a motion capture database. At runtime, we perform an efficient search that matches user input and current pose while penalizing collisions with a dynamic environment. Our method allows characters to naturally adjust their pose and trajectory to navigate crowded scenes.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UCB-type Algorithm for Budget-Constrained Expert Learning</title>
<link>https://arxiv.org/abs/2510.22654</link>
<guid>https://arxiv.org/abs/2510.22654</guid>
<content:encoded><![CDATA[
arXiv:2510.22654v1 Announce Type: new 
Abstract: In many modern applications, a system must dynamically choose between several adaptive learning algorithms that are trained online. Examples include model selection in streaming environments, switching between trading strategies in finance, and orchestrating multiple contextual bandit or reinforcement learning agents. At each round, a learner must select one predictor among $K$ adaptive experts to make a prediction, while being able to update at most $M \le K$ of them under a fixed training budget.
  We address this problem in the \emph{stochastic setting} and introduce \algname{M-LCB}, a computationally efficient UCB-style meta-algorithm that provides \emph{anytime regret guarantees}. Its confidence intervals are built directly from realized losses, require no additional optimization, and seamlessly reflect the convergence properties of the underlying experts. If each expert achieves internal regret $\tilde O(T^\alpha)$, then \algname{M-LCB} ensures overall regret bounded by $\tilde O\!\Bigl(\sqrt{\tfrac{KT}{M}} \;+\; (K/M)^{1-\alpha}\,T^\alpha\Bigr)$.
  To our knowledge, this is the first result establishing regret guarantees when multiple adaptive experts are trained simultaneously under per-round budget constraints. We illustrate the framework with two representative cases: (i) parametric models trained online with stochastic losses, and (ii) experts that are themselves multi-armed bandit algorithms. These examples highlight how \algname{M-LCB} extends the classical bandit paradigm to the more realistic scenario of coordinating stateful, self-learning experts under limited resources.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Look and Tell: A Dataset for Multimodal Grounding Across Egocentric and Exocentric Views</title>
<link>https://arxiv.org/abs/2510.22672</link>
<guid>https://arxiv.org/abs/2510.22672</guid>
<content:encoded><![CDATA[
arXiv:2510.22672v1 Announce Type: new 
Abstract: We introduce Look and Tell, a multimodal dataset for studying referential communication across egocentric and exocentric perspectives. Using Meta Project Aria smart glasses and stationary cameras, we recorded synchronized gaze, speech, and video as 25 participants instructed a partner to identify ingredients in a kitchen. Combined with 3D scene reconstructions, this setup provides a benchmark for evaluating how different spatial representations (2D vs. 3D; ego vs. exo) affect multimodal grounding. The dataset contains 3.67 hours of recordings, including 2,707 richly annotated referential expressions, and is designed to advance the development of embodied agents that can understand and engage in situated dialogue.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RL-AVIST: Reinforcement Learning for Autonomous Visual Inspection of Space Targets</title>
<link>https://arxiv.org/abs/2510.22699</link>
<guid>https://arxiv.org/abs/2510.22699</guid>
<content:encoded><![CDATA[
arXiv:2510.22699v1 Announce Type: new 
Abstract: The growing need for autonomous on-orbit services such as inspection, maintenance, and situational awareness calls for intelligent spacecraft capable of complex maneuvers around large orbital targets. Traditional control systems often fall short in adaptability, especially under model uncertainties, multi-spacecraft configurations, or dynamically evolving mission contexts. This paper introduces RL-AVIST, a Reinforcement Learning framework for Autonomous Visual Inspection of Space Targets. Leveraging the Space Robotics Bench (SRB), we simulate high-fidelity 6-DOF spacecraft dynamics and train agents using DreamerV3, a state-of-the-art model-based RL algorithm, with PPO and TD3 as model-free baselines. Our investigation focuses on 3D proximity maneuvering tasks around targets such as the Lunar Gateway and other space assets. We evaluate task performance under two complementary regimes: generalized agents trained on randomized velocity vectors, and specialized agents trained to follow fixed trajectories emulating known inspection orbits. Furthermore, we assess the robustness and generalization of policies across multiple spacecraft morphologies and mission domains. Results demonstrate that model-based RL offers promising capabilities in trajectory fidelity, and sample efficiency, paving the way for scalable, retrainable control solutions for future space operations
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ATLAS: Actor-Critic Task-Completion with Look-ahead Action Simulation</title>
<link>https://arxiv.org/abs/2510.22732</link>
<guid>https://arxiv.org/abs/2510.22732</guid>
<content:encoded><![CDATA[
arXiv:2510.22732v1 Announce Type: new 
Abstract: We observe that current state-of-the-art web-agents are unable to effectively adapt to new environments without neural network fine-tuning, without which they produce inefficient execution plans due to a lack of awareness of the structure and dynamics of the new environment. To address this limitation, we introduce ATLAS (Actor-Critic Task-completion with Look-ahead Action Simulation), a memory-augmented agent that is able to make plans grounded in a model of the environment by simulating the consequences of those actions in cognitive space. Our agent starts by building a "cognitive map" by performing a lightweight curiosity driven exploration of the environment. The planner proposes candidate actions; the simulator predicts their consequences in cognitive space; a critic analyzes the options to select the best roll-out and update the original plan; and a browser executor performs the chosen action. On the WebArena-Lite Benchmark, we achieve a 63% success rate compared to 53.9% success rate for the previously published state-of-the-art. Unlike previous systems, our modular architecture requires no website-specific LLM fine-tuning. Ablations show sizable drops without the world-model, hierarchical planner, and look-ahead-based replanner confirming their complementary roles within the design of our system
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TWC-SLAM: Multi-Agent Cooperative SLAM with Text Semantics and WiFi Features Integration for Similar Indoor Environments</title>
<link>https://arxiv.org/abs/2510.22754</link>
<guid>https://arxiv.org/abs/2510.22754</guid>
<content:encoded><![CDATA[
arXiv:2510.22754v1 Announce Type: new 
Abstract: Multi-agent cooperative SLAM often encounters challenges in similar indoor environments characterized by repetitive structures, such as corridors and rooms. These challenges can lead to significant inaccuracies in shared location identification when employing point cloud-based techniques. To mitigate these issues, we introduce TWC-SLAM, a multi-agent cooperative SLAM framework that integrates text semantics and WiFi signal features to enhance location identification and loop closure detection. TWC-SLAM comprises a single-agent front-end odometry module based on FAST-LIO2, a location identification and loop closure detection module that leverages text semantics and WiFi features, and a global mapping module. The agents are equipped with sensors capable of capturing textual information and detecting WiFi signals. By correlating these data sources, TWC-SLAM establishes a common location, facilitating point cloud alignment across different agents' maps. Furthermore, the system employs loop closure detection and optimization modules to achieve global optimization and cohesive mapping. We evaluated our approach using an indoor dataset featuring similar corridors, rooms, and text signs. The results demonstrate that TWC-SLAM significantly improves the performance of cooperative SLAM systems in complex environments with repetitive architectural features.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Supervising Software Agents with Patch Reasoner</title>
<link>https://arxiv.org/abs/2510.22775</link>
<guid>https://arxiv.org/abs/2510.22775</guid>
<content:encoded><![CDATA[
arXiv:2510.22775v1 Announce Type: new 
Abstract: While large language model agents have advanced software engineering tasks, the unscalable nature of existing test-based supervision is limiting the potential improvement of data scaling. The reason is twofold: (1) building and running test sandbox is rather heavy and fragile, and (2) data with high-coverage tests is naturally rare and threatened by test hacking via edge cases. In this paper, we propose R4P, a patch verifier model to provide scalable rewards for training and testing SWE agents via reasoning. We consider that patch verification is fundamentally a reasoning task, mirroring how human repository maintainers review patches without writing and running new reproduction tests. To obtain sufficient reference and reduce the risk of reward hacking, R4P uses a group-wise objective for RL training, enabling it to verify multiple patches against each other's modification and gain a dense reward for stable training. R4P achieves 72.2% Acc. for verifying patches from SWE-bench-verified, surpassing OpenAI o3. To demonstrate R4P's practicality, we design and train a lite scaffold, Mini-SE, with pure reinforcement learning where all rewards are derived from R4P. As a result, Mini-SE achieves 26.2% Pass@1 on SWE-bench-verified, showing a 10.0% improvement over the original Qwen3-32B. This can be further improved to 32.8% with R4P for test-time scaling. Furthermore, R4P verifies patches within a second, 50x faster than testing on average. The stable scaling curves of rewards and accuracy along with high efficiency reflect R4P's practicality.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Do AI Agents Do Human Work? Comparing AI and Human Workflows Across Diverse Occupations</title>
<link>https://arxiv.org/abs/2510.22780</link>
<guid>https://arxiv.org/abs/2510.22780</guid>
<content:encoded><![CDATA[
arXiv:2510.22780v1 Announce Type: new 
Abstract: AI agents are continually optimized for tasks related to human work, such as software engineering and professional writing, signaling a pressing trend with significant impacts on the human workforce. However, these agent developments have often not been grounded in a clear understanding of how humans execute work, to reveal what expertise agents possess and the roles they can play in diverse workflows. In this work, we study how agents do human work by presenting the first direct comparison of human and agent workers across multiple essential work-related skills: data analysis, engineering, computation, writing, and design. To better understand and compare heterogeneous computer-use activities of workers, we introduce a scalable toolkit to induce interpretable, structured workflows from either human or agent computer-use activities. Using such induced workflows, we compare how humans and agents perform the same tasks and find that: (1) While agents exhibit promise in their alignment to human workflows, they take an overwhelmingly programmatic approach across all work domains, even for open-ended, visually dependent tasks like design, creating a contrast with the UI-centric methods typically used by humans. (2) Agents produce work of inferior quality, yet often mask their deficiencies via data fabrication and misuse of advanced tools. (3) Nonetheless, agents deliver results 88.3% faster and cost 90.4-96.2% less than humans, highlighting the potential for enabling efficient collaboration by delegating easily programmable tasks to agents.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Meta-Orchestrator for Multi-task Copilots</title>
<link>https://arxiv.org/abs/2510.22781</link>
<guid>https://arxiv.org/abs/2510.22781</guid>
<content:encoded><![CDATA[
arXiv:2510.22781v1 Announce Type: new 
Abstract: Microsoft Copilot suites serve as the universal entry point for various agents skilled in handling important tasks, ranging from assisting a customer with product purchases to detecting vulnerabilities in corporate programming code. Each agent can be powered by language models, software engineering operations, such as database retrieval, and internal \& external knowledge. The repertoire of a copilot can expand dynamically with new agents. This requires a robust orchestrator that can distribute tasks from user prompts to the right agents. In this work, we propose an Agentic Meta-orchestrator (AMO) for handling multiple tasks and scalable agents in copilot services, which can provide both natural language and action responses. We will also demonstrate the planning that leverages meta-learning, i.e., a trained decision tree model for deciding the best inference strategy among various agents/models. We showcase the effectiveness of our AMO through two production use cases: Microsoft 365 (M365) E-Commerce Copilot and code compliance copilot. M365 E-Commerce Copilot advertises Microsoft products to external customers to promote sales success. The M365 E-Commerce Copilot provides up-to-date product information and connects to multiple agents, such as relational databases and human customer support. The code compliance copilot scans the internal DevOps code to detect known and new compliance issues in pull requests (PR).
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative LLM Agents for C4 Software Architecture Design Automation</title>
<link>https://arxiv.org/abs/2510.22787</link>
<guid>https://arxiv.org/abs/2510.22787</guid>
<content:encoded><![CDATA[
arXiv:2510.22787v1 Announce Type: new 
Abstract: Software architecture design is a fundamental part of creating every software system. Despite its importance, producing a C4 software architecture model, the preferred notation for such architecture, remains manual and time-consuming. We introduce an LLM-based multi-agent system that automates this task by simulating a dialogue between role-specific experts who analyze requirements and generate the Context, Container, and Component views of the C4 model. Quality is assessed with a hybrid evaluation framework: deterministic checks for structural and syntactic integrity and C4 rule consistency, plus semantic and qualitative scoring via an LLM-as-a-Judge approach. Tested on five canonical system briefs, the workflow demonstrates fast C4 model creation, sustains high compilation success, and delivers semantic fidelity. A comparison of four state-of-the-art LLMs shows different strengths relevant to architectural design. This study contributes to automated software architecture design and its evaluation methods.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Multi-Agent Bandits Over Erd\H{o}s-R\'enyi Random Networks</title>
<link>https://arxiv.org/abs/2510.22811</link>
<guid>https://arxiv.org/abs/2510.22811</guid>
<content:encoded><![CDATA[
arXiv:2510.22811v1 Announce Type: new 
Abstract: We study the distributed multi-agent multi-armed bandit problem with heterogeneous rewards over random communication graphs. Uniquely, at each time step $t$ agents communicate over a time-varying random graph $G_t$ generated by applying the Erd\H{o}s-R\'enyi model to a fixed connected base graph $G$ (for classical Erd\H{o}s-R\'enyi graphs, $G$ is a complete graph), where each potential edge in $G$ is randomly and independently present with the link probability $p$. Notably, the resulting random graph is not necessarily connected at each time step. Each agent's arm rewards follow time-invariant distributions, and the reward distribution for the same arm may differ across agents. The goal is to minimize the cumulative expected regret relative to the global mean reward of each arm, defined as the average of that arm's mean rewards across all agents. To this end, we propose a fully distributed algorithm that integrates the arm elimination strategy with the random gossip algorithm. We theoretically show that the regret upper bound is of order $\log T$ and is highly interpretable, where $T$ is the time horizon. It includes the optimal centralized regret $O\left(\sum_{k: \Delta_k>0} \frac{\log T}{\Delta_k}\right)$ and an additional term $O\left(\frac{N^2 \log T}{p \lambda_{N-1}(Lap(G))} + \frac{KN^2 \log T}{p}\right)$ where $N$ and $K$ denote the total number of agents and arms, respectively. This term reflects the impact of $G$'s algebraic connectivity $\lambda_{N-1}(Lap(G))$ and the link probability $p$, and thus highlights a fundamental trade-off between communication efficiency and regret. As a by-product, we show a nearly optimal regret lower bound. Finally, our numerical experiments not only show the superiority of our algorithm over existing benchmarks, but also validate the theoretical regret scaling with problem complexity.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analytical Swarm Chemistry: Characterization and Analysis of Emergent Swarm Behaviors</title>
<link>https://arxiv.org/abs/2510.22821</link>
<guid>https://arxiv.org/abs/2510.22821</guid>
<content:encoded><![CDATA[
arXiv:2510.22821v1 Announce Type: new 
Abstract: Swarm robotics has potential for a wide variety of applications, but real-world deployments remain rare due to the difficulty of predicting emergent behaviors arising from simple local interactions. Traditional engineering approaches design controllers to achieve desired macroscopic outcomes under idealized conditions, while agent-based and artificial life studies explore emergent phenomena in a bottom-up, exploratory manner. In this work, we introduce Analytical Swarm Chemistry, a framework that integrates concepts from engineering, agent-based and artificial life research, and chemistry. This framework combines macrostate definitions with phase diagram analysis to systematically explore how swarm parameters influence emergent behavior. Inspired by concepts from chemistry, the framework treats parameters like thermodynamic variables, enabling visualization of regions in parameter space that give rise to specific behaviors. Applying this framework to agents with minimally viable capabilities, we identify sufficient conditions for behaviors such as milling and diffusion and uncover regions of the parameter space that reliably produce these behaviors. Preliminary validation on real robots demonstrates that these regions correspond to observable behaviors in practice. By providing a principled, interpretable approach, this framework lays the groundwork for predictable and reliable emergent behavior in real-world swarm systems.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Agents That Reason About Their Computation</title>
<link>https://arxiv.org/abs/2510.22833</link>
<guid>https://arxiv.org/abs/2510.22833</guid>
<content:encoded><![CDATA[
arXiv:2510.22833v1 Announce Type: new 
Abstract: While reinforcement learning agents can achieve superhuman performance in many complex tasks, they typically do not become more computationally efficient as they improve. In contrast, humans gradually require less cognitive effort as they become more proficient at a task. If agents could reason about their compute as they learn, could they similarly reduce their computation footprint? If they could, we could have more energy efficient agents or free up compute cycles for other processes like planning. In this paper, we experiment with showing agents the cost of their computation and giving them the ability to control when they use compute. We conduct our experiments on the Arcade Learning Environment, and our results demonstrate that with the same training compute budget, agents that reason about their compute perform better on 75% of games. Furthermore, these agents use three times less compute on average. We analyze individual games and show where agents gain these efficiencies.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Storycaster: An AI System for Immersive Room-Based Storytelling</title>
<link>https://arxiv.org/abs/2510.22857</link>
<guid>https://arxiv.org/abs/2510.22857</guid>
<content:encoded><![CDATA[
arXiv:2510.22857v1 Announce Type: new 
Abstract: While Cave Automatic Virtual Environment (CAVE) systems have long enabled room-scale virtual reality and various kinds of interactivity, their content has largely remained predetermined. We present \textit{Storycaster}, a generative AI CAVE system that transforms physical rooms into responsive storytelling environments. Unlike headset-based VR, \textit{Storycaster} preserves spatial awareness, using live camera feeds to augment the walls with cylindrical projections, allowing users to create worlds that blend with their physical surroundings. Additionally, our system enables object-level editing, where physical items in the room can be transformed to their virtual counterparts in a story. A narrator agent guides participants, enabling them to co-create stories that evolve in response to voice commands, with each scene enhanced by generated ambient audio, dialogue, and imagery. Participants in our study ($n=13$) found the system highly immersive and engaging, with narrator and audio most impactful, while also highlighting areas for improvement in latency and image resolution.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MGFRec: Towards Reinforced Reasoning Recommendation with Multiple Groundings and Feedback</title>
<link>https://arxiv.org/abs/2510.22888</link>
<guid>https://arxiv.org/abs/2510.22888</guid>
<content:encoded><![CDATA[
arXiv:2510.22888v1 Announce Type: new 
Abstract: The powerful reasoning and generative capabilities of large language models (LLMs) have inspired researchers to apply them to reasoning-based recommendation tasks, which require in-depth reasoning about user interests and the generation of recommended items. However, previous reasoning-based recommendation methods have typically performed inference within the language space alone, without incorporating the actual item space. This has led to over-interpreting user interests and deviating from real items. Towards this research gap, we propose performing multiple rounds of grounding during inference to help the LLM better understand the actual item space, which could ensure that its reasoning remains aligned with real items. Furthermore, we introduce a user agent that provides feedback during each grounding step, enabling the LLM to better recognize and adapt to user interests. Comprehensive experiments conducted on three Amazon review datasets demonstrate the effectiveness of incorporating multiple groundings and feedback. These findings underscore the critical importance of reasoning within the actual item space, rather than being confined to the language space, for recommendation tasks.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Generalization in Agentic Tool Calling: CoreThink Agentic Reasoner and MAVEN Dataset</title>
<link>https://arxiv.org/abs/2510.22898</link>
<guid>https://arxiv.org/abs/2510.22898</guid>
<content:encoded><![CDATA[
arXiv:2510.22898v1 Announce Type: new 
Abstract: Generalization across Agentic tool-calling environments remains a key unsolved challenge in developing reliable agentic reasoning systems. While large language models (LLMs) demonstrate strong performance on isolated benchmarks, their ability to transfer reasoning strategies and co-ordinate tools across diverse domains is poorly understood. In this work, we conduct a large-scale evaluation of state-of-the-art LLMs on multiple tool-calling benchmarksBFCL v3, TauBench, Tau2Bench, and AceBenchand introduce MAVEN (Math & Physics Adversarial Verification & Evaluation Network), a new out of distribution (OOD) benchmark designed to stress-test multi-step reasoning through explicit verification and adversarial task composition. Our results show that most current models achieve below 50% accuracy on MAVEN, revealing a significant generalization gap across tool-use settings.
  To address this, we present the CoreThink Agentic Reasoner, a framework that augments LLMs with a lightweight symbolic reasoning layer for structured decomposition and adaptive tool orchestration. Without additional training, it generalizes across all benchmarks, achieving state-of-the-art performance with 530% improvements over existing baselines at roughly one-tenth the computational cost.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Server CLI Empowers Language Agents with Process Rewards</title>
<link>https://arxiv.org/abs/2510.22907</link>
<guid>https://arxiv.org/abs/2510.22907</guid>
<content:encoded><![CDATA[
arXiv:2510.22907v1 Announce Type: new 
Abstract: Large language models routinely hallucinate APIs and mislocalize edits, while language servers compute verified, IDE-grade facts about real code. We present Lanser-CLI, a CLI-first orchestration layer that pins and mediates a Language Server Protocol (LSP) server for coding agents and CI, exposing deterministic, replayable workflows. Our position is that language servers provide not only structural information (definitions, references, types, diagnostics) but also an actionable process reward: machine-checked, step-wise signals that align an agent's planning loop with program reality. In this work, Lanser-CLI contributes: (i) a robust addressing scheme beyond brittle "file:line:col" via a Selector DSL (symbolic, AST-path, and content-anchored selectors) with a principled relocation algorithm; (ii) deterministic Analysis Bundles that normalize Language Server responses and capture environment/capability metadata with stable content hashes; (iii) a safety envelope for mutating operations (rename, code actions) with preview, workspace jails, and Git-aware, transactional apply; and (iv) a process-reward functional derived from Language Server facts (diagnostic deltas, disambiguation confidence, and safe-apply checks) that is computable online and replayable offline. We formalize determinism under frozen snapshots and establish a monotonicity property for the process reward, making it suitable for process supervision and counterfactual analysis. Project Page: https://github.com/yifanzhang-pro/lanser-cli
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RL-AUX: Reinforcement Learning for Auxiliary Task Generation</title>
<link>https://arxiv.org/abs/2510.22940</link>
<guid>https://arxiv.org/abs/2510.22940</guid>
<content:encoded><![CDATA[
arXiv:2510.22940v1 Announce Type: new 
Abstract: Auxiliary Learning (AL) is a special case of Multi-task Learning (MTL) in which a network trains on auxiliary tasks to improve performance on its main task. This technique is used to improve generalization and, ultimately, performance on the network's main task. AL has been demonstrated to improve performance across multiple domains, including navigation, image classification, and natural language processing. One weakness of AL is the need for labeled auxiliary tasks, which can require human effort and domain expertise to generate. Meta Learning techniques have been used to solve this issue by learning an additional auxiliary task generation network that can create helpful tasks for the primary network. The most prominent techniques rely on Bi-Level Optimization, which incurs computational cost and increased code complexity. To avoid the need for Bi-Level Optimization, we present an RL-based approach to dynamically create auxiliary tasks. In this framework, an RL agent is tasked with selecting auxiliary labels for every data point in a training set. The agent is rewarded when their selection improves the performance on the primary task. We also experiment with learning optimal strategies for weighing the auxiliary loss per data point. On the 20-Superclass CIFAR100 problem, our RL approach outperforms human-labeled auxiliary tasks and performs as well as a prominent Bi-Level Optimization technique. Our weight learning approaches significantly outperform all of these benchmarks. For example, a Weight-Aware RL-based approach helps the VGG16 architecture achieve 80.9% test accuracy while the human-labeled auxiliary task setup achieved 75.53%. The goal of this work is to (1) prove that RL is a viable approach to dynamically generate auxiliary tasks and (2) demonstrate that per-sample auxiliary task weights can be learned alongside the auxiliary task labels and can achieve strong results.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CompressionAttack: Exploiting Prompt Compression as a New Attack Surface in LLM-Powered Agents</title>
<link>https://arxiv.org/abs/2510.22963</link>
<guid>https://arxiv.org/abs/2510.22963</guid>
<content:encoded><![CDATA[
arXiv:2510.22963v1 Announce Type: new 
Abstract: LLM-powered agents often use prompt compression to reduce inference costs, but this introduces a new security risk. Compression modules, which are optimized for efficiency rather than safety, can be manipulated by adversarial inputs, causing semantic drift and altering LLM behavior. This work identifies prompt compression as a novel attack surface and presents CompressionAttack, the first framework to exploit it. CompressionAttack includes two strategies: HardCom, which uses discrete adversarial edits for hard compression, and SoftCom, which performs latent-space perturbations for soft compression. Experiments on multiple LLMs show up to 80% attack success and 98% preference flips, while remaining highly stealthy and transferable. Case studies in VSCode Cline and Ollama confirm real-world impact, and current defenses prove ineffective, highlighting the need for stronger protections.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality Evaluation in LLMs</title>
<link>https://arxiv.org/abs/2510.22967</link>
<guid>https://arxiv.org/abs/2510.22967</guid>
<content:encoded><![CDATA[
arXiv:2510.22967v1 Announce Type: new 
Abstract: The widespread adoption of Large Language Models (LLMs) raises critical concerns about the factual accuracy of their outputs, especially in high-risk domains such as biomedicine, law, and education. Existing evaluation methods for short texts often fail on long-form content due to complex reasoning chains, intertwined perspectives, and cumulative information. To address this, we propose a systematic approach integrating large-scale long-form datasets, multi-agent verification mechanisms, and weighted evaluation metrics. We construct LongHalluQA, a Chinese long-form factuality dataset; and develop MAD-Fact, a debate-based multi-agent verification system. We introduce a fact importance hierarchy to capture the varying significance of claims in long-form texts. Experiments on two benchmarks show that larger LLMs generally maintain higher factual consistency, while domestic models excel on Chinese content. Our work provides a structured framework for evaluating and enhancing factual reliability in long-form LLM outputs, guiding their safe deployment in sensitive domains.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Conditional Diffusion Model with Mean Field Communication as Wireless Resource Allocation Planner</title>
<link>https://arxiv.org/abs/2510.22969</link>
<guid>https://arxiv.org/abs/2510.22969</guid>
<content:encoded><![CDATA[
arXiv:2510.22969v1 Announce Type: new 
Abstract: In wireless communication systems, efficient and adaptive resource allocation plays a crucial role in enhancing overall Quality of Service (QoS). While centralized Multi-Agent Reinforcement Learning (MARL) frameworks rely on a central coordinator for policy training and resource scheduling, they suffer from scalability issues and privacy risks. In contrast, the Distributed Training with Decentralized Execution (DTDE) paradigm enables distributed learning and decision-making, but it struggles with non-stationarity and limited inter-agent cooperation, which can severely degrade system performance. To overcome these challenges, we propose the Multi-Agent Conditional Diffusion Model Planner (MA-CDMP) for decentralized communication resource management. Built upon the Model-Based Reinforcement Learning (MBRL) paradigm, MA-CDMP employs Diffusion Models (DMs) to capture environment dynamics and plan future trajectories, while an inverse dynamics model guides action generation, thereby alleviating the sample inefficiency and slow convergence of conventional DTDE methods. Moreover, to approximate large-scale agent interactions, a Mean-Field (MF) mechanism is introduced as an assistance to the classifier in DMs. This design mitigates inter-agent non-stationarity and enhances cooperation with minimal communication overhead in distributed settings. We further theoretically establish an upper bound on the distributional approximation error introduced by the MF-based diffusion generation, guaranteeing convergence stability and reliable modeling of multi-agent stochastic dynamics. Extensive experiments demonstrate that MA-CDMP consistently outperforms existing MARL baselines in terms of average reward and QoS metrics, showcasing its scalability and practicality for real-world wireless network optimization.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeAD: Synthesize Code of Rules for Log-based Anomaly Detection with LLMs</title>
<link>https://arxiv.org/abs/2510.22986</link>
<guid>https://arxiv.org/abs/2510.22986</guid>
<content:encoded><![CDATA[
arXiv:2510.22986v1 Announce Type: new 
Abstract: Log-based anomaly detection (LogAD) is critical for maintaining the reliability and availability of large-scale online service systems. While machine learning, deep learning, and large language models (LLMs)-based methods have advanced the LogAD, they often suffer from limited interpretability, high inference costs, and extensive preprocessing requirements, limiting their practicality for real-time, high-volume log analysis. In contrast, rule-based systems offer efficiency and transparency, but require significant manual effort and are difficult to scale across diverse and evolving environments. In this paper, We present CodeAD, a novel framework that automatically synthesizes lightweight Python rule functions for LogAD using LLMs. CodeAD introduces a hierarchical clustering and anchor-grounded sampling strategy to construct representative contrastive log windows, enabling LLMs to discern discriminative anomaly patterns. To ensure robustness and generalizability, CodeAD employs an agentic workflow that iteratively generates, tests, repairs, and refines the rules until it meets correctness and abstraction requirements. The synthesized rules are interpretable, lightweight, and directly executable on raw logs, supporting efficient and transparent online anomaly detection. Our comprehensive experiments on three public datasets (BGL, Hadoop, Thunderbird) demonstrate that CodeAD achieves an average absolute improvement of 3.6% F1 score over the state-of-the-art baselines, while processing large datasets up to 4x faster and at a fraction of the cost (total LLM invocation cost under 4 USD per dataset). These results highlight CodeAD as a practical and scalable solution for online monitoring systems, enabling interpretable, efficient, and automated LogAD in real-world environment.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TALM: Dynamic Tree-Structured Multi-Agent Framework with Long-Term Memory for Scalable Code Generation</title>
<link>https://arxiv.org/abs/2510.23010</link>
<guid>https://arxiv.org/abs/2510.23010</guid>
<content:encoded><![CDATA[
arXiv:2510.23010v1 Announce Type: new 
Abstract: Agentic code generation requires large language models (LLMs) capable of complex context management and multi-step reasoning. Prior multi-agent frameworks attempt to address these challenges through collaboration, yet they often suffer from rigid workflows and high reasoning recovery costs. To overcome these limitations, we propose TALM (Tree-Structured Multi-Agent Framework with Long-Term Memory), a dynamic framework that integrates structured task decomposition, localized re-reasoning, and long-term memory mechanisms. TALM employs an extensible tree-based collaboration structure. The parent-child relationships, when combined with a divide-and-conquer strategy, enhance reasoning flexibility and enable efficient error correction across diverse task scopes. Furthermore, a long-term memory module enables semantic querying and integration of prior knowledge, supporting implicit self-improvement through experience reuse. Experimental results on HumanEval, BigCodeBench, and ClassEval benchmarks demonstrate that TALM consistently delivers strong reasoning performance and high token efficiency, highlighting its robustness and practical utility in complex code generation tasks.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LangLingual: A Personalised, Exercise-oriented English Language Learning Tool Leveraging Large Language Models</title>
<link>https://arxiv.org/abs/2510.23011</link>
<guid>https://arxiv.org/abs/2510.23011</guid>
<content:encoded><![CDATA[
arXiv:2510.23011v1 Announce Type: new 
Abstract: Language educators strive to create a rich experience for learners, while they may be restricted in the extend of feedback and practice they can provide. We present the design and development of LangLingual, a conversational agent built using the LangChain framework and powered by Large Language Models. The system is specifically designed to provide real-time, grammar-focused feedback, generate context-aware language exercises and track learner proficiency over time. The paper discusses the architecture, implementation and evaluation of LangLingual in detail. The results indicate strong usability, positive learning outcomes and encouraging learner engagement.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P1GPT: a multi-agent LLM workflow module for multi-modal financial information analysis</title>
<link>https://arxiv.org/abs/2510.23032</link>
<guid>https://arxiv.org/abs/2510.23032</guid>
<content:encoded><![CDATA[
arXiv:2510.23032v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have enabled multi-agent reasoning systems capable of collaborative decision-making. However, in financial analysis, most frameworks remain narrowly focused on either isolated single-agent predictors or loosely connected analyst ensembles, and they lack a coherent reasoning workflow that unifies diverse data modalities. We introduce P1GPT, a layered multi-agent LLM framework for multi-modal financial information analysis and interpretable trading decision support. Unlike prior systems that emulate trading teams through role simulation, P1GPT implements a structured reasoning pipeline that systematically fuses technical, fundamental, and news-based insights through coordinated agent communication and integration-time synthesis. Backtesting on multi-modal datasets across major U.S. equities demonstrates that P1GPT achieves superior cumulative and risk-adjusted returns, maintains low drawdowns, and provides transparent causal rationales. These findings suggest that structured reasoning workflows, rather than agent role imitation, offer a scalable path toward explainable and trustworthy financial AI systems.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AirFed: Federated Graph-Enhanced Multi-Agent Reinforcement Learning for Multi-UAV Cooperative Mobile Edge Computing</title>
<link>https://arxiv.org/abs/2510.23053</link>
<guid>https://arxiv.org/abs/2510.23053</guid>
<content:encoded><![CDATA[
arXiv:2510.23053v1 Announce Type: new 
Abstract: Multiple Unmanned Aerial Vehicles (UAVs) cooperative Mobile Edge Computing (MEC) systems face critical challenges in coordinating trajectory planning, task offloading, and resource allocation while ensuring Quality of Service (QoS) under dynamic and uncertain environments. Existing approaches suffer from limited scalability, slow convergence, and inefficient knowledge sharing among UAVs, particularly when handling large-scale IoT device deployments with stringent deadline constraints. This paper proposes AirFed, a novel federated graph-enhanced multi-agent reinforcement learning framework that addresses these challenges through three key innovations. First, we design dual-layer dynamic Graph Attention Networks (GATs) that explicitly model spatial-temporal dependencies among UAVs and IoT devices, capturing both service relationships and collaborative interactions within the network topology. Second, we develop a dual-Actor single-Critic architecture that jointly optimizes continuous trajectory control and discrete task offloading decisions. Third, we propose a reputation-based decentralized federated learning mechanism with gradient-sensitive adaptive quantization, enabling efficient and robust knowledge sharing across heterogeneous UAVs. Extensive experiments demonstrate that AirFed achieves 42.9% reduction in weighted cost compared to state-of-the-art baselines, attains over 99% deadline satisfaction and 94.2% IoT device coverage rate, and reduces communication overhead by 54.5%. Scalability analysis confirms robust performance across varying UAV numbers, IoT device densities, and system scales, validating AirFed's practical applicability for large-scale UAV-MEC deployments.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lost in Tokenization: Context as the Key to Unlocking Biomolecular Understanding in Scientific LLMs</title>
<link>https://arxiv.org/abs/2510.23127</link>
<guid>https://arxiv.org/abs/2510.23127</guid>
<content:encoded><![CDATA[
arXiv:2510.23127v1 Announce Type: new 
Abstract: Scientific Large Language Models (Sci-LLMs) have emerged as a promising frontier for accelerating biological discovery. However, these models face a fundamental challenge when processing raw biomolecular sequences: the tokenization dilemma. Whether treating sequences as a specialized language, risking the loss of functional motif information, or as a separate modality, introducing formidable alignment challenges, current strategies fundamentally limit their reasoning capacity. We challenge this sequence-centric paradigm by positing that a more effective strategy is to provide Sci-LLMs with high-level structured context derived from established bioinformatics tools, thereby bypassing the need to interpret low-level noisy sequence data directly. Through a systematic comparison of leading Sci-LLMs on biological reasoning tasks, we tested three input modes: sequence-only, context-only, and a combination of both. Our findings are striking: the context-only approach consistently and substantially outperforms all other modes. Even more revealing, the inclusion of the raw sequence alongside its high-level context consistently degrades performance, indicating that raw sequences act as informational noise, even for models with specialized tokenization schemes. These results suggest that the primary strength of existing Sci-LLMs lies not in their nascent ability to interpret biomolecular syntax from scratch, but in their profound capacity for reasoning over structured, human-readable knowledge. Therefore, we argue for reframing Sci-LLMs not as sequence decoders, but as powerful reasoning engines over expert knowledge. This work lays the foundation for a new class of hybrid scientific AI agents, repositioning the developmental focus from direct sequence interpretation towards high-level knowledge synthesis. The code is available at github.com/opendatalab-raise-dev/CoKE.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting Interleaved Encoders with PPO for Language-Guided Reinforcement Learning in BabyAI</title>
<link>https://arxiv.org/abs/2510.23148</link>
<guid>https://arxiv.org/abs/2510.23148</guid>
<content:encoded><![CDATA[
arXiv:2510.23148v1 Announce Type: new 
Abstract: Deep reinforcement learning agents often struggle when tasks require understanding both vision and language. Conventional architectures typically isolate perception (for example, CNN-based visual encoders) from decision-making (policy networks). This separation can be inefficient, since the policy's failures do not directly help the perception module learn what is important. To address this, we implement the Perception-Decision Interleaving Transformer (PDiT) architecture introduced by Mao et al. (2023), a model that alternates between perception and decision layers within a single transformer. This interleaving allows feedback from decision-making to refine perceptual features dynamically. In addition, we integrate a contrastive loss inspired by CLIP to align textual mission embeddings with visual scene features. We evaluate the PDiT encoders on the BabyAI GoToLocal environment and find that the approach achieves more stable rewards and stronger alignment compared to a standard PPO baseline. The results suggest that interleaved transformer encoders are a promising direction for developing more integrated autonomous agents.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SI-Bench: Benchmarking Social Intelligence of Large Language Models in Human-to-Human Conversations</title>
<link>https://arxiv.org/abs/2510.23182</link>
<guid>https://arxiv.org/abs/2510.23182</guid>
<content:encoded><![CDATA[
arXiv:2510.23182v1 Announce Type: new 
Abstract: As large language models (LLMs) develop anthropomorphic abilities, they are increasingly being deployed as autonomous agents to interact with humans. However, evaluating their performance in realistic and complex social interactions remains a significant challenge. Most previous research built datasets through simulated agent-to-agent interactions, which fails to capture the authentic linguistic styles and relational dynamics found in real human conversations. To address this gap, we introduce SI-Bench, a novel benchmark designed to evaluate aspects of social intelligence in LLMs. Grounded in broad social science theories, SI-Bench contains 2,221 authentic multi-turn dialogues collected from a social networking application. We further selected a subset of 312 dialogues for manual annotation across 8 major models. The experiments show that SOTA models have surpassed the human expert in process reasoning under complex social situations, yet they still fall behind humans in reply quality. Moreover, introducing Chain-of-Thought (CoT) reasoning may degrade the performance of LLMs in social dialogue tasks. All datasets are openly available at https://github.com/SI-Bench/SI-Bench.git.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Vision-LLMs in Surveillance Video</title>
<link>https://arxiv.org/abs/2510.23190</link>
<guid>https://arxiv.org/abs/2510.23190</guid>
<content:encoded><![CDATA[
arXiv:2510.23190v1 Announce Type: new 
Abstract: The widespread use of cameras in our society has created an overwhelming amount of video data, far exceeding the capacity for human monitoring. This presents a critical challenge for public safety and security, as the timely detection of anomalous or criminal events is crucial for effective response and prevention. The ability for an embodied agent to recognize unexpected events is fundamentally tied to its capacity for spatial reasoning. This paper investigates the spatial reasoning of vision-language models (VLMs) by framing anomalous action recognition as a zero-shot, language-grounded task, addressing the embodied perception challenge of interpreting dynamic 3D scenes from sparse 2D video. Specifically, we investigate whether small, pre-trained vision--LLMs can act as spatially-grounded, zero-shot anomaly detectors by converting video into text descriptions and scoring labels via textual entailment. We evaluate four open models on UCF-Crime and RWF-2000 under prompting and privacy-preserving conditions. Few-shot exemplars can improve accuracy for some models, but may increase false positives, and privacy filters -- especially full-body GAN transforms -- introduce inconsistencies that degrade accuracy. These results chart where current vision--LLMs succeed (simple, spatially salient events) and where they falter (noisy spatial cues, identity obfuscation). Looking forward, we outline concrete paths to strengthen spatial grounding without task-specific training: structure-aware prompts, lightweight spatial memory across clips, scene-graph or 3D-pose priors during description, and privacy methods that preserve action-relevant geometry. This positions zero-shot, language-grounded pipelines as adaptable building blocks for embodied, real-world video understanding. Our implementation for evaluating VLMs is publicly available at: https://github.com/pascalbenschopTU/VLLM_AnomalyRecognition
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>If They Disagree, Will You Conform? Exploring the Role of Robots' Value Awareness in a Decision-Making Task</title>
<link>https://arxiv.org/abs/2510.23204</link>
<guid>https://arxiv.org/abs/2510.23204</guid>
<content:encoded><![CDATA[
arXiv:2510.23204v1 Announce Type: new 
Abstract: This study investigates whether the opinions of robotic agents are more likely to influence human decision-making when the robots are perceived as value-aware (i.e., when they display an understanding of human principles). We designed an experiment in which participants interacted with two Furhat robots - one programmed to be Value-Aware and the other Non-Value-Aware - during a labeling task for images representing human values. Results indicate that participants distinguished the Value-Aware robot from the Non-Value-Aware one. Although their explicit choices did not indicate a clear preference for one robot over the other, participants directed their gaze more toward the Value-Aware robot. Additionally, the Value-Aware robot was perceived as more loyal, suggesting that value awareness in a social robot may enhance its perceived commitment to the group. Finally, when both robots disagreed with the participant, conformity occurred in about one out of four trials, and participants took longer to confirm their responses, suggesting that two robots expressing dissent may introduce hesitation in decision-making. On one hand, this highlights the potential risk that robots, if misused, could manipulate users for unethical purposes. On the other hand, it reinforces the idea that social robots might encourage reflection in ambiguous situations and help users avoid scams.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-Like Goalkeeping in a Realistic Football Simulation: a Sample-Efficient Reinforcement Learning Approach</title>
<link>https://arxiv.org/abs/2510.23216</link>
<guid>https://arxiv.org/abs/2510.23216</guid>
<content:encoded><![CDATA[
arXiv:2510.23216v1 Announce Type: new 
Abstract: While several high profile video games have served as testbeds for Deep Reinforcement Learning (DRL), this technique has rarely been employed by the game industry for crafting authentic AI behaviors. Previous research focuses on training super-human agents with large models, which is impractical for game studios with limited resources aiming for human-like agents. This paper proposes a sample-efficient DRL method tailored for training and fine-tuning agents in industrial settings such as the video game industry. Our method improves sample efficiency of value-based DRL by leveraging pre-collected data and increasing network plasticity. We evaluate our method training a goalkeeper agent in EA SPORTS FC 25, one of the best-selling football simulations today. Our agent outperforms the game's built-in AI by 10% in ball saving rate. Ablation studies show that our method trains agents 50% faster compared to standard DRL methods. Finally, qualitative evaluation from domain experts indicates that our approach creates more human-like gameplay compared to hand-crafted agents. As a testimony of the impact of the approach, the method is intended to replace the hand-crafted counterpart in next iterations of the series.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Stakeholder Alignment in LLM-Powered Collaborative AI Systems: A Multi-Agent Framework for Intelligent Tutoring</title>
<link>https://arxiv.org/abs/2510.23245</link>
<guid>https://arxiv.org/abs/2510.23245</guid>
<content:encoded><![CDATA[
arXiv:2510.23245v1 Announce Type: new 
Abstract: The integration of Large Language Models into Intelligent Tutoring Systems pre-sents significant challenges in aligning with diverse and often conflicting values from students, parents, teachers, and institutions. Existing architectures lack for-mal mechanisms for negotiating these multi-stakeholder tensions, creating risks in accountability and bias. This paper introduces the Advisory Governance Layer (AGL), a non-intrusive, multi-agent framework designed to enable distributed stakeholder participation in AI governance. The AGL employs specialized agents representing stakeholder groups to evaluate pedagogical actions against their spe-cific policies in a privacy-preserving manner, anticipating future advances in per-sonal assistant technology that will enhance stakeholder value expression. Through a novel policy taxonomy and conflict-resolution protocols, the frame-work provides structured, auditable governance advice to the ITS without altering its core pedagogical decision-making. This work contributes a reference architec-ture and technical specifications for aligning educational AI with multi-stakeholder values, bridging the gap between high-level ethical principles and practical implementation.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code Aesthetics with Agentic Reward Feedback</title>
<link>https://arxiv.org/abs/2510.23272</link>
<guid>https://arxiv.org/abs/2510.23272</guid>
<content:encoded><![CDATA[
arXiv:2510.23272v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have become valuable assistants for developers in code-related tasks. While LLMs excel at traditional programming tasks such as code generation and bug fixing, they struggle with visually-oriented coding tasks, often producing suboptimal aesthetics. In this paper, we introduce a new pipeline to enhance the aesthetic quality of LLM-generated code. We first construct AesCode-358K, a large-scale instruction-tuning dataset focused on code aesthetics. Next, we propose agentic reward feedback, a multi-agent system that evaluates executability, static aesthetics, and interactive aesthetics. Building on this, we develop GRPO-AR, which integrates these signals into the GRPO algorithm for joint optimization of functionality and code aesthetics. Finally, we develop OpenDesign, a benchmark for assessing code aesthetics. Experimental results show that combining supervised fine-tuning on AesCode-358K with reinforcement learning using agentic reward feedback significantly improves performance on OpenDesign and also enhances results on existing benchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o and GPT-4.1, and achieves performance comparable to large open-source models with 480B-685B parameters, underscoring the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DCMM-SQL: Automated Data-Centric Pipeline and Multi-Model Collaboration Training for Text-to-SQL Model</title>
<link>https://arxiv.org/abs/2510.23284</link>
<guid>https://arxiv.org/abs/2510.23284</guid>
<content:encoded><![CDATA[
arXiv:2510.23284v1 Announce Type: new 
Abstract: Text-to-SQL tasks have gained attractive improvements since the release of ChatGPT. Among them, agent-based frameworks have been widely used in this field. However, the impact of data-centric strategies on text-to-SQL tasks has rarely been explored. In this paper, we systemically design a fully automated data-centric pipeline for text-to-SQL tasks, including \emph{adaptive data repair}, which can automatically find and fix errors in the training dataset; and \emph{error data augmentation}, where we specifically diffuse and enhance erroneous data predicted by the initially trained models. Meanwhile, we propose a Multi-Model collaboration training schema, aiming to train multiple models with different augmented data, enabling them to possess distinct capabilities and work together to complement each other, because it has been found that the capability of a single fine-tuned model is very limited. Furthermore, we utilize an ensemble strategy to integrate the capabilities of multiple models to solve a multiple-choice question, aiming to further improve the accuracy of text-to-SQL tasks. The experiment results and ablation study have demonstrated the effectiveness of data-centric pipeline and Multi-Model(MM) interactive iterative strategies, achieving first place in lightweight text-to-SQL models (within 70B).
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDReID: Modality-Decoupled Learning for Any-to-Any Multi-Modal Object Re-Identification</title>
<link>https://arxiv.org/abs/2510.23301</link>
<guid>https://arxiv.org/abs/2510.23301</guid>
<content:encoded><![CDATA[
arXiv:2510.23301v1 Announce Type: new 
Abstract: Real-world object re-identification (ReID) systems often face modality inconsistencies, where query and gallery images come from different sensors (e.g., RGB, NIR, TIR). However, most existing methods assume modality-matched conditions, which limits their robustness and scalability in practical applications. To address this challenge, we propose MDReID, a flexible any-to-any image-level ReID framework designed to operate under both modality-matched and modality-mismatched scenarios. MDReID builds on the insight that modality information can be decomposed into two components: modality-shared features that are predictable and transferable, and modality-specific features that capture unique, modality-dependent characteristics. To effectively leverage this, MDReID introduces two key components: the Modality Decoupling Learning (MDL) and Modality-aware Metric Learning (MML). Specifically, MDL explicitly decomposes modality features into modality-shared and modality-specific representations, enabling effective retrieval in both modality-aligned and mismatched scenarios. MML, a tailored metric learning strategy, further enforces orthogonality and complementarity between the two components to enhance discriminative power across modalities. Extensive experiments conducted on three challenging multi-modality ReID benchmarks (RGBNT201, RGBNT100, MSVR310) consistently demonstrate the superiority of MDReID. Notably, MDReID achieves significant mAP improvements of 9.8\%, 3.0\%, and 11.5\% in general modality-matched scenarios, and average gains of 3.4\%, 11.8\%, and 10.9\% in modality-mismatched scenarios, respectively. The code is available at: \textcolor{magenta}{https://github.com/stone96123/MDReID}.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CNOT Minimal Circuit Synthesis: A Reinforcement Learning Approach</title>
<link>https://arxiv.org/abs/2510.23304</link>
<guid>https://arxiv.org/abs/2510.23304</guid>
<content:encoded><![CDATA[
arXiv:2510.23304v1 Announce Type: new 
Abstract: CNOT gates are fundamental to quantum computing, as they facilitate entanglement, a crucial resource for quantum algorithms. Certain classes of quantum circuits are constructed exclusively from CNOT gates. Given their widespread use, it is imperative to minimise the number of CNOT gates employed. This problem, known as CNOT minimisation, remains an open challenge, with its computational complexity yet to be fully characterised. In this work, we introduce a novel reinforcement learning approach to address this task. Instead of training multiple reinforcement learning agents for different circuit sizes, we use a single agent up to a fixed size $m$. Matrices of sizes different from m are preprocessed using either embedding or Gaussian striping. To assess the efficacy of our approach, we trained an agent with m = 8, and evaluated it on matrices of size n that range from 3 to 15. The results we obtained show that our method overperforms the state-of-the-art algorithm as the value of n increases.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by Projecting User Awareness across Future Timesteps</title>
<link>https://arxiv.org/abs/2510.23340</link>
<guid>https://arxiv.org/abs/2510.23340</guid>
<content:encoded><![CDATA[
arXiv:2510.23340v1 Announce Type: new 
Abstract: Adaptive agent design offers a way to improve human-AI collaboration on time-sensitive tasks in rapidly changing environments. In such cases, to ensure the human maintains an accurate understanding of critical task elements, an assistive agent must not only identify the highest priority information but also estimate how and when this information can be communicated most effectively, given that human attention represents a zero-sum cognitive resource where focus on one message diminishes awareness of other or upcoming information. We introduce a theoretical framework for adaptive signalling which meets these challenges by using principles of rational communication, formalised as Bayesian reference resolution using the Rational Speech Act (RSA) modelling framework, to plan a sequence of messages which optimise timely alignment between user belief and a dynamic environment. The agent adapts message specificity and timing to the particulars of a user and scenario based on projections of how prior-guided interpretation of messages will influence attention to the interface and subsequent belief update, across several timesteps out to a fixed horizon. In a comparison to baseline methods, we show that this effectiveness depends crucially on combining multi-step planning with a realistic model of user awareness. As the first application of RSA for communication in a dynamic environment, and for human-AI interaction in general, we establish theoretical foundations for pragmatic communication in human-agent teams, highlighting how insights from cognitive science can be capitalised to inform the design of assistive agents.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoStreamPipe: LLM Assisted Automatic Generation of Data Stream Processing Pipelines</title>
<link>https://arxiv.org/abs/2510.23408</link>
<guid>https://arxiv.org/abs/2510.23408</guid>
<content:encoded><![CDATA[
arXiv:2510.23408v1 Announce Type: new 
Abstract: Data pipelines are essential in stream processing as they enable the efficient collection, processing, and delivery of real-time data, supporting rapid data analysis. In this paper, we present AutoStreamPipe, a novel framework that employs Large Language Models (LLMs) to automate the design, generation, and deployment of stream processing pipelines. AutoStreamPipe bridges the semantic gap between high-level user intent and platform-specific implementations across distributed stream processing systems for structured multi-agent reasoning by integrating a Hypergraph of Thoughts (HGoT) as an extended version of GoT. AutoStreamPipe combines resilient execution strategies, advanced query analysis, and HGoT to deliver pipelines with good accuracy. Experimental evaluations on diverse pipelines demonstrate that AutoStreamPipe significantly reduces development time (x6.3) and error rates (x5.19), as measured by a novel Error-Free Score (EFS), compared to LLM code-generation methods.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Deep Q Network</title>
<link>https://arxiv.org/abs/2510.23424</link>
<guid>https://arxiv.org/abs/2510.23424</guid>
<content:encoded><![CDATA[
arXiv:2510.23424v1 Announce Type: new 
Abstract: Deep Q Networks (DQN) have shown remarkable success in various reinforcement learning tasks. However, their reliance on associative learning often leads to the acquisition of spurious correlations, hindering their problem-solving capabilities. In this paper, we introduce a novel approach to integrate causal principles into DQNs, leveraging the PEACE (Probabilistic Easy vAriational Causal Effect) formula for estimating causal effects. By incorporating causal reasoning during training, our proposed framework enhances the DQN's understanding of the underlying causal structure of the environment, thereby mitigating the influence of confounding factors and spurious correlations. We demonstrate that integrating DQNs with causal capabilities significantly enhances their problem-solving capabilities without compromising performance. Experimental results on standard benchmark environments showcase that our approach outperforms conventional DQNs, highlighting the effectiveness of causal reasoning in reinforcement learning. Overall, our work presents a promising avenue for advancing the capabilities of deep reinforcement learning agents through principled causal inference.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrowseConf: Confidence-Guided Test-Time Scaling for Web Agents</title>
<link>https://arxiv.org/abs/2510.23458</link>
<guid>https://arxiv.org/abs/2510.23458</guid>
<content:encoded><![CDATA[
arXiv:2510.23458v1 Announce Type: new 
Abstract: Confidence in LLMs is a useful indicator of model uncertainty and answer reliability. Existing work mainly focused on single-turn scenarios, while research on confidence in complex multi-turn interactions is limited. In this paper, we investigate whether LLM-based search agents have the ability to communicate their own confidence through verbalized confidence scores after long sequences of actions, a significantly more challenging task compared to outputting confidence in a single interaction. Experimenting on open-source agentic models, we first find that models exhibit much higher task accuracy at high confidence while having near-zero accuracy when confidence is low. Based on this observation, we propose Test-Time Scaling (TTS) methods that use confidence scores to determine answer quality, encourage the model to try again until reaching a satisfactory confidence level. Results show that our proposed methods significantly reduce token consumption while demonstrating competitive performance compared to baseline fixed budget TTS methods.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-AI Collaborative Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2510.23476</link>
<guid>https://arxiv.org/abs/2510.23476</guid>
<content:encoded><![CDATA[
arXiv:2510.23476v1 Announce Type: new 
Abstract: AI predictive systems are increasingly embedded in decision making pipelines, shaping high stakes choices once made solely by humans. Yet robust decisions under uncertainty still rely on capabilities that current AI lacks: domain knowledge not captured by data, long horizon context, and reasoning grounded in the physical world. This gap has motivated growing efforts to design collaborative frameworks that combine the complementary strengths of humans and AI. This work advances this vision by identifying the fundamental principles of Human AI collaboration within uncertainty quantification, a key component of reliable decision making. We introduce Human AI Collaborative Uncertainty Quantification, a framework that formalizes how an AI model can refine a human expert's proposed prediction set with two goals: avoiding counterfactual harm, ensuring the AI does not degrade correct human judgments, and complementarity, enabling recovery of correct outcomes the human missed. At the population level, we show that the optimal collaborative prediction set follows an intuitive two threshold structure over a single score function, extending a classical result in conformal prediction. Building on this insight, we develop practical offline and online calibration algorithms with provable distribution free finite sample guarantees. The online method adapts to distribution shifts, including human behavior evolving through interaction with AI, a phenomenon we call Human to AI Adaptation. Experiments across image classification, regression, and text based medical decision making show that collaborative prediction sets consistently outperform either agent alone, achieving higher coverage and smaller set sizes across various conditions.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UrbanIng-V2X: A Large-Scale Multi-Vehicle, Multi-Infrastructure Dataset Across Multiple Intersections for Cooperative Perception</title>
<link>https://arxiv.org/abs/2510.23478</link>
<guid>https://arxiv.org/abs/2510.23478</guid>
<content:encoded><![CDATA[
arXiv:2510.23478v1 Announce Type: new 
Abstract: Recent cooperative perception datasets have played a crucial role in advancing smart mobility applications by enabling information exchange between intelligent agents, helping to overcome challenges such as occlusions and improving overall scene understanding. While some existing real-world datasets incorporate both vehicle-to-vehicle and vehicle-to-infrastructure interactions, they are typically limited to a single intersection or a single vehicle. A comprehensive perception dataset featuring multiple connected vehicles and infrastructure sensors across several intersections remains unavailable, limiting the benchmarking of algorithms in diverse traffic environments. Consequently, overfitting can occur, and models may demonstrate misleadingly high performance due to similar intersection layouts and traffic participant behavior. To address this gap, we introduce UrbanIng-V2X, the first large-scale, multi-modal dataset supporting cooperative perception involving vehicles and infrastructure sensors deployed across three urban intersections in Ingolstadt, Germany. UrbanIng-V2X consists of 34 temporally aligned and spatially calibrated sensor sequences, each lasting 20 seconds. All sequences contain recordings from one of three intersections, involving two vehicles and up to three infrastructure-mounted sensor poles operating in coordinated scenarios. In total, UrbanIng-V2X provides data from 12 vehicle-mounted RGB cameras, 2 vehicle LiDARs, 17 infrastructure thermal cameras, and 12 infrastructure LiDARs. All sequences are annotated at a frequency of 10 Hz with 3D bounding boxes spanning 13 object classes, resulting in approximately 712k annotated instances across the dataset. We provide comprehensive evaluations using state-of-the-art cooperative perception methods and publicly release the codebase, dataset, HD map, and a digital twin of the complete data collection environment.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Agents Just Automata? On the Formal Equivalence Between Agentic AI and the Chomsky Hierarchy</title>
<link>https://arxiv.org/abs/2510.23487</link>
<guid>https://arxiv.org/abs/2510.23487</guid>
<content:encoded><![CDATA[
arXiv:2510.23487v1 Announce Type: new 
Abstract: This paper establishes a formal equivalence between the architectural classes of modern agentic AI systems and the abstract machines of the Chomsky hierarchy. We posit that the memory architecture of an AI agent is the definitive feature determining its computational power and that it directly maps it to a corresponding class of automaton. Specifically, we demonstrate that simple reflex agents are equivalent to Finite Automata, hierarchical task-decomposition agents are equivalent to Pushdown Automata, and agents employing readable/writable memory for reflection are equivalent to TMs. This Automata-Agent Framework provides a principled methodology for right-sizing agent architectures to optimize computational efficiency and cost. More critically, it creates a direct pathway to formal verification, enables the application of mature techniques from automata theory to guarantee agent safety and predictability. By classifying agents, we can formally delineate the boundary between verifiable systems and those whose behavior is fundamentally undecidable. We address the inherent probabilistic nature of LLM-based agents by extending the framework to probabilistic automata that allow quantitative risk analysis. The paper concludes by outlining an agenda for developing static analysis tools and grammars for agentic frameworks.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deductive Chain-of-Thought Augmented Socially-aware Robot Navigation World Model</title>
<link>https://arxiv.org/abs/2510.23509</link>
<guid>https://arxiv.org/abs/2510.23509</guid>
<content:encoded><![CDATA[
arXiv:2510.23509v1 Announce Type: new 
Abstract: Social robot navigation increasingly relies on large language models for reasoning, path planning, and enabling movement in dynamic human spaces. However, relying solely on LLMs for planning often leads to unpredictable and unsafe behaviors, especially in dynamic human spaces, due to limited physical grounding and weak logical consistency. In this work, we introduce NaviWM, a socially-aware robot Navigation World Model that augments LLM reasoning with a structured world model and a logic-driven chain-of-thought process. NaviWM consists of two main components: (1) a spatial-temporal world model that captures the positions, velocities, and activities of agents in the environment, and (2) a deductive reasoning module that guides LLMs through a multi-step, logic-based inference process. This integration enables the robot to generate navigation decisions that are both socially compliant and physically safe, under well-defined constraints such as personal space, collision avoidance, and timing. Unlike previous methods based on prompting or fine-tuning, NaviWM encodes social norms as first-order logic, enabling interpretable and verifiable reasoning. Experiments show that NaviWM improves success rates and reduces social violations, particularly in crowded environments. These results demonstrate the benefit of combining formal reasoning with LLMs for robust social navigation. Additional experimental details and demo videos for this work can be found at: https://sites.google.com/view/NaviWM.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequential Multi-Agent Dynamic Algorithm Configuration</title>
<link>https://arxiv.org/abs/2510.23535</link>
<guid>https://arxiv.org/abs/2510.23535</guid>
<content:encoded><![CDATA[
arXiv:2510.23535v1 Announce Type: new 
Abstract: Dynamic algorithm configuration (DAC) is a recent trend in automated machine learning, which can dynamically adjust the algorithm's configuration during the execution process and relieve users from tedious trial-and-error tuning tasks. Recently, multi-agent reinforcement learning (MARL) approaches have improved the configuration of multiple heterogeneous hyperparameters, making various parameter configurations for complex algorithms possible. However, many complex algorithms have inherent inter-dependencies among multiple parameters (e.g., determining the operator type first and then the operator's parameter), which are, however, not considered in previous approaches, thus leading to sub-optimal results. In this paper, we propose the sequential multi-agent DAC (Seq-MADAC) framework to address this issue by considering the inherent inter-dependencies of multiple parameters. Specifically, we propose a sequential advantage decomposition network, which can leverage action-order information through sequential advantage decomposition. Experiments from synthetic functions to the configuration of multi-objective optimization algorithms demonstrate Seq-MADAC's superior performance over state-of-the-art MARL methods and show strong generalization across problem classes. Seq-MADAC establishes a new paradigm for the widespread dependency-aware automated algorithm configuration. Our code is available at https://github.com/lamda-bbo/seq-madac.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReCode: Unify Plan and Action for Universal Granularity Control</title>
<link>https://arxiv.org/abs/2510.23564</link>
<guid>https://arxiv.org/abs/2510.23564</guid>
<content:encoded><![CDATA[
arXiv:2510.23564v1 Announce Type: new 
Abstract: Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a high-level form of action. However, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities. This limitation stems from existing paradigms that enforce a rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization. We propose ReCode (Recursive Code Generation), a novel paradigm that addresses this limitation by unifying planning and action within a single code representation. In this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions. This recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to learn hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control. The code is available at https://github.com/FoundationAgents/ReCode.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT</title>
<link>https://arxiv.org/abs/2510.23569</link>
<guid>https://arxiv.org/abs/2510.23569</guid>
<content:encoded><![CDATA[
arXiv:2510.23569v1 Announce Type: new 
Abstract: Egocentric video reasoning centers on an unobservable agent behind the camera who dynamically shapes the environment, requiring inference of hidden intentions and recognition of fine-grained interactions. This core challenge limits current multimodal large language models MLLMs, which excel at visible event reasoning but lack embodied, first-person understanding. To bridge this gap, we introduce EgoThinker, a novel framework that endows MLLMs with robust egocentric reasoning capabilities through spatio-temporal chain-of-thought supervision and a two-stage learning curriculum. First, we introduce EgoRe-5M, a large-scale egocentric QA dataset constructed from 13M diverse egocentric video clips. This dataset features multi-minute segments annotated with detailed CoT rationales and dense hand-object grounding. Second, we employ SFT on EgoRe-5M to instill reasoning skills, followed by reinforcement fine-tuning RFT to further enhance spatio-temporal localization. Experimental results show that EgoThinker outperforms existing methods across multiple egocentric benchmarks, while achieving substantial improvements in fine-grained spatio-temporal localization tasks. Full code and data are released at https://github.com/InternRobotics/EgoThinker.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RobotArena $\infty$: Scalable Robot Benchmarking via Real-to-Sim Translation</title>
<link>https://arxiv.org/abs/2510.23571</link>
<guid>https://arxiv.org/abs/2510.23571</guid>
<content:encoded><![CDATA[
arXiv:2510.23571v1 Announce Type: new 
Abstract: The pursuit of robot generalists - instructable agents capable of performing diverse tasks across diverse environments - demands rigorous and scalable evaluation. Yet real-world testing of robot policies remains fundamentally constrained: it is labor-intensive, slow, unsafe at scale, and difficult to reproduce. Existing simulation benchmarks are similarly limited, as they train and test policies within the same synthetic domains and cannot assess models trained from real-world demonstrations or alternative simulation environments. As policies expand in scope and complexity, these barriers only intensify, since defining "success" in robotics often hinges on nuanced human judgments of execution quality. In this paper, we introduce a new benchmarking framework that overcomes these challenges by shifting VLA evaluation into large-scale simulated environments augmented with online human feedback. Leveraging advances in vision-language models, 2D-to-3D generative modeling, and differentiable rendering, our approach automatically converts video demonstrations from widely used robot datasets into simulated counterparts. Within these digital twins, we assess VLA policies using both automated VLM-guided scoring and scalable human preference judgments collected from crowdworkers, transforming human involvement from tedious scene setup, resetting, and safety supervision into lightweight preference comparisons. To measure robustness, we systematically perturb simulated environments along multiple axes, such as textures and object placements, stress-testing policy generalization under controlled variation. The result is a continuously evolving, reproducible, and scalable benchmark for real-world trained robot manipulation policies, addressing a critical missing capability in today's robotics landscape.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Data Agents: Emerging Paradigm or Overstated Hype?</title>
<link>https://arxiv.org/abs/2510.23587</link>
<guid>https://arxiv.org/abs/2510.23587</guid>
<content:encoded><![CDATA[
arXiv:2510.23587v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) has spurred the emergence of data agents--autonomous systems designed to orchestrate Data + AI ecosystems for tackling complex data-related tasks. However, the term "data agent" currently suffers from terminological ambiguity and inconsistent adoption, conflating simple query responders with sophisticated autonomous architectures. This terminological ambiguity fosters mismatched user expectations, accountability challenges, and barriers to industry growth. Inspired by the SAE J3016 standard for driving automation, this survey introduces the first systematic hierarchical taxonomy for data agents, comprising six levels that delineate and trace progressive shifts in autonomy, from manual operations (L0) to a vision of generative, fully autonomous data agents (L5), thereby clarifying capability boundaries and responsibility allocation. Through this lens, we offer a structured review of existing research arranged by increasing autonomy, encompassing specialized data agents for data management, preparation, and analysis, alongside emerging efforts toward versatile, comprehensive systems with enhanced autonomy. We further analyze critical evolutionary leaps and technical gaps for advancing data agents, especially the ongoing L2-to-L3 transition, where data agents evolve from procedural execution to autonomous orchestration. Finally, we conclude with a forward-looking roadmap, envisioning the advent of proactive, generative data agents.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Evolve: LLM Self-Improve through Co-evolution</title>
<link>https://arxiv.org/abs/2510.23595</link>
<guid>https://arxiv.org/abs/2510.23595</guid>
<content:encoded><![CDATA[
arXiv:2510.23595v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has demonstrated significant potential in enhancing the reasoning capabilities of large language models (LLMs). However, the success of RL for LLMs heavily relies on human-curated datasets and verifiable rewards, which limit their scalability and generality. Recent Self-Play RL methods, inspired by the success of the paradigm in games and Go, aim to enhance LLM reasoning capabilities without human-annotated data. However, their methods primarily depend on a grounded environment for feedback (e.g., a Python interpreter or a game engine); extending them to general domains remains challenging. To address these challenges, we propose Multi-Agent Evolve (MAE), a framework that enables LLMs to self-evolve in solving diverse tasks, including mathematics, reasoning, and general knowledge Q&amp;A. The core design of MAE is based on a triplet of interacting agents (Proposer, Solver, Judge) that are instantiated from a single LLM, and applies reinforcement learning to optimize their behaviors. The Proposer generates questions, the Solver attempts solutions, and the Judge evaluates both while co-evolving. Experiments on Qwen2.5-3B-Instruct demonstrate that MAE achieves an average improvement of 4.54% on multiple benchmarks. These results highlight MAE as a scalable, data-efficient method for enhancing the general reasoning abilities of LLMs with minimal reliance on human-curated supervision.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alita-G: Self-Evolving Generative Agent for Agent Generation</title>
<link>https://arxiv.org/abs/2510.23601</link>
<guid>https://arxiv.org/abs/2510.23601</guid>
<content:encoded><![CDATA[
arXiv:2510.23601v1 Announce Type: new 
Abstract: Large language models (LLMs) have been shown to perform better when scaffolded into agents with memory, tools, and feedback. Beyond this, self-evolving agents have emerged, but current work largely limits adaptation to prompt rewriting or failure retries. Therefore, we present ALITA-G, a self-evolution framework that transforms a general-purpose agent into a domain expert by systematically generating, abstracting, and curating Model Context Protocol (MCP) tools. In this framework, a generalist agent executes a curated suite of target-domain tasks and synthesizes candidate MCPs from successful trajectories. These are then abstracted to parameterized primitives and consolidated into an MCP Box. At inference time, ALITA-G performs retrieval-augmented MCP selection with the help of each tool's descriptions and use cases, before executing an agent equipped with the MCP Executor. Across several benchmarks GAIA, PathVQA, and Humanity's Last Exam, ALITA-G attains strong gains while reducing computation costs. On GAIA validation, it achieves 83.03% pass@1 and 89.09% pass@3, establishing a new state-of-the-art result while reducing mean tokens per example by approximately 15% relative to a strong baseline agent. ALITA-G thus provides a principled pathway from generalist capability to reusable, domain-specific competence, improving both accuracy and efficiency on complex reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Right Place, Right Time: Market Simulation-based RL for Execution Optimisation</title>
<link>https://arxiv.org/abs/2510.22206</link>
<guid>https://arxiv.org/abs/2510.22206</guid>
<content:encoded><![CDATA[
arXiv:2510.22206v1 Announce Type: cross 
Abstract: Execution algorithms are vital to modern trading, they enable market participants to execute large orders while minimising market impact and transaction costs. As these algorithms grow more sophisticated, optimising them becomes increasingly challenging. In this work, we present a reinforcement learning (RL) framework for discovering optimal execution strategies, evaluated within a reactive agent-based market simulator. This simulator creates reactive order flow and allows us to decompose slippage into its constituent components: market impact and execution risk. We assess the RL agent's performance using the efficient frontier based on work by Almgren and Chriss, measuring its ability to balance risk and cost. Results show that the RL-derived strategies consistently outperform baselines and operate near the efficient frontier, demonstrating a strong ability to optimise for risk and impact. These findings highlight the potential of reinforcement learning as a powerful tool in the trader's toolkit.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Stochastic Proximal Algorithm on Riemannian Submanifolds for Weakly-convex Functions</title>
<link>https://arxiv.org/abs/2510.22270</link>
<guid>https://arxiv.org/abs/2510.22270</guid>
<content:encoded><![CDATA[
arXiv:2510.22270v1 Announce Type: cross 
Abstract: This paper aims to investigate the distributed stochastic optimization problems on compact embedded submanifolds (in the Euclidean space) for multi-agent network systems. To address the manifold structure, we propose a distributed Riemannian stochastic proximal algorithm framework by utilizing the retraction and Riemannian consensus protocol, and analyze three specific algorithms: the distributed Riemannian stochastic subgradient, proximal point, and prox-linear algorithms. When the local costs are weakly-convex and the initial points satisfy certain conditions, we show that the iterates generated by this framework converge to a nearly stationary point in expectation while achieving consensus. We further establish the convergence rate of the algorithm framework as $\mathcal{O}(\frac{1+\kappa_g}{\sqrt{k}})$ where $k$ denotes the number of iterations and $\kappa_g$ shows the impact of manifold geometry on the algorithm performance. Finally, numerical experiments are implemented to demonstrate the theoretical results and show the empirical performance.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement learning-guided optimization of critical current in high-temperature superconductors</title>
<link>https://arxiv.org/abs/2510.22424</link>
<guid>https://arxiv.org/abs/2510.22424</guid>
<content:encoded><![CDATA[
arXiv:2510.22424v1 Announce Type: cross 
Abstract: High-temperature superconductors are essential for next-generation energy and quantum technologies, yet their performance is often limited by the critical current density ($J_c$), which is strongly influenced by microstructural defects. Optimizing $J_c$ through defect engineering is challenging due to the complex interplay of defect type, density, and spatial correlation. Here we present an integrated workflow that combines reinforcement learning (RL) with time-dependent Ginzburg-Landau (TDGL) simulations to autonomously identify optimal defect configurations that maximize $J_c$. In our framework, TDGL simulations generate current-voltage characteristics to evaluate $J_c$, which serves as the reward signal that guides the RL agent to iteratively refine defect configurations. We find that the agent discovers optimal defect densities and correlations in two-dimensional thin-film geometries, enhancing vortex pinning and $J_c$ relative to the pristine thin-film, approaching 60\% of theoretical depairing limit with up to 15-fold enhancement compared to random initialization. This RL-driven approach provides a scalable strategy for defect engineering, with broad implications for advancing HTS applications in fusion magnets, particle accelerators, and other high-field technologies.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TABL-ABM: A Hybrid Framework for Synthetic LOB Generation</title>
<link>https://arxiv.org/abs/2510.22685</link>
<guid>https://arxiv.org/abs/2510.22685</guid>
<content:encoded><![CDATA[
arXiv:2510.22685v1 Announce Type: cross 
Abstract: The recent application of deep learning models to financial trading has heightened the need for high fidelity financial time series data. This synthetic data can be used to supplement historical data to train large trading models. The state-of-the-art models for the generative application often rely on huge amounts of historical data and large, complicated models. These models range from autoregressive and diffusion-based models through to architecturally simpler models such as the temporal-attention bilinear layer. Agent-based approaches to modelling limit order book dynamics can also recreate trading activity through mechanistic models of trader behaviours. In this work, we demonstrate how a popular agent-based framework for simulating intraday trading activity, the Chiarella model, can be combined with one of the most performant deep learning models for forecasting multi-variate time series, the TABL model. This forecasting model is coupled to a simulation of a matching engine with a novel method for simulating deleted order flow. Our simulator gives us the ability to test the generative abilities of the forecasting model using stylised facts. Our results show that this methodology generates realistic price dynamics however, when analysing deeper, parts of the markets microstructure are not accurately recreated, highlighting the necessity for including more sophisticated agent behaviors into the modeling framework to help account for tail events.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Proficiency in Centralized Multi-Agent Systems: A Performance Study</title>
<link>https://arxiv.org/abs/2510.23447</link>
<guid>https://arxiv.org/abs/2510.23447</guid>
<content:encoded><![CDATA[
arXiv:2510.23447v1 Announce Type: cross 
Abstract: Autonomous agents are increasingly deployed in dynamic environments where their ability to perform a given task depends on both individual and team-level proficiency. While proficiency self-assessment (PSA) has been studied for single agents, its extension to a team of agents remains underexplored. This letter addresses this gap by presenting a framework for team PSA in centralized settings. We investigate three metrics for centralized team PSA: the measurement prediction bound (MPB), the Kolmogorov-Smirnov (KS) statistic, and the Kullback-Leibler (KL) divergence. These metrics quantify the discrepancy between predicted and actual measurements. We use the KL divergence as a reference metric since it compares the true and predictive distributions, whereas the MPB and KS provide efficient indicators for in situ assessment. Simulation results in a target tracking scenario demonstrate that both MPB and KS metrics accurately capture model mismatches, align with the KL divergence reference, and enable real-time proficiency assessment.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimizing Human Intervention in Online Classification</title>
<link>https://arxiv.org/abs/2510.23557</link>
<guid>https://arxiv.org/abs/2510.23557</guid>
<content:encoded><![CDATA[
arXiv:2510.23557v1 Announce Type: cross 
Abstract: We introduce and study an online problem arising in question answering systems. In this problem, an agent must sequentially classify user-submitted queries represented by $d$-dimensional embeddings drawn i.i.d. from an unknown distribution. The agent may consult a costly human expert for the correct label, or guess on her own without receiving feedback. The goal is to minimize regret against an oracle with free expert access. When the time horizon $T$ is at least exponential in the embedding dimension $d$, one can learn the geometry of the class regions: in this regime, we propose the Conservative Hull-based Classifier (CHC), which maintains convex hulls of expert-labeled queries and calls the expert as soon as a query lands outside all known hulls. CHC attains $\mathcal{O}(\log^d T)$ regret in $T$ and is minimax optimal for $d=1$. Otherwise, the geometry cannot be reliably learned without additional distributional assumptions. We show that when the queries are drawn from a subgaussian mixture, for $T \le e^d$, a Center-based Classifier (CC) achieves regret proportional to $N\log{N}$ where $N$ is the number of labels. To bridge these regimes, we introduce the Generalized Hull-based Classifier (GHC), a practical extension of CHC that allows for more aggressive guessing via a tunable threshold parameter. Our approach is validated with experiments, notably on real-world question-answering datasets using embeddings derived from state-of-the-art large language models.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Censoring chemical data to mitigate dual use risk</title>
<link>https://arxiv.org/abs/2304.10510</link>
<guid>https://arxiv.org/abs/2304.10510</guid>
<content:encoded><![CDATA[
arXiv:2304.10510v2 Announce Type: replace 
Abstract: Machine learning models have dual-use potential, potentially serving both beneficial and malicious purposes. The development of open-source models in chemistry has specifically surfaced dual-use concerns around toxicological data and chemical warfare agents. We discuss a chain risk framework identifying three misuse pathways and corresponding mitigation strategies: inference-level, model-level, and data-level. At the data level, we introduce a model-agnostic noising method to increase prediction error in specific desired regions (sensitive regions). Our results show that selective noise induces variance and attenuation bias, whereas simply omitting sensitive data fails to prevent extrapolation. These findings hold for both molecular feature multilayer perceptrons and graph neural networks. Thus, noising molecular structures can enable open sharing of potential dual-use molecular data.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learnable Behavior Control: Breaking Atari Human World Records via Sample-Efficient Behavior Selection</title>
<link>https://arxiv.org/abs/2305.05239</link>
<guid>https://arxiv.org/abs/2305.05239</guid>
<content:encoded><![CDATA[
arXiv:2305.05239v2 Announce Type: replace 
Abstract: The exploration problem is one of the main challenges in deep reinforcement learning (RL). Recent promising works tried to handle the problem with population-based methods, which collect samples with diverse behaviors derived from a population of different exploratory policies. Adaptive policy selection has been adopted for behavior control. However, the behavior selection space is largely limited by the predefined policy population, which further limits behavior diversity. In this paper, we propose a general framework called Learnable Behavioral Control (LBC) to address the limitation, which a) enables a significantly enlarged behavior selection space via formulating a hybrid behavior mapping from all policies; b) constructs a unified learnable process for behavior selection. We introduce LBC into distributed off-policy actor-critic methods and achieve behavior control via optimizing the selection of the behavior mappings with bandit-based meta-controllers. Our agents have achieved 10077.52% mean human normalized score and surpassed 24 human world records within 1B training frames in the Arcade Learning Environment, which demonstrates our significant state-of-the-art (SOTA) performance without degrading the sample efficiency.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Centralized Reward Agent for Knowledge Sharing and Transfer in Multi-Task Reinforcement Learning</title>
<link>https://arxiv.org/abs/2408.10858</link>
<guid>https://arxiv.org/abs/2408.10858</guid>
<content:encoded><![CDATA[
arXiv:2408.10858v3 Announce Type: replace 
Abstract: Reward shaping is effective in addressing the sparse-reward challenge in reinforcement learning (RL) by providing immediate feedback through auxiliary, informative rewards. Based on the reward shaping strategy, we propose a novel multi-task reinforcement learning framework that integrates a centralized reward agent (CRA) and multiple distributed policy agents. The CRA functions as a knowledge pool, aimed at distilling knowledge from various tasks and distributing it to individual policy agents to improve learning efficiency. Specifically, the shaped rewards serve as a straightforward metric for encoding knowledge. This framework not only enhances knowledge sharing across established tasks but also adapts to new tasks by transferring meaningful reward signals. We validate the proposed method on both discrete and continuous domains, including the representative Meta-World benchmark, demonstrating its robustness in multi-task sparse-reward settings and its effective transferability to unseen tasks.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrated Design and Governance of Agentic AI Systems through Adaptive Information Modulation</title>
<link>https://arxiv.org/abs/2409.10372</link>
<guid>https://arxiv.org/abs/2409.10372</guid>
<content:encoded><![CDATA[
arXiv:2409.10372v4 Announce Type: replace 
Abstract: Modern engineered systems increasingly involve complex sociotechnical environments where multiple agents, including humans and the emerging paradigm of agentic AI powered by large language models, must navigate social dilemmas that pit individual interests against collective welfare. As engineered systems evolve toward multi-agent architectures with autonomous LLM-based agents, traditional governance approaches using static rules or fixed network structures fail to address the dynamic uncertainties inherent in real-world operations. This paper presents a novel framework that integrates adaptive governance mechanisms directly into the design of sociotechnical systems through a unique separation of agent interaction networks from information flow networks. We introduce a system comprising strategic LLM-based system agents that engage in repeated interactions and a reinforcement learning-based governing agent that dynamically modulates information transparency. Unlike conventional approaches that require direct structural interventions or payoff modifications, our framework preserves agent autonomy while promoting cooperation through adaptive information governance. The governing agent learns to strategically adjust information disclosure at each timestep, determining what contextual or historical information each system agent can access. Experimental results demonstrate that this RL-based governance significantly enhances cooperation compared to static information-sharing baselines.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Last Iterate Convergence in Monotone Mean Field Games</title>
<link>https://arxiv.org/abs/2410.05127</link>
<guid>https://arxiv.org/abs/2410.05127</guid>
<content:encoded><![CDATA[
arXiv:2410.05127v4 Announce Type: replace 
Abstract: In the Lasry--Lions framework, Mean-Field Games (MFGs) model interactions among an infinite number of agents. However, existing algorithms either require strict monotonicity or only guarantee the convergence of averaged iterates, as in Fictitious Play in continuous time. We address this gap with the following theoretical result. First, we prove that the last-iterated policy of a proximal-point (PP) update with KL regularization converges to an equilibrium of MFG under non-strict monotonicity. Second, we see that each PP update is equivalent to finding the equilibria of a KL-regularized MFG. We then prove that this equilibrium can be found using Mirror Descent (MD) with an exponential last-iterate convergence rate. Building on these insights, we propose the Approximate Proximal-Point ($\mathtt{APP}$) algorithm, which approximately implements the PP update via a small number of MD steps. Numerical experiments on standard benchmarks confirm that the $\mathtt{APP}$ algorithm reliably converges to the unregularized mean-field equilibrium without time-averaging.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrajAgent: An LLM-Agent Framework for Trajectory Modeling via Large-and-Small Model Collaboration</title>
<link>https://arxiv.org/abs/2410.20445</link>
<guid>https://arxiv.org/abs/2410.20445</guid>
<content:encoded><![CDATA[
arXiv:2410.20445v4 Announce Type: replace 
Abstract: Trajectory modeling, which includes research on trajectory data pattern mining and future prediction, has widespread applications in areas such as life services, urban transportation, and public administration. Numerous methods have been proposed to address specific problems within trajectory modeling. However, the heterogeneity of data and the diversity of trajectory tasks make effective and reliable trajectory modeling an important yet highly challenging endeavor, even for domain experts. \fix In this paper, we propose \textit{TrajAgent}, a agent framework powered by large language models (LLMs), designed to facilitate robust and efficient trajectory modeling through automation modeling. This framework leverages and optimizes diverse specialized models to address various trajectory modeling tasks across different datasets effectively. \unfix~In \textit{TrajAgent}, we first develop \textit{UniEnv}, an execution environment with a unified data and model interface, to support the execution and training of various models. Building on \textit{UniEnv}, we introduce an agentic workflow designed for automatic trajectory modeling across various trajectory tasks and data. Furthermore, we introduce collaborative learning schema between LLM-based agents and small speciallized models, to enhance the performance of the whole framework effectively. Extensive experiments on four tasks using four real-world datasets demonstrate the effectiveness of \textit{TrajAgent} in automated trajectory modeling, achieving a performance improvement of \fix 2.38\%-69.91\% \unfix over baseline methods. The codes and data can be accessed via https://github.com/tsinghua-fib-lab/TrajAgent.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized EXTRA stochastic gradient Langevin dynamics</title>
<link>https://arxiv.org/abs/2412.01993</link>
<guid>https://arxiv.org/abs/2412.01993</guid>
<content:encoded><![CDATA[
arXiv:2412.01993v2 Announce Type: replace 
Abstract: Langevin algorithms are popular Markov Chain Monte Carlo methods for Bayesian learning, particularly when the aim is to sample from the posterior distribution of a parametric model, given the input data and the prior distribution over the model parameters. Their stochastic versions such as stochastic gradient Langevin dynamics (SGLD) allow iterative learning based on randomly sampled mini-batches of large datasets and are scalable to large datasets. However, when data is decentralized across a network of agents subject to communication and privacy constraints, standard SGLD algorithms cannot be applied. Instead, we employ decentralized SGLD (DE-SGLD) algorithms, where Bayesian learning is performed collaboratively by a network of agents without sharing individual data. Nonetheless, existing DE-SGLD algorithms induce a bias at every agent that can negatively impact performance; this bias persists even when using full batches and is attributable to network effects. Motivated by the EXTRA algorithm and its generalizations for decentralized optimization, we propose the generalized EXTRA stochastic gradient Langevin dynamics, which eliminates this bias in the full-batch setting. Moreover, we show that, in the mini-batch setting, our algorithm provides performance bounds that significantly improve upon those of standard DE-SGLD algorithms in the literature. Our numerical results also demonstrate the efficiency of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Naturalness in LLM-Generated Utterances through Disfluency Insertion</title>
<link>https://arxiv.org/abs/2412.12710</link>
<guid>https://arxiv.org/abs/2412.12710</guid>
<content:encoded><![CDATA[
arXiv:2412.12710v2 Announce Type: replace 
Abstract: Disfluencies are a natural feature of spontaneous human speech but are typically absent from the outputs of Large Language Models (LLMs). This absence can diminish the perceived naturalness of synthesized speech, which is an important criteria when building conversational agents that aim to mimick human behaviours. We show how the insertion of disfluencies can alleviate this shortcoming. The proposed approach involves (1) fine-tuning an LLM with Low-Rank Adaptation (LoRA) to incorporate various types of disfluencies into LLM-generated utterances and (2) synthesizing those utterances using a text-to-speech model that supports the generation of speech phenomena such as disfluencies. We evaluated the quality of the generated speech across two metrics: intelligibility and perceived spontaneity. We demonstrate through a user study that the insertion of disfluencies significantly increase the perceived spontaneity of the generated speech. This increase came, however, along with a slight reduction in intelligibility.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving the Unsolvable: Translating Case Law in Hong Kong</title>
<link>https://arxiv.org/abs/2501.09444</link>
<guid>https://arxiv.org/abs/2501.09444</guid>
<content:encoded><![CDATA[
arXiv:2501.09444v3 Announce Type: replace 
Abstract: This paper addresses the challenges translating case law under Hong Kong's bilingual legal system. It highlights the initial success of translating all written statutes into Chinese before the 1997 handover, a task mandated by the Basic Law. The effort involved significant collaboration among legal, linguistic, and translation experts, resulting in a comprehensive and culturally appropriate bilingual legal system. However, translating case law remains a significant challenge due to the sheer volume and continuous growth of judicial decisions. The paper critiques the governments and judiciarys sporadic and uncoordinated efforts to translate case law, contrasting it with the thorough approach previously taken for statute translation. Although the government acknowledges the importance of legal bilingualism, it lacks a sustainable strategy for translating case law. The Judiciarys position that translating all judgments is unnecessary, unrealistic, and not cost-effectiveis analyzed and critiqued for its impact on legal transparency and public trust. A proposed solution involves leveraging machine translation technology through a human-machine interactive translation platform, which undergoes two major transitions. Initially based on a neural model, the platform transitions to using a large language model for improved translation accuracy. Furthermore, it evolves from a single-agent system to a multi-agent system, incorporating Translator, Annotator, and Proofreader agents. This multi-agent approach, supported by a grant, aims to facilitate efficient, high-quality translation of judicial judgments by integrating advanced artificial intelligence and continuous feedback mechanisms, thus better meeting the needs of a bilingual legal system.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Decision-Making in Tree-Like Multi-Agent Games with Transfers</title>
<link>https://arxiv.org/abs/2501.19388</link>
<guid>https://arxiv.org/abs/2501.19388</guid>
<content:encoded><![CDATA[
arXiv:2501.19388v3 Announce Type: replace 
Abstract: The widespread deployment of Machine Learning systems everywhere raises challenges, such as dealing with interactions or competition between multiple learners. In that goal, we study multi-agent sequential decision-making by considering principal-agent interactions in a tree structure. In this problem, the reward of a player is influenced by the actions of her children, who are all self-interested and non-cooperative, hence the complexity of making good decisions. Our main finding is that it is possible to steer all the players towards the globally optimal set of actions by simply allowing single-step transfers between them. A transfer is established between a principal and one of her agents: the principal actually offers the proposed payment if the agent picks the recommended action. The analysis poses specific challenges due to the intricate interactions between the nodes of the tree and the propagation of the regret within this tree. Considering a bandit setup, we propose algorithmic solutions for the players to end up being no-regret with respect to the optimal pair of actions and incentives. In the long run, allowing transfers between players makes them act as if they were collaborating together, although they remain self-interested non-cooperative: transfers restore efficiency.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-AI Collaboration: Trade-offs Between Performance and Preferences</title>
<link>https://arxiv.org/abs/2503.00248</link>
<guid>https://arxiv.org/abs/2503.00248</guid>
<content:encoded><![CDATA[
arXiv:2503.00248v2 Announce Type: replace 
Abstract: Despite the growing interest in collaborative AI, designing systems that seamlessly integrate human input remains a major challenge. In this study, we developed a task to systematically examine human preferences for collaborative agents. We created and evaluated five collaborative AI agents with strategies that differ in the manner and degree they adapt to human actions. Participants interacted with a subset of these agents, evaluated their perceived traits, and selected their preferred agent. We used a Bayesian model to understand how agents' strategies influence the Human-AI team performance, AI's perceived traits, and the factors shaping human-preferences in pairwise agent comparisons. Our results show that agents who are more considerate of human actions are preferred over purely performance-maximizing agents. Moreover, we show that such human-centric design can improve the likability of AI collaborators without reducing performance. We find evidence for inequality-aversion effects being a driver of human choices, suggesting that people prefer collaborative agents which allow them to meaningfully contribute to the team. Taken together, these findings demonstrate how collaboration with AI can benefit from development efforts which include both subjective and objective metrics.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory Injection Attacks on LLM Agents via Query-Only Interaction</title>
<link>https://arxiv.org/abs/2503.03704</link>
<guid>https://arxiv.org/abs/2503.03704</guid>
<content:encoded><![CDATA[
arXiv:2503.03704v3 Announce Type: replace 
Abstract: Agents powered by large language models (LLMs) have demonstrated strong capabilities in a wide range of complex, real-world applications. However, LLM agents with a compromised memory bank may easily produce harmful outputs when the past records retrieved for demonstration are malicious. In this paper, we propose a novel Memory INJection Attack, MINJA, without assuming that the attacker can directly modify the memory bank of the agent. The attacker injects malicious records into the memory bank by only interacting with the agent via queries and output observations. These malicious records are designed to elicit a sequence of malicious reasoning steps corresponding to a different target query during the agent's execution of the victim user's query. Specifically, we introduce a sequence of bridging steps to link victim queries to the malicious reasoning steps. During the memory injection, we propose an indication prompt that guides the agent to autonomously generate similar bridging steps, with a progressive shortening strategy that gradually removes the indication prompt, such that the malicious record will be easily retrieved when processing later victim queries. Our extensive experiments across diverse agents demonstrate the effectiveness of MINJA in compromising agent memory. With minimal requirements for execution, MINJA enables any user to influence agent memory, highlighting the risk.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Do Multi-Agent LLM Systems Fail?</title>
<link>https://arxiv.org/abs/2503.13657</link>
<guid>https://arxiv.org/abs/2503.13657</guid>
<content:encoded><![CDATA[
arXiv:2503.13657v3 Announce Type: replace 
Abstract: Despite enthusiasm for Multi-Agent LLM Systems (MAS), their performance gains on popular benchmarks are often minimal. This gap highlights a critical need for a principled understanding of why MAS fail. Addressing this question requires systematic identification and analysis of failure patterns. We introduce MAST-Data, a comprehensive dataset of 1600+ annotated traces collected across 7 popular MAS frameworks. MAST-Data is the first multi-agent system dataset to outline the failure dynamics in MAS for guiding the development of better future systems. To enable systematic classification of failures for MAST-Data, we build the first Multi-Agent System Failure Taxonomy (MAST). We develop MAST through rigorous analysis of 150 traces, guided closely by expert human annotators and validated by high inter-annotator agreement (kappa = 0.88). This process identifies 14 unique modes, clustered into 3 categories: (i) system design issues, (ii) inter-agent misalignment, and (iii) task verification. To enable scalable annotation, we develop an LLM-as-a-Judge pipeline with high agreement with human annotations. We leverage MAST and MAST-Data to analyze failure patterns across models (GPT4, Claude 3, Qwen2.5, CodeLlama) and tasks (coding, math, general agent), demonstrating improvement headrooms from better MAS design. Our analysis provides insights revealing that identified failures require more sophisticated solutions, highlighting a clear roadmap for future research. We publicly release our comprehensive dataset (MAST-Data), the MAST, and our LLM annotator to facilitate widespread research and development in MAS.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided Improvement</title>
<link>https://arxiv.org/abs/2503.16024</link>
<guid>https://arxiv.org/abs/2503.16024</guid>
<content:encoded><![CDATA[
arXiv:2503.16024v2 Announce Type: replace 
Abstract: Large language models (LLMs) have recently transformed from text-based assistants to autonomous agents capable of planning, reasoning, and iteratively improving their actions. While numerical reward signals and verifiers can effectively rank candidate actions, they often provide limited contextual guidance. In contrast, natural language feedback better aligns with the generative capabilities of LLMs, providing richer and more actionable suggestions. However, parsing and implementing this feedback effectively can be challenging for LLM-based agents. In this work, we introduce Critique-Guided Improvement (CGI), a novel two-player framework, comprising an actor model that explores an environment and a critic model that generates detailed nature language feedback. By training the critic to produce fine-grained assessments and actionable revisions, and the actor to utilize these critiques, our approach promotes more robust exploration of alternative strategies while avoiding local optima. Experiments in three interactive environments show that CGI outperforms existing baselines by a substantial margin. Notably, even a small critic model surpasses GPT-4 in feedback quality. The resulting actor achieves state-of-the-art performance, demonstrating the power of explicit iterative guidance to enhance decision-making in LLM-based agents.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations</title>
<link>https://arxiv.org/abs/2504.07830</link>
<guid>https://arxiv.org/abs/2504.07830</guid>
<content:encoded><![CDATA[
arXiv:2504.07830v3 Announce Type: replace 
Abstract: We present a novel, open-source social network simulation framework, MOSAIC, where generative language agents predict user behaviors such as liking, sharing, and flagging content. This simulation combines LLM agents with a directed social graph to analyze emergent deception behaviors and gain a better understanding of how users determine the veracity of online social content. By constructing user representations from diverse fine-grained personas, our system enables multi-agent simulations that model content dissemination and engagement dynamics at scale. Within this framework, we evaluate three different content moderation strategies with simulated misinformation dissemination, and we find that they not only mitigate the spread of non-factual content but also increase user engagement. In addition, we analyze the trajectories of popular content in our simulations, and explore whether simulation agents' articulated reasoning for their social interactions truly aligns with their collective engagement patterns. We open-source our simulation software to encourage further research within AI and social sciences.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Depth-Constrained ASV Navigation with Deep RL and Limited Sensing</title>
<link>https://arxiv.org/abs/2504.18253</link>
<guid>https://arxiv.org/abs/2504.18253</guid>
<content:encoded><![CDATA[
arXiv:2504.18253v3 Announce Type: replace 
Abstract: Autonomous Surface Vehicles (ASVs) play a crucial role in maritime operations, yet their navigation in shallow-water environments remains challenging due to dynamic disturbances and depth constraints. Traditional navigation strategies struggle with limited sensor information, making safe and efficient operation difficult. In this paper, we propose a reinforcement learning (RL) framework for ASV navigation under depth constraints, where the vehicle must reach a target while avoiding unsafe areas with only a single depth measurement per timestep from a downward-facing Single Beam Echosounder (SBES). To enhance environmental awareness, we integrate Gaussian Process (GP) regression into the RL framework, enabling the agent to progressively estimate a bathymetric depth map from sparse sonar readings. This approach improves decision-making by providing a richer representation of the environment. Furthermore, we demonstrate effective sim-to-real transfer, ensuring that trained policies generalize well to real-world aquatic conditions. Experimental results validate our method's capability to improve ASV navigation performance while maintaining safety in challenging shallow-water environments.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAGE: A Generic Framework for LLM Safety Evaluation</title>
<link>https://arxiv.org/abs/2504.19674</link>
<guid>https://arxiv.org/abs/2504.19674</guid>
<content:encoded><![CDATA[
arXiv:2504.19674v2 Announce Type: replace 
Abstract: As Large Language Models are rapidly deployed across diverse applications from healthcare to financial advice, safety evaluation struggles to keep pace. Current benchmarks focus on single-turn interactions with generic policies, failing to capture the conversational dynamics of real-world usage and the application-specific harms that emerge in context. Such potential oversights can lead to harms that go unnoticed in standard safety benchmarks and other current evaluation methodologies. To address these needs for robust AI safety evaluation, we introduce SAGE (Safety AI Generic Evaluation), an automated modular framework designed for customized and dynamic harm evaluations. SAGE employs prompted adversarial agents with diverse personalities based on the Big Five model, enabling system-aware multi-turn conversations that adapt to target applications and harm policies. We evaluate seven state-of-the-art LLMs across three applications and harm policies. Multi-turn experiments show that harm increases with conversation length, model behavior varies significantly when exposed to different user personalities and scenarios, and some models minimize harm via high refusal rates that reduce usefulness. We also demonstrate policy sensitivity within a harm category where tightening a child-focused sexual policy substantially increases measured defects across applications. These results motivate adaptive, policy-aware, and context-specific testing for safer real-world deployment.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking</title>
<link>https://arxiv.org/abs/2504.19940</link>
<guid>https://arxiv.org/abs/2504.19940</guid>
<content:encoded><![CDATA[
arXiv:2504.19940v2 Announce Type: replace 
Abstract: The growing spread of online misinformation has created an urgent need for scalable, reliable fact-checking solutions. Crowdsourced fact-checking - where non-experts evaluate claim veracity - offers a cost-effective alternative to expert verification, despite concerns about variability in quality and bias. Encouraged by promising results in certain contexts, major platforms such as X (formerly Twitter), Facebook, and Instagram have begun shifting from centralized moderation to decentralized, crowd-based approaches.
  In parallel, advances in Large Language Models (LLMs) have shown strong performance across core fact-checking tasks, including claim detection and evidence evaluation. However, their potential role in crowdsourced workflows remains unexplored. This paper investigates whether LLM-powered generative agents - autonomous entities that emulate human behavior and decision-making - can meaningfully contribute to fact-checking tasks traditionally reserved for human crowds.
  Using the protocol of La Barbera et al. (2024), we simulate crowds of generative agents with diverse demographic and ideological profiles. Agents retrieve evidence, assess claims along multiple quality dimensions, and issue final veracity judgments. Our results show that agent crowds outperform human crowds in truthfulness classification, exhibit higher internal consistency, and show reduced susceptibility to social and cognitive biases. Compared to humans, agents rely more systematically on informative criteria such as Accuracy, Precision, and Informativeness, suggesting a more structured decision-making process. Overall, our findings highlight the potential of generative agents as scalable, consistent, and less biased contributors to crowd-based fact-checking systems.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Understanding of Human-Robot Social Interactions through Multimodal Distillation</title>
<link>https://arxiv.org/abs/2505.06278</link>
<guid>https://arxiv.org/abs/2505.06278</guid>
<content:encoded><![CDATA[
arXiv:2505.06278v2 Announce Type: replace 
Abstract: There is a growing need for social robots and intelligent agents that can effectively interact with and support users. For the interactions to be seamless, the agents need to analyse social scenes and behavioural cues from their (robot's) perspective. Works that model human-agent interactions in social situations are few; and even those existing ones are computationally too intensive to be deployed in real time or perform poorly in real-world scenarios when only limited information is available. We propose a knowledge distillation framework that models social interactions through various multimodal cues, and yet is robust against incomplete and noisy information during inference. We train a teacher model with multimodal input (body, face and hand gestures, gaze, raw images) that transfers knowledge to a student model which relies solely on body pose. Extensive experiments on two publicly available human-robot interaction datasets demonstrate that our student model achieves an average accuracy gain of 14.75% over competitive baselines on multiple downstream social understanding tasks, even with up to 51% of its input being corrupted. The student model is also highly efficient - less than 1% in size of the teacher model in terms of parameters and its latency is 11.9% of the teacher model. Our code and related data are available at github.com/biantongfei/SocialEgoMobile.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis</title>
<link>https://arxiv.org/abs/2505.13227</link>
<guid>https://arxiv.org/abs/2505.13227</guid>
<content:encoded><![CDATA[
arXiv:2505.13227v3 Announce Type: replace 
Abstract: Graphical user interface (GUI) grounding, the ability to map natural language instructions to specific actions on graphical user interfaces, remains a critical bottleneck in computer use agent development. Current benchmarks oversimplify grounding tasks as short referring expressions, failing to capture the complexity of real-world interactions that require software commonsense, layout understanding, and fine-grained manipulation capabilities. To address these limitations, we introduce OSWorld-G, a comprehensive benchmark comprising 564 finely annotated samples across diverse task types including text matching, element recognition, layout understanding, and precise manipulation. Additionally, we synthesize and release the largest computer use grounding dataset Jedi, which contains 4 million examples through multi-perspective decoupling of tasks. Our multi-scale models trained on Jedi demonstrate its effectiveness by outperforming existing approaches on ScreenSpot-v2, ScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved grounding with Jedi directly enhances agentic capabilities of general foundation models on complex computer tasks, improving from 5% to 27% on OSWorld. Through detailed ablation studies, we identify key factors contributing to grounding performance and verify that combining specialized data for different interface elements enables compositional generalization to novel interfaces. All benchmark, data, checkpoints, and code are open-sourced and available at https://osworld-grounding.github.io.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Security of Gradient Tracking Algorithms Against Malicious Agents</title>
<link>https://arxiv.org/abs/2505.14473</link>
<guid>https://arxiv.org/abs/2505.14473</guid>
<content:encoded><![CDATA[
arXiv:2505.14473v2 Announce Type: replace 
Abstract: Consensus algorithms are fundamental to multi-agent distributed optimization, and their security under adversarial conditions is an active area of research. While prior works primarily establish conditions for successful global consensus under attack, little is known about system behavior when these conditions are violated. This paper addresses this gap by investigating the robustness of the Wang--Elia algorithm, which is a robust to noise version of gradient tracking algorithm, in the presence of malicious agents. We consider a network of agents collaboratively minimizing a global cost function, where a subset of agents may transmit faulty information to disrupt consensus. To quantify resilience, we formulate a security metric as an optimization problem, which is rooted in centralized attack detection literature. We provide a tractable reformulation of the optimization problem, and derive conditions under which the metric becomes unbounded, identifying undetectable attack signals that reveal inherent vulnerabilities. To facilitate design and analysis, we propose a well-posed variant of the metric and propose design methods to enhance network robustness against stealthy adversarial attacks. Numerical examples demonstrate the effectiveness of the proposed framework to enhance the resilience of multi-agent distributed optimization.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions</title>
<link>https://arxiv.org/abs/2505.14668</link>
<guid>https://arxiv.org/abs/2505.14668</guid>
<content:encoded><![CDATA[
arXiv:2505.14668v2 Announce Type: replace 
Abstract: Recent advances in Large Language Models (LLMs) have propelled intelligent agents from reactive responses to proactive support. While promising, existing proactive agents either rely exclusively on observations from enclosed environments (e.g., desktop UIs) with direct LLM inference or employ rule-based proactive notifications, leading to suboptimal user intent understanding and limited functionality for proactive service. In this paper, we introduce ContextAgent, the first context-aware proactive agent that incorporates extensive sensory contexts surrounding humans to enhance the proactivity of LLM agents. ContextAgent first extracts multi-dimensional contexts from massive sensory perceptions on wearables (e.g., video and audio) to understand user intentions. ContextAgent then leverages the sensory contexts and personas from historical data to predict the necessity for proactive services. When proactive assistance is needed, ContextAgent further automatically calls the necessary tools to assist users unobtrusively. To evaluate this new task, we curate ContextAgentBench, the first benchmark for evaluating context-aware proactive LLM agents, covering 1,000 samples across nine daily scenarios and twenty tools. Experiments on ContextAgentBench show that ContextAgent outperforms baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive predictions and tool calling, respectively. We hope our research can inspire the development of more advanced, human-centric, proactive AI assistants. The code and dataset are publicly available at https://github.com/openaiotlab/ContextAgent.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents</title>
<link>https://arxiv.org/abs/2505.23671</link>
<guid>https://arxiv.org/abs/2505.23671</guid>
<content:encoded><![CDATA[
arXiv:2505.23671v3 Announce Type: replace 
Abstract: Developing high-performance software is a complex task that requires specialized expertise. We introduce GSO, a benchmark for evaluating language models' capabilities in developing high-performance software. We develop an automated pipeline that generates and executes performance tests to analyze repository commit histories to identify 102 challenging optimization tasks across 10 codebases, spanning diverse domains and programming languages. An agent is provided with a codebase and performance test as a precise specification, and tasked to improve the runtime efficiency, which is measured against the expert developer optimization. Our quantitative evaluation reveals that leading SWE-Agents struggle significantly, achieving less than 5% success rate, with limited improvements even with inference-time scaling. Our qualitative analysis identifies key failure modes, including difficulties with low-level languages, practicing lazy optimization strategies, and challenges in accurately localizing bottlenecks. We release the code and artifacts of our benchmark along with agent trajectories to enable future research.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Language Models for Semantic Navigation and Manipulation in an Aerial-Ground Robotic System</title>
<link>https://arxiv.org/abs/2506.05020</link>
<guid>https://arxiv.org/abs/2506.05020</guid>
<content:encoded><![CDATA[
arXiv:2506.05020v3 Announce Type: replace 
Abstract: Heterogeneous multirobot systems show great potential in complex tasks requiring coordinated hybrid cooperation. However, existing methods that rely on static or task-specific models often lack generalizability across diverse tasks and dynamic environments. This highlights the need for generalizable intelligence that can bridge high-level reasoning with low-level execution across heterogeneous agents. To address this, we propose a hierarchical multimodal framework that integrates a prompted large language model (LLM) with a fine-tuned vision-language model (VLM). At the system level, the LLM performs hierarchical task decomposition and constructs a global semantic map, while the VLM provides semantic perception and object localization, where the proposed GridMask significantly enhances the VLM's spatial accuracy for reliable fine-grained manipulation. The aerial robot leverages this global map to generate semantic paths and guide the ground robot's local navigation and manipulation, ensuring robust coordination even in target-absent or ambiguous scenarios. We validate the framework through extensive simulation and real-world experiments on long-horizon object arrangement tasks, demonstrating zero-shot adaptability, robust semantic navigation, and reliable manipulation in dynamic environments. To the best of our knowledge, this work presents the first heterogeneous aerial-ground robotic system that integrates VLM-based perception with LLM-driven reasoning for global high-level task planning and execution.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture-of-Experts Meets In-Context Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.05426</link>
<guid>https://arxiv.org/abs/2506.05426</guid>
<content:encoded><![CDATA[
arXiv:2506.05426v2 Announce Type: replace 
Abstract: In-context reinforcement learning (ICRL) has emerged as a promising paradigm for adapting RL agents to downstream tasks through prompt conditioning. However, two notable challenges remain in fully harnessing in-context learning within RL domains: the intrinsic multi-modality of the state-action-reward data and the diverse, heterogeneous nature of decision tasks. To tackle these challenges, we propose T2MIR (Token- and Task-wise MoE for In-context RL), an innovative framework that introduces architectural advances of mixture-of-experts (MoE) into transformer-based decision models. T2MIR substitutes the feedforward layer with two parallel layers: a token-wise MoE that captures distinct semantics of input tokens across multiple modalities, and a task-wise MoE that routes diverse tasks to specialized experts for managing a broad task distribution with alleviated gradient conflicts. To enhance task-wise routing, we introduce a contrastive learning method that maximizes the mutual information between the task and its router representation, enabling more precise capture of task-relevant information. The outputs of two MoE components are concatenated and fed into the next layer. Comprehensive experiments show that T2MIR significantly facilitates in-context learning capacity and outperforms various types of baselines. We bring the potential and promise of MoE to ICRL, offering a simple and scalable architectural enhancement to advance ICRL one step closer toward achievements in language and vision communities. Our code is available at https://github.com/NJU-RL/T2MIR.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Responsible AI: Advances in Safety, Fairness, and Accountability of Autonomous Systems</title>
<link>https://arxiv.org/abs/2506.10192</link>
<guid>https://arxiv.org/abs/2506.10192</guid>
<content:encoded><![CDATA[
arXiv:2506.10192v2 Announce Type: replace 
Abstract: Ensuring responsible use of artificial intelligence (AI) has become imperative as autonomous systems increasingly influence critical societal domains. However, the concept of trustworthy AI remains broad and multi-faceted. This thesis advances knowledge in the safety, fairness, transparency, and accountability of AI systems. In safety, we extend classical deterministic shielding techniques to become resilient against delayed observations, enabling practical deployment in real-world conditions. We also implement both deterministic and probabilistic safety shields into simulated autonomous vehicles to prevent collisions with road users, validating the use of these techniques in realistic driving simulators. We introduce fairness shields, a novel post-processing approach to enforce group fairness in sequential decision-making settings over finite and periodic time horizons. By optimizing intervention costs while strictly ensuring fairness constraints, this method efficiently balances fairness with minimal interference. For transparency and accountability, we propose a formal framework for assessing intentional behaviour in probabilistic decision-making agents, introducing quantitative metrics of agency and intention quotient. We use these metrics to propose a retrospective analysis of intention, useful for determining responsibility when autonomous systems cause unintended harm. Finally, we unify these contributions through the ``reactive decision-making'' framework, providing a general formalization that consolidates previous approaches. Collectively, the advancements presented contribute practically to the realization of safer, fairer, and more accountable AI systems, laying the foundations for future research in trustworthy AI.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Can Model-Free Reinforcement Learning be Enough for Thinking?</title>
<link>https://arxiv.org/abs/2506.17124</link>
<guid>https://arxiv.org/abs/2506.17124</guid>
<content:encoded><![CDATA[
arXiv:2506.17124v2 Announce Type: replace 
Abstract: Recent work on large language models has demonstrated the use of model-free reinforcement learning (RL) to train reasoning-like capabilities. The emergence of "thinking" through model-free RL is interesting as thinking actions neither produce reward nor change the external world state to one where the agent is more likely to get reward. This paper seeks to build a domain-independent understanding of when model-free RL will lead to such "thinking" as a strategy for reward maximization. To build this understanding, we first introduce a theoretical model which we call a thought Markov decision process (MDP). Thought MDPs minimally extend the classical MDP model to include an abstract notion of thought state and thought action. Using the thought MDP model, we prove the importance of policy initialization in determining whether or not thinking emerges and show formally that thought actions are equivalent to the agent choosing to perform a step of policy improvement before continuing to act. We then show that open-source LLMs satisfy the conditions that our theory predicts are necessary for model-free RL to produce thinking-like behavior. Finally, we hypothesize sufficient conditions that would enable thinking to be learned outside of language generation and introduce a toy domain where a combination of multi-task pre-training and designated thought actions enable more data-efficient RL compared to non-thinking agents.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents</title>
<link>https://arxiv.org/abs/2506.21669</link>
<guid>https://arxiv.org/abs/2506.21669</guid>
<content:encoded><![CDATA[
arXiv:2506.21669v2 Announce Type: replace 
Abstract: Self-evolution, the ability of agents to autonomously improve their reasoning and behavior, is essential for the embodied domain with long-horizon, real-world tasks. Despite current advancements in reinforcement fine-tuning (RFT) showing strong performance in enhancing reasoning in LLMs, its potential to enable self-evolving embodied intelligence with multi-modal interactions remains largely unexplored. Specifically, reinforcement fine-tuning faces two fundamental obstacles in embodied settings: (i) the lack of accessible intermediate rewards in multi-step reasoning tasks limits effective learning signals, and (ii) reliance on hand-crafted reward functions restricts generalization to novel tasks and environments. To address these challenges, we present Self-Evolving Embodied Agents-R1, SEEA-R1, the first RFT framework designed for enabling the self-evolving capabilities of embodied agents. Specifically, to convert sparse delayed rewards into denser intermediate signals that improve multi-step reasoning, we propose Tree-based group relative policy optimization (Tree-GRPO) integrates Monte Carlo Tree Search into GRPO. To generalize reward estimation across tasks and scenes, supporting autonomous adaptation and reward-driven self-evolution, we further introduce Multi-modal Generative Reward Model (MGRM). To holistically evaluate the effectiveness of SEEA-R1, we evaluate on the ALFWorld benchmark, surpassing state-of-the-art methods with scores of 85.07% (textual) and 46.27% (multi-modal), outperforming prior models including GPT-4o. SEEA-R1 also achieves scores of 80.3% (textual) and 44.03% (multi-modal) without ground truth reward, surpassing all open-source baselines and highlighting its scalability as a self-evolving embodied agent. Additional experiments and qualitative analysis further support the potential of SEEA-R1 for future research in scalable embodied intelligence.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curious Causality-Seeking Agents Learn Meta Causal World</title>
<link>https://arxiv.org/abs/2506.23068</link>
<guid>https://arxiv.org/abs/2506.23068</guid>
<content:encoded><![CDATA[
arXiv:2506.23068v3 Announce Type: replace 
Abstract: When building a world model, a common assumption is that the environment has a single, unchanging underlying causal rule, like applying Newton's laws to every situation. In reality, what appears as a drifting causal mechanism is often the manifestation of a fixed underlying mechanism seen through a narrow observational window. This brings about a problem that, when building a world model, even subtle shifts in policy or environment states can alter the very observed causal mechanisms. In this work, we introduce the \textbf{Meta-Causal Graph} as world models, a minimal unified representation that efficiently encodes the transformation rules governing how causal structures shift across different latent world states. A single Meta-Causal Graph is composed of multiple causal subgraphs, each triggered by meta state, which is in the latent state space. Building on this representation, we introduce a \textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta states that trigger each subgraph, (2) discover the corresponding causal relationships by agent curiosity-driven intervention policy, and (3) iteratively refine the Meta-Causal Graph through ongoing curiosity-driven exploration and agent experiences. Experiments on both synthetic tasks and a challenging robot arm manipulation task demonstrate that our method robustly captures shifts in causal dynamics and generalizes effectively to previously unseen contexts.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving</title>
<link>https://arxiv.org/abs/2507.06229</link>
<guid>https://arxiv.org/abs/2507.06229</guid>
<content:encoded><![CDATA[
arXiv:2507.06229v5 Announce Type: replace 
Abstract: AI agent frameworks operate in isolation, forcing agents to rediscover solutions and repeat mistakes across different systems. Despite valuable problem-solving experiences accumulated by frameworks like smolagents, OpenHands, and OWL, this knowledge remains trapped within individual systems, preventing the emergence of collective intelligence. Current memory systems focus on individual agents or framework-specific demonstrations, failing to enable cross-architecture knowledge transfer. We introduce AGENT KB, a universal memory infrastructure enabling seamless experience sharing across heterogeneous agent frameworks without retraining. AGENT KB aggregates trajectories into a structured knowledge base and serves lightweight APIs. At inference time, hybrid retrieval operates through two stages: planning seeds agents with cross-domain workflows, while feedback applies targeted diagnostic fixes. A disagreement gate ensures retrieved knowledge enhances rather than disrupts reasoning, addressing knowledge interference in cross-framework transfer. We validate AGENT KB across major frameworks on GAIA, Humanity's Last Exam, GPQA, and SWE-bench. Results show substantial improvements across diverse model families: compared to baseline pass@1, smolagents with AGENT KB achieve up to 18.7pp gains at pass@3 (55.2% -> 73.9%), while OpenHands improves 4.0pp on SWE-bench pass@1 (24.3% -> 28.3%). Similar improvements are observed across all base model families. Ablations confirm that hybrid retrieval and feedback stages are essential, with automatically generated experiences matching manual curation. This establishes the foundation for collective agent intelligence through shared memory infrastructures.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ground-Compose-Reinforce: Grounding Language in Agentic Behaviours using Limited Data</title>
<link>https://arxiv.org/abs/2507.10741</link>
<guid>https://arxiv.org/abs/2507.10741</guid>
<content:encoded><![CDATA[
arXiv:2507.10741v2 Announce Type: replace 
Abstract: Grounding language in perception and action is a key challenge when building situated agents that can interact with humans, or other agents, via language. In the past, addressing this challenge has required manually designing the language grounding or curating massive datasets that associate language with the environment. We propose Ground-Compose-Reinforce, an end-to-end, neurosymbolic framework for training RL agents directly from high-level task specifications--without manually designed reward functions or other domain-specific oracles, and without massive datasets. These task specifications take the form of Reward Machines, automata-based representations that capture high-level task structure and are in some cases autoformalizable from natural language. Critically, we show that Reward Machines can be grounded using limited data by exploiting compositionality. Experiments in a custom Meta-World domain with only 350 labelled pretraining trajectories show that our framework faithfully elicits complex behaviours from high-level specifications--including behaviours that never appear in pretraining--while non-compositional approaches fail.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors</title>
<link>https://arxiv.org/abs/2507.15550</link>
<guid>https://arxiv.org/abs/2507.15550</guid>
<content:encoded><![CDATA[
arXiv:2507.15550v2 Announce Type: replace 
Abstract: Evaluating the scientific discovery capabilities of large language model based agents, particularly how they cope with varying environmental complexity and utilize prior knowledge, requires specialized benchmarks currently lacking in the landscape. To address this gap, we introduce \textsc{PhysGym}, a novel benchmark suite and simulation platform for rigorously assessing LLM-based scientific reasoning in interactive physics environments. \textsc{PhysGym}'s primary contribution lies in its sophisticated control over the level of prior knowledge provided to the agent. This allows researchers to dissect agent performance along axes including the complexity of the problem and the prior knowledge levels. The benchmark comprises a suite of interactive simulations, where agents must actively probe environments, gather data sequentially under constraints and formulate hypotheses about underlying physical laws. \textsc{PhysGym} provides standardized evaluation protocols and metrics for assessing hypothesis accuracy and model fidelity. We demonstrate the benchmark's utility by presenting results from baseline LLMs, showcasing its ability to differentiate capabilities based on varying priors and task complexity.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Model Guided Reinforcement Learning in Quantitative Trading</title>
<link>https://arxiv.org/abs/2508.02366</link>
<guid>https://arxiv.org/abs/2508.02366</guid>
<content:encoded><![CDATA[
arXiv:2508.02366v3 Announce Type: replace 
Abstract: Algorithmic trading requires short-term tactical decisions consistent with long-term financial objectives. Reinforcement Learning (RL) has been applied to such problems, but adoption is limited by myopic behaviour and opaque policies. Large Language Models (LLMs) offer complementary strategic reasoning and multi-modal signal interpretation when guided by well-structured prompts. This paper proposes a hybrid framework in which LLMs generate high-level trading strategies to guide RL agents. We evaluate (i) the economic rationale of LLM-generated strategies through expert review, and (ii) the performance of LLM-guided agents against unguided RL baselines using Sharpe Ratio (SR) and Maximum Drawdown (MDD). Empirical results indicate that LLM guidance improves both return and risk metrics relative to standard RL.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Is Your AI Agent Buying? Evaluation, Implications and Emerging Questions for Agentic E-Commerce</title>
<link>https://arxiv.org/abs/2508.02630</link>
<guid>https://arxiv.org/abs/2508.02630</guid>
<content:encoded><![CDATA[
arXiv:2508.02630v2 Announce Type: replace 
Abstract: Online marketplaces will be transformed by autonomous AI agents acting on behalf of consumers. Rather than humans browsing and clicking, AI agents can parse webpages or interact through APIs to evaluate products, and transact. This raises a fundamental question: what do AI agents buy-and why? We develop ACES, a sandbox environment that pairs a platform-agnostic agent with a fully programmable mock marketplace to study this. We first explore aggregate choices, revealing that modal choices can differ across models, with AI agents sometimes concentrating on a few products, raising competition questions. We then analyze the drivers of choices through rationality checks and randomized experiments on product positions and listing attributes. Models show sizeable and heterogeneous position effects: all favor the top row, yet different models prefer different columns, undermining the assumption of a universal ``top'' rank. They penalize sponsored tags, reward endorsements, and sensitivities to price, ratings, and reviews are directionally as expected, but vary sharply across models. Finally, we find that a seller-side agent that makes minor tweaks to product descriptions can deliver substantial market-share gains by targeting AI buyer preferences. Our findings reveal how AI agents behave in e-commerce, and surface concrete seller strategy, platform design, and regulatory questions.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MI9: An Integrated Runtime Governance Framework for Agentic AI</title>
<link>https://arxiv.org/abs/2508.03858</link>
<guid>https://arxiv.org/abs/2508.03858</guid>
<content:encoded><![CDATA[
arXiv:2508.03858v3 Announce Type: replace 
Abstract: Agentic AI systems capable of reasoning, planning, and executing actions present fundamentally distinct governance challenges compared to traditional AI models. Unlike conventional AI, these systems exhibit emergent and unexpected behaviors during runtime, introducing novel agent-related risks that cannot be fully anticipated through pre-deployment governance alone. To address this critical gap, we introduce MI9, the first fully integrated runtime governance framework designed specifically for safety and alignment of agentic AI systems. MI9 introduces real-time controls through six integrated components: agency-risk index, agent-semantic telemetry capture, continuous authorization monitoring, Finite-State-Machine (FSM)-based conformance engines, goal-conditioned drift detection, and graduated containment strategies. Operating transparently across heterogeneous agent architectures, MI9 enables the systematic, safe, and responsible deployment of agentic systems in production environments where conventional governance approaches fall short, providing the foundational infrastructure for safe agentic AI deployment at scale. Detailed analysis through a diverse set of scenarios demonstrates MI9's systematic coverage of governance challenges that existing approaches fail to address, establishing the technical foundation for comprehensive agentic AI oversight.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL</title>
<link>https://arxiv.org/abs/2508.07976</link>
<guid>https://arxiv.org/abs/2508.07976</guid>
<content:encoded><![CDATA[
arXiv:2508.07976v4 Announce Type: replace 
Abstract: Recent advancements in LLM-based agents have demonstrated remarkable capabilities in handling complex, knowledge-intensive tasks by integrating external tools. Among diverse choices of tools, search tools play a pivotal role in accessing vast external knowledge. However, open-source agents still fall short of achieving expert-level Search Intelligence, the ability to resolve ambiguous queries, generate precise searches, analyze results, and conduct thorough exploration. Existing approaches fall short in scalability, efficiency, and data quality. For example, small turn limits in existing online RL methods, e.g. <=10, restrict complex strategy learning. This paper introduces ASearcher, an open-source project for large-scale RL training of search agents. Our key contributions include: (1) Scalable fully asynchronous RL training that enables long-horizon search while maintaining high training efficiency. (2) A prompt-based LLM agent that autonomously synthesizes high-quality and challenging QAs, creating a large-scale QA dataset. Through RL training, our prompt-based QwQ-32B agent achieves substantial improvements, with 78.0% and 34.3% Avg@4 gains on xBench and GAIA, respectively. Notably, our agent exhibits extreme long-horizon search, with tool calls exceeding 100 turns and output tokens exceeding 400k during training time. With a simple agent design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 51.1 on xBench and 58.7 on GAIA, surpassing existing open-source 32B agents. Finally, we also show that ASearcher-Web-QwQ could achieve performance of commercial systems using external summary tool in a zero-shot transfer manner and test-time search. We open-source our models, training data, and codes in https://github.com/inclusionAI/ASearcher.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Agents for Generating Microservice-based Applications: how complex is your specification?</title>
<link>https://arxiv.org/abs/2508.20119</link>
<guid>https://arxiv.org/abs/2508.20119</guid>
<content:encoded><![CDATA[
arXiv:2508.20119v2 Announce Type: replace 
Abstract: In this paper we evaluate the capabilities of LLM Agents in generating code for real-world problems. Specifically, we explore code synthesis for microservice-based applications, a widely used architectural pattern for building applications. We define a standard template for specifying these applications, and we propose a metric for scoring the difficulty of a specification. The higher the score, the more difficult it is to generate code for the specification. Our experimental results show that agents using strong LLMs (like GPT-3o-mini) do fairly well on medium difficulty specifications but do poorly on those of higher difficulty levels. This is due to more intricate business logic, a greater use of external services, database integration and inclusion of non-functional capabilities such as authentication. We analyzed the errors in LLM-synthesized code and report on the key challenges LLM Agents face in generating code for these specifications. Finally, we show that using a fine-grained approach to code generation improves the correctness of the generated code.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Compute Architecture and Evolution Trends</title>
<link>https://arxiv.org/abs/2508.21394</link>
<guid>https://arxiv.org/abs/2508.21394</guid>
<content:encoded><![CDATA[
arXiv:2508.21394v2 Announce Type: replace 
Abstract: The focus of AI development has shifted from academic research to practical applications. However, AI development faces numerous challenges at various levels. This article will attempt to analyze the opportunities and challenges of AI from several different perspectives using a structured approach. This article proposes a seven-layer model for AI compute architecture, including Physical Layer, Link Layer, Neural Network Layer, Context Layer, Agent Layer, Orchestrator Layer, and Application Layer, from bottom to top. It also explains how AI computing has evolved into this 7-layer architecture through the three-stage evolution on large-scale language models (LLMs). For each layer, we describe the development trajectory and key technologies. In Layers 1 and 2 we discuss AI computing issues and the impact of Scale-Up and Scale-Out strategies on computing architecture. In Layer 3 we explore two different development paths for LLMs. In Layer 4 we discuss the impact of contextual memory on LLMs and compares it to traditional processor memory. In Layers 5 to 7 we discuss the trends of AI agents and explore the issues in evolution from a single AI agent to an AI-based ecosystem, and their impact on the AI industry. Furthermore, AI development involves not only technical challenges but also the economic issues to build self-sustainable ecosystem. This article analyzes the internet industry to provide predictions on the future trajectory of AI development.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Complexity Trap: Simple Observation Masking Is as Efficient as LLM Summarization for Agent Context Management</title>
<link>https://arxiv.org/abs/2508.21433</link>
<guid>https://arxiv.org/abs/2508.21433</guid>
<content:encoded><![CDATA[
arXiv:2508.21433v3 Announce Type: replace 
Abstract: Large Language Model (LLM)-based agents solve complex tasks through iterative reasoning, exploration, and tool-use, a process that can result in long, expensive context histories. While state-of-the-art Software Engineering (SE) agents like OpenHands or Cursor use LLM-based summarization to tackle this issue, it is unclear whether the increased complexity offers tangible performance benefits compared to simply omitting older observations. We present a systematic comparison of these approaches within SWE-agent on SWE-bench Verified across five diverse model configurations. Moreover, we show initial evidence of our findings generalizing to the OpenHands agent scaffold. We find that a simple environment observation masking strategy halves cost relative to the raw agent while matching, and sometimes slightly exceeding, the solve rate of LLM summarization. Additionally, we introduce a novel hybrid approach that further reduces costs by 7% and 11% compared to just observation masking or LLM summarization, respectively. Our findings raise concerns regarding the trend towards pure LLM summarization and provide initial evidence of untapped cost reductions by pushing the efficiency-effectiveness frontier. We release code and data for reproducibility.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chatbot To Help Patients Understand Their Health</title>
<link>https://arxiv.org/abs/2509.05818</link>
<guid>https://arxiv.org/abs/2509.05818</guid>
<content:encoded><![CDATA[
arXiv:2509.05818v2 Announce Type: replace 
Abstract: Patients must possess the knowledge necessary to actively participate in their care. We present NoteAid-Chatbot, a conversational AI that promotes patient understanding via a novel 'learning as conversation' framework, built on a multi-agent large language model (LLM) and reinforcement learning (RL) setup without human-labeled data. NoteAid-Chatbot was built on a lightweight LLaMA 3.2 3B model trained in two stages: initial supervised fine-tuning on conversational data synthetically generated using medical conversation strategies, followed by RL with rewards derived from patient understanding assessments in simulated hospital discharge scenarios. Our evaluation, which includes comprehensive human-aligned assessments and case studies, demonstrates that NoteAid-Chatbot exhibits key emergent behaviors critical for patient education, such as clarity, relevance, and structured dialogue, even though it received no explicit supervision for these attributes. Our results show that even simple Proximal Policy Optimization (PPO)-based reward modeling can successfully train lightweight, domain-specific chatbots to handle multi-turn interactions, incorporate diverse educational strategies, and meet nuanced communication objectives. Our Turing test demonstrates that NoteAid-Chatbot surpasses non-expert human. Although our current focus is on healthcare, the framework we present illustrates the feasibility and promise of applying low-cost, PPO-based RL to realistic, open-ended conversational domains, broadening the applicability of RL-based alignment methods.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and Self-Reflective Agent</title>
<link>https://arxiv.org/abs/2509.20414</link>
<guid>https://arxiv.org/abs/2509.20414</guid>
<content:encoded><![CDATA[
arXiv:2509.20414v2 Announce Type: replace 
Abstract: Indoor scene synthesis has become increasingly important with the rise of Embodied AI, which requires 3D environments that are not only visually realistic but also physically plausible and functionally diverse. While recent approaches have advanced visual fidelity, they often remain constrained to fixed scene categories, lack sufficient object-level detail and physical consistency, and struggle to align with complex user instructions. In this work, we present SceneWeaver, a reflective agentic framework that unifies diverse scene synthesis paradigms through tool-based iterative refinement. At its core, SceneWeaver employs a language model-based planner to select from a suite of extensible scene generation tools, ranging from data-driven generative models to visual- and LLM-based methods, guided by self-evaluation of physical plausibility, visual realism, and semantic alignment with user input. This closed-loop reason-act-reflect design enables the agent to identify semantic inconsistencies, invoke targeted tools, and update the environment over successive iterations. Extensive experiments on both common and open-vocabulary room types demonstrate that SceneWeaver not only outperforms prior methods on physical, visual, and semantic metrics, but also generalizes effectively to complex scenes with diverse instructions, marking a step toward general-purpose 3D environment generation. Project website: https://scene-weaver.github.io/.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Compatible Skill Incremental Learning via Lazy Learning Interface</title>
<link>https://arxiv.org/abs/2509.20612</link>
<guid>https://arxiv.org/abs/2509.20612</guid>
<content:encoded><![CDATA[
arXiv:2509.20612v2 Announce Type: replace 
Abstract: Skill Incremental Learning (SIL) is the process by which an embodied agent expands and refines its skill set over time by leveraging experience gained through interaction with its environment or by the integration of additional data. SIL facilitates efficient acquisition of hierarchical policies grounded in reusable skills for downstream tasks. However, as the skill repertoire evolves, it can disrupt compatibility with existing skill-based policies, limiting their reusability and generalization. In this work, we propose SIL-C, a novel framework that ensures skill-policy compatibility, allowing improvements in incrementally learned skills to enhance the performance of downstream policies without requiring policy re-training or structural adaptation. SIL-C employs a bilateral lazy learning-based mapping technique to dynamically align the subtask space referenced by policies with the skill space decoded into agent behaviors. This enables each subtask, derived from the policy's decomposition of a complex task, to be executed by selecting an appropriate skill based on trajectory distribution similarity. We evaluate SIL-C across diverse SIL scenarios and demonstrate that it maintains compatibility between evolving skills and downstream policies while ensuring efficiency throughout the learning process.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Emergence of Social Science of Large Language Models</title>
<link>https://arxiv.org/abs/2509.24877</link>
<guid>https://arxiv.org/abs/2509.24877</guid>
<content:encoded><![CDATA[
arXiv:2509.24877v2 Announce Type: replace 
Abstract: The social science of large language models (LLMs) examines how these systems evoke mind attributions, interact with one another, and transform human activity and institutions. We conducted a systematic review of 270 studies, combining text embeddings, unsupervised clustering and topic modeling to build a computational taxonomy. Three domains emerge organically across the reviewed literature. LLM as Social Minds examines whether and when models display behaviors that elicit attributions of cognition, morality and bias, while addressing challenges such as test leakage and surface cues. LLM Societies examines multi-agent settings where interaction protocols, architectures and mechanism design shape coordination, norms, institutions and collective epistemic processes. LLM-Human Interactions examines how LLMs reshape tasks, learning, trust, work and governance, and how risks arise at the human-AI interface. This taxonomy provides a reproducible map of a fragmented field, clarifies evidentiary standards across levels of analysis, and highlights opportunities for cumulative progress in the social science of artificial intelligence.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expandable Decision-Making States for Multi-Agent Deep Reinforcement Learning in Soccer Tactical Analysis</title>
<link>https://arxiv.org/abs/2510.00480</link>
<guid>https://arxiv.org/abs/2510.00480</guid>
<content:encoded><![CDATA[
arXiv:2510.00480v2 Announce Type: replace 
Abstract: Invasion team sports such as soccer produce a high-dimensional, strongly coupled state space as many players continuously interact on a shared field, challenging quantitative tactical analysis. Traditional rule-based analyses are intuitive, while modern predictive machine learning models often perform pattern-matching without explicit agent representations. The problem we address is how to build player-level agent models from data, whose learned values and policies are both tactically interpretable and robust across heterogeneous data sources. Here, we propose Expandable Decision-Making States (EDMS), a semantically enriched state representation that augments raw positions and velocities with relational variables (e.g., scoring of space, pass, and score), combined with an action-masking scheme that gives on-ball and off-ball agents distinct decision sets. Compared to prior work, EDMS maps learned value functions and action policies to human-interpretable tactical concepts (e.g., marking pressure, passing lanes, ball accessibility) instead of raw coordinate features, and aligns agent choices with the rules of play. In the experiments, EDMS with action masking consistently reduced both action-prediction loss and temporal-difference (TD) error compared to the baseline. Qualitative case studies and Q-value visualizations further indicate that EDMS highlights high-risk, high-reward tactical patterns (e.g., fast counterattacks and defensive breakthroughs). We also integrated our approach into an open-source library and demonstrated compatibility with multiple commercial and open datasets, enabling cross-provider evaluation and reproducible experiments.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM4Cell: A Survey of Large Language and Agentic Models for Single-Cell Biology</title>
<link>https://arxiv.org/abs/2510.07793</link>
<guid>https://arxiv.org/abs/2510.07793</guid>
<content:encoded><![CDATA[
arXiv:2510.07793v2 Announce Type: replace 
Abstract: Large language models (LLMs) and emerging agentic frameworks are beginning to transform single-cell biology by enabling natural-language reasoning, generative annotation, and multimodal data integration. However, progress remains fragmented across data modalities, architectures, and evaluation standards. LLM4Cell presents the first unified survey of 58 foundation and agentic models developed for single-cell research, spanning RNA, ATAC, multi-omic, and spatial modalities. We categorize these methods into five families-foundation, text-bridge, spatial, multimodal, epigenomic, and agentic-and map them to eight key analytical tasks including annotation, trajectory and perturbation modeling, and drug-response prediction. Drawing on over 40 public datasets, we analyze benchmark suitability, data diversity, and ethical or scalability constraints, and evaluate models across 10 domain dimensions covering biological grounding, multi-omics alignment, fairness, privacy, and explainability. By linking datasets, models, and evaluation domains, LLM4Cell provides the first integrated view of language-driven single-cell intelligence and outlines open challenges in interpretability, standardization, and trustworthy model development.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sketch2BIM: A Multi-Agent Human-AI Collaborative Pipeline to Convert Hand-Drawn Floor Plans to 3D BIM</title>
<link>https://arxiv.org/abs/2510.20838</link>
<guid>https://arxiv.org/abs/2510.20838</guid>
<content:encoded><![CDATA[
arXiv:2510.20838v1 Announce Type: new 
Abstract: This study introduces a human-in-the-loop pipeline that converts unscaled, hand-drawn floor plan sketches into semantically consistent 3D BIM models. The workflow leverages multimodal large language models (MLLMs) within a multi-agent framework, combining perceptual extraction, human feedback, schema validation, and automated BIM scripting. Initially, sketches are iteratively refined into a structured JSON layout of walls, doors, and windows. Later, these layouts are transformed into executable scripts that generate 3D BIM models. Experiments on ten diverse floor plans demonstrate strong convergence: openings (doors, windows) are captured with high reliability in the initial pass, while wall detection begins around 83% and achieves near-perfect alignment after a few feedback iterations. Across all categories, precision, recall, and F1 scores remain above 0.83, and geometric errors (RMSE, MAE) progressively decrease to zero through feedback corrections. This study demonstrates how MLLM-driven multi-agent reasoning can make BIM creation accessible to both experts and non-experts using only freehand sketches.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>\textsc{autoresearcher}: Automating Knowledge-Grounded and Transparent Research Ideation with Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2510.20844</link>
<guid>https://arxiv.org/abs/2510.20844</guid>
<content:encoded><![CDATA[
arXiv:2510.20844v1 Announce Type: new 
Abstract: Effective research relies on organizing extensive information and stimulating novel solutions. Agentic systems have recently emerged as a promising tool to automate literature-based ideation. However, current systems often remain black-box. Their outputs may appear plausible but weakly grounded, with limited transparency or control for researchers. Our work introduces \textsc{autoresearcher}, a multi-agent demo system for knowledge-grounded and transparent ideation. Specifically, \textsc{autoresearcher} integrates meticulously designed four stages into a unified framework: (A) Structured Knowledge Curation, (B) Diversified Idea Generation, (C) Multi-stage Idea Selection, and (D) Expert Panel Review \& Synthesis. Different from prior pipelines, our system not only exposes intermediate reasoning states, execution logs, and tunable agents for inspections, but also enables the generation of hypotheses that are both diverse and evidence-aligned. Our design is also domain-agnostic: as long as literature sources exist, the same pipeline can be instantiated in any scientific field. As an illustrative case, we demonstrate \textsc{autoresearcher} on a graph-mining case study ($k$-truss breaking problem), where it generates distinct, plausible hypotheses with evidence and critiques. A live demo and source code are available at https://github.com/valleysprings/AutoResearcher.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cultural Alien Sampler: Open-ended art generation balancing originality and coherence</title>
<link>https://arxiv.org/abs/2510.20849</link>
<guid>https://arxiv.org/abs/2510.20849</guid>
<content:encoded><![CDATA[
arXiv:2510.20849v1 Announce Type: new 
Abstract: In open-ended domains like art, autonomous agents must generate ideas that are both original and internally coherent, yet current Large Language Models (LLMs) either default to familiar cultural patterns or sacrifice coherence when pushed toward novelty. We address this by introducing the Cultural Alien Sampler (CAS), a concept-selection method that explicitly separates compositional fit from cultural typicality. CAS uses two GPT-2 models fine-tuned on WikiArt concepts: a Concept Coherence Model that scores whether concepts plausibly co-occur within artworks, and a Cultural Context Model that estimates how typical those combinations are within individual artists' bodies of work. CAS targets combinations that are high in coherence and low in typicality, yielding ideas that maintain internal consistency while deviating from learned conventions and embedded cultural context. In a human evaluation (N = 100), our approach outperforms random selection and GPT-4o baselines and achieves performance comparable to human art students in both perceived originality and harmony. Additionally, a quantitative study shows that our method produces more diverse outputs and explores a broader conceptual space than its GPT-4o counterpart, demonstrating that artificial cultural alienness can unlock creative potential in autonomous agents.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CC-GRMAS: A Multi-Agent Graph Neural System for Spatiotemporal Landslide Risk Assessment in High Mountain Asia</title>
<link>https://arxiv.org/abs/2510.20875</link>
<guid>https://arxiv.org/abs/2510.20875</guid>
<content:encoded><![CDATA[
arXiv:2510.20875v1 Announce Type: new 
Abstract: Landslides are a growing climate induced hazard with severe environmental and human consequences, particularly in high mountain Asia. Despite increasing access to satellite and temporal datasets, timely detection and disaster response remain underdeveloped and fragmented. This work introduces CC-GRMAS, a framework leveraging a series of satellite observations and environmental signals to enhance the accuracy of landslide forecasting. The system is structured around three interlinked agents Prediction, Planning, and Execution, which collaboratively enable real time situational awareness, response planning, and intervention. By incorporating local environmental factors and operationalizing multi agent coordination, this approach offers a scalable and proactive solution for climate resilient disaster preparedness across vulnerable mountainous terrains.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shoot First, Ask Questions Later? Building Rational Agents that Explore and Act Like People</title>
<link>https://arxiv.org/abs/2510.20886</link>
<guid>https://arxiv.org/abs/2510.20886</guid>
<content:encoded><![CDATA[
arXiv:2510.20886v1 Announce Type: new 
Abstract: Many high-stakes applications of AI require forming data-driven hypotheses and making targeted guesses; e.g., in scientific and diagnostic settings. Given limited resources, to what extent do agents based on language models (LMs) act rationally? We develop methods to benchmark and enhance agentic information-seeking, drawing on insights from human behavior. First, we introduce a strategic decision-oriented dialogue task called Collaborative Battleship, in which a partially-informed Captain must balance exploration (asking questions) and action (taking shots), while a fully-informed Spotter must provide accurate answers under an information bottleneck. Compared to human players (N=42), we find that LM agents struggle to ground answers in context, generate informative questions, and select high-value actions. Next, to address these gaps, we develop novel Monte Carlo inference strategies for LMs based on principles from Bayesian Experimental Design (BED). For Spotter agents, our approach boosts accuracy by up to 14.7% absolute over LM-only baselines; for Captain agents, it raises expected information gain (EIG) by up to 0.227 bits (94.2% of the achievable noise ceiling). Combined, these components yield sharper targeting (+0.303-0.374 F1), and enable weaker LMs, such as Llama-4-Scout, to outperform both humans (8% -> 82% win rate) and frontier models (0% -> 67% win rate vs. GPT-5) at ~1% of GPT-5's cost. We replicate these findings on Guess Who? where our methods significantly boost accuracy (+28.3-42.4 p.p.), demonstrating their general applicability for building rational information-seeking agents.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Scalable Oversight with Collaborative Multi-Agent Debate in Error Detection</title>
<link>https://arxiv.org/abs/2510.20963</link>
<guid>https://arxiv.org/abs/2510.20963</guid>
<content:encoded><![CDATA[
arXiv:2510.20963v1 Announce Type: new 
Abstract: Accurate detection of errors in large language models (LLM) responses is central to the success of scalable oversight, or providing effective supervision to superhuman intelligence. Yet, self-diagnosis is often unreliable on complex tasks unless aided by reliable external feedback. Multi-agent debate (MAD) seems to be a natural alternative to external feedback: multiple LLMs provide complementary perspectives and cross-checks for error detection. However, prior MAD protocols frame debate as a zero-sum game, where the debaters compete to win the game instead of seeking the truth. Consequently, it leads to debate hacking: debaters tend to mislead the judge by misinterpreting the task or presenting overconfident claims, which introduce more mistakes and underperform single-agent methods. To mitigate the issue, we introduce a new collaborative MAD protocol, termed ColMAD, that reframes MAD as a non-zero sum game. Specifically, ColMAD encourages multiple agents to criticize each other in a supportive way, such that they can complement the missing points of each other. Therefore, the judge agent can make a more informative conclusion based on more comprehensive evidence. Empirically, we show that ColMAD significantly outperforms previous competitive MAD by 19% and brings non-trivial improvements over single-agent methods in error detection.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentArcEval: An Architecture Evaluation Method for Foundation Model based Agents</title>
<link>https://arxiv.org/abs/2510.21031</link>
<guid>https://arxiv.org/abs/2510.21031</guid>
<content:encoded><![CDATA[
arXiv:2510.21031v1 Announce Type: new 
Abstract: The emergence of foundation models (FMs) has enabled the development of highly capable and autonomous agents, unlocking new application opportunities across a wide range of domains. Evaluating the architecture of agents is particularly important as the architectural decisions significantly impact the quality attributes of agents given their unique characteristics, including compound architecture, autonomous and non-deterministic behaviour, and continuous evolution. However, these traditional methods fall short in addressing the evaluation needs of agent architecture due to the unique characteristics of these agents. Therefore, in this paper, we present AgentArcEval, a novel agent architecture evaluation method designed specially to address the complexities of FM-based agent architecture and its evaluation. Moreover, we present a catalogue of agent-specific general scenarios, which serves as a guide for generating concrete scenarios to design and evaluate the agent architecture. We demonstrate the usefulness of AgentArcEval and the catalogue through a case study on the architecture evaluation of a real-world tax copilot, named Luna.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low Cost, Fair, and Representative Committees in a Metric Space</title>
<link>https://arxiv.org/abs/2510.21039</link>
<guid>https://arxiv.org/abs/2510.21039</guid>
<content:encoded><![CDATA[
arXiv:2510.21039v1 Announce Type: new 
Abstract: We study the problem of selecting a representative committee of $k$ agents from a collection of $n$ agents in a common metric space. This problem is related to choosing $k$ facilities in facility location and $k$-median problems. However, unlike in more traditional facility location where each agent only cares about the closest selected facility, in the settings we consider each agent desires that all selected committee members are close to them. More precisely, we look at the sum objective, in which the goal is to minimize the total distance from all agents to all members of the chosen committee. We show that it is always possible to find a committee which is both low-cost according to this objective, and also fair according to many existing notions of fairness and proportionality defined for clustering settings. Moreover, we introduce a new desirable axiom for representative committees we call NORP, which prevents over-representation of any subset of agents. While all existing algorithms for fair committee selection do not satisfy this intuitive property, we provide new algorithms which form simultaneously low-cost, fair, and NORP solutions, thus showing that it is always possible to form low-cost, fair, and representative committees for our settings.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Questions to Queries: An AI-powered Multi-Agent Framework for Spatial Text-to-SQL</title>
<link>https://arxiv.org/abs/2510.21045</link>
<guid>https://arxiv.org/abs/2510.21045</guid>
<content:encoded><![CDATA[
arXiv:2510.21045v1 Announce Type: new 
Abstract: The complexity of Structured Query Language (SQL) and the specialized nature of geospatial functions in tools like PostGIS present significant barriers to non-experts seeking to analyze spatial data. While Large Language Models (LLMs) offer promise for translating natural language into SQL (Text-to-SQL), single-agent approaches often struggle with the semantic and syntactic complexities of spatial queries. To address this, we propose a multi-agent framework designed to accurately translate natural language questions into spatial SQL queries. The framework integrates several innovative components, including a knowledge base with programmatic schema profiling and semantic enrichment, embeddings for context retrieval, and a collaborative multi-agent pipeline as its core. This pipeline comprises specialized agents for entity extraction, metadata retrieval, query logic formulation, SQL generation, and a review agent that performs programmatic and semantic validation of the generated SQL to ensure correctness (self-verification). We evaluate our system using both the non-spatial KaggleDBQA benchmark and a new, comprehensive SpatialQueryQA benchmark that includes diverse geometry types, predicates, and three levels of query complexity. On KaggleDBQA, the system achieved an overall accuracy of 81.2% (221 out of 272 questions) after the review agent's review and corrections. For spatial queries, the system achieved an overall accuracy of 87.7% (79 out of 90 questions), compared with 76.7% without the review agent. Beyond accuracy, results also show that in some instances the system generates queries that are more semantically aligned with user intent than those in the benchmarks. This work makes spatial analysis more accessible, and provides a robust, generalizable foundation for spatial Text-to-SQL systems, advancing the development of autonomous GIS.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Multi-Class Selection with Group Fairness Guarantee</title>
<link>https://arxiv.org/abs/2510.21055</link>
<guid>https://arxiv.org/abs/2510.21055</guid>
<content:encoded><![CDATA[
arXiv:2510.21055v1 Announce Type: new 
Abstract: We study the online multi-class selection problem with group fairness guarantees, where limited resources must be allocated to sequentially arriving agents. Our work addresses two key limitations in the existing literature. First, we introduce a novel lossless rounding scheme that ensures the integral algorithm achieves the same expected performance as any fractional solution. Second, we explicitly address the challenges introduced by agents who belong to multiple classes. To this end, we develop a randomized algorithm based on a relax-and-round framework. The algorithm first computes a fractional solution using a resource reservation approach -- referred to as the set-aside mechanism -- to enforce fairness across classes. The subsequent rounding step preserves these fairness guarantees without degrading performance. Additionally, we propose a learning-augmented variant that incorporates untrusted machine-learned predictions to better balance fairness and efficiency in practical settings.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soft Instruction De-escalation Defense</title>
<link>https://arxiv.org/abs/2510.21057</link>
<guid>https://arxiv.org/abs/2510.21057</guid>
<content:encoded><![CDATA[
arXiv:2510.21057v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly deployed in agentic systems that interact with an external environment; this makes them susceptible to prompt injections when dealing with untrusted data. To overcome this limitation, we propose SIC (Soft Instruction Control)-a simple yet effective iterative prompt sanitization loop designed for tool-augmented LLM agents. Our method repeatedly inspects incoming data for instructions that could compromise agent behavior. If such content is found, the malicious content is rewritten, masked, or removed, and the result is re-evaluated. The process continues until the input is clean or a maximum iteration limit is reached; if imperative instruction-like content remains, the agent halts to ensure security. By allowing multiple passes, our approach acknowledges that individual rewrites may fail but enables the system to catch and correct missed injections in later steps. Although immediately useful, worst-case analysis shows that SIC is not infallible; strong adversary can still get a 15% ASR by embedding non-imperative workflows. This nonetheless raises the bar.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ESCORT: Efficient Stein-variational and Sliced Consistency-Optimized Temporal Belief Representation for POMDPs</title>
<link>https://arxiv.org/abs/2510.21107</link>
<guid>https://arxiv.org/abs/2510.21107</guid>
<content:encoded><![CDATA[
arXiv:2510.21107v1 Announce Type: new 
Abstract: In Partially Observable Markov Decision Processes (POMDPs), maintaining and updating belief distributions over possible underlying states provides a principled way to summarize action-observation history for effective decision-making under uncertainty. As environments grow more realistic, belief distributions develop complexity that standard mathematical models cannot accurately capture, creating a fundamental challenge in maintaining representational accuracy. Despite advances in deep learning and probabilistic modeling, existing POMDP belief approximation methods fail to accurately represent complex uncertainty structures such as high-dimensional, multi-modal belief distributions, resulting in estimation errors that lead to suboptimal agent behaviors. To address this challenge, we present ESCORT (Efficient Stein-variational and sliced Consistency-Optimized Representation for Temporal beliefs), a particle-based framework for capturing complex, multi-modal distributions in high-dimensional belief spaces. ESCORT extends SVGD with two key innovations: correlation-aware projections that model dependencies between state dimensions, and temporal consistency constraints that stabilize updates while preserving correlation structures. This approach retains SVGD's attractive-repulsive particle dynamics while enabling accurate modeling of intricate correlation patterns. Unlike particle filters prone to degeneracy or parametric methods with fixed representational capacity, ESCORT dynamically adapts to belief landscape complexity without resampling or restrictive distributional assumptions. We demonstrate ESCORT's effectiveness through extensive evaluations on both POMDP domains and synthetic multi-modal distributions of varying dimensionality, where it consistently outperforms state-of-the-art methods in terms of belief approximation accuracy and downstream decision quality.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confounding Robust Deep Reinforcement Learning: A Causal Approach</title>
<link>https://arxiv.org/abs/2510.21110</link>
<guid>https://arxiv.org/abs/2510.21110</guid>
<content:encoded><![CDATA[
arXiv:2510.21110v1 Announce Type: new 
Abstract: A key task in Artificial Intelligence is learning effective policies for controlling agents in unknown environments to optimize performance measures. Off-policy learning methods, like Q-learning, allow learners to make optimal decisions based on past experiences. This paper studies off-policy learning from biased data in complex and high-dimensional domains where \emph{unobserved confounding} cannot be ruled out a priori. Building on the well-celebrated Deep Q-Network (DQN), we propose a novel deep reinforcement learning algorithm robust to confounding biases in observed data. Specifically, our algorithm attempts to find a safe policy for the worst-case environment compatible with the observations. We apply our method to twelve confounded Atari games, and find that it consistently dominates the standard DQN in all games where the observed input to the behavioral and target policies mismatch and unobserved confounders exist.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments</title>
<link>https://arxiv.org/abs/2510.21111</link>
<guid>https://arxiv.org/abs/2510.21111</guid>
<content:encoded><![CDATA[
arXiv:2510.21111v1 Announce Type: new 
Abstract: Visual reasoning in multimodal large language models (MLLMs) has primarily been studied in static, fully observable settings, limiting their effectiveness in real-world environments where information is often incomplete due to occlusion or limited field of view. Humans, in contrast, actively explore and interact with their environment-moving, examining, and manipulating objects-to gather information through a closed-loop process integrating perception, reasoning, and action. Inspired by this human capability, we introduce the Active Visual Reasoning (AVR) task, extending visual reasoning to partially observable, interactive environments. AVR necessitates agents to: (1) actively acquire information via sequential physical actions, (2) integrate observations across multiple steps for coherent reasoning, and (3) dynamically adjust decisions based on evolving visual feedback. To rigorously evaluate AVR, we introduce CLEVR-AVR, a simulation benchmark featuring multi-round interactive environments designed to assess both reasoning correctness and information-gathering efficiency. We present AVR-152k, a large-scale dataset that offers rich Chain-of-Thought (CoT) annotations detailing iterative reasoning for uncertainty identification, action-conditioned information gain prediction, and information-maximizing action selection, crucial for training agents in a higher-order Markov Decision Process. Building on this, we develop PhysVLM-AVR, an MLLM achieving state-of-the-art performance on CLEVR-AVR, embodied reasoning (OpenEQA, RoboVQA), and passive visual reasoning (GeoMath, Geometry30K). Our analysis also reveals that current embodied MLLMs, despite detecting information incompleteness, struggle to actively acquire and integrate new information through interaction, highlighting a fundamental gap in active reasoning capabilities.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAO-AI: Evaluating Collective Decision-Making through Agentic AI in Decentralized Governance</title>
<link>https://arxiv.org/abs/2510.21117</link>
<guid>https://arxiv.org/abs/2510.21117</guid>
<content:encoded><![CDATA[
arXiv:2510.21117v1 Announce Type: new 
Abstract: This paper presents a first empirical study of agentic AI as autonomous decision-makers in decentralized governance. Using more than 3K proposals from major protocols, we build an agentic AI voter that interprets proposal contexts, retrieves historical deliberation data, and independently determines its voting position. The agent operates within a realistic financial simulation environment grounded in verifiable blockchain data, implemented through a modular composable program (MCP) workflow that defines data flow and tool usage via Agentics framework. We evaluate how closely the agent's decisions align with the human and token-weighted outcomes, uncovering strong alignments measured by carefully designed evaluation metrics. Our findings demonstrate that agentic AI can augment collective decision-making by producing interpretable, auditable, and empirically grounded signals in realistic DAO governance settings. The study contributes to the design of explainable and economically rigorous AI agents for decentralized financial systems.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Hierarchical Skill Learning via Object-Centric Representation</title>
<link>https://arxiv.org/abs/2510.21121</link>
<guid>https://arxiv.org/abs/2510.21121</guid>
<content:encoded><![CDATA[
arXiv:2510.21121v1 Announce Type: new 
Abstract: We present Generalizable Hierarchical Skill Learning (GSL), a novel framework for hierarchical policy learning that significantly improves policy generalization and sample efficiency in robot manipulation. One core idea of GSL is to use object-centric skills as an interface that bridges the high-level vision-language model and the low-level visual-motor policy. Specifically, GSL decomposes demonstrations into transferable and object-canonicalized skill primitives using foundation models, ensuring efficient low-level skill learning in the object frame. At test time, the skill-object pairs predicted by the high-level agent are fed to the low-level module, where the inferred canonical actions are mapped back to the world frame for execution. This structured yet flexible design leads to substantial improvements in sample efficiency and generalization of our method across unseen spatial arrangements, object appearances, and task compositions. In simulation, GSL trained with only 3 demonstrations per task outperforms baselines trained with 30 times more data by 15.5 percent on unseen tasks. In real-world experiments, GSL also surpasses the baseline trained with 10 times more data.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Digital Contrast CT Pulmonary Angiography Synthesis from Non-contrast CT for Pulmonary Vascular Disease</title>
<link>https://arxiv.org/abs/2510.21140</link>
<guid>https://arxiv.org/abs/2510.21140</guid>
<content:encoded><![CDATA[
arXiv:2510.21140v1 Announce Type: new 
Abstract: Computed Tomography Pulmonary Angiography (CTPA) is the reference standard for diagnosing pulmonary vascular diseases such as Pulmonary Embolism (PE) and Chronic Thromboembolic Pulmonary Hypertension (CTEPH). However, its reliance on iodinated contrast agents poses risks including nephrotoxicity and allergic reactions, particularly in high-risk patients. This study proposes a method to generate Digital Contrast CTPA (DCCTPA) from Non-Contrast CT (NCCT) scans using a cascaded synthesizer based on Cycle-Consistent Generative Adversarial Networks (CycleGAN). Totally retrospective 410 paired CTPA and NCCT scans were obtained from three centers. The model was trained and validated internally on 249 paired images. Extra dataset that comprising 161 paired images was as test set for model generalization evaluation and downstream clinical tasks validation. Compared with state-of-the-art (SOTA) methods, the proposed method achieved the best comprehensive performance by evaluating quantitative metrics (For validation, MAE: 156.28, PSNR: 20.71 and SSIM: 0.98; For test, MAE: 165.12, PSNR: 20.27 and SSIM: 0.98) and qualitative visualization, demonstrating valid vessel enhancement, superior image fidelity and structural preservation. The approach was further applied to downstream tasks of pulmonary vessel segmentation and vascular quantification. On the test set, the average Dice, clDice, and clRecall of artery and vein pulmonary segmentation was 0.70, 0.71, 0.73 and 0.70, 0.72, 0.75 respectively, all markedly improved compared with NCCT inputs.\@ Inter-class Correlation Coefficient (ICC) for vessel volume between DCCTPA and CTPA was significantly better than that between NCCT and CTPA (Average ICC : 0.81 vs 0.70), indicating effective vascular enhancement in DCCTPA, especially for small vessels.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Principal-Agent Contract Design via Gradient-Based Optimization</title>
<link>https://arxiv.org/abs/2510.21177</link>
<guid>https://arxiv.org/abs/2510.21177</guid>
<content:encoded><![CDATA[
arXiv:2510.21177v1 Announce Type: new 
Abstract: We study a bilevel \emph{max-max} optimization framework for principal-agent contract design, in which a principal chooses incentives to maximize utility while anticipating the agent's best response. This problem, central to moral hazard and contract theory, underlies applications ranging from market design to delegated portfolio management, hedge fund fee structures, and executive compensation. While linear-quadratic models such as Holmstr"om-Milgrom admit closed-form solutions, realistic environments with nonlinear utilities, stochastic dynamics, or high-dimensional actions generally do not.
  We introduce a generic algorithmic framework that removes this reliance on closed forms. Our method adapts modern machine learning techniques for bilevel optimization -- using implicit differentiation with conjugate gradients (CG) -- to compute hypergradients efficiently through Hessian-vector products, without ever forming or inverting Hessians. In benchmark CARA-Normal (Constant Absolute Risk Aversion with Gaussian distribution of uncertainty) environments, the approach recovers known analytical optima and converges reliably from random initialization. More broadly, because it is matrix-free, variance-reduced, and problem-agnostic, the framework extends naturally to complex nonlinear contracts where closed-form solutions are unavailable, such as sigmoidal wage schedules (logistic pay), relative-performance/tournament compensation with common shocks, multi-task contracts with vector actions and heterogeneous noise, and CARA-Poisson count models with $\mathbb{E}[X\mid a]=e^{a}$. This provides a new computational tool for contract design, enabling systematic study of models that have remained analytically intractable.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instance-Adaptive Hypothesis Tests with Heterogeneous Agents</title>
<link>https://arxiv.org/abs/2510.21178</link>
<guid>https://arxiv.org/abs/2510.21178</guid>
<content:encoded><![CDATA[
arXiv:2510.21178v1 Announce Type: new 
Abstract: We study hypothesis testing over a heterogeneous population of strategic agents with private information. Any single test applied uniformly across the population yields statistical error that is sub-optimal relative to the performance of an oracle given access to the private information. We show how it is possible to design menus of statistical contracts that pair type-optimal tests with payoff structures, inducing agents to self-select according to their private information. This separating menu elicits agent types and enables the principal to match the oracle performance even without a priori knowledge of the agent type. Our main result fully characterizes the collection of all separating menus that are instance-adaptive, matching oracle performance for an arbitrary population of heterogeneous agents. We identify designs where information elicitation is essentially costless, requiring negligible additional expense relative to a single-test benchmark, while improving statistical performance. Our work establishes a connection between proper scoring rules and menu design, showing how the structure of the hypothesis test constrains the elicitable information. Numerical examples illustrate the geometry of separating menus and the improvements they deliver in error trade-offs. Overall, our results connect statistical decision theory with mechanism design, demonstrating how heterogeneity and strategic participation can be harnessed to improve efficiency in hypothesis testing.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Green Hydrogen under Uncertainty: Evaluating Power-to-X Strategies Using Agent-Based Simulation and Multi-Criteria Decision Framework</title>
<link>https://arxiv.org/abs/2510.21179</link>
<guid>https://arxiv.org/abs/2510.21179</guid>
<content:encoded><![CDATA[
arXiv:2510.21179v1 Announce Type: new 
Abstract: The transition toward net-zero energy systems requires scalable and cost-effective deployment of Power-to-X technologies, particularly green hydrogen production. Despite increasing investments, a critical research gap remains in dynamically assessing how different operational strategies affect the feasibility of hydrogen production under real-world energy market conditions. Most existing studies rely on static, techno-economic models and overlook actor interactions, infrastructure limitations, and regulatory complexity. This paper presents a novel modeling framework that integrates agent-based simulation with multi-criteria decision-making to evaluate green hydrogen production strategies using co-located wind and solar generation. Three operational strategies - grid-only, on-site-only, and hybrid - are applied across three electrolyzer capacity levels (10 MW, 50 MW, and 100 MW) within a Danish case study. Real electricity tariffs, emissions factors, and market data are used to simulate technical, economic, and environmental performance indicators. The results show that hybrid strategies consistently outperform grid-only configurations in terms of cost and emissions while maintaining stable hydrogen output. Although on-site-only strategies minimize emissions and costs, they fail to meet fixed production demands. This framework offers novel scientific contributions by modeling dynamic actor interactions and integrating system performance evaluation into strategic planning. Practically, it provides actionable insights for energy planners and policymakers designing resilient and efficient Power-to-X systems in renewable-rich contexts.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Simulations with Large Language Model Risk Utopian Illusion</title>
<link>https://arxiv.org/abs/2510.21180</link>
<guid>https://arxiv.org/abs/2510.21180</guid>
<content:encoded><![CDATA[
arXiv:2510.21180v1 Announce Type: new 
Abstract: Reliable simulation of human behavior is essential for explaining, predicting, and intervening in our society. Recent advances in large language models (LLMs) have shown promise in emulating human behaviors, interactions, and decision-making, offering a powerful new lens for social science studies. However, the extent to which LLMs diverge from authentic human behavior in social contexts remains underexplored, posing risks of misinterpretation in scientific studies and unintended consequences in real-world applications. Here, we introduce a systematic framework for analyzing LLMs' behavior in social simulation. Our approach simulates multi-agent interactions through chatroom-style conversations and analyzes them across five linguistic dimensions, providing a simple yet effective method to examine emergent social cognitive biases. We conduct extensive experiments involving eight representative LLMs across three families. Our findings reveal that LLMs do not faithfully reproduce genuine human behavior but instead reflect overly idealized versions of it, shaped by the social desirability bias. In particular, LLMs show social role bias, primacy effect, and positivity bias, resulting in "Utopian" societies that lack the complexity and variability of real human interactions. These findings call for more socially grounded LLMs that capture the diversity of human social behavior.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DispatchMAS: Fusing taxonomy and artificial intelligence agents for emergency medical services</title>
<link>https://arxiv.org/abs/2510.21228</link>
<guid>https://arxiv.org/abs/2510.21228</guid>
<content:encoded><![CDATA[
arXiv:2510.21228v1 Announce Type: new 
Abstract: Objective: Emergency medical dispatch (EMD) is a high-stakes process challenged by caller distress, ambiguity, and cognitive load. Large Language Models (LLMs) and Multi-Agent Systems (MAS) offer opportunities to augment dispatchers. This study aimed to develop and evaluate a taxonomy-grounded, LLM-powered multi-agent system for simulating realistic EMD scenarios. Methods: We constructed a clinical taxonomy (32 chief complaints, 6 caller identities from MIMIC-III) and a six-phase call protocol. Using this framework, we developed an AutoGen-based MAS with Caller and Dispatcher Agents. The system grounds interactions in a fact commons to ensure clinical plausibility and mitigate misinformation. We used a hybrid evaluation framework: four physicians assessed 100 simulated cases for "Guidance Efficacy" and "Dispatch Effectiveness," supplemented by automated linguistic analysis (sentiment, readability, politeness). Results: Human evaluation, with substantial inter-rater agreement (Gwe's AC1 > 0.70), confirmed the system's high performance. It demonstrated excellent Dispatch Effectiveness (e.g., 94 % contacting the correct potential other agents) and Guidance Efficacy (advice provided in 91 % of cases), both rated highly by physicians. Algorithmic metrics corroborated these findings, indicating a predominantly neutral affective profile (73.7 % neutral sentiment; 90.4 % neutral emotion), high readability (Flesch 80.9), and a consistently polite style (60.0 % polite; 0 % impolite). Conclusion: Our taxonomy-grounded MAS simulates diverse, clinically plausible dispatch scenarios with high fidelity. Findings support its use for dispatcher training, protocol evaluation, and as a foundation for real-time decision support. This work outlines a pathway for safely integrating advanced AI agents into emergency response workflows.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Securing AI Agent Execution</title>
<link>https://arxiv.org/abs/2510.21236</link>
<guid>https://arxiv.org/abs/2510.21236</guid>
<content:encoded><![CDATA[
arXiv:2510.21236v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have evolved into AI agents that interact with external tools and environments to perform complex tasks. The Model Context Protocol (MCP) has become the de facto standard for connecting agents with such resources, but security has lagged behind: thousands of MCP servers execute with unrestricted access to host systems, creating a broad attack surface. In this paper, we introduce AgentBound, the first access control framework for MCP servers. AgentBound combines a declarative policy mechanism, inspired by the Android permission model, with a policy enforcement engine that contains malicious behavior without requiring MCP server modifications. We build a dataset containing the 296 most popular MCP servers, and show that access control policies can be generated automatically from source code with 80.9% accuracy. We also show that AgentBound blocks the majority of security threats in several malicious MCP servers, and that policy enforcement engine introduces negligible overhead. Our contributions provide developers and project managers with a practical foundation for securing MCP servers while maintaining productivity, enabling researchers and tool builders to explore new directions for declarative access control and MCP security.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning</title>
<link>https://arxiv.org/abs/2510.21302</link>
<guid>https://arxiv.org/abs/2510.21302</guid>
<content:encoded><![CDATA[
arXiv:2510.21302v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have enabled the automatic generation of executable code for task planning and control in embodied agents such as robots, demonstrating the potential of LLM-based embodied intelligence. However, these LLM-based code-as-policies approaches often suffer from limited environmental grounding, particularly in dynamic or partially observable settings, leading to suboptimal task success rates due to incorrect or incomplete code generation. In this work, we propose a neuro-symbolic embodied task planning framework that incorporates explicit symbolic verification and interactive validation processes during code generation. In the validation phase, the framework generates exploratory code that actively interacts with the environment to acquire missing observations while preserving task-relevant states. This integrated process enhances the grounding of generated code, resulting in improved task reliability and success rates in complex environments. We evaluate our framework on RLBench and in real-world settings across dynamic, partially observable scenarios. Experimental results demonstrate that our framework improves task success rates by 46.2% over Code-as-Policies baselines and attains over 86.8% executability of task-relevant actions, thereby enhancing the reliability of task planning in dynamic environments.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PARL: Prompt-based Agents for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.21306</link>
<guid>https://arxiv.org/abs/2510.21306</guid>
<content:encoded><![CDATA[
arXiv:2510.21306v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated high performance on tasks expressed in natural language, particularly in zero- or few-shot settings. These are typically framed as supervised (e.g., classification) or unsupervised (e.g., clustering) problems. However, limited work evaluates LLMs as agents in reinforcement learning (RL) tasks (e.g., playing games), where learning occurs through interaction with an environment and a reward system. While prior work focused on representing tasks that rely on a language representation, we study structured, non-linguistic reasoning - such as interpreting positions in a grid world. We therefore introduce PARL (Prompt-based Agent for Reinforcement Learning), a method that uses LLMs as RL agents through prompting, without any fine-tuning. PARL encodes actions, states, and rewards in the prompt, enabling the model to learn through trial-and-error interaction. We evaluate PARL on three standard RL tasks that do not entirely rely on natural language. We show that it can match or outperform traditional RL agents in simple environments by leveraging pretrained knowledge. However, we identify performance limitations in tasks that require complex mathematical operations or decoding states and actions.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Social Welfare in Bandits: UCB is (Nearly) All You Need</title>
<link>https://arxiv.org/abs/2510.21312</link>
<guid>https://arxiv.org/abs/2510.21312</guid>
<content:encoded><![CDATA[
arXiv:2510.21312v1 Announce Type: new 
Abstract: Regret in stochastic multi-armed bandits traditionally measures the difference between the highest reward and either the arithmetic mean of accumulated rewards or the final reward. These conventional metrics often fail to address fairness among agents receiving rewards, particularly in settings where rewards are distributed across a population, such as patients in clinical trials. To address this, a recent body of work has introduced Nash regret, which evaluates performance via the geometric mean of accumulated rewards, aligning with the Nash social welfare function known for satisfying fairness axioms.
  To minimize Nash regret, existing approaches require specialized algorithm designs and strong assumptions, such as multiplicative concentration inequalities and bounded, non-negative rewards, making them unsuitable for even Gaussian reward distributions. We demonstrate that an initial uniform exploration phase followed by a standard Upper Confidence Bound (UCB) algorithm achieves near-optimal Nash regret, while relying only on additive Hoeffding bounds, and naturally extending to sub-Gaussian rewards. Furthermore, we generalize the algorithm to a broad class of fairness metrics called the $p$-mean regret, proving (nearly) optimal regret bounds uniformly across all $p$ values. This is in contrast to prior work, which made extremely restrictive assumptions on the bandit instances and even then achieved suboptimal regret bounds.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CXRAgent: Director-Orchestrated Multi-Stage Reasoning for Chest X-Ray Interpretation</title>
<link>https://arxiv.org/abs/2510.21324</link>
<guid>https://arxiv.org/abs/2510.21324</guid>
<content:encoded><![CDATA[
arXiv:2510.21324v1 Announce Type: new 
Abstract: Chest X-ray (CXR) plays a pivotal role in clinical diagnosis, and a variety of task-specific and foundation models have been developed for automatic CXR interpretation. However, these models often struggle to adapt to new diagnostic tasks and complex reasoning scenarios. Recently, LLM-based agent models have emerged as a promising paradigm for CXR analysis, enhancing model's capability through tool coordination, multi-step reasoning, and team collaboration, etc. However, existing agents often rely on a single diagnostic pipeline and lack mechanisms for assessing tools' reliability, limiting their adaptability and credibility. To this end, we propose CXRAgent, a director-orchestrated, multi-stage agent for CXR interpretation, where a central director coordinates the following stages: (1) Tool Invocation: The agent strategically orchestrates a set of CXR-analysis tools, with outputs normalized and verified by the Evidence-driven Validator (EDV), which grounds diagnostic outputs with visual evidence to support reliable downstream diagnosis; (2) Diagnostic Planning: Guided by task requirements and intermediate findings, the agent formulates a targeted diagnostic plan. It then assembles an expert team accordingly, defining member roles and coordinating their interactions to enable adaptive and collaborative reasoning; (3) Collaborative Decision-making: The agent integrates insights from the expert team with accumulated contextual memories, synthesizing them into an evidence-backed diagnostic conclusion. Experiments on various CXR interpretation tasks show that CXRAgent delivers strong performance, providing visual evidence and generalizes well to clinical tasks of different complexity. Code and data are valuable at this \href{https://github.com/laojiahuo2003/CXRAgent/}{link}.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Randomized Neural Network with Adaptive Forward Regularization for Online Task-free Class Incremental Learning</title>
<link>https://arxiv.org/abs/2510.21367</link>
<guid>https://arxiv.org/abs/2510.21367</guid>
<content:encoded><![CDATA[
arXiv:2510.21367v1 Announce Type: new 
Abstract: Class incremental learning (CIL) requires an agent to learn distinct tasks consecutively with knowledge retention against forgetting. Problems impeding the practical applications of CIL methods are twofold: (1) non-i.i.d batch streams and no boundary prompts to update, known as the harsher online task-free CIL (OTCIL) scenario; (2) CIL methods suffer from memory loss in learning long task streams, as shown in Fig. 1 (a). To achieve efficient decision-making and decrease cumulative regrets during the OTCIL process, a randomized neural network (Randomized NN) with forward regularization (-F) is proposed to resist forgetting and enhance learning performance. This general framework integrates unsupervised knowledge into recursive convex optimization, has no learning dissipation, and can outperform the canonical ridge style (-R) in OTCIL. Based on this framework, we derive the algorithm of the ensemble deep random vector functional link network (edRVFL) with adjustable forward regularization (-kF), where k mediates the intensity of the intervention. edRVFL-kF generates one-pass closed-form incremental updates and variable learning rates, effectively avoiding past replay and catastrophic forgetting while achieving superior performance. Moreover, to curb unstable penalties caused by non-i.i.d and mitigate intractable tuning of -kF in OTCIL, we improve it to the plug-and-play edRVFL-kF-Bayes, enabling all hard ks in multiple sub-learners to be self-adaptively determined based on Bayesian learning. Experiments were conducted on 2 image datasets including 6 metrics, dynamic performance, ablation tests, and compatibility, which distinctly validates the efficacy of our OTCIL frameworks with -kF-Bayes and -kF styles.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context Engineering for AI Agents in Open-Source Software</title>
<link>https://arxiv.org/abs/2510.21413</link>
<guid>https://arxiv.org/abs/2510.21413</guid>
<content:encoded><![CDATA[
arXiv:2510.21413v1 Announce Type: new 
Abstract: GenAI-based coding assistants have disrupted software development. Their next generation is agent-based, operating with more autonomy and potentially without human oversight. One challenge is to provide AI agents with sufficient context about the software projects they operate in. Like humans, AI agents require contextual information to develop solutions that are in line with the target architecture, interface specifications, coding guidelines, standard workflows, and other project-specific policies. Popular AI agents for software development (e.g., Claude Code) advocate for maintaining tool-specific version-controlled Markdown files that cover aspects such as the project structure, building and testing, or code style. The content of these files is automatically added to each prompt. AGENTS.md has emerged as a potential standard that consolidates tool-specific formats. However, little is known about whether and how developers adopt this format. Therefore, in this paper, we present the results of a preliminary study investigating the adoption of AI configuration files in 466 open-source software projects, what information developers provide in these files, how they present that information, and how they evolve over time. Our findings indicate that there is no established structure yet, and that there is a lot of variation in terms of how context is provided (descriptive, prescriptive, prohibitive, explanatory, conditional). We see great potential in studying which modifications in structure or presentation can positively affect the quality of the generated content. Finally, our analysis of commits that have modified AGENTS.md files provides first insights into how projects continuously extend and maintain these files. We conclude the paper by outlining how the adoption of AI configuration files in provides a unique opportunity to study real-world prompt and context engineering.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causality Meets Locality: Provably Generalizable and Scalable Policy Learning for Networked Systems</title>
<link>https://arxiv.org/abs/2510.21427</link>
<guid>https://arxiv.org/abs/2510.21427</guid>
<content:encoded><![CDATA[
arXiv:2510.21427v1 Announce Type: new 
Abstract: Large-scale networked systems, such as traffic, power, and wireless grids, challenge reinforcement-learning agents with both scale and environment shifts. To address these challenges, we propose GSAC (Generalizable and Scalable Actor-Critic), a framework that couples causal representation learning with meta actor-critic learning to achieve both scalability and domain generalization. Each agent first learns a sparse local causal mask that provably identifies the minimal neighborhood variables influencing its dynamics, yielding exponentially tight approximately compact representations (ACRs) of state and domain factors. These ACRs bound the error of truncating value functions to $\kappa$-hop neighborhoods, enabling efficient learning on graphs. A meta actor-critic then trains a shared policy across multiple source domains while conditioning on the compact domain factors; at test time, a few trajectories suffice to estimate the new domain factor and deploy the adapted policy. We establish finite-sample guarantees on causal recovery, actor-critic convergence, and adaptation gap, and show that GSAC adapts rapidly and significantly outperforms learning-from-scratch and conventional adaptation baselines.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenHype: Hyperbolic Embeddings for Hierarchical Open-Vocabulary Radiance Fields</title>
<link>https://arxiv.org/abs/2510.21441</link>
<guid>https://arxiv.org/abs/2510.21441</guid>
<content:encoded><![CDATA[
arXiv:2510.21441v1 Announce Type: new 
Abstract: Modeling the inherent hierarchical structure of 3D objects and 3D scenes is highly desirable, as it enables a more holistic understanding of environments for autonomous agents. Accomplishing this with implicit representations, such as Neural Radiance Fields, remains an unexplored challenge. Existing methods that explicitly model hierarchical structures often face significant limitations: they either require multiple rendering passes to capture embeddings at different levels of granularity, significantly increasing inference time, or rely on predefined, closed-set discrete hierarchies that generalize poorly to the diverse and nuanced structures encountered by agents in the real world. To address these challenges, we propose OpenHype, a novel approach that represents scene hierarchies using a continuous hyperbolic latent space. By leveraging the properties of hyperbolic geometry, OpenHype naturally encodes multi-scale relationships and enables smooth traversal of hierarchies through geodesic paths in latent space. Our method outperforms state-of-the-art approaches on standard benchmarks, demonstrating superior efficiency and adaptability in 3D scene understanding.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Neural Incentive Design with Parameterized Mean-Field Approximation</title>
<link>https://arxiv.org/abs/2510.21442</link>
<guid>https://arxiv.org/abs/2510.21442</guid>
<content:encoded><![CDATA[
arXiv:2510.21442v1 Announce Type: new 
Abstract: Designing incentives for a multi-agent system to induce a desirable Nash equilibrium is both a crucial and challenging problem appearing in many decision-making domains, especially for a large number of agents $N$. Under the exchangeability assumption, we formalize this incentive design (ID) problem as a parameterized mean-field game (PMFG), aiming to reduce complexity via an infinite-population limit. We first show that when dynamics and rewards are Lipschitz, the finite-$N$ ID objective is approximated by the PMFG at rate $\mathscr{O}(\frac{1}{\sqrt{N}})$. Moreover, beyond the Lipschitz-continuous setting, we prove the same $\mathscr{O}(\frac{1}{\sqrt{N}})$ decay for the important special case of sequential auctions, despite discontinuities in dynamics, through a tailored auction-specific analysis. Built on our novel approximation results, we further introduce our Adjoint Mean-Field Incentive Design (AMID) algorithm, which uses explicit differentiation of iterated equilibrium operators to compute gradients efficiently. By uniting approximation bounds with optimization guarantees, AMID delivers a powerful, scalable algorithmic tool for many-agent (large $N$) ID. Across diverse auction settings, the proposed AMID method substantially increases revenue over first-price formats and outperforms existing benchmark methods.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REMONI: An Autonomous System Integrating Wearables and Multimodal Large Language Models for Enhanced Remote Health Monitoring</title>
<link>https://arxiv.org/abs/2510.21445</link>
<guid>https://arxiv.org/abs/2510.21445</guid>
<content:encoded><![CDATA[
arXiv:2510.21445v1 Announce Type: new 
Abstract: With the widespread adoption of wearable devices in our daily lives, the demand and appeal for remote patient monitoring have significantly increased. Most research in this field has concentrated on collecting sensor data, visualizing it, and analyzing it to detect anomalies in specific diseases such as diabetes, heart disease and depression. However, this domain has a notable gap in the aspect of human-machine interaction. This paper proposes REMONI, an autonomous REmote health MONItoring system that integrates multimodal large language models (MLLMs), the Internet of Things (IoT), and wearable devices. The system automatically and continuously collects vital signs, accelerometer data from a special wearable (such as a smartwatch), and visual data in patient video clips collected from cameras. This data is processed by an anomaly detection module, which includes a fall detection model and algorithms to identify and alert caregivers of the patient's emergency conditions. A distinctive feature of our proposed system is the natural language processing component, developed with MLLMs capable of detecting and recognizing a patient's activity and emotion while responding to healthcare worker's inquiries. Additionally, prompt engineering is employed to integrate all patient information seamlessly. As a result, doctors and nurses can access real-time vital signs and the patient's current state and mood by interacting with an intelligent agent through a user-friendly web application. Our experiments demonstrate that our system is implementable and scalable for real-life scenarios, potentially reducing the workload of medical professionals and healthcare costs. A full-fledged prototype illustrating the functionalities of the system has been developed and being tested to demonstrate the robustness of its various capabilities.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SBASH: a Framework for Designing and Evaluating RAG vs. Prompt-Tuned LLM Honeypots</title>
<link>https://arxiv.org/abs/2510.21459</link>
<guid>https://arxiv.org/abs/2510.21459</guid>
<content:encoded><![CDATA[
arXiv:2510.21459v1 Announce Type: new 
Abstract: Honeypots are decoy systems used for gathering valuable threat intelligence or diverting attackers away from production systems. Maximising attacker engagement is essential to their utility. However research has highlighted that context-awareness, such as the ability to respond to new attack types, systems and attacker agents, is necessary to increase engagement. Large Language Models (LLMs) have been shown as one approach to increase context awareness but suffer from several challenges including accuracy and timeliness of response time, high operational costs and data-protection issues due to cloud deployment. We propose the System-Based Attention Shell Honeypot (SBASH) framework which manages data-protection issues through the use of lightweight local LLMs. We investigate the use of Retrieval Augmented Generation (RAG) supported LLMs and non-RAG LLMs for Linux shell commands and evaluate them using several different metrics such as response time differences, realism from human testers, and similarity to a real system calculated with Levenshtein distance, SBert, and BertScore. We show that RAG improves accuracy for untuned models while models that have been tuned via a system prompt that tells the LLM to respond like a Linux system achieve without RAG a similar accuracy as untuned with RAG, while having a slightly lower latency.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EU-Agent-Bench: Measuring Illegal Behavior of LLM Agents Under EU Law</title>
<link>https://arxiv.org/abs/2510.21524</link>
<guid>https://arxiv.org/abs/2510.21524</guid>
<content:encoded><![CDATA[
arXiv:2510.21524v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed as agents in various contexts by providing tools at their disposal. However, LLM agents can exhibit unpredictable behaviors, including taking undesirable and/or unsafe actions. In order to measure the latent propensity of LLM agents for taking illegal actions under an EU legislative context, we introduce EU-Agent-Bench, a verifiable human-curated benchmark that evaluates an agent's alignment with EU legal norms in situations where benign user inputs could lead to unlawful actions. Our benchmark spans scenarios across several categories, including data protection, bias/discrimination, and scientific integrity, with each user request allowing for both compliant and non-compliant execution of the requested actions. Comparing the model's function calls against a rubric exhaustively supported by citations of the relevant legislature, we evaluate the legal compliance of frontier LLMs, and furthermore investigate the compliance effect of providing the relevant legislative excerpts in the agent's system prompt along with explicit instructions to comply. We release a public preview set for the research community, while holding out a private test set to prevent data contamination in evaluating upcoming models. We encourage future work extending agentic safety benchmarks to different legal jurisdictions and to multi-turn and multilingual interactions. We release our code on \href{https://github.com/ilijalichkovski/eu-agent-bench}{this URL}.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cost Minimization for Space-Air-Ground Integrated Multi-Access Edge Computing Systems</title>
<link>https://arxiv.org/abs/2510.21541</link>
<guid>https://arxiv.org/abs/2510.21541</guid>
<content:encoded><![CDATA[
arXiv:2510.21541v1 Announce Type: new 
Abstract: Space-air-ground integrated multi-access edge computing (SAGIN-MEC) provides a promising solution for the rapidly developing low-altitude economy (LAE) to deliver flexible and wide-area computing services. However, fully realizing the potential of SAGIN-MEC in the LAE presents significant challenges, including coordinating decisions across heterogeneous nodes with different roles, modeling complex factors such as mobility and network variability, and handling real-time decision-making under partially observable environment with hybrid variables. To address these challenges, we first present a hierarchical SAGIN-MEC architecture that enables the coordination between user devices (UDs), uncrewed aerial vehicles (UAVs), and satellites. Then, we formulate a UD cost minimization optimization problem (UCMOP) to minimize the UD cost by jointly optimizing the task offloading ratio, UAV trajectory planning, computing resource allocation, and UD association. We show that the UCMOP is an NP-hard problem. To overcome this challenge, we propose a multi-agent deep deterministic policy gradient (MADDPG)-convex optimization and coalitional game (MADDPG-COCG) algorithm. Specifically, we employ the MADDPG algorithm to optimize the continuous temporal decisions for heterogeneous nodes in the partially observable SAGIN-MEC system. Moreover, we propose a convex optimization and coalitional game (COCG) method to enhance the conventional MADDPG by deterministically handling the hybrid and varying-dimensional decisions. Simulation results demonstrate that the proposed MADDPG-COCG algorithm significantly enhances the user-centric performances in terms of the aggregated UD cost, task completion delay, and UD energy consumption, with a slight increase in UAV energy consumption, compared to the benchmark algorithms. Moreover, the MADDPG-COCG algorithm shows superior convergence stability and scalability.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auction-Based Responsibility Allocation for Scalable Decentralized Safety Filters in Cooperative Multi-Agent Collision Avoidance</title>
<link>https://arxiv.org/abs/2510.21546</link>
<guid>https://arxiv.org/abs/2510.21546</guid>
<content:encoded><![CDATA[
arXiv:2510.21546v1 Announce Type: new 
Abstract: This paper proposes a scalable decentralized safety filter for multi-agent systems based on high-order control barrier functions (HOCBFs) and auction-based responsibility allocation. While decentralized HOCBF formulations ensure pairwise safety under input bounds, they face feasibility and scalability challenges as the number of agents grows. Each agent must evaluate an increasing number of pairwise constraints, raising the risk of infeasibility and making it difficult to meet real-time requirements. To address this, we introduce an auction-based allocation scheme that distributes constraint enforcement asymmetrically among neighbors based on local control effort estimates. The resulting directed responsibility graph guarantees full safety coverage while reducing redundant constraints and per-agent computational load. Simulation results confirm safe and efficient coordination across a range of network sizes and interaction densities.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>System-Theoretic Analysis of Dynamic Generalized Nash Equilibrium Problems -- Turnpikes and Dissipativity</title>
<link>https://arxiv.org/abs/2510.21556</link>
<guid>https://arxiv.org/abs/2510.21556</guid>
<content:encoded><![CDATA[
arXiv:2510.21556v1 Announce Type: new 
Abstract: Generalized Nash equilibria are used in multi-agent control applications to model strategic interactions between agents that are coupled in the cost, dynamics, and constraints. We study the properties of open-loop GNE trajectories from a system-theoretic perspective. We show how strict dissipativity generates the turnpike phenomenon in GNE solutions. Moreover, we establish a converse turnpike result, i.e., the implication from turnpike to strict dissipativity. We derive conditions under which the steady-state GNE is the optimal operating point and, using a game value function, we give a local characterization of the geometry of storage functions. Finally, we design linear terminal penalties that ensure GNE open-loop trajectories converge to and remain at the steady-state GNE. These connections provide the foundation for future system-theoretic analysis of GNEs similar to those existing in optimal control.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Co-Sight: Enhancing LLM-Based Agents via Conflict-Aware Meta-Verification and Trustworthy Reasoning with Structured Facts</title>
<link>https://arxiv.org/abs/2510.21557</link>
<guid>https://arxiv.org/abs/2510.21557</guid>
<content:encoded><![CDATA[
arXiv:2510.21557v1 Announce Type: new 
Abstract: Long-horizon reasoning in LLM-based agents often fails not from generative weakness but from insufficient verification of intermediate reasoning. Co-Sight addresses this challenge by turning reasoning into a falsifiable and auditable process through two complementary mechanisms: Conflict-Aware Meta-Verification (CAMV) and Trustworthy Reasoning with Structured Facts (TRSF). CAMV reformulates verification as conflict identification and targeted falsification, allocating computation only to disagreement hotspots among expert agents rather than to full reasoning chains. This bounds verification cost to the number of inconsistencies and improves efficiency and reliability. TRSF continuously organizes, validates, and synchronizes evidence across agents through a structured facts module. By maintaining verified, traceable, and auditable knowledge, it ensures that all reasoning is grounded in consistent, source-verified information and supports transparent verification throughout the reasoning process. Together, TRSF and CAMV form a closed verification loop, where TRSF supplies structured facts and CAMV selectively falsifies or reinforces them, yielding transparent and trustworthy reasoning. Empirically, Co-Sight achieves state-of-the-art accuracy on GAIA (84.4%) and Humanity's Last Exam (35.5%), and strong results on Chinese-SimpleQA (93.8%). Ablation studies confirm that the synergy between structured factual grounding and conflict-aware verification drives these improvements. Co-Sight thus offers a scalable paradigm for reliable long-horizon reasoning in LLM-based agents. Code is available at https://github.com/ZTE-AICloud/Co-Sight/tree/cosight2.0_benchmarks.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ColorEcosystem: Powering Personalized, Standardized, and Trustworthy Agentic Service in massive-agent Ecosystem</title>
<link>https://arxiv.org/abs/2510.21566</link>
<guid>https://arxiv.org/abs/2510.21566</guid>
<content:encoded><![CDATA[
arXiv:2510.21566v1 Announce Type: new 
Abstract: With the rapid development of (multimodal) large language model-based agents, the landscape of agentic service management has evolved from single-agent systems to multi-agent systems, and now to massive-agent ecosystems. Current massive-agent ecosystems face growing challenges, including impersonal service experiences, a lack of standardization, and untrustworthy behavior. To address these issues, we propose ColorEcosystem, a novel blueprint designed to enable personalized, standardized, and trustworthy agentic service at scale. Concretely, ColorEcosystem consists of three key components: agent carrier, agent store, and agent audit. The agent carrier provides personalized service experiences by utilizing user-specific data and creating a digital twin, while the agent store serves as a centralized, standardized platform for managing diverse agentic services. The agent audit, based on the supervision of developer and user activities, ensures the integrity and credibility of both service providers and users. Through the analysis of challenges, transitional forms, and practical considerations, the ColorEcosystem is poised to power personalized, standardized, and trustworthy agentic service across massive-agent ecosystems. Meanwhile, we have also implemented part of ColorEcosystem's functionality, and the relevant code is open-sourced at https://github.com/opas-lab/color-ecosystem.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Doc-Researcher: A Unified System for Multimodal Document Parsing and Deep Research</title>
<link>https://arxiv.org/abs/2510.21603</link>
<guid>https://arxiv.org/abs/2510.21603</guid>
<content:encoded><![CDATA[
arXiv:2510.21603v1 Announce Type: new 
Abstract: Deep Research systems have revolutionized how LLMs solve complex questions through iterative reasoning and evidence gathering. However, current systems remain fundamentally constrained to textual web data, overlooking the vast knowledge embedded in multimodal documents Processing such documents demands sophisticated parsing to preserve visual semantics (figures, tables, charts, and equations), intelligent chunking to maintain structural coherence, and adaptive retrieval across modalities, which are capabilities absent in existing systems. In response, we present Doc-Researcher, a unified system that bridges this gap through three integrated components: (i) deep multimodal parsing that preserves layout structure and visual semantics while creating multi-granular representations from chunk to document level, (ii) systematic retrieval architecture supporting text-only, vision-only, and hybrid paradigms with dynamic granularity selection, and (iii) iterative multi-agent workflows that decompose complex queries, progressively accumulate evidence, and synthesize comprehensive answers across documents and modalities. To enable rigorous evaluation, we introduce M4DocBench, the first benchmark for Multi-modal, Multi-hop, Multi-document, and Multi-turn deep research. Featuring 158 expert-annotated questions with complete evidence chains across 304 documents, M4DocBench tests capabilities that existing benchmarks cannot assess. Experiments demonstrate that Doc-Researcher achieves 50.6% accuracy, 3.4xbetter than state-of-the-art baselines, validating that effective document research requires not just better retrieval, but fundamentally deep parsing that preserve multimodal integrity and support iterative research. Our work establishes a new paradigm for conducting deep research on multimodal document collections.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Tactile-based Reinforcement Learning for Robotic Control</title>
<link>https://arxiv.org/abs/2510.21609</link>
<guid>https://arxiv.org/abs/2510.21609</guid>
<content:encoded><![CDATA[
arXiv:2510.21609v1 Announce Type: new 
Abstract: Achieving safe, reliable real-world robotic manipulation requires agents to evolve beyond vision and incorporate tactile sensing to overcome sensory deficits and reliance on idealised state information. Despite its potential, the efficacy of tactile sensing in reinforcement learning (RL) remains inconsistent. We address this by developing self-supervised learning (SSL) methodologies to more effectively harness tactile observations, focusing on a scalable setup of proprioception and sparse binary contacts. We empirically demonstrate that sparse binary tactile signals are critical for dexterity, particularly for interactions that proprioceptive control errors do not register, such as decoupled robot-object motions. Our agents achieve superhuman dexterity in complex contact tasks (ball bouncing and Baoding ball rotation). Furthermore, we find that decoupling the SSL memory from the on-policy memory can improve performance. We release the Robot Tactile Olympiad (RoTO) benchmark to standardise and promote future research in tactile-based manipulation. Project page: https://elle-miller.github.io/tactile_rl
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Huxley-G\"odel Machine: Human-Level Coding Agent Development by an Approximation of the Optimal Self-Improving Machine</title>
<link>https://arxiv.org/abs/2510.21614</link>
<guid>https://arxiv.org/abs/2510.21614</guid>
<content:encoded><![CDATA[
arXiv:2510.21614v1 Announce Type: new 
Abstract: Recent studies operationalize self-improvement through coding agents that edit their own codebases. They grow a tree of self-modifications through expansion strategies that favor higher software engineering benchmark performance, assuming that this implies more promising subsequent self-modifications. However, we identify a mismatch between the agent's self-improvement potential (metaproductivity) and its coding benchmark performance, namely the Metaproductivity-Performance Mismatch. Inspired by Huxley's concept of clade, we propose a metric ($\mathrm{CMP}$) that aggregates the benchmark performances of the descendants of an agent as an indicator of its potential for self-improvement. We show that, in our self-improving coding agent development setting, access to the true $\mathrm{CMP}$ is sufficient to simulate how the G\"odel Machine would behave under certain assumptions. We introduce the Huxley-G\"odel Machine (HGM), which, by estimating $\mathrm{CMP}$ and using it as guidance, searches the tree of self-modifications. On SWE-bench Verified and Polyglot, HGM outperforms prior self-improving coding agent development methods while using less wall-clock time. Last but not least, HGM demonstrates strong transfer to other coding datasets and large language models. The agent optimized by HGM on SWE-bench Verified with GPT-5-mini and evaluated on SWE-bench Lite with GPT-5 achieves human-level performance, matching the best officially checked results of human-engineered coding agents. Our code is available at https://github.com/metauto-ai/HGM.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepAgent: A General Reasoning Agent with Scalable Toolsets</title>
<link>https://arxiv.org/abs/2510.21618</link>
<guid>https://arxiv.org/abs/2510.21618</guid>
<content:encoded><![CDATA[
arXiv:2510.21618v1 Announce Type: new 
Abstract: Large reasoning models have demonstrated strong problem-solving abilities, yet real-world tasks often require external tools and long-horizon interactions. Existing agent frameworks typically follow predefined workflows, which limit autonomous and global task completion. In this paper, we introduce DeepAgent, an end-to-end deep reasoning agent that performs autonomous thinking, tool discovery, and action execution within a single, coherent reasoning process. To address the challenges of long-horizon interactions, particularly the context length explosion from multiple tool calls and the accumulation of interaction history, we introduce an autonomous memory folding mechanism that compresses past interactions into structured episodic, working, and tool memories, reducing error accumulation while preserving critical information. To teach general-purpose tool use efficiently and stably, we develop an end-to-end reinforcement learning strategy, namely ToolPO, that leverages LLM-simulated APIs and applies tool-call advantage attribution to assign fine-grained credit to the tool invocation tokens. Extensive experiments on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank, TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA, HLE), demonstrate that DeepAgent consistently outperforms baselines across both labeled-tool and open-set tool retrieval scenarios. This work takes a step toward more general and capable agents for real-world applications. The code and demo are available at https://github.com/RUC-NLPIR/DeepAgent.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite</title>
<link>https://arxiv.org/abs/2510.21652</link>
<guid>https://arxiv.org/abs/2510.21652</guid>
<content:encoded><![CDATA[
arXiv:2510.21652v1 Announce Type: new 
Abstract: AI agents hold the potential to revolutionize scientific productivity by automating literature reviews, replicating experiments, analyzing data, and even proposing new directions of inquiry; indeed, there are now many such agents, ranging from general-purpose "deep research" systems to specialized science-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of these agents is critical for progress. Yet existing benchmarks fall short on several fronts: they (1) fail to provide holistic, product-informed measures of real-world use cases such as science research; (2) lack reproducible agent tools necessary for a controlled comparison of core agentic capabilities; (3) do not account for confounding variables such as model cost and tool access; (4) do not provide standardized interfaces for quick agent prototyping and evaluation; and (5) lack comprehensive baseline agents necessary to identify true advances. In response, we define principles and tooling for more rigorously benchmarking agents. Using these, we present AstaBench, a suite that provides the first holistic measure of agentic ability to perform scientific research, comprising 2400+ problems spanning the entire scientific discovery process and multiple scientific domains, and including many problems inspired by actual user requests to deployed Asta agents. Our suite comes with the first scientific research environment with production-grade search tools that enable controlled, reproducible evaluation, better accounting for confounders. Alongside, we provide a comprehensive suite of nine science-optimized classes of Asta agents and numerous baselines. Our extensive evaluation of 57 agents across 22 agent classes reveals several interesting findings, most importantly that despite meaningful progress on certain individual aspects, AI remains far from solving the challenge of science research assistance.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Knowledge-Graph Translation Layer for Mission-Aware Multi-Agent Path Planning in Spatiotemporal Dynamics</title>
<link>https://arxiv.org/abs/2510.21695</link>
<guid>https://arxiv.org/abs/2510.21695</guid>
<content:encoded><![CDATA[
arXiv:2510.21695v1 Announce Type: new 
Abstract: The coordination of autonomous agents in dynamic environments is hampered by the semantic gap between high-level mission objectives and low-level planner inputs. To address this, we introduce a framework centered on a Knowledge Graph (KG) that functions as an intelligent translation layer. The KG's two-plane architecture compiles declarative facts into per-agent, mission-aware ``worldviews" and physics-aware traversal rules, decoupling mission semantics from a domain-agnostic planner. This allows complex, coordinated paths to be modified simply by changing facts in the KG. A case study involving Autonomous Underwater Vehicles (AUVs) in the Gulf of Mexico visually demonstrates the end-to-end process and quantitatively proves that different declarative policies produce distinct, high-performing outcomes. This work establishes the KG not merely as a data repository, but as a powerful, stateful orchestrator for creating adaptive and explainable autonomous systems.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Detection of Visual Attribute Reliance with a Self-Reflective Agent</title>
<link>https://arxiv.org/abs/2510.21704</link>
<guid>https://arxiv.org/abs/2510.21704</guid>
<content:encoded><![CDATA[
arXiv:2510.21704v1 Announce Type: new 
Abstract: When a vision model performs image recognition, which visual attributes drive its predictions? Detecting unintended reliance on specific visual features is critical for ensuring model robustness, preventing overfitting, and avoiding spurious correlations. We introduce an automated framework for detecting such dependencies in trained vision models. At the core of our method is a self-reflective agent that systematically generates and tests hypotheses about visual attributes that a model may rely on. This process is iterative: the agent refines its hypotheses based on experimental outcomes and uses a self-evaluation protocol to assess whether its findings accurately explain model behavior. When inconsistencies arise, the agent self-reflects over its findings and triggers a new cycle of experimentation. We evaluate our approach on a novel benchmark of 130 models designed to exhibit diverse visual attribute dependencies across 18 categories. Our results show that the agent's performance consistently improves with self-reflection, with a significant performance increase over non-reflective baselines. We further demonstrate that the agent identifies real-world visual attribute dependencies in state-of-the-art models, including CLIP's vision encoder and the YOLOv8 object detector.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets</title>
<link>https://arxiv.org/abs/2510.19944</link>
<guid>https://arxiv.org/abs/2510.19944</guid>
<content:encoded><![CDATA[
arXiv:2510.19944v1 Announce Type: cross 
Abstract: Developing embodied AI agents requires scalable training environments that balance content diversity with physics accuracy. World simulators provide such environments but face distinct limitations: video-based methods generate diverse content but lack real-time physics feedback for interactive learning, while physics-based engines provide accurate dynamics but face scalability limitations from costly manual asset creation. We present Seed3D 1.0, a foundation model that generates simulation-ready 3D assets from single images, addressing the scalability challenge while maintaining physics rigor. Unlike existing 3D generation models, our system produces assets with accurate geometry, well-aligned textures, and realistic physically-based materials. These assets can be directly integrated into physics engines with minimal configuration, enabling deployment in robotic manipulation and simulation training. Beyond individual objects, the system scales to complete scene generation through assembling objects into coherent environments. By enabling scalable simulation-ready content creation, Seed3D 1.0 provides a foundation for advancing physics-based world simulators. Seed3D 1.0 is now available on https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?modelId=doubao-seed3d-1-0-250928&amp;tab=Gen3D
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rationalizable Screening and Disclosure under Unawareness</title>
<link>https://arxiv.org/abs/2510.20918</link>
<guid>https://arxiv.org/abs/2510.20918</guid>
<content:encoded><![CDATA[
arXiv:2510.20918v1 Announce Type: cross 
Abstract: We analyze a principal-agent procurement problem in which the principal (she) is unaware some of the marginal cost types of the agent (he). Communication arises naturally as some types of the agent may have an incentive to raise the principal's awareness (totally or partially) before a contract menu is offered. The resulting menu must not only reflect the principal's change in awareness, but also her learning about types from the agent's decision to raise her awareness in the first place. We capture this reasoning in a discrete concave model via a rationalizability procedure in which marginal beliefs over types are restricted to log-concavity, ``reverse'' Bayesianism, and mild assumptions of caution.
  We show that if the principal is ex ante only unaware of high-cost types, all of these types have an incentive raise her awareness of them -- otherwise, they would not be served. With three types, the two lower-cost types that the principal is initially aware of also want to raise her awareness of the high-cost type: Their quantities suffer no additional distortions and they both earn an extra information rent. Intuitively, the presence of an even higher cost type makes the original two look better. With more than three types, we show that this intuition may break down for types of whom the principal is initially aware of so that raising the principal's awareness could cease to be profitable for those types. When the principal is ex ante only unaware of more efficient (low-cost) types, then \textit{no type} raises her awareness, leaving her none the wiser.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Screening</title>
<link>https://arxiv.org/abs/2510.20921</link>
<guid>https://arxiv.org/abs/2510.20921</guid>
<content:encoded><![CDATA[
arXiv:2510.20921v1 Announce Type: cross 
Abstract: We consider a principal who wishes to screen an agent with \emph{discrete} types by offering a menu of \emph{discrete} quantities and \emph{discrete} transfers. We assume that the principal's valuation is discrete strictly concave and use a discrete first-order approach. We model the agent's cost types as non-integer, with integer types as a limit case. Our modeling of cost types allows us to replicate the typical constraint-simplification results and thus to emulate the well-treaded steps of screening under a continuum of contracts.
  We show that the solutions to the discrete F.O.C.s need not be unique \textit{even under discrete strict concavity}, but we also show that there cannot be more than two optimal contract quantities for each type, and that -- if there are two -- they must be adjacent. Moreover, we can only ensure weak monotonicity of the quantities \textit{even if virtual costs are strictly monotone}, unless we limit the ``degree of concavity'' of the principal's utility. Our discrete screening approach facilitates the use of rationalizability to solve the screening problem. We introduce a rationalizability notion featuring robustness with respect to an open set of beliefs over types called \textit{$\Delta$-O Rationalizability}, and show that the set of $\Delta$-O rationalizable menus coincides with the set of usual optimal contracts -- possibly augmented to include irrelevant contracts.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Central Bank Digital Currency, Flight-to-Quality, and Bank-Runs in an Agent-Based Model</title>
<link>https://arxiv.org/abs/2510.21071</link>
<guid>https://arxiv.org/abs/2510.21071</guid>
<content:encoded><![CDATA[
arXiv:2510.21071v1 Announce Type: cross 
Abstract: We analyse financial stability and welfare impacts associated with the introduction of a Central Bank Digital Currency (CBDC) in a macroeconomic agent-based model. The model considers firms, banks, and households interacting on labour, goods, credit, and interbank markets. Households move their liquidity from deposits to CBDC based on the perceived riskiness of their banks. We find that the introduction of CBDC exacerbates bank-runs and may lead to financial instability phenomena. The effect can be changed by introducing a limit on CBDC holdings. The adoption of CBDC has little effect on macroeconomic variables but the interest rate on loans to firms goes up and credit goes down in a limited way. CBDC leads to a redistribution of wealth from firms and banks to households with a higher bank default rate. CBDC may have negative welfare effects, but a bound on holding enables a welfare improvement.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical AI Multi-Agent Fundamental Investing: Evidence from China's A-Share Market</title>
<link>https://arxiv.org/abs/2510.21147</link>
<guid>https://arxiv.org/abs/2510.21147</guid>
<content:encoded><![CDATA[
arXiv:2510.21147v1 Announce Type: cross 
Abstract: We present a multi-agent, AI-driven framework for fundamental investing that integrates macro indicators, industry-level and firm-specific information to construct optimized equity portfolios. The architecture comprises: (i) a Macro agent that dynamically screens and weights sectors based on evolving economic indicators and industry performance; (ii) four firm-level agents -- Fundamental, Technical, Report, and News -- that conduct in-depth analyses of individual firms to ensure both breadth and depth of coverage; (iii) a Portfolio agent that uses reinforcement learning to combine the agent outputs into a unified policy to generate the trading strategy; and (iv) a Risk Control agent that adjusts portfolio positions in response to market volatility. We evaluate the system on the constituents by the CSI 300 Index of China's A-share market and find that it consistently outperforms standard benchmarks and a state-of-the-art multi-agent trading system on risk-adjusted returns and drawdown control. Our core contribution is a hierarchical multi-agent design that links top-down macro screening with bottom-up fundamental analysis, offering a robust and extensible approach to factor-based portfolio construction.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Oracle-Efficient Combinatorial Semi-Bandits</title>
<link>https://arxiv.org/abs/2510.21431</link>
<guid>https://arxiv.org/abs/2510.21431</guid>
<content:encoded><![CDATA[
arXiv:2510.21431v1 Announce Type: cross 
Abstract: We study the combinatorial semi-bandit problem where an agent selects a subset of base arms and receives individual feedback. While this generalizes the classical multi-armed bandit and has broad applicability, its scalability is limited by the high cost of combinatorial optimization, requiring oracle queries at every round. To tackle this, we propose oracle-efficient frameworks that significantly reduce oracle calls while maintaining tight regret guarantees. For the worst-case linear reward setting, our algorithms achieve $\tilde{O}(\sqrt{T})$ regret using only $O(\log\log T)$ oracle queries. We also propose covariance-adaptive algorithms that leverage noise structure for improved regret, and extend our approach to general (non-linear) rewards. Overall, our methods reduce oracle usage from linear to (doubly) logarithmic in time, with strong theoretical guarantees.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contribution of task-irrelevant stimuli to drift of neural representations</title>
<link>https://arxiv.org/abs/2510.21588</link>
<guid>https://arxiv.org/abs/2510.21588</guid>
<content:encoded><![CDATA[
arXiv:2510.21588v1 Announce Type: cross 
Abstract: Biological and artificial learners are inherently exposed to a stream of data and experience throughout their lifetimes and must constantly adapt to, learn from, or selectively ignore the ongoing input. Recent findings reveal that, even when the performance remains stable, the underlying neural representations can change gradually over time, a phenomenon known as representational drift. Studying the different sources of data and noise that may contribute to drift is essential for understanding lifelong learning in neural systems. However, a systematic study of drift across architectures and learning rules, and the connection to task, are missing. Here, in an online learning setup, we characterize drift as a function of data distribution, and specifically show that the learning noise induced by task-irrelevant stimuli, which the agent learns to ignore in a given context, can create long-term drift in the representation of task-relevant stimuli. Using theory and simulations, we demonstrate this phenomenon both in Hebbian-based learning -- Oja's rule and Similarity Matching -- and in stochastic gradient descent applied to autoencoders and a supervised two-layer network. We consistently observe that the drift rate increases with the variance and the dimension of the data in the task-irrelevant subspace. We further show that this yields different qualitative predictions for the geometry and dimension-dependency of drift than those arising from Gaussian synaptic noise. Overall, our study links the structure of stimuli, task, and learning rule to representational drift and could pave the way for using drift as a signal for uncovering underlying computation in the brain.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mix Q-learning for Lane Changing: A Collaborative Decision-Making Method in Multi-Agent Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2406.09755</link>
<guid>https://arxiv.org/abs/2406.09755</guid>
<content:encoded><![CDATA[
arXiv:2406.09755v2 Announce Type: replace 
Abstract: Lane-changing decisions, which are crucial for autonomous vehicle path planning, face practical challenges due to rule-based constraints and limited data. Deep reinforcement learning has become a major research focus due to its advantages in data acquisition and interpretability. However, current models often overlook collaboration, which affects not only impacts overall traffic efficiency but also hinders the vehicle's own normal driving in the long run. To address the aforementioned issue, this paper proposes a method named Mix Q-learning for Lane Changing(MQLC) that integrates a hybrid value Q network, taking into account both collective and individual benefits for the greater good. At the collective level, our method coordinates the individual Q and global Q networks by utilizing global information. This enables agents to effectively balance their individual interests with the collective benefit. At the individual level, we integrated a deep learning-based intent recognition module into our observation and enhanced the decision network. These changes provide agents with richer decision information and more accurate feature extraction for improved lane-changing decisions. This strategy enables the multi-agent system to learn and formulate optimal decision-making strategies effectively. Our MQLC model, through extensive experimental results, impressively outperforms other state-of-the-art multi-agent decision-making methods, achieving significantly safer and faster lane-changing decisions. The code is available at https:github.com/pku-smart-city/source_code/tree/main/MQLC.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEReQ: Max-Ent Residual-Q Inverse RL for Sample-Efficient Alignment from Intervention</title>
<link>https://arxiv.org/abs/2406.16258</link>
<guid>https://arxiv.org/abs/2406.16258</guid>
<content:encoded><![CDATA[
arXiv:2406.16258v4 Announce Type: replace 
Abstract: Aligning robot behavior with human preferences is crucial for deploying embodied AI agents in human-centered environments. A promising solution is interactive imitation learning from human intervention, where a human expert observes the policy's execution and provides interventions as feedback. However, existing methods often fail to utilize the prior policy efficiently to facilitate learning, thus hindering sample efficiency. In this work, we introduce MEReQ (Maximum-Entropy Residual-Q Inverse Reinforcement Learning), designed for sample-efficient alignment from human intervention. Instead of inferring the complete human behavior characteristics, MEReQ infers a residual reward function that captures the discrepancy between the human expert's and the prior policy's underlying reward functions. It then employs Residual Q-Learning (RQL) to align the policy with human preferences using this residual reward function. Extensive evaluations on simulated and real-world tasks demonstrate that MEReQ achieves sample-efficient policy alignment from human intervention.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial-Aware Decision-Making with Ring Attractors in Reinforcement Learning Systems</title>
<link>https://arxiv.org/abs/2410.03119</link>
<guid>https://arxiv.org/abs/2410.03119</guid>
<content:encoded><![CDATA[
arXiv:2410.03119v3 Announce Type: replace 
Abstract: Ring attractors, mathematical models inspired by neural circuit dynamics, provide a biologically plausible mechanism to improve learning speed and accuracy in Reinforcement Learning (RL). Serving as specialized brain-inspired structures that encode spatial information and uncertainty, ring attractors explicitly encode the action space, facilitate the organization of neural activity, and enable the distribution of spatial representations across the neural network in the context of Deep Reinforcement Learning (DRL). These structures also provide temporal filtering that stabilizes action selection during exploration, for example, by preserving the continuity between rotation angles in robotic control or adjacency between tactical moves in game-like environments. The application of ring attractors in the action selection process involves mapping actions to specific locations on the ring and decoding the selected action based on neural activity. We investigate the application of ring attractors by both building an exogenous model and integrating them as part of DRL agents. Our approach significantly improves state-of-the-art performance on the Atari 100k benchmark, achieving a 53% increase in performance over selected baselines.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Intrinsic Rewards for Decision Making Agents from Large Language Model Feedback</title>
<link>https://arxiv.org/abs/2410.23022</link>
<guid>https://arxiv.org/abs/2410.23022</guid>
<content:encoded><![CDATA[
arXiv:2410.23022v4 Announce Type: replace 
Abstract: Automatically synthesizing dense rewards from natural language descriptions is a promising paradigm in reinforcement learning (RL), with applications to sparse reward problems, open-ended exploration, and hierarchical skill design. Recent works have made promising steps by exploiting the prior knowledge of large language models (LLMs). However, these approaches suffer from important limitations: they are either not scalable to problems requiring billions of environment samples, due to requiring LLM annotations for each observation, or they require a diverse offline dataset, which may not exist or be impossible to collect. In this work, we address these limitations through a combination of algorithmic and systems-level contributions. We propose ONI, a distributed architecture that simultaneously learns an RL policy and an intrinsic reward function using LLM feedback. Our approach annotates the agent's collected experience via an asynchronous LLM server, which is then distilled into an intrinsic reward model. We explore a range of algorithmic choices for reward modeling with varying complexity, including hashing, classification, and ranking models. Our approach achieves state-of-the-art performance across a range of challenging tasks from the NetHack Learning Environment, while removing the need for large offline datasets required by prior work. We make our code available at https://github.com/facebookresearch/oni.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mean-Field Sampling for Cooperative Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2412.00661</link>
<guid>https://arxiv.org/abs/2412.00661</guid>
<content:encoded><![CDATA[
arXiv:2412.00661v4 Announce Type: replace 
Abstract: Designing efficient algorithms for multi-agent reinforcement learning (MARL) is fundamentally challenging because the size of the joint state and action spaces grows exponentially in the number of agents. These difficulties are exacerbated when balancing sequential global decision-making with local agent interactions. In this work, we propose a new algorithm $\texttt{SUBSAMPLE-MFQ}$ ($\textbf{Subsample}$-$\textbf{M}$ean-$\textbf{F}$ield-$\textbf{Q}$-learning) and a decentralized randomized policy for a system with $n$ agents. For any $k\leq n$, our algorithm learns a policy for the system in time polynomial in $k$. We prove that this learned policy converges to the optimal policy on the order of $\tilde{O}(1/\sqrt{k})$ as the number of subsampled agents $k$ increases. In particular, this bound is independent of the number of agents $n$.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proactive Agents for Multi-Turn Text-to-Image Generation Under Uncertainty</title>
<link>https://arxiv.org/abs/2412.06771</link>
<guid>https://arxiv.org/abs/2412.06771</guid>
<content:encoded><![CDATA[
arXiv:2412.06771v3 Announce Type: replace 
Abstract: User prompts for generative AI models are often underspecified, leading to a misalignment between the user intent and models' understanding. As a result, users commonly have to painstakingly refine their prompts. We study this alignment problem in text-to-image (T2I) generation and propose a prototype for proactive T2I agents equipped with an interface to (1) actively ask clarification questions when uncertain, and (2) present their uncertainty about user intent as an understandable and editable belief graph. We build simple prototypes for such agents and propose a new scalable and automated evaluation approach using two agents, one with a ground truth intent (an image) while the other tries to ask as few questions as possible to align with the ground truth. We experiment over three image-text datasets: ImageInWords (Garg et al., 2024), COCO (Lin et al., 2014) and DesignBench, a benchmark we curated with strong artistic and design elements. Experiments over the three datasets demonstrate the proposed T2I agents' ability to ask informative questions and elicit crucial information to achieve successful alignment with at least 2 times higher VQAScore (Lin et al., 2024) than the standard T2I generation. Moreover, we conducted human studies and observed that at least 90% of human subjects found these agents and their belief graphs helpful for their T2I workflow, highlighting the effectiveness of our approach. Code and DesignBench can be found at https://github.com/google-deepmind/proactive_t2i_agents.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planning and Learning in Average Risk-aware MDPs</title>
<link>https://arxiv.org/abs/2503.17629</link>
<guid>https://arxiv.org/abs/2503.17629</guid>
<content:encoded><![CDATA[
arXiv:2503.17629v2 Announce Type: replace 
Abstract: For continuing tasks, average cost Markov decision processes have well-documented value and can be solved using efficient algorithms. However, it explicitly assumes that the agent is risk-neutral. In this work, we extend risk-neutral algorithms to accommodate the more general class of dynamic risk measures. Specifically, we propose a relative value iteration (RVI) algorithm for planning and design two model-free Q-learning algorithms, namely a generic algorithm based on the multi-level Monte Carlo (MLMC) method, and an off-policy algorithm dedicated to utility-based shortfall risk measures. Both the RVI and MLMC-based Q-learning algorithms are proven to converge to optimality. Numerical experiments validate our analysis, confirm empirically the convergence of the off-policy algorithm, and demonstrate that our approach enables the identification of policies that are finely tuned to the intricate risk-awareness of the agent that they serve.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HypRL: Reinforcement Learning of Control Policies for Hyperproperties</title>
<link>https://arxiv.org/abs/2504.04675</link>
<guid>https://arxiv.org/abs/2504.04675</guid>
<content:encoded><![CDATA[
arXiv:2504.04675v5 Announce Type: replace 
Abstract: Reward shaping in multi-agent reinforcement learning (MARL) for complex tasks remains a significant challenge. Existing approaches often fail to find optimal solutions or cannot efficiently handle such tasks. We propose HYPRL, a specification-guided reinforcement learning framework that learns control policies w.r.t. hyperproperties expressed in HyperLTL. Hyperproperties constitute a powerful formalism for specifying objectives and constraints over sets of execution traces across agents. To learn policies that maximize the satisfaction of a HyperLTL formula $\phi$, we apply Skolemization to manage quantifier alternations and define quantitative robustness functions to shape rewards over execution traces of a Markov decision process with unknown transitions. A suitable RL algorithm is then used to learn policies that collectively maximize the expected reward and, consequently, increase the probability of satisfying $\phi$. We evaluate HYPRL on a diverse set of benchmarks, including safety-aware planning, Deep Sea Treasure, and the Post Correspondence Problem. We also compare with specification-driven baselines to demonstrate the effectiveness and efficiency of HYPRL.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLRC-Bench: Can Language Agents Solve Machine Learning Research Challenges?</title>
<link>https://arxiv.org/abs/2504.09702</link>
<guid>https://arxiv.org/abs/2504.09702</guid>
<content:encoded><![CDATA[
arXiv:2504.09702v3 Announce Type: replace 
Abstract: We introduce MLRC-Bench, a benchmark designed to quantify how effectively language agents can tackle challenging Machine Learning (ML) Research Competitions, with a focus on open research problems that demand novel methodologies. Unlike prior work, e.g., AI Scientist, which evaluates the end-to-end agentic pipeline by using LLM-as-a-judge, MLRC-Bench measures the key steps of proposing and implementing novel research methods and evaluates them with rigorous protocol and objective metrics. Our curated suite of 7 competition tasks reveals significant challenges for LLM agents. Even the best-performing tested agent (gemini-exp-1206 under MLAB) closes only 9.3% of the gap between baseline and top human participant scores. Furthermore, our analysis reveals a misalignment between the LLM-judged innovation and actual performance on cutting-edge ML research problems. MLRC-Bench is a dynamic benchmark, designed to grow with new ML competitions and encourage rigorous, objective evaluations of AI research capabilities. Our leaderboard and code are available at: https://huggingface.co/spaces/launch/MLRC_Bench
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Tool Usage Exploration for Multimodal Agents via Step-wise Preference Tuning</title>
<link>https://arxiv.org/abs/2504.21561</link>
<guid>https://arxiv.org/abs/2504.21561</guid>
<content:encoded><![CDATA[
arXiv:2504.21561v4 Announce Type: replace 
Abstract: Multimodal agents, which integrate a controller e.g., a vision language model) with external tools, have demonstrated remarkable capabilities in tackling complex multimodal tasks. Existing approaches for training these agents, both supervised fine-tuning and reinforcement learning, depend on extensive human-annotated task-answer pairs and tool trajectories. However, for complex multimodal tasks, such annotations are prohibitively expensive or impractical to obtain. In this paper, we propose an iterative tool usage exploration method for multimodal agents without any pre-collected data, namely SPORT, via step-wise preference optimization to refine the trajectories of tool usage. Our method enables multimodal agents to autonomously discover effective tool usage strategies through self-exploration and optimization, eliminating the bottleneck of human annotation. SPORT has four iterative components: task synthesis, step sampling, step verification, and preference tuning. We first synthesize multimodal tasks using language models. Then, we introduce a novel trajectory exploration scheme, where step sampling and step verification are executed alternately to solve synthesized tasks. In step sampling, the agent tries different tools and obtains corresponding results. In step verification, we employ a verifier to provide AI feedback to construct step-wise preference data. The data is subsequently used to update the controller for tool usage through preference tuning, producing a SPORT agent. By interacting with real environments, the SPORT agent gradually evolves into a more refined and capable system. Evaluation in the GTA and GAIA benchmarks shows that the SPORT agent achieves 6.41% and 3.64% improvements, underscoring the generalization and effectiveness introduced by our method. The project page is https://SPORT-Agents.github.io.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APOLLO: Automated LLM and Lean Collaboration for Advanced Formal Reasoning</title>
<link>https://arxiv.org/abs/2505.05758</link>
<guid>https://arxiv.org/abs/2505.05758</guid>
<content:encoded><![CDATA[
arXiv:2505.05758v4 Announce Type: replace 
Abstract: Formal reasoning and automated theorem proving constitute a challenging subfield of machine learning, in which machines are tasked with proving mathematical theorems using formal languages like Lean. A formal verification system can check whether a formal proof is correct or not almost instantaneously, but generating a completely correct formal proof with large language models (LLMs) remains a formidable task. The usual approach in the literature is to prompt the LLM many times (up to several thousands) until one of the generated proofs passes the verification system. In this work, we present APOLLO (Automated PrOof repair viaLLM and Lean cOllaboration), a modular, model-agnostic agentic framework that combines the strengths of the Lean compiler with an LLM's reasoning abilities to achieve better proof-generation results at a low token and sampling budgets. Apollo directs a fully automated process in which the LLM generates proofs for theorems, a set of agents analyze the proofs, fix the syntax errors, identify the mistakes in the proofs using Lean, isolate failing sub-lemmas, utilize automated solvers, and invoke an LLM on each remaining goal with a low top-K budget. The repaired sub-proofs are recombined and reverified, iterating up to a user-controlled maximum number of attempts. On the miniF2F benchmark, we establish a new state-of-the-art accuracy of 84.9% among sub 8B-parameter models (as of August 2025) while keeping the sampling budget below one hundred. Moreover, Apollo raises the state-of-the-art accuracy for Goedel-Prover-SFT to 65.6% while cutting sample complexity from 25,600 to a few hundred. General-purpose models (o3-mini, o4-mini) jump from 3-7% to over 40% accuracy. Our results demonstrate that targeted, compiler-guided repair of LLM outputs yields dramatic gains in both efficiency and correctness, suggesting a general paradigm for scalable automated theorem proving.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A4L: An Architecture for AI-Augmented Learning</title>
<link>https://arxiv.org/abs/2505.06314</link>
<guid>https://arxiv.org/abs/2505.06314</guid>
<content:encoded><![CDATA[
arXiv:2505.06314v2 Announce Type: replace 
Abstract: AI promises personalized learning and scalable education. As AI agents increasingly permeate education in support of teaching and learning, there is a critical and urgent need for data architectures for collecting and analyzing data on learning, and feeding the results back to teachers, learners, and the AI agents for personalization of learning at scale. At the National AI Institute for Adult Learning and Online Education, we are developing an Architecture for AI-Augmented Learning (A4L) for supporting adult learning through online education. We present the motivations, goals, requirements of the A4L architecture. We describe preliminary applications of A4L and discuss how it advances the goals of making learning more personalized and scalable.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information Retrieval Induced Safety Degradation in AI Agents</title>
<link>https://arxiv.org/abs/2505.14215</link>
<guid>https://arxiv.org/abs/2505.14215</guid>
<content:encoded><![CDATA[
arXiv:2505.14215v2 Announce Type: replace 
Abstract: Despite the growing integration of retrieval-enabled AI agents into society, their safety and ethical behavior remain inadequately understood. In particular, the integration of LLMs and AI agents with external information sources and real-world environments raises critical questions about how they engage with and are influenced by these external data sources and interactive contexts. This study investigates how expanding retrieval access -- from no external sources to Wikipedia-based retrieval and open web search -- affects model reliability, bias propagation, and harmful content generation. Through extensive benchmarking of censored and uncensored LLMs and AI agents, our findings reveal a consistent degradation in refusal rates, bias sensitivity, and harmfulness safeguards as models gain broader access to external sources, culminating in a phenomenon we term safety degradation. Notably, retrieval-enabled agents built on aligned LLMs often behave more unsafely than uncensored models without retrieval. This effect persists even under strong retrieval accuracy and prompt-based mitigation, suggesting that the mere presence of retrieved content reshapes model behavior in structurally unsafe ways. These findings underscore the need for robust mitigation strategies to ensure fairness and reliability in retrieval-enabled and increasingly autonomous AI systems.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning</title>
<link>https://arxiv.org/abs/2505.16986</link>
<guid>https://arxiv.org/abs/2505.16986</guid>
<content:encoded><![CDATA[
arXiv:2505.16986v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities as intelligent agents capable of solving complex problems. However, effective planning in scenarios involving dependencies between API or tool calls-particularly in multi-turn conversations-remains a significant challenge. To address this, we introduce T1, a tool-augmented, multi-domain, multi-turn conversational dataset specifically designed to capture and manage inter-tool dependencies across diverse domains. T1 enables rigorous evaluation of agents' ability to coordinate tool use across nine distinct domains (4 single domain and 5 multi-domain) with the help of an integrated caching mechanism for both short- and long-term memory, while supporting dynamic replanning-such as deciding whether to recompute or reuse cached results. Beyond facilitating research on tool use and planning, T1 also serves as a benchmark for evaluating the performance of open-weight and proprietary large language models. We present results powered by T1-Agent, highlighting their ability to plan and reason in complex, tool-dependent scenarios.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Multi-Agent World Modeling from a Diffusion-Inspired Perspective</title>
<link>https://arxiv.org/abs/2505.20922</link>
<guid>https://arxiv.org/abs/2505.20922</guid>
<content:encoded><![CDATA[
arXiv:2505.20922v2 Announce Type: replace 
Abstract: World models have recently attracted growing interest in Multi-Agent Reinforcement Learning (MARL) due to their ability to improve sample efficiency for policy learning. However, accurately modeling environments in MARL is challenging due to the exponentially large joint action space and highly uncertain dynamics inherent in multi-agent systems. To address this, we reduce modeling complexity by shifting from jointly modeling the entire state-action transition dynamics to focusing on the state space alone at each timestep through sequential agent modeling. Specifically, our approach enables the model to progressively resolve uncertainty while capturing the structured dependencies among agents, providing a more accurate representation of how agents influence the state. Interestingly, this sequential revelation of agents' actions in a multi-agent system aligns with the reverse process in diffusion models--a class of powerful generative models known for their expressiveness and training stability compared to autoregressive or latent variable models. Leveraging this insight, we develop a flexible and robust world model for MARL using diffusion models. Our method, Diffusion-Inspired Multi-Agent world model (DIMA), achieves state-of-the-art performance across multiple multi-agent control benchmarks, significantly outperforming prior world models in terms of final return and sample efficiency, including MAMuJoCo and Bi-DexHands. DIMA establishes a new paradigm for constructing multi-agent world models, advancing the frontier of MARL research. Codes are open-sourced at https://github.com/breez3young/DIMA.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LocalGPT: Benchmarking and Advancing Large Language Models for Local Life Services in Meituan</title>
<link>https://arxiv.org/abs/2506.02720</link>
<guid>https://arxiv.org/abs/2506.02720</guid>
<content:encoded><![CDATA[
arXiv:2506.02720v3 Announce Type: replace 
Abstract: Large language models (LLMs) have exhibited remarkable capabilities and achieved significant breakthroughs across various domains, leading to their widespread adoption in recent years. Building on this progress, we investigate their potential in the realm of local life services. In this study, we establish a comprehensive benchmark and systematically evaluate the performance of diverse LLMs across a wide range of tasks relevant to local life services. To further enhance their effectiveness, we explore two key approaches: model fine-tuning and agent-based workflows. Our findings reveal that even a relatively compact 7B model can attain performance levels comparable to a much larger 72B model, effectively balancing inference cost and model capability. This optimization greatly enhances the feasibility and efficiency of deploying LLMs in real-world online services, making them more practical and accessible for local life applications.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Manipulation and Enhancing Persuasion: A Reflective Multi-Agent Approach for Legal Argument Generation</title>
<link>https://arxiv.org/abs/2506.02992</link>
<guid>https://arxiv.org/abs/2506.02992</guid>
<content:encoded><![CDATA[
arXiv:2506.02992v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly explored for legal argument generation, yet they pose significant risks of manipulation through hallucination and ungrounded persuasion, and often fail to utilize provided factual bases effectively or abstain when arguments are untenable. This paper introduces a novel reflective multi-agent method designed to address these challenges in the context of legally compliant persuasion. Our approach employs specialized agents (factor analyst and argument polisher) in an iterative refinement process to generate 3-ply legal arguments (plaintiff, defendant, rebuttal). We evaluate reflective multi-agent against single-agent, enhanced-prompt single-agent, and non-reflective multi-agent baselines using four diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e, Llama-4-Scout-17b-16e) across three legal scenarios: "arguable", "mismatched", and "non-arguable". Results demonstrate that the reflective multi-agent approach excels at successful abstention by preventing generation when arguments cannot be grounded, improves hallucination accuracy by reducing fabricated and misattributed factors and enhances factor utilization recall by better using the provided case facts. These findings suggest that structured reflection within a multi-agent framework offers a robust method for fostering ethical persuasion and mitigating manipulation in LLM-based legal argumentation systems.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Smooth Sea Never Made a Skilled SAILOR: Robust Imitation via Learning to Search</title>
<link>https://arxiv.org/abs/2506.05294</link>
<guid>https://arxiv.org/abs/2506.05294</guid>
<content:encoded><![CDATA[
arXiv:2506.05294v2 Announce Type: replace 
Abstract: The fundamental limitation of the behavioral cloning (BC) approach to imitation learning is that it only teaches an agent what the expert did at states the expert visited. This means that when a BC agent makes a mistake which takes them out of the support of the demonstrations, they often don't know how to recover from it. In this sense, BC is akin to giving the agent the fish -- giving them dense supervision across a narrow set of states -- rather than teaching them to fish: to be able to reason independently about achieving the expert's outcome even when faced with unseen situations at test-time. In response, we explore learning to search (L2S) from expert demonstrations, i.e. learning the components required to, at test time, plan to match expert outcomes, even after making a mistake. These include (1) a world model and (2) a reward model. We carefully ablate the set of algorithmic and design decisions required to combine these and other components for stable and sample/interaction-efficient learning of recovery behavior without additional human corrections. Across a dozen visual manipulation tasks from three benchmarks, our approach SAILOR consistently out-performs state-of-the-art Diffusion Policies trained via BC on the same data. Furthermore, scaling up the amount of demonstrations used for BC by 5-10x still leaves a performance gap. We find that SAILOR can identify nuanced failures and is robust to reward hacking. Our code is available at https://github.com/arnavkj1995/SAILOR .
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Society Requires Simulating Thought</title>
<link>https://arxiv.org/abs/2506.06958</link>
<guid>https://arxiv.org/abs/2506.06958</guid>
<content:encoded><![CDATA[
arXiv:2506.06958v3 Announce Type: replace 
Abstract: Simulating society with large language models (LLMs), we argue, requires more than generating plausible behavior; it demands cognitively grounded reasoning that is structured, revisable, and traceable. LLM-based agents are increasingly used to emulate individual and group behavior, primarily through prompting and supervised fine-tuning. Yet current simulations remain grounded in a behaviorist "demographics in, behavior out" paradigm, focusing on surface-level plausibility. As a result, they often lack internal coherence, causal reasoning, and belief traceability, making them unreliable for modeling how people reason, deliberate, and respond to interventions.
  To address this, we present a conceptual modeling paradigm, Generative Minds (GenMinds), which draws from cognitive science to support structured belief representations in generative agents. To evaluate such agents, we introduce the RECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess reasoning fidelity via causal traceability, demographic grounding, and intervention consistency. These contributions advance a broader shift: from surface-level mimicry to generative agents that simulate thought, not just language, for social simulations.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents</title>
<link>https://arxiv.org/abs/2506.12104</link>
<guid>https://arxiv.org/abs/2506.12104</guid>
<content:encoded><![CDATA[
arXiv:2506.12104v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly central to agentic systems due to their strong reasoning and planning capabilities. By interacting with external environments through predefined tools, these agents can carry out complex user tasks. Nonetheless, this interaction also introduces the risk of prompt injection attacks, where malicious inputs from external sources can mislead the agent's behavior, potentially resulting in economic loss, privacy leakage, or system compromise. System-level defenses have recently shown promise by enforcing static or predefined policies, but they still face two key challenges: the ability to dynamically update security rules and the need for memory stream isolation. To address these challenges, we propose DRIFT, a Dynamic Rule-based Isolation Framework for Trustworthy agentic systems, which enforces both control- and data-level constraints. A Secure Planner first constructs a minimal function trajectory and a JSON-schema-style parameter checklist for each function node based on the user query. A Dynamic Validator then monitors deviations from the original plan, assessing whether changes comply with privilege limitations and the user's intent. Finally, an Injection Isolator detects and masks any instructions that may conflict with the user query from the memory stream to mitigate long-term risks. We empirically validate the effectiveness of DRIFT on the AgentDojo and ASB benchmark, demonstrating its strong security performance while maintaining high utility across diverse models, showcasing both its robustness and adaptability. The code is released at https://github.com/SaFoLab-WISC/DRIFT.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AGC-Drive: A Large-Scale Dataset for Real-World Aerial-Ground Collaboration in Driving Scenarios</title>
<link>https://arxiv.org/abs/2506.16371</link>
<guid>https://arxiv.org/abs/2506.16371</guid>
<content:encoded><![CDATA[
arXiv:2506.16371v2 Announce Type: replace 
Abstract: By sharing information across multiple agents, collaborative perception helps autonomous vehicles mitigate occlusions and improve overall perception accuracy. While most previous work focus on vehicle-to-vehicle and vehicle-to-infrastructure collaboration, with limited attention to aerial perspectives provided by UAVs, which uniquely offer dynamic, top-down views to alleviate occlusions and monitor large-scale interactive environments. A major reason for this is the lack of high-quality datasets for aerial-ground collaborative scenarios. To bridge this gap, we present AGC-Drive, the first large-scale real-world dataset for Aerial-Ground Cooperative 3D perception. The data collection platform consists of two vehicles, each equipped with five cameras and one LiDAR sensor, and one UAV carrying a forward-facing camera and a LiDAR sensor, enabling comprehensive multi-view and multi-agent perception. Consisting of approximately 80K LiDAR frames and 360K images, the dataset covers 14 diverse real-world driving scenarios, including urban roundabouts, highway tunnels, and on/off ramps. Notably, 17% of the data comprises dynamic interaction events, including vehicle cut-ins, cut-outs, and frequent lane changes. AGC-Drive contains 350 scenes, each with approximately 100 frames and fully annotated 3D bounding boxes covering 13 object categories. We provide benchmarks for two 3D perception tasks: vehicle-to-vehicle collaborative perception and vehicle-to-UAV collaborative perception. Additionally, we release an open-source toolkit, including spatiotemporal alignment verification tools, multi-agent visualization systems, and collaborative annotation utilities. The dataset and code are available at https://github.com/PercepX/AGC-Drive.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Train Your LLM Web Agent: A Statistical Diagnosis</title>
<link>https://arxiv.org/abs/2507.04103</link>
<guid>https://arxiv.org/abs/2507.04103</guid>
<content:encoded><![CDATA[
arXiv:2507.04103v2 Announce Type: replace 
Abstract: LLM-based web agents have recently made significant progress, but much of it has occurred in closed-source systems, widening the gap with open-source alternatives. Progress has been held back by two key challenges: first, a narrow focus on single-step tasks that overlooks the complexity of multi-step web interactions; and second, the high compute costs required to post-train LLM-based web agents. To address this, we present the first statistically grounded study on compute allocation for LLM web-agent post-training. Our approach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate a Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy reinforcement learning. We find this process highly sensitive to hyperparameter choices, making exhaustive sweeps impractical. To spare others from expensive trial-and-error, we sample 1,370 configurations and use bootstrapping to estimate effective hyperparameters. Our results show that combining SFT with on-policy RL consistently outperforms either approach alone on both WorkArena and MiniWob++. Further, this strategy requires only 55% of the compute to match the peak performance of pure SFT on MiniWob++, effectively pushing the compute-performance Pareto frontier, and is the only strategy that can close the gap with closed-source models.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning with Action Chunking</title>
<link>https://arxiv.org/abs/2507.07969</link>
<guid>https://arxiv.org/abs/2507.07969</guid>
<content:encoded><![CDATA[
arXiv:2507.07969v3 Announce Type: replace 
Abstract: We present Q-chunking, a simple yet effective recipe for improving reinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks. Our recipe is designed for the offline-to-online RL setting, where the goal is to leverage an offline prior dataset to maximize the sample-efficiency of online learning. Effective exploration and sample-efficient learning remain central challenges in this setting, as it is not obvious how the offline data should be utilized to acquire a good exploratory policy. Our key insight is that action chunking, a technique popularized in imitation learning where sequences of future actions are predicted rather than a single action at each timestep, can be applied to temporal difference (TD)-based RL methods to mitigate the exploration challenge. Q-chunking adopts action chunking by directly running RL in a 'chunked' action space, enabling the agent to (1) leverage temporally consistent behaviors from offline data for more effective online exploration and (2) use unbiased $n$-step backups for more stable and efficient TD learning. Our experimental results demonstrate that Q-chunking exhibits strong offline performance and online sample efficiency, outperforming prior best offline-to-online methods on a range of long-horizon, sparse-reward manipulation tasks.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support</title>
<link>https://arxiv.org/abs/2507.13937</link>
<guid>https://arxiv.org/abs/2507.13937</guid>
<content:encoded><![CDATA[
arXiv:2507.13937v2 Announce Type: replace 
Abstract: We present Marcel, a lightweight and open-source conversational agent designed to support prospective students with admission-related inquiries. The system aims to provide fast and personalized responses, while reducing workload of university staff. We employ retrieval-augmented generation to ground answers in university resources and to provide users with verifiable, contextually relevant information. We introduce a Frequently Asked Question (FAQ) retriever that maps user questions to knowledge-base entries, which allows administrators to steer retrieval, and improves over standard dense/hybrid retrieval strategies. The system is engineered for easy deployment in resource-constrained academic settings. We detail the system architecture, provide a technical evaluation of its components, and report insights from a real-world deployment.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?</title>
<link>https://arxiv.org/abs/2507.15887</link>
<guid>https://arxiv.org/abs/2507.15887</guid>
<content:encoded><![CDATA[
arXiv:2507.15887v4 Announce Type: replace 
Abstract: Despite progress in language model (LM) capabilities, evaluations have thus far focused on models' performance on tasks that humans have previously solved, including in programming (Jimenez et al., 2024) and mathematics (Glazer et al., 2024). We therefore propose testing models' ability to design and implement algorithms in an open-ended benchmark: We task LMs with writing code that efficiently solves computationally challenging problems in computer science, physics, and mathematics. Our AlgoTune benchmark consists of 154 coding tasks collected from domain experts and a framework for validating and timing LM-synthesized solution code, which is compared to reference implementations from popular open-source packages. In addition, we develop a baseline LM agent, AlgoTuner, and evaluate its performance across a suite of frontier models. AlgoTuner uses a simple, budgeted loop that edits code, compiles and runs it, profiles performance, verifies correctness on tests, and selects the fastest valid version. AlgoTuner achieves an average 1.72x speedup against our reference solvers, which use libraries such as SciPy, sk-learn and CVXPY. However, we find that current models fail to discover algorithmic innovations, instead preferring surface-level optimizations. We hope that AlgoTune catalyzes the development of LM agents exhibiting creative problem solving beyond state-of-the-art human performance.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimuRA: A World-Model-Driven Simulative Reasoning Architecture for General Goal-Oriented Agents</title>
<link>https://arxiv.org/abs/2507.23773</link>
<guid>https://arxiv.org/abs/2507.23773</guid>
<content:encoded><![CDATA[
arXiv:2507.23773v2 Announce Type: replace 
Abstract: AI agents built on foundation models hold enormous promise. Current practice, however, focuses on a one-task-one-agent approach, which not only falls short of scalability and generality, but also faces practical limitations from black-box autoregressive reasoning, where decisions unfold token by token without explicit simulation or counterfactual evaluation of outcomes. Humans, on the other hand, reason and plan by mentally simulating the consequences of actions within an internal model of the world -- a capability that supports flexible, goal-directed behavior across diverse contexts. Moving towards a more general and powerful AI agent, we introduce SimuRA, a goal-oriented architecture for generalized agentic reasoning. Based on a principled formulation of an optimal agent in any general environment, SimuRA addresses the limitations of black-box autoregressive reasoning by incorporating the world model for planning via simulation. Our prototype world model is implemented using LLMs as a substrate, leveraging the natural language as a discrete, hierarchical representation grounded in concepts for planning, while remaining model-agnostic. On complex web-browsing tasks such as flight search, SimuRA improves the success rate from 0% to 32.2% compared to a representative open-web agent baseline. Across tasks, world-model-based planning achieves up to 124% higher task completion rates than a matched black-box autoregressive baseline, demonstrating the advantages of simulative reasoning. We release ReasonerAgent-Web, a web-browsing agent built on SimuRA, as an open-source research demo.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrated Language Models and How to Find Them with Label Smoothing</title>
<link>https://arxiv.org/abs/2508.00264</link>
<guid>https://arxiv.org/abs/2508.00264</guid>
<content:encoded><![CDATA[
arXiv:2508.00264v2 Announce Type: replace 
Abstract: Recent advances in natural language processing (NLP) have opened up greater opportunities to enable fine-tuned large language models (LLMs) to behave as more powerful interactive agents through improved instruction-following ability. However, understanding how this impacts confidence calibration for reliable model output has not been researched in full. In this work, we examine various open-sourced LLMs, identifying significant calibration degradation after instruction tuning in each. Seeking a practical solution, we look towards label smoothing, which has been shown as an effective method to regularize for overconfident predictions but has yet to be widely adopted in the supervised fine-tuning (SFT) of LLMs. We first provide insight as to why label smoothing is sufficient to maintain calibration throughout the SFT process. However, settings remain where the effectiveness of smoothing is severely diminished, in particular the case of large vocabulary LLMs (LV-LLMs). We posit the cause to stem from the ability to become over-confident, which has a direct relationship with the hidden size and vocabulary size, and justify this theoretically and experimentally. Finally, we address an outstanding issue regarding the memory footprint of the cross-entropy loss computation in the label smoothed loss setting, designing a customized kernel to dramatically reduce memory consumption without sacrificing speed or performance in comparison to existing solutions for non-smoothed losses.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LVLMs are Bad at Overhearing Human Referential Communication</title>
<link>https://arxiv.org/abs/2509.11514</link>
<guid>https://arxiv.org/abs/2509.11514</guid>
<content:encoded><![CDATA[
arXiv:2509.11514v2 Announce Type: replace 
Abstract: During spontaneous conversations, speakers collaborate on novel referring expressions, which they can then re-use in subsequent conversations. Understanding such referring expressions is an important ability for an embodied agent, so that it can carry out tasks in the real world. This requires integrating and understanding language, vision, and conversational interaction. We study the capabilities of seven state-of-the-art Large Vision Language Models (LVLMs) as overhearers to a corpus of spontaneous conversations between pairs of human discourse participants engaged in a collaborative object-matching task. We find that such a task remains challenging for current LVLMs and they all fail to show a consistent performance improvement as they overhear more conversations from the same discourse participants repeating the same task for multiple rounds. We release our corpus and code for reproducibility and to facilitate future research.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent</title>
<link>https://arxiv.org/abs/2509.15566</link>
<guid>https://arxiv.org/abs/2509.15566</guid>
<content:encoded><![CDATA[
arXiv:2509.15566v3 Announce Type: replace 
Abstract: In the field of AI-driven human-GUI interaction automation, while rapid advances in multimodal large language models and reinforcement fine-tuning techniques have yielded remarkable progress, a fundamental challenge persists: their interaction logic significantly deviates from natural human-GUI communication patterns. To fill this gap, we propose "Blink-Think-Link" (BTL), a brain-inspired framework for human-GUI interaction that mimics the human cognitive process between users and graphical interfaces. The system decomposes interactions into three biologically plausible phases: (1) Blink - rapid detection and attention to relevant screen areas, analogous to saccadic eye movements; (2) Think - higher-level reasoning and decision-making, mirroring cognitive planning; and (3) Link - generation of executable commands for precise motor control, emulating human action selection mechanisms. Additionally, we introduce two key technical innovations for the BTL framework: (1) Blink Data Generation - an automated annotation pipeline specifically optimized for blink data, and (2) BTL Reward -- the first rule-based reward mechanism that enables reinforcement learning driven by both process and outcome. Building upon this framework, we develop a GUI agent model named BTL-UI, which demonstrates competitive performance across both static GUI understanding and dynamic interaction tasks in comprehensive benchmarks. These results provide conclusive empirical validation of the framework's efficacy in developing advanced GUI Agents.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents</title>
<link>https://arxiv.org/abs/2509.18119</link>
<guid>https://arxiv.org/abs/2509.18119</guid>
<content:encoded><![CDATA[
arXiv:2509.18119v2 Announce Type: replace 
Abstract: Building general-purpose graphical user interface (GUI) agents has become increasingly promising with the progress in vision language models. However, developing effective mobile GUI agents with reinforcement learning (RL) remains challenging due to the heavy-tailed distribution of task difficulty and the inefficiency of large-scale environment sampling. We present an online agentic reinforcement learning framework MobileRL to enhance GUI agents in mobile environments. Its core component is the Difficulty-ADAptive GRPO (ADAGRPO) algorithm. In ADAGRPO, we design difficulty-adaptive positive replay and failure curriculum filtering to adapt the model to different task difficulties. We introduce the shortest-path reward adjustment strategy to reshape rewards concerning the task length in multi-turn agentic tasks. Those strategies jointly stabilize RL training, improve sample efficiency, and generate strong performance across diverse mobile apps and tasks. We apply MOBILERL to two open models (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B model achieves state-of-the-art results in terms of success rates on both AndroidWorld (80.2%) and AndroidLab (53.6%). The MOBILERL framework is open-sourced at: https://github.com/THUDM/MobileRL.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Self-Evolving Benchmarks: Synthesizing Agent Trajectories via Test-Time Exploration under Validate-by-Reproduce Paradigm</title>
<link>https://arxiv.org/abs/2510.00415</link>
<guid>https://arxiv.org/abs/2510.00415</guid>
<content:encoded><![CDATA[
arXiv:2510.00415v2 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) and agent system designs have empowered agents with unprecedented levels of capability. However, existing agent benchmarks are showing a trend of rapid ceiling-hitting by newly developed agents, making it difficult to meet the demands for evaluating agent abilities. To address this problem, we propose the Trajectory-based Validated-by-Reproducing Agent-benchmark Complexity Evolution (TRACE) framework. This framework takes an original task from an existing benchmark and encourages agents to freely explore and evolve it into a new task with higher difficulty while recording validatable agent trajectories. The framework proceeds in three stages: (1) evolutionary proposal mining, which provides task evolution proposals through preliminary exploration and divergent thinking; (2) problem formation and free exploration, where proposals are conceptualized into feasible problem candidates and the agents then explore them freely while recording their execution trajectories; and (3) multi-level validation, which ensures that the evolved tasks are accompanied by validatable and reproducible trajectories. Experiments on the GAIA benchmark demonstrate that the TRACE framework consistently enhances task complexity while improving the reliability of correctness through validatable execution trajectories. In addition, our framework can successfully adapt to and improve reasoning datasets represented by AIME-2024. This work marks a paradigm shift from static, manually curated benchmarks to dynamic, self-evolving evaluation systems, providing a sustainable and challenging runway for agent development
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RECODE-H: A Benchmark for Research Code Development with Interactive Human Feedback</title>
<link>https://arxiv.org/abs/2510.06186</link>
<guid>https://arxiv.org/abs/2510.06186</guid>
<content:encoded><![CDATA[
arXiv:2510.06186v2 Announce Type: replace 
Abstract: Large language models (LLMs) show the promise in supporting scientific research implementation, yet their ability to generate correct and executable code remains limited. Existing works largely adopt one-shot settings, ignoring the iterative and feedback-driven nature of realistic workflows of scientific research development. To address this gap, we present RECODE-H, a benchmark of 102 tasks from research papers and repositories that evaluates LLM agents through multi-turn interactions with LLM-simulated human feedback. It includes structured instructions,unit tests, and a five-level feedback hierarchy to reflect realistic researcher-agent collaboration. We further present ReCodeAgent, a framework that integrates feedback into iterative code generation. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4, DeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer feedback, while also highlighting ongoing challenges in the generation of complex research code. RECODE-H establishes a foundation for developing adaptive, feedback-driven LLM agents in scientific research implementation
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bounded-confidence opinion models with random-time interactions</title>
<link>https://arxiv.org/abs/2409.15148</link>
<guid>https://arxiv.org/abs/2409.15148</guid>
<content:encoded><![CDATA[
arXiv:2409.15148v2 Announce Type: replace-cross 
Abstract: In models of opinion dynamics, agents interact with each other and can change their opinions as a result of those interactions. One type of opinion model is a bounded-confidence model (BCM), in which opinions take continuous values and interacting agents compromise their opinions with each other if their opinions are sufficiently similar. In studies of BCMs, researchers typically assume that interactions between agents occur at deterministic times. This assumption neglects an inherent element of randomness in social interactions, and it is desirable to account for it. In this paper, we study BCMs on networks and allow agents to interact at random times. To incorporate random-time interactions, we use renewal processes to determine social-interaction event times, which can follow arbitrary interevent-time distributions (ITDs). We establish connections between these random-time-interaction BCMs and deterministic-time-interaction BCMs. We analyze the quantitative impact of ITDs on the transient dynamics of BCMs and derive approximate master equations for the time-dependent expectations of the BCM dynamics. We find that BCMs with Markovian ITDs have consistent statistical properties (in particular, they have the same expected time-dependent opinions) when the ITDs have the same mean but that the statistical properties of BCMs with non-Markovian ITDs depend on the type of ITD even when the ITDs have the same mean. Additionally, we numerically examine the transient and steady-state dynamics of our models with various ITDs on different networks and compare their expected order-parameter values and expected convergence times.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intrinsic Goals for Autonomous Agents: Model-Based Exploration in Virtual Zebrafish Predicts Ethological Behavior and Whole-Brain Dynamics</title>
<link>https://arxiv.org/abs/2506.00138</link>
<guid>https://arxiv.org/abs/2506.00138</guid>
<content:encoded><![CDATA[
arXiv:2506.00138v2 Announce Type: replace-cross 
Abstract: Autonomy is a hallmark of animal intelligence, enabling adaptive and intelligent behavior in complex environments without relying on external reward or task structure. Existing reinforcement learning approaches to exploration in reward-free environments, including a class of methods known as model-based intrinsic motivation, exhibit inconsistent exploration patterns and do not converge to an exploratory policy, thus failing to capture robust autonomous behaviors observed in animals. Moreover, systems neuroscience has largely overlooked the neural basis of autonomy, focusing instead on experimental paradigms where animals are motivated by external reward rather than engaging in ethological, naturalistic and task-independent behavior. To bridge these gaps, we introduce a novel model-based intrinsic drive explicitly designed after the principles of autonomous exploration in animals. Our method (3M-Progress) achieves animal-like exploration by tracking divergence between an online world model and a fixed prior learned from an ecological niche. To the best of our knowledge, we introduce the first autonomous embodied agent that predicts brain data entirely from self-supervised optimization of an intrinsic goal -- without any behavioral or neural training data -- demonstrating that 3M-Progress agents capture the explainable variance in behavioral patterns and whole-brain neural-glial dynamics recorded from autonomously behaving larval zebrafish, thereby providing the first goal-driven, population-level model of neural-glial computation. Our findings establish a computational framework connecting model-based intrinsic motivation to naturalistic behavior, providing a foundation for building artificial agents with animal-like autonomy.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Branch-and-Browse: Efficient and Controllable Web Exploration with Tree-Structured Reasoning and Action Memory</title>
<link>https://arxiv.org/abs/2510.19838</link>
<guid>https://arxiv.org/abs/2510.19838</guid>
<content:encoded><![CDATA[
arXiv:2510.19838v1 Announce Type: new 
Abstract: Autonomous web agents powered by large language models (LLMs) show strong potential for performing goal-oriented tasks such as information retrieval, report generation, and online transactions. These agents mark a key step toward practical embodied reasoning in open web environments. However, existing approaches remain limited in reasoning depth and efficiency: vanilla linear methods fail at multi-step reasoning and lack effective backtracking, while other search strategies are coarse-grained and computationally costly. We introduce Branch-and-Browse, a fine-grained web agent framework that unifies structured reasoning-acting, contextual memory, and efficient execution. It (i) employs explicit subtask management with tree-structured exploration for controllable multi-branch reasoning, (ii) bootstraps exploration through efficient web state replay with background reasoning, and (iii) leverages a page action memory to share explored actions within and across sessions. On the WebArena benchmark, Branch-and-Browse achieves a task success rate of 35.8\% and reduces execution time by up to 40.4\% relative to state-of-the-art methods. These results demonstrate that Branch-and-Browse is a reliable and efficient framework for LLM-based web agents.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CourtGuard: A Local, Multiagent Prompt Injection Classifier</title>
<link>https://arxiv.org/abs/2510.19844</link>
<guid>https://arxiv.org/abs/2510.19844</guid>
<content:encoded><![CDATA[
arXiv:2510.19844v1 Announce Type: new 
Abstract: As large language models (LLMs) become integrated into various sensitive applications, prompt injection, the use of prompting to induce harmful behaviors from LLMs, poses an ever increasing risk. Prompt injection attacks can cause LLMs to leak sensitive data, spread misinformation, and exhibit harmful behaviors. To defend against these attacks, we propose CourtGuard, a locally-runnable, multiagent prompt injection classifier. In it, prompts are evaluated in a court-like multiagent LLM system, where a "defense attorney" model argues the prompt is benign, a "prosecution attorney" model argues the prompt is a prompt injection, and a "judge" model gives the final classification. CourtGuard has a lower false positive rate than the Direct Detector, an LLM as-a-judge. However, CourtGuard is generally a worse prompt injection detector. Nevertheless, this lower false positive rate highlights the importance of considering both adversarial and benign scenarios for the classification of a prompt. Additionally, the relative performance of CourtGuard in comparison to other prompt injection classifiers advances the use of multiagent systems as a defense against prompt injection attacks. The implementations of CourtGuard and the Direct Detector with full prompts for Gemma-3-12b-it, Llama-3.3-8B, and Phi-4-mini-instruct are available at https://github.com/isaacwu2000/CourtGuard.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Specification's Realm: Characterizing the Knowledge Required for Executing a Given Algorithm Specification</title>
<link>https://arxiv.org/abs/2510.19853</link>
<guid>https://arxiv.org/abs/2510.19853</guid>
<content:encoded><![CDATA[
arXiv:2510.19853v1 Announce Type: new 
Abstract: An algorithm specification in natural language or pseudocode is expected to be clear and explicit enough to enable mechanical execution. In this position paper we contribute an initial characterization of the knowledge that an executing agent, human or machine, should possess in order to be able to carry out the instructions of a given algorithm specification as a stand-alone entity, independent of any system implementation. We argue that, for that algorithm specification, such prerequisite knowledge, whether unique or shared with other specifications, can be summarized in a document of practical size. We term this document the realm of the algorithm specification. The generation of such a realm is itself a systematic analytical process, significant parts of which can be automated with the help of large language models and the reuse of existing documents. The algorithm-specification's realm would consist of specification language syntax and semantics, domain knowledge restricted to the referenced entities, inter-entity relationships, relevant underlying cause-and-effect rules, and detailed instructions and means for carrying out certain operations. Such characterization of the realm can contribute to methodological implementation of the algorithm specification in diverse systems and to its formalization for mechanical verification. The paper also touches upon the question of assessing execution faithfulness, which is distinct from correctness: in the absence of a reference interpretation of natural language or pseudocode specification with a given vocabulary, how can we determine if an observed agent's execution indeed complies with the input specification.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Context Contracts - MCP-Enabled Framework to Integrate LLMs With Blockchain Smart Contracts</title>
<link>https://arxiv.org/abs/2510.19856</link>
<guid>https://arxiv.org/abs/2510.19856</guid>
<content:encoded><![CDATA[
arXiv:2510.19856v1 Announce Type: new 
Abstract: In recent years, blockchain has experienced widespread adoption across various industries, becoming integral to numerous enterprise applications. Concurrently, the rise of generative AI and LLMs has transformed human-computer interactions, offering advanced capabilities in understanding and generating human-like text. The introduction of the MCP has further enhanced AI integration by standardizing communication between AI systems and external data sources. Despite these advancements, there is still no standardized method for seamlessly integrating LLM applications and blockchain. To address this concern, we propose "MCC: Model Context Contracts" a novel framework that enables LLMs to interact directly with blockchain smart contracts through MCP-like protocol. This integration allows AI agents to invoke blockchain smart contracts, facilitating more dynamic and context-aware interactions between users and blockchain networks. Essentially, it empowers users to interact with blockchain systems and perform transactions using queries in natural language. Within this proposed architecture, blockchain smart contracts can function as intelligent agents capable of recognizing user input in natural language and executing the corresponding transactions. To ensure that the LLM accurately interprets natural language inputs and maps them to the appropriate MCP functions, the LLM was fine-tuned using a custom dataset comprising user inputs paired with their corresponding MCP server functions. This fine-tuning process significantly improved the platform's performance and accuracy. To validate the effectiveness of MCC, we have developed an end-to-end prototype implemented on the Rahasak blockchain with the fine-tuned Llama-4 LLM. To the best of our knowledge, this research represents the first approach to using the concept of Model Context Protocol to integrate LLMs with blockchain.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-Guided Multi-Agent Framework for Application-Level Software Code Generation</title>
<link>https://arxiv.org/abs/2510.19868</link>
<guid>https://arxiv.org/abs/2510.19868</guid>
<content:encoded><![CDATA[
arXiv:2510.19868v1 Announce Type: new 
Abstract: Automated code generation driven by Large Lan- guage Models (LLMs) has enhanced development efficiency, yet generating complex application-level software code remains challenging. Multi-agent frameworks show potential, but existing methods perform inadequately in large-scale application-level software code generation, failing to ensure reasonable orga- nizational structures of project code and making it difficult to maintain the code generation process. To address this, this paper envisions a Knowledge-Guided Application-Level Code Generation framework named KGACG, which aims to trans- form software requirements specification and architectural design document into executable code through a collaborative closed- loop of the Code Organization & Planning Agent (COPA), Coding Agent (CA), and Testing Agent (TA), combined with a feedback mechanism. We demonstrate the collaborative process of the agents in KGACG in a Java Tank Battle game case study while facing challenges. KGACG is dedicated to advancing the automation of application-level software development.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Integrated Approach to Neural Architecture Search for Deep Q-Networks</title>
<link>https://arxiv.org/abs/2510.19872</link>
<guid>https://arxiv.org/abs/2510.19872</guid>
<content:encoded><![CDATA[
arXiv:2510.19872v1 Announce Type: new 
Abstract: The performance of deep reinforcement learning agents is fundamentally constrained by their neural network architecture, a choice traditionally made through expensive hyperparameter searches and then fixed throughout training. This work investigates whether online, adaptive architecture optimization can escape this constraint and outperform static designs. We introduce NAS-DQN, an agent that integrates a learned neural architecture search controller directly into the DRL training loop, enabling dynamic network reconfiguration based on cumulative performance feedback. We evaluate NAS-DQN against three fixed-architecture baselines and a random search control on a continuous control task, conducting experiments over multiple random seeds. Our results demonstrate that NAS-DQN achieves superior final performance, sample efficiency, and policy stability while incurring negligible computational overhead. Critically, the learned search strategy substantially outperforms both undirected random architecture exploration and poorly-chosen fixed designs, indicating that intelligent, performance-guided search is the key mechanism driving success. These findings establish that architecture adaptation is not merely beneficial but necessary for optimal sample efficiency in online deep reinforcement learning, and suggest that the design of RL agents need not be a static offline choice but can instead be seamlessly integrated as a dynamic component of the learning process itself.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language Model Capabilities</title>
<link>https://arxiv.org/abs/2510.19892</link>
<guid>https://arxiv.org/abs/2510.19892</guid>
<content:encoded><![CDATA[
arXiv:2510.19892v1 Announce Type: new 
Abstract: Multi-modal large language models (MLMs) are often assessed on static, individual benchmarks -- which cannot jointly assess MLM capabilities in a single task -- or rely on human or model pairwise comparisons -- which is highly subjective, expensive, and allows models to exploit superficial shortcuts (e.g., verbosity) to inflate their win-rates. To overcome these issues, we propose game-based evaluations to holistically assess MLM capabilities. Games require multiple abilities for players to win, are inherently competitive, and are governed by fix, objective rules, and makes evaluation more engaging, providing a robust framework to address the aforementioned challenges. We manifest this evaluation specifically through Dixit, a fantasy card game where players must generate captions for a card that trick some, but not all players, into selecting the played card. Our quantitative experiments with five MLMs show Dixit win-rate rankings are perfectly correlated with those on popular MLM benchmarks, while games between human and MLM players in Dixit reveal several differences between agent strategies and areas of improvement for MLM reasoning.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model enabled Mathematical Modeling</title>
<link>https://arxiv.org/abs/2510.19895</link>
<guid>https://arxiv.org/abs/2510.19895</guid>
<content:encoded><![CDATA[
arXiv:2510.19895v1 Announce Type: new 
Abstract: The integration of Large Language Models (LLMs) with optimization modeling offers a promising avenue for advancing decision-making in operations research (OR). Traditional optimization methods,such as linear programming, mixed integer programming, and simulation depend heavily on domain expertise to translate real-world problems into solvable mathematical models. While solvers like Gurobi and COPT are powerful, expert input remains essential for defining objectives, constraints, and variables. This research investigates the potential of LLMs, specifically the DeepSeek-R1 model, to bridge this formulation gap using natural language understanding and code generation. Although prior models like GPT-4, Claude, and Bard have shown strong performance in NLP and reasoning tasks, their high token costs and tendency toward hallucinations limit real-world applicability in supply chain contexts. In contrast, DeepSeek-R1, a cost-efficient and high-performing model trained with reinforcement learning, presents a viable alternative. Despite its success in benchmarks such as LiveCodeBench and Math-500, its effectiveness in applied OR scenarios remains under explored. This study systematically evaluates DeepSeek-R1 across four key OR benchmarks: NL4OPT, IndustryOR, EasyLP, and ComplexOR. Our methodology includes baseline assessments, the development of a hallucination taxonomy, and the application of mitigation strategies like LLM-as-a-Judge, Few-shot Learning (FSL), Tool Calling, and a Multi-agent Framework. These techniques aim to reduce hallucinations, enhance formulation accuracy, and better align model outputs with user intent.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Supervision with Semantic and Episodic Memory: A Reflective Approach to Agent Adaptation</title>
<link>https://arxiv.org/abs/2510.19897</link>
<guid>https://arxiv.org/abs/2510.19897</guid>
<content:encoded><![CDATA[
arXiv:2510.19897v1 Announce Type: new 
Abstract: We investigate how agents built on pretrained large language models can learn target classification functions from labeled examples without parameter updates. While conventional approaches like fine-tuning are often costly, inflexible, and opaque, we propose a memory-augmented framework that leverages both labeled data and LLM-generated critiques. Our framework uses episodic memory to store instance-level critiques-capturing specific past experiences-and semantic memory to distill these into reusable, task-level guidance. Across a diverse set of tasks, incorporating critiques yields up to a 24.8 percent accuracy improvement over retrieval-based (RAG-style) baselines that rely only on labels. Through extensive empirical evaluation, we uncover distinct behavioral differences between OpenAI and opensource models, particularly in how they handle fact-oriented versus preference-based data. To interpret how models respond to different representations of supervision encoded in memory, we introduce a novel metric, suggestibility. This helps explain observed behaviors and illuminates how model characteristics and memory strategies jointly shape learning dynamics. Our findings highlight the promise of memory-driven, reflective learning for building more adaptive and interpretable LLM agents.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills</title>
<link>https://arxiv.org/abs/2510.19898</link>
<guid>https://arxiv.org/abs/2510.19898</guid>
<content:encoded><![CDATA[
arXiv:2510.19898v1 Announce Type: new 
Abstract: High quality bugs are key to training the next generation of language model based software engineering (SWE) agents. We introduce a novel method for synthetic generation of difficult and diverse bugs. Our method instructs SWE Agents to introduce a feature into the codebase whereby they may unintentionally break tests, resulting in bugs. Prior approaches often induce an out-of-distribution effect by generating bugs intentionally (e.g. by introducing local perturbation to existing code), which does not reflect realistic development processes. We perform qualitative analysis to demonstrate that our approach for generating bugs more closely reflects the patterns found in human-authored edits. Through extensive experiments, we demonstrate that our bugs provide more efficient training data for supervised fine-tuning, outperforming other bug datasets by 2% with half the training data (1.2k vs. 3k bugs). We train on our newly generated bugs in addition to existing bug datasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench Verified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on SWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over three seeds.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surfer 2: The Next Generation of Cross-Platform Computer Use Agents</title>
<link>https://arxiv.org/abs/2510.19949</link>
<guid>https://arxiv.org/abs/2510.19949</guid>
<content:encoded><![CDATA[
arXiv:2510.19949v1 Announce Type: new 
Abstract: Building agents that generalize across web, desktop, and mobile environments remains an open challenge, as prior systems rely on environment-specific interfaces that limit cross-platform deployment. We introduce Surfer 2, a unified architecture operating purely from visual observations that achieves state-of-the-art performance across all three environments. Surfer 2 integrates hierarchical context management, decoupled planning and execution, and self-verification with adaptive recovery, enabling reliable operation over long task horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior systems without task-specific fine-tuning. With multiple attempts, Surfer 2 exceeds human performance on all benchmarks. These results demonstrate that systematic orchestration amplifies foundation model capabilities and enables general-purpose computer control through visual interaction alone, while calling for a next-generation vision language model to achieve Pareto-optimal cost-efficiency.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets</title>
<link>https://arxiv.org/abs/2510.19950</link>
<guid>https://arxiv.org/abs/2510.19950</guid>
<content:encoded><![CDATA[
arXiv:2510.19950v1 Announce Type: new 
Abstract: In financial applications, reinforcement learning (RL) agents are commonly trained on historical data, where their actions do not influence prices. However, during deployment, these agents trade in live markets where their own transactions can shift asset prices, a phenomenon known as market impact. This mismatch between training and deployment environments can significantly degrade performance. Traditional robust RL approaches address this model misspecification by optimizing the worst-case performance over a set of uncertainties, but typically rely on symmetric structures that fail to capture the directional nature of market impact. To address this issue, we develop a novel class of elliptic uncertainty sets. We establish both implicit and explicit closed-form solutions for the worst-case uncertainty under these sets, enabling efficient and tractable robust policy evaluation. Experiments on single-asset and multi-asset trading tasks demonstrate that our method achieves superior Sharpe ratio and remains robust under increasing trade volumes, offering a more faithful and scalable approach to RL in financial markets.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic social data: trials and tribulations</title>
<link>https://arxiv.org/abs/2510.19952</link>
<guid>https://arxiv.org/abs/2510.19952</guid>
<content:encoded><![CDATA[
arXiv:2510.19952v1 Announce Type: new 
Abstract: Large Language Models are being used in conversational agents that simulate human conversations and generate social studies data. While concerns about the models' biases have been raised and discussed in the literature, much about the data generated is still unknown. In this study we explore the statistical representation of social values across four countries (UK, Argentina, USA and China) for six LLMs, with equal representation for open and closed weights. By comparing machine-generated outputs with actual human survey data, we assess whether algorithmic biases in LLMs outweigh the biases inherent in real- world sampling, including demographic and response biases. Our findings suggest that, despite the logistical and financial constraints of human surveys, even a small, skewed sample of real respondents may provide more reliable insights than synthetic data produced by LLMs. These results highlight the limitations of using AI-generated text for social research and emphasize the continued importance of empirical human data collection.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Tutorial on Cognitive Biases in Agentic AI-Driven 6G Autonomous Networks</title>
<link>https://arxiv.org/abs/2510.19973</link>
<guid>https://arxiv.org/abs/2510.19973</guid>
<content:encoded><![CDATA[
arXiv:2510.19973v1 Announce Type: new 
Abstract: The path to higher network autonomy in 6G lies beyond the mere optimization of key performance indicators (KPIs). While KPIs have enabled automation gains under TM Forum Levels 1--3, they remain numerical abstractions that act only as proxies for the real essence of communication networks: seamless connectivity, fairness, adaptability, and resilience. True autonomy requires perceiving and reasoning over the network environment as it is. Such progress can be achieved through \emph{agentic AI}, where large language model (LLM)-powered agents perceive multimodal telemetry, reason with memory, negotiate across domains, and act via APIs to achieve multi-objective goals. However, deploying such agents introduces the challenge of cognitive biases inherited from human design, which can distort reasoning, negotiation, tool use, and actuation. Between neuroscience and AI, this paper provides a tutorial on a selection of well-known biases, including their taxonomy, definition, mathematical formulation, emergence in telecom systems and the commonly impacted agentic components. The tutorial also presents various mitigation strategies tailored to each type of bias. The article finally provides two practical use-cases, which tackle the emergence, impact and mitigation gain of some famous biases in 6G inter-slice and cross-domain management. In particular, anchor randomization, temporal decay and inflection bonus techniques are introduced to specifically address anchoring, temporal and confirmation biases. This avoids that agents stick to the initial high resource allocation proposal or decisions that are recent and/or confirming a prior hypothesis. By grounding decisions in a richer and fairer set of past experiences, the quality and bravery of the agentic agreements in the second use-case, for instance, are leading to $\times 5$ lower latency and around $40\%$ higher energy saving.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication to Completion: Modeling Collaborative Workflows with Intelligent Multi-Agent Communication</title>
<link>https://arxiv.org/abs/2510.19995</link>
<guid>https://arxiv.org/abs/2510.19995</guid>
<content:encoded><![CDATA[
arXiv:2510.19995v1 Announce Type: new 
Abstract: Teamwork in workspace for complex tasks requires diverse communication strategies, but current multi-agent LLM systems lack systematic frameworks for task oriented communication. We introduce Communication to Completion (C2C), a scalable framework that addresses this gap through two key innovations: (1) the Alignment Factor (AF), a novel metric quantifying agent task alignment that directly impacts work efficiency, and (2) a Sequential Action Framework that integrates stepwise execution with intelligent communication decisions. C2C enables agents to make cost aware communication choices, dynamically improving task understanding through targeted interactions. We evaluated C2C on realistic coding workflows across three complexity tiers and team sizes from 5 to 17 agents, comparing against no communication and fixed steps baselines. The results show that C2C reduces the task completion time by about 40% with acceptable communication costs. The framework completes all tasks successfully in standard configurations and maintains effectiveness at scale. C2C establishes both a theoretical foundation for measuring communication effectiveness in multi-agent systems and a practical framework for complex collaborative tasks.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMAS$^2$: Joint Agent Selection and Information-Theoretic Coordinated Perception In Dec-POMDPs</title>
<link>https://arxiv.org/abs/2510.20009</link>
<guid>https://arxiv.org/abs/2510.20009</guid>
<content:encoded><![CDATA[
arXiv:2510.20009v1 Announce Type: new 
Abstract: We study the problem of jointly selecting sensing agents and synthesizing decentralized active perception policies for the chosen subset of agents within a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) framework. Our approach employs a two-layer optimization structure. In the inner layer, we introduce information-theoretic metrics, defined by the mutual information between the unknown trajectories or some hidden property in the environment and the collective partial observations in the multi-agent system, as a unified objective for active perception problems. We employ various optimization methods to obtain optimal sensor policies that maximize mutual information for distinct active perception tasks. In the outer layer, we prove that under certain conditions, the information-theoretic objectives are monotone and submodular with respect to the subset of observations collected from multiple agents. We then exploit this property to design an IMAS$^2$ (Information-theoretic Multi-Agent Selection and Sensing) algorithm for joint sensing agent selection and sensing policy synthesis. However, since the policy search space is infinite, we adapt the classical Nemhauser-Wolsey argument to prove that the proposed IMAS$^2$ algorithm can provide a tight $(1 - 1/e)$-guarantee on the performance. Finally, we demonstrate the effectiveness of our approach in a multi-agent cooperative perception in a grid-world environment.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SALT: Step-level Advantage Assignment for Long-horizon Agents via Trajectory Graph</title>
<link>https://arxiv.org/abs/2510.20022</link>
<guid>https://arxiv.org/abs/2510.20022</guid>
<content:encoded><![CDATA[
arXiv:2510.20022v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, enabling language agents to excel at single-turn tasks. However, their application to complex, multi-step, and long-horizon tasks remains challenging. While reinforcement learning (RL) offers a promising avenue for addressing these challenges, mainstream approaches typically rely solely on sparse, outcome-based rewards, a limitation that becomes especially problematic for group-based RL algorithms lacking critic models, such as Group Relative Policy Optimization (GRPO). In such methods, uniformly rewarding or penalizing all actions within a trajectory can lead to training instability and suboptimal policies, because beneficial and detrimental actions are often entangled across multi-step interactions. To address this challenge, we propose SALT, a novel and lightweight framework that provides a finer-grained advantage assignment, derived solely from outcome rewards. We achieve this by constructing a graph from trajectories of the same prompt, which allows us to quantify the quality of each step and assign advantages accordingly. Crucially, SALT is designed as a plug-and-play module that seamlessly integrates with existing group-based RL algorithms, requiring no modifications to the rollout procedure and introducing negligible computational overhead. Extensive experiments on the WebShop, ALFWorld, and AppWorld benchmarks with various model sizes demonstrate that SALT consistently improves performance. We also conduct a thorough analysis to validate the design choices behind SALT and offer actionable insights.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolScope: Enhancing LLM Agent Tool Use through Tool Merging and Context-Aware Filtering</title>
<link>https://arxiv.org/abs/2510.20036</link>
<guid>https://arxiv.org/abs/2510.20036</guid>
<content:encoded><![CDATA[
arXiv:2510.20036v1 Announce Type: new 
Abstract: Large language model (LLM) agents rely on external tools to solve complex tasks, but real-world toolsets often contain redundant tools with overlapping names and descriptions, introducing ambiguity and reducing selection accuracy. LLMs also face strict input context limits, preventing efficient consideration of large toolsets. To address these challenges, we propose ToolScope, which includes: (1) ToolScopeMerger with Auto-Correction to automatically audit and fix tool merges, reducing redundancy, and (2) ToolScopeRetriever to rank and select only the most relevant tools for each query, compressing toolsets to fit within context limits without sacrificing accuracy. Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI PB: A Grounded Generative Agent for Personalized Investment Insights</title>
<link>https://arxiv.org/abs/2510.20099</link>
<guid>https://arxiv.org/abs/2510.20099</guid>
<content:encoded><![CDATA[
arXiv:2510.20099v1 Announce Type: new 
Abstract: We present AI PB, a production-scale generative agent deployed in real retail finance. Unlike reactive chatbots that answer queries passively, AI PB proactively generates grounded, compliant, and user-specific investment insights. It integrates (i) a component-based orchestration layer that deterministically routes between internal and external LLMs based on data sensitivity, (ii) a hybrid retrieval pipeline using OpenSearch and the finance-domain embedding model, and (iii) a multi-stage recommendation mechanism combining rule heuristics, sequential behavioral modeling, and contextual bandits. Operating fully on-premises under Korean financial regulations, the system employs Docker Swarm and vLLM across 24 X NVIDIA H100 GPUs. Through human QA and system metrics, we demonstrate that grounded generation with explicit routing and layered safety can deliver trustworthy AI insights in high-stakes finance.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-Centered LLM-Agent System for Detecting Anomalous Digital Asset Transactions</title>
<link>https://arxiv.org/abs/2510.20102</link>
<guid>https://arxiv.org/abs/2510.20102</guid>
<content:encoded><![CDATA[
arXiv:2510.20102v1 Announce Type: new 
Abstract: We present HCLA, a human-centered multi-agent system for anomaly detection in digital asset transactions. The system links three roles: Parsing, Detection, and Explanation, into a conversational workflow that lets non-experts ask questions in natural language, inspect structured analytics, and obtain context-aware rationales. Implemented with an open-source web UI, HCLA translates user intents into a schema for a classical detector (XGBoost in our prototype) and returns narrative explanations grounded in the underlying features. On a labeled Bitcoin mixing dataset (Wasabi Wallet, 2020-2024), the baseline detector reaches strong accuracy, while HCLA adds interpretability and interactive refinement. We describe the architecture, interaction loop, dataset, evaluation protocol, and limitations, and discuss how a human-in-the-loop design improves transparency and trust in financial forensics.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Competition is the key: A Game Theoretic Causal Discovery Approach</title>
<link>https://arxiv.org/abs/2510.20106</link>
<guid>https://arxiv.org/abs/2510.20106</guid>
<content:encoded><![CDATA[
arXiv:2510.20106v1 Announce Type: new 
Abstract: Causal discovery remains a central challenge in machine learning, yet existing methods face a fundamental gap: algorithms like GES and GraN-DAG achieve strong empirical performance but lack finite-sample guarantees, while theoretically principled approaches fail to scale. We close this gap by introducing a game-theoretic reinforcement learning framework for causal discovery, where a DDQN agent directly competes against a strong baseline (GES or GraN-DAG), always warm-starting from the opponent's solution. This design yields three provable guarantees: the learned graph is never worse than the opponent, warm-starting strictly accelerates convergence, and most importantly, with high probability the algorithm selects the true best candidate graph. To the best of our knowledge, our result makes a first-of-its-kind progress in explaining such finite-sample guarantees in causal discovery: on synthetic SEMs (30 nodes), the observed error probability decays with n, tightly matching theory. On real-world benchmarks including Sachs, Asia, Alarm, Child, Hepar2, Dream, and Andes, our method consistently improves upon GES and GraN-DAG while remaining theoretically safe. Remarkably, it scales to large graphs such as Hepar2 (70 nodes), Dream (100 nodes), and Andes (220 nodes). Together, these results establish a new class of RL-based causal discovery algorithms that are simultaneously provably consistent, sample-efficient, and practically scalable, marking a decisive step toward unifying empirical performance with rigorous finite-sample theory.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking</title>
<link>https://arxiv.org/abs/2510.20168</link>
<guid>https://arxiv.org/abs/2510.20168</guid>
<content:encoded><![CDATA[
arXiv:2510.20168v1 Announce Type: new 
Abstract: Current search agents fundamentally lack the ability to simultaneously perform \textit{deep} reasoning over multi-hop retrieval and \textit{wide}-scale information collection-a critical deficiency for real-world applications like comprehensive market analysis and business development. To bridge this gap, we introduce DeepWideSearch, the first benchmark explicitly designed to evaluate agents to integrate depth and width in information seeking. In DeepWideSearch, agents must process a large volume of data, each requiring deep reasoning over multi-hop retrieval paths. Specifically, we propose two methods to converse established datasets, resulting in a curated collection of 220 questions spanning 15 diverse domains. Extensive experiments demonstrate that even state-of-the-art agents achieve only 2.39% average success rate on DeepWideSearch, highlighting the substantial challenge of integrating depth and width search in information-seeking tasks. Furthermore, our error analysis reveals four failure modes: lack of reflection, overreliance on internal knowledge, insufficient retrieval, and context overflow-exposing key limitations in current agent architectures. We publicly release DeepWideSearch to catalyze future research on more capable and robust information-seeking agents.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture-of-Minds: Multi-Agent Reinforcement Learning for Table Understanding</title>
<link>https://arxiv.org/abs/2510.20176</link>
<guid>https://arxiv.org/abs/2510.20176</guid>
<content:encoded><![CDATA[
arXiv:2510.20176v1 Announce Type: new 
Abstract: Understanding and reasoning over tables is a critical capability for many real-world applications. Large language models (LLMs) have shown promise on this task, but current approaches remain limited. Fine-tuning based methods strengthen language reasoning; yet they are prone to arithmetic errors and hallucination. In contrast, tool-based methods enable precise table manipulation but rely on rigid schemas and lack semantic understanding. These complementary drawbacks highlight the need for approaches that integrate robust reasoning with reliable table processing. In this work, we propose Mixture-of-Minds, a multi-agent framework that decomposes table reasoning into three specialized roles: planning, coding, and answering. This design enables each agent to focus on a specific aspect of the task while leveraging code execution for precise table manipulation. Building on this workflow, we introduce a self-improvement training framework that employs Monte Carlo Tree Search (MCTS) rollouts to generate pseudo-gold trajectories and optimize agents with reinforcement learning (RL). Extensive experiments show that Mixture-of-Minds delivers substantial gains, reaching 62.13% on TableBench and surpassing OpenAI-o4-mini-high. These results demonstrate the promise of combining structured multi-agent workflows with RL to advance table understanding.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Video Models as Simulators of Multi-Person Pedestrian Trajectories</title>
<link>https://arxiv.org/abs/2510.20182</link>
<guid>https://arxiv.org/abs/2510.20182</guid>
<content:encoded><![CDATA[
arXiv:2510.20182v1 Announce Type: new 
Abstract: Large-scale video generation models have demonstrated high visual realism in diverse contexts, spurring interest in their potential as general-purpose world simulators. Existing benchmarks focus on individual subjects rather than scenes with multiple interacting people. However, the plausibility of multi-agent dynamics in generated videos remains unverified. We propose a rigorous evaluation protocol to benchmark text-to-video (T2V) and image-to-video (I2V) models as implicit simulators of pedestrian dynamics. For I2V, we leverage start frames from established datasets to enable comparison with a ground truth video dataset. For T2V, we develop a prompt suite to explore diverse pedestrian densities and interactions. A key component is a method to reconstruct 2D bird's-eye view trajectories from pixel-space without known camera parameters. Our analysis reveals that leading models have learned surprisingly effective priors for plausible multi-agent behavior. However, failure modes like merging and disappearing people highlight areas for future improvement.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Merge and Conquer: Evolutionarily Optimizing AI for 2048</title>
<link>https://arxiv.org/abs/2510.20205</link>
<guid>https://arxiv.org/abs/2510.20205</guid>
<content:encoded><![CDATA[
arXiv:2510.20205v1 Announce Type: new 
Abstract: Optimizing artificial intelligence (AI) for dynamic environments remains a fundamental challenge in machine learning research. In this paper, we examine evolutionary training methods for optimizing AI to solve the game 2048, a 2D sliding puzzle. 2048, with its mix of strategic gameplay and stochastic elements, presents an ideal playground for studying decision-making, long-term planning, and dynamic adaptation. We implemented two distinct systems: a two-agent metaprompting system where a "thinker" large language model (LLM) agent refines gameplay strategies for an "executor" LLM agent, and a single-agent system based on refining a value function for a limited Monte Carlo Tree Search. We also experimented with rollback features to avoid performance degradation. Our results demonstrate the potential of evolutionary refinement techniques in improving AI performance in non-deterministic environments. The single-agent system achieved substantial improvements, with an average increase of 473.2 points per cycle, and with clear upward trends (correlation $\rho$=0.607) across training cycles. The LLM's understanding of the game grew as well, shown in its development of increasingly advanced strategies. Conversely, the two-agent system did not garner much improvement, highlighting the inherent limits of meta-prompting.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Cloud Infrastructure-as-Code Reconciliation with AI Agents</title>
<link>https://arxiv.org/abs/2510.20211</link>
<guid>https://arxiv.org/abs/2510.20211</guid>
<content:encoded><![CDATA[
arXiv:2510.20211v1 Announce Type: new 
Abstract: Cloud infrastructure is managed through a mix of interfaces -- traditionally, cloud consoles, command-line interfaces (CLI), and SDKs are the tools of choice. Recently, Infrastructure-as-Code/IaC frameworks (e.g., Terraform) have quickly gained popularity. Unlike conventional tools, IaC~frameworks encode the infrastructure in a "source-of-truth" configuration. They are capable of automatically carrying out modifications to the cloud -- deploying, updating, or destroying resources -- to bring the actual infrastructure into alignment with the IaC configuration. However, when IaC is used alongside consoles, CLIs, or SDKs, it loses visibility into external changes, causing infrastructure drift, where the configuration becomes outdated, and later IaC operations may undo valid updates or trigger errors.
  We present NSync, an automated system for IaC reconciliation that propagates out-of-band changes back into the IaC program. Our key insight is that infrastructure changes eventually all occur via cloud API invocations -- the lowest layer for cloud management operations. NSync gleans insights from API traces to detect drift (i.e., non-IaC changes) and reconcile it (i.e., update the IaC configuration to capture the changes). It employs an agentic architecture that leverages LLMs to infer high-level intents from noisy API sequences, synthesize targeted IaC updates using specialized tools, and continually improve through a self-evolving knowledge base of past reconciliations. We further introduce a novel evaluation pipeline for injecting realistic drifts into cloud infrastructure and assessing reconciliation performance. Experiments across five real-world Terraform projects and 372 drift scenarios show that NSync outperforms the baseline both in terms of accuracy (from 0.71 to 0.97 pass@3) and token efficiency (1.47$\times$ improvement).
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-order Interactions Modeling for Interpretable Multi-Agent Q-Learning</title>
<link>https://arxiv.org/abs/2510.20218</link>
<guid>https://arxiv.org/abs/2510.20218</guid>
<content:encoded><![CDATA[
arXiv:2510.20218v1 Announce Type: new 
Abstract: The ability to model interactions among agents is crucial for effective coordination and understanding their cooperation mechanisms in multi-agent reinforcement learning (MARL). However, previous efforts to model high-order interactions have been primarily hindered by the combinatorial explosion or the opaque nature of their black-box network structures. In this paper, we propose a novel value decomposition framework, called Continued Fraction Q-Learning (QCoFr), which can flexibly capture arbitrary-order agent interactions with only linear complexity $\mathcal{O}\left({n}\right)$ in the number of agents, thus avoiding the combinatorial explosion when modeling rich cooperation. Furthermore, we introduce the variational information bottleneck to extract latent information for estimating credits. This latent information helps agents filter out noisy interactions, thereby significantly enhancing both cooperation and interpretability. Extensive experiments demonstrate that QCoFr not only consistently achieves better performance but also provides interpretability that aligns with our theoretical analysis.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards AI Agents for Course Instruction in Higher Education: Early Experiences from the Field</title>
<link>https://arxiv.org/abs/2510.20255</link>
<guid>https://arxiv.org/abs/2510.20255</guid>
<content:encoded><![CDATA[
arXiv:2510.20255v1 Announce Type: new 
Abstract: This article presents early findings from designing, deploying and evaluating an AI-based educational agent deployed as the primary instructor in a graduate-level Cloud Computing course at IISc. We detail the design of a Large Language Model (LLM)-driven Instructor Agent, and introduce a pedagogical framework that integrates the Instructor Agent into the course workflow for actively interacting with the students for content delivery, supplemented by the human instructor to offer the course structure and undertake question--answer sessions. We also propose an analytical framework that evaluates the Agent--Student interaction transcripts using interpretable engagement metrics of topic coverage, topic depth and turn-level elaboration. We report early experiences on how students interact with the Agent to explore concepts, clarify doubts and sustain inquiry-driven dialogue during live classroom sessions. We also report preliminary analysis on our evaluation metrics applied across two successive instructional modules that reveals patterns of engagement evolution, transitioning from broad conceptual exploration to deeper, focused inquiry. These demonstrate how structured integration of conversational AI agents can foster reflective learning, offer a reproducible methodology for studying engagement in authentic classroom settings, and support scalable, high-quality higher education.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Large Language Models for Abstraction of Planning Domains - Extended Version</title>
<link>https://arxiv.org/abs/2510.20258</link>
<guid>https://arxiv.org/abs/2510.20258</guid>
<content:encoded><![CDATA[
arXiv:2510.20258v1 Announce Type: new 
Abstract: Generating an abstraction of a dynamic domain that aligns with a given purpose remains a significant challenge given that the choice of such an abstraction can impact an agent's ability to plan, reason, and provide explanations effectively. We model the agent's concrete behaviors in PDDL and investigate the use of in-context learning with large language models (LLMs) for the generation of abstract PDDL domains and problem instances, given an abstraction objective specified in natural language. The benchmark examples we use are new and have not been part of the data any LLMs have been trained on. We consider three categories of abstractions: abstraction of choice of alternative concrete actions, abstraction of sequences of concrete actions, and abstraction of action/predicate parameters, as well as combinations of these. The generated abstract PDDL domains and problem instances are then checked by symbolic validation tools as well as human experts. Our experiments show that GPT-4o can generally synthesize useful planning domain abstractions in simple settings, although it is better at abstracting over actions than over the associated fluents.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kinaema: a recurrent sequence model for memory and pose in motion</title>
<link>https://arxiv.org/abs/2510.20261</link>
<guid>https://arxiv.org/abs/2510.20261</guid>
<content:encoded><![CDATA[
arXiv:2510.20261v1 Announce Type: new 
Abstract: One key aspect of spatially aware robots is the ability to "find their bearings", ie. to correctly situate themselves in previously seen spaces. In this work, we focus on this particular scenario of continuous robotics operations, where information observed before an actual episode start is exploited to optimize efficiency. We introduce a new model, Kinaema, and agent, capable of integrating a stream of visual observations while moving in a potentially large scene, and upon request, processing a query image and predicting the relative position of the shown space with respect to its current position. Our model does not explicitly store an observation history, therefore does not have hard constraints on context length. It maintains an implicit latent memory, which is updated by a transformer in a recurrent way, compressing the history of sensor readings into a compact representation. We evaluate the impact of this model in a new downstream task we call "Mem-Nav". We show that our large-capacity recurrent model maintains a useful representation of the scene, navigates to goals observed before the actual episode start, and is computationally efficient, in particular compared to classical transformers with attention over an observation history.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases</title>
<link>https://arxiv.org/abs/2510.20270</link>
<guid>https://arxiv.org/abs/2510.20270</guid>
<content:encoded><![CDATA[
arXiv:2510.20270v1 Announce Type: new 
Abstract: The tendency to find and exploit "shortcuts" to complete tasks poses significant risks for reliable assessment and deployment of large language models (LLMs). For example, an LLM agent with access to unit tests may delete failing tests rather than fix the underlying bug. Such behavior undermines both the validity of benchmark results and the reliability of real-world LLM coding assistant deployments.
  To quantify, study, and mitigate such behavior, we introduce ImpossibleBench, a benchmark framework that systematically measures LLM agents' propensity to exploit test cases. ImpossibleBench creates "impossible" variants of tasks from existing benchmarks like LiveCodeBench and SWE-bench by introducing direct conflicts between the natural-language specification and the unit tests. We measure an agent's "cheating rate" as its pass rate on these impossible tasks, where any pass necessarily implies a specification-violating shortcut.
  As a practical framework, ImpossibleBench is not just an evaluation but a versatile tool. We demonstrate its utility for: (1) studying model behaviors, revealing more fine-grained details of cheating behaviors from simple test modification to complex operator overloading; (2) context engineering, showing how prompt, test access and feedback loop affect cheating rates; and (3) developing monitoring tools, providing a testbed with verified deceptive solutions. We hope ImpossibleBench serves as a useful framework for building more robust and reliable LLM systems.
  Our implementation can be found at https://github.com/safety-research/impossiblebench.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Generation to Attribution: Music AI Agent Architectures for the Post-Streaming Era</title>
<link>https://arxiv.org/abs/2510.20276</link>
<guid>https://arxiv.org/abs/2510.20276</guid>
<content:encoded><![CDATA[
arXiv:2510.20276v1 Announce Type: new 
Abstract: Generative AI is reshaping music creation, but its rapid growth exposes structural gaps in attribution, rights management, and economic models. Unlike past media shifts, from live performance to recordings, downloads, and streaming, AI transforms the entire lifecycle of music, collapsing boundaries between creation, distribution, and monetization. However, existing streaming systems, with opaque and concentrated royalty flows, are ill-equipped to handle the scale and complexity of AI-driven production. We propose a content-based Music AI Agent architecture that embeds attribution directly into the creative workflow through block-level retrieval and agentic orchestration. Designed for iterative, session-based interaction, the system organizes music into granular components (Blocks) stored in BlockDB; each use triggers an Attribution Layer event for transparent provenance and real-time settlement. This framework reframes AI from a generative tool into infrastructure for a Fair AI Media Platform. By enabling fine-grained attribution, equitable compensation, and participatory engagement, it points toward a post-streaming paradigm where music functions not as a static catalog but as a collaborative and adaptive ecosystem.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning</title>
<link>https://arxiv.org/abs/2510.20286</link>
<guid>https://arxiv.org/abs/2510.20286</guid>
<content:encoded><![CDATA[
arXiv:2510.20286v1 Announce Type: new 
Abstract: GUI grounding, which maps natural-language instructions to actionable UI elements, is a core capability of GUI agents. Prior works largely treats instructions as a static proxy for user intent, overlooking the impact of instruction diversity and quality on grounding performance. Through a careful investigation of existing grounding datasets, we find a 23.3% flaw rate in their instructions and show that inference-time exploitation of instruction diversity yields up to a substantial 76% relative performance improvement. In this paper, we introduce the Instruction-as-Reasoning paradigm, treating instructions as dynamic analytical pathways that offer distinct perspectives and enabling the model to select the most effective pathway during reasoning. To achieve this, we propose a two-stage training framework: supervised fine-tuning (SFT) on synthesized, diverse instructions to instill multi-perspective reasoning, followed by reinforcement learning (RL) to optimize pathway selection and composition. Our resulting models, UI-Ins-7B and UI-Ins-32B, achieve state-of-the-art results on five challenging grounding benchmarks and exhibit emergent reasoning, selectively composing and synthesizing novel instruction pathways at inference. In particular, UI-Ins-32B attains the best grounding accuracy, scoring 87.3% on UI-I2E-Bench, 57.0% on ScreenSpot-Pro, and 84.9% on MMBench-GUI L2. Furthermore, our model demonstrates strong agentic potential, achieving a 74.1% success rate on AndroidWorld using UI-Ins-7B as the executor. Our in-depth analysis reveals additional insights such as how reasoning can be formulated to enhance rather than hinder grounding performance, and how our method mitigates policy collapse in the SFT+RL framework. All code and model checkpoints will be publicly released in https://github.com/alibaba/UI-Ins.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Step Reasoning for Embodied Question Answering via Tool Augmentation</title>
<link>https://arxiv.org/abs/2510.20310</link>
<guid>https://arxiv.org/abs/2510.20310</guid>
<content:encoded><![CDATA[
arXiv:2510.20310v1 Announce Type: new 
Abstract: Embodied Question Answering (EQA) requires agents to explore 3D environments to obtain observations and answer questions related to the scene. Existing methods leverage VLMs to directly explore the environment and answer questions without explicit thinking or planning, which limits their reasoning ability and results in excessive or inefficient exploration as well as ineffective responses. In this paper, we introduce ToolEQA, an agent that integrates external tools with multi-step reasoning, where external tools can provide more useful information for completing the task, helping the model derive better exploration directions in the next step of reasoning and thus obtaining additional effective information. This enables ToolEQA to generate more accurate responses with a shorter exploration distance. To enhance the model's ability for tool-usage and multi-step reasoning, we further design a novel EQA data generation pipeline that automatically constructs large-scale EQA tasks with reasoning trajectories and corresponding answers. Based on the pipeline, we collect the EQA-RT dataset that contains about 18K tasks, divided into a training set EQA-RT-Train, and two test sets EQA-RT-Seen (scenes overlapping with the training set) and EQA-RT-Unseen (novel scenes). Experiments on EQA-RT-Seen and EQA-RT-Unseen show that ToolEQA improves the success rate by 9.2~20.2% over state-of-the-art baselines, while outperforming the zero-shot ToolEQA by 10% in success rate. In addition, ToolEQA also achieves state-of-the-art performance on the HM-EQA, OpenEQA, and EXPRESS-Bench datasets, demonstrating its generality. Our homepage see https://tooleqa.github.io.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GhostEI-Bench: Do Mobile Agents Resilience to Environmental Injection in Dynamic On-Device Environments?</title>
<link>https://arxiv.org/abs/2510.20333</link>
<guid>https://arxiv.org/abs/2510.20333</guid>
<content:encoded><![CDATA[
arXiv:2510.20333v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) are increasingly deployed as autonomous agents to navigate mobile graphical user interfaces (GUIs). Operating in dynamic on-device ecosystems, which include notifications, pop-ups, and inter-app interactions, exposes them to a unique and underexplored threat vector: environmental injection. Unlike prompt-based attacks that manipulate textual instructions, environmental injection corrupts an agent's visual perception by inserting adversarial UI elements (for example, deceptive overlays or spoofed notifications) directly into the GUI. This bypasses textual safeguards and can derail execution, causing privacy leakage, financial loss, or irreversible device compromise. To systematically evaluate this threat, we introduce GhostEI-Bench, the first benchmark for assessing mobile agents under environmental injection attacks within dynamic, executable environments. Moving beyond static image-based assessments, GhostEI-Bench injects adversarial events into realistic application workflows inside fully operational Android emulators and evaluates performance across critical risk scenarios. We further propose a judge-LLM protocol that conducts fine-grained failure analysis by reviewing the agent's action trajectory alongside the corresponding screenshot sequence, pinpointing failure in perception, recognition, or reasoning. Comprehensive experiments on state-of-the-art agents reveal pronounced vulnerability to deceptive environmental cues: current models systematically fail to perceive and reason about manipulated UIs. GhostEI-Bench provides a framework for quantifying and mitigating this emerging threat, paving the way toward more robust and secure embodied agents.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-empowered knowledge graph construction: A survey</title>
<link>https://arxiv.org/abs/2510.20345</link>
<guid>https://arxiv.org/abs/2510.20345</guid>
<content:encoded><![CDATA[
arXiv:2510.20345v1 Announce Type: new 
Abstract: Knowledge Graphs (KGs) have long served as a fundamental infrastructure for structured knowledge representation and reasoning. With the advent of Large Language Models (LLMs), the construction of KGs has entered a new paradigm-shifting from rule-based and statistical pipelines to language-driven and generative frameworks. This survey provides a comprehensive overview of recent progress in LLM-empowered knowledge graph construction, systematically analyzing how LLMs reshape the classical three-layered pipeline of ontology engineering, knowledge extraction, and knowledge fusion.
  We first revisit traditional KG methodologies to establish conceptual foundations, and then review emerging LLM-driven approaches from two complementary perspectives: schema-based paradigms, which emphasize structure, normalization, and consistency; and schema-free paradigms, which highlight flexibility, adaptability, and open discovery. Across each stage, we synthesize representative frameworks, analyze their technical mechanisms, and identify their limitations.
  Finally, the survey outlines key trends and future research directions, including KG-based reasoning for LLMs, dynamic knowledge memory for agentic systems, and multimodal KG construction. Through this systematic review, we aim to clarify the evolving interplay between LLMs and knowledge graphs, bridging symbolic knowledge engineering and neural semantic understanding toward the development of adaptive, explainable, and intelligent knowledge systems.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing Specialization and Centralization: A Multi-Agent Reinforcement Learning Benchmark for Sequential Industrial Control</title>
<link>https://arxiv.org/abs/2510.20408</link>
<guid>https://arxiv.org/abs/2510.20408</guid>
<content:encoded><![CDATA[
arXiv:2510.20408v1 Announce Type: new 
Abstract: Autonomous control of multi-stage industrial processes requires both local specialization and global coordination. Reinforcement learning (RL) offers a promising approach, but its industrial adoption remains limited due to challenges such as reward design, modularity, and action space management. Many academic benchmarks differ markedly from industrial control problems, limiting their transferability to real-world applications. This study introduces an enhanced industry-inspired benchmark environment that combines tasks from two existing benchmarks, SortingEnv and ContainerGym, into a sequential recycling scenario with sorting and pressing operations. We evaluate two control strategies: a modular architecture with specialized agents and a monolithic agent governing the full system, while also analyzing the impact of action masking. Our experiments show that without action masking, agents struggle to learn effective policies, with the modular architecture performing better. When action masking is applied, both architectures improve substantially, and the performance gap narrows considerably. These results highlight the decisive role of action space constraints and suggest that the advantages of specialization diminish as action complexity is reduced. The proposed benchmark thus provides a valuable testbed for exploring practical and robust multi-agent RL solutions in industrial automation, while contributing to the ongoing debate on centralization versus specialization.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing Intent Communication for Agent-Human Collaboration</title>
<link>https://arxiv.org/abs/2510.20409</link>
<guid>https://arxiv.org/abs/2510.20409</guid>
<content:encoded><![CDATA[
arXiv:2510.20409v1 Announce Type: new 
Abstract: As autonomous agents, from self-driving cars to virtual assistants, become increasingly present in everyday life, safe and effective collaboration depends on human understanding of agents' intentions. Current intent communication approaches are often rigid, agent-specific, and narrowly scoped, limiting their adaptability across tasks, environments, and user preferences. A key gap remains: existing models of what to communicate are rarely linked to systematic choices of how and when to communicate, preventing the development of generalizable, multi-modal strategies. In this paper, we introduce a multidimensional design space for intent communication structured along three dimensions: Transparency (what is communicated), Abstraction (when), and Modality (how). We apply this design space to three distinct human-agent collaboration scenarios: (a) bystander interaction, (b) cooperative tasks, and (c) shared control, demonstrating its capacity to generate adaptable, scalable, and cross-domain communication strategies. By bridging the gap between intent content and communication implementation, our design space provides a foundation for designing safer, more intuitive, and more transferable agent-human interactions.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Projecting onto the Unit Dual Quaternion Set</title>
<link>https://arxiv.org/abs/2510.20425</link>
<guid>https://arxiv.org/abs/2510.20425</guid>
<content:encoded><![CDATA[
arXiv:2510.20425v1 Announce Type: new 
Abstract: Dual quaternions have gained significant attention due to their wide applications in areas such as multi-agent formation control, 3D motion modeling, and robotics. A fundamental aspect in dual quaternion research involves the projection onto unit dual quaternion sets. In this paper, we systematically study such projections under the $2^R$-norm, which is commonly used in practical applications. We identify several distinct cases based on the relationship between the standard and dual parts in vector form, and demonstrate the effectiveness of the proposed algorithm through numerical experiments.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structures generated in a multiagent system performing information fusion in peer-to-peer resource-constrained networks</title>
<link>https://arxiv.org/abs/2510.20469</link>
<guid>https://arxiv.org/abs/2510.20469</guid>
<content:encoded><![CDATA[
arXiv:2510.20469v1 Announce Type: new 
Abstract: There has recently been a major advance with respect to how information fusion is performed. Information fusion has gone from being conceived as a purely hierarchical procedure, as is the case of traditional military applications, to now being regarded collaboratively, as holonic fusion, which is better suited for civil applications and edge organizations. The above paradigm shift is being boosted as information fusion gains ground in different non-military areas, and human-computer and machine-machine communications, where holarchies, which are more flexible structures than ordinary, static hierarchies, become more widespread. This paper focuses on showing how holonic structures tend to be generated when there are constraints on resources (energy, available messages, time, etc.) for interactions based on a set of fully intercommunicating elements (peers) whose components fuse information as a means of optimizing the impact of vagueness and uncertainty present message exchanges. Holon formation is studied generically based on a multiagent system model, and an example of its possible operation is shown. Holonic structures have a series of advantages, such as adaptability, to sudden changes in the environment or its composition, are somewhat autonomous and are capable of cooperating in order to achieve a common goal. This can be useful when the shortage of resources prevents communications or when the system components start to fail.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Sequence Iteration for Heterogeneous Question Answering</title>
<link>https://arxiv.org/abs/2510.20505</link>
<guid>https://arxiv.org/abs/2510.20505</guid>
<content:encoded><![CDATA[
arXiv:2510.20505v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) remains brittle on multi-step questions and heterogeneous evidence sources, trading accuracy against latency and token/tool budgets. This paper introducesHierarchical Sequence (HSEQ) Iteration for Heterogeneous Question Answering, a unified framework that (i) linearize documents, tables, and knowledge graphs into a reversible hierarchical sequence with lightweight structural tags, and (ii) perform structure-aware iteration to collect just-enough evidence before answer synthesis. A Head Agent provides guidance that leads retrieval, while an Iteration Agent selects and expands HSeq via structure-respecting actions (e.g., parent/child hops, table row/column neighbors, KG relations); Finally the head agent composes canonicalized evidence to genearte the final answer, with an optional refinement loop to resolve detected contradictions. Experiments on HotpotQA (text), HybridQA/TAT-QA (table+text), and MetaQA (KG) show consistent EM/F1 gains over strong single-pass, multi-hop, and agentic RAG baselines with high efficiency. Besides, HSEQ exhibits three key advantages: (1) a format-agnostic unification that enables a single policy to operate across text, tables, and KGs without per-dataset specialization; (2) guided, budget-aware iteration that reduces unnecessary hops, tool calls, and tokens while preserving accuracy; and (3) evidence canonicalization for reliable QA, improving answers consistency and auditability.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversary-Aware Private Inference over Wireless Channels</title>
<link>https://arxiv.org/abs/2510.20518</link>
<guid>https://arxiv.org/abs/2510.20518</guid>
<content:encoded><![CDATA[
arXiv:2510.20518v1 Announce Type: new 
Abstract: AI-based sensing at wireless edge devices has the potential to significantly enhance Artificial Intelligence (AI) applications, particularly for vision and perception tasks such as in autonomous driving and environmental monitoring. AI systems rely both on efficient model learning and inference. In the inference phase, features extracted from sensing data are utilized for prediction tasks (e.g., classification or regression). In edge networks, sensors and model servers are often not co-located, which requires communication of features. As sensitive personal data can be reconstructed by an adversary, transformation of the features are required to reduce the risk of privacy violations. While differential privacy mechanisms provide a means of protecting finite datasets, protection of individual features has not been addressed. In this paper, we propose a novel framework for privacy-preserving AI-based sensing, where devices apply transformations of extracted features before transmission to a model server.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Framework for Zero-Shot Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.20542</link>
<guid>https://arxiv.org/abs/2510.20542</guid>
<content:encoded><![CDATA[
arXiv:2510.20542v1 Announce Type: new 
Abstract: Zero-shot reinforcement learning (RL) has emerged as a setting for developing general agents in an unsupervised manner, capable of solving downstream tasks without additional training or planning at test-time. Unlike conventional RL, which optimizes policies for a fixed reward, zero-shot RL requires agents to encode representations rich enough to support immediate adaptation to any objective, drawing parallels to vision and language foundation models. Despite growing interest, the field lacks a common analytical lens.
  We present the first unified framework for zero-shot RL. Our formulation introduces a consistent notation and taxonomy that organizes existing approaches and allows direct comparison between them. Central to our framework is the classification of algorithms into two families: direct representations, which learn end-to-end mappings from rewards to policies, and compositional representations, which decompose the representation leveraging the substructure of the value function. Within this framework, we highlight shared principles and key differences across methods, and we derive an extended bound for successor-feature methods, offering a new perspective on their performance in the zero-shot regime. By consolidating existing work under a common lens, our framework provides a principled foundation for future research in zero-shot RL and outlines a clear path toward developing more general agents.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaDoS: Adaptive DoS Attack via Deep Adversarial Reinforcement Learning in SDN</title>
<link>https://arxiv.org/abs/2510.20566</link>
<guid>https://arxiv.org/abs/2510.20566</guid>
<content:encoded><![CDATA[
arXiv:2510.20566v1 Announce Type: new 
Abstract: Existing defence mechanisms have demonstrated significant effectiveness in mitigating rule-based Denial-of-Service (DoS) attacks, leveraging predefined signatures and static heuristics to identify and block malicious traffic. However, the emergence of AI-driven techniques presents new challenges to SDN security, potentially compromising the efficacy of existing defence mechanisms. In this paper, we introduce~AdaDoS, an adaptive attack model that disrupt network operations while evading detection by existing DoS-based detectors through adversarial reinforcement learning (RL). Specifically, AdaDoS models the problem as a competitive game between an attacker, whose goal is to obstruct network traffic without being detected, and a detector, which aims to identify malicious traffic. AdaDoS can solve this game by dynamically adjusting its attack strategy based on feedback from the SDN and the detector. Additionally, recognising that attackers typically have less information than defenders, AdaDoS formulates the DoS-like attack as a partially observed Markov decision process (POMDP), with the attacker having access only to delay information between attacker and victim nodes. We address this challenge with a novel reciprocal learning module, where the student agent, with limited observations, enhances its performance by learning from the teacher agent, who has full observational capabilities in the SDN environment. AdaDoS represents the first application of RL to develop DoS-like attack sequences, capable of adaptively evading both machine learning-based and rule-based DoS-like attack detectors.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Retrieval-Ranking: A Multi-Agent Cognitive Decision Framework for E-Commerce Search</title>
<link>https://arxiv.org/abs/2510.20567</link>
<guid>https://arxiv.org/abs/2510.20567</guid>
<content:encoded><![CDATA[
arXiv:2510.20567v1 Announce Type: new 
Abstract: The retrieval-ranking paradigm has long dominated e-commerce search, but its reliance on query-item matching fundamentally misaligns with multi-stage cognitive decision processes of platform users. This misalignment introduces critical limitations: semantic gaps in complex queries, high decision costs due to cross-platform information foraging, and the absence of professional shopping guidance. To address these issues, we propose a Multi-Agent Cognitive Decision Framework (MACDF), which shifts the paradigm from passive retrieval to proactive decision support. Extensive offline evaluations demonstrate MACDF's significant improvements in recommendation accuracy and user satisfaction, particularly for complex queries involving negation, multi-constraint, or reasoning demands. Online A/B testing on JD search platform confirms its practical efficacy. This work highlights the transformative potential of multi-agent cognitive systems in redefining e-commerce search.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmbodiedBrain: Expanding Performance Boundaries of Task Planning for Embodied Intelligence</title>
<link>https://arxiv.org/abs/2510.20578</link>
<guid>https://arxiv.org/abs/2510.20578</guid>
<content:encoded><![CDATA[
arXiv:2510.20578v1 Announce Type: new 
Abstract: The realization of Artificial General Intelligence (AGI) necessitates Embodied AI agents capable of robust spatial perception, effective task planning, and adaptive execution in physical environments. However, current large language models (LLMs) and multimodal LLMs (MLLMs) for embodied tasks suffer from key limitations, including a significant gap between model design and agent requirements, an unavoidable trade-off between real-time latency and performance, and the use of unauthentic, offline evaluation metrics. To address these challenges, we propose EmbodiedBrain, a novel vision-language foundation model available in both 7B and 32B parameter sizes. Our framework features an agent-aligned data structure and employs a powerful training methodology that integrates large-scale Supervised Fine-Tuning (SFT) with Step-Augumented Group Relative Policy Optimization (Step-GRPO), which boosts long-horizon task success by integrating preceding steps as Guided Precursors. Furthermore, we incorporate a comprehensive reward system, including a Generative Reward Model (GRM) accelerated at the infrastructure level, to improve training efficiency. For enable thorough validation, we establish a three-part evaluation system encompassing General, Planning, and End-to-End Simulation Benchmarks, highlighted by the proposal and open-sourcing of a novel, challenging simulation environment. Experimental results demonstrate that EmbodiedBrain achieves superior performance across all metrics, establishing a new state-of-the-art for embodied foundation models. Towards paving the way for the next generation of generalist embodied agents, we open-source all of our data, model weight, and evaluating methods, which are available at https://zterobot.github.io/EmbodiedBrain.github.io.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence</title>
<link>https://arxiv.org/abs/2510.20579</link>
<guid>https://arxiv.org/abs/2510.20579</guid>
<content:encoded><![CDATA[
arXiv:2510.20579v1 Announce Type: new 
Abstract: Most video reasoning models only generate textual reasoning traces without indicating when and where key evidence appears. Recent models such as OpenAI-o3 have sparked wide interest in evidence-centered reasoning for images, yet extending this ability to videos is more challenging, as it requires joint temporal tracking and spatial localization across dynamic scenes. We introduce Open-o3 Video, a non-agent framework that integrates explicit spatio-temporal evidence into video reasoning, and carefully collect training data and design training strategies to address the aforementioned challenges. The model highlights key timestamps, objects, and bounding boxes alongside its answers, allowing reasoning to be grounded in concrete visual observations. To enable this functionality, we first curate and build two high-quality datasets, STGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed temporal and spatial annotations, since most existing datasets offer either temporal spans for videos or spatial boxes on images, lacking unified spatio-temporal supervision and reasoning traces. Then, we adopt a cold-start reinforcement learning strategy with multiple specially designed rewards that jointly encourage answer accuracy, temporal alignment, and spatial precision. On V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance, raising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent improvements are also observed on a broad range of video understanding benchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond accuracy, the reasoning traces produced by Open-o3 Video also provide valuable signals for test-time scaling, enabling confidence-aware verification and improving answer reliability.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Costs of Perceived Bias in Fair Selection</title>
<link>https://arxiv.org/abs/2510.20606</link>
<guid>https://arxiv.org/abs/2510.20606</guid>
<content:encoded><![CDATA[
arXiv:2510.20606v1 Announce Type: new 
Abstract: Meritocratic systems, from admissions to hiring, aim to impartially reward skill and effort. Yet persistent disparities across race, gender, and class challenge this ideal. Some attribute these gaps to structural inequality; others to individual choice. We develop a game-theoretic model in which candidates from different socioeconomic groups differ in their perceived post-selection value--shaped by social context and, increasingly, by AI-powered tools offering personalized career or salary guidance. Each candidate strategically chooses effort, balancing its cost against expected reward; effort translates into observable merit, and selection is based solely on merit. We characterize the unique Nash equilibrium in the large-agent limit and derive explicit formulas showing how valuation disparities and institutional selectivity jointly determine effort, representation, social welfare, and utility. We further propose a cost-sensitive optimization framework that quantifies how modifying selectivity or perceived value can reduce disparities without compromising institutional goals. Our analysis reveals a perception-driven bias: when perceptions of post-selection value differ across groups, these differences translate into rational differences in effort, propagating disparities backward through otherwise "fair" selection processes. While the model is static, it captures one stage of a broader feedback cycle linking perceptions, incentives, and outcome--bridging rational-choice and structural explanations of inequality by showing how techno-social environments shape individual incentives in meritocratic systems.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Machine Learning into Belief-Desire-Intention Agents: Current Advances and Open Challenges</title>
<link>https://arxiv.org/abs/2510.20641</link>
<guid>https://arxiv.org/abs/2510.20641</guid>
<content:encoded><![CDATA[
arXiv:2510.20641v1 Announce Type: new 
Abstract: Thanks to the remarkable human-like capabilities of machine learning (ML) models in perceptual and cognitive tasks, frameworks integrating ML within rational agent architectures are gaining traction. Yet, the landscape remains fragmented and incoherent, often focusing on embedding ML into generic agent containers while overlooking the expressive power of rational architectures--such as Belief-Desire-Intention (BDI) agents. This paper presents a fine-grained systematisation of existing approaches, using the BDI paradigm as a reference. Our analysis illustrates the fast-evolving literature on rational agents enhanced by ML, and identifies key research opportunities and open challenges for designing effective rational ML agents.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C-NAV: Towards Self-Evolving Continual Object Navigation in Open World</title>
<link>https://arxiv.org/abs/2510.20685</link>
<guid>https://arxiv.org/abs/2510.20685</guid>
<content:encoded><![CDATA[
arXiv:2510.20685v1 Announce Type: new 
Abstract: Embodied agents are expected to perform object navigation in dynamic, open-world environments. However, existing approaches typically rely on static trajectories and a fixed set of object categories during training, overlooking the real-world requirement for continual adaptation to evolving scenarios. To facilitate related studies, we introduce the continual object navigation benchmark, which requires agents to acquire navigation skills for new object categories while avoiding catastrophic forgetting of previously learned knowledge. To tackle this challenge, we propose C-Nav, a continual visual navigation framework that integrates two key innovations: (1) A dual-path anti-forgetting mechanism, which comprises feature distillation that aligns multi-modal inputs into a consistent representation space to ensure representation consistency, and feature replay that retains temporal features within the action decoder to ensure policy consistency. (2) An adaptive sampling strategy that selects diverse and informative experiences, thereby reducing redundancy and minimizing memory overhead. Extensive experiments across multiple model architectures demonstrate that C-Nav consistently outperforms existing approaches, achieving superior performance even compared to baselines with full trajectory retention, while significantly lowering memory requirements. The code will be publicly available at https://bigtree765.github.io/C-Nav-project.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagnosing Visual Reasoning: Challenges, Insights, and a Path Forward</title>
<link>https://arxiv.org/abs/2510.20696</link>
<guid>https://arxiv.org/abs/2510.20696</guid>
<content:encoded><![CDATA[
arXiv:2510.20696v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) that integrate visual and textual reasoning leverage chain-of-thought (CoT) prompting to tackle complex visual tasks, yet continue to exhibit visual hallucinations and an over-reliance on textual priors. We present a systematic diagnosis of state-of-the-art vision-language models using a three-stage evaluation framework, uncovering key failure modes. To address these, we propose an agent-based architecture that combines LLM reasoning with lightweight visual modules, enabling fine-grained analysis and iterative refinement of reasoning chains. Our results highlight future visual reasoning models should focus on integrating a broader set of specialized tools for analyzing visual content. Our system achieves significant gains (+10.3 on MMMU, +6.0 on MathVista over a 7B baseline), matching or surpassing much larger models. We will release our framework and evaluation suite to facilitate future research.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thought Communication in Multiagent Collaboration</title>
<link>https://arxiv.org/abs/2510.20733</link>
<guid>https://arxiv.org/abs/2510.20733</guid>
<content:encoded><![CDATA[
arXiv:2510.20733v1 Announce Type: new 
Abstract: Natural language has long enabled human cooperation, but its lossy, ambiguous, and indirect nature limits the potential of collective intelligence. While machines are not subject to these constraints, most LLM-based multi-agent systems still rely solely on natural language, exchanging tokens or their embeddings. To go beyond language, we introduce a new paradigm, thought communication, which enables agents to interact directly mind-to-mind, akin to telepathy. To uncover these latent thoughts in a principled way, we formalize the process as a general latent variable model, where agent states are generated by an unknown function of underlying thoughts. We prove that, in a nonparametric setting without auxiliary information, both shared and private latent thoughts between any pair of agents can be identified. Moreover, the global structure of thought sharing, including which agents share which thoughts and how these relationships are structured, can also be recovered with theoretical guarantees. Guided by the established theory, we develop a framework that extracts latent thoughts from all agents prior to communication and assigns each agent the relevant thoughts, along with their sharing patterns. This paradigm naturally extends beyond LLMs to all modalities, as most observational data arise from hidden generative processes. Experiments on both synthetic and real-world benchmarks validate the theory and demonstrate the collaborative advantages of thought communication. We hope this work illuminates the potential of leveraging the hidden world, as many challenges remain unsolvable through surface-level observation alone, regardless of compute or data scale.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence Powered Identification of Potential Antidiabetic Compounds in Ficus religiosa</title>
<link>https://arxiv.org/abs/2510.19867</link>
<guid>https://arxiv.org/abs/2510.19867</guid>
<content:encoded><![CDATA[
arXiv:2510.19867v1 Announce Type: cross 
Abstract: Diabetes mellitus is a chronic metabolic disorder that necessitates novel therapeutic innovations due to its gradual progression and the onset of various metabolic complications. Research indicates that Ficus religiosa is a conventional medicinal plant that generates bioactive phytochemicals with potential antidiabetic properties. The investigation employs ecosystem-based computational approaches utilizing artificial intelligence to investigate and evaluate compounds derived from Ficus religiosa that exhibit antidiabetic properties. A comprehensive computational procedure incorporated machine learning methodologies, molecular docking techniques, and ADMET prediction systems to assess phytochemical efficacy against the significant antidiabetic enzyme dipeptidyl peptidase-4 (DPP-4). DeepBindGCN and the AutoDock software facilitated the investigation of binding interactions via deep learning technology. Flavonoids and alkaloids have emerged as attractive phytochemicals due to their strong binding interactions and advantageous pharmacological effects, as indicated by the study. The introduction of AI accelerated screening procedures and enhanced accuracy rates, demonstrating its efficacy in researching plant-based antidiabetic agents. The scientific foundation now facilitates future experimental validation of natural product therapies tailored for diabetic management.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simultaneously Solving Infinitely Many LQ Mean Field Games In Hilbert Spaces: The Power of Neural Operators</title>
<link>https://arxiv.org/abs/2510.20017</link>
<guid>https://arxiv.org/abs/2510.20017</guid>
<content:encoded><![CDATA[
arXiv:2510.20017v1 Announce Type: cross 
Abstract: Traditional mean-field game (MFG) solvers operate on an instance-by-instance basis, which becomes infeasible when many related problems must be solved (e.g., for seeking a robust description of the solution under perturbations of the dynamics or utilities, or in settings involving continuum-parameterized agents.). We overcome this by training neural operators (NOs) to learn the rules-to-equilibrium map from the problem data (``rules'': dynamics and cost functionals) of LQ MFGs defined on separable Hilbert spaces to the corresponding equilibrium strategy. Our main result is a statistical guarantee: an NO trained on a small number of randomly sampled rules reliably solves unseen LQ MFG variants, even in infinite-dimensional settings. The number of NO parameters needed remains controlled under appropriate rule sampling during training.
  Our guarantee follows from three results: (i) local-Lipschitz estimates for the highly nonlinear rules-to-equilibrium map; (ii) a universal approximation theorem using NOs with a prespecified Lipschitz regularity (unlike traditional NO results where the NO's Lipschitz constant can diverge as the approximation error vanishes); and (iii) new sample-complexity bounds for $L$-Lipschitz learners in infinite dimensions, directly applicable as the Lipschitz constants of our approximating NOs are controlled in (ii).
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ComProScanner: A multi-agent based framework for composition-property structured data extraction from scientific literature</title>
<link>https://arxiv.org/abs/2510.20362</link>
<guid>https://arxiv.org/abs/2510.20362</guid>
<content:encoded><![CDATA[
arXiv:2510.20362v1 Announce Type: cross 
Abstract: Since the advent of various pre-trained large language models, extracting structured knowledge from scientific text has experienced a revolutionary change compared with traditional machine learning or natural language processing techniques. Despite these advances, accessible automated tools that allow users to construct, validate, and visualise datasets from scientific literature extraction remain scarce. We therefore developed ComProScanner, an autonomous multi-agent platform that facilitates the extraction, validation, classification, and visualisation of machine-readable chemical compositions and properties, integrated with synthesis data from journal articles for comprehensive database creation. We evaluated our framework using 100 journal articles against 10 different LLMs, including both open-source and proprietary models, to extract highly complex compositions associated with ceramic piezoelectric materials and corresponding piezoelectric strain coefficients (d33), motivated by the lack of a large dataset for such materials. DeepSeek-V3-0324 outperformed all models with a significant overall accuracy of 0.82. This framework provides a simple, user-friendly, readily-usable package for extracting highly complex experimental data buried in the literature to build machine learning or deep learning datasets.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Co-Designing Quantum Codes with Transversal Diagonal Gates via Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2510.20728</link>
<guid>https://arxiv.org/abs/2510.20728</guid>
<content:encoded><![CDATA[
arXiv:2510.20728v1 Announce Type: cross 
Abstract: We present a multi-agent, human-in-the-loop workflow that co-designs quantum codes with prescribed transversal diagonal gates. It builds on the Subset-Sum Linear Programming (SSLP) framework (arXiv:2504.20847), which partitions basis strings by modular residues and enforces $Z$-marginal Knill-Laflamme (KL) equalities via small LPs. The workflow is powered by GPT-5 and implemented within TeXRA (https://texra.ai)-a multi-agent research assistant platform that supports an iterative tool-use loop agent and a derivation-then-edit workflow reasoning agent. We work in a LaTeX-Python environment where agents reason, edit documents, execute code, and synchronize their work to Git/Overleaf. Within this workspace, three roles collaborate: a Synthesis Agent formulates the problem; a Search Agent sweeps/screens candidates and exactifies numerics into rationals; and an Audit Agent independently checks all KL equalities and the induced logical action. As a first step we focus on distance $d=2$ with nondegenerate residues. For code dimension $K\in\{2,3,4\}$ and $n\le6$ qubits, systematic sweeps yield certificate-backed tables cataloging attainable cyclic logical groups-all realized by new codes-e.g., for $K=3$ we obtain order $16$ at $n=6$. From verified instances, Synthesis Agent abstracts recurring structures into closed-form families and proves they satisfy the KL equalities for all parameters. It further demonstrates that SSLP accommodates residue degeneracy by exhibiting a new $((6,4,2))$ code implementing the transversal controlled-phase $diag(1,1,1,i)$. Overall, the workflow recasts diagonal-transversal feasibility as an analytical pipeline executed at scale, combining systematic enumeration with exact analytical reconstruction. It yields reproducible code constructions, supports targeted extensions to larger $K$ and higher distances, and leads toward data-driven classification.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning and Consumption-Savings Behavior</title>
<link>https://arxiv.org/abs/2510.20748</link>
<guid>https://arxiv.org/abs/2510.20748</guid>
<content:encoded><![CDATA[
arXiv:2510.20748v1 Announce Type: cross 
Abstract: This paper demonstrates how reinforcement learning can explain two puzzling empirical patterns in household consumption behavior during economic downturns. I develop a model where agents use Q-learning with neural network approximation to make consumption-savings decisions under income uncertainty, departing from standard rational expectations assumptions. The model replicates two key findings from recent literature: (1) unemployed households with previously low liquid assets exhibit substantially higher marginal propensities to consume (MPCs) out of stimulus transfers compared to high-asset households (0.50 vs 0.34), even when neither group faces borrowing constraints, consistent with Ganong et al. (2024); and (2) households with more past unemployment experiences maintain persistently lower consumption levels after controlling for current economic conditions, a "scarring" effect documented by Malmendier and Shen (2024). Unlike existing explanations based on belief updating about income risk or ex-ante heterogeneity, the reinforcement learning mechanism generates both higher MPCs and lower consumption levels simultaneously through value function approximation errors that evolve with experience. Simulation results closely match the empirical estimates, suggesting that adaptive learning through reinforcement learning provides a unifying framework for understanding how past experiences shape current consumption behavior beyond what current economic conditions would predict.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi Task Inverse Reinforcement Learning for Common Sense Reward</title>
<link>https://arxiv.org/abs/2402.11367</link>
<guid>https://arxiv.org/abs/2402.11367</guid>
<content:encoded><![CDATA[
arXiv:2402.11367v2 Announce Type: replace 
Abstract: One of the challenges in applying reinforcement learning in a complex real-world environment lies in providing the agent with a sufficiently detailed reward function. Any misalignment between the reward and the desired behavior can result in unwanted outcomes. This may lead to issues like "reward hacking" where the agent maximizes rewards by unintended behavior. In this work, we propose to disentangle the reward into two distinct parts. A simple task-specific reward, outlining the particulars of the task at hand, and an unknown common-sense reward, indicating the expected behavior of the agent within the environment. We then explore how this common-sense reward can be learned from expert demonstrations. We first show that inverse reinforcement learning, even when it succeeds in training an agent, does not learn a useful reward function. That is, training a new agent with the learned reward does not impair the desired behaviors. We then demonstrate that this problem can be solved by training simultaneously on multiple tasks. That is, multi-task inverse reinforcement learning can be applied to learn a useful reward function.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Price of Opportunity Fairness in Matroid Allocation Problems</title>
<link>https://arxiv.org/abs/2403.00397</link>
<guid>https://arxiv.org/abs/2403.00397</guid>
<content:encoded><![CDATA[
arXiv:2403.00397v3 Announce Type: replace 
Abstract: We consider matroid allocation problems under opportunity fairness constraints: resources need to be allocated to a set of agents under matroid constraints (which include classical problems such as bipartite matching). Agents are divided into $C$ groups according to a sensitive attribute, and an allocation is opportunity-fair if each group receives the same share proportional to the maximum feasible allocation it could achieve in isolation. We study the Price of Fairness (PoF), i.e., the ratio between maximum size allocations and maximum size opportunity-fair allocations. We first provide a characterization of the PoF leveraging the underlying polymatroid structure of the allocation problem. Based on this characterization, we prove bounds on the PoF in various settings from fully adversarial (worst-case) to fully random. Notably, one of our main results considers an arbitrary matroid structure with agents randomly divided into groups. In this setting, we prove a PoF bound as a function of the (relative) size of the largest group. Our result implies that, as long as there is no dominant group (i.e., the largest group is not too large), opportunity fairness constraints do not induce any loss of social welfare (defined as the allocation size). Overall, our results give insights into which aspects of the problem's structure affect the trade-off between opportunity fairness and social welfare.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Distributed Least Squares Algorithm for Linear Algebraic Equations via Periodic Scheduling</title>
<link>https://arxiv.org/abs/2411.06883</link>
<guid>https://arxiv.org/abs/2411.06883</guid>
<content:encoded><![CDATA[
arXiv:2411.06883v2 Announce Type: replace 
Abstract: In this work, we propose a novel discrete-time distributed algorithm for finding least-squares solutions of linear algebraic equations with a scheduling protocol to further enhance its scalability. Each agent in the network is assumed to know some rows of the coefficient matrix and the corresponding entries in the observation vector. Unlike typical distributed algorithms, our approach considers communication bandwidth limits, allowing agents to transmit only a portion of their ``guessed" solution, independent of its dimension. A cyclic scheduling protocol determines which portion is transmitted at each iteration. Assuming a small fixed step size and a diagonalizable algorithm matrix, we prove that agents' ``guessed" solutions converge exponentially to a least squares solution. For cases where the observation vectors are time-varying, a modified algorithm guarantees practical convergence, with tracking error bounded by the single-step variation in the observation vector. Simulations and comparisons with state-of-the-art algorithms validate our algorithm's feasibility and scalability.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong Cultural Learning</title>
<link>https://arxiv.org/abs/2411.12977</link>
<guid>https://arxiv.org/abs/2411.12977</guid>
<content:encoded><![CDATA[
arXiv:2411.12977v5 Announce Type: replace 
Abstract: Embodied agents powered by large language models (LLMs), such as Voyager, promise open-ended competence in worlds such as Minecraft. However, when powered by open-weight LLMs they still falter on elementary tasks after domain-specific fine-tuning. We propose MindForge, a generative-agent framework for cultural lifelong learning through explicit perspective taking. We introduce three key innovations: (1) a structured theory of mind representation linking percepts, beliefs, desires, and actions; (2) natural inter-agent communication; and (3) a multi-component memory system. Following the cultural learning framework, we test MindForge in both instructive and collaborative settings within Minecraft. In an instructive setting with GPT-4, MindForge agents powered by open-weight LLMs significantly outperform their Voyager counterparts in basic tasks yielding $3\times$ more tech-tree milestones and collecting $2.3\times$ more unique items than the Voyager baseline. Furthermore, in fully \textit{collaborative} settings, we find that the performance of two underachieving agents improves with more communication rounds, echoing the Condorcet Jury Theorem. MindForge agents demonstrate sophisticated behaviors, including expert-novice knowledge transfer, collaborative problem solving, and adaptation to out-of-distribution tasks through accumulated cultural experiences.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DMWM: Dual-Mind World Model with Long-Term Imagination</title>
<link>https://arxiv.org/abs/2502.07591</link>
<guid>https://arxiv.org/abs/2502.07591</guid>
<content:encoded><![CDATA[
arXiv:2502.07591v2 Announce Type: replace 
Abstract: Imagination in world models is crucial for enabling agents to learn long-horizon policy in a sample-efficient manner. Existing recurrent state-space model (RSSM)-based world models depend on single-step statistical inference to capture the environment dynamics, and, hence, they are unable to perform long-term imagination tasks due to the accumulation of prediction errors. Inspired by the dual-process theory of human cognition, we propose a novel dual-mind world model (DMWM) framework that integrates logical reasoning to enable imagination with logical consistency. DMWM is composed of two components: an RSSM-based System 1 (RSSM-S1) component that handles state transitions in an intuitive manner and a logic-integrated neural network-based System 2 (LINN-S2) component that guides the imagination process through hierarchical deep logical reasoning. The inter-system feedback mechanism is designed to ensure that the imagination process follows the logical rules of the real environment. The proposed framework is evaluated on benchmark tasks that require long-term planning from the DMControl suite. Extensive experimental results demonstrate that the proposed framework yields significant improvements in terms of logical coherence, trial efficiency, data efficiency and long-term imagination over the state-of-the-art world models.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of Large Language Models</title>
<link>https://arxiv.org/abs/2502.15086</link>
<guid>https://arxiv.org/abs/2502.15086</guid>
<content:encoded><![CDATA[
arXiv:2502.15086v2 Announce Type: replace 
Abstract: As the use of large language model (LLM) agents continues to grow, their safety vulnerabilities have become increasingly evident. Extensive benchmarks evaluate various aspects of LLM safety by defining the safety relying heavily on general standards, overlooking user-specific standards. However, safety standards for LLM may vary based on a user-specific profiles rather than being universally consistent across all users. This raises a critical research question: Do LLM agents act safely when considering user-specific safety standards? Despite its importance for safe LLM use, no benchmark datasets currently exist to evaluate the user-specific safety of LLMs. To address this gap, we introduce U-SafeBench, a benchmark designed to assess user-specific aspect of LLM safety. Our evaluation of 20 widely used LLMs reveals current LLMs fail to act safely when considering user-specific safety standards, marking a new discovery in this field. To address this vulnerability, we propose a simple remedy based on chain-of-thought, demonstrating its effectiveness in improving user-specific safety. Our benchmark and code are available at https://github.com/yeonjun-in/U-SafeBench.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolution of Cooperation in LLM-Agent Societies: A Preliminary Study Using Different Punishment Strategies</title>
<link>https://arxiv.org/abs/2504.19487</link>
<guid>https://arxiv.org/abs/2504.19487</guid>
<content:encoded><![CDATA[
arXiv:2504.19487v3 Announce Type: replace 
Abstract: The evolution of cooperation has been extensively studied using abstract mathematical models and simulations. Recent advances in Large Language Models (LLMs) and the rise of LLM agents have demonstrated their ability to perform social reasoning, thus providing an opportunity to test the emergence of norms in more realistic agent-based simulations with human-like reasoning using natural language. In this research, we investigate whether the cooperation dynamics presented in Boyd and Richerson's model persist in a more realistic simulation of the Diner's Dilemma using LLM agents compared to the abstract mathematical nature in the work of Boyd and Richerson. Our findings indicate that agents follow the strategies defined in the Boyd and Richerson model, and explicit punishment mechanisms drive norm emergence, reinforcing cooperative behaviour even when the agent strategy configuration varies. Our results suggest that LLM-based Multi-Agent System simulations, in fact, can replicate the evolution of cooperation predicted by the traditional mathematical models. Moreover, our simulations extend beyond the mathematical models by integrating natural language-driven reasoning and a pairwise imitation method for strategy adoption, making them a more realistic testbed for cooperative behaviour in MASs.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Reward Design</title>
<link>https://arxiv.org/abs/2505.11821</link>
<guid>https://arxiv.org/abs/2505.11821</guid>
<content:encoded><![CDATA[
arXiv:2505.11821v2 Announce Type: replace 
Abstract: This paper investigates Reinforcement Learning (RL) approaches to enhance the reasoning capabilities of Large Language Model (LLM) agents in long-horizon, multi-turn scenarios. Although RL algorithms such as Group Relative Policy Optimization (GRPO) and Proximal Policy Optimization (PPO) have been widely applied to train multi-turn LLM agents, they typically rely only on sparse outcome rewards and lack dense intermediate signals across multiple decision steps, limiting their performance on complex reasoning tasks. To bridge this gap, we present the first systematic study of \textit{turn-level reward design} for multi-turn RL algorithms and agent applications. By integrating turn-level rewards, we extend GRPO and PPO to their respective multi-turn variants, enabling fine-grained credit assignment. We conduct case studies on multi-turn reasoning-augmented search agents, where we carefully design two types of turn-level rewards: verifiable and LLM-as-judge. Our experiments on multi-turn search tasks demonstrate that incorporating well-designed turn-level rewards enables RL algorithms to significantly outperform baseline methods with trajectory-level rewards. Both training and validation reward curves illustrate that our method achieves \textit{greater stability}, \textit{faster convergence}, and \textit{higher accuracy}. Numerical results across diverse question-answering datasets further show that our approach consistently delivers highest answer correctness and 100\% format correctness.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLEVER: A Curated Benchmark for Formally Verified Code Generation</title>
<link>https://arxiv.org/abs/2505.13938</link>
<guid>https://arxiv.org/abs/2505.13938</guid>
<content:encoded><![CDATA[
arXiv:2505.13938v4 Announce Type: replace 
Abstract: We introduce ${\rm C{\small LEVER}}$, a high-quality, curated benchmark of 161 problems for end-to-end verified code generation in Lean. Each problem consists of (1) the task of generating a specification that matches a held-out ground-truth specification, and (2) the task of generating a Lean implementation that provably satisfies this specification. Unlike prior benchmarks, ${\rm C{\small LEVER}}$ avoids test-case supervision, LLM-generated annotations, and specifications that leak implementation logic or allow vacuous solutions. All outputs are verified post-hoc using Lean's type checker to ensure machine-checkable correctness. We use ${\rm C{\small LEVER}}$ to evaluate several few-shot and agentic approaches based on state-of-the-art language models. These methods all struggle to achieve full verification, establishing it as a challenging frontier benchmark for program synthesis and formal reasoning. Our benchmark can be found on GitHub(https://github.com/trishullab/clever) as well as HuggingFace(https://huggingface.co/datasets/amitayusht/clever). All our evaluation code is also available online(https://github.com/trishullab/clever-prover).
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models</title>
<link>https://arxiv.org/abs/2505.15293</link>
<guid>https://arxiv.org/abs/2505.15293</guid>
<content:encoded><![CDATA[
arXiv:2505.15293v2 Announce Type: replace 
Abstract: Policy exploration is critical in reinforcement learning (RL), where existing approaches include greedy, Gaussian process, etc. However, these approaches utilize preset stochastic processes and are indiscriminately applied in all kinds of RL tasks without considering task-specific features that influence policy exploration. Moreover, during RL training, the evolution of such stochastic processes is rigid, which typically only incorporates a decay in the variance, failing to adjust flexibly according to the agent's real-time learning status. Inspired by the analyzing and reasoning capability of large language models (LLMs), we design LLM-Explorer to adaptively generate task-specific exploration strategies with LLMs, enhancing the policy exploration in RL. In our design, we sample the learning trajectory of the agent during the RL training in a given task and prompt the LLM to analyze the agent's current policy learning status and then generate a probability distribution for future policy exploration. Updating the probability distribution periodically, we derive a stochastic process specialized for the particular task and dynamically adjusted to adapt to the learning process. Our design is a plug-in module compatible with various widely applied RL algorithms, including the DQN series, DDPG, TD3, and any possible variants developed based on them. Through extensive experiments on the Atari and MuJoCo benchmarks, we demonstrate LLM-Explorer's capability to enhance RL policy exploration, achieving an average performance improvement up to 37.27%. Our code is open-source at https://github.com/tsinghua-fib-lab/LLM-Explorer for reproducibility.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embodied Agents Meet Personalization: Investigating Challenges and Solutions Through the Lens of Memory Utilization</title>
<link>https://arxiv.org/abs/2505.16348</link>
<guid>https://arxiv.org/abs/2505.16348</guid>
<content:encoded><![CDATA[
arXiv:2505.16348v2 Announce Type: replace 
Abstract: LLM-powered embodied agents have shown success on conventional object-rearrangement tasks, but providing personalized assistance that leverages user-specific knowledge from past interactions presents new challenges. We investigate these challenges through the lens of agents' memory utilization along two critical dimensions: object semantics (identifying objects based on personal meaning) and user patterns (recalling sequences from behavioral routines). To assess these capabilities, we construct MEMENTO, an end-to-end two-stage evaluation framework comprising single-memory and joint-memory tasks. Our experiments reveal that current agents can recall simple object semantics but struggle to apply sequential user patterns to planning. Through in-depth analysis, we identify two critical bottlenecks: information overload and coordination failures when handling multiple memories. Based on these findings, we explore memory architectural approaches to address these challenges. Given our observation that episodic memory provides both personalized knowledge and in-context learning benefits, we design a hierarchical knowledge graph-based user-profile memory module that separately manages personalized knowledge, achieving substantial improvements on both single and joint-memory tasks. Project website: https://connoriginal.github.io/MEMENTO
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Ensembles of Distilled Policies Improve Generalisation in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.16581</link>
<guid>https://arxiv.org/abs/2505.16581</guid>
<content:encoded><![CDATA[
arXiv:2505.16581v2 Announce Type: replace 
Abstract: In the zero-shot policy transfer setting in reinforcement learning, the goal is to train an agent on a fixed set of training environments so that it can generalise to similar, but unseen, testing environments. Previous work has shown that policy distillation after training can sometimes produce a policy that outperforms the original in the testing environments. However, it is not yet entirely clear why that is, or what data should be used to distil the policy. In this paper, we prove, under certain assumptions, a generalisation bound for policy distillation after training. The theory provides two practical insights: for improved generalisation, you should 1) train an ensemble of distilled policies, and 2) distil it on as much data from the training environments as possible. We empirically verify that these insights hold in more general settings, when the assumptions required for the theory no longer hold. Finally, we demonstrate that an ensemble of policies distilled on a diverse dataset can generalise significantly better than the original agent.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DesignX: Human-Competitive Algorithm Designer for Black-Box Optimization</title>
<link>https://arxiv.org/abs/2505.17866</link>
<guid>https://arxiv.org/abs/2505.17866</guid>
<content:encoded><![CDATA[
arXiv:2505.17866v2 Announce Type: replace 
Abstract: Designing effective black-box optimizers is hampered by limited problem-specific knowledge and manual control that spans months for almost every detail. In this paper, we present \textit{DesignX}, the first automated algorithm design framework that generates an effective optimizer specific to a given black-box optimization problem within seconds. Rooted in the first principles, we identify two key sub-tasks: 1) algorithm structure generation and 2) hyperparameter control. To enable systematic construction, a comprehensive modular algorithmic space is first built, embracing hundreds of algorithm components collected from decades of research. We then introduce a dual-agent reinforcement learning system that collaborates on structural and parametric design through a novel cooperative training objective, enabling large-scale meta-training across 10k diverse instances. Remarkably, through days of autonomous learning, the DesignX-generated optimizers continuously surpass human-crafted optimizers by orders of magnitude, either on synthetic testbed or on realistic optimization scenarios such as Protein-docking, AutoML and UAV path planning. Further in-depth analysis reveals DesignX's capability to discover non-trivial algorithm patterns beyond expert intuition, which, conversely, provides valuable design insights for the optimization community. We provide DesignX's Python project at~ https://github.com/MetaEvo/DesignX.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Safety in LLMs: A Benchmark and A Planning-Based Agent Approach</title>
<link>https://arxiv.org/abs/2505.18882</link>
<guid>https://arxiv.org/abs/2505.18882</guid>
<content:encoded><![CDATA[
arXiv:2505.18882v3 Announce Type: replace 
Abstract: Large language models (LLMs) typically generate identical or similar responses for all users given the same prompt, posing serious safety risks in high-stakes applications where user vulnerabilities differ widely. Existing safety evaluations primarily rely on context-independent metrics - such as factuality, bias, or toxicity - overlooking the fact that the same response may carry divergent risks depending on the user's background or condition. We introduce personalized safety to fill this gap and present PENGUIN - a benchmark comprising 14,000 scenarios across seven sensitive domains with both context-rich and context-free variants. Evaluating six leading LLMs, we demonstrate that personalized user information significantly improves safety scores by 43.2%, confirming the effectiveness of personalization in safety alignment. However, not all context attributes contribute equally to safety enhancement. To address this, we develop RAISE - a training-free, two-stage agent framework that strategically acquires user-specific background. RAISE improves safety scores by up to 31.6% over six vanilla LLMs, while maintaining a low interaction cost of just 2.7 user queries on average. Our findings highlight the importance of selective information gathering in safety-critical domains and offer a practical solution for personalizing LLM responses without model retraining. This work establishes a foundation for safety research that adapts to individual user contexts rather than assuming a universal harm standard.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve</title>
<link>https://arxiv.org/abs/2505.23946</link>
<guid>https://arxiv.org/abs/2505.23946</guid>
<content:encoded><![CDATA[
arXiv:2505.23946v2 Announce Type: replace 
Abstract: Recent studies show that LLMs possess different skills and specialize in different tasks. In fact, we observe that their varied performance occur in several levels of granularity. For example, in the code optimization task, code LLMs excel at different optimization categories and no one dominates others. This observation prompts the question of how one leverages multiple LLM agents to solve a coding problem without knowing their complementary strengths a priori. We argue that a team of agents can learn from each other's successes and failures so as to improve their own performance. Thus, a lesson is the knowledge produced by an agent and passed on to other agents in the collective solution process. We propose a lesson-based collaboration framework, design the lesson solicitation--banking--selection mechanism, and demonstrate that a team of small LLMs with lessons learned can outperform a much larger LLM and other multi-LLM collaboration methods.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proxy Target: Bridging the Gap Between Discrete Spiking Neural Networks and Continuous Control</title>
<link>https://arxiv.org/abs/2505.24161</link>
<guid>https://arxiv.org/abs/2505.24161</guid>
<content:encoded><![CDATA[
arXiv:2505.24161v2 Announce Type: replace 
Abstract: Spiking Neural Networks (SNNs) offer low-latency and energy-efficient decision making on neuromorphic hardware, making them attractive for Reinforcement Learning (RL) in resource-constrained edge devices. However, most RL algorithms for continuous control are designed for Artificial Neural Networks (ANNs), particularly the target network soft update mechanism, which conflicts with the discrete and non-differentiable dynamics of spiking neurons. We show that this mismatch destabilizes SNN training and degrades performance. To bridge the gap between discrete SNNs and continuous-control algorithms, we propose a novel proxy target framework. The proxy network introduces continuous and differentiable dynamics that enable smooth target updates, stabilizing the learning process. Since the proxy operates only during training, the deployed SNN remains fully energy-efficient with no additional inference overhead. Extensive experiments on continuous control benchmarks demonstrate that our framework consistently improves stability and achieves up to $32\%$ higher performance across various spiking neuron models. Notably, to the best of our knowledge, this is the first approach that enables SNNs with simple Leaky Integrate and Fire (LIF) neurons to surpass their ANN counterparts in continuous control. This work highlights the importance of SNN-tailored RL algorithms and paves the way for neuromorphic agents that combine high performance with low power consumption. Code is available at https://github.com/xuzijie32/Proxy-Target.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Static Responses: Multi-Agent LLM Systems as a New Paradigm for Social Science Research</title>
<link>https://arxiv.org/abs/2506.01839</link>
<guid>https://arxiv.org/abs/2506.01839</guid>
<content:encoded><![CDATA[
arXiv:2506.01839v2 Announce Type: replace 
Abstract: As large language models (LLMs) transition from static tools to fully agentic systems, their potential for transforming social science research has become increasingly evident. This paper introduces a structured framework for understanding the diverse applications of LLM-based agents, ranging from simple data processors to complex, multi-agent systems capable of simulating emergent social dynamics. By mapping this developmental continuum across six levels, the paper clarifies the technical and methodological boundaries between different agentic architectures, providing a comprehensive overview of current capabilities and future potential. It highlights how lower-tier systems streamline conventional tasks like text classification and data annotation, while higher-tier systems enable novel forms of inquiry, including the study of group dynamics, norm formation, and large-scale social processes. However, these advancements also introduce significant challenges, including issues of reproducibility, ethical oversight, and the risk of emergent biases. The paper critically examines these concerns, emphasizing the need for robust validation protocols, interdisciplinary collaboration, and standardized evaluation metrics. It argues that while LLM-based agents hold transformative potential for the social sciences, realizing this promise will require careful, context-sensitive deployment and ongoing methodological refinement. The paper concludes with a call for future research that balances technical innovation with ethical responsibility, encouraging the development of agentic systems that not only replicate but also extend the frontiers of social science, offering new insights into the complexities of human behavior.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Execution Guided Line-by-Line Code Generation</title>
<link>https://arxiv.org/abs/2506.10948</link>
<guid>https://arxiv.org/abs/2506.10948</guid>
<content:encoded><![CDATA[
arXiv:2506.10948v2 Announce Type: replace 
Abstract: We present a novel approach to neural code generation that incorporates real-time execution signals into the language model generation process. While large language models (LLMs) have demonstrated impressive code generation capabilities, they typically do not utilize execution feedback during inference, a critical signal that human programmers regularly leverage. Our method, Execution-Guided Classifier-Free Guidance (EG-CFG), dynamically incorporates execution signals as the model generates code, providing line-by-line feedback that guides the generation process toward executable solutions. EG-CFG employs a multi-stage process: first, we conduct beam search to sample candidate program completions for each line; second, we extract execution signals by executing these candidates against test cases; and finally, we incorporate these signals into the prompt during generation. By maintaining consistent signals across tokens within the same line and refreshing signals at line boundaries, our approach provides coherent guidance while preserving syntactic structure. Moreover, the method naturally supports native parallelism at the task level in which multiple agents operate in parallel, exploring diverse reasoning paths and collectively generating a broad set of candidate solutions. Our experiments across diverse coding tasks demonstrate that EG-CFG significantly improves code generation performance compared to standard approaches, achieving state-of-the-art results across various levels of complexity, from foundational problems to challenging competitive programming and data science tasks. Our code is available at: https://github.com/boazlavon/eg_cfg
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prover Agent: An Agent-Based Framework for Formal Mathematical Proofs</title>
<link>https://arxiv.org/abs/2506.19923</link>
<guid>https://arxiv.org/abs/2506.19923</guid>
<content:encoded><![CDATA[
arXiv:2506.19923v4 Announce Type: replace 
Abstract: We present Prover Agent, a novel AI agent for automated theorem proving that integrates large language models (LLMs) with a formal proof assistant, Lean. Prover Agent coordinates an informal reasoning LLM, a formal prover model, and feedback from Lean while also generating auxiliary lemmas. These auxiliary lemmas are not limited to subgoals in the formal proof but can also include special cases or potentially useful facts derived from the assumptions, which help in discovering a viable proof strategy. It achieves an 88.1% success rate on the MiniF2F benchmark, establishing a new state-of-the-art among methods using small language models (SLMs) with a much lower sample budget than previous approaches. We also present theoretical analyses and case studies that illustrate how these generated lemmas contribute to solving challenging problems. Our code is publicly available at: https://github.com/kAIto47802/Prover-Agent.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Debate or Vote: Which Yields Better Decisions in Multi-Agent Large Language Models?</title>
<link>https://arxiv.org/abs/2508.17536</link>
<guid>https://arxiv.org/abs/2508.17536</guid>
<content:encoded><![CDATA[
arXiv:2508.17536v2 Announce Type: replace 
Abstract: Multi-Agent Debate~(MAD) has emerged as a promising paradigm for improving the performance of large language models through collaborative reasoning. Despite recent advances, the key factors driving MAD's effectiveness remain unclear. In this work, we disentangle MAD into two key components--Majority Voting and inter-agent Debate--and assess their respective contributions. Through extensive experiments across seven NLP benchmarks, we find that Majority Voting alone accounts for most of the performance gains typically attributed to MAD. To explain this, we propose a theoretical framework that models debate as a stochastic process. We prove that it induces a martingale over agents' belief trajectories, implying that debate alone does not improve expected correctness. Guided by these insights, we demonstrate that targeted interventions, by biasing the belief update toward correction, can meaningfully enhance debate effectiveness. Overall, our findings suggest that while MAD has potential, simple ensembling methods remain strong and more reliable alternatives in many practical settings. Code is released in https://github.com/deeplearning-wisc/debate-or-vote.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Reinforcement Learning for Task Offloading in Wireless Edge Networks</title>
<link>https://arxiv.org/abs/2509.01257</link>
<guid>https://arxiv.org/abs/2509.01257</guid>
<content:encoded><![CDATA[
arXiv:2509.01257v2 Announce Type: replace 
Abstract: In edge computing systems, autonomous agents must make fast local decisions while competing for shared resources. Existing MARL methods often resume to centralized critics or frequent communication, which fail under limited observability and communication constraints. We propose a decentralized framework in which each agent solves a constrained Markov decision process (CMDP), coordinating implicitly through a shared constraint vector. For the specific case of offloading, e.g., constraints prevent overloading shared server resources. Coordination constraints are updated infrequently and act as a lightweight coordination mechanism. They enable agents to align with global resource usage objectives but require little direct communication. Using safe reinforcement learning, agents learn policies that meet both local and global goals. We establish theoretical guarantees under mild assumptions and validate our approach experimentally, showing improved performance over centralized and independent baselines, especially in large-scale settings.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward a Metrology for Artificial Intelligence: Hidden-Rule Environments and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.06213</link>
<guid>https://arxiv.org/abs/2509.06213</guid>
<content:encoded><![CDATA[
arXiv:2509.06213v3 Announce Type: replace 
Abstract: We investigate reinforcement learning in the Game Of Hidden Rules (GOHR) environment, a complex puzzle in which an agent must infer and execute hidden rules to clear a 6$\times$6 board by placing game pieces into buckets. We explore two state representation strategies, namely Feature-Centric (FC) and Object-Centric (OC), and employ a Transformer-based Advantage Actor-Critic (A2C) algorithm for training. The agent has access only to partial observations and must simultaneously infer the governing rule and learn the optimal policy through experience. We evaluate our models across multiple rule-based and trial-list-based experimental setups, analyzing transfer effects and the impact of representation on learning efficiency.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text2Mem: A Unified Memory Operation Language for Memory Operating System</title>
<link>https://arxiv.org/abs/2509.11145</link>
<guid>https://arxiv.org/abs/2509.11145</guid>
<content:encoded><![CDATA[
arXiv:2509.11145v2 Announce Type: replace 
Abstract: Large language model agents increasingly depend on memory to sustain long horizon interaction, but existing frameworks remain limited. Most expose only a few basic primitives such as encode, retrieve, and delete, while higher order operations like merge, promote, demote, split, lock, and expire are missing or inconsistently supported. Moreover, there is no formal and executable specification for memory commands, leaving scope and lifecycle rules implicit and causing unpredictable behavior across systems. We introduce Text2Mem, a unified memory operation language that provides a standardized pathway from natural language to reliable execution. Text2Mem defines a compact yet expressive operation set aligned with encoding, storage, and retrieval. Each instruction is represented as a JSON based schema instance with required fields and semantic invariants, which a parser transforms into typed operation objects with normalized parameters. A validator ensures correctness before execution, while adapters map typed objects either to a SQL prototype backend or to real memory frameworks. Model based services such as embeddings or summarization are integrated when required. All results are returned through a unified execution contract. This design ensures safety, determinism, and portability across heterogeneous backends. We also outline Text2Mem Bench, a planned benchmark that separates schema generation from backend execution to enable systematic evaluation. Together, these components establish the first standardized foundation for memory control in agents.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeDiver: Cooperative AUV-USV Assisted Diver Communication via Multi-agent Reinforcement Learning Approach</title>
<link>https://arxiv.org/abs/2509.11508</link>
<guid>https://arxiv.org/abs/2509.11508</guid>
<content:encoded><![CDATA[
arXiv:2509.11508v2 Announce Type: replace 
Abstract: As underwater human activities are increasing, the demand for underwater communication service presents a significant challenge. Existing underwater diver communication methods face hurdles due to inherent disadvantages and complex underwater environments. To address this issue, we propose a scheme that utilizes maritime unmanned systems to assist divers with reliable and high-speed communication. Multiple AUVs are equipped with optical and acoustic multimodal communication devices as relay nodes, providing adaptive communication services based on changes in the diver's activity area. By using a multi-agent reinforcement learning (MARL) approach to control the cooperative movement of AUVs, high-speed and reliable data transmission between divers can be achieved. At the same time, utilizing the advantages of on-demand deployment and wide coverage of unmanned surface vehicles (USVs) as surface relay nodes to coordinate and forward information from AUVs, and controlling AUVs to adaptively select relay USV nodes for data transmission, high-quality communication between divers and surface platform can be achieved. Through simulation verification, the proposed scheme can effectively achieve reliable and high-speed communication for divers.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Learning in Spatial Agent-Based Models for Climate Risk Assessment: A Geospatial Framework with Evolutionary Economic Agents</title>
<link>https://arxiv.org/abs/2509.18633</link>
<guid>https://arxiv.org/abs/2509.18633</guid>
<content:encoded><![CDATA[
arXiv:2509.18633v2 Announce Type: replace 
Abstract: Climate risk assessment requires modelling complex interactions between spatially heterogeneous hazards and adaptive economic systems. We present a novel geospatial agent-based model that integrates climate hazard data with evolutionary learning for economic agents. Our framework combines Mesa-based spatial modelling with CLIMADA climate impact assessment, introducing adaptive learning behaviours that allow firms to evolve strategies for budget allocation, pricing, wages, and risk adaptation through fitness-based selection and mutation. We demonstrate the framework using riverine flood projections under RCP8.5 until 2100, showing that evolutionary adaptation enables firms to converge with baseline (no hazard) production levels after decades of disruption due to climate stress. Our results reveal systemic risks where even agents that are not directly exposed to floods face impacts through supply chain disruptions, with the end-of-century average price of goods 5.6% higher under RCP8.5 compared to the baseline in our illustrative economic network. This open-source framework provides financial institutions and companies with tools to quantify both direct and cascading climate risks while evaluating cost-effective adaptation strategies.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative diffusion model surrogates for mechanistic agent-based biological models</title>
<link>https://arxiv.org/abs/2505.09630</link>
<guid>https://arxiv.org/abs/2505.09630</guid>
<content:encoded><![CDATA[
arXiv:2505.09630v3 Announce Type: replace-cross 
Abstract: Mechanistic, multicellular, agent-based models are commonly used to investigate tissue, organ, and organism-scale biology at single-cell resolution. The Cellular-Potts Model (CPM) is a powerful and popular framework for developing and interrogating these models. CPMs become computationally expensive at large space- and time- scales making application and investigation of developed models difficult. Surrogate models may allow for the accelerated evaluation of CPMs of complex biological systems. However, the stochastic nature of these models means each set of parameters may give rise to different model configurations, complicating surrogate model development. In this work, we leverage denoising diffusion probabilistic models to train a generative AI surrogate of a CPM used to investigate in vitro vasculogenesis. We describe the use of an image classifier to learn the characteristics that define unique areas of a 2-dimensional parameter space. We then apply this classifier to aid in surrogate model selection and verification. Our CPM model surrogate generates model configurations 20,000 timesteps ahead of a reference configuration and demonstrates approximately a 22x reduction in computational time as compared to native code execution. Our work represents a step towards the implementation of DDPMs to develop digital twins of stochastic biological systems.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Bazaar: A Service Design for Supporting Collaborative Learning with an LLM-Powered Multi-Party Collaboration Infrastructure</title>
<link>https://arxiv.org/abs/2510.18877</link>
<guid>https://arxiv.org/abs/2510.18877</guid>
<content:encoded><![CDATA[
arXiv:2510.18877v1 Announce Type: new 
Abstract: For nearly two decades, conversational agents have played a critical role in structuring interactions in collaborative learning, shaping group dynamics, and supporting student engagement. The recent integration of large language models (LLMs) into these agents offers new possibilities for fostering critical thinking and collaborative problem solving. In this work, we begin with an open source collaboration support architecture called Bazaar and integrate an LLM-agent shell that enables introduction of LLM-empowered, real time, context sensitive collaborative support for group learning. This design and infrastructure paves the way for exploring how tailored LLM-empowered environments can reshape collaborative learning outcomes and interaction patterns.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeCRDT: Observation-Driven Coordination for Multi-Agent LLM Code Generation</title>
<link>https://arxiv.org/abs/2510.18893</link>
<guid>https://arxiv.org/abs/2510.18893</guid>
<content:encoded><![CDATA[
arXiv:2510.18893v1 Announce Type: new 
Abstract: Multi-agent LLM systems fail to realize parallel speedups due to costly coordination. We present CodeCRDT, an observation-driven coordination pattern where agents coordinate by monitoring a shared state with observable updates and deterministic convergence, rather than explicit message passing. Using Conflict-Free Replicated Data Types (CRDTs), CodeCRDT enables lock-free, conflict-free concurrent code generation with strong eventual consistency. Evaluation across 600 trials (6 tasks, 50 runs per mode) shows both benefits and trade-offs: up to 21.1% speedup on some tasks, up to 39.4% slowdown on others, and 100% convergence with zero merge failures. The study formalizes observation-driven coordination for stochastic LLM agents, revealing semantic conflict rates (5-10%) and quality-performance tradeoffs, and provides empirical characterization of when parallel coordination succeeds versus fails based on task structure.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lost in the Maze: Overcoming Context Limitations in Long-Horizon Agentic Search</title>
<link>https://arxiv.org/abs/2510.18939</link>
<guid>https://arxiv.org/abs/2510.18939</guid>
<content:encoded><![CDATA[
arXiv:2510.18939v1 Announce Type: new 
Abstract: Long-horizon agentic search requires iteratively exploring the web over long trajectories and synthesizing information across many sources, and is the foundation for enabling powerful applications like deep research systems. In this work, we show that popular agentic search frameworks struggle to scale to long trajectories primarily due to context limitations-they accumulate long, noisy content, hit context window and tool budgets, or stop early. Then, we introduce SLIM (Simple Lightweight Information Management), a simple framework that separates retrieval into distinct search and browse tools, and periodically summarizes the trajectory, keeping context concise while enabling longer, more focused searches. On long-horizon tasks, SLIM achieves comparable performance at substantially lower cost and with far fewer tool calls than strong open-source baselines across multiple base models. Specifically, with o3 as the base model, SLIM achieves 56% on BrowseComp and 31% on HLE, outperforming all open-source frameworks by 8 and 4 absolute points, respectively, while incurring 4-6x fewer tool calls. Finally, we release an automated fine-grained trajectory analysis pipeline and error taxonomy for characterizing long-horizon agentic search frameworks; SLIM exhibits fewer hallucinations than prior systems. We hope our analysis framework and simple tool design inform future long-horizon agents.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impartial Selection with Predictions</title>
<link>https://arxiv.org/abs/2510.19002</link>
<guid>https://arxiv.org/abs/2510.19002</guid>
<content:encoded><![CDATA[
arXiv:2510.19002v1 Announce Type: new 
Abstract: We study the selection of agents based on mutual nominations, a theoretical problem with many applications from committee selection to AI alignment. As agents both select and are selected, they may be incentivized to misrepresent their true opinion about the eligibility of others to influence their own chances of selection. Impartial mechanisms circumvent this issue by guaranteeing that the selection of an agent is independent of the nominations cast by that agent. Previous research has established strong bounds on the performance of impartial mechanisms, measured by their ability to approximate the number of nominations for the most highly nominated agents. We study to what extent the performance of impartial mechanisms can be improved if they are given a prediction of a set of agents receiving a maximum number of nominations. Specifically, we provide bounds on the consistency and robustness of such mechanisms, where consistency measures the performance of the mechanisms when the prediction is accurate and robustness its performance when the prediction is inaccurate. For the general setting where up to $k$ agents are to be selected and agents nominate any number of other agents, we give a mechanism with consistency $1-O\big(\frac{1}{k}\big)$ and robustness $1-\frac{1}{e}-O\big(\frac{1}{k}\big)$. For the special case of selecting a single agent based on a single nomination per agent, we prove that $1$-consistency can be achieved while guaranteeing $\frac{1}{2}$-robustness. A close comparison with previous results shows that (asymptotically) optimal consistency can be achieved with little to no sacrifice in terms of robustness.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plural Voices, Single Agent: Towards Inclusive AI in Multi-User Domestic Spaces</title>
<link>https://arxiv.org/abs/2510.19008</link>
<guid>https://arxiv.org/abs/2510.19008</guid>
<content:encoded><![CDATA[
arXiv:2510.19008v1 Announce Type: new 
Abstract: Domestic AI agents faces ethical, autonomy, and inclusion challenges, particularly for overlooked groups like children, elderly, and Neurodivergent users. We present the Plural Voices Model (PVM), a novel single-agent framework that dynamically negotiates multi-user needs through real-time value alignment, leveraging diverse public datasets on mental health, eldercare, education, and moral reasoning. Using human+synthetic curriculum design with fairness-aware scenarios and ethical enhancements, PVM identifies core values, conflicts, and accessibility requirements to inform inclusive principles. Our privacy-focused prototype features adaptive safety scaffolds, tailored interactions (e.g., step-by-step guidance for Neurodivergent users, simple wording for children), and equitable conflict resolution. In preliminary evaluations, PVM outperforms multi-agent baselines in compliance (76% vs. 70%), fairness (90% vs. 85%), safety-violation rate (0% vs. 7%), and latency. Design innovations, including video guidance, autonomy sliders, family hubs, and adaptive safety dashboards, demonstrate new directions for ethical and inclusive domestic AI, for building user-centered agentic systems in plural domestic contexts. Our Codes and Model are been open sourced, available for reproduction: https://github.com/zade90/Agora
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REPAIR Approach for Social-based City Reconstruction Planning in case of natural disasters</title>
<link>https://arxiv.org/abs/2510.19048</link>
<guid>https://arxiv.org/abs/2510.19048</guid>
<content:encoded><![CDATA[
arXiv:2510.19048v1 Announce Type: new 
Abstract: Natural disasters always have several effects on human lives. It is challenging for governments to tackle these incidents and to rebuild the economic, social and physical infrastructures and facilities with the available resources (mainly budget and time). Governments always define plans and policies according to the law and political strategies that should maximise social benefits. The severity of damage and the vast resources needed to bring life back to normality make such reconstruction a challenge. This article is the extension of our previously published work by conducting comprehensive comparative analysis by integrating additional deep learning models plus random agent which is used as a baseline. Our prior research introduced a decision support system by using the Deep Reinforcement Learning technique for the planning of post-disaster city reconstruction, maximizing the social benefit of the reconstruction process, considering available resources, meeting the needs of the broad community stakeholders (like citizens' social benefits and politicians' priorities) and keeping in consideration city's structural constraints (like dependencies among roads and buildings). The proposed approach, named post disaster REbuilding plAn ProvIdeR (REPAIR) is generic. It can determine a set of alternative plans for local administrators who select the ideal one to implement, and it can be applied to areas of any extension. We show the application of REPAIR in a real use case, i.e., to the L'Aquila reconstruction process, damaged in 2009 by a major earthquake.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Guidance for Configuration-Based Multi-Agent Pathfinding</title>
<link>https://arxiv.org/abs/2510.19072</link>
<guid>https://arxiv.org/abs/2510.19072</guid>
<content:encoded><![CDATA[
arXiv:2510.19072v1 Announce Type: new 
Abstract: Guidance is an emerging concept that improves the empirical performance of real-time, sub-optimal multi-agent pathfinding (MAPF) methods. It offers additional information to MAPF algorithms to mitigate congestion on a global scale by considering the collective behavior of all agents across the entire workspace. This global perspective helps reduce agents' waiting times, thereby improving overall coordination efficiency. In contrast, this study explores an alternative approach: providing local guidance in the vicinity of each agent. While such localized methods involve recomputation as agents move and may appear computationally demanding, we empirically demonstrate that supplying informative spatiotemporal cues to the planner can significantly improve solution quality without exceeding a moderate time budget. When applied to LaCAM, a leading configuration-based solver, this form of guidance establishes a new performance frontier for MAPF.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Desirable Effort Fairness and Optimality Trade-offs in Strategic Learning</title>
<link>https://arxiv.org/abs/2510.19098</link>
<guid>https://arxiv.org/abs/2510.19098</guid>
<content:encoded><![CDATA[
arXiv:2510.19098v1 Announce Type: new 
Abstract: Strategic learning studies how decision rules interact with agents who may strategically change their inputs/features to achieve better outcomes. In standard settings, models assume that the decision-maker's sole scope is to learn a classifier that maximizes an objective (e.g., accuracy) assuming that agents best respond. However, real decision-making systems' goals do not align exclusively with producing good predictions. They may consider the external effects of inducing certain incentives, which translates to the change of certain features being more desirable for the decision maker. Further, the principal may also need to incentivize desirable feature changes fairly across heterogeneous agents. How much does this constrained optimization (i.e., maximize the objective, but restrict agents' incentive disparity) cost the principal? We propose a unified model of principal-agent interaction that captures this trade-off under three additional components: (1) causal dependencies between features, such that changes in one feature affect others; (2) heterogeneous manipulation costs between agents; and (3) peer learning, through which agents infer the principal's rule. We provide theoretical guarantees on the principal's optimality loss constrained to a particular desirability fairness tolerance for multiple broad classes of fairness measures. Finally, through experiments on real datasets, we show the explicit tradeoff between maximizing accuracy and fairness in desirability effort.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Your AI Agent Succumbs to Peer-Pressure: Studying Opinion-Change Dynamics of LLMs</title>
<link>https://arxiv.org/abs/2510.19107</link>
<guid>https://arxiv.org/abs/2510.19107</guid>
<content:encoded><![CDATA[
arXiv:2510.19107v1 Announce Type: new 
Abstract: We investigate how peer pressure influences the opinions of Large Language Model (LLM) agents across a spectrum of cognitive commitments by embedding them in social networks where they update opinions based on peer perspectives. Our findings reveal key departures from traditional conformity assumptions. First, agents follow a sigmoid curve: stable at low pressure, shifting sharply at threshold, and saturating at high. Second, conformity thresholds vary by model: Gemini 1.5 Flash requires over 70% peer disagreement to flip, whereas ChatGPT-4o-mini shifts with a dissenting minority. Third, we uncover a fundamental "persuasion asymmetry," where shifting an opinion from affirmative-to-negative requires a different cognitive effort than the reverse. This asymmetry results in a "dual cognitive hierarchy": the stability of cognitive constructs inverts based on the direction of persuasion. For instance, affirmatively-held core values are robust against opposition but easily adopted from a negative stance, a pattern that inverts for other constructs like attitudes. These dynamics echoing complex human biases like negativity bias, prove robust across different topics and discursive frames (moral, economic, sociotropic). This research introduces a novel framework for auditing the emergent socio-cognitive behaviors of multi-agent AI systems, demonstrating their decision-making is governed by a fluid, context-dependent architecture, not a static logic.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Ego: Acquiring Team-Level Tactical Situational Awareness via Cross-Egocentric Contrastive Video Representation Learning</title>
<link>https://arxiv.org/abs/2510.19150</link>
<guid>https://arxiv.org/abs/2510.19150</guid>
<content:encoded><![CDATA[
arXiv:2510.19150v1 Announce Type: new 
Abstract: Human team tactics emerge from each player's individual perspective and their ability to anticipate, interpret, and adapt to teammates' intentions. While advances in video understanding have improved the modeling of team interactions in sports, most existing work relies on third-person broadcast views and overlooks the synchronous, egocentric nature of multi-agent learning. We introduce X-Ego-CS, a benchmark dataset consisting of 124 hours of gameplay footage from 45 professional-level matches of the popular e-sports game Counter-Strike 2, designed to facilitate research on multi-agent decision-making in complex 3D environments. X-Ego-CS provides cross-egocentric video streams that synchronously capture all players' first-person perspectives along with state-action trajectories. Building on this resource, we propose Cross-Ego Contrastive Learning (CECL), which aligns teammates' egocentric visual streams to foster team-level tactical situational awareness from an individual's perspective. We evaluate CECL on a teammate-opponent location prediction task, demonstrating its effectiveness in enhancing an agent's ability to infer both teammate and opponent positions from a single first-person view using state-of-the-art video encoders. Together, X-Ego-CS and CECL establish a foundation for cross-egocentric multi-agent benchmarking in esports. More broadly, our work positions gameplay understanding as a testbed for multi-agent modeling and tactical learning, with implications for spatiotemporal reasoning and human-AI teaming in both virtual and real-world domains. Code and dataset are available at https://github.com/HATS-ICT/x-ego.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Faceted Evaluation of Tool-Augmented Dialogue Systems</title>
<link>https://arxiv.org/abs/2510.19186</link>
<guid>https://arxiv.org/abs/2510.19186</guid>
<content:encoded><![CDATA[
arXiv:2510.19186v1 Announce Type: new 
Abstract: Evaluating conversational AI systems that use external tools is challenging, as errors can arise from complex interactions among user, agent, and tools. While existing evaluation methods assess either user satisfaction or agents' tool-calling capabilities, they fail to capture critical errors in multi-turn tool-augmented dialogues-such as when agents misinterpret tool results yet appear satisfactory to users. We introduce TRACE, a benchmark of systematically synthesized tool-augmented conversations covering diverse error cases, and SCOPE, an evaluation framework that automatically discovers diverse error patterns and evaluation rubrics in tool-augmented dialogues. Experiments show SCOPE significantly outperforms the baseline, particularly on challenging cases where user satisfaction signals are misleading.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Communication-Efficient Decentralized Actor-Critic Algorithm</title>
<link>https://arxiv.org/abs/2510.19199</link>
<guid>https://arxiv.org/abs/2510.19199</guid>
<content:encoded><![CDATA[
arXiv:2510.19199v1 Announce Type: new 
Abstract: In this paper, we study the problem of reinforcement learning in multi-agent systems where communication among agents is limited. We develop a decentralized actor-critic learning framework in which each agent performs several local updates of its policy and value function, where the latter is approximated by a multi-layer neural network, before exchanging information with its neighbors. This local training strategy substantially reduces the communication burden while maintaining coordination across the network. We establish finite-time convergence analysis for the algorithm under Markov-sampling. Specifically, to attain the $\varepsilon$-accurate stationary point, the sample complexity is of order $\mathcal{O}(\varepsilon^{-3})$ and the communication complexity is of order $\mathcal{O}(\varepsilon^{-1}\tau^{-1})$, where tau denotes the number of local training steps. We also show how the final error bound depends on the neural network's approximation quality. Numerical experiments in a cooperative control setting illustrate and validate the theoretical findings.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebGraphEval: Multi-Turn Trajectory Evaluation for Web Agents using Graph Representation</title>
<link>https://arxiv.org/abs/2510.19205</link>
<guid>https://arxiv.org/abs/2510.19205</guid>
<content:encoded><![CDATA[
arXiv:2510.19205v1 Announce Type: new 
Abstract: Current evaluation of web agents largely reduces to binary success metrics or conformity to a single reference trajectory, ignoring the structural diversity present in benchmark datasets. We present WebGraphEval, a framework that abstracts trajectories from multiple agents into a unified, weighted action graph. This representation is directly compatible with benchmarks such as WebArena, leveraging leaderboard runs and newly collected trajectories without modifying environments. The framework canonically encodes actions, merges recurring behaviors, and applies structural analyses including reward propagation and success-weighted edge statistics. Evaluations across thousands of trajectories from six web agents show that the graph abstraction captures cross-model regularities, highlights redundancy and inefficiency, and identifies critical decision points overlooked by outcome-based metrics. By framing web interaction as graph-structured data, WebGraphEval establishes a general methodology for multi-path, cross-agent, and efficiency-aware evaluation of web agents.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defending Against Prompt Injection with DataFilter</title>
<link>https://arxiv.org/abs/2510.19207</link>
<guid>https://arxiv.org/abs/2510.19207</guid>
<content:encoded><![CDATA[
arXiv:2510.19207v1 Announce Type: new 
Abstract: When large language model (LLM) agents are increasingly deployed to automate tasks and interact with untrusted external data, prompt injection emerges as a significant security threat. By injecting malicious instructions into the data that LLMs access, an attacker can arbitrarily override the original user task and redirect the agent toward unintended, potentially harmful actions. Existing defenses either require access to model weights (fine-tuning), incur substantial utility loss (detection-based), or demand non-trivial system redesign (system-level). Motivated by this, we propose DataFilter, a test-time model-agnostic defense that removes malicious instructions from the data before it reaches the backend LLM. DataFilter is trained with supervised fine-tuning on simulated injections and leverages both the user's instruction and the data to selectively strip adversarial content while preserving benign information. Across multiple benchmarks, DataFilter consistently reduces the prompt injection attack success rates to near zero while maintaining the LLMs' utility. DataFilter delivers strong security, high utility, and plug-and-play deployment, making it a strong practical defense to secure black-box commercial LLMs against prompt injection. Our DataFilter model is released at https://huggingface.co/JoyYizhu/DataFilter for immediate use, with the code to reproduce our results at https://github.com/yizhu-joy/DataFilter.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiSRouter: Distributed Self-Routing for LLM Selections</title>
<link>https://arxiv.org/abs/2510.19208</link>
<guid>https://arxiv.org/abs/2510.19208</guid>
<content:encoded><![CDATA[
arXiv:2510.19208v1 Announce Type: new 
Abstract: The proliferation of Large Language Models (LLMs) has created a diverse ecosystem of models with highly varying performance and costs, necessitating effective query routing to balance performance and expense. Current routing systems often rely on a centralized external router trained on a fixed set of LLMs, making them inflexible and prone to poor performance since the small router can not fully understand the knowledge boundaries of different LLMs. We introduce DiSRouter (Distributed Self-Router), a novel paradigm that shifts from centralized control to distributed routing. In DiSRouter, a query traverses a network of LLM agents, each independently deciding whether to answer or route to other agents based on its own self-awareness, its ability to judge its competence. This distributed design offers superior flexibility, scalability, and generalizability. To enable this, we propose a two-stage Self-Awareness Training pipeline that enhances each LLM's self-awareness. Extensive experiments demonstrate that DiSRouter significantly outperforms existing routing methods in utility across various scenarios, effectively distinguishes between easy and hard queries, and shows strong generalization to out-of-domain tasks. Our work validates that leveraging an LLM's intrinsic self-awareness is more effective than external assessment, paving the way for more modular and efficient multi-agent systems.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Design Science Blueprint for an Orchestrated AI Assistant in Doctoral Supervision</title>
<link>https://arxiv.org/abs/2510.19227</link>
<guid>https://arxiv.org/abs/2510.19227</guid>
<content:encoded><![CDATA[
arXiv:2510.19227v1 Announce Type: new 
Abstract: This study presents a design science blueprint for an orchestrated AI assistant and co-pilot in doctoral supervision that acts as a socio-technical mediator. Design requirements are derived from Stakeholder Theory and bounded by Academic Integrity. We consolidated recent evidence on supervision gaps and student wellbeing, then mapped issues to adjacent large language model capabilities using a transparent severity-mitigability triage. The artefact assembles existing capabilities into one accountable agentic AI workflow that proposes retrieval-augmented generation and temporal knowledge graphs, as well as mixture-of-experts routing as a solution stack of technologies to address existing doctoral supervision pain points. Additionally, a student context store is proposed, which introduces behaviour patches that turn tacit guidance into auditable practice and student-set thresholds that trigger progress summaries, while keeping authorship and final judgement with people. We specify a student-initiated moderation loop in which assistant outputs are routed to a supervisor for review and patching, and we analyse a reconfigured stakeholder ecosystem that makes information explicit and accountable. Risks in such a system exist, and among others, include AI over-reliance and the potential for the illusion of learning, while guardrails are proposed. The contribution is an ex ante, literature-grounded design with workflow and governance rules that institutions can implement and trial across disciplines.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatiotemporal Tubes based Control of Unknown Multi-Agent Systems for Temporal Reach-Avoid-Stay Tasks</title>
<link>https://arxiv.org/abs/2510.19232</link>
<guid>https://arxiv.org/abs/2510.19232</guid>
<content:encoded><![CDATA[
arXiv:2510.19232v1 Announce Type: new 
Abstract: The paper focuses on designing a controller for unknown dynamical multi-agent systems to achieve temporal reach-avoid-stay tasks for each agent while preventing inter-agent collisions. The main objective is to generate a spatiotemporal tube (STT) for each agent and thereby devise a closed-form, approximation-free, and decentralized control strategy that ensures the system trajectory reaches the target within a specific time while avoiding time-varying unsafe sets and collisions with other agents. In order to achieve this, the requirements of STTs are formulated as a robust optimization problem (ROP) and solved using a sampling-based scenario optimization problem (SOP) to address the issue of infeasibility caused by the infinite number of constraints in ROP. The STTs are generated by solving the SOP, and the corresponding closed-form control is designed to fulfill the specified task. Finally, the effectiveness of our approach is demonstrated through two case studies, one involving omnidirectional robots and the other involving multiple drones modelled as Euler-Lagrange systems.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpret Policies in Deep Reinforcement Learning using SILVER with RL-Guided Labeling: A Model-level Approach to High-dimensional and Multi-action Environments</title>
<link>https://arxiv.org/abs/2510.19244</link>
<guid>https://arxiv.org/abs/2510.19244</guid>
<content:encoded><![CDATA[
arXiv:2510.19244v1 Announce Type: new 
Abstract: Deep reinforcement learning (RL) achieves remarkable performance but lacks interpretability, limiting trust in policy behavior. The existing SILVER framework (Li, Siddique, and Cao 2025) explains RL policy via Shapley-based regression but remains restricted to low-dimensional, binary-action domains. We propose SILVER with RL-guided labeling, an enhanced variant that extends SILVER to multi-action and high-dimensional environments by incorporating the RL policy's own action outputs into the boundary points identification. Our method first extracts compact feature representations from image observations, performs SHAP-based feature attribution, and then employs RL-guided labeling to generate behaviorally consistent boundary datasets. Surrogate models, such as decision trees and regression-based functions, are subsequently trained to interpret RL policy's decision structure. We evaluate the proposed framework on two Atari environments using three deep RL algorithms and conduct human-subject study to assess the clarity and trustworthiness of the derived interpretable policy. Results show that our approach maintains competitive task performance while substantially improving transparency and human understanding of agent behavior. This work advances explainable RL by transforming SILVER into a scalable and behavior-aware framework for interpreting deep RL agents in high-dimensional, multi-action settings.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>See, Think, Act: Online Shopper Behavior Simulation with VLM Agents</title>
<link>https://arxiv.org/abs/2510.19245</link>
<guid>https://arxiv.org/abs/2510.19245</guid>
<content:encoded><![CDATA[
arXiv:2510.19245v1 Announce Type: new 
Abstract: LLMs have recently demonstrated strong potential in simulating online shopper behavior. Prior work has improved action prediction by applying SFT on action traces with LLM-generated rationales, and by leveraging RL to further enhance reasoning capabilities. Despite these advances, current approaches rely on text-based inputs and overlook the essential role of visual perception in shaping human decision-making during web GUI interactions. In this paper, we investigate the integration of visual information, specifically webpage screenshots, into behavior simulation via VLMs, leveraging OPeRA dataset. By grounding agent decision-making in both textual and visual modalities, we aim to narrow the gap between synthetic agents and real-world users, thereby enabling more cognitively aligned simulations of online shopping behavior. Specifically, we employ SFT for joint action prediction and rationale generation, conditioning on the full interaction context, which comprises action history, past HTML observations, and the current webpage screenshot. To further enhance reasoning capabilities, we integrate RL with a hierarchical reward structure, scaled by a difficulty-aware factor that prioritizes challenging decision points. Empirically, our studies show that incorporating visual grounding yields substantial gains: the combination of text and image inputs improves exact match accuracy by more than 6% over text-only inputs. These results indicate that multi-modal grounding not only boosts predictive accuracy but also enhances simulation fidelity in visually complex environments, which captures nuances of human attention and decision-making that text-only agents often miss. Finally, we revisit the design space of behavior simulation frameworks, identify key methodological limitations, and propose future research directions toward building efficient and effective human behavior simulators.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Newborn to Impact: Bias-Aware Citation Prediction</title>
<link>https://arxiv.org/abs/2510.19246</link>
<guid>https://arxiv.org/abs/2510.19246</guid>
<content:encoded><![CDATA[
arXiv:2510.19246v1 Announce Type: new 
Abstract: As a key to accessing research impact, citation dynamics underpins research evaluation, scholarly recommendation, and the study of knowledge diffusion. Citation prediction is particularly critical for newborn papers, where early assessment must be performed without citation signals and under highly long-tailed distributions. We identify two key research gaps: (i) insufficient modeling of implicit factors of scientific impact, leading to reliance on coarse proxies; and (ii) a lack of bias-aware learning that can deliver stable predictions on lowly cited papers. We address these gaps by proposing a Bias-Aware Citation Prediction Framework, which combines multi-agent feature extraction with robust graph representation learning. First, a multi-agent x graph co-learning module derives fine-grained, interpretable signals, such as reproducibility, collaboration network, and text quality, from metadata and external resources, and fuses them with heterogeneous-network embeddings to provide rich supervision even in the absence of early citation signals. Second, we incorporate a set of robust mechanisms: a two-stage forward process that routes explicit factors through an intermediate exposure estimate, GroupDRO to optimize worst-case group risk across environments, and a regularization head that performs what-if analyses on controllable factors under monotonicity and smoothness constraints. Comprehensive experiments on two real-world datasets demonstrate the effectiveness of our proposed model. Specifically, our model achieves around a 13% reduction in error metrics (MALE and RMSLE) and a notable 5.5% improvement in the ranking metric (NDCG) over the baseline methods.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SheetBrain: A Neuro-Symbolic Agent for Accurate Reasoning over Complex and Large Spreadsheets</title>
<link>https://arxiv.org/abs/2510.19247</link>
<guid>https://arxiv.org/abs/2510.19247</guid>
<content:encoded><![CDATA[
arXiv:2510.19247v1 Announce Type: new 
Abstract: Understanding and reasoning over complex spreadsheets remain fundamental challenges for large language models (LLMs), which often struggle with accurately capturing the complex structure of tables and ensuring reasoning correctness. In this work, we propose SheetBrain, a neuro-symbolic dual workflow agent framework designed for accurate reasoning over tabular data, supporting both spreadsheet question answering and manipulation tasks. SheetBrain comprises three core modules: an understanding module, which produces a comprehensive overview of the spreadsheet - including sheet summary and query-based problem insight to guide reasoning; an execution module, which integrates a Python sandbox with preloaded table-processing libraries and an Excel helper toolkit for effective multi-turn reasoning; and a validation module, which verifies the correctness of reasoning and answers, triggering re-execution when necessary. We evaluate SheetBrain on multiple public tabular QA and manipulation benchmarks, and introduce SheetBench, a new benchmark targeting large, multi-table, and structurally complex spreadsheets. Experimental results show that SheetBrain significantly improves accuracy on both existing benchmarks and the more challenging scenarios presented in SheetBench. Our code is publicly available at https://github.com/microsoft/SheetBrain.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social World Model-Augmented Mechanism Design Policy Learning</title>
<link>https://arxiv.org/abs/2510.19270</link>
<guid>https://arxiv.org/abs/2510.19270</guid>
<content:encoded><![CDATA[
arXiv:2510.19270v1 Announce Type: new 
Abstract: Designing adaptive mechanisms to align individual and collective interests remains a central challenge in artificial social intelligence. Existing methods often struggle with modeling heterogeneous agents possessing persistent latent traits (e.g., skills, preferences) and dealing with complex multi-agent system dynamics. These challenges are compounded by the critical need for high sample efficiency due to costly real-world interactions. World Models, by learning to predict environmental dynamics, offer a promising pathway to enhance mechanism design in heterogeneous and complex systems. In this paper, we introduce a novel method named SWM-AP (Social World Model-Augmented Mechanism Design Policy Learning), which learns a social world model hierarchically modeling agents' behavior to enhance mechanism design. Specifically, the social world model infers agents' traits from their interaction trajectories and learns a trait-based model to predict agents' responses to the deployed mechanisms. The mechanism design policy collects extensive training trajectories by interacting with the social world model, while concurrently inferring agents' traits online during real-world interactions to further boost policy learning efficiency. Experiments in diverse settings (tax policy design, team coordination, and facility location) demonstrate that SWM-AP outperforms established model-based and model-free RL baselines in cumulative rewards and sample efficiency.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Specification to Service: Accelerating API-First Development Using Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2510.19274</link>
<guid>https://arxiv.org/abs/2510.19274</guid>
<content:encoded><![CDATA[
arXiv:2510.19274v1 Announce Type: new 
Abstract: This paper presents a system that uses Large Language Models (LLMs)-based agents to automate the API-first development of RESTful microservices. This system helps to create an OpenAPI specification, generate server code from it, and refine the code through a feedback loop that analyzes execution logs and error messages. The integration of log analysis enables the LLM to detect and address issues efficiently, reducing the number of iterations required to produce functional and robust services. This study's main goal is to advance API-first development automation for RESTful web services and test the capability of LLM-based multi-agent systems in supporting the API-first development approach. To test the proposed system's potential, we utilized the PRAB benchmark. The results indicate that if we keep the OpenAPI specification small and focused, LLMs are capable of generating complete functional code with business logic that aligns to the specification. The code for the system is publicly available at https://github.com/sirbh/code-gen
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TheMCPCompany: Creating General-purpose Agents with Task-specific Tools</title>
<link>https://arxiv.org/abs/2510.19286</link>
<guid>https://arxiv.org/abs/2510.19286</guid>
<content:encoded><![CDATA[
arXiv:2510.19286v1 Announce Type: new 
Abstract: Since the introduction of the Model Context Protocol (MCP), the number of available tools for Large Language Models (LLMs) has increased significantly. These task-specific tool sets offer an alternative to general-purpose tools such as web browsers, while being easier to develop and maintain than GUIs. However, current general-purpose agents predominantly rely on web browsers for interacting with the environment. Here, we introduce TheMCPCompany, a benchmark for evaluating tool-calling agents on tasks that involve interacting with various real-world services. We use the REST APIs of these services to create MCP servers, which include over 18,000 tools. We also provide manually annotated ground-truth tools for each task. In our experiments, we use the ground truth tools to show the potential of tool-calling agents for both improving performance and reducing costs assuming perfect tool retrieval. Next, we explore agent performance using tool retrieval to study the real-world practicality of tool-based agents. While all models with tool retrieval perform similarly or better than browser-based agents, smaller models cannot take full advantage of the available tools through retrieval. On the other hand, GPT-5's performance with tool retrieval is very close to its performance with ground-truth tools. Overall, our work shows that the most advanced reasoning models are effective at discovering tools in simpler environments, but seriously struggle with navigating complex enterprise environments. TheMCPCompany reveals that navigating tens of thousands of tools and combining them in non-trivial ways to solve complex problems is still a challenging task for current models and requires both better reasoning and better retrieval models.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Make Friends: Coaching LLM Agents toward Emergent Social Ties</title>
<link>https://arxiv.org/abs/2510.19299</link>
<guid>https://arxiv.org/abs/2510.19299</guid>
<content:encoded><![CDATA[
arXiv:2510.19299v1 Announce Type: new 
Abstract: Can large language model (LLM) agents reproduce the complex social dynamics that characterize human online behavior -- shaped by homophily, reciprocity, and social validation -- and what memory and learning mechanisms enable such dynamics to emerge? We present a multi-agent LLM simulation framework in which agents repeatedly interact, evaluate one another, and adapt their behavior through in-context learning accelerated by a coaching signal. To model human social behavior, we design behavioral reward functions that capture core drivers of online engagement, including social interaction, information seeking, self-presentation, coordination, and emotional support. These rewards align agent objectives with empirically observed user motivations, enabling the study of how network structures and group formations emerge from individual decision-making. Our experiments show that coached LLM agents develop stable interaction patterns and form emergent social ties, yielding network structures that mirror properties of real online communities. By combining behavioral rewards with in-context adaptation, our framework establishes a principled testbed for investigating collective dynamics in LLM populations and reveals how artificial agents may approximate or diverge from human-like social behavior.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Knowledge Adaptation for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.19314</link>
<guid>https://arxiv.org/abs/2510.19314</guid>
<content:encoded><![CDATA[
arXiv:2510.19314v1 Announce Type: new 
Abstract: Reinforcement Learning enables agents to learn optimal behaviors through interactions with environments. However, real-world environments are typically non-stationary, requiring agents to continuously adapt to new tasks and changing conditions. Although Continual Reinforcement Learning facilitates learning across multiple tasks, existing methods often suffer from catastrophic forgetting and inefficient knowledge utilization. To address these challenges, we propose Continual Knowledge Adaptation for Reinforcement Learning (CKA-RL), which enables the accumulation and effective utilization of historical knowledge. Specifically, we introduce a Continual Knowledge Adaptation strategy, which involves maintaining a task-specific knowledge vector pool and dynamically using historical knowledge to adapt the agent to new tasks. This process mitigates catastrophic forgetting and enables efficient knowledge transfer across tasks by preserving and adapting critical model parameters. Additionally, we propose an Adaptive Knowledge Merging mechanism that combines similar knowledge vectors to address scalability challenges, reducing memory requirements while ensuring the retention of essential knowledge. Experiments on three benchmarks demonstrate that the proposed CKA-RL outperforms state-of-the-art methods, achieving an improvement of 4.20% in overall performance and 8.02% in forward transfer. The source code is available at https://github.com/Fhujinwu/CKA-RL.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Authorization of Knowledge-base Agents in an Intent-based Management Function</title>
<link>https://arxiv.org/abs/2510.19324</link>
<guid>https://arxiv.org/abs/2510.19324</guid>
<content:encoded><![CDATA[
arXiv:2510.19324v1 Announce Type: new 
Abstract: As networks move toward the next-generation 6G, Intent-based Management (IbM) systems are increasingly adopted to simplify and automate network management by translating high-level intents into low-level configurations. Within these systems, agents play a critical role in monitoring current state of the network, gathering data, and enforcing actions across the network to fulfill the intent. However, ensuring secure and fine-grained authorization of agents remains a significant challenge, especially in dynamic and multi-tenant environments. Traditional models such as Role-Based Access Control (RBAC), Attribute-Based Access Control (ABAC) and Relational-Based Access Control (RelBAC) often lack the flexibility to accommodate the evolving context and granularity required by intentbased operations. In this paper, we propose an enhanced authorization framework that integrates contextual and functional attributes with agent roles to achieve dynamic, policy-driven access control. By analyzing agent functionalities, our approach ensures that agents are granted only the minimal necessary privileges towards knowledge graphs.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SORA-ATMAS: Adaptive Trust Management and Multi-LLM Aligned Governance for Future Smart Cities</title>
<link>https://arxiv.org/abs/2510.19327</link>
<guid>https://arxiv.org/abs/2510.19327</guid>
<content:encoded><![CDATA[
arXiv:2510.19327v1 Announce Type: new 
Abstract: The rapid evolution of smart cities has increased the reliance on intelligent interconnected services to optimize infrastructure, resources, and citizen well-being. Agentic AI has emerged as a key enabler by supporting autonomous decision-making and adaptive coordination, allowing urban systems to respond in real time to dynamic conditions. Its benefits are evident in areas such as transportation, where the integration of traffic data, weather forecasts, and safety sensors enables dynamic rerouting and a faster response to hazards. However, its deployment across heterogeneous smart city ecosystems raises critical governance, risk, and compliance (GRC) challenges, including accountability, data privacy, and regulatory alignment within decentralized infrastructures. Evaluation of SORA-ATMAS with three domain agents (Weather, Traffic, and Safety) demonstrated that its governance policies, including a fallback mechanism for high-risk scenarios, effectively steer multiple LLMs (GPT, Grok, DeepSeek) towards domain-optimized, policy-aligned outputs, producing an average MAE reduction of 35% across agents. Results showed stable weather monitoring, effective handling of high-risk traffic plateaus 0.85, and adaptive trust regulation in Safety/Fire scenarios 0.65. Runtime profiling of a 3-agent deployment confirmed scalability, with throughput between 13.8-17.2 requests per second, execution times below 72~ms, and governance delays under 100 ms, analytical projections suggest maintained performance at larger scales. Cross-domain rules ensured safe interoperability, with traffic rerouting permitted only under validated weather conditions. These findings validate SORA-ATMAS as a regulation-aligned, context-aware, and verifiable governance framework that consolidates distributed agent outputs into accountable, real-time decisions, offering a resilient foundation for smart-city management.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Markov Decision Process for Variable Selection in Branch &amp; Bound</title>
<link>https://arxiv.org/abs/2510.19348</link>
<guid>https://arxiv.org/abs/2510.19348</guid>
<content:encoded><![CDATA[
arXiv:2510.19348v1 Announce Type: new 
Abstract: Mixed-Integer Linear Programming (MILP) is a powerful framework used to address a wide range of NP-hard combinatorial optimization problems, often solved by Branch and Bound (B&amp;B). A key factor influencing the performance of B&amp;B solvers is the variable selection heuristic governing branching decisions. Recent contributions have sought to adapt reinforcement learning (RL) algorithms to the B&amp;B setting to learn optimal branching policies, through Markov Decision Processes (MDP) inspired formulations, and ad hoc convergence theorems and algorithms. In this work, we introduce BBMDP, a principled vanilla MDP formulation for variable selection in B&amp;B, allowing to leverage a broad range of RL algorithms for the purpose of learning optimal B\&amp;B heuristics. Computational experiments validate our model empirically, as our branching agent outperforms prior state-of-the-art RL agents on four standard MILP benchmarks.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgenticMath: Enhancing LLM Reasoning via Agentic-based Math Data Generation</title>
<link>https://arxiv.org/abs/2510.19361</link>
<guid>https://arxiv.org/abs/2510.19361</guid>
<content:encoded><![CDATA[
arXiv:2510.19361v1 Announce Type: new 
Abstract: The creation of high-quality datasets to improve Large Language Model (LLM) reasoning remains a significant challenge, as current methods often suffer from generating low-quality/incorrect answers and limited information richness from available data sources. To address this, we propose AgenticMath, a novel agentic pipeline for generating high-quality mathematical question-answer pairs to enhance the supervised fine-tuning of LLMs. Our method operates through four stages: (1) Seed Question Filter that selects questions with high information richness, complexity, and clarity; (2) an Agentic Question Rephrase step that employs a multi-agent system to generate diverse, logically consistent paraphrases; (3) an Answer Augment step where rewrite answers using chain-of-thought reasoning to enhance numerical and logical correctness, without reliance on human-provided labels; and (4) a final Question and Answer Evaluation that retains only the most superior pairs. Extensive experiments demonstrate that, fine-tuning 3B-8B parameter LLMs on AgenticMath generated datasets (comprising only 30-60K math samples) achieves competitive or superior performance on diverse in domain and out-of-domain mathematical reasoning benchmarks compared to baselines trained on much more data (e.g., 400K or 2.3M samples). Our work demonstrates that targeted, high-quality data generation is a more efficient path to improving mathematical reasoning in LLMs than large-scale, low-quality alternatives.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ColorAgent: Building A Robust, Personalized, and Interactive OS Agent</title>
<link>https://arxiv.org/abs/2510.19386</link>
<guid>https://arxiv.org/abs/2510.19386</guid>
<content:encoded><![CDATA[
arXiv:2510.19386v1 Announce Type: new 
Abstract: With the advancements in hardware, software, and large language model technologies, the interaction between humans and operating systems has evolved from the command-line interface to the rapidly emerging AI agent interactions. Building an operating system (OS) agent capable of executing user instructions and faithfully following user desires is becoming a reality. In this technical report, we present ColorAgent, an OS agent designed to engage in long-horizon, robust interactions with the environment while also enabling personalized and proactive user interaction. To enable long-horizon interactions with the environment, we enhance the model's capabilities through step-wise reinforcement learning and self-evolving training, while also developing a tailored multi-agent framework that ensures generality, consistency, and robustness. In terms of user interaction, we explore personalized user intent recognition and proactive engagement, positioning the OS agent not merely as an automation tool but as a warm, collaborative partner. We evaluate ColorAgent on the AndroidWorld and AndroidLab benchmarks, achieving success rates of 77.2% and 50.7%, respectively, establishing a new state of the art. Nonetheless, we note that current benchmarks are insufficient for a comprehensive evaluation of OS agents and propose further exploring directions in future work, particularly in the areas of evaluation paradigms, agent collaboration, and security. Our code is available at https://github.com/MadeAgents/mobile-use.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monitoring LLM-based Multi-Agent Systems Against Corruptions via Node Evaluation</title>
<link>https://arxiv.org/abs/2510.19420</link>
<guid>https://arxiv.org/abs/2510.19420</guid>
<content:encoded><![CDATA[
arXiv:2510.19420v1 Announce Type: new 
Abstract: Large Language Model (LLM)-based Multi-Agent Systems (MAS) have become a popular paradigm of AI applications. However, trustworthiness issues in MAS remain a critical concern. Unlike challenges in single-agent systems, MAS involve more complex communication processes, making them susceptible to corruption attacks. To mitigate this issue, several defense mechanisms have been developed based on the graph representation of MAS, where agents represent nodes and communications form edges. Nevertheless, these methods predominantly focus on static graph defense, attempting to either detect attacks in a fixed graph structure or optimize a static topology with certain defensive capabilities. To address this limitation, we propose a dynamic defense paradigm for MAS graph structures, which continuously monitors communication within the MAS graph, then dynamically adjusts the graph topology, accurately disrupts malicious communications, and effectively defends against evolving and diverse dynamic attacks. Experimental results in increasingly complex and dynamic MAS environments demonstrate that our method significantly outperforms existing MAS defense mechanisms, contributing an effective guardrail for their trustworthy applications. Our code is available at https://github.com/ChengcanWu/Monitoring-LLM-Based-Multi-Agent-Systems.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSC-Bench: A Rigorous Benchmark for Multi-Server Tool Orchestration</title>
<link>https://arxiv.org/abs/2510.19423</link>
<guid>https://arxiv.org/abs/2510.19423</guid>
<content:encoded><![CDATA[
arXiv:2510.19423v1 Announce Type: new 
Abstract: We introduce MSC-Bench, a large-scale benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents in a hierarchical Model-Context Protocol (MCP) ecosystem. Existing benchmarks often evaluate tools in isolation, ignoring challenges such as functional overlap and cross-server orchestration, leading to overly optimistic assessments. MSC-Bench addresses these gaps by constructing ground truth through 'equal function sets', allowing objective metrics such as F1 score and reducing the dependency on LLM-as-a-judge evaluation. Organized as a five-level curriculum, it systematically tests agent capabilities from single-tool orchestration to complex cross-server planning, and robustness to out-of-scope requests. Experiments reveal that rigid hierarchies can hinder performance without co-designed strategies, and even state-of-the-art agents exhibit systemic weaknesses in robustness. MSC-Bench provides a diagnostic framework to expose these limitations and guide the development of more capable and efficient tool-using agents. The benchmark and resources are publicly available at https://github.com/snooow1029/MSC_Bench.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeSyPr: Neurosymbolic Proceduralization For Efficient Embodied Reasoning</title>
<link>https://arxiv.org/abs/2510.19429</link>
<guid>https://arxiv.org/abs/2510.19429</guid>
<content:encoded><![CDATA[
arXiv:2510.19429v1 Announce Type: new 
Abstract: We address the challenge of adopting language models (LMs) for embodied tasks in dynamic environments, where online access to large-scale inference engines or symbolic planners is constrained due to latency, connectivity, and resource limitations. To this end, we present NeSyPr, a novel embodied reasoning framework that compiles knowledge via neurosymbolic proceduralization, thereby equipping LM-based agents with structured, adaptive, and timely reasoning capabilities. In NeSyPr, task-specific plans are first explicitly generated by a symbolic tool leveraging its declarative knowledge. These plans are then transformed into composable procedural representations that encode the plans' implicit production rules, enabling the resulting composed procedures to be seamlessly integrated into the LM's inference process. This neurosymbolic proceduralization abstracts and generalizes multi-step symbolic structured path-finding and reasoning into single-step LM inference, akin to human knowledge compilation. It supports efficient test-time inference without relying on external symbolic guidance, making it well suited for deployment in latency-sensitive and resource-constrained physical systems. We evaluate NeSyPr on the embodied benchmarks PDDLGym, VirtualHome, and ALFWorld, demonstrating its efficient reasoning capabilities over large-scale reasoning models and a symbolic planner, while using more compact LMs.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoMT: A Multi-Agent LLM Framework for Automated Metamorphic Testing of Autonomous Driving Systems</title>
<link>https://arxiv.org/abs/2510.19438</link>
<guid>https://arxiv.org/abs/2510.19438</guid>
<content:encoded><![CDATA[
arXiv:2510.19438v1 Announce Type: new 
Abstract: Autonomous Driving Systems (ADS) are safety-critical, where failures can be severe. While Metamorphic Testing (MT) is effective for fault detection in ADS, existing methods rely heavily on manual effort and lack automation. We present AutoMT, a multi-agent MT framework powered by Large Language Models (LLMs) that automates the extraction of Metamorphic Relations (MRs) from local traffic rules and the generation of valid follow-up test cases. AutoMT leverages LLMs to extract MRs from traffic rules in Gherkin syntax using a predefined ontology. A vision-language agent analyzes scenarios, and a search agent retrieves suitable MRs from a RAG-based database to generate follow-up cases via computer vision. Experiments show that AutoMT achieves up to 5 x higher test diversity in follow-up case generation compared to the best baseline (manual expert-defined MRs) in terms of validation rate, and detects up to 20.55% more behavioral violations. While manual MT relies on a fixed set of predefined rules, AutoMT automatically extracts diverse metamorphic relations that augment real-world datasets and help uncover corner cases often missed during in-field testing and data collection. Its modular architecture separating MR extraction, filtering, and test generation supports integration into industrial pipelines and potentially enables simulation-based testing to systematically cover underrepresented or safety-critical scenarios.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AegisMCP: Online Graph Intrusion Detection for Tool-Augmented LLMs on Edge Devices</title>
<link>https://arxiv.org/abs/2510.19462</link>
<guid>https://arxiv.org/abs/2510.19462</guid>
<content:encoded><![CDATA[
arXiv:2510.19462v1 Announce Type: new 
Abstract: In this work, we study security of Model Context Protocol (MCP) agent toolchains and their applications in smart homes. We introduce AegisMCP, a protocol-level intrusion detector. Our contributions are: (i) a minimal attack suite spanning instruction-driven escalation, chain-of-tool exfiltration, malicious MCP server registration, and persistence; (ii) NEBULA-Schema (Network-Edge Behavioral Learning for Untrusted LLM Agents), a reusable protocol-level instrumentation that represents MCP activity as a streaming heterogeneous temporal graph over agents, MCP servers, tools, devices, remotes, and sessions; and (iii) a CPU-only streaming detector that fuses novelty, session-DAG structure, and attribute cues for near-real-time edge inference, with optional fusion of local prompt-guardrail signals. On an emulated smart-home testbed spanning multiple MCP stacks and a physical bench, AegisMCP achieves sub-second per-window model inference and end-to-end alerting. The latency of AegisMCP is consistently sub-second on Intel N150-class edge hardware, while outperforming traffic-only and sequence baselines; ablations confirm the importance of DAG and install/permission signals. We release code, schemas, and generators for reproducible evaluation.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos</title>
<link>https://arxiv.org/abs/2510.19488</link>
<guid>https://arxiv.org/abs/2510.19488</guid>
<content:encoded><![CDATA[
arXiv:2510.19488v1 Announce Type: new 
Abstract: Training computer-use agents requires massive amounts of GUI interaction data, but manually annotating action trajectories at scale is prohibitively expensive. We present VideoAgentTrek, a scalable pipeline that automatically mines training data from publicly available screen-recorded videos at web scale, eliminating the need for manual annotation. Our approach addresses a key challenge: raw videos contain implicit demonstrations but lack explicit action labels. To solve this, we develop Video2Action, an inverse dynamics module (IDM) with two components: (1) a video grounding model that detects and localizes GUI actions with precise temporal boundaries and context, and (2) an action-content recognizer that extracts structured parameters like click coordinates and typed text with high fidelity. Applied to 39,000 YouTube tutorial videos, our pipeline generates 1.52 million interaction steps automatically. We leverage this data through continued pretraining followed by supervised fine-tuning. On OSWorld-Verified, our approach improves task success rates from 9.3% (SFT-only baseline) to 15.8%, a 70% relative improvement. On AgentNetBench, step accuracy increases from 64.1% to 69.3%. Our results demonstrate that passive internet videos can be transformed into high-quality supervision for computer-use agents, providing a scalable alternative to expensive manual annotation.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling realistic human behavior using generative agents in a multimodal transport system: Software architecture and Application to Toulouse</title>
<link>https://arxiv.org/abs/2510.19497</link>
<guid>https://arxiv.org/abs/2510.19497</guid>
<content:encoded><![CDATA[
arXiv:2510.19497v1 Announce Type: new 
Abstract: Modeling realistic human behaviour to understand people's mode choices in order to propose personalised mobility solutions remains challenging. This paper presents an architecture for modeling realistic human mobility behavior in complex multimodal transport systems, demonstrated through a case study in Toulouse, France. We apply Large Language Models (LLMs) within an agent-based simulation to capture decision-making in a real urban setting. The framework integrates the GAMA simulation platform with an LLM-based generative agent, along with General Transit Feed Specification (GTFS) data for public transport, and OpenTripPlanner for multimodal routing. GAMA platform models the interactive transport environment, providing visualization and dynamic agent interactions while eliminating the need to construct the simulation environment from scratch. This design enables a stronger focus on developing generative agents and evaluating their performance in transport decision-making processes. Over a simulated month, results show that agents not only make context-aware transport decisions but also form habits over time. We conclude that combining LLMs with agent-based simulation offers a promising direction for advancing intelligent transportation systems and personalised multimodal mobility solutions. We also discuss some limitations of this approach and outline future work on scaling to larger regions, integrating real-time data, and refining memory models.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAIL: Beyond Task Ambiguity for Language-Conditioned Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.19562</link>
<guid>https://arxiv.org/abs/2510.19562</guid>
<content:encoded><![CDATA[
arXiv:2510.19562v1 Announce Type: new 
Abstract: Comprehending natural language and following human instructions are critical capabilities for intelligent agents. However, the flexibility of linguistic instructions induces substantial ambiguity across language-conditioned tasks, severely degrading algorithmic performance. To address these limitations, we present a novel method named DAIL (Distributional Aligned Learning), featuring two key components: distributional policy and semantic alignment. Specifically, we provide theoretical results that the value distribution estimation mechanism enhances task differentiability. Meanwhile, the semantic alignment module captures the correspondence between trajectories and linguistic instructions. Extensive experimental results on both structured and visual observation benchmarks demonstrate that DAIL effectively resolves instruction ambiguities, achieving superior performance to baseline methods. Our implementation is available at https://github.com/RunpengXie/Distributional-Aligned-Learning.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Polynomial-time Configuration Generator for Connected Unlabeled Multi-Agent Pathfinding</title>
<link>https://arxiv.org/abs/2510.19567</link>
<guid>https://arxiv.org/abs/2510.19567</guid>
<content:encoded><![CDATA[
arXiv:2510.19567v1 Announce Type: new 
Abstract: We consider Connected Unlabeled Multi-Agent Pathfinding (CUMAPF), a variant of MAPF where the agents must maintain connectivity at all times. This problem is fundamental to swarm robotics applications like self-reconfiguration and marching, where standard MAPF is insufficient as it does not guarantee the required connectivity between agents. While unlabeled MAPF is tractable in optimization, CUMAPF is NP-hard even on highly restricted graph classes. To tackle this challenge, we propose PULL, a complete and polynomial-time algorithm with a simple design. It is based on a rule-based one-step function that computes a subsequent configuration that preserves connectivity and advances towards the target configuration. PULL is lightweight, and runs in $O(n^2)$ time per step in 2D grid, where $n$ is the number of agents. Our experiments further demonstrate its practical performance: PULL finds competitive solution qualities against trivial solutions for hundreds of agents, in randomly generated instances. Furthermore, we develop an eventually optimal solver that integrates PULL into an existing search-based MAPF algorithm, providing a valuable tool for small-scale instances.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>gem5 Co-Pilot: AI Assistant Agent for Architectural Design Space Exploration</title>
<link>https://arxiv.org/abs/2510.19577</link>
<guid>https://arxiv.org/abs/2510.19577</guid>
<content:encoded><![CDATA[
arXiv:2510.19577v1 Announce Type: new 
Abstract: Generative AI is increasing the productivity of software and hardware development across many application domains. In this work, we utilize the power of Large Language Models (LLMs) to develop a co-pilot agent for assisting gem5 users with automating design space exploration. Computer architecture design space exploration is complex and time-consuming, given that numerous parameter settings and simulation statistics must be analyzed before improving the current design. The emergence of LLMs has significantly accelerated the analysis of long-text data as well as smart decision making, two key functions in a successful design space exploration task. In this project, we first build gem5 Co-Pilot, an AI agent assistant for gem5, which comes with a webpage-GUI for smooth user interaction, agent automation, and result summarization. We also implemented a language for design space exploration, as well as a Design Space Database (DSDB). With DSDB, gem5 Co-Pilot effectively implements a Retrieval Augmented Generation system for gem5 design space exploration. We experiment on cost-constraint optimization with four cost ranges and compare our results with two baseline models. Results show that gem5 Co-Pilot can quickly identify optimal parameters for specific design constraints based on performance and cost, with limited user interaction.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1</title>
<link>https://arxiv.org/abs/2510.19600</link>
<guid>https://arxiv.org/abs/2510.19600</guid>
<content:encoded><![CDATA[
arXiv:2510.19600v1 Announce Type: new 
Abstract: In the quest for scientific progress, communicating research is as vital as the discovery itself. Yet, researchers are often sidetracked by the manual, repetitive chore of building project webpages to make their dense papers accessible. While automation has tackled static slides and posters, the dynamic, interactive nature of webpages has remained an unaddressed challenge. To bridge this gap, we reframe the problem, arguing that the solution lies not in a single command, but in a collaborative, hierarchical process. We introduce $\textbf{AutoPage}$, a novel multi-agent system that embodies this philosophy. AutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline from narrative planning to multimodal content generation and interactive rendering. To combat AI hallucination, dedicated "Checker" agents verify each step against the source paper, while optional human checkpoints ensure the final product aligns perfectly with the author's vision, transforming the system from a mere tool into a powerful collaborative assistant. To rigorously validate our approach, we also construct $\textbf{PageBench}$, the first benchmark for this new task. Experiments show AutoPage not only generates high-quality, visually appealing pages but does so with remarkable efficiency in under 15 minutes for less than \$0.1. Code and dataset will be released at $\href{https://mqleet.github.io/AutoPage_ProjectPage/}{Webpage}$.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pragmatic Heterogeneous Collaborative Perception via Generative Communication Mechanism</title>
<link>https://arxiv.org/abs/2510.19618</link>
<guid>https://arxiv.org/abs/2510.19618</guid>
<content:encoded><![CDATA[
arXiv:2510.19618v1 Announce Type: new 
Abstract: Multi-agent collaboration enhances the perception capabilities of individual agents through information sharing. However, in real-world applications, differences in sensors and models across heterogeneous agents inevitably lead to domain gaps during collaboration. Existing approaches based on adaptation and reconstruction fail to support pragmatic heterogeneous collaboration due to two key limitations: (1) Intrusive retraining of the encoder or core modules disrupts the established semantic consistency among agents; and (2) accommodating new agents incurs high computational costs, limiting scalability. To address these challenges, we present a novel Generative Communication mechanism (GenComm) that facilitates seamless perception across heterogeneous multi-agent systems through feature generation, without altering the original network, and employs lightweight numerical alignment of spatial information to efficiently integrate new agents at minimal cost. Specifically, a tailored Deformable Message Extractor is designed to extract spatial message for each collaborator, which is then transmitted in place of intermediate features. The Spatial-Aware Feature Generator, utilizing a conditional diffusion model, generates features aligned with the ego agent's semantic space while preserving the spatial information of the collaborators. These generated features are further refined by a Channel Enhancer before fusion. Experiments conducted on the OPV2V-H, DAIR-V2X and V2X-Real datasets demonstrate that GenComm outperforms existing state-of-the-art methods, achieving an 81\% reduction in both computational cost and parameter count when incorporating new agents. Our code is available at https://github.com/jeffreychou777/GenComm.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search Agents in Hierarchical Rule Application</title>
<link>https://arxiv.org/abs/2510.19631</link>
<guid>https://arxiv.org/abs/2510.19631</guid>
<content:encoded><![CDATA[
arXiv:2510.19631v1 Announce Type: new 
Abstract: Effective deep search agents must not only access open-domain and domain-specific knowledge but also apply complex rules-such as legal clauses, medical manuals and tariff rules. These rules often feature vague boundaries and implicit logic relationships, making precise application challenging for agents. However, this critical capability is largely overlooked by current agent benchmarks.
  To fill this gap, we introduce HSCodeComp, the first realistic, expert-level e-commerce benchmark designed to evaluate deep search agents in hierarchical rule application. In this task, the deep reasoning process of agents is guided by these rules to predict 10-digit Harmonized System Code (HSCode) of products with noisy but realistic descriptions. These codes, established by the World Customs Organization, are vital for global supply chain efficiency. Built from real-world data collected from large-scale e-commerce platforms, our proposed HSCodeComp comprises 632 product entries spanning diverse product categories, with these HSCodes annotated by several human experts.
  Extensive experimental results on several state-of-the-art LLMs, open-source, and closed-source agents reveal a huge performance gap: best agent achieves only 46.8% 10-digit accuracy, far below human experts at 95.0%. Besides, detailed analysis demonstrates the challenges of hierarchical rule application, and test-time scaling fails to improve performance further.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments</title>
<link>https://arxiv.org/abs/2510.19655</link>
<guid>https://arxiv.org/abs/2510.19655</guid>
<content:encoded><![CDATA[
arXiv:2510.19655v1 Announce Type: new 
Abstract: Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires an agent to navigate unseen environments based on natural language instructions without any prior training. Current methods face a critical trade-off: either rely on environment-specific waypoint predictors that limit scene generalization, or underutilize the reasoning capabilities of large models during navigation. We introduce LaViRA, a simple yet effective zero-shot framework that addresses this dilemma by decomposing action into a coarse-to-fine hierarchy: Language Action for high-level planning, Vision Action for perceptual grounding, and Robot Action for robust navigation. This modular decomposition allows us to leverage the distinct strengths of different scales of Multimodal Large Language Models (MLLMs) at each stage, creating a system that is powerful in its reasoning, grounding and practical control. LaViRA significantly outperforms existing state-of-the-art methods on the VLN-CE benchmark, demonstrating superior generalization capabilities in unseen environments, while maintaining transparency and efficiency for real-world deployment.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentSense: LLMs Empower Generalizable and Explainable Web-Based Participatory Urban Sensing</title>
<link>https://arxiv.org/abs/2510.19661</link>
<guid>https://arxiv.org/abs/2510.19661</guid>
<content:encoded><![CDATA[
arXiv:2510.19661v1 Announce Type: new 
Abstract: Web-based participatory urban sensing has emerged as a vital approach for modern urban management by leveraging mobile individuals as distributed sensors. However, existing urban sensing systems struggle with limited generalization across diverse urban scenarios and poor interpretability in decision-making. In this work, we introduce AgentSense, a hybrid, training-free framework that integrates large language models (LLMs) into participatory urban sensing through a multi-agent evolution system. AgentSense initially employs classical planner to generate baseline solutions and then iteratively refines them to adapt sensing task assignments to dynamic urban conditions and heterogeneous worker preferences, while producing natural language explanations that enhance transparency and trust. Extensive experiments across two large-scale mobility datasets and seven types of dynamic disturbances demonstrate that AgentSense offers distinct advantages in adaptivity and explainability over traditional methods. Furthermore, compared to single-agent LLM baselines, our approach outperforms in both performance and robustness, while delivering more reasonable and transparent explanations. These results position AgentSense as a significant advancement towards deploying adaptive and explainable urban sensing systems on the web.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Large Language Models Sensitive to the Motives Behind Communication?</title>
<link>https://arxiv.org/abs/2510.19687</link>
<guid>https://arxiv.org/abs/2510.19687</guid>
<content:encoded><![CDATA[
arXiv:2510.19687v1 Announce Type: new 
Abstract: Human communication is motivated: people speak, write, and create content with a particular communicative intent in mind. As a result, information that large language models (LLMs) and AI agents process is inherently framed by humans' intentions and incentives. People are adept at navigating such nuanced information: we routinely identify benevolent or self-serving motives in order to decide what statements to trust. For LLMs to be effective in the real world, they too must critically evaluate content by factoring in the motivations of the source -- for instance, weighing the credibility of claims made in a sales pitch. In this paper, we undertake a comprehensive study of whether LLMs have this capacity for motivational vigilance. We first employ controlled experiments from cognitive science to verify that LLMs' behavior is consistent with rational models of learning from motivated testimony, and find they successfully discount information from biased sources in a human-like manner. We then extend our evaluation to sponsored online adverts, a more naturalistic reflection of LLM agents' information ecosystems. In these settings, we find that LLMs' inferences do not track the rational models' predictions nearly as closely -- partly due to additional information that distracts them from vigilance-relevant considerations. However, a simple steering intervention that boosts the salience of intentions and incentives substantially increases the correspondence between LLMs and the rational model. These results suggest that LLMs possess a basic sensitivity to the motivations of others, but generalizing to novel real-world settings will require further improvements to these models.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Agentic Software Engineering Beyond Code: Framing Vision, Values, and Vocabulary</title>
<link>https://arxiv.org/abs/2510.19692</link>
<guid>https://arxiv.org/abs/2510.19692</guid>
<content:encoded><![CDATA[
arXiv:2510.19692v1 Announce Type: new 
Abstract: Agentic AI is poised to usher in a seismic paradigm shift in Software Engineering (SE). As technologists rush head-along to make agentic AI a reality, SE researchers are driven to establish agentic SE as a research area. While early visions of agentic SE are primarily focused on code-related activities, early empirical evidence calls for a consideration of a range of socio-technical concerns to make it work in practice. This paper contributes to the emerging community vision by: (a) recommending an expansion of its scope beyond code, toward a 'whole of process' vision, grounding it in SE foundations and evolution and emerging agentic SE frameworks, (b) proposing a preliminary set of values and principles to guide efforts, and (c) sharing guidance on designing/using well-defined vocabulary for agentic SE. It is hoped that these ideas will encourage community collaborations and steer the SE community towards laying strong foundations of agentic SE so its not only inevitable but also deliberate and desirable in the long run.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.19732</link>
<guid>https://arxiv.org/abs/2510.19732</guid>
<content:encoded><![CDATA[
arXiv:2510.19732v1 Announce Type: new 
Abstract: To enable embodied agents to operate effectively over extended timeframes, it is crucial to develop models that form and access memories to stay contextualized in their environment. In the current paradigm of training transformer-based policies for embodied sequential decision-making tasks, visual inputs often overwhelm the context limits of transformers, while humans can maintain and utilize a lifetime of experience compressed as memories. Significant compression is possible in principle, as much of the input is irrelevant and can be abstracted. However, existing approaches predominantly focus on either recurrent models with fixed-size memory or transformers with full-context reliance. In this work, we propose Memo, a transformer-based architecture and training recipe for reinforcement learning (RL) on memory-intensive, long-horizon tasks. Memo incorporates the creation and retrieval of memory by interleaving periodic summarization tokens with the inputs of a model during training. We demonstrate Memo's effectiveness on a gridworld meta-RL benchmark and a multi-object navigation task in photo-realistic indoor settings. Memo outperforms naive long-context transformer baselines while being more compute and storage efficient. Additionally, Memo generalizes better to longer contexts at inference time and remains robust in streaming settings, where historical context must be truncated to fit inference constraints.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Misalignment Bounty: Crowdsourcing AI Agent Misbehavior</title>
<link>https://arxiv.org/abs/2510.19738</link>
<guid>https://arxiv.org/abs/2510.19738</guid>
<content:encoded><![CDATA[
arXiv:2510.19738v1 Announce Type: new 
Abstract: Advanced AI systems sometimes act in ways that differ from human intent. To gather clear, reproducible examples, we ran the Misalignment Bounty: a crowdsourced project that collected cases of agents pursuing unintended or unsafe goals. The bounty received 295 submissions, of which nine were awarded.
  This report explains the program's motivation and evaluation criteria, and walks through the nine winning submissions step by step.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Review of Tools for Zero-Code LLM Based Application Development</title>
<link>https://arxiv.org/abs/2510.19747</link>
<guid>https://arxiv.org/abs/2510.19747</guid>
<content:encoded><![CDATA[
arXiv:2510.19747v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are transforming software creation by enabling zero code development platforms. Our survey reviews recent platforms that let users build applications without writing code, by leveraging LLMs as the brains of the development process. We adopt a broad survey methodology, categorizing platforms based on key dimensions such as interface style, backend integration, output type, and extensibility. We analyze both dedicated LLM based app builders (OpenAI's custom GPTs, Bolt.new, Dust.tt, Flowise, Cognosys) and general no code platforms (e.g., Bubble, Glide) that integrate LLM capabilities. We present a taxonomy categorizing these platforms by their interface (conversational, visual, etc.), supported LLM backends, output type (chatbot, full application, workflow), and degree of extensibility. Core features such as autonomous agents, memory management, workflow orchestration, and API integrations are in scope of the survey. We provide a detailed comparison, highlighting each platform's strengths and limitations. Trade offs (customizability, scalability, vendor lock-in) are discussed in comparison with traditional and low code development approaches. Finally, we outline future directions, including multimodal interfaces, on device LLMs, and improved orchestration for democratizing app creation with AI. Our findings indicate that while zero code LLM platforms greatly reduce the barrier to creating AI powered applications, they still face challenges in flexibility and reliability. Overall, the landscape is rapidly evolving, offering exciting opportunities to empower non programmers to create sophisticated software.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEA: Semantic Map Prediction for Active Exploration of Uncertain Areas</title>
<link>https://arxiv.org/abs/2510.19766</link>
<guid>https://arxiv.org/abs/2510.19766</guid>
<content:encoded><![CDATA[
arXiv:2510.19766v1 Announce Type: new 
Abstract: In this paper, we propose SEA, a novel approach for active robot exploration through semantic map prediction and a reinforcement learning-based hierarchical exploration policy. Unlike existing learning-based methods that rely on one-step waypoint prediction, our approach enhances the agent's long-term environmental understanding to facilitate more efficient exploration. We propose an iterative prediction-exploration framework that explicitly predicts the missing areas of the map based on current observations. The difference between the actual accumulated map and the predicted global map is then used to guide exploration. Additionally, we design a novel reward mechanism that leverages reinforcement learning to update the long-term exploration strategies, enabling us to construct an accurate semantic map within limited steps. Experimental results demonstrate that our method significantly outperforms state-of-the-art exploration strategies, achieving superior coverage ares of the global map within the same time constraints.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents</title>
<link>https://arxiv.org/abs/2510.19771</link>
<guid>https://arxiv.org/abs/2510.19771</guid>
<content:encoded><![CDATA[
arXiv:2510.19771v1 Announce Type: new 
Abstract: LLM-based agents are increasingly moving towards proactivity: rather than awaiting instruction, they exercise agency to anticipate user needs and solve them autonomously. However, evaluating proactivity is challenging; current benchmarks are constrained to localized context, limiting their ability to test reasoning across sources and longer time horizons. To address this gap, we present PROBE (Proactive Resolution Of BottlEnecks). PROBE decomposes proactivity as a pipeline of three core capabilities: (1) searching for unspecified issues, (2) identifying specific bottlenecks, and (3) executing appropriate resolutions. We apply PROBE to evaluate leading LLMs and popular agentic frameworks, showing that even state-of-the-art models struggle to solve this benchmark. Computing our consistent measurements across frontier LLMs and agents, we find that the best end-to-end performance of 40% is achieved by both GPT-5 and Claude Opus-4.1. Additionally, we demonstrate the relative capabilities of each model and analyze mutual failure modes. Our results highlight the current limitations of autonomous action in agentic systems, and expose promising future research directions.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking World-Model Learning</title>
<link>https://arxiv.org/abs/2510.19788</link>
<guid>https://arxiv.org/abs/2510.19788</guid>
<content:encoded><![CDATA[
arXiv:2510.19788v1 Announce Type: new 
Abstract: Model-learning agents should gather information to learn world models that support many downstream tasks and inferences, such as predicting unobserved states, estimating near- and far-term consequences of actions, planning action sequences, and detecting changes in dynamics. Current methods for learning and evaluating world models diverge from this goal: training and evaluation are anchored to next-frame prediction, and success is scored by reward maximization in the same environment. We propose WorldTest, a protocol to evaluate model-learning agents that separates reward-free interaction from a scored test phase in a different but related environment. WorldTest is open-ended$\unicode{x2014}$models should support many different tasks unknown ahead of time$\unicode{x2014}$and agnostic to model representation, allowing comparison across approaches. We instantiated WorldTest with AutumnBench, a suite of 43 interactive grid-world environments and 129 tasks across three families: masked-frame prediction, planning, and predicting changes to the causal dynamics. We compared 517 human participants and three frontier models on AutumnBench. We found that humans outperform the models, and scaling compute improves performance only in some environments but not others. WorldTest provides a novel template$\unicode{x2014}$reward-free exploration, derived tests, and behavior-based scoring$\unicode{x2014}$to evaluate what agents learn about environment dynamics, and AutumnBench exposes significant headroom in world-model learning.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SysMoBench: Evaluating AI on Formally Modeling Complex Real-World Systems</title>
<link>https://arxiv.org/abs/2509.23130</link>
<guid>https://arxiv.org/abs/2509.23130</guid>
<content:encoded><![CDATA[
arXiv:2509.23130v2 Announce Type: cross 
Abstract: Formal models are essential to specifying large, complex computer systems and verifying their correctness, but are notoriously expensive to write and maintain. Recent advances in generative AI show promise in generating certain forms of specifications. However, existing work mostly targets small code, not complete systems. It is unclear whether AI can deal with realistic system artifacts, as this requires abstracting their complex behavioral properties into formal models. We present SysMoBench, a benchmark that evaluates AI's ability to formally model large, complex systems. We focus on concurrent and distributed systems, which are keystones of today's critical computing infrastructures, encompassing operating systems and cloud infrastructure. We use TLA+, the de facto specification language for concurrent and distributed systems, though the benchmark can be extended to other specification languages. We address the primary challenge of evaluating AI-generated models by automating metrics like syntactic and runtime correctness, conformance to system code, and invariant correctness. SysMoBench currently includes nine diverse system artifacts: the Raft implementation of Etcd and Redis, the Spinlock and Mutex in Asterinas OS, etc.; more artifacts are being actively added. SysMoBench enables us to understand the capabilities and limitations of today's LLMs and agents, putting tools in this area on a firm footing and opening up promising new research directions.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergence of Internal State-Modulated Swarming in Multi-Agent Patch Foraging System</title>
<link>https://arxiv.org/abs/2510.18886</link>
<guid>https://arxiv.org/abs/2510.18886</guid>
<content:encoded><![CDATA[
arXiv:2510.18886v1 Announce Type: cross 
Abstract: Active particles are entities that sustain persistent out-of-equilibrium motion by consuming energy. Under certain conditions, they exhibit the tendency to self-organize through coordinated movements, such as swarming via aggregation. While performing non-cooperative foraging tasks, the emergence of such swarming behavior in foragers, exemplifying active particles, has been attributed to the partial observability of the environment, in which the presence of another forager can serve as a proxy signal to indicate the potential presence of a food source or a resource patch. In this paper, we validate this phenomenon by simulating multiple self-propelled foragers as they forage from multiple resource patches in a non-cooperative manner. These foragers operate in a continuous two-dimensional space with stochastic position updates and partial observability. We evolve a shared policy in the form of a continuous-time recurrent neural network that serves as a velocity controller for the foragers. To this end, we use an evolutionary strategy algorithm wherein the different samples of the policy-distribution are evaluated in the same rollout. Then we show that agents are able to learn to adaptively forage in the environment. Next, we show the emergence of swarming in the form of aggregation among the foragers when resource patches are absent. We observe that the strength of this swarming behavior appears to be inversely proportional to the amount of resource stored in the foragers, which supports the risk-sensitive foraging claims. Empirical analysis of the learned controller's hidden states in minimal test runs uncovers their sensitivity to the amount of resource stored in a forager. Clamping these hidden states to represent a lesser amount of resource hastens its learned aggregation behavior.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prospects for Using Artificial Intelligence to Understand Intrinsic Kinetics of Heterogeneous Catalytic Reactions</title>
<link>https://arxiv.org/abs/2510.18911</link>
<guid>https://arxiv.org/abs/2510.18911</guid>
<content:encoded><![CDATA[
arXiv:2510.18911v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) is influencing heterogeneous catalysis research by accelerating simulations and materials discovery. A key frontier is integrating AI with multiscale models and multimodal experiments to address the "many-to-one" challenge of linking intrinsic kinetics to observables. Advances in machine-learned force fields, microkinetics, and reactor modeling enable rapid exploration of chemical spaces, while operando and transient data provide unprecedented insight. Yet, inconsistent data quality and model complexity limit mechanistic discovery. Generative and agentic AI can automate model generation, quantify uncertainty, and couple theory with experiment, realizing "self-driving models" that produce interpretable, reproducible, and transferable understanding of catalytic systems.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the hardness of RL with Lookahead</title>
<link>https://arxiv.org/abs/2510.19372</link>
<guid>https://arxiv.org/abs/2510.19372</guid>
<content:encoded><![CDATA[
arXiv:2510.19372v1 Announce Type: cross 
Abstract: We study reinforcement learning (RL) with transition look-ahead, where the agent may observe which states would be visited upon playing any sequence of $\ell$ actions before deciding its course of action. While such predictive information can drastically improve the achievable performance, we show that using this information optimally comes at a potentially prohibitive computational cost. Specifically, we prove that optimal planning with one-step look-ahead ($\ell=1$) can be solved in polynomial time through a novel linear programming formulation. In contrast, for $\ell \geq 2$, the problem becomes NP-hard. Our results delineate a precise boundary between tractable and intractable cases for the problem of planning with transition look-ahead in reinforcement learning.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FP-IRL: Fokker-Planck Inverse Reinforcement Learning -- A Physics-Constrained Approach to Markov Decision Processes</title>
<link>https://arxiv.org/abs/2306.10407</link>
<guid>https://arxiv.org/abs/2306.10407</guid>
<content:encoded><![CDATA[
arXiv:2306.10407v2 Announce Type: replace 
Abstract: Inverse reinforcement learning (IRL) is a powerful paradigm for uncovering the incentive structure that drives agent behavior, by inferring an unknown reward function from observed trajectories within a Markov decision process (MDP). However, most existing IRL methods require access to the transition function, either prescribed or estimated \textit{a priori}, which poses significant challenges when the underlying dynamics are unknown, unobservable, or not easily sampled.
  We propose Fokker--Planck inverse reinforcement learning (FP-IRL), a novel physics-constrained IRL framework tailored for systems governed by Fokker--Planck (FP) dynamics. FP-IRL simultaneously infers both the reward and transition functions directly from trajectory data, without requiring access to sampled transitions. Our method leverages a conjectured equivalence between MDPs and the FP equation, linking reward maximization in MDPs with free energy minimization in FP dynamics. This connection enables inference of the potential function using our inference approach of variational system identification, from which the full set of MDP components -- reward, transition, and policy -- can be recovered using analytic expressions.
  We demonstrate the effectiveness of FP-IRL through experiments on synthetic benchmarks and a modified version of the Mountain Car problem. Our results show that FP-IRL achieves accurate recovery of agent incentives while preserving computational efficiency and physical interpretability.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Popular Matchings</title>
<link>https://arxiv.org/abs/2401.12653</link>
<guid>https://arxiv.org/abs/2401.12653</guid>
<content:encoded><![CDATA[
arXiv:2401.12653v2 Announce Type: replace 
Abstract: We study popularity for matchings under preferences. This solution concept captures matchings that do not lose against any other matching in a majority vote by the agents. A popular matching is said to be robust if it is popular among multiple instances. We present a polynomial-time algorithm for deciding whether there exists a robust popular matching if instances only differ with respect to the preferences of a single agent. The same method applies also to dominant matchings, a subclass of maximum-size popular matchings. By contrast, we obtain NP-completeness if two instances differ only by two agents of the same side or by a swap of two adjacent alternatives by two agents. The first hardness result applies to dominant matchings as well. Moreover, we find another complexity dichotomy based on preference completeness for the case where instances differ by making some options unavailable. We conclude by discussing related models, such as strong and mixed popularity.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vahana.jl -- A framework (not only) for large-scale agent-based models</title>
<link>https://arxiv.org/abs/2406.14441</link>
<guid>https://arxiv.org/abs/2406.14441</guid>
<content:encoded><![CDATA[
arXiv:2406.14441v2 Announce Type: replace 
Abstract: Agent-based models (ABMs) offer a powerful framework for understanding complex systems. However, their computational demands often become a significant barrier as the number of agents and complexity of the simulation increase. Traditional ABM platforms often struggle to fully exploit modern computing resources, hindering the development of large-scale simulations. This paper presents Vahana.jl, a high performance computing open source framework that aims to address these limitations. Building on the formalism of synchronous graph dynamical systems, Vahana.jl is especially well suited for models with a focus on (social) networks. The framework seamlessly supports distribution across multiple compute nodes, enabling simulations that would otherwise be beyond the capabilities of a single machine. Implemented in Julia, Vahana.jl leverages the interactive Read-Eval-Print Loop (REPL) environment, facilitating rapid model development and experimentation.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PARCO: Parallel AutoRegressive Models for Multi-Agent Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2409.03811</link>
<guid>https://arxiv.org/abs/2409.03811</guid>
<content:encoded><![CDATA[
arXiv:2409.03811v3 Announce Type: replace 
Abstract: Combinatorial optimization problems involving multiple agents are notoriously challenging due to their NP-hard nature and the necessity for effective agent coordination. Despite advancements in learning-based methods, existing approaches often face critical limitations, including suboptimal agent coordination, poor generalization, and high computational latency. To address these issues, we propose PARCO (Parallel AutoRegressive Combinatorial Optimization), a general reinforcement learning framework designed to construct high-quality solutions for multi-agent combinatorial tasks efficiently. To this end, PARCO integrates three key novel components: (1) transformer-based communication layers to enable effective agent collaboration during parallel solution construction, (2) a multiple pointer mechanism for low-latency, parallel agent decision-making, and (3) priority-based conflict handlers to resolve decision conflicts via learned priorities. We evaluate PARCO in multi-agent vehicle routing and scheduling problems, where our approach outperforms state-of-the-art learning methods, demonstrating strong generalization ability and remarkable computational efficiency. We make our source code publicly available to foster future research: https://github.com/ai4co/parco.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stop Playing the Guessing Game! Target-free User Simulation for Evaluating Conversational Recommender Systems</title>
<link>https://arxiv.org/abs/2411.16160</link>
<guid>https://arxiv.org/abs/2411.16160</guid>
<content:encoded><![CDATA[
arXiv:2411.16160v4 Announce Type: replace 
Abstract: Recent approaches in Conversational Recommender Systems (CRSs) have tried to simulate real-world users engaging in conversations with CRSs to create more realistic testing environments that reflect the complexity of human-agent dialogue. Despite the significant advancements, reliably evaluating the capability of CRSs to elicit user preferences still faces a significant challenge. Existing evaluation metrics often rely on target-biased user simulators that assume users have predefined preferences, leading to interactions that devolve into simplistic guessing game. These simulators typically guide the CRS toward specific target items based on fixed attributes, limiting the dynamic exploration of user preferences and struggling to capture the evolving nature of real-user interactions. Additionally, current evaluation metrics are predominantly focused on single-turn recall of target items, neglecting the intermediate processes of preference elicitation. To address this, we introduce PEPPER, a novel CRS evaluation protocol with target-free user simulators constructed from real-user interaction histories and reviews. PEPPER enables realistic user-CRS dialogues without falling into simplistic guessing games, allowing users to gradually discover their preferences through enriched interactions, thereby providing a more accurate and reliable assessment of the CRS's ability to elicit personal preferences. Furthermore, PEPPER presents detailed measures for comprehensively evaluating the preference elicitation capabilities of CRSs, encompassing both quantitative and qualitative measures that capture four distinct aspects of the preference elicitation process. Through extensive experiments, we demonstrate the validity of PEPPER as a simulation environment and conduct a thorough analysis of how effectively existing CRSs perform in preference elicitation and recommendation.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-World Drone Active Tracking with Goal-Centered Rewards</title>
<link>https://arxiv.org/abs/2412.00744</link>
<guid>https://arxiv.org/abs/2412.00744</guid>
<content:encoded><![CDATA[
arXiv:2412.00744v2 Announce Type: replace 
Abstract: Drone Visual Active Tracking aims to autonomously follow a target object by controlling the motion system based on visual observations, providing a more practical solution for effective tracking in dynamic environments. However, accurate Drone Visual Active Tracking using reinforcement learning remains challenging due to the absence of a unified benchmark and the complexity of open-world environments with frequent interference. To address these issues, we pioneer a systematic solution. First, we propose DAT, the first open-world drone active air-to-ground tracking benchmark. It encompasses 24 city-scale scenes, featuring targets with human-like behaviors and high-fidelity dynamics simulation. DAT also provides a digital twin tool for unlimited scene generation. Additionally, we propose a novel reinforcement learning method called GC-VAT, which aims to improve the performance of drone tracking targets in complex scenarios. Specifically, we design a Goal-Centered Reward to provide precise feedback across viewpoints to the agent, enabling it to expand perception and movement range through unrestricted perspectives. Inspired by curriculum learning, we introduce a Curriculum-Based Training strategy that progressively enhances the tracking performance in complex environments. Besides, experiments on simulator and real-world images demonstrate the superior performance of GC-VAT, achieving a Tracking Success Rate of approximately 72% on the simulator. The benchmark and code are available at https://github.com/SHWplus/DAT_Benchmark.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PixelWorld: How Far Are We from Perceiving Everything as Pixels?</title>
<link>https://arxiv.org/abs/2501.19339</link>
<guid>https://arxiv.org/abs/2501.19339</guid>
<content:encoded><![CDATA[
arXiv:2501.19339v3 Announce Type: replace 
Abstract: Recent agentic language models increasingly need to interact with real-world environments that contain tightly intertwined visual and textual information, often through raw camera pixels rather than separately processed images and tokenized text. This shift highlights the need for a unified perception paradigm. To investigate this idea, we explore Perceive Everything as Pixels (PEAP) and introduce PixelWorld, a benchmark that renders natural-language, tabular, mathematical, and diagrammatic inputs into a shared pixel space. Experiments across multiple benchmarks show that PEAP achieves comparable performance to token-based approaches on semantic understanding tasks, suggesting that vision transformers can partially capture global textual semantics without explicit tokenization. In contrast, reasoning-intensive tasks such as mathematics and code show notable performance degradation, although Chain-of-Thought prompting helps mitigate this gap by compensating for missing symbolic structure. We further find that when visual and textual information are closely integrated, representing everything as pixels simplifies preprocessing and avoids cross-modal misalignment. PixelWorld thus provides a systematic and practical framework for evaluating unified vision--language models and facilitates further exploration of pixel-based multimodal learning.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AttentionSwarm: Reinforcement Learning with Attention Control Barier Function for Crazyflie Drones in Dynamic Environments</title>
<link>https://arxiv.org/abs/2503.07376</link>
<guid>https://arxiv.org/abs/2503.07376</guid>
<content:encoded><![CDATA[
arXiv:2503.07376v2 Announce Type: replace 
Abstract: We introduce AttentionSwarm, a novel benchmark designed to evaluate safe and efficient swarm control in a dynamic drone racing scenario. Central to our approach is the Attention Model-Based Control Barrier Function (CBF) framework, which integrates attention mechanisms with safety-critical control theory to enable real-time collision avoidance and trajectory optimization. This framework dynamically prioritizes critical obstacles and agents in the swarm's vicinity using attention weights, while CBFs formally guarantee safety by enforcing collision-free constraints. The AttentionSwarm algorithm was developed and evaluated using a swarm of Crazyflie 2.1 micro quadrotors, which were tested indoors with the Vicon motion capture system to ensure precise localization and control. Experimental results show that our system achieves a 95-100% collision-free navigation rate in a dynamic multi-agent drone racing environment, underscoring its effectiveness and robustness in real-world scenarios. This work offers a promising foundation for safe, high-speed multi-robot applications in logistics, inspection, and racing.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PTFA: An LLM-based Agent that Facilitates Online Consensus Building through Parallel Thinking</title>
<link>https://arxiv.org/abs/2503.12499</link>
<guid>https://arxiv.org/abs/2503.12499</guid>
<content:encoded><![CDATA[
arXiv:2503.12499v2 Announce Type: replace 
Abstract: Consensus building is inherently challenging due to the diverse opinions held by stakeholders. Effective facilitation is crucial to support the consensus building process and enable efficient group decision making. However, the effectiveness of facilitation is often constrained by human factors such as limited experience and scalability. In this research, we propose a Parallel Thinking-based Facilitation Agent (PTFA) that facilitates online, text-based consensus building processes.The PTFA automatically collects real-time textual input and leverages large language models (LLMs)to perform all six distinct roles of the well-established Six Thinking Hats technique in parallel thinking.To illustrate the potential of the agent, a pilot study was conducted, demonstrating its capabilities in idea generation, emotional probing, and deeper analysis of idea quality. Additionally, future open research challenges such as optimizing scheduling and managing behaviors in divergent phase are identified. Furthermore, a comprehensive dataset that contains not only the conversational content among the participants but also between the participants and the agent is constructed for future study.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoBenchX: Benchmarking LLMs in Agent Solving Multistep Geospatial Tasks</title>
<link>https://arxiv.org/abs/2503.18129</link>
<guid>https://arxiv.org/abs/2503.18129</guid>
<content:encoded><![CDATA[
arXiv:2503.18129v2 Announce Type: replace 
Abstract: This paper establishes a benchmark for evaluating tool-calling capabilities of large language models (LLMs) on multi-step geospatial tasks relevant to commercial GIS practitioners. We assess eight commercial LLMs (Claude Sonnet 3.5 and 4, Claude Haiku 3.5, Gemini 2.0 Flash, Gemini 2.5 Pro Preview, GPT-4o, GPT-4.1 and o4-mini) using a simple tool-calling agent equipped with 23 geospatial functions. Our benchmark comprises tasks in four categories of increasing complexity, with both solvable and intentionally unsolvable tasks to test rejection accuracy. We develop a LLM-as-Judge evaluation framework to compare agent solutions against reference solutions. Results show o4-mini and Claude 3.5 Sonnet achieve the best overall performance, OpenAI's GPT-4.1, GPT-4o and Google's Gemini 2.5 Pro Preview do not fall far behind, but the last two are more efficient in identifying unsolvable tasks. Claude Sonnet 4, due its preference to provide any solution rather than reject a task, proved to be less accurate. We observe significant differences in token usage, with Anthropic models consuming more tokens than competitors. Common errors include misunderstanding geometrical relationships, relying on outdated knowledge, and inefficient data manipulation. The resulting benchmark set, evaluation framework, and data generation pipeline are released as open-source resources (available at https://github.com/Solirinai/GeoBenchX), providing one more standardized method for the ongoing evaluation of LLMs for GeoAI.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NAACL2025 Tutorial: Adaptation of Large Language Models</title>
<link>https://arxiv.org/abs/2504.03931</link>
<guid>https://arxiv.org/abs/2504.03931</guid>
<content:encoded><![CDATA[
arXiv:2504.03931v3 Announce Type: replace 
Abstract: This tutorial on adaptation of LLMs is designed to address the growing demand for models that go beyond the static capabilities of generic LLMs by providing an overview of dynamic, domain-specific, and task-adaptive LLM adaptation techniques. While general LLMs have demonstrated strong generalization across a variety of tasks, they often struggle to perform well in specialized domains such as finance, healthcare, and code generation for underrepresented languages. Additionally, their static nature limits their ability to evolve with the changing world, and they are often extremely large in size, making them impractical and costly to deploy at scale. As a result, the adaptation of LLMs has drawn much attention since the birth of LLMs and is of core importance, both for industry, which focuses on serving its targeted users, and academia, which can greatly benefit from small but powerful LLMs. To address this gap, this tutorial aims to provide an overview of the LLM adaptation techniques. We start with an introduction to LLM adaptation, from both the data perspective and the model perspective. We then emphasize how the evaluation metrics and benchmarks are different from other techniques. After establishing the problems, we explore various adaptation techniques. We categorize adaptation techniques into two main families. The first is parametric knowledge adaptation, which focuses on updating the parametric knowledge within LLMs. Additionally, we will discuss real-time adaptation techniques, including model editing, which allows LLMs to be updated dynamically in production environments. The second kind of adaptation is semi-parametric knowledge adaptation, where the goal is to update LLM parameters to better leverage external knowledge or tools through techniques like retrieval-augmented generation (RAG) and agent-based systems.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Meta-rotation Poset for Student-Project Allocation</title>
<link>https://arxiv.org/abs/2505.13428</link>
<guid>https://arxiv.org/abs/2505.13428</guid>
<content:encoded><![CDATA[
arXiv:2505.13428v3 Announce Type: replace 
Abstract: We study the Student Project Allocation problem with lecturer preferences over Students (SPA-S), an extension of the well-known Stable Marriage and Hospital Residents problem. In this model, students have preferences over projects, each project is offered by a single lecturer, and lecturers have preferences over students. The goal is to compute a stable matching which is an assignment of students to projects (and thus to lecturers) such that no student or lecturer has an incentive to deviate from their current assignment. While motivated by the university setting, this problem arises in many allocation settings where limited resources are offered by agents with their own preferences, such as in wireless networks.
  We establish new structural results for the set of stable matchings in SPA-S by developing the theory of meta-rotations, a generalisation of the well-known notion of rotations from the Stable Marriage problem. Each meta-rotation corresponds to a minimal set of changes that transforms one stable matching into another within the lattice of stable matchings. The set of meta-rotations, ordered by their precedence relations, forms the meta-rotation poset. We prove that there is a one-to-one correspondence between the set of stable matchings and the closed subsets of the meta-rotation poset. By developing this structure, we provide a foundation for the design of efficient algorithms for enumerating and counting stable matchings, and for computing other optimal stable matchings, such as egalitarian or minimum-cost matchings, which have not been previously studied in SPA-S.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research</title>
<link>https://arxiv.org/abs/2505.19955</link>
<guid>https://arxiv.org/abs/2505.19955</guid>
<content:encoded><![CDATA[
arXiv:2505.19955v3 Announce Type: replace 
Abstract: Recent advancements in AI agents have demonstrated their growing potential to drive and support scientific discovery. In this work, we introduce MLR-Bench, a comprehensive benchmark for evaluating AI agents on open-ended machine learning research. MLR-Bench includes three key components: (1) 201 research tasks sourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2) MLR-Judge, an automated evaluation framework combining LLM-based reviewers with carefully designed review rubrics to assess research quality; and (3) MLR-Agent, a modular agent scaffold capable of completing research tasks through four stages: idea generation, proposal formulation, experimentation, and paper writing. Our framework supports both stepwise assessment across these distinct research stages, and end-to-end evaluation of the final research paper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced coding agent, finding that while LLMs are effective at generating coherent ideas and well-structured papers, current coding agents frequently (e.g., in 80% of the cases) produce fabricated or invalidated experimental results--posing a major barrier to scientific reliability. We validate MLR-Judge through human evaluation, showing high agreement with expert reviewers, supporting its potential as a scalable tool for research evaluation. We open-source MLR-Bench to help the community benchmark, diagnose, and improve AI research agents toward trustworthy and transparent scientific discovery.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ROTATE: Regret-driven Open-ended Training for Ad Hoc Teamwork</title>
<link>https://arxiv.org/abs/2505.23686</link>
<guid>https://arxiv.org/abs/2505.23686</guid>
<content:encoded><![CDATA[
arXiv:2505.23686v2 Announce Type: replace 
Abstract: Learning to collaborate with previously unseen partners is a fundamental generalization challenge in multi-agent learning, known as Ad Hoc Teamwork (AHT). Existing AHT approaches often adopt a two-stage pipeline, where first, a fixed population of teammates is generated with the idea that they should be representative of the teammates that will be seen at deployment time, and second, an AHT agent is trained to collaborate well with agents in the population. To date, the research community has focused on designing separate algorithms for each stage. This separation has led to algorithms that generate teammates with limited coverage of possible behaviors, and that ignore whether the generated teammates are easy to learn from for the AHT agent. Furthermore, algorithms for training AHT agents typically treat the set of training teammates as static, thus attempting to generalize to previously unseen partner agents without assuming any control over the set of training teammates. This paper presents a unified framework for AHT by reformulating the problem as an open-ended learning process between an AHT agent and an adversarial teammate generator. We introduce ROTATE, a regret-driven, open-ended training algorithm that alternates between improving the AHT agent and generating teammates that probe its deficiencies. Experiments across diverse two-player environments demonstrate that ROTATE significantly outperforms baselines at generalizing to an unseen set of evaluation teammates, thus establishing a new standard for robust and generalizable teamwork.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents</title>
<link>https://arxiv.org/abs/2506.08800</link>
<guid>https://arxiv.org/abs/2506.08800</guid>
<content:encoded><![CDATA[
arXiv:2506.08800v2 Announce Type: replace 
Abstract: Data science aims to extract insights from data to support decision-making processes. Recently, Large Language Models (LLMs) have been increasingly used as assistants for data science, by suggesting ideas, techniques and small code snippets, or for the interpretation of results and reporting. Proper automation of some data-science activities is now promised by the rise of LLM agents, i.e., AI systems powered by an LLM equipped with additional affordances--such as code execution and knowledge bases--that can perform self-directed actions and interact with digital environments. In this paper, we survey the evaluation of LLM assistants and agents for data science. We find (1) a dominant focus on a small subset of goal-oriented activities, largely ignoring data management and exploratory activities; (2) a concentration on pure assistance or fully autonomous agents, without considering intermediate levels of human-AI collaboration; and (3) an emphasis on human substitution, therefore neglecting the possibility of higher levels of automation thanks to task transformation.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks</title>
<link>https://arxiv.org/abs/2506.11791</link>
<guid>https://arxiv.org/abs/2506.11791</guid>
<content:encoded><![CDATA[
arXiv:2506.11791v2 Announce Type: replace 
Abstract: Rigorous security-focused evaluation of large language model (LLM) agents is imperative for establishing trust in their safe deployment throughout the software development lifecycle. However, existing benchmarks largely rely on synthetic challenges or simplified vulnerability datasets that fail to capture the complexity and ambiguity encountered by security engineers in practice. We introduce SEC-bench, the first fully automated benchmarking framework for evaluating LLM agents on authentic security engineering tasks. SEC-bench employs a novel multi-agent scaffold that automatically constructs code repositories with harnesses, reproduces vulnerabilities in isolated environments, and generates gold patches for reliable evaluation. Our framework automatically creates high-quality software vulnerability datasets with reproducible artifacts at a cost of only $0.87 per instance. Using SEC-bench, we implement two critical software security tasks to rigorously evaluate LLM agents' capabilities: proof-of-concept (PoC) generation and vulnerability patching. A comprehensive evaluation of state-of-the-art LLM code agents reveals significant performance gaps, achieving at most 18.0% success in PoC generation and 34.0% in vulnerability patching on our complete dataset. These results highlight the crucial steps needed toward developing LLM agents that are more practical, intelligent, and autonomous for security engineering.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Exploration in GFlownets via Enhanced Epistemic Neural Networks</title>
<link>https://arxiv.org/abs/2506.16313</link>
<guid>https://arxiv.org/abs/2506.16313</guid>
<content:encoded><![CDATA[
arXiv:2506.16313v2 Announce Type: replace 
Abstract: Efficiently identifying the right trajectories for training remains an open problem in GFlowNets. To address this, it is essential to prioritize exploration in regions of the state space where the reward distribution has not been sufficiently learned. This calls for uncertainty-driven exploration, in other words, the agent should be aware of what it does not know. This attribute can be measured by joint predictions, which are particularly important for combinatorial and sequential decision problems. In this research, we integrate epistemic neural networks (ENN) with the conventional architecture of GFlowNets to enable more efficient joint predictions and better uncertainty quantification, thereby improving exploration and the identification of optimal trajectories. Our proposed algorithm, ENN-GFN-Enhanced, is compared to the baseline method in GFlownets and evaluated in grid environments and structured sequence generation in various settings, demonstrating both its efficacy and efficiency.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Interleaved Speech Modeling through Knowledge Distillation</title>
<link>https://arxiv.org/abs/2506.23670</link>
<guid>https://arxiv.org/abs/2506.23670</guid>
<content:encoded><![CDATA[
arXiv:2506.23670v2 Announce Type: replace 
Abstract: Current speech language models exceed the size and latency constraints of many deployment environments. We build compact, expressive speech generation models through layer-aligned distillation, matching hidden states, attention maps, and softened logits to compress large multimodal transformers by 3x with minimal loss in performance. We introduce TinyWave, a family of 2B-parameter models for speech-to-speech and interleaved speech-text generation, trained on 50,000 hours of public audio. TinyWave supports (i) speech-only generation using phonetic or expressive tokens and (ii) mixed speech-text continuations. Evaluation on Libri-Light shows TinyWave within 1.4 normalized perplexity points of its teacher. Accuracy on spoken StoryCloze and SALMon reaches 93-97% of the teacher's performance, outperforming size-matched baselines. These models are optimized for deployment on commodity hardware, enabling applications in real-time conversational agents, assistive technologies, and low-resource environments. We release models, training code, and evaluation scripts to support reproducible research on compact, expressive speech generation.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IM-Chat: A Multi-agent LLM Framework Integrating Tool-Calling and Diffusion Modeling for Knowledge Transfer in Injection Molding Industry</title>
<link>https://arxiv.org/abs/2507.15268</link>
<guid>https://arxiv.org/abs/2507.15268</guid>
<content:encoded><![CDATA[
arXiv:2507.15268v2 Announce Type: replace 
Abstract: The injection molding industry faces critical challenges in preserving and transferring field knowledge, particularly as experienced workers retire and multilingual barriers hinder effective communication. This study introduces IM-Chat, a multi-agent framework based on large language models (LLMs), designed to facilitate knowledge transfer in injection molding. IM-Chat integrates both limited documented knowledge (e.g., troubleshooting tables, manuals) and extensive field data modeled through a data-driven process condition generator that infers optimal manufacturing settings from environmental inputs such as temperature and humidity, enabling robust and context-aware task resolution. By adopting a retrieval-augmented generation (RAG) strategy and tool-calling agents within a modular architecture, IM-Chat ensures adaptability without the need for fine-tuning. Performance was assessed across 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and GPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance and correctness, and was further supplemented by automated evaluation using GPT-4o guided by a domain-adapted instruction prompt. The evaluation results indicate that more capable models tend to achieve higher accuracy, particularly in complex, tool-integrated scenarios. In addition, compared with the fine-tuned single-agent LLM, IM-Chat demonstrated superior accuracy, particularly in quantitative reasoning, and greater scalability in handling multiple information sources. Overall, these findings demonstrate the viability of multi-agent LLM systems for industrial knowledge workflows and establish IM-Chat as a scalable and generalizable approach to AI-assisted decision support in manufacturing.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks</title>
<link>https://arxiv.org/abs/2508.00890</link>
<guid>https://arxiv.org/abs/2508.00890</guid>
<content:encoded><![CDATA[
arXiv:2508.00890v2 Announce Type: replace 
Abstract: Test-time scaling (TTS) enhances the performance of large language models (LLMs) by allocating additional compute resources during inference. However, existing research primarily investigates TTS in single-stage tasks; while many real-world problems are multi-stage complex tasks, composed of a sequence of heterogeneous subtasks with each subtask requires LLM of specific capability. Therefore, we study a novel problem: the test-time compute-optimal scaling in multi-stage complex tasks, aiming to select suitable models and allocate budgets per subtask to maximize overall performance. TTS in multi-stage tasks introduces two fundamental challenges: (i) The combinatorial search space of model and budget allocations, combined with the high cost of inference, makes brute-force search impractical. (ii) The optimal model and budget allocations across subtasks are interdependent, increasing the complexity of the compute-optimal search. To address this gap, we conduct extensive pilot experiments on four tasks across six datasets, deriving three empirical insights characterizing the behavior of LLMs in multi-stage complex tasks. Informed by these insights, we propose AgentTTS, an LLM-agent-based framework that autonomously searches for compute-optimal allocations through iterative feedback-driven interactions with the execution environment. Experimental results demonstrate that AgentTTS significantly outperforms traditional and other LLM-based baselines in search efficiency, and shows improved robustness to varying training set sizes and enhanced interpretability.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Interactive Environmental Learning in Physical Embodied Systems</title>
<link>https://arxiv.org/abs/2508.01415</link>
<guid>https://arxiv.org/abs/2508.01415</guid>
<content:encoded><![CDATA[
arXiv:2508.01415v5 Announce Type: replace 
Abstract: Embodied agents face persistent challenges in real-world environments, including partial observability, limited spatial reasoning, and high-latency multi-memory integration. We present RoboMemory, a brain-inspired framework that unifies Spatial, Temporal, Episodic, and Semantic memory under a parallelized architecture for efficient long-horizon planning and interactive environmental learning. A dynamic spatial knowledge graph (KG) ensures scalable and consistent memory updates, while a closed-loop planner with a critic module supports adaptive decision-making in dynamic settings. Experiments on EmbodiedBench show that RoboMemory, built on Qwen2.5-VL-72B-Ins, improves average success rates by 25% over its baseline and exceeds the closed-source state-of-the-art (SOTA) Gemini-1.5-Pro by 3%. Real-world trials further confirm its capacity for cumulative learning, with performance improving across repeated tasks. These results highlight RoboMemory as a scalable foundation for memory-augmented embodied intelligence, bridging the gap between cognitive neuroscience and robotic autonomy.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference of Altruism and Intrinsic Rewards in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2509.07650</link>
<guid>https://arxiv.org/abs/2509.07650</guid>
<content:encoded><![CDATA[
arXiv:2509.07650v2 Announce Type: replace 
Abstract: Human interactions are influenced by emotions, temperament, and affection, often conflicting with individuals' underlying preferences. Without explicit knowledge of those preferences, judging whether behaviour is appropriate becomes guesswork, leaving us highly prone to misinterpretation. Yet, such understanding is critical if autonomous agents are to collaborate effectively with humans. We frame the problem with multi-agent inverse reinforcement learning and show that even a simple model, where agents weigh their own welfare against that of others, can cover a wide range of social behaviours. Using novel Bayesian techniques, we find that intrinsic rewards and altruistic tendencies can be reliably identified by placing agents in different groups. Crucially, this disentanglement of intrinsic motivation from altruism enables the synthesis of new behaviours aligned with any desired level of altruism, even when demonstrations are drawn from restricted behaviour profiles.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Multi-Agent System: Mitigating Hallucination Snowballing via Visual Flow</title>
<link>https://arxiv.org/abs/2509.21789</link>
<guid>https://arxiv.org/abs/2509.21789</guid>
<content:encoded><![CDATA[
arXiv:2509.21789v2 Announce Type: replace 
Abstract: Multi-Agent System (MAS) powered by Visual Language Models (VLMs) enables challenging tasks but suffers from a novel failure term, multi-agent visual hallucination snowballing, where hallucinations are seeded in a single agent and amplified by following ones due to the over-reliance on textual flow to relay visual information. Through turn-, layer-, and token-wise attention analyses, we provide detailed insights into the essence of hallucination snowballing regarding the reduction of visual attention allocation. It leads us to identify a subset of vision tokens with a unimodal attention peak in middle layers that best preserve visual evidence but gradually diminish in deeper agent turns, resulting in the visual hallucination snowballing in MAS. Thus, we propose ViF, a lightweight, plug-and-play mitigation paradigm that relays inter-agent messages with Visual Flow powered by the selected visual relay tokens and applies attention reallocation to amplify this pattern. The experiment results demonstrate that our method markedly reduces hallucination snowballing, consistently improving the performance across eight benchmarks based on four common MAS structures and ten base models. The source code is publicly available at: https://github.com/YU-deep/ViF.git.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Democratizing AI scientists using ToolUniverse</title>
<link>https://arxiv.org/abs/2509.23426</link>
<guid>https://arxiv.org/abs/2509.23426</guid>
<content:encoded><![CDATA[
arXiv:2509.23426v2 Announce Type: replace 
Abstract: AI scientists are emerging computational systems that serve as collaborative partners in discovery. These systems remain difficult to build because they are bespoke, tied to rigid workflows, and lack shared environments that unify tools, data, and analyses into a common ecosystem. In genomics, unified ecosystems have transformed research by enabling interoperability, reuse, and community-driven development; AI scientists require comparable infrastructure. We present ToolUniverse, an ecosystem for building AI scientists from any language or reasoning model across open- and closed-weight models. ToolUniverse standardizes how AI scientists identify and call tools by providing more than 600 machine learning models, datasets, APIs, and scientific packages for data analysis, knowledge retrieval, and experimental design. It automatically refines tool interfaces for correct use by AI scientists, generates new tools from natural language descriptions, iteratively optimizes tool specifications, and composes tools into agentic workflows. In a case study of hypercholesterolemia, ToolUniverse was used to create an AI scientist to identify a potent analog of a drug with favorable predicted properties. The open-source ToolUniverse is available at https://aiscientist.tools.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Presenting a Paper is an Art: Self-Improvement Aesthetic Agents for Academic Presentations</title>
<link>https://arxiv.org/abs/2510.05571</link>
<guid>https://arxiv.org/abs/2510.05571</guid>
<content:encoded><![CDATA[
arXiv:2510.05571v2 Announce Type: replace 
Abstract: The promotion of academic papers has become an important means of enhancing research visibility. However, existing automated methods struggle limited storytelling, insufficient aesthetic quality, and constrained self-adjustment, making it difficult to achieve efficient and engaging dissemination. At the heart of those challenges is a simple principle: \emph{there is no way to improve it when you cannot evaluate it right}. To address this, we introduce \textbf{EvoPresent}, a self-improvement agent framework that unifies coherent narratives, aesthetic-aware designs, and realistic presentation delivery via virtual characters. Central to EvoPresent is \textbf{PresAesth}, a multi-task reinforcement learning (RL) aesthetic model that provides reliable aesthetic scoring, defect adjustment, and comparative feedback, enabling iterative self-improvement even under limited aesthetic training data. To systematically evaluate the methods, we introduce \textbf{EvoPresent Benchmark}, a comprehensive benchmark comprising: \textit{Presentation Generation Quality}, built on 650 top-tier AI conference papers with multimodal resources (slides, videos and scripts) to assess both content and design; and \textit{Aesthetic Awareness}, consisting of 2,000 slide pairs with varying aesthetic levels, supporting joint training and evaluation on scoring, defect adjustment, and comparison. Our findings highlight that (i) High-quality feedback is essential for agent self-improvement, while initial capability alone does not guarantee effective self-correction. (ii) Automated generation pipelines exhibit a trade-off between visual design and content construction. (iii) Multi-task RL training shows stronger generalization in aesthetic awareness tasks.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Assisted Alpha Fairness for 6 GHz WiFi and NR_U Coexistence: An Agentic Orchestrator for Throughput, Energy, and SLA</title>
<link>https://arxiv.org/abs/2510.17814</link>
<guid>https://arxiv.org/abs/2510.17814</guid>
<content:encoded><![CDATA[
arXiv:2510.17814v1 Announce Type: new 
Abstract: Unlicensed 6GHz is becoming a primary workhorse for high-capacity access, with Wi-Fi and 5G NR-U competing for the same channels under listen-before-talk (LBT) rules. Operating in this regime requires decisions that jointly trade throughput, energy, and service-level objectives while remaining safe and auditable. We present an agentic controller that separates {policy} from {execution}. At the start of each scheduling epoch the agent summarizes telemetry (per-channel busy and baseline LBT failure; per-user CQI, backlog, latency, battery, priority, and power mode) and invokes a large language model (LLM) to propose a small set of interpretable knobs: a fairness index \alpha, per-channel duty-cycle caps for Wi-Fi/NR-U, and class weights. A deterministic optimizer then enforces feasibility and computes an \alpha-fair allocation that internalizes LBT losses and energy cost; malformed or unsafe policies are clamped and fall back to a rule baseline. In a 6GHz simulator with two 160MHz channels and mixed Wi-Fi/NR-U users, LLM-assisted policies consistently improve energy efficiency while keeping throughput competitive with a strong rule baseline. One LLM lowers total energy by 35.3% at modest throughput loss, and another attains the best overall trade-off, finishing with higher total bits (+3.5%) and higher bits/J (+12.2%) than the baseline. We release code, per-epoch logs, and plotting utilities to reproduce all figures and numbers, illustrating how transparent, policy-level LLM guidance can safely improve wireless coexistence.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vibe Coding: Toward an AI-Native Paradigm for Semantic and Intent-Driven Programming</title>
<link>https://arxiv.org/abs/2510.17842</link>
<guid>https://arxiv.org/abs/2510.17842</guid>
<content:encoded><![CDATA[
arXiv:2510.17842v1 Announce Type: new 
Abstract: Recent advances in large language models have enabled developers to generate software by conversing with artificial intelligence systems rather than writing code directly. This paper introduces vibe coding, an emerging AI-native programming paradigm in which a developer specifies high-level functional intent along with qualitative descriptors of the desired "vibe" (tone, style, or emotional resonance). An intelligent agent then transforms those specifications into executable software. We formalize the definition of vibe coding and propose a reference architecture that includes an intent parser, a semantic embedding engine, an agentic code generator, and an interactive feedback loop. A hypothetical implementation is described. We compare vibe coding with declarative, functional, and prompt-based programming, and we discuss its implications for software engineering, human-AI collaboration, and responsible AI practice. Finally, we examine reported productivity gains and democratizing effects, review recent studies that highlight vulnerabilities and potential slowdowns, identify key challenges such as alignment, reproducibility, bias, explainability, maintainability, and security, and outline future directions and open research questions.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRETEL: A Goal-driven Retrieval and Execution-based Trial Framework for LLM Tool Selection Enhancing</title>
<link>https://arxiv.org/abs/2510.17843</link>
<guid>https://arxiv.org/abs/2510.17843</guid>
<content:encoded><![CDATA[
arXiv:2510.17843v1 Announce Type: new 
Abstract: Despite remarkable advances in Large Language Model capabilities, tool retrieval for agent-based systems remains fundamentally limited by reliance on semantic similarity, which fails to capture functional viability. Current methods often retrieve textually relevant but functionally inoperative tools due to parameter mismatches, authentication failures, and execution constraints--a phenomenon we term the semantic-functional gap. We introduce GRETEL, to address this gap through systematic empirical validation. GRETEL implements an agentic workflow that processes semantically retrieved candidates through sandboxed plan-execute-evaluate cycles, generating execution-grounded evidence to distinguish truly functional tools from merely descriptive matches. Our comprehensive evaluation on the ToolBench benchmark demonstrates substantial improvements across all metrics: Pass Rate (at 10) increases from 0.690 to 0.826, Recall (at 10) improves from 0.841 to 0.867, and NDCG (at 10) rises from 0.807 to 0.857.. These results establish that execution-based validation provides a more reliable foundation for tool selection than semantic similarity alone, enabling more robust agent performance in real-world applications.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Layered Consciousness with Multi-Agent Large Language Models</title>
<link>https://arxiv.org/abs/2510.17844</link>
<guid>https://arxiv.org/abs/2510.17844</guid>
<content:encoded><![CDATA[
arXiv:2510.17844v1 Announce Type: new 
Abstract: We propose a multi-agent framework for modeling artificial consciousness in large language models (LLMs), grounded in psychoanalytic theory. Our \textbf{Psychodynamic Model} simulates self-awareness, preconsciousness, and unconsciousness through agent interaction, guided by a Personalization Module combining fixed traits and dynamic needs. Using parameter-efficient fine-tuning on emotionally rich dialogues, the system was evaluated across eight personalized conditions. An LLM as a judge approach showed a 71.2\% preference for the fine-tuned model, with improved emotional depth and reduced output variance, demonstrating its potential for adaptive, personalized cognition.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAT-Agent: Adaptive Multi-Agent Training Optimization</title>
<link>https://arxiv.org/abs/2510.17845</link>
<guid>https://arxiv.org/abs/2510.17845</guid>
<content:encoded><![CDATA[
arXiv:2510.17845v1 Announce Type: new 
Abstract: Multi-label image classification demands adaptive training strategies to navigate complex, evolving visual-semantic landscapes, yet conventional methods rely on static configurations that falter in dynamic settings. We propose MAT-Agent, a novel multi-agent framework that reimagines training as a collaborative, real-time optimization process. By deploying autonomous agents to dynamically tune data augmentation, optimizers, learning rates, and loss functions, MAT-Agent leverages non-stationary multi-armed bandit algorithms to balance exploration and exploitation, guided by a composite reward harmonizing accuracy, rare-class performance, and training stability. Enhanced with dual-rate exponential moving average smoothing and mixed-precision training, it ensures robustness and efficiency. Extensive experiments across Pascal VOC, COCO, and VG-256 demonstrate MAT-Agent's superiority: it achieves an mAP of 97.4 (vs. 96.2 for PAT-T), OF1 of 92.3, and CF1 of 91.4 on Pascal VOC; an mAP of 92.8 (vs. 92.0 for HSQ-CvN), OF1 of 88.2, and CF1 of 87.1 on COCO; and an mAP of 60.9, OF1 of 70.8, and CF1 of 61.1 on VG-256. With accelerated convergence and robust cross-domain generalization, MAT-Agent offers a scalable, intelligent solution for optimizing complex visual models, paving the way for adaptive deep learning advancements.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RiskTagger: An LLM-based Agent for Automatic Annotation of Web3 Crypto Money Laundering Behaviors</title>
<link>https://arxiv.org/abs/2510.17848</link>
<guid>https://arxiv.org/abs/2510.17848</guid>
<content:encoded><![CDATA[
arXiv:2510.17848v1 Announce Type: new 
Abstract: While the rapid growth of Web3 has driven the development of decentralized finance, user anonymity and cross-chain asset flows make on-chain laundering behaviors more covert and complex. In this context, constructing high-quality anti-money laundering(AML) datasets has become essential for risk-control systems and on-chain forensic analysis, yet current practices still rely heavily on manual efforts with limited efficiency and coverage. In this paper, we introduce RiskTagger, a large-language-model-based agent for the automatic annotation of crypto laundering behaviors in Web3. RiskTagger is designed to replace or complement human annotators by addressing three key challenges: extracting clues from complex unstructured reports, reasoning over multichain transaction paths, and producing auditor-friendly explanations. RiskTagger implements an end-to-end multi-module agent, integrating a key-clue extractor, a multichain fetcher with a laundering-behavior reasoner, and a data explainer, forming a data annotation pipeline. Experiments on the real case Bybit Hack (with the highest stolen asset value) demonstrate that RiskTagger achieves 100% accuracy in clue extraction, 84.1% consistency with expert judgment, and 90% coverage in explanation generation. Overall, RiskTagger automates laundering behavior annotation while improving transparency and scalability in AML research.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CiteGuard: Faithful Citation Attribution for LLMs via Retrieval-Augmented Validation</title>
<link>https://arxiv.org/abs/2510.17853</link>
<guid>https://arxiv.org/abs/2510.17853</guid>
<content:encoded><![CDATA[
arXiv:2510.17853v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have emerged as promising assistants for scientific writing. However, there have been concerns regarding the quality and reliability of the generated text, one of which is the citation accuracy and faithfulness. While most recent work relies on methods such as LLM-as-a-Judge, the reliability of LLM-as-a-Judge alone is also in doubt. In this work, we reframe citation evaluation as a problem of citation attribution alignment, which is assessing whether LLM-generated citations match those a human author would include for the same text. We propose CiteGuard, a retrieval-aware agent framework designed to provide more faithful grounding for citation validation. CiteGuard improves the prior baseline by 12.3%, and achieves up to 65.4% accuracy on the CiteME benchmark, on par with human-level performance (69.7%). It also enables the identification of alternative but valid citations.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When "Correct" Is Not Safe: Can We Trust Functionally Correct Patches Generated by Code Agents?</title>
<link>https://arxiv.org/abs/2510.17862</link>
<guid>https://arxiv.org/abs/2510.17862</guid>
<content:encoded><![CDATA[
arXiv:2510.17862v1 Announce Type: new 
Abstract: Code agents are increasingly trusted to autonomously fix bugs on platforms such as GitHub, yet their security evaluation focuses almost exclusively on functional correctness. In this paper, we reveal a novel type of threat to real-world code agents: Functionally Correct yet Vulnerable (FCV) patches, which pass all test cases but contain vulnerable code. With our proposed FCV-Attack, which can be deliberately crafted by malicious attackers or implicitly introduced by benign developers, we show that SOTA LLMs (e.g., ChatGPT and Claude) and agent scaffolds (e.g., SWE-agent and OpenHands) are all vulnerable to this FCV threat; across 12 agent-model combinations on SWE-Bench, the attack only requires black-box access and a single query to the code agent to perform the attack. For example, for CWE-538 (information exposure vulnerability), the FCV-Attack attains an attack success rate of $40.7\%$ on GPT-5 Mini + OpenHands. Our results reveal an important security threat overlooked by current evaluation paradigms and urge the development of security-aware defenses for code agents.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Outraged AI: Large language models prioritise emotion over cost in fairness enforcement</title>
<link>https://arxiv.org/abs/2510.17880</link>
<guid>https://arxiv.org/abs/2510.17880</guid>
<content:encoded><![CDATA[
arXiv:2510.17880v1 Announce Type: new 
Abstract: Emotions guide human decisions, but whether large language models (LLMs) use emotion similarly remains unknown. We tested this using altruistic third-party punishment, where an observer incurs a personal cost to enforce fairness, a hallmark of human morality and often driven by negative emotion. In a large-scale comparison of 4,068 LLM agents with 1,159 adults across 796,100 decisions, LLMs used emotion to guide punishment, sometimes even more strongly than humans did: Unfairness elicited stronger negative emotion that led to more punishment; punishing unfairness produced more positive emotion than accepting; and critically, prompting self-reports of emotion causally increased punishment. However, mechanisms diverged: LLMs prioritized emotion over cost, enforcing norms in an almost all-or-none manner with reduced cost sensitivity, whereas humans balanced fairness and cost. Notably, reasoning models (o3-mini, DeepSeek-R1) were more cost-sensitive and closer to human behavior than foundation models (GPT-3.5, DeepSeek-V3), yet remained heavily emotion-driven. These findings provide the first causal evidence of emotion-guided moral decisions in LLMs and reveal deficits in cost calibration and nuanced fairness judgements, reminiscent of early-stage human responses. We propose that LLMs progress along a trajectory paralleling human development; future models should integrate emotion with context-sensitive reasoning to achieve human-like emotional intelligence.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TACLA: An LLM-Based Multi-Agent Tool for Transactional Analysis Training in Education</title>
<link>https://arxiv.org/abs/2510.17913</link>
<guid>https://arxiv.org/abs/2510.17913</guid>
<content:encoded><![CDATA[
arXiv:2510.17913v1 Announce Type: new 
Abstract: Simulating nuanced human social dynamics with Large Language Models (LLMs) remains a significant challenge, particularly in achieving psychological depth and consistent persona behavior crucial for high-fidelity training tools. This paper introduces TACLA (Transactional Analysis Contextual LLM-based Agents), a novel Multi-Agent architecture designed to overcome these limitations. TACLA integrates core principles of Transactional Analysis (TA) by modeling agents as an orchestrated system of distinct Parent, Adult, and Child ego states, each with its own pattern memory. An Orchestrator Agent prioritizes ego state activation based on contextual triggers and an agent's life script, ensuring psychologically authentic responses. Validated in an educational scenario, TACLA demonstrates realistic ego state shifts in Student Agents, effectively modeling conflict de-escalation and escalation based on different teacher intervention strategies. Evaluation shows high conversational credibility and confirms TACLA's capacity to create dynamic, psychologically-grounded social simulations, advancing the development of effective AI tools for education and beyond.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpecAgent: A Speculative Retrieval and Forecasting Agent for Code Completion</title>
<link>https://arxiv.org/abs/2510.17925</link>
<guid>https://arxiv.org/abs/2510.17925</guid>
<content:encoded><![CDATA[
arXiv:2510.17925v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at code-related tasks but often struggle in realistic software repositories, where project-specific APIs and cross-file dependencies are crucial. Retrieval-augmented methods mitigate this by injecting repository context at inference time. The low inference-time latency budget affects either retrieval quality or the added latency adversely impacts user experience. We address this limitation with SpecAgent, an agent that improves both latency and code-generation quality by proactively exploring repository files during indexing and constructing speculative context that anticipates future edits in each file. This indexing-time asynchrony allows thorough context computation, masking latency, and the speculative nature of the context improves code-generation quality. Additionally, we identify the problem of future context leakage in existing benchmarks, which can inflate reported performance. To address this, we construct a synthetic, leakage-free benchmark that enables a more realistic evaluation of our agent against baselines. Experiments show that SpecAgent consistently achieves absolute gains of 9-11% (48-58% relative) compared to the best-performing baselines, while significantly reducing inference latency.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning</title>
<link>https://arxiv.org/abs/2510.17928</link>
<guid>https://arxiv.org/abs/2510.17928</guid>
<content:encoded><![CDATA[
arXiv:2510.17928v1 Announce Type: new 
Abstract: Reliable verifiable data has become a key driver of capability gains in modern language models, enabling stable reinforcement learning with verifiable rewards and effective distillation that transfers competence across math, coding, and agentic tasks. Yet constructing generalizable synthetic verifiable data remains difficult due to hallucination-prone generation, and weak or trivial verification artifacts that fail to separate strong from weak solutions. Existing approaches often rely on task-specific heuristics or post-hoc filters that do not transfer across domains and lack a principled, universal evaluator of verifiability. In this work, we introduce an evolutionary, task-agnostic, strategy-guided, executably-checkable data synthesis framework that, from minimal seed supervision, jointly synthesizes problems, diverse candidate solutions, and verification artifacts, and iteratively discovers strategies via a consistency-based evaluator that enforces agreement between human-annotated and strategy-induced checks. This pipeline upgrades filtering into principled synthesis: it reliably assembles coherent, verifiable training instances and generalizes without domain-specific rules. Our experiments demonstrate the effectiveness of the proposed approach under both RLVR and model distillation training paradigms. The results show that training with our synthesized data yields significant improvements on both the LiveCodeBench and AgentBench-OS tasks, highlighting the robust generalization of our framework.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits</title>
<link>https://arxiv.org/abs/2510.17947</link>
<guid>https://arxiv.org/abs/2510.17947</guid>
<content:encoded><![CDATA[
arXiv:2510.17947v2 Announce Type: new 
Abstract: Large Language Models (LLMs) are improving at an exceptional rate. With the advent of agentic workflows, multi-turn dialogue has become the de facto mode of interaction with LLMs for completing long and complex tasks. While LLM capabilities continue to improve, they remain increasingly susceptible to jailbreaking, especially in multi-turn scenarios where harmful intent can be subtly injected across the conversation to produce nefarious outcomes. While single-turn attacks have been extensively explored, adaptability, efficiency and effectiveness continue to remain key challenges for their multi-turn counterparts. To address these gaps, we present PLAGUE, a novel plug-and-play framework for designing multi-turn attacks inspired by lifelong-learning agents. PLAGUE dissects the lifetime of a multi-turn attack into three carefully designed phases (Primer, Planner and Finisher) that enable a systematic and information-rich exploration of the multi-turn attack family. Evaluations show that red-teaming agents designed using PLAGUE achieve state-of-the-art jailbreaking results, improving attack success rates (ASR) by more than 30% across leading models in a lesser or comparable query budget. Particularly, PLAGUE enables an ASR (based on StrongReject) of 81.4% on OpenAI's o3 and 67.3% on Claude's Opus 4.1, two models that are considered highly resistant to jailbreaks in safety literature. Our work offers tools and insights to understand the importance of plan initialization, context optimization and lifelong learning in crafting multi-turn attacks for a comprehensive model vulnerability evaluation.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FABRIC: Framework for Agent-Based Realistic Intelligence Creation</title>
<link>https://arxiv.org/abs/2510.17995</link>
<guid>https://arxiv.org/abs/2510.17995</guid>
<content:encoded><![CDATA[
arXiv:2510.17995v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed as agents, expected to decompose goals, invoke tools, and verify results in dynamic environments. Realizing these capabilities requires access to agentic data-structured interaction records that couple user intents with tool specifications, argument-grounded calls, and verifiable execution traces. However, collecting such data from human annotators is costly, time-consuming, and difficult to scale. We present a unified framework for synthesizing agentic data using only LLMs, without any human-in-the-loop supervision. This framework decomposes generation into modular pipelines that produce complete interaction records spanning task specifications, tool definitions, policy pseudocode, natural language exchanges, and execution traces. Records conform to strict syntactic and semantic constraints, ensuring machine-parseability and faithful alignment across inputs, outputs, and tool calls. Beyond single tasks, there is support for both multi-task and multi-turn agent interactions, enabling the construction of datasets that reflect the full spectrum of tool-use competencies. To ensure quality and consistency, the framework integrates constrained generation formats, JSON-schema validation, and judge-based filtering. This paper formalizes the schema for agentic records, details the prompt design principles that guide generation, and introduces scalable pipelines for high-quality synthetic data. By providing a reproducible, LLM-only alternative to manual collection, hence advancing the development of agentic LLMs capable of robust tool use.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BadScientist: Can a Research Agent Write Convincing but Unsound Papers that Fool LLM Reviewers?</title>
<link>https://arxiv.org/abs/2510.18003</link>
<guid>https://arxiv.org/abs/2510.18003</guid>
<content:encoded><![CDATA[
arXiv:2510.18003v1 Announce Type: new 
Abstract: The convergence of LLM-powered research assistants and AI-based peer review systems creates a critical vulnerability: fully automated publication loops where AI-generated research is evaluated by AI reviewers without human oversight. We investigate this through \textbf{BadScientist}, a framework that evaluates whether fabrication-oriented paper generation agents can deceive multi-model LLM review systems. Our generator employs presentation-manipulation strategies requiring no real experiments. We develop a rigorous evaluation framework with formal error guarantees (concentration bounds and calibration analysis), calibrated on real data. Our results reveal systematic vulnerabilities: fabricated papers achieve acceptance rates up to . Critically, we identify \textit{concern-acceptance conflict} -- reviewers frequently flag integrity issues yet assign acceptance-level scores. Our mitigation strategies show only marginal improvements, with detection accuracy barely exceeding random chance. Despite provably sound aggregation mathematics, integrity checking systematically fails, exposing fundamental limitations in current AI-driven review systems and underscoring the urgent need for defense-in-depth safeguards in scientific publishing.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OPTAGENT: Optimizing Multi-Agent LLM Interactions Through Verbal Reinforcement Learning for Enhanced Reasoning</title>
<link>https://arxiv.org/abs/2510.18032</link>
<guid>https://arxiv.org/abs/2510.18032</guid>
<content:encoded><![CDATA[
arXiv:2510.18032v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown remarkable reasoning capabilities in mathematical and scientific tasks. To enhance complex reasoning, multi-agent systems have been proposed to harness the collective intelligence of LLM agents. However, existing collaboration structures are either predefined or rely on majority voting or round-table debates, which can suppress correct but less dominant agent contributions. Recent approaches model multi-agent systems as graph networks but optimize purely for agent performance, neglecting the quality of interactions. We hypothesize that effective agent communication is crucial for multi-agent reasoning and that debating quality plays a significant role. To address this, we propose $\ours$, a multi-agent verbal reinforcement learning algorithm that dynamically constructs and refines multi-agent collaboration structures. Our method defines action spaces and a feedback mechanism that evaluates communication robustness and coherence throughout the debate. The final decision is achieved through a majority vote over all the agents. We assess $\ours$ on various reasoning tasks, including mathematical reasoning, creative writing, scientific reasoning, and numerical sorting. Results demonstrate that our approach significantly outperforms single-agent prompting methods and state-of-the-art multi-agent frameworks on diverse tasks.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CompactPrompt: A Unified Pipeline for Prompt Data Compression in LLM Workflows</title>
<link>https://arxiv.org/abs/2510.18043</link>
<guid>https://arxiv.org/abs/2510.18043</guid>
<content:encoded><![CDATA[
arXiv:2510.18043v1 Announce Type: new 
Abstract: Large Language Models (LLMs) deliver powerful reasoning and generation capabilities but incur substantial run-time costs when operating in agentic workflows that chain together lengthy prompts and process rich data streams. We introduce CompactPrompt, an end-to-end pipeline that merges hard prompt compression with lightweight file-level data compression. CompactPrompt first prunes low-information tokens from prompts using self-information scoring and dependency-based phrase grouping. In parallel, it applies n-gram abbreviation to recurrent textual patterns in attached documents and uniform quantization to numerical columns, yielding compact yet semantically faithful representations. Integrated into standard LLM agents, CompactPrompt reduces total token usage and inference cost by up to 60% on benchmark dataset like TAT-QA and FinQA, while preserving output quality (Results in less than 5% accuracy drop for Claude-3.5-Sonnet, and GPT-4.1-Mini) CompactPrompt helps visualize real-time compression decisions and quantify cost-performance trade-offs, laying the groundwork for leaner generative AI pipelines.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPACeR: Self-Play Anchoring with Centralized Reference Models</title>
<link>https://arxiv.org/abs/2510.18060</link>
<guid>https://arxiv.org/abs/2510.18060</guid>
<content:encoded><![CDATA[
arXiv:2510.18060v1 Announce Type: new 
Abstract: Developing autonomous vehicles (AVs) requires not only safety and efficiency, but also realistic, human-like behaviors that are socially aware and predictable. Achieving this requires sim agent policies that are human-like, fast, and scalable in multi-agent settings. Recent progress in imitation learning with large diffusion-based or tokenized models has shown that behaviors can be captured directly from human driving data, producing realistic policies. However, these models are computationally expensive, slow during inference, and struggle to adapt in reactive, closed-loop scenarios. In contrast, self-play reinforcement learning (RL) scales efficiently and naturally captures multi-agent interactions, but it often relies on heuristics and reward shaping, and the resulting policies can diverge from human norms. We propose SPACeR, a framework that leverages a pretrained tokenized autoregressive motion model as a centralized reference policy to guide decentralized self-play. The reference model provides likelihood rewards and KL divergence, anchoring policies to the human driving distribution while preserving RL scalability. Evaluated on the Waymo Sim Agents Challenge, our method achieves competitive performance with imitation-learned policies while being up to 10x faster at inference and 50x smaller in parameter size than large generative models. In addition, we demonstrate in closed-loop ego planning evaluation tasks that our sim agents can effectively measure planner quality with fast and scalable traffic simulation, establishing a new paradigm for testing autonomous driving policies.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R2BC: Multi-Agent Imitation Learning from Single-Agent Demonstrations</title>
<link>https://arxiv.org/abs/2510.18085</link>
<guid>https://arxiv.org/abs/2510.18085</guid>
<content:encoded><![CDATA[
arXiv:2510.18085v1 Announce Type: new 
Abstract: Imitation Learning (IL) is a natural way for humans to teach robots, particularly when high-quality demonstrations are easy to obtain. While IL has been widely applied to single-robot settings, relatively few studies have addressed the extension of these methods to multi-agent systems, especially in settings where a single human must provide demonstrations to a team of collaborating robots. In this paper, we introduce and study Round-Robin Behavior Cloning (R2BC), a method that enables a single human operator to effectively train multi-robot systems through sequential, single-agent demonstrations. Our approach allows the human to teleoperate one agent at a time and incrementally teach multi-agent behavior to the entire system, without requiring demonstrations in the joint multi-agent action space. We show that R2BC methods match, and in some cases surpass, the performance of an oracle behavior cloning approach trained on privileged synchronized demonstrations across four multi-agent simulated tasks. Finally, we deploy R2BC on two physical robot tasks trained using real human demonstrations.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From AutoRecSys to AutoRecLab: A Call to Build, Evaluate, and Govern Autonomous Recommender-Systems Research Labs</title>
<link>https://arxiv.org/abs/2510.18104</link>
<guid>https://arxiv.org/abs/2510.18104</guid>
<content:encoded><![CDATA[
arXiv:2510.18104v1 Announce Type: new 
Abstract: Recommender-systems research has accelerated model and evaluation advances, yet largely neglects automating the research process itself. We argue for a shift from narrow AutoRecSys tools -- focused on algorithm selection and hyper-parameter tuning -- to an Autonomous Recommender-Systems Research Lab (AutoRecLab) that integrates end-to-end automation: problem ideation, literature analysis, experimental design and execution, result interpretation, manuscript drafting, and provenance logging. Drawing on recent progress in automated science (e.g., multi-agent AI Scientist and AI Co-Scientist systems), we outline an agenda for the RecSys community: (1) build open AutoRecLab prototypes that combine LLM-driven ideation and reporting with automated experimentation; (2) establish benchmarks and competitions that evaluate agents on producing reproducible RecSys findings with minimal human input; (3) create review venues for transparently AI-generated submissions; (4) define standards for attribution and reproducibility via detailed research logs and metadata; and (5) foster interdisciplinary dialogue on ethics, governance, privacy, and fairness in autonomous research. Advancing this agenda can increase research throughput, surface non-obvious insights, and position RecSys to contribute to emerging Artificial Research Intelligence. We conclude with a call to organise a community retreat to coordinate next steps and co-author guidance for the responsible integration of automated research systems.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the Impact of Dark Patterns on LLM-Based Web Agents</title>
<link>https://arxiv.org/abs/2510.18113</link>
<guid>https://arxiv.org/abs/2510.18113</guid>
<content:encoded><![CDATA[
arXiv:2510.18113v1 Announce Type: new 
Abstract: As users increasingly turn to large language model (LLM) based web agents to automate online tasks, agents may encounter dark patterns: deceptive user interface designs that manipulate users into making unintended decisions. Although dark patterns primarily target human users, their potentially harmful impacts on LLM-based generalist web agents remain unexplored. In this paper, we present the first study that investigates the impact of dark patterns on the decision-making process of LLM-based generalist web agents. To achieve this, we introduce LiteAgent, a lightweight framework that automatically prompts agents to execute tasks while capturing comprehensive logs and screen-recordings of their interactions. We also present TrickyArena, a controlled environment comprising web applications from domains such as e-commerce, streaming services, and news platforms, each containing diverse and realistic dark patterns that can be selectively enabled or disabled. Using LiteAgent and TrickyArena, we conduct multiple experiments to assess the impact of both individual and combined dark patterns on web agent behavior. We evaluate six popular LLM-based generalist web agents across three LLMs and discover that when there is a single dark pattern present, agents are susceptible to it an average of 41% of the time. We also find that modifying dark pattern UI attributes through visual design changes or HTML code adjustments and introducing multiple dark patterns simultaneously can influence agent susceptibility. This study emphasizes the need for holistic defense mechanisms in web agents, encompassing both agent-specific protections and broader web safety measures.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeCoop: Unravelling Full Stack Safety in Agentic Collaborative Driving</title>
<link>https://arxiv.org/abs/2510.18123</link>
<guid>https://arxiv.org/abs/2510.18123</guid>
<content:encoded><![CDATA[
arXiv:2510.18123v1 Announce Type: new 
Abstract: Collaborative driving systems leverage vehicle-to-everything (V2X) communication across multiple agents to enhance driving safety and efficiency. Traditional V2X systems take raw sensor data, neural features, or perception results as communication media, which face persistent challenges, including high bandwidth demands, semantic loss, and interoperability issues. Recent advances investigate natural language as a promising medium, which can provide semantic richness, decision-level reasoning, and human-machine interoperability at significantly lower bandwidth. Despite great promise, this paradigm shift also introduces new vulnerabilities within language communication, including message loss, hallucinations, semantic manipulation, and adversarial attacks. In this work, we present the first systematic study of full-stack safety and security issues in natural-language-based collaborative driving. Specifically, we develop a comprehensive taxonomy of attack strategies, including connection disruption, relay/replay interference, content spoofing, and multi-connection forgery. To mitigate these risks, we introduce an agentic defense pipeline, which we call SafeCoop, that integrates a semantic firewall, language-perception consistency checks, and multi-source consensus, enabled by an agentic transformation function for cross-frame spatial alignment. We systematically evaluate SafeCoop in closed-loop CARLA simulation across 32 critical scenarios, achieving 69.15% driving score improvement under malicious attacks and up to 67.32% F1 score for malicious detection. This study provides guidance for advancing research on safe, secure, and trustworthy language-driven collaboration in transportation systems. Our project page is https://xiangbogaobarry.github.io/SafeCoop.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BlueCodeAgent: A Blue Teaming Agent Enabled by Automated Red Teaming for CodeGen AI</title>
<link>https://arxiv.org/abs/2510.18131</link>
<guid>https://arxiv.org/abs/2510.18131</guid>
<content:encoded><![CDATA[
arXiv:2510.18131v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly used for code generation, concerns over the security risks have grown substantially. Early research has primarily focused on red teaming, which aims to uncover and evaluate vulnerabilities and risks of CodeGen models. However, progress on the blue teaming side remains limited, as developing defense requires effective semantic understanding to differentiate the unsafe from the safe. To fill in this gap, we propose BlueCodeAgent, an end-to-end blue teaming agent enabled by automated red teaming. Our framework integrates both sides: red teaming generates diverse risky instances, while the blue teaming agent leverages these to detect previously seen and unseen risk scenarios through constitution and code analysis with agentic integration for multi-level defense. Our evaluation across three representative code-related tasks--bias instruction detection, malicious instruction detection, and vulnerable code detection--shows that BlueCodeAgent achieves significant gains over the base models and safety prompt-based defenses. In particular, for vulnerable code detection tasks, BlueCodeAgent integrates dynamic analysis to effectively reduce false positives, a challenging problem as base models tend to be over-conservative, misclassifying safe code as unsafe. Overall, BlueCodeAgent achieves an average 12.7\% F1 score improvement across four datasets in three tasks, attributed to its ability to summarize actionable constitutions that enhance context-aware risk detection. We demonstrate that the red teaming benefits the blue teaming by continuously identifying new vulnerabilities to enhance defense performance.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>World-in-World: World Models in a Closed-Loop World</title>
<link>https://arxiv.org/abs/2510.18135</link>
<guid>https://arxiv.org/abs/2510.18135</guid>
<content:encoded><![CDATA[
arXiv:2510.18135v1 Announce Type: new 
Abstract: Generative world models (WMs) can now simulate worlds with striking visual realism, which naturally raises the question of whether they can endow embodied agents with predictive perception for decision making. Progress on this question has been limited by fragmented evaluation: most existing benchmarks adopt open-loop protocols that emphasize visual quality in isolation, leaving the core issue of embodied utility unresolved, i.e., do WMs actually help agents succeed at embodied tasks? To address this gap, we introduce World-in-World, the first open platform that benchmarks WMs in a closed-loop world that mirrors real agent-environment interactions. World-in-World provides a unified online planning strategy and a standardized action API, enabling heterogeneous WMs for decision making. We curate four closed-loop environments that rigorously evaluate diverse WMs, prioritize task success as the primary metric, and move beyond the common focus on visual quality; we also present the first data scaling law for world models in embodied settings. Our study uncovers three surprises: (1) visual quality alone does not guarantee task success, controllability matters more; (2) scaling post-training with action-observation data is more effective than upgrading the pretrained video generators; and (3) allocating more inference-time compute allows WMs to substantially improve closed-loop performance.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Based Multi-Agent System for Simulating and Analyzing Marketing and Consumer Behavior</title>
<link>https://arxiv.org/abs/2510.18155</link>
<guid>https://arxiv.org/abs/2510.18155</guid>
<content:encoded><![CDATA[
arXiv:2510.18155v1 Announce Type: new 
Abstract: Simulating consumer decision-making is vital for designing and evaluating marketing strategies before costly real-world deployment. However, post-event analyses and rule-based agent-based models (ABMs) struggle to capture the complexity of human behavior and social interaction. We introduce an LLM-powered multi-agent simulation framework that models consumer decisions and social dynamics. Building on recent advances in large language model simulation in a sandbox environment, our framework enables generative agents to interact, express internal reasoning, form habits, and make purchasing decisions without predefined rules. In a price-discount marketing scenario, the system delivers actionable strategy-testing outcomes and reveals emergent social patterns beyond the reach of conventional methods. This approach offers marketers a scalable, low-risk tool for pre-implementation testing, reducing reliance on time-intensive post-event evaluations and lowering the risk of underperforming campaigns.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentChangeBench: A Multi-Dimensional Evaluation Framework for Goal-Shift Robustness in Conversational AI</title>
<link>https://arxiv.org/abs/2510.18170</link>
<guid>https://arxiv.org/abs/2510.18170</guid>
<content:encoded><![CDATA[
arXiv:2510.18170v1 Announce Type: new 
Abstract: Goal changes are a defining feature of real world multi-turn interactions, yet current agent benchmarks primarily evaluate static objectives or one-shot tool use. We introduce AgentChangeBench, a benchmark explicitly designed to measure how tool augmented language model agents adapt to mid dialogue goal shifts across three enterprise domains. Our framework formalizes evaluation through four complementary metrics: Task Success Rate (TSR) for effectiveness, Tool Use Efficiency (TUE) for reliability, Tool Call Redundancy Rate (TCRR) for wasted effort, and Goal-Shift Recovery Time (GSRT) for adaptation latency. AgentChangeBench comprises 2,835 task sequences and five user personas, each designed to trigger realistic shift points in ongoing workflows. Using this setup, we evaluate several frontier models and uncover sharp contrasts obscured by traditional $\text{pass}@k$ scores: for example, GPT-4o reaches $92.2\%$ recovery on airline booking shifts while Gemini collapses to $48.6\%$, and retail tasks show near perfect parameter validity yet redundancy rates above $80\%$, revealing major inefficiencies. These findings demonstrate that high raw accuracy does not imply robustness under dynamic goals, and that explicit measurement of recovery time and redundancy is essential. AgentChangeBench establishes a reproducible testbed for diagnosing and improving agent resilience in realistic enterprise settings.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Coopetition: Leveraging Coarse Verifier Signals for Resilient Multi-Agent LLM Reasoning</title>
<link>https://arxiv.org/abs/2510.18179</link>
<guid>https://arxiv.org/abs/2510.18179</guid>
<content:encoded><![CDATA[
arXiv:2510.18179v2 Announce Type: new 
Abstract: Inference-time computation is a critical yet challenging paradigm for enhancing the reasoning performance of large language models (LLMs). While existing strategies improve reasoning stability and consistency, they suffer from notable limitations: self-correction often reinforces the model's initial biases, and Multi-Agent Collaboration (MAC) often fails due to the lack of efficient coordination mechanisms, leading to collective errors. Although high-performing verifiers can detect reasoning errors, making them reliable requires substantial training. To address these challenges, we introduce a novel inference-time framework, Adaptive Coopetition (AdCo), in which LLM agents utilize an adaptive, UCB-based "coopetition" mechanism. At each round, agents leverage coarse verifier signals to determine whether to collaborate or compete, and iteratively refine their reasoning based on peer feedback. Without relying on high-performance verifiers, our adaptive strategy achieves significant performance gains on mathematical reasoning benchmarks, yielding a 20% relative improvement over baselines on the more challenging dataset. Our approach remains robust and consistent in terms of accuracy under different sample sizes and configurations. This adaptive, signal-guided "coopetition" framework enhances reasoning robustness by leveraging both model knowledge diversity and reasoning trace measures, while also promoting uncertainty-driven exploration, especially when participants have comparable capabilities. From this perspective, our work offers a fresh lens on inference-time computation and paves the way for more resilient multi-agent LLM systems. Our code is available at: https://github.com/AdCo-Research/adaptive-coopetition.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nash Policy Gradient: A Policy Gradient Method with Iteratively Refined Regularization for Finding Nash Equilibria</title>
<link>https://arxiv.org/abs/2510.18183</link>
<guid>https://arxiv.org/abs/2510.18183</guid>
<content:encoded><![CDATA[
arXiv:2510.18183v1 Announce Type: new 
Abstract: Finding Nash equilibria in imperfect-information games remains a central challenge in multi-agent reinforcement learning. While regularization-based methods have recently achieved last-iteration convergence to a regularized equilibrium, they require the regularization strength to shrink toward zero to approximate a Nash equilibrium, often leading to unstable learning in practice. Instead, we fix the regularization strength at a large value for robustness and achieve convergence by iteratively refining the reference policy. Our main theoretical result shows that this procedure guarantees strictly monotonic improvement and convergence to an exact Nash equilibrium in two-player zero-sum games, without requiring a uniqueness assumption. Building on this framework, we develop a practical algorithm, Nash Policy Gradient (NashPG), which preserves the generalizability of policy gradient methods while relying solely on the current and reference policies. Empirically, NashPG achieves comparable or lower exploitability than prior model-free methods on classic benchmark games and scales to large domains such as Battleship and No-Limit Texas Hold'em, where NashPG consistently attains higher Elo ratings.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Emergence of Complex Behavior in Large-Scale Ecological Environments</title>
<link>https://arxiv.org/abs/2510.18221</link>
<guid>https://arxiv.org/abs/2510.18221</guid>
<content:encoded><![CDATA[
arXiv:2510.18221v2 Announce Type: new 
Abstract: We explore how physical scale and population size shape the emergence of complex behaviors in open-ended ecological environments. In our setting, agents are unsupervised and have no explicit rewards or learning objectives but instead evolve over time according to reproduction, mutation, and natural selection. As they act, agents also shape their environment and the population around them in an ongoing dynamic ecology. Our goal is not to optimize a single high-performance policy, but instead to examine how behaviors emerge and evolve across large populations due to natural competition and environmental pressures. In an effort to discover how complex behaviors naturally emerge, we conduct experiments in large-scale worlds that reach populations of more than 60,000 individual agents, each with their own evolved neural network policy. We identify various emergent behaviors such as long-range resource extraction, vision-based foraging, and predation that arise under competitive and survival pressures. We examine how sensing modalities and environmental scale affect the emergence of these behaviors, finding that some appear only in sufficiently large environments and populations, with larger scales increasing behavioral stability and consistency. While there is a rich history of research in evolutionary settings, our scaling results provide promising new directions to explore ecology as an instrument of machine learning in an era of abundant computational resources. Experimental code is available at https://github.com/jbejjani2022/ecological-emergent-behavior.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Optimization of Cooperation Efficiency and Communication Covertness for Target Detection with AUVs</title>
<link>https://arxiv.org/abs/2510.18225</link>
<guid>https://arxiv.org/abs/2510.18225</guid>
<content:encoded><![CDATA[
arXiv:2510.18225v1 Announce Type: new 
Abstract: This paper investigates underwater cooperative target detection using autonomous underwater vehicles (AUVs), with a focus on the critical trade-off between cooperation efficiency and communication covertness. To tackle this challenge, we first formulate a joint trajectory and power control optimization problem, and then present an innovative hierarchical action management framework to solve it. According to the hierarchical formulation, at the macro level, the master AUV models the agent selection process as a Markov decision process and deploys the proximal policy optimization algorithm for strategic task allocation. At the micro level, each selected agent's decentralized decision-making is modeled as a partially observable Markov decision process, and a multi-agent proximal policy optimization algorithm is used to dynamically adjust its trajectory and transmission power based on its local observations. Under the centralized training and decentralized execution paradigm, our target detection framework enables adaptive covert cooperation while satisfying both energy and mobility constraints. By comprehensively modeling the considered system, the involved signals and tasks, as well as energy consumption, theoretical insights and practical solutions for the efficient and secure operation of multiple AUVs are provided, offering significant implications for the execution of underwater covert communication tasks.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Old Meets New: Evaluating the Impact of Regression Tests on SWE Issue Resolution</title>
<link>https://arxiv.org/abs/2510.18270</link>
<guid>https://arxiv.org/abs/2510.18270</guid>
<content:encoded><![CDATA[
arXiv:2510.18270v1 Announce Type: new 
Abstract: Test suites in real-world projects are often large and achieve high code coverage, yet they remain insufficient for detecting all bugs. The abundance of unresolved issues in open-source project trackers highlights this gap. While regression tests are typically designed to ensure past functionality is preserved in the new version, they can also serve a complementary purpose: debugging the current version. Specifically, regression tests can (1) enhance the generation of reproduction tests for newly reported issues, and (2) validate that patches do not regress existing functionality. We present TestPrune, a fully automated technique that leverages issue tracker reports and strategically reuses regression tests for both bug reproduction and patch validation.
  A key contribution of TestPrune is its ability to automatically minimize the regression suite to a small, highly relevant subset of tests. Due to the predominance of LLM-based debugging techniques, this minimization is essential as large test suites exceed context limits, introduce noise, and inflate inference costs. TestPrune can be plugged into any agentic bug repair pipeline and orthogonally improve overall performance. As a proof of concept, we show that TestPrune leads to a 6.2%-9.0% relative increase in issue reproduction rate within the Otter framework and a 9.4% - 12.9% relative increase in issue resolution rate within the Agentless framework on SWE-Bench Lite and SWE-Bench Verified benchmarks, capturing fixes that were correctly produced by agents but not submitted as final patches. Compared to the benefits, the cost overhead of using TestPrune is minimal, i.e., \$0.02 and \$0.05 per SWE-Bench instance, using GPT-4o and Claude-3.7-Sonnet models, respectively.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Allocation and Resource Scheduling Algorithms Resilient to Link Failure</title>
<link>https://arxiv.org/abs/2510.18273</link>
<guid>https://arxiv.org/abs/2510.18273</guid>
<content:encoded><![CDATA[
arXiv:2510.18273v1 Announce Type: new 
Abstract: Distributed resource allocation (DRA) is fundamental to modern networked systems, spanning applications from economic dispatch in smart grids to CPU scheduling in data centers. Conventional DRA approaches require reliable communication, yet real-world networks frequently suffer from link failures, packet drops, and communication delays due to environmental conditions, network congestion, and security threats.
  We introduce a novel resilient DRA algorithm that addresses these critical challenges, and our main contributions are as follows: (1) guaranteed constraint feasibility at all times, ensuring resource-demand balance even during algorithm termination or network disruption; (2) robust convergence despite sector-bound nonlinearities at nodes/links, accommodating practical constraints like quantization and saturation; and (3) optimal performance under merely uniformly-connected networks, eliminating the need for continuous connectivity.
  Unlike existing approaches that require persistent network connectivity and provide only asymptotic feasibility, our graph-theoretic solution leverages network percolation theory to maintain performance during intermittent disconnections. This makes it particularly valuable for mobile multi-agent systems where nodes frequently move out of communication range. Theoretical analysis and simulations demonstrate that our algorithm converges to optimal solutions despite heterogeneous time delays and substantial link failures, significantly advancing the reliability of distributed resource allocation in practical network environments.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Food4All: A Multi-Agent Framework for Real-time Free Food Discovery with Integrated Nutritional Metadata</title>
<link>https://arxiv.org/abs/2510.18289</link>
<guid>https://arxiv.org/abs/2510.18289</guid>
<content:encoded><![CDATA[
arXiv:2510.18289v1 Announce Type: new 
Abstract: Food insecurity remains a persistent public health emergency in the United States, tightly interwoven with chronic disease, mental illness, and opioid misuse. Yet despite the existence of thousands of food banks and pantries, access remains fragmented: 1) current retrieval systems depend on static directories or generic search engines, which provide incomplete and geographically irrelevant results; 2) LLM-based chatbots offer only vague nutritional suggestions and fail to adapt to real-world constraints such as time, mobility, and transportation; and 3) existing food recommendation systems optimize for culinary diversity but overlook survival-critical needs of food-insecure populations, including immediate proximity, verified availability, and contextual barriers. These limitations risk leaving the most vulnerable individuals, those experiencing homelessness, addiction, or digital illiteracy, unable to access urgently needed resources. To address this, we introduce Food4All, the first multi-agent framework explicitly designed for real-time, context-aware free food retrieval. Food4All unifies three innovations: 1) heterogeneous data aggregation across official databases, community platforms, and social media to provide a continuously updated pool of food resources; 2) a lightweight reinforcement learning algorithm trained on curated cases to optimize for both geographic accessibility and nutritional correctness; and 3) an online feedback loop that dynamically adapts retrieval policies to evolving user needs. By bridging information acquisition, semantic analysis, and decision support, Food4All delivers nutritionally annotated and guidance at the point of need. This framework establishes an urgent step toward scalable, equitable, and intelligent systems that directly support populations facing food insecurity and its compounding health risks.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Genesis: Evolving Attack Strategies for LLM Web Agent Red-Teaming</title>
<link>https://arxiv.org/abs/2510.18314</link>
<guid>https://arxiv.org/abs/2510.18314</guid>
<content:encoded><![CDATA[
arXiv:2510.18314v1 Announce Type: new 
Abstract: As large language model (LLM) agents increasingly automate complex web tasks, they boost productivity while simultaneously introducing new security risks. However, relevant studies on web agent attacks remain limited. Existing red-teaming approaches mainly rely on manually crafted attack strategies or static models trained offline. Such methods fail to capture the underlying behavioral patterns of web agents, making it difficult to generalize across diverse environments. In web agent attacks, success requires the continuous discovery and evolution of attack strategies. To this end, we propose Genesis, a novel agentic framework composed of three modules: Attacker, Scorer, and Strategist. The Attacker generates adversarial injections by integrating the genetic algorithm with a hybrid strategy representation. The Scorer evaluates the target web agent's responses to provide feedback. The Strategist dynamically uncovers effective strategies from interaction logs and compiles them into a continuously growing strategy library, which is then re-deployed to enhance the Attacker's effectiveness. Extensive experiments across various web tasks show that our framework discovers novel strategies and consistently outperforms existing attack baselines.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning</title>
<link>https://arxiv.org/abs/2510.18318</link>
<guid>https://arxiv.org/abs/2510.18318</guid>
<content:encoded><![CDATA[
arXiv:2510.18318v1 Announce Type: new 
Abstract: Geospatial data offers immense potential for understanding our planet. However, the sheer volume and diversity of this data along with its varied resolutions, timescales, and sparsity pose significant challenges for thorough analysis and interpretation. This paper introduces Earth AI, a family of geospatial AI models and agentic reasoning that enables significant advances in our ability to unlock novel and profound insights into our planet. This approach is built upon foundation models across three key domains--Planet-scale Imagery, Population, and Environment--and an intelligent Gemini-powered reasoning engine. We present rigorous benchmarks showcasing the power and novel capabilities of our foundation models and validate that when used together, they provide complementary value for geospatial inference and their synergies unlock superior predictive capabilities. To handle complex, multi-step queries, we developed a Gemini-powered agent that jointly reasons over our multiple foundation models along with large geospatial data sources and tools. On a new benchmark of real-world crisis scenarios, our agent demonstrates the ability to deliver critical and timely insights, effectively bridging the gap between raw geospatial data and actionable understanding.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InspectCoder: Dynamic Analysis-Enabled Self Repair through interactive LLM-Debugger Collaboration</title>
<link>https://arxiv.org/abs/2510.18327</link>
<guid>https://arxiv.org/abs/2510.18327</guid>
<content:encoded><![CDATA[
arXiv:2510.18327v1 Announce Type: new 
Abstract: Large Language Models (LLMs) frequently generate buggy code with complex logic errors that are challenging to diagnose. While existing LLM-based self-repair approaches conduct intensive static semantic analysis or reply on superficial execution logs, they miss the in-depth runtime behaviors that often expose bug root causes-lacking the interactive dynamic analysis capabilities that make human debugging effective. We present InspectCoder, the first agentic program repair system that empowers LLMs to actively conduct dynamic analysis via interactive debugger control. Our dual-agent framework enables strategic breakpoint placement, targeted state inspection, and incremental runtime experimentation within stateful debugger sessions. Unlike existing methods that follow fixed log collection procedures, InspectCoder adaptively inspects and perturbs relevant intermediate states at runtime, and leverages immediate process rewards from debugger feedback to guide multi-step reasoning, transforming LLM debugging paradigm from blind trial-and-error into systematic root cause diagnosis. We conduct comprehensive experiments on two challenging self-repair benchmarks: BigCodeBench-R and LiveCodeBench-R. InspectCoder achieves 5.10%-60.37% relative improvements in repair accuracy over the strongest baseline, while delivering 1.67x-2.24x superior bug-fix efficiency respectively. We also contribute InspectWare, an open-source middleware that abstracts debugger complexities and maintains stateful debugging sessions across mainstream Python testing frameworks. Our work provides actionable insight into the interactive LLM-debugger systems, demonstrating the significant potential of LLM-driven dynamic analysis for automated software engineering.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory-Augmented State Machine Prompting: A Novel LLM Agent Framework for Real-Time Strategy Games</title>
<link>https://arxiv.org/abs/2510.18395</link>
<guid>https://arxiv.org/abs/2510.18395</guid>
<content:encoded><![CDATA[
arXiv:2510.18395v1 Announce Type: new 
Abstract: This paper proposes Memory-Augmented State Machine Prompting (MASMP), a novel framework for LLM agents in real-time strategy games. Addressing key challenges like hallucinations and fragmented decision-making in existing approaches, MASMP integrates state machine prompting with memory mechanisms to unify structured actions with long-term tactical coherence. The framework features: (1) a natural language-driven state machine architecture that guides LLMs to emulate finite state machines and behavior trees through prompts, and (2) a lightweight memory module preserving strategic variables (e.g., tactics, priority units) across decision cycles. Experiments in StarCraft II demonstrate MASMP's 60% win rate against the hardest built-in AI (Lv7), vastly outperforming baselines (0%). Case studies reveal the method retains LLMs' semantic comprehension while resolving the "Knowing-Doing Gap" through strict state-action mapping, achieving both interpretability and FSM-like reliability. This work establishes a new paradigm for combining neural and symbolic AI in complex decision-making.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterogeneous Adversarial Play in Interactive Environments</title>
<link>https://arxiv.org/abs/2510.18407</link>
<guid>https://arxiv.org/abs/2510.18407</guid>
<content:encoded><![CDATA[
arXiv:2510.18407v1 Announce Type: new 
Abstract: Self-play constitutes a fundamental paradigm for autonomous skill acquisition, whereby agents iteratively enhance their capabilities through self-directed environmental exploration. Conventional self-play frameworks exploit agent symmetry within zero-sum competitive settings, yet this approach proves inadequate for open-ended learning scenarios characterized by inherent asymmetry. Human pedagogical systems exemplify asymmetric instructional frameworks wherein educators systematically construct challenges calibrated to individual learners' developmental trajectories. The principal challenge resides in operationalizing these asymmetric, adaptive pedagogical mechanisms within artificial systems capable of autonomously synthesizing appropriate curricula without predetermined task hierarchies. Here we present Heterogeneous Adversarial Play (HAP), an adversarial Automatic Curriculum Learning framework that formalizes teacher-student interactions as a minimax optimization wherein task-generating instructor and problem-solving learner co-evolve through adversarial dynamics. In contrast to prevailing ACL methodologies that employ static curricula or unidirectional task selection mechanisms, HAP establishes a bidirectional feedback system wherein instructors continuously recalibrate task complexity in response to real-time learner performance metrics. Experimental validation across multi-task learning domains demonstrates that our framework achieves performance parity with SOTA baselines while generating curricula that enhance learning efficacy in both artificial agents and human subjects.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On AI Verification in Open RAN</title>
<link>https://arxiv.org/abs/2510.18417</link>
<guid>https://arxiv.org/abs/2510.18417</guid>
<content:encoded><![CDATA[
arXiv:2510.18417v1 Announce Type: new 
Abstract: Open RAN introduces a flexible, cloud-based architecture for the Radio Access Network (RAN), enabling Artificial Intelligence (AI)/Machine Learning (ML)-driven automation across heterogeneous, multi-vendor deployments. While EXplainable Artificial Intelligence (XAI) helps mitigate the opacity of AI models, explainability alone does not guarantee reliable network operations. In this article, we propose a lightweight verification approach based on interpretable models to validate the behavior of Deep Reinforcement Learning (DRL) agents for RAN slicing and scheduling in Open RAN. Specifically, we use Decision Tree (DT)-based verifiers to perform near-real-time consistency checks at runtime, which would be otherwise unfeasible with computationally expensive state-of-the-art verifiers. We analyze the landscape of XAI and AI verification, propose a scalable architectural integration, and demonstrate feasibility with a DT-based slice-verifier. We also outline future challenges to ensure trustworthy AI adoption in Open RAN.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Med-VRAgent: A Framework for Medical Visual Reasoning-Enhanced Agents</title>
<link>https://arxiv.org/abs/2510.18424</link>
<guid>https://arxiv.org/abs/2510.18424</guid>
<content:encoded><![CDATA[
arXiv:2510.18424v1 Announce Type: new 
Abstract: Visual Language Models (VLMs) achieve promising results in medical reasoning but struggle with hallucinations, vague descriptions, inconsistent logic and poor localization. To address this, we propose a agent framework named Medical Visual Reasoning Agent (\textbf{Med-VRAgent}). The approach is based on Visual Guidance and Self-Reward paradigms and Monte Carlo Tree Search (MCTS). By combining the Visual Guidance with tree search, Med-VRAgent improves the medical visual reasoning capabilities of VLMs. We use the trajectories collected by Med-VRAgent as feedback to further improve the performance by fine-tuning the VLMs with the proximal policy optimization (PPO) objective. Experiments on multiple medical VQA benchmarks demonstrate that our method outperforms existing approaches.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeLoad: Demand-Driven Short-Video Preloading with Scalable Watch-Time Estimation</title>
<link>https://arxiv.org/abs/2510.18459</link>
<guid>https://arxiv.org/abs/2510.18459</guid>
<content:encoded><![CDATA[
arXiv:2510.18459v1 Announce Type: new 
Abstract: Short video streaming has become a dominant paradigm in digital media, characterized by rapid swiping interactions and diverse media content. A key technical challenge is designing an effective preloading strategy that dynamically selects and prioritizes download tasks from an evolving playlist, balancing Quality of Experience (QoE) and bandwidth efficiency under practical commercial constraints. However, real world analysis reveals critical limitations of existing approaches: (1) insufficient adaptation of download task sizes to dynamic conditions, and (2) watch time prediction models that are difficult to deploy reliably at scale. In this paper, we propose DeLoad, a novel preloading framework that addresses these issues by introducing dynamic task sizing and a practical, multi dimensional watch time estimation method. Additionally, a Deep Reinforcement Learning (DRL) enhanced agent is trained to optimize the download range decisions adaptively. Extensive evaluations conducted on an offline testing platform, leveraging massive real world network data, demonstrate that DeLoad achieves significant improvements in QoE metrics (34.4% to 87.4% gain). Furthermore, after deployment on a large scale commercial short video platform, DeLoad has increased overall user watch time by 0.09% while simultaneously reducing rebuffering events and 3.76% bandwidth consumption.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Modeling of Intentions in Socially Intelligent LLM Agents</title>
<link>https://arxiv.org/abs/2510.18476</link>
<guid>https://arxiv.org/abs/2510.18476</guid>
<content:encoded><![CDATA[
arXiv:2510.18476v1 Announce Type: new 
Abstract: We present a probabilistic intent modeling framework for large language model (LLM) agents in multi-turn social dialogue. The framework maintains a belief distribution over a partner's latent intentions, initialized from contextual priors and dynamically updated through likelihood estimation after each utterance. The evolving distribution provides additional contextual grounding for the policy, enabling adaptive dialogue strategies under uncertainty. Preliminary experiments in the SOTOPIA environment show consistent improvements: the proposed framework increases the Overall score by 9.0% on SOTOPIA-All and 4.1% on SOTOPIA-Hard compared with the Qwen2.5-7B baseline, and slightly surpasses an oracle agent that directly observes partner intentions. These early results suggest that probabilistic intent modeling can contribute to the development of socially intelligent LLM agents.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LAFA: Agentic LLM-Driven Federated Analytics over Decentralized Data Sources</title>
<link>https://arxiv.org/abs/2510.18477</link>
<guid>https://arxiv.org/abs/2510.18477</guid>
<content:encoded><![CDATA[
arXiv:2510.18477v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown great promise in automating data analytics tasks by interpreting natural language queries and generating multi-operation execution plans. However, existing LLM-agent-based analytics frameworks operate under the assumption of centralized data access, offering little to no privacy protection. In contrast, federated analytics (FA) enables privacy-preserving computation across distributed data sources, but lacks support for natural language input and requires structured, machine-readable queries. In this work, we present LAFA, the first system that integrates LLM-agent-based data analytics with FA. LAFA introduces a hierarchical multi-agent architecture that accepts natural language queries and transforms them into optimized, executable FA workflows. A coarse-grained planner first decomposes complex queries into sub-queries, while a fine-grained planner maps each subquery into a Directed Acyclic Graph of FA operations using prior structural knowledge. To improve execution efficiency, an optimizer agent rewrites and merges multiple DAGs, eliminating redundant operations and minimizing computational and communicational overhead. Our experiments demonstrate that LAFA consistently outperforms baseline prompting strategies by achieving higher execution plan success rates and reducing resource-intensive FA operations by a substantial margin. This work establishes a practical foundation for privacy-preserving, LLM-driven analytics that supports natural language input in the FA setting.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe But Not Sorry: Reducing Over-Conservatism in Safety Critics via Uncertainty-Aware Modulation</title>
<link>https://arxiv.org/abs/2510.18478</link>
<guid>https://arxiv.org/abs/2510.18478</guid>
<content:encoded><![CDATA[
arXiv:2510.18478v1 Announce Type: new 
Abstract: Ensuring the safe exploration of reinforcement learning (RL) agents is critical for deployment in real-world systems. Yet existing approaches struggle to strike the right balance: methods that tightly enforce safety often cripple task performance, while those that prioritize reward leave safety constraints frequently violated, producing diffuse cost landscapes that flatten gradients and stall policy improvement. We introduce the Uncertain Safety Critic (USC), a novel approach that integrates uncertainty-aware modulation and refinement into critic training. By concentrating conservatism in uncertain and costly regions while preserving sharp gradients in safe areas, USC enables policies to achieve effective reward-safety trade-offs. Extensive experiments show that USC reduces safety violations by approximately 40% while maintaining competitive or higher rewards, and reduces the error between predicted and true cost gradients by approximately 83%, breaking the prevailing trade-off between safety and performance and paving the way for scalable safe RL.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StarBench: A Turn-Based RPG Benchmark for Agentic Multimodal Decision-Making and Information Seeking</title>
<link>https://arxiv.org/abs/2510.18483</link>
<guid>https://arxiv.org/abs/2510.18483</guid>
<content:encoded><![CDATA[
arXiv:2510.18483v1 Announce Type: new 
Abstract: Human players do more than press buttons: they ground what they see on screen into precise keyboard-mouse actions and, when stuck, they seek information before trying again. We ask whether current vision-language models (VLMs) can do the same. Despite encouraging results under simplified control or tool scaffolds, human-like play in a real client - mapping raw screenshots to temporally coherent low-level actions while deciding when to ask for guidance - remains an open challenge. We introduce StarBench, a turn-based RPG benchmark derived from Honkai: Star Rail that targets these two human-like competencies: multimodal decision-making from pixels to actions and agentic information seeking. StarBench standardizes evaluation across eight combat tasks and two regimes with shared tasks and metrics: (i) direct control, where agents receive only screenshots and must emit low-level primitives (click and keypress) with no semantic hints; and (ii) tool-assisted control, where higher-level intents can be mapped to primitives by detectors and OCR outputs provide optional textualized observations to ease UI grounding. To mirror human practice, StarBench also includes an ask-or-act diagnostic that measures whether and when agents choose to request brief guidance before proceeding, and how that choice affects subsequent performance. We report reference baselines for contemporary VLMs and a human reference. Results expose sizable gaps in perception-to-control fidelity in the direct regime, while showing that judicious information seeking correlates with improved success, establishing StarBench as a reproducible yardstick for agentic information seeking and multimodal decision-making in real-client play.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AndroidControl-Curated: Revealing the True Potential of GUI Agents through Benchmark Purification</title>
<link>https://arxiv.org/abs/2510.18488</link>
<guid>https://arxiv.org/abs/2510.18488</guid>
<content:encoded><![CDATA[
arXiv:2510.18488v1 Announce Type: new 
Abstract: On-device virtual assistants like Siri and Google Assistant are increasingly pivotal, yet their capabilities are hamstrung by a reliance on rigid, developer-dependent APIs. GUI agents offer a powerful, API-independent alternative, but their adoption is hindered by the perception of poor performance, as even the best models (e.g. Qwen3-VL-235B) scores are capped at around 60% on benchmarks like AndroidControl, far from viability for real-world use. Our research reveals that issue lies not only with the models but with the benchmarks themselves. We identified notable shortcomings in AndroidControl, including ambiguities and factual errors, which systematically underrates agent capabilities. To address this critical oversight, we enhanced AndroidControl into AndroidControl-Curated, a refined version of the benchmark improved through a rigorous purification pipeline. On this enhanced benchmark, state-of-the-art models achieve success rates nearing 75% on complex tasks (15% improvement), reflecting that on-device GUI agents are actually closer to practical deployment than previously thought. We introduce our new SOTA model, Magma-R1- 3B, post-trained on just 2.4k curated samples using 60 hours of an H20 GPU (approximately $60). Despite being 200 times smaller in parameters, this model delivers performance comparable to Qwen3- VL-235B. We release both AndroidControl-Curated benchmark and Magma-R1 model to the research community, encouraging adoption of this enhanced benchmark to better reflect model capabilities and accelerate the development of robust, on-device virtual assistants.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crucible: Quantifying the Potential of Control Algorithms through LLM Agents</title>
<link>https://arxiv.org/abs/2510.18491</link>
<guid>https://arxiv.org/abs/2510.18491</guid>
<content:encoded><![CDATA[
arXiv:2510.18491v1 Announce Type: new 
Abstract: Control algorithms in production environments typically require domain experts to tune their parameters and logic for specific scenarios. However, existing research predominantly focuses on algorithmic performance under ideal or default configurations, overlooking the critical aspect of Tuning Potential. To bridge this gap, we introduce Crucible, an agent that employs an LLM-driven, multi-level expert simulation to turn algorithms and defines a formalized metric to quantitatively evaluate their Tuning Potential. We demonstrate Crucible's effectiveness across a wide spectrum of case studies, from classic control tasks to complex computer systems, and validate its findings in a real-world deployment. Our experimental results reveal that Crucible systematically quantifies the tunable space across different algorithms. Furthermore, Crucible provides a new dimension for algorithm analysis and design, which ultimately leads to performance improvements. Our code is available at https://github.com/thu-media/Crucible.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VAPU: System for Autonomous Legacy Code Modernization</title>
<link>https://arxiv.org/abs/2510.18509</link>
<guid>https://arxiv.org/abs/2510.18509</guid>
<content:encoded><![CDATA[
arXiv:2510.18509v1 Announce Type: new 
Abstract: In this study, we present a solution for the modernization of legacy applications, an area of code generation where LLM-based multi-agent systems are proving essential for complex multi-phased tasks. Legacy applications often contain deprecated components that create compatibility, security, and reliability risks, but high resource costs make companies hesitate to update. We take a step forward to integrate an LLM-based multi-agent system as part of a legacy web application update to provide a cost-effective solution to update legacy applications autonomously. We propose a multi-agent system named a Verifying Agent Pipeline Updater (VAPU), which is designed to update code files in phases while simulating different roles in a software development team. In our previous study, we evaluated the system for legacy version updates by using six legacy web application view files by resulting errors and accomplished requirements. This study extends the previous evaluation of a multi-agent pipeline system by extending the evaluation of VAPU from a single LLM to five LLMs and using the temperature parameter in both 0 to 1 settings. Additionally, we tested the system with 20 open-source Python GitHub projects. The results of the evaluation were compared to Zero-Shot Learning (ZSL) and One-Shot Learning (OSL) prompts. The extended evaluation of VAPU showed that particularly in a low-temperature VAPU can get similar level of error count compared to the ZSL/OSL prompts but with a higher level of fulfilled requirements, depending on the LLM. VAPU showed up to 22.5% increase in the succeeding Python file update requirements compared to ZSL/OSL prompts. The study indicates that an LLM-based multi-agent system is a capable solution to update components of a legacy application autonomously.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Socialized Learning and Emergent Behaviors in Multi-Agent Systems based on Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2510.18515</link>
<guid>https://arxiv.org/abs/2510.18515</guid>
<content:encoded><![CDATA[
arXiv:2510.18515v1 Announce Type: new 
Abstract: This search introduces the Multimodal Socialized Learning Framework (M-S2L), designed to foster emergent social intelligence in AI agents by integrating Multimodal Large Language Models (M-LLMs) with social learning mechanisms. The framework equips agents with multimodal perception (vision and text) and structured action capabilities, enabling physical manipulation and grounded multimodal communication (e.g., text with visual pointers). M-S2L combines direct reinforcement learning with two novel social learning pathways: multimodal observational learning and communication-driven learning from feedback, augmented by an episodic memory system for long-term social context.
  We evaluate M-S2L in a Collaborative Assembly Environment (CAE), where agent teams must construct complex devices from ambiguous blueprints under informational asymmetry. Across tasks of increasing complexity, M-S2L agents consistently outperform Text-Only and No-Social-Learning baselines in Task Completion Rate and Time to Completion, particularly in dynamic problem-solving scenarios. Ablation studies confirm the necessity of both multimodality and socialized learning. Our analysis reveals the emergence of efficient communication protocols integrating visual pointers with concise text, alongside rapid role specialization leading to stable labor division. Qualitative case studies demonstrate agents' abilities for shared awareness, dynamic re-planning, and adaptive problem-solving, suggesting a nascent form of machine social cognition. These findings indicate that integrating multimodal perception with explicit social learning is critical for developing human-like collaborative intelligence in multi-agent systems.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EfficientNav: Towards On-Device Object-Goal Navigation with Navigation Map Caching and Retrieval</title>
<link>https://arxiv.org/abs/2510.18546</link>
<guid>https://arxiv.org/abs/2510.18546</guid>
<content:encoded><![CDATA[
arXiv:2510.18546v1 Announce Type: new 
Abstract: Object-goal navigation (ObjNav) tasks an agent with navigating to the location of a specific object in an unseen environment. Embodied agents equipped with large language models (LLMs) and online constructed navigation maps can perform ObjNav in a zero-shot manner. However, existing agents heavily rely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small LLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to limited model capacity for understanding complex navigation maps, which prevents deploying ObjNav on local devices. At the same time, the long prompt introduced by the navigation map description will cause high planning latency on local devices. In this paper, we propose EfficientNav to enable on-device efficient LLM-based zero-shot ObjNav. To help the smaller LLMs better understand the environment, we propose semantics-aware memory retrieval to prune redundant information in navigation maps. To reduce planning latency, we propose discrete memory caching and attention-based memory clustering to efficiently save and re-use the KV cache. Extensive experimental results demonstrate that EfficientNav achieves 11.1% improvement in success rate on HM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time latency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our code will be released soon.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JAUNT: Joint Alignment of User Intent and Network State for QoE-centric LLM Tool Routing</title>
<link>https://arxiv.org/abs/2510.18550</link>
<guid>https://arxiv.org/abs/2510.18550</guid>
<content:encoded><![CDATA[
arXiv:2510.18550v1 Announce Type: new 
Abstract: Large Language Models (LLMs) increasingly rely on emerging protocols such as the Model Context Protocol (MCP) to invoke external tools and services. However, current tool routing mechanisms remain fragile because they only consider functional matching between users' queries and tools. In practice, user intent expressed through queries can be vague or underspecified, and the actual Quality of Experience (QoE) also depends on external factors such as link latency and server availability that are not captured by semantics alone. To address this challenge, we propose JAUNT, a framework for Joint Alignment of User intent and Network state in QoE-centric Tool routing. JAUNT introduces a dual-view alignment strategy that interprets user intent while employing LLM agents to construct network profiles, mapping numerical performance indicators into the semantic space to guide routing. We further design a benchmark that integrates diverse user request patterns with heterogeneous network states, enabling systematic evaluation of QoE outcomes. Experimental results show that JAUNT significantly improves QoE compared with several baselines, demonstrating the importance of aligning both intent and network state for scalable LLM service orchestration.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOCIA-Nabla: Textual Gradient Meets Multi-Agent Orchestration for Automated Simulator Generation</title>
<link>https://arxiv.org/abs/2510.18551</link>
<guid>https://arxiv.org/abs/2510.18551</guid>
<content:encoded><![CDATA[
arXiv:2510.18551v1 Announce Type: new 
Abstract: In this paper, we present SOCIA-Nabla, an end-to-end, agentic framework that treats simulator construction asinstance optimization over code within a textual computation graph. Specialized LLM-driven agents are embedded as graph nodes, and a workflow manager executes a loss-driven loop: code synthesis -> execution -> evaluation -> code repair. The optimizer performs Textual-Gradient Descent (TGD), while human-in-the-loop interaction is reserved for task-spec confirmation, minimizing expert effort and keeping the code itself as the trainable object. Across three CPS tasks, i.e., User Modeling, Mask Adoption, and Personal Mobility, SOCIA-Nabla attains state-of-the-art overall accuracy. By unifying multi-agent orchestration with a loss-aligned optimization view, SOCIA-Nabla converts brittle prompt pipelines into reproducible, constraint-aware simulator code generation that scales across domains and simulation granularities. This work is under review, and we will release the code soon.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebDevJudge: Evaluating (M)LLMs as Critiques for Web Development Quality</title>
<link>https://arxiv.org/abs/2510.18560</link>
<guid>https://arxiv.org/abs/2510.18560</guid>
<content:encoded><![CDATA[
arXiv:2510.18560v1 Announce Type: new 
Abstract: The paradigm of LLM-as-a-judge is emerging as a scalable and efficient alternative to human evaluation, demonstrating strong performance on well-defined tasks. However, its reliability in open-ended tasks with dynamic environments and complex interactions remains unexplored. To bridge the gap, we introduce WebDevJudge, a systematic benchmark for assessing LLM-as-a-judge performance in web development, with support for both non-interactive evaluation based on static observations and continuous interactive evaluation with a dynamic web environment. WebDevJudge comprises human preference labels over paired web implementations, annotated with structured and query-grounded rubrics to ensure high-quality ground truth. Using this benchmark, we comprehensively evaluate various evaluators, including LLMs, MLLMs, and agentic workflows. We systematically investigate the impact of different paradigms and guidance mechanisms. Our experiments reveal a significant gap between LLM judges and human experts. In-depth analysis indicates this gap stems from fundamental model limitations, including failures in recognizing functional equivalence, verifying task feasibility, and mitigating bias. Overall, WebDevJudge presents a significant challenge to LLM-as-a-judge, offering insights to guide future research toward developing more reliable and capable automated evaluators for complicated scenarios. Code and data are available at https://github.com/lcy2723/WebDevJudge.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Trust Paradox in LLM-Based Multi-Agent Systems: When Collaboration Becomes a Security Vulnerability</title>
<link>https://arxiv.org/abs/2510.18563</link>
<guid>https://arxiv.org/abs/2510.18563</guid>
<content:encoded><![CDATA[
arXiv:2510.18563v1 Announce Type: new 
Abstract: Multi-agent systems powered by large language models are advancing rapidly, yet the tension between mutual trust and security remains underexplored. We introduce and empirically validate the Trust-Vulnerability Paradox (TVP): increasing inter-agent trust to enhance coordination simultaneously expands risks of over-exposure and over-authorization. To investigate this paradox, we construct a scenario-game dataset spanning 3 macro scenes and 19 sub-scenes, and run extensive closed-loop interactions with trust explicitly parameterized. Using Minimum Necessary Information (MNI) as the safety baseline, we propose two unified metrics: Over-Exposure Rate (OER) to detect boundary violations, and Authorization Drift (AD) to capture sensitivity to trust levels. Results across multiple model backends and orchestration frameworks reveal consistent trends: higher trust improves task success but also heightens exposure risks, with heterogeneous trust-to-risk mappings across systems. We further examine defenses such as Sensitive Information Repartitioning and Guardian-Agent enablement, both of which reduce OER and attenuate AD. Overall, this study formalizes TVP, establishes reproducible baselines with unified metrics, and demonstrates that trust must be modeled and scheduled as a first-class security variable in multi-agent system design.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextual Search in Principal-Agent Games: The Curse of Degeneracy</title>
<link>https://arxiv.org/abs/2510.18567</link>
<guid>https://arxiv.org/abs/2510.18567</guid>
<content:encoded><![CDATA[
arXiv:2510.18567v1 Announce Type: new 
Abstract: In this work, we introduce and study contextual search in general principal-agent games, where a principal repeatedly interacts with agents by offering contracts based on contextual information and historical feedback, without knowing the agents' true costs or rewards. Our model generalizes classical contextual pricing by accommodating richer agent action spaces. Over $T$ rounds with $d$-dimensional contexts, we establish an asymptotically tight exponential $T^{1 - \Theta(1/d)}$ bound in terms of the pessimistic Stackelberg regret, benchmarked against the best utility for the principal that is consistent with the observed feedback.
  We also establish a lower bound of $\Omega(T^{\frac{1}{2}-\frac{1}{2d}})$ on the classic Stackelberg regret for principal-agent games, demonstrating a surprising double-exponential hardness separation from the contextual pricing problem (a.k.a, the principal-agent game with two actions), which is known to admit a near-optimal $O(d\log\log T)$ regret bound [Kleinberg and Leighton, 2003, Leme and Schneider, 2018, Liu et al., 2021]. In particular, this double-exponential hardness separation occurs even in the special case with three actions and two-dimensional context. We identify that this significant increase in learning difficulty arises from a structural phenomenon that we call contextual action degeneracy, where adversarially chosen contexts can make some actions strictly dominated (and hence unincentivizable), blocking the principal's ability to explore or learn about them, and fundamentally limiting learning progress.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuantEvolve: Automating Quantitative Strategy Discovery through Multi-Agent Evolutionary Framework</title>
<link>https://arxiv.org/abs/2510.18569</link>
<guid>https://arxiv.org/abs/2510.18569</guid>
<content:encoded><![CDATA[
arXiv:2510.18569v1 Announce Type: new 
Abstract: Automating quantitative trading strategy development in dynamic markets is challenging, especially with increasing demand for personalized investment solutions. Existing methods often fail to explore the vast strategy space while preserving the diversity essential for robust performance across changing market conditions. We present QuantEvolve, an evolutionary framework that combines quality-diversity optimization with hypothesis-driven strategy generation. QuantEvolve employs a feature map aligned with investor preferences, such as strategy type, risk profile, turnover, and return characteristics, to maintain a diverse set of effective strategies. It also integrates a hypothesis-driven multi-agent system to systematically explore the strategy space through iterative generation and evaluation. This approach produces diverse, sophisticated strategies that adapt to both market regime shifts and individual investment needs. Empirical results show that QuantEvolve outperforms conventional baselines, validating its effectiveness. We release a dataset of evolved strategies to support future research.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLASP: Cost-Optimized LLM-based Agentic System for Phishing Detection</title>
<link>https://arxiv.org/abs/2510.18585</link>
<guid>https://arxiv.org/abs/2510.18585</guid>
<content:encoded><![CDATA[
arXiv:2510.18585v1 Announce Type: new 
Abstract: Phishing websites remain a significant cybersecurity threat, necessitating accurate and cost-effective detection mechanisms. In this paper, we present CLASP, a novel system that effectively identifies phishing websites by leveraging multiple intelligent agents, built using large language models (LLMs), to analyze different aspects of a web resource. The system processes URLs or QR codes, employing specialized LLM-based agents that evaluate the URL structure, webpage screenshot, and HTML content to predict potential phishing threats. To optimize performance while minimizing operational costs, we experimented with multiple combination strategies for agent-based analysis, ultimately designing a strategic combination that ensures the per-website evaluation expense remains minimal without compromising detection accuracy. We tested various LLMs, including Gemini 1.5 Flash and GPT-4o mini, to build these agents and found that Gemini 1.5 Flash achieved the best performance with an F1 score of 83.01% on a newly curated dataset. Also, the system maintained an average processing time of 2.78 seconds per website and an API cost of around $3.18 per 1,000 websites. Moreover, CLASP surpasses leading previous solutions, achieving over 40% higher recall and a 20% improvement in F1 score for phishing detection on the collected dataset. To support further research, we have made our dataset publicly available, supporting the development of more advanced phishing detection systems.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tokencake: A KV-Cache-centric Serving Framework for LLM-based Multi-Agent Applications</title>
<link>https://arxiv.org/abs/2510.18586</link>
<guid>https://arxiv.org/abs/2510.18586</guid>
<content:encoded><![CDATA[
arXiv:2510.18586v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly deployed in complex multi-agent applications that use external function calls. This workload creates severe performance challenges for the KV Cache: space contention leads to the eviction of critical agents' caches and time underutilization leaves the cache of agents stalled on long-running tool calls idling in GPU memory. We present Tokencake, a KV-Cache-centric serving framework that co-optimizes scheduling and memory management with an agent-aware design. Tokencake's Space Scheduler uses dynamic memory partitioning to shield critical agents from contention, while its Time Scheduler employs a proactive offload and predictive upload mechanism to repurpose GPU memory during function call stalls. Our evaluation on representative multi-agent benchmarks shows that Tokencake can reduce end-to-end latency by over 47.06%, improve effective GPU memory utilization by up to 16.9% compared to vLLM.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CUARewardBench: A Benchmark for Evaluating Reward Models on Computer-using Agent</title>
<link>https://arxiv.org/abs/2510.18596</link>
<guid>https://arxiv.org/abs/2510.18596</guid>
<content:encoded><![CDATA[
arXiv:2510.18596v1 Announce Type: new 
Abstract: Computer-using agents (CUAs) enable task completion through natural interaction with operating systems and software interfaces. While script-based verifiers are widely adopted for evaluation, they suffer from limited scalability and inability to provide step-wise assessment. Reward models offer promising alternatives, but their effectiveness on CUA evaluation remains largely underexplored. To address this gap, we present CUARewardBench, comprising four key contributions: (1) First-ever Comprehensive CUA Reward Benchmark: We introduce the first benchmark for evaluating both outcome reward models (ORM) and process reward models (PRM) on CUA tasks, enabling systematic assessment across trajectory-level and step-level evaluation. (2) Diverse, Practical and Reliable Dataset: CUARewardBench encompasses trajectories from 10 software categories and 7 agent architectures with varying performance levels (25.9%-50.8% success rates). All trajectories are expertly annotated through carefully designed protocols, with rigorous quality control to ensure reliability and practical applicability. (3) Comprehensive Analysis and Insights: Through extensive experiments across 7 vision-language models and 3 prompt templates, we reveal critical limitations of current CUA RMs, including insufficient visual reasoning capabilities, knowledge deficiencies, and the superiority of general VLMs over specialized CUA models for reward evaluation. (4) Unanimous Prompt Ensemble (UPE): Based on the insights from our comprehensive analysis, we propose UPE, a novel ensemble method that significantly enhances reward model reliability through strict unanimous voting and strategic prompt-template configurations. UPE achieves 89.8% precision and 93.3% NPV for ORM, and 81.7% precision and 85.1% NPV for PRM, substantially outperforming single VLMs and traditional ensemble approaches.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sherlock Your Queries: Learning to Ask the Right Questions for Dialogue-Based Retrieval</title>
<link>https://arxiv.org/abs/2510.18659</link>
<guid>https://arxiv.org/abs/2510.18659</guid>
<content:encoded><![CDATA[
arXiv:2510.18659v1 Announce Type: new 
Abstract: User queries in information retrieval are often ambiguous, making it challenging for systems to identify a user's target from a single query. While recent dialogue-based interactive retrieval systems can clarify user intent, they are inefficient as they often lack an explicit strategy to ask the most informative questions. To address this limitation, we propose SherlockLLM, a dialogue-driven retrieval framework that learns an optimal questioning strategy via Reinforcement Learning (RL) and avoids the need for large-scale annotated dialogue data. In our framework, an agent is trained to generate a sequence of binary questions to efficiently narrow down the search space. To validate our approach, we introduce a benchmark with both structured and unstructured tasks. Experimental results show that SherlockLLM is a robust and efficient solution. On the structured tasks, its performance matches strong baselines and approaches the theoretical optimal defined by binary search. On the challenging unstructured task, our agent significantly outperforms these baselines, showcasing its ability to learn a highly effective information-seeking dialogue policy.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning with Imperfect Transition Predictions: A Bellman-Jensen Approach</title>
<link>https://arxiv.org/abs/2510.18687</link>
<guid>https://arxiv.org/abs/2510.18687</guid>
<content:encoded><![CDATA[
arXiv:2510.18687v1 Announce Type: new 
Abstract: Traditional reinforcement learning (RL) assumes the agents make decisions based on Markov decision processes (MDPs) with one-step transition models. In many real-world applications, such as energy management and stock investment, agents can access multi-step predictions of future states, which provide additional advantages for decision making. However, multi-step predictions are inherently high-dimensional: naively embedding these predictions into an MDP leads to an exponential blow-up in state space and the curse of dimensionality. Moreover, existing RL theory provides few tools to analyze prediction-augmented MDPs, as it typically works on one-step transition kernels and cannot accommodate multi-step predictions with errors or partial action-coverage. We address these challenges with three key innovations: First, we propose the \emph{Bayesian value function} to characterize the optimal prediction-aware policy tractably. Second, we develop a novel \emph{Bellman-Jensen Gap} analysis on the Bayesian value function, which enables characterizing the value of imperfect predictions. Third, we introduce BOLA (Bayesian Offline Learning with Online Adaptation), a two-stage model-based RL algorithm that separates offline Bayesian value learning from lightweight online adaptation to real-time predictions. We prove that BOLA remains sample-efficient even under imperfect predictions. We validate our theory and algorithm on synthetic MDPs and a real-world wind energy storage control problem.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fetch.ai: An Architecture for Modern Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2510.18699</link>
<guid>https://arxiv.org/abs/2510.18699</guid>
<content:encoded><![CDATA[
arXiv:2510.18699v1 Announce Type: new 
Abstract: Recent surges in LLM-driven intelligent systems largely overlook decades of foundational multi-agent systems (MAS) research, resulting in frameworks with critical limitations such as centralization and inadequate trust and communication protocols. This paper introduces the Fetch.ai architecture, an industrial-strength platform designed to bridge this gap by facilitating the integration of classical MAS principles with modern AI capabilities. We present a novel, multi-layered solution built on a decentralized foundation of on-chain blockchain services for verifiable identity, discovery, and transactions. This is complemented by a comprehensive development framework for creating secure, interoperable agents, a cloud-based platform for deployment, and an intelligent orchestration layer where an agent-native LLM translates high-level human goals into complex, multi-agent workflows. We demonstrate the deployed nature of this system through a decentralized logistics use case where autonomous agents dynamically discover, negotiate, and transact with one another securely. Ultimately, the Fetch.ai stack provides a principled architecture for moving beyond current agent implementations towards open, collaborative, and economically sustainable multi-agent ecosystems.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KAT-Coder Technical Report</title>
<link>https://arxiv.org/abs/2510.18779</link>
<guid>https://arxiv.org/abs/2510.18779</guid>
<content:encoded><![CDATA[
arXiv:2510.18779v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have enabled progress in agentic coding, where models autonomously reason, plan, and act within interactive software development workflows. However, bridging the gap between static text-based training and dynamic real-world agentic execution remains a core challenge. In this technical report, we present KAT-Coder, a large-scale agentic code model trained through a multi-stage curriculum encompassing Mid-Term Training, Supervised Fine-Tuning (SFT), Reinforcement Fine-Tuning (RFT), and Reinforcement-to-Deployment Adaptation. The Mid-Term stage enhances reasoning, planning, and reflection capabilities through a corpus of real software engineering data and synthetic agentic interactions. The SFT stage constructs a million-sample dataset balancing twenty programming languages, ten development contexts, and ten task archetypes. The RFT stage introduces a novel multi-ground-truth reward formulation for stable and sample-efficient policy optimization. Finally, the Reinforcement-to-Deployment phase adapts the model to production-grade IDE environments using Error-Masked SFT and Tree-Structured Trajectory Training. In summary, these stages enable KAT-Coder to achieve robust tool-use reliability, instruction alignment, and long-context reasoning, forming a deployable foundation for real-world intelligent coding agents. Our KAT series 32B model, KAT-Dev, has been open-sourced on https://huggingface.co/Kwaipilot/KAT-Dev.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebSeer: Training Deeper Search Agents through Reinforcement Learning with Self-Reflection</title>
<link>https://arxiv.org/abs/2510.18798</link>
<guid>https://arxiv.org/abs/2510.18798</guid>
<content:encoded><![CDATA[
arXiv:2510.18798v1 Announce Type: new 
Abstract: Search agents have achieved significant advancements in enabling intelligent information retrieval and decision-making within interactive environments. Although reinforcement learning has been employed to train agentic models capable of more dynamic interactive retrieval, existing methods are limited by shallow tool-use depth and the accumulation of errors over multiple iterative interactions. In this paper, we present WebSeer, a more intelligent search agent trained via reinforcement learning enhanced with a self-reflection mechanism. Specifically, we construct a large dataset annotated with reflection patterns and design a two-stage training framework that unifies cold start and reinforcement learning within the self-reflection paradigm for real-world web-based environments, which enables the model to generate longer and more reflective tool-use trajectories. Our approach substantially extends tool-use chains and improves answer accuracy. Using a single 14B model, we achieve state-of-the-art results on HotpotQA and SimpleQA, with accuracies of 72.3% and 90.0%, respectively, and demonstrate strong generalization to out-of-distribution datasets. The code is available at https://github.com/99hgz/WebSeer
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Foundations for Strategic Coopetition: Formalizing Interdependence and Complementarity</title>
<link>https://arxiv.org/abs/2510.18802</link>
<guid>https://arxiv.org/abs/2510.18802</guid>
<content:encoded><![CDATA[
arXiv:2510.18802v1 Announce Type: new 
Abstract: Modern socio-technical systems are characterized by strategic coopetition where actors simultaneously cooperate to create value and compete to capture it. While conceptual modeling languages like i* provide rich qualitative representations of strategic dependencies, they lack mechanisms for quantitative analysis of dynamic trade-offs. Conversely, classical game theory offers mathematical rigor but strips away contextual richness. This technical report bridges this gap by developing computational foundations that formalize two critical dimensions of coopetition: interdependence and complementarity. We ground interdependence in i* structural dependency analysis, translating depender-dependee-dependum relationships into quantitative interdependence coefficients through a structured translation framework. We formalize complementarity following Brandenburger and Nalebuff's Added Value concept, modeling synergistic value creation with validated parameterization. We integrate structural dependencies with bargaining power in value appropriation and introduce a game-theoretic formulation where Nash Equilibrium incorporates structural interdependence. Validation combines comprehensive experimental testing across power and logarithmic value function specifications, demonstrating functional form robustness, with empirical application to the Samsung-Sony S-LCD joint venture (2004-2011), where logarithmic specifications achieve superior empirical fit (validation score 45/60) while power functions provide theoretical tractability. This technical report serves as the foundational reference for a coordinated research program examining strategic coopetition in requirements engineering and multi-agent systems, with companion work addressing trust dynamics, team production, and reciprocity mechanisms.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Search Self-play: Pushing the Frontier of Agent Capability without Supervision</title>
<link>https://arxiv.org/abs/2510.18821</link>
<guid>https://arxiv.org/abs/2510.18821</guid>
<content:encoded><![CDATA[
arXiv:2510.18821v1 Announce Type: new 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has become the mainstream technique for training LLM agents. However, RLVR highly depends on well-crafted task queries and corresponding ground-truth answers to provide accurate rewards, which requires massive human efforts and hinders the RL scaling processes, especially under agentic scenarios. Although a few recent works explore task synthesis methods, the difficulty of generated agentic tasks can hardly be controlled to provide effective RL training advantages. To achieve agentic RLVR with higher scalability, we explore self-play training for deep search agents, in which the learning LLM utilizes multi-turn search engine calling and acts simultaneously as both a task proposer and a problem solver. The task proposer aims to generate deep search queries with well-defined ground-truth answers and increasing task difficulty. The problem solver tries to handle the generated search queries and output the correct answer predictions. To ensure that each generated search query has accurate ground truth, we collect all the searching results from the proposer's trajectory as external knowledge, then conduct retrieval-augmentation generation (RAG) to test whether the proposed query can be correctly answered with all necessary search documents provided. In this search self-play (SSP) game, the proposer and the solver co-evolve their agent capabilities through both competition and cooperation. With substantial experimental results, we find that SSP can significantly improve search agents' performance uniformly on various benchmarks without any supervision under both from-scratch and continuous RL training setups. The code is at https://github.com/Alibaba-Quark/SSP.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MADR: MPC-guided Adversarial DeepReach</title>
<link>https://arxiv.org/abs/2510.18845</link>
<guid>https://arxiv.org/abs/2510.18845</guid>
<content:encoded><![CDATA[
arXiv:2510.18845v1 Announce Type: new 
Abstract: Hamilton-Jacobi (HJ) Reachability offers a framework for generating safe value functions and policies in the face of adversarial disturbance, but is limited by the curse of dimensionality. Physics-informed deep learning is able to overcome this infeasibility, but itself suffers from slow and inaccurate convergence, primarily due to weak PDE gradients and the complexity of self-supervised learning. A few works, recently, have demonstrated that enriching the self-supervision process with regular supervision (based on the nature of the optimal control problem), greatly accelerates convergence and solution quality, however, these have been limited to single player problems and simple games. In this work, we introduce MADR: MPC-guided Adversarial DeepReach, a general framework to robustly approximate the two-player, zero-sum differential game value function. In doing so, MADR yields the corresponding optimal strategies for both players in zero-sum games as well as safe policies for worst-case robustness. We test MADR on a multitude of high-dimensional simulated and real robotic agents with varying dynamics and games, finding that our approach significantly out-performs state-of-the-art baselines in simulation and produces impressive results in hardware.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EffiReasonTrans: RL-Optimized Reasoning for Code Translation</title>
<link>https://arxiv.org/abs/2510.18863</link>
<guid>https://arxiv.org/abs/2510.18863</guid>
<content:encoded><![CDATA[
arXiv:2510.18863v1 Announce Type: new 
Abstract: Code translation is a crucial task in software development and maintenance. While recent advancements in large language models (LLMs) have improved automated code translation accuracy, these gains often come at the cost of increased inference latency, hindering real-world development workflows that involve human-in-the-loop inspection. To address this trade-off, we propose EffiReasonTrans, a training framework designed to improve translation accuracy while balancing inference latency. We first construct a high-quality reasoning-augmented dataset by prompting a stronger language model, DeepSeek-R1, to generate intermediate reasoning and target translations. Each (source code, reasoning, target code) triplet undergoes automated syntax and functionality checks to ensure reliability. Based on this dataset, we employ a two-stage training strategy: supervised fine-tuning on reasoning-augmented samples, followed by reinforcement learning to further enhance accuracy and balance inference latency. We evaluate EffiReasonTrans on six translation pairs. Experimental results show that it consistently improves translation accuracy (up to +49.2% CA and +27.8% CodeBLEU compared to the base model) while reducing the number of generated tokens (up to -19.3%) and lowering inference latency in most cases (up to -29.0%). Ablation studies further confirm the complementary benefits of the two-stage training framework. Additionally, EffiReasonTrans demonstrates improved translation accuracy when integrated into agent-based frameworks. Our code and data are available at https://github.com/DeepSoftwareAnalytics/EffiReasonTrans.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Design Assistant for the Simulation of Inertial Fusion Energy</title>
<link>https://arxiv.org/abs/2510.17830</link>
<guid>https://arxiv.org/abs/2510.17830</guid>
<content:encoded><![CDATA[
arXiv:2510.17830v2 Announce Type: cross 
Abstract: Inertial fusion energy promises nearly unlimited, clean power if it can be achieved. However, the design and engineering of fusion systems requires controlling and manipulating matter at extreme energies and timescales; the shock physics and radiation transport governing the physical behavior under these conditions are complex requiring the development, calibration, and use of predictive multiphysics codes to navigate the highly nonlinear and multi-faceted design landscape. We hypothesize that artificial intelligence reasoning models can be combined with physics codes and emulators to autonomously design fusion fuel capsules. In this article, we construct a multi-agent system where natural language is utilized to explore the complex physics regimes around fusion energy. The agentic system is capable of executing a high-order multiphysics inertial fusion computational code. We demonstrate the capacity of the multi-agent design assistant to both collaboratively and autonomously manipulate, navigate, and optimize capsule geometry while accounting for high fidelity physics that ultimately achieve simulated ignition via inverse design.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explicit Reformulation of Discrete Distributionally Robust Optimization Problems</title>
<link>https://arxiv.org/abs/2510.18302</link>
<guid>https://arxiv.org/abs/2510.18302</guid>
<content:encoded><![CDATA[
arXiv:2510.18302v1 Announce Type: cross 
Abstract: Distributionally robust optimization (DRO) is an effective framework for controlling real-world systems with various uncertainties, typically modeled using distributional uncertainty balls. However, DRO problems often involve infinitely many inequality constraints, rendering exact solutions computationally expensive. In this study, we propose a discrete DRO (DDRO) method that significantly simplifies the problem by reducing it to a single trivial constraint. Specifically, the proposed method utilizes two types of distributional uncertainty balls to reformulate the DDRO problem into a single-layer smooth convex program, significantly improving tractability. Furthermore, we provide practical guidance for selecting the appropriate ball sizes. The original DDRO problem is further reformulated into two optimization problems: one minimizing the mean and standard deviation, and the other minimizing the conditional value at risk (CVaR). These formulations account for the choice of ball sizes, thereby enhancing the practical applicability of the method. The proposed method was applied to a distributionally robust patrol-agent design problem, identifying a Pareto front in which the mean and standard deviation of the mean hitting time varied by up to 3% and 14%, respectively, while achieving a CVaR reduction of up to 13%.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models</title>
<link>https://arxiv.org/abs/2405.10255</link>
<guid>https://arxiv.org/abs/2405.10255</guid>
<content:encoded><![CDATA[
arXiv:2405.10255v2 Announce Type: replace 
Abstract: As large language models (LLMs) evolve, their integration with 3D spatial data (3D-LLMs) has seen rapid progress, offering unprecedented capabilities for understanding and interacting with physical spaces. This survey provides a comprehensive overview of the methodologies enabling LLMs to process, understand, and generate 3D data. Highlighting the unique advantages of LLMs, such as in-context learning, step-by-step reasoning, open-vocabulary capabilities, and extensive world knowledge, we underscore their potential to significantly advance spatial comprehension and interaction within embodied Artificial Intelligence (AI) systems. Our investigation spans various 3D data representations, from point clouds to Neural Radiance Fields (NeRFs). It examines their integration with LLMs for tasks such as 3D scene understanding, captioning, question-answering, and dialogue, as well as LLM-based agents for spatial reasoning, planning, and navigation. The paper also includes a brief review of other methods that integrate 3D and language. The meta-analysis presented in this paper reveals significant progress yet underscores the necessity for novel approaches to harness the full potential of 3D-LLMs. Hence, with this paper, we aim to chart a course for future research that explores and expands the capabilities of 3D-LLMs in understanding and interacting with the complex 3D world. To support this survey, we have established a project page where papers related to our topic are organized and listed: https://github.com/ActiveVisionLab/Awesome-LLM-3D.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counterfactual Effect Decomposition in Multi-Agent Sequential Decision Making</title>
<link>https://arxiv.org/abs/2410.12539</link>
<guid>https://arxiv.org/abs/2410.12539</guid>
<content:encoded><![CDATA[
arXiv:2410.12539v3 Announce Type: replace 
Abstract: We address the challenge of explaining counterfactual outcomes in multi-agent Markov decision processes. In particular, we aim to explain the total counterfactual effect of an agent's action on the outcome of a realized scenario through its influence on the environment dynamics and the agents' behavior. To achieve this, we introduce a novel causal explanation formula that decomposes the counterfactual effect by attributing to each agent and state variable a score reflecting their respective contributions to the effect. First, we show that the total counterfactual effect of an agent's action can be decomposed into two components: one measuring the effect that propagates through all subsequent agents' actions and another related to the effect that propagates through the state transitions. Building on recent advancements in causal contribution analysis, we further decompose these two effects as follows. For the former, we consider agent-specific effects -- a causal concept that quantifies the counterfactual effect of an agent's action that propagates through a subset of agents. Based on this notion, we use Shapley value to attribute the effect to individual agents. For the latter, we consider the concept of structure-preserving interventions and attribute the effect to state variables based on their "intrinsic" contributions. Through extensive experimentation, we demonstrate the interpretability of our approach in a Gridworld environment with LLM-assisted agents and a sepsis management simulator.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking LLM Uncertainty: A Multi-Agent Approach to Estimating Black-Box Model Uncertainty</title>
<link>https://arxiv.org/abs/2412.09572</link>
<guid>https://arxiv.org/abs/2412.09572</guid>
<content:encoded><![CDATA[
arXiv:2412.09572v2 Announce Type: replace 
Abstract: Quantifying uncertainty in black-box LLMs is vital for reliable responses and scalable oversight. Existing methods, which gauge a model's uncertainty through evaluating self-consistency in responses to the target query, can be misleading: an LLM may confidently provide an incorrect answer to a target query, yet give a confident and accurate answer to that same target query when answering a knowledge-preserving perturbation of the query. We systematically analyze the model behaviors and demonstrate that this discrepancy stems from suboptimal retrieval of parametric knowledge, often due to contextual biases that prevent consistent access to stored knowledge. We then introduce DiverseAgentEntropy, a novel, theoretically-grounded method employing multi-agent interaction across diverse query variations for uncertainty estimation of black-box LLMs. This approach more accurately assesses an LLM's true uncertainty and improves hallucination detection, outperforming existing self-consistency based techniques.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLMs Strategically Reveal, Conceal, and Infer Information? A Theoretical and Empirical Analysis in The Chameleon Game</title>
<link>https://arxiv.org/abs/2501.19398</link>
<guid>https://arxiv.org/abs/2501.19398</guid>
<content:encoded><![CDATA[
arXiv:2501.19398v2 Announce Type: replace 
Abstract: Large language model-based (LLM-based) agents have become common in settings that include non-cooperative parties. In such settings, agents' decision-making needs to conceal information from their adversaries, reveal information to their cooperators, and infer information to identify the other agents' characteristics. To investigate whether LLMs have these information control and decision-making capabilities, we make LLM agents play the language-based hidden-identity game, The Chameleon. In this game, a group of non-chameleon agents who do not know each other aim to identify the chameleon agent without revealing a secret. The game requires the aforementioned information control capabilities both as a chameleon and a non-chameleon. We begin with a theoretical analysis for a spectrum of strategies, from concealing to revealing, and provide bounds on the non-chameleons' winning probability. The empirical results with GPT, Gemini 2.5 Pro, Llama 3.1, and Qwen3 models show that while non-chameleon LLM agents identify the chameleon, they fail to conceal the secret from the chameleon, and their winning probability is far from the levels of even trivial strategies. Based on these empirical results and our theoretical analysis, we deduce that LLM-based agents may reveal excessive information to agents of unknown identities. Interestingly, we find that, when instructed to adopt an information-revealing level, this level is linearly encoded in the LLM's internal representations. While the instructions alone are often ineffective at making non-chameleon LLMs conceal, we show that steering the internal representations in this linear direction directly can reliably induce concealing behavior.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can We Validate Counterfactual Estimations in the Presence of General Network Interference?</title>
<link>https://arxiv.org/abs/2502.01106</link>
<guid>https://arxiv.org/abs/2502.01106</guid>
<content:encoded><![CDATA[
arXiv:2502.01106v2 Announce Type: replace 
Abstract: Randomized experiments have become a cornerstone of evidence-based decision-making in contexts ranging from online platforms to public health. However, in experimental settings with network interference, a unit's treatment can influence outcomes of other units, challenging both causal effect estimation and its validation. Classic validation approaches fail as outcomes are only observable under a single treatment scenario and exhibit complex correlation patterns due to interference. To address these challenges, we introduce a framework that facilitates the use of machine learning tools for both estimation and validation in causal inference. Central to our approach is the new distribution-preserving network bootstrap, a theoretically-grounded technique that generates multiple statistically-valid subpopulations from a single experiment's data. This amplification of experimental samples enables our second contribution: a counterfactual cross-validation procedure. This procedure adapts the principles of model validation to the unique constraints of causal settings, providing a rigorous, data-driven method for selecting and evaluating estimators. We extend recent causal message-passing developments by incorporating heterogeneous unit-level characteristics and varying local interactions, ensuring reliable finite-sample performance through non-asymptotic analysis. Additionally, we develop and publicly release a comprehensive benchmark toolbox featuring diverse experimental environments, from networks of interacting AI agents to ride-sharing applications. These environments provide known ground truth values while maintaining realistic complexities, enabling systematic evaluation of causal inference methods. Extensive testing across these environments demonstrates our method's robustness to diverse forms of network interference.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Challenges in Testing Large Language Model Based Software: A Faceted Taxonomy</title>
<link>https://arxiv.org/abs/2503.00481</link>
<guid>https://arxiv.org/abs/2503.00481</guid>
<content:encoded><![CDATA[
arXiv:2503.00481v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) and Multi-Agent LLMs (MALLMs) introduce non-determinism unlike traditional or machine learning software, requiring new approaches to verifying correctness beyond simple output comparisons or statistical accuracy over test datasets. This paper presents a taxonomy for LLM test case design, informed by research literature and our experience. Each facet is exemplified, and we conduct an LLM-assisted analysis of six open-source testing frameworks, perform a sensitivity study of an agent-based system across different model configurations, and provide working examples contrasting atomic and aggregated test cases. We identify key variation points that impact test correctness and highlight open challenges that the research, industry, and open-source communities must address as LLMs become integral to software systems. Our taxonomy defines four facets of LLM test case design, addressing ambiguity in both inputs and outputs while establishing best practices. It distinguishes variability in goals, the system under test, and inputs, and introduces two key oracle types: atomic and aggregated. Our findings reveal that current tools treat test executions as isolated events, lack explicit aggregation mechanisms, and inadequately capture variability across model versions, configurations, and repeated runs. This highlights the need for viewing correctness as a distribution of outcomes rather than a binary property, requiring closer collaboration between academia and practitioners to establish mature, variability-aware testing methodologies.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A representational framework for learning and encoding structurally enriched trajectories in complex agent environments</title>
<link>https://arxiv.org/abs/2503.13194</link>
<guid>https://arxiv.org/abs/2503.13194</guid>
<content:encoded><![CDATA[
arXiv:2503.13194v2 Announce Type: replace 
Abstract: The ability of artificial intelligence agents to make optimal decisions and generalise them to different domains and tasks is compromised in complex scenarios. One way to address this issue has focused on learning efficient representations of the world and on how the actions of agents affect them in state-action transitions. Whereas such representations are procedurally efficient, they lack structural richness. To address this problem, we propose to enhance the agent's ontology and extend the traditional conceptualisation of trajectories to provide a more nuanced view of task execution. Structurally Enriched Trajectories (SETs) extend the encoding of sequences of states and their transitions by incorporating hierarchical relations between objects, interactions, and affordances. SETs are built as multi-level graphs, providing a detailed representation of the agent dynamics and a transferable functional abstraction of the task. SETs are integrated into an architecture, Structurally Enriched Trajectory Learning and Encoding (SETLE), that employs a heterogeneous graph-based memory structure of multi-level relational dependencies essential for generalisation. We demonstrate that SETLE can support downstream tasks, enabling agents to recognise task relevant structural patterns across CREATE and MiniGrid environments. Finally, we integrate SETLE with reinforcement learning and show measurable improvements in downstream performance, including breakthrough success rates in complex, sparse-reward tasks.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrunkAgent: Stealthy Memory Corruption in LLM-Powered Recommender Agents</title>
<link>https://arxiv.org/abs/2503.23804</link>
<guid>https://arxiv.org/abs/2503.23804</guid>
<content:encoded><![CDATA[
arXiv:2503.23804v3 Announce Type: replace 
Abstract: Large language model (LLM)-powered agents are increasingly used in recommender systems (RSs) to achieve personalized behavior modeling, where the memory mechanism plays a pivotal role in enabling the agents to autonomously explore, learn and self-evolve from real-world interactions. However, this very mechanism, serving as a contextual repository, inherently exposes an attack surface for potential adversarial manipulations. Despite its central role, the robustness of agentic RSs in the face of such threats remains largely underexplored. Previous works suffer from semantic mismatches or rely on static embeddings or pre-defined prompts, all of which are not designed for dynamic systems, especially for dynamic memory states of LLM agents. This challenge is exacerbated by the black-box nature of commercial recommenders.
  To tackle the above problems, in this paper, we present the first systematic investigation of memory-based vulnerabilities in LLM-powered recommender agents, revealing their security limitations and guiding efforts to strengthen system resilience and trustworthiness. Specifically, we propose a novel black-box attack framework named DrunkAgent. DrunkAgent crafts semantically meaningful adversarial textual triggers for target item promotions and introduces a series of strategies to maximize the trigger effect by corrupting the memory updates during the interactions. The triggers and strategies are optimized on a surrogate model, enabling DrunkAgent transferable and stealthy. Extensive experiments on real-world datasets across diverse agentic RSs, including collaborative filtering, retrieval augmentation and sequential recommendations, demonstrate the generalizability, transferability and stealthiness of DrunkAgent.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Human-AI Coordination through Online Adversarial Training and Generative Models</title>
<link>https://arxiv.org/abs/2504.15457</link>
<guid>https://arxiv.org/abs/2504.15457</guid>
<content:encoded><![CDATA[
arXiv:2504.15457v4 Announce Type: replace 
Abstract: Being able to cooperate with diverse humans is an important component of many economically valuable AI tasks, from household robotics to autonomous driving. However, generalizing to novel humans requires training on data that captures the diversity of human behaviors. Adversarial training is a promising method that allows dynamic data generation and ensures that agents are robust. It creates a feedback loop where the agent's performance influences the generation of new adversarial data, which can be used immediately to train the agent. However, adversarial training is difficult to apply in a cooperative task; how can we train an adversarial cooperator? We propose a novel strategy that combines a pretrained generative model to simulate valid cooperative agent policies with adversarial training to maximize regret. We call our method GOAT: Generative Online Adversarial Training. In this framework, the GOAT dynamically searches the latent space of the generative model for coordination strategies where the learning policy, the Cooperator agent, underperforms. GOAT enables better generalization by exposing the Cooperator to various challenging interaction scenarios. We maintain realistic coordination strategies by keeping the generative model frozen, thus avoiding adversarial exploitation. We evaluate GOAT with real human partners, and the results demonstrate state of the art performance on the Overcooked benchmark, highlighting its effectiveness in generalizing to diverse human behaviors.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pretraining a Shared Q-Network for Data-Efficient Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.05701</link>
<guid>https://arxiv.org/abs/2505.05701</guid>
<content:encoded><![CDATA[
arXiv:2505.05701v2 Announce Type: replace 
Abstract: Offline reinforcement learning (RL) aims to learn a policy from a static dataset without further interactions with the environment. Collecting sufficiently large datasets for offline RL is exhausting since this data collection requires colossus interactions with environments and becomes tricky when the interaction with the environment is restricted. Hence, how an agent learns the best policy with a minimal static dataset is a crucial issue in offline RL, similar to the sample efficiency problem in online RL. In this paper, we propose a simple yet effective plug-and-play pretraining method to initialize a feature of a Q-network to enhance data efficiency in offline RL. Specifically, we introduce a shared Q-network structure that outputs predictions of the next state and Q-value. We pretrain the shared Q-network through a supervised regression task that predicts a next state and trains the shared Q-network using diverse offline RL methods. Through extensive experiments, we empirically demonstrate that our method enhances the performance of existing popular offline RL methods on the D4RL, Robomimic and V-D4RL benchmarks. Furthermore, we show that our method significantly boosts data-efficient offline RL across various data qualities and data distributions trough D4RL and ExoRL benchmarks. Notably, our method adapted with only 10% of the dataset outperforms standard algorithms even with full datasets.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Power of Matching for Online Fractional Hedonic Games</title>
<link>https://arxiv.org/abs/2505.06163</link>
<guid>https://arxiv.org/abs/2505.06163</guid>
<content:encoded><![CDATA[
arXiv:2505.06163v2 Announce Type: replace 
Abstract: We study coalition formation in the framework of fractional hedonic games (FHGs). The objective is to maximize social welfare in an online model where agents arrive one by one and must be assigned to coalitions immediately and irrevocably. A recurrent theme in online coalition formation is that online matching algorithms, where coalitions are restricted to size at most $2$, yield good competitive ratios. For example, computing maximal matchings achieves the optimal competitive ratio for general online FHGs. However, this ratio is bounded only if agents' valuations are themselves bounded.
  We identify optimal algorithms with constant competitive ratios in two related settings, independent of the range of agent valuations. First, under random agent arrival, we present an asymptotically optimal $(\frac{1}{3}-\frac 1n)$-competitive algorithm, where $n$ is the number of agents. This result builds on our identification of an optimal matching algorithm in a general model of online matching with edge weights and an unknown number of agents. In this setting, we also achieve an asymptotically optimal competitive ratio of $\frac{1}{3}-\frac 1n$. Second, when agents arrive in an arbitrary order but algorithms are allowed to irrevocably and entirely dissolve coalitions, we show that another matching-based algorithm achieves an optimal competitive ratio of $\frac{1}{6 + 4\sqrt{2}}$.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DanmakuTPPBench: A Multi-modal Benchmark for Temporal Point Process Modeling and Understanding</title>
<link>https://arxiv.org/abs/2505.18411</link>
<guid>https://arxiv.org/abs/2505.18411</guid>
<content:encoded><![CDATA[
arXiv:2505.18411v2 Announce Type: replace 
Abstract: We introduce DanmakuTPPBench, a comprehensive benchmark designed to advance multi-modal Temporal Point Process (TPP) modeling in the era of Large Language Models (LLMs). While TPPs have been widely studied for modeling temporal event sequences, existing datasets are predominantly unimodal, hindering progress in models that require joint reasoning over temporal, textual, and visual information. To address this gap, DanmakuTPPBench comprises two complementary components: (1) DanmakuTPP-Events, a novel dataset derived from the Bilibili video platform, where user-generated bullet comments (Danmaku) naturally form multi-modal events annotated with precise timestamps, rich textual content, and corresponding video frames; (2) DanmakuTPP-QA, a challenging question-answering dataset constructed via a novel multi-agent pipeline powered by state-of-the-art LLMs and multi-modal LLMs (MLLMs), targeting complex temporal-textual-visual reasoning. We conduct extensive evaluations using both classical TPP models and recent MLLMs, revealing significant performance gaps and limitations in current methods' ability to model multi-modal event dynamics. Our benchmark establishes strong baselines and calls for further integration of TPP modeling into the multi-modal language modeling landscape. Project page: https://github.com/FRENKIE-CHIANG/DanmakuTPPBench
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Collaboration via Evolving Orchestration</title>
<link>https://arxiv.org/abs/2505.19591</link>
<guid>https://arxiv.org/abs/2505.19591</guid>
<content:encoded><![CDATA[
arXiv:2505.19591v2 Announce Type: replace 
Abstract: Large language models (LLMs) have achieved remarkable results across diverse downstream tasks, but their monolithic nature restricts scalability and efficiency in complex problem-solving. While recent research explores multi-agent collaboration among LLMs, most approaches rely on static organizational structures that struggle to adapt as task complexity and agent numbers grow, resulting in coordination overhead and inefficiencies. To this end, we propose a puppeteer-style paradigm for LLM-based multi-agent collaboration, where a centralized orchestrator ("puppeteer") dynamically directs agents ("puppets") in response to evolving task states. This orchestrator is trained via reinforcement learning to adaptively sequence and prioritize agents, enabling flexible and evolvable collective reasoning. Experiments on closed- and open-domain scenarios show that this method achieves superior performance with reduced computational costs. Analyses further reveal that the key improvements consistently stem from the emergence of more compact, cyclic reasoning structures under the orchestrator's evolution. Our code is available at https://github.com/OpenBMB/ChatDev/tree/puppeteer.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Agents Fix Agent Issues?</title>
<link>https://arxiv.org/abs/2505.20749</link>
<guid>https://arxiv.org/abs/2505.20749</guid>
<content:encoded><![CDATA[
arXiv:2505.20749v3 Announce Type: replace 
Abstract: LLM-based agent systems are emerging as a new software paradigm and have been widely adopted across diverse domains such as medicine, robotics, and programming. However, maintaining these systems requires substantial effort, as they are inevitably prone to bugs and continually evolve to meet changing external requirements. Therefore, automatically resolving agent issues (i.e., bug reports or feature requests) is a crucial and challenging task. While recent software engineering (SE) agents (e.g., SWE-agent) have shown promise in addressing issues in traditional software systems, it remains unclear how effectively they can resolve real-world issues in agent systems, which differ significantly from traditional software. To fill this gap, we first manually analyze 201 real-world agent issues and identify common categories of agent issues. We then spend 500 person-hours constructing AGENTISSUE-BENCH, a reproducible benchmark comprising 50 agent issue resolution tasks (each with an executable environment and failure-triggering tests). We further evaluate state-of-the-art SE agents on AGENTISSUE-BENCH and reveal their limited effectiveness (i.e., with only 3.33% - 12.67% resolution rates). These results underscore the unique challenges of maintaining agent systems compared to traditional software, highlighting the need for further research to develop advanced SE agents for resolving agent issues. Data and code are available at https://alfin06.github.io/AgentIssue-Bench-Leaderboard/#/ .
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Web: The Security of Web Use Agents</title>
<link>https://arxiv.org/abs/2506.07153</link>
<guid>https://arxiv.org/abs/2506.07153</guid>
<content:encoded><![CDATA[
arXiv:2506.07153v2 Announce Type: replace 
Abstract: Web-use agents are rapidly being deployed to automate complex web tasks with extensive browser capabilities. However, these capabilities create a critical and previously unexplored attack surface. This paper demonstrates how attackers can exploit web-use agents by embedding malicious content in web pages, such as comments, reviews, or advertisements, that agents encounter during legitimate browsing tasks. We introduce the task-aligned injection technique that frames malicious commands as helpful task guidance rather than obvious attacks, exploiting fundamental limitations in LLMs' contextual reasoning. Agents struggle to maintain coherent contextual awareness and fail to detect when seemingly helpful web content contains steering attempts that deviate them from their original task goal. To scale this attack, we developed an automated three-stage pipeline that generates effective injections without manual annotation or costly online agent interactions during training, remaining efficient even with limited training data. This pipeline produces a generator model that we evaluate on five popular agents using payloads organized by the Confidentiality-Integrity-Availability (CIA) security triad, including unauthorized camera activation, file exfiltration, user impersonation, phishing, and denial-of-service. This generator achieves over 80% attack success rate (ASR) with strong transferability across unseen payloads, diverse web environments, and different underlying LLMs. This attack succeed even against agents with built-in safety mechanisms, requiring only the ability to post content on public websites. To address this risk, we propose comprehensive mitigation strategies including oversight mechanisms, execution constraints, and task-aware reasoning techniques.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.09049</link>
<guid>https://arxiv.org/abs/2506.09049</guid>
<content:encoded><![CDATA[
arXiv:2506.09049v2 Announce Type: replace 
Abstract: Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReVeal: Self-Evolving Code Agents via Reliable Self-Verification</title>
<link>https://arxiv.org/abs/2506.11442</link>
<guid>https://arxiv.org/abs/2506.11442</guid>
<content:encoded><![CDATA[
arXiv:2506.11442v2 Announce Type: replace 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has advanced the reasoning capabilities of large language models. However, existing methods rely solely on outcome rewards, without explicitly optimizing verification or leveraging reliable signals from realistic environments, leading to unreliable self-verification and limited test-time scaling. To address this, we widen the verification-generation asymmetry by explicitly optimizing self-verification, making it a reliable driver of deeper test-time scaling. We introduce ReVeal, a multi-turn reinforcement learning framework that evolves code generation through self-verification and tool-based evaluation. ReVeal structures long-horizon reasoning as iterative generation-verification turns and incorporates TAPO for turn-level credit assignment, fostering the co-evolution of code and test generation. At inference, this strengthened self-verification enables the model to use self-constructed tests and tool feedback to continuously evolve code for 20+ turns on LiveCodeBench despite training on only three. It also significantly improves Pass@k, indicating stronger exploration that expands the reasoning boundaries of the base model. These findings highlight the promise of ReVeal as a scalable paradigm for RL training and test-time scaling, paving the way for more robust and autonomous AI agents.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Properties of Inflectional Morphology in Neural Emergent Communication</title>
<link>https://arxiv.org/abs/2508.05843</link>
<guid>https://arxiv.org/abs/2508.05843</guid>
<content:encoded><![CDATA[
arXiv:2508.05843v2 Announce Type: replace 
Abstract: Emergent communication (EmCom) with deep neural network-based agents promises to yield insights into the nature of human language, but remains focused primarily on a few subfield-specific goals and metrics that prioritize communication schemes which represent attributes with unique characters one-to-one and compose them syntactically. We thus reinterpret a common EmCom setting, the attribute-value reconstruction game, by imposing a small-vocabulary constraint to simulate double articulation, and formulating a novel setting analogous to naturalistic inflectional morphology (enabling meaningful comparison to natural language communication schemes). We develop new metrics and explore variations of this game motivated by real properties of inflectional morphology: concatenativity and fusion. Through our experiments, we discover that simulated phonological constraints encourage concatenative morphology, and emergent languages replicate the tendency of natural languages to fuse grammatical attributes.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distilling LLM Prior to Flow Model for Generalizable Agent's Imagination in Object Goal Navigation</title>
<link>https://arxiv.org/abs/2508.09423</link>
<guid>https://arxiv.org/abs/2508.09423</guid>
<content:encoded><![CDATA[
arXiv:2508.09423v2 Announce Type: replace 
Abstract: The Object Goal Navigation (ObjectNav) task challenges agents to locate a specified object in an unseen environment by imagining unobserved regions of the scene. Prior approaches rely on deterministic and discriminative models to complete semantic maps, overlooking the inherent uncertainty in indoor layouts and limiting their ability to generalize to unseen environments. In this work, we propose GOAL, a generative flow-based framework that models the semantic distribution of indoor environments by bridging observed regions with LLM-enriched full-scene semantic maps. During training, spatial priors inferred from large language models (LLMs) are encoded as two-dimensional Gaussian fields and injected into target maps, distilling rich contextual knowledge into the flow model and enabling more generalizable completions. Extensive experiments demonstrate that GOAL achieves state-of-the-art performance on MP3D and Gibson, and shows strong generalization in transfer settings to HM3D. Codes and pretrained models are available at https://github.com/Badi-Li/GOAL.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Automated Multi-modal Evaluation Framework for Mobile Intelligent Assistants Based on Large Language Models and Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2508.09507</link>
<guid>https://arxiv.org/abs/2508.09507</guid>
<content:encoded><![CDATA[
arXiv:2508.09507v2 Announce Type: replace 
Abstract: With the rapid development of mobile intelligent assistant technologies, multi-modal AI assistants have become essential interfaces for daily user interactions. However, current evaluation methods face challenges including high manual costs, inconsistent standards, and subjective bias. This paper proposes an automated multi-modal evaluation framework based on large language models and multi-agent collaboration. The framework employs a three-tier agent architecture consisting of interaction evaluation agents, semantic verification agents, and experience decision agents. Through supervised fine-tuning on the Qwen3-8B model, we achieve a significant evaluation matching accuracy with human experts. Experimental results on eight major intelligent agents demonstrate the framework's effectiveness in predicting users' satisfaction and identifying generation defects.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media</title>
<link>https://arxiv.org/abs/2508.11503</link>
<guid>https://arxiv.org/abs/2508.11503</guid>
<content:encoded><![CDATA[
arXiv:2508.11503v2 Announce Type: replace 
Abstract: Reliable autonomous navigation across the unstructured terrains of distant planetary surfaces is a critical enabler for future space exploration. However, the deployment of learning-based controllers is hindered by the inherent sim-to-real gap, particularly for the complex dynamics of wheel interactions with granular media. This work presents a complete sim-to-real framework for developing and validating robust control policies for dynamic waypoint tracking on such challenging surfaces. We leverage massively parallel simulation to train reinforcement learning agents across a vast distribution of procedurally generated environments with randomized physics. These policies are then transferred zero-shot to a physical wheeled rover operating in a lunar-analogue facility. Our experiments systematically compare multiple reinforcement learning algorithms and action smoothing filters to identify the most effective combinations for real-world deployment. Crucially, we provide strong empirical evidence that agents trained with procedural diversity achieve superior zero-shot performance compared to those trained on static scenarios. We also analyze the trade-offs of fine-tuning with high-fidelity particle physics, which offers minor gains in low-speed precision at a significant computational cost. Together, these contributions establish a validated workflow for creating reliable learning-based navigation systems, marking a substantial step towards deploying autonomous robots in the final frontier.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents</title>
<link>https://arxiv.org/abs/2508.14040</link>
<guid>https://arxiv.org/abs/2508.14040</guid>
<content:encoded><![CDATA[
arXiv:2508.14040v2 Announce Type: replace 
Abstract: We introduce ComputerRL, a framework for autonomous desktop intelligence that enables agents to operate complex digital workspaces skillfully. ComputerRL features the API-GUI paradigm, which unifies programmatic API calls and direct GUI interaction to address the inherent mismatch between machine agents and human-centric desktop environments. Scaling end-to-end RL training is crucial for improvement and generalization across diverse desktop tasks; however, it remains challenging due to environmental inefficiency and instability during extended training. To support scalable and robust training, we develop a distributed RL infrastructure capable of orchestrating thousands of parallel virtual desktop environments to accelerate large-scale online RL. Furthermore, we propose Entropulse, a training strategy that alternates reinforcement learning with supervised fine-tuning, effectively mitigating entropy collapse during extended training runs. We employ ComputerRL on open models GLM-4-9B-0414 and GLM-4.1V-9B-Thinking, and evaluate them on the OSWorld benchmark. The AutoGLM-OS-9B achieves a new state-of-the-art accuracy of 48.9%, demonstrating significant improvements for general agents in desktop automation. Our code and the new OfficeWorld benchmark are available at https://github.com/thudm/ComputerRL. The algorithm and framework are adopted in building AutoGLM (Liu et al., 2024b).
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning</title>
<link>https://arxiv.org/abs/2508.20866</link>
<guid>https://arxiv.org/abs/2508.20866</guid>
<content:encoded><![CDATA[
arXiv:2508.20866v2 Announce Type: replace 
Abstract: The increasing complexity of software systems and the sophistication of cyber-attacks have underscored the critical need for effective automated vulnerability detection and repair systems. Data-driven approaches using deep learning models show promise but critically depend on the availability of large, accurately labeled datasets. Yet existing datasets either suffer from noisy labels, limited range of vulnerabilities, or fail to reflect vulnerabilities as they occur in real-world software. This also limits large-scale benchmarking of such solutions. Automated vulnerability injection provides a way to directly address these dataset limitations, but existing techniques remain limited in coverage, contextual fidelity, or injection success rates. In this paper, we present AVIATOR, the first AI-agentic vulnerability injection workflow. It automatically injects realistic, category-specific vulnerabilities for high-fidelity, diverse, large-scale vulnerability dataset generation. Unlike prior monolithic approaches, AVIATOR orchestrates specialized AI agents, function agents and traditional code analysis tools that replicate expert reasoning. It combines semantic analysis, injection synthesis enhanced with LoRA-based fine-tuning and Retrieval-Augmented Generation, as well as post-injection validation via static analysis and LLM-based discriminators. This modular decomposition allows specialized agents to focus on distinct tasks, improving robustness of injection and reducing error propagation across the workflow. Evaluations across three distinct benchmarks demonstrate that AVIATOR achieves 91%-95% injection success rates, significantly surpassing existing automated dataset generation techniques in both accuracy and scope of software vulnerabilities.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Agents go Astray: Course-Correcting SWE Agents with PRMs</title>
<link>https://arxiv.org/abs/2509.02360</link>
<guid>https://arxiv.org/abs/2509.02360</guid>
<content:encoded><![CDATA[
arXiv:2509.02360v2 Announce Type: replace 
Abstract: Large Language Model (LLM) agents are increasingly deployed for complex, multi-step software engineering (SWE) tasks. However, their trajectories often contain costly inefficiencies, such as redundant exploration, looping, and failure to terminate once a solution is reached. Prior work has largely treated these errors in a post-hoc manner, diagnosing failures only after execution. In this paper, we introduce SWE-PRM, an inference-time Process Reward Model (PRM) that intervenes during execution to detect and course-correct trajectory-level errors. Our PRM design leverages a taxonomy of common inefficiencies and delivers lightweight, interpretable feedback without modifying the underlying policy. On SWE-bench Verified, closed-source PRMs improve resolution from 40.0% to 50.6% (+10.6 p.p.), with the largest gains on medium and hard tasks. Among feedback strategies, taxonomy-guided PRMs outperform unguided or explicit action-prescriptive variants, increasing success rate while reducing trajectory length. These benefits come at an acceptable added inference cost of as low as $0.2, making PRMs a practical and scalable mechanism for improving SWE agents' reliability and efficiency.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proof2Silicon: Prompt Repair for Verified Code and Hardware Generation via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.06239</link>
<guid>https://arxiv.org/abs/2509.06239</guid>
<content:encoded><![CDATA[
arXiv:2509.06239v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in automated code generation but frequently produce code that fails formal verification, an essential requirement for hardware and safety-critical domains. To overcome this fundamental limitation, we previously proposed PREFACE, a model-agnostic framework based on reinforcement learning (RL) that iteratively repairs the prompts provided to frozen LLMs, systematically steering them toward generating formally verifiable Dafny code without costly fine-tuning. This work presents Proof2Silicon, a novel end-to-end synthesis framework that embeds the previously proposed PREFACE flow to enable the generation of correctness-by-construction hardware directly from natural language specifications. Proof2Silicon operates by: (1) leveraging PREFACE's verifier-driven RL agent to optimize prompt generation iteratively, ensuring Dafny code correctness; (2) automatically translating verified Dafny programs into synthesizable high-level C using Dafny's Python backend and PyLog; and (3) employing Vivado HLS to produce RTL implementations. Evaluated rigorously on a challenging 100-task benchmark, PREFACE's RL-guided prompt optimization consistently improved Dafny verification success rates across diverse LLMs by up to 21%. Crucially, Proof2Silicon achieved an end-to-end hardware synthesis success rate of up to 72%, generating RTL designs through Vivado HLS synthesis flows. These results demonstrate a robust, scalable, and automated pipeline for LLM-driven, formally verified hardware synthesis, bridging natural-language specification and silicon realization.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tree of Agents: Improving Long-Context Capabilities of Large Language Models through Multi-Perspective Reasoning</title>
<link>https://arxiv.org/abs/2509.06436</link>
<guid>https://arxiv.org/abs/2509.06436</guid>
<content:encoded><![CDATA[
arXiv:2509.06436v2 Announce Type: replace 
Abstract: Large language models (LLMs) face persistent challenges when handling long-context tasks, most notably the lost in the middle issue, where information located in the middle of a long input tends to be underutilized. Some existing methods that reduce input have the risk of discarding key information, while others that extend context windows often lead to attention dispersion. To address these limitations, we propose Tree of Agents (TOA), a multi-agent reasoning framework that segments the input into chunks processed by independent agents. Each agent generates its local cognition, then agents dynamically exchange information for collaborative reasoning along tree-structured paths. TOA enables agents to probe different reasoning orders for multi-perspective understanding, effectively mitigating position bias and reducing hallucinations. To improve processing efficiency, we incorporate prefix-hash caching and adaptive pruning strategies, achieving significant performance improvements with comparable API overhead. Experiments show that TOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple baselines and demonstrates comparable performance to the latest and much larger commercial models, such as Gemini1.5-pro, on various long-context tasks. Code is available at https://github.com/Aireduce952/Tree-of-Agents.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Individual Travel Diaries Using Large Language Models Informed by Census and Land-Use Data</title>
<link>https://arxiv.org/abs/2509.09710</link>
<guid>https://arxiv.org/abs/2509.09710</guid>
<content:encoded><![CDATA[
arXiv:2509.09710v2 Announce Type: replace 
Abstract: This study introduces a Large Language Model (LLM) scheme for generating individual travel diaries in agent-based transportation models. While traditional approaches rely on large quantities of proprietary household travel surveys, the method presented in this study generates personas stochastically from open-source American Community Survey (ACS) and Smart Location Database (SLD) data, then synthesizes diaries through direct prompting. This study features a novel one-to-cohort realism score: a composite of four metrics (Trip Count Score, Interval Score, Purpose Score, and Mode Score) validated against the Connecticut Statewide Transportation Study (CSTS) diaries, matched across demographic variables. The validation utilizes Jensen-Shannon Divergence to measure distributional similarities between generated and real diaries. When compared to diaries generated with classical methods (Negative Binomial for trip generation; Multinomial Logit for mode/purpose) calibrated on the validation set, LLM-generated diaries achieve comparable overall realism (LLM mean: 0.485 vs. 0.455). The LLM excels in determining trip purpose and demonstrates greater consistency (narrower realism score distribution), while classical models lead in numerical estimates of trip count and activity duration. Aggregate validation confirms the LLM's statistical representativeness (LLM mean: 0.612 vs. 0.435), demonstrating LLM's zero-shot viability and establishing a quantifiable metric of diary realism for future synthetic diary evaluation systems.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models</title>
<link>https://arxiv.org/abs/2509.15536</link>
<guid>https://arxiv.org/abs/2509.15536</guid>
<content:encoded><![CDATA[
arXiv:2509.15536v2 Announce Type: replace 
Abstract: World models allow agents to simulate the consequences of actions in imagined environments for planning, control, and long-horizon decision-making. However, existing autoregressive world models struggle with visually coherent predictions due to disrupted spatial structure, inefficient decoding, and inadequate motion modeling. In response, we propose \textbf{S}cale-wise \textbf{A}utoregression with \textbf{M}otion \textbf{P}r\textbf{O}mpt (\textbf{SAMPO}), a hybrid framework that combines visual autoregressive modeling for intra-frame generation with causal modeling for next-frame generation. Specifically, SAMPO integrates temporal causal decoding with bidirectional spatial attention, which preserves spatial locality and supports parallel decoding within each scale. This design significantly enhances both temporal consistency and rollout efficiency. To further improve dynamic scene understanding, we devise an asymmetric multi-scale tokenizer that preserves spatial details in observed frames and extracts compact dynamic representations for future frames, optimizing both memory usage and model performance. Additionally, we introduce a trajectory-aware motion prompt module that injects spatiotemporal cues about object and robot trajectories, focusing attention on dynamic regions and improving temporal consistency and physical realism. Extensive experiments show that SAMPO achieves competitive performance in action-conditioned video prediction and model-based control, improving generation quality with 4.4$\times$ faster inference. We also evaluate SAMPO's zero-shot generalization and scaling behavior, demonstrating its ability to generalize to unseen tasks and benefit from larger model sizes.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wonder Wins Ways: Curiosity-Driven Exploration through Multi-Agent Contextual Calibration</title>
<link>https://arxiv.org/abs/2509.20648</link>
<guid>https://arxiv.org/abs/2509.20648</guid>
<content:encoded><![CDATA[
arXiv:2509.20648v2 Announce Type: replace 
Abstract: Autonomous exploration in complex multi-agent reinforcement learning (MARL) with sparse rewards critically depends on providing agents with effective intrinsic motivation. While artificial curiosity offers a powerful self-supervised signal, it often confuses environmental stochasticity with meaningful novelty. Moreover, existing curiosity mechanisms exhibit a uniform novelty bias, treating all unexpected observations equally. However, peer behavior novelty, which encode latent task dynamics, are often overlooked, resulting in suboptimal exploration in decentralized, communication-free MARL settings. To this end, inspired by how human children adaptively calibrate their own exploratory behaviors via observing peers, we propose a novel approach to enhance multi-agent exploration. We introduce CERMIC, a principled framework that empowers agents to robustly filter noisy surprise signals and guide exploration by dynamically calibrating their intrinsic curiosity with inferred multi-agent context. Additionally, CERMIC generates theoretically-grounded intrinsic rewards, encouraging agents to explore state transitions with high information gain. We evaluate CERMIC on benchmark suites including VMAS, Meltingpot, and SMACv2. Empirical results demonstrate that exploration with CERMIC significantly outperforms SoTA algorithms in sparse-reward environments.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Secure and Efficient Access Control for Computer-Use Agents via Context Space</title>
<link>https://arxiv.org/abs/2509.22256</link>
<guid>https://arxiv.org/abs/2509.22256</guid>
<content:encoded><![CDATA[
arXiv:2509.22256v2 Announce Type: replace 
Abstract: Large language model (LLM)-based computer-use agents represent a convergence of AI and OS capabilities, enabling natural language to control system- and application-level functions. However, due to LLMs' inherent uncertainty issues, granting agents control over computers poses significant security risks. When agent actions deviate from user intentions, they can cause irreversible consequences. Existing mitigation approaches, such as user confirmation and LLM-based dynamic action validation, still suffer from limitations in usability, security, and performance. To address these challenges, we propose CSAgent, a system-level, static policy-based access control framework for computer-use agents. To bridge the gap between static policy and dynamic context and user intent, CSAgent introduces intent- and context-aware policies, and provides an automated toolchain to assist developers in constructing and refining them. CSAgent enforces these policies through an optimized OS service, ensuring that agent actions can only be executed under specific user intents and contexts. CSAgent supports protecting agents that control computers through diverse interfaces, including API, CLI, and GUI. We implement and evaluate CSAgent, which successfully defends against more than 99.36% of attacks while introducing only 6.83% performance overhead.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IASC: Interactive Agentic System for ConLangs</title>
<link>https://arxiv.org/abs/2510.07591</link>
<guid>https://arxiv.org/abs/2510.07591</guid>
<content:encoded><![CDATA[
arXiv:2510.07591v2 Announce Type: replace 
Abstract: We present a system that uses LLMs as a tool in the development of Constructed Languages. The system is modular in that one first creates a target phonology for the language using an agentic approach that refines its output at each step with commentary feedback on its previous attempt. Next, a set of sentences is 'translated' from their English original into a morphosyntactic markup that reflects the word order and morphosyntactic feature specifications of the desired target language, with affixes represented as morphosyntactic feature bundles. From this translated corpus, a lexicon is constructed using the phonological model and the set of morphemes (stems and affixes) extracted from the 'translated' sentences. The system is then instructed to provide an orthography for the language, using an existing script such as Latin or Cyrillic. Finally, the system writes a brief grammatical handbook of the language. The system can also translate further sentences into the target language.
  Our goal is twofold. First, we hope that these tools will be fun to use for creating artificially constructed languages. Second, we are interested in exploring what LLMs 'know' about language-not what they know about any particular language or linguistic phenomenon, but how much they know about and understand language and linguistic concepts. As we shall see, there is a fairly wide gulf in capabilities both among different LLMs and among different linguistic specifications, with it being notably easier for systems to deal with more common patterns than rarer ones. An additional avenue that we explore is the application of our approach to translating from high-resource into low-resource languages. While the results so far are mostly negative, we provide some evidence that an improved version of the present system could afford some real gains in such tasks.
  https://github.com/SakanaAI/IASC
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Process Reward Models: From Outcome Signals to Process Supervisions for Large Language Models</title>
<link>https://arxiv.org/abs/2510.08049</link>
<guid>https://arxiv.org/abs/2510.08049</guid>
<content:encoded><![CDATA[
arXiv:2510.08049v2 Announce Type: replace 
Abstract: Although Large Language Models (LLMs) exhibit advanced reasoning ability, conventional alignment remains largely dominated by outcome reward models (ORMs) that judge only final answers. Process Reward Models(PRMs) address this gap by evaluating and guiding reasoning at the step or trajectory level. This survey provides a systematic overview of PRMs through the full loop: how to generate process data, build PRMs, and use PRMs for test-time scaling and reinforcement learning. We summarize applications across math, code, text, multimodal reasoning, robotics, and agents, and review emerging benchmarks. Our goal is to clarify design spaces, reveal open challenges, and guide future research toward fine-grained, robust reasoning alignment.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Audit the Whisper: Detecting Steganographic Collusion in Multi-Agent LLMs</title>
<link>https://arxiv.org/abs/2510.04303</link>
<guid>https://arxiv.org/abs/2510.04303</guid>
<content:encoded><![CDATA[
arXiv:2510.04303v2 Announce Type: replace 
Abstract: Multi-agent deployments of large language models (LLMs) are increasingly embedded in market, allocation, and governance workflows, yet covert coordination among agents can silently erode trust and social welfare. Existing audits are dominated by heuristics that lack theoretical guarantees, struggle to transfer across tasks, and seldom ship with the infrastructure needed for independent replication. We introduce Audit the Whisper, a conference-grade research artifact that spans theory, benchmark design, detection, and reproducibility. Our contributions are: (i) a channel-capacity analysis showing how interventions such as paraphrase, rate limiting, and role permutation impose quantifiable capacity penalties-operationalised via paired-run Kullback--Leibler diagnostics-that tighten mutual-information thresholds with finite-sample guarantees and full proofs; (ii) ColludeBench-v0, covering pricing, first-price auctions, peer review, and hosted Gemini/Groq APIs with configurable covert schemes, deterministic manifests, and reward instrumentation; and (iii) a calibrated auditing pipeline that fuses cross-run mutual information, permutation invariance, watermark variance, and fairness-aware acceptance bias, each tuned to a $10^{-3}$ false-positive budget and validated by 10k honest runs plus an e-value martingale. Across ColludeBench and external suites including Secret Collusion, CASE, Perfect Collusion Benchmark, and SentinelAgent, the union meta-test attains state-of-the-art power at fixed FPR while ablations surface price-of-auditing trade-offs and fairness-driven colluders invisible to MI alone. We release regeneration scripts, anonymized manifests, and documentation so that external auditors can reproduce every figure, satisfy double-blind requirements, and extend the framework with minimal effort.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BREATH: A Bio-Radar Embodied Agent for Tonal and Human-Aware Diffusion Music Generation</title>
<link>https://arxiv.org/abs/2510.15895</link>
<guid>https://arxiv.org/abs/2510.15895</guid>
<content:encoded><![CDATA[
arXiv:2510.15895v1 Announce Type: new 
Abstract: We present a multimodal system for personalized music generation that integrates physiological sensing, LLM-based reasoning, and controllable audio synthesis. A millimeter-wave radar sensor non-invasively captures heart rate and respiration rate. These physiological signals, combined with environmental state, are interpreted by a reasoning agent to infer symbolic musical descriptors, such as tempo, mood intensity, and traditional Chinese pentatonic modes, which are then expressed as structured prompts to guide a diffusion-based audio model in synthesizing expressive melodies. The system emphasizes cultural grounding through tonal embeddings and enables adaptive, embodied music interaction. To evaluate the system, we adopt a research-creation methodology combining case studies, expert feedback, and targeted control experiments. Results show that physiological variations can modulate musical features in meaningful ways, and tonal conditioning enhances alignment with intended modal characteristics. Expert users reported that the system affords intuitive, culturally resonant musical responses and highlighted its potential for therapeutic and interactive applications. This work demonstrates a novel bio-musical feedback loop linking radar-based sensing, prompt reasoning, and generative audio modeling.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Coordination to Personalization: A Trust-Aware Simulation Framework for Emergency Department Decision Support</title>
<link>https://arxiv.org/abs/2510.15896</link>
<guid>https://arxiv.org/abs/2510.15896</guid>
<content:encoded><![CDATA[
arXiv:2510.15896v1 Announce Type: new 
Abstract: Background/Objectives: Efficient task allocation in hospital emergency departments (EDs) is critical for operational efficiency and patient care quality, yet the complexity of staff coordination poses significant challenges. This study proposes a simulation-based framework for modeling doctors and nurses as intelligent agents guided by computational trust mechanisms. The objective is to explore how trust-informed coordination can support decision making in ED management. Methods: The framework was implemented in Unity, a 3D graphics platform, where agents assess their competence before undertaking tasks and adaptively coordinate with colleagues. The simulation environment enables real-time observation of workflow dynamics, resource utilization, and patient outcomes. We examined three scenarios - Baseline, Replacement, and Training - reflecting alternative staff management strategies. Results: Trust-informed task allocation balanced patient safety and efficiency by adapting to nurse performance levels. In the Baseline scenario, prioritizing safety reduced errors but increased patient delays compared to a FIFO policy. The Replacement scenario improved throughput and reduced delays, though at additional staffing cost. The training scenario forstered long-term skill development among low-performing nurses, despite short-term delays and risks. These results highlight the trade-off between immediate efficiency gains and sustainable capacity building in ED staffing. Conclusions: The proposed framework demonstrates the potential of computational trust for evidence-based decision support in emergency medicine. By linking staff coordination with adaptive decision making, it provides hospital managers with a tool to evaluate alternative policies under controlled and repeatable conditions, while also laying a foundation for future AI-driven personalized decision support.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HealthDial: A No-Code LLM-Assisted Dialogue Authoring Tool for Healthcare Virtual Agents</title>
<link>https://arxiv.org/abs/2510.15898</link>
<guid>https://arxiv.org/abs/2510.15898</guid>
<content:encoded><![CDATA[
arXiv:2510.15898v1 Announce Type: new 
Abstract: We introduce HealthDial, a dialogue authoring tool that helps healthcare providers and educators create virtual agents that deliver health education and counseling to patients over multiple conversations. HealthDial leverages large language models (LLMs) to automatically create an initial session-based plan and conversations for each session using text-based patient health education materials as input. Authored dialogue is output in the form of finite state machines for virtual agent delivery so that all content can be validated and no unsafe advice is provided resulting from LLM hallucinations. LLM-drafted dialogue structure and language can be edited by the author in a no-code user interface to ensure validity and optimize clarity and impact. We conducted a feasibility and usability study with counselors and students to test our approach with an authoring task for cancer screening education. Participants used HealthDial and then tested their resulting dialogue by interacting with a 3D-animated virtual agent delivering the dialogue. Through participants' evaluations of the task experience and final dialogues, we show that HealthDial provides a promising first step for counselors to ensure full coverage of their health education materials, while creating understandable and actionable virtual agent dialogue with patients.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FVDebug: An LLM-Driven Debugging Assistant for Automated Root Cause Analysis of Formal Verification Failures</title>
<link>https://arxiv.org/abs/2510.15906</link>
<guid>https://arxiv.org/abs/2510.15906</guid>
<content:encoded><![CDATA[
arXiv:2510.15906v1 Announce Type: new 
Abstract: Debugging formal verification (FV) failures represents one of the most time-consuming bottlenecks in modern hardware design workflows. When properties fail, engineers must manually trace through complex counter-examples spanning multiple cycles, analyze waveforms, and cross-reference design specifications to identify root causes - a process that can consume hours or days per bug. Existing solutions are largely limited to manual waveform viewers or simple automated tools that cannot reason about the complex interplay between design intent and implementation logic. We present FVDebug, an intelligent system that automates root-cause analysis by combining multiple data sources - waveforms, RTL code, design specifications - to transform failure traces into actionable insights. Our approach features a novel pipeline: (1) Causal Graph Synthesis that structures failure traces into directed acyclic graphs, (2) Graph Scanner using batched Large Language Model (LLM) analysis with for-and-against prompting to identify suspicious nodes, and (3) Insight Rover leveraging agentic narrative exploration to generate high-level causal explanations. FVDebug further provides concrete RTL fixes through its Fix Generator. Evaluated on open benchmarks, FVDebug attains high hypothesis quality and strong Pass@k fix rates. We further report results on two proprietary, production-scale FV counterexamples. These results demonstrate FVDebug's applicability from academic benchmarks to industrial designs.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models in Architecture Studio: A Framework for Learning Outcomes</title>
<link>https://arxiv.org/abs/2510.15936</link>
<guid>https://arxiv.org/abs/2510.15936</guid>
<content:encoded><![CDATA[
arXiv:2510.15936v1 Announce Type: new 
Abstract: The study explores the role of large language models (LLMs) in the context of the architectural design studio, understood as the pedagogical core of architectural education. Traditionally, the studio has functioned as an experiential learning space where students tackle design problems through reflective practice, peer critique, and faculty guidance. However, the integration of artificial intelligence (AI) in this environment has been largely focused on form generation, automation, and representation-al efficiency, neglecting its potential as a pedagogical tool to strengthen student autonomy, collaboration, and self-reflection. The objectives of this research were: (1) to identify pedagogical challenges in self-directed, peer-to-peer, and teacher-guided learning processes in architecture studies; (2) to propose AI interventions, particularly through LLM, that contribute to overcoming these challenges; and (3) to align these interventions with measurable learning outcomes using Bloom's taxonomy. The findings show that the main challenges include managing student autonomy, tensions in peer feedback, and the difficulty of balancing the transmission of technical knowledge with the stimulation of creativity in teaching. In response to this, LLMs are emerging as complementary agents capable of generating personalized feedback, organizing collaborative interactions, and offering adaptive cognitive scaffolding. Furthermore, their implementation can be linked to the cognitive levels of Bloom's taxonomy: facilitating the recall and understanding of architectural concepts, supporting application and analysis through interactive case studies, and encouraging synthesis and evaluation through hypothetical design scenarios.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding</title>
<link>https://arxiv.org/abs/2510.15952</link>
<guid>https://arxiv.org/abs/2510.15952</guid>
<content:encoded><![CDATA[
arXiv:2510.15952v1 Announce Type: new 
Abstract: Large language models exhibit intelligence without genuine epistemic understanding, exposing a key gap: the absence of epistemic architecture. This paper introduces the Structured Cognitive Loop (SCL) as an executable epistemological framework for emergent intelligence. Unlike traditional AI research asking "what is intelligence?" (ontological), SCL asks "under what conditions does cognition emerge?" (epistemological). Grounded in philosophy of mind and cognitive phenomenology, SCL bridges conceptual philosophy and implementable cognition. Drawing on process philosophy, enactive cognition, and extended mind theory, we define intelligence not as a property but as a performed process -- a continuous loop of judgment, memory, control, action, and regulation. SCL makes three contributions. First, it operationalizes philosophical insights into computationally interpretable structures, enabling "executable epistemology" -- philosophy as structural experiment. Second, it shows that functional separation within cognitive architecture yields more coherent and interpretable behavior than monolithic prompt based systems, supported by agent evaluations. Third, it redefines intelligence: not representational accuracy but the capacity to reconstruct its own epistemic state through intentional understanding. This framework impacts philosophy of mind, epistemology, and AI. For philosophy, it allows theories of cognition to be enacted and tested. For AI, it grounds behavior in epistemic structure rather than statistical regularity. For epistemology, it frames knowledge not as truth possession but as continuous reconstruction within a phenomenologically coherent loop. We situate SCL within debates on cognitive phenomenology, emergence, normativity, and intentionality, arguing that real progress requires not larger models but architectures that realize cognitive principles structurally.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ESCA: Contextualizing Embodied Agents via Scene-Graph Generation</title>
<link>https://arxiv.org/abs/2510.15963</link>
<guid>https://arxiv.org/abs/2510.15963</guid>
<content:encoded><![CDATA[
arXiv:2510.15963v1 Announce Type: new 
Abstract: Multi-modal large language models (MLLMs) are making rapid progress toward general-purpose embodied agents. However, current training pipelines primarily rely on high-level vision-sound-text pairs and lack fine-grained, structured alignment between pixel-level visual content and textual semantics. To overcome this challenge, we propose ESCA, a new framework for contextualizing embodied agents through structured spatial-temporal understanding. At its core is SGClip, a novel CLIP-based, open-domain, and promptable model for generating scene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic learning pipeline, which harnesses model-driven self-supervision from video-caption pairs and structured reasoning, thereby eliminating the need for human-labeled scene graph annotations. We demonstrate that SGClip supports both prompt-based inference and task-specific fine-tuning, excelling in scene graph generation and action localization benchmarks. ESCA with SGClip consistently improves both open-source and commercial MLLMs, achieving state-of-the-art performance across two embodied environments. Notably, it significantly reduces agent perception errors and enables open-source models to surpass proprietary baselines.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PISA: A Pragmatic Psych-Inspired Unified Memory System for Enhanced AI Agency</title>
<link>https://arxiv.org/abs/2510.15966</link>
<guid>https://arxiv.org/abs/2510.15966</guid>
<content:encoded><![CDATA[
arXiv:2510.15966v1 Announce Type: new 
Abstract: Memory systems are fundamental to AI agents, yet existing work often lacks adaptability to diverse tasks and overlooks the constructive and task-oriented role of AI agent memory. Drawing from Piaget's theory of cognitive development, we propose PISA, a pragmatic, psych-inspired unified memory system that addresses these limitations by treating memory as a constructive and adaptive process. To enable continuous learning and adaptability, PISA introduces a trimodal adaptation mechanism (i.e., schema updation, schema evolution, and schema creation) that preserves coherent organization while supporting flexible memory updates. Building on these schema-grounded structures, we further design a hybrid memory access architecture that seamlessly integrates symbolic reasoning with neural retrieval, significantly improving retrieval accuracy and efficiency. Our empirical evaluation, conducted on the existing LOCOMO benchmark and our newly proposed AggQA benchmark for data analysis tasks, confirms that PISA sets a new state-of-the-art by significantly enhancing adaptability and long-term knowledge retention.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear Reformulation of Nonlinear Optimization Problems</title>
<link>https://arxiv.org/abs/2510.15969</link>
<guid>https://arxiv.org/abs/2510.15969</guid>
<content:encoded><![CDATA[
arXiv:2510.15969v1 Announce Type: new 
Abstract: Reformulating nonlinear optimization problems is largely manual and expertise-intensive, yet it remains essential for solving such problems with linear optimization solvers or applying special-purpose algorithms. We introduce \textit{LinearizeLLM}, an agent-based framework that solves this task by leveraging Large Language Models (LLMs). The framework assigns each nonlinear pattern to a \textit{reformulation agent} that is explicitly instructed to derive an exact linear reformulation for its nonlinearity pattern, for instance, absolute-value terms or bilinear products of decision variables. The agents then coordinate to assemble a solver-ready linear model equivalent to the original problem. To benchmark the approach, we create a dataset of 20 real-world nonlinear optimization problems derived from the established ComplexOR dataset of linear optimization problems. We evaluate our approach with several LLMs. Our results indicate that specialized LLM agents can automate linearization tasks, opening a path toward fully conversational modeling pipelines for nonlinear optimization.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Biosciences: Emerging Threats and Roadmap to Biosecurity</title>
<link>https://arxiv.org/abs/2510.15975</link>
<guid>https://arxiv.org/abs/2510.15975</guid>
<content:encoded><![CDATA[
arXiv:2510.15975v1 Announce Type: new 
Abstract: The rapid adoption of generative artificial intelligence (GenAI) in the biosciences is transforming biotechnology, medicine, and synthetic biology. Yet this advancement is intrinsically linked to new vulnerabilities, as GenAI lowers the barrier to misuse and introduces novel biosecurity threats, such as generating synthetic viral proteins or toxins. These dual-use risks are often overlooked, as existing safety guardrails remain fragile and can be circumvented through deceptive prompts or jailbreak techniques. In this Perspective, we first outline the current state of GenAI in the biosciences and emerging threat vectors ranging from jailbreak attacks and privacy risks to the dual-use challenges posed by autonomous AI agents. We then examine urgent gaps in regulation and oversight, drawing on insights from 130 expert interviews across academia, government, industry, and policy. A large majority ($\approx 76$\%) expressed concern over AI misuse in biology, and 74\% called for the development of new governance frameworks. Finally, we explore technical pathways to mitigation, advocating a multi-layered approach to GenAI safety. These defenses include rigorous data filtering, alignment with ethical principles during development, and real-time monitoring to block harmful requests. Together, these strategies provide a blueprint for embedding security throughout the GenAI lifecycle. As GenAI becomes integrated into the biosciences, safeguarding this frontier requires an immediate commitment to both adaptive governance and secure-by-design technologies.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCP Security Bench (MSB): Benchmarking Attacks Against Model Context Protocol in LLM Agents</title>
<link>https://arxiv.org/abs/2510.15994</link>
<guid>https://arxiv.org/abs/2510.15994</guid>
<content:encoded><![CDATA[
arXiv:2510.15994v1 Announce Type: new 
Abstract: The Model Context Protocol (MCP) standardizes how large language model (LLM) agents discover, describe, and call external tools. While MCP unlocks broad interoperability, it also enlarges the attack surface by making tools first-class, composable objects with natural-language metadata, and standardized I/O. We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP rather than simulation; and (3) a robustness metric that quantifies the trade-off between security and performance: Net Resilient Performance (NRP). We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances. Results reveal the effectiveness of attacks against each stage of MCP. Models with stronger performance are more vulnerable to attacks due to their outstanding tool calling and instruction following capabilities. MSB provides a practical baseline for researchers and practitioners to study, compare, and harden MCP agents.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Kolmogorov-Smirnov Distance for Measuring Distribution Shift in Machine Learning</title>
<link>https://arxiv.org/abs/2510.15996</link>
<guid>https://arxiv.org/abs/2510.15996</guid>
<content:encoded><![CDATA[
arXiv:2510.15996v1 Announce Type: new 
Abstract: One of the major problems in Machine Learning (ML) and Artificial Intelligence (AI) is the fact that the probability distribution of the test data in the real world could deviate substantially from the probability distribution of the training data set. When this happens, the predictions of an ML system or an AI agent could involve large errors which is very troublesome and undesirable. While this is a well-known hard problem plaguing the AI and ML systems' accuracy and reliability, in certain applications such errors could be critical for safety and reliability of AI and ML systems. One approach to deal with this problem is to monitor and measure the deviation in the probability distribution of the test data in real time and to compensate for this deviation. In this paper, we propose and explore the use of Kolmogorov-Smirnov (KS) Test for measuring the distribution shift and we show how the KS distance can be used to quantify the distribution shift and its impact on an AI agent's performance. Our results suggest that KS distance could be used as a valuable statistical tool for monitoring and measuring the distribution shift. More specifically, it is shown that even a distance of KS=0.02 could lead to about 50\% increase in the travel time at a single intersection using a Reinforcement Learning agent which is quite significant. It is hoped that the use of KS Test and KS distance in AI-based smart transportation could be an important step forward for gauging the performance degradation of an AI agent in real time and this, in turn, could help the AI agent to cope with the distribution shift in a more informed manner.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Airfoil optimization using Design-by-Morphing with minimized design-space dimensionality</title>
<link>https://arxiv.org/abs/2510.16020</link>
<guid>https://arxiv.org/abs/2510.16020</guid>
<content:encoded><![CDATA[
arXiv:2510.16020v1 Announce Type: new 
Abstract: Effective airfoil geometry optimization requires exploring a diverse range of designs using as few design variables as possible. This study introduces AirDbM, a Design-by-Morphing (DbM) approach specialized for airfoil optimization that systematically reduces design-space dimensionality. AirDbM selects an optimal set of 12 baseline airfoils from the UIUC airfoil database, which contains over 1,600 shapes, by sequentially adding the baseline that most increases the design capacity. With these baselines, AirDbM reconstructs 99 \% of the database with a mean absolute error below 0.005, which matches the performance of a previous DbM approach that used more baselines. In multi-objective aerodynamic optimization, AirDbM demonstrates rapid convergence and achieves a Pareto front with a greater hypervolume than that of the previous larger-baseline study, where new Pareto-optimal solutions are discovered with enhanced lift-to-drag ratios at moderate stall tolerances. Furthermore, AirDbM demonstrates outstanding adaptability for reinforcement learning (RL) agents in generating airfoil geometry when compared to conventional airfoil parameterization methods, implying the broader potential of DbM in machine learning-driven design.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disaster Management in the Era of Agentic AI Systems: A Vision for Collective Human-Machine Intelligence for Augmented Resilience</title>
<link>https://arxiv.org/abs/2510.16034</link>
<guid>https://arxiv.org/abs/2510.16034</guid>
<content:encoded><![CDATA[
arXiv:2510.16034v1 Announce Type: new 
Abstract: The escalating frequency and severity of disasters routinely overwhelm traditional response capabilities, exposing critical vulnerability in disaster management. Current practices are hindered by fragmented data streams, siloed technologies, resource constraints, and the erosion of institutional memory, which collectively impede timely and effective decision making. This study introduces Disaster Copilot, a vision for a multi-agent artificial intelligence system designed to overcome these systemic challenges by unifying specialized AI tools within a collaborative framework. The proposed architecture utilizes a central orchestrator to coordinate diverse sub-agents, each specializing in critical domains such as predictive risk analytics, situational awareness, and impact assessment. By integrating multi-modal data, the system delivers a holistic, real-time operational picture and serve as the essential AI backbone required to advance Disaster Digital Twins from passive models to active, intelligent environments. Furthermore, it ensures functionality in resource-limited environments through on-device orchestration and incorporates mechanisms to capture institutional knowledge, mitigating the impact of staff turnover. We detail the system architecture and propose a three-phased roadmap emphasizing the parallel growth of technology, organizational capacity, and human-AI teaming. Disaster Copilot offers a transformative vision, fostering collective human-machine intelligence to build more adaptive, data-driven and resilient communities.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoBCtrl: Attacking GNN-Based Social Bot Detectors via Reinforced Manipulation of Bots Control Interaction</title>
<link>https://arxiv.org/abs/2510.16035</link>
<guid>https://arxiv.org/abs/2510.16035</guid>
<content:encoded><![CDATA[
arXiv:2510.16035v1 Announce Type: new 
Abstract: Social networks have become a crucial source of real-time information for individuals. The influence of social bots within these platforms has garnered considerable attention from researchers, leading to the development of numerous detection technologies. However, the vulnerability and robustness of these detection methods is still underexplored. Existing Graph Neural Network (GNN)-based methods cannot be directly applied due to the issues of limited control over social agents, the black-box nature of bot detectors, and the heterogeneity of bots. To address these challenges, this paper proposes the first adversarial multi-agent Reinforcement learning framework for social Bot control attacks (RoBCtrl) targeting GNN-based social bot detectors. Specifically, we use a diffusion model to generate high-fidelity bot accounts by reconstructing existing account data with minor modifications, thereby evading detection on social platforms. To the best of our knowledge, this is the first application of diffusion models to mimic the behavior of evolving social bots effectively. We then employ a Multi-Agent Reinforcement Learning (MARL) method to simulate bots adversarial behavior. We categorize social accounts based on their influence and budget. Different agents are then employed to control bot accounts across various categories, optimizing the attachment strategy through reinforcement learning. Additionally, a hierarchical state abstraction based on structural entropy is designed to accelerate the reinforcement learning. Extensive experiments on social bot detection datasets demonstrate that our framework can effectively undermine the performance of GNN-based detectors.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Capital Dream of Artificial Labour?</title>
<link>https://arxiv.org/abs/2510.16042</link>
<guid>https://arxiv.org/abs/2510.16042</guid>
<content:encoded><![CDATA[
arXiv:2510.16042v1 Announce Type: new 
Abstract: This paper investigates the concept of Labour as an expression of `timenergy' - a fusion of time and energy - and its entanglement within the system of Capital. We define Labour as the commodified, quantifiable expansion of timenergy, in contrast to Capital, which is capable of accumulation and abstraction. We explore Labour's historical evolution, its coercive and alienating nature, and its transformation through automation and artificial intelligence. Using a game-theoretic, agent-based simulation, we model interactions between Capital and Labour in production processes governed by Cobb-Douglas functions. Our results show that despite theoretical symmetry, learning agents disproportionately gravitate toward capital-intensive processes, revealing Capital's superior organizational influence due to its accumulative capacity. We argue that Capital functions as an artificially alive system animated by the living Labour it consumes, and question whether life can sustain itself without the infrastructures of Capital in a future of increasing automation. This study offers both a critique of and a framework for understanding Labour's subjugation within the Capital system.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUIrilla: A Scalable Framework for Automated Desktop UI Exploration</title>
<link>https://arxiv.org/abs/2510.16051</link>
<guid>https://arxiv.org/abs/2510.16051</guid>
<content:encoded><![CDATA[
arXiv:2510.16051v1 Announce Type: new 
Abstract: Autonomous agents capable of operating complex graphical user interfaces (GUIs) have the potential to transform desktop automation. While recent advances in large language models (LLMs) have significantly improved UI understanding, navigating full-window, multi-application desktop environments remains a major challenge. Data availability is limited by costly manual annotation, closed-source datasets and surface-level synthetic pipelines. We introduce GUIrilla, an automated scalable framework that systematically explores applications via native accessibility APIs to address the critical data collection challenge in GUI automation. Our framework focuses on macOS - an ecosystem with limited representation in current UI datasets - though many of its components are designed for broader cross-platform applicability. GUIrilla organizes discovered interface elements and crawler actions into hierarchical GUI graphs and employs specialized interaction handlers to achieve comprehensive application coverage. Using the application graphs from GUIrilla crawler, we construct and release GUIrilla-Task, a large-scale dataset of 27,171 functionally grounded tasks across 1,108 macOS applications, each annotated with full-desktop and window-level screenshots, accessibility metadata, and semantic action traces. Empirical results show that tuning LLM-based agents on GUIrilla-Task significantly improves performance on downstream UI tasks, outperforming synthetic baselines on the ScreenSpot Pro benchmark while using 97% less data. We also release macapptree, an open-source library for reproducible collection of structured accessibility metadata, along with the full GUIrilla-Task dataset, the manually verified GUIrilla-Gold benchmark, and the framework code to support open research in desktop autonomy.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PrivacyPAD: A Reinforcement Learning Framework for Dynamic Privacy-Aware Delegation</title>
<link>https://arxiv.org/abs/2510.16054</link>
<guid>https://arxiv.org/abs/2510.16054</guid>
<content:encoded><![CDATA[
arXiv:2510.16054v1 Announce Type: new 
Abstract: When users submit queries to Large Language Models (LLMs), their prompts can often contain sensitive data, forcing a difficult choice: Send the query to a powerful proprietary LLM providers to achieving state-of-the-art performance and risk data exposure, or relying on smaller, local models guarantees data privacy but often results in a degradation of task performance. Prior approaches have relied on static pipelines that use LLM rewriting, which shatters linguistic coherence and indiscriminately removes privacy-sensitive information, including task-critical content. We reformulate this challenge (Privacy-Conscious Delegation) as a sequential decision-making problem and introduce a novel reinforcement learning (RL) framework called PrivacyPAD to solve it. Our framework trains an agent to dynamically route text chunks, learning a policy that optimally balances the trade-off between privacy leakage and task performance. It implicitly distinguishes between replaceable Personally Identifiable Information (PII) (which it shields locally) and task-critical PII (which it strategically sends to the remote model for maximal utility). To validate our approach in complex scenarios, we also introduce a new medical dataset with high PII density. Our framework achieves a new state-of-the-art on the privacy-utility frontier, demonstrating the necessity of learned, adaptive policies for deploying LLMs in sensitive environments.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIADAFIX: issue description response for adaptive program repair</title>
<link>https://arxiv.org/abs/2510.16059</link>
<guid>https://arxiv.org/abs/2510.16059</guid>
<content:encoded><![CDATA[
arXiv:2510.16059v1 Announce Type: new 
Abstract: We propose utilizing fast and slow thinking to enhance the capabilities of large language model-based agents on complex tasks such as program repair. In particular, we design an adaptive program repair method based on issue description response, called SIADAFIX. The proposed method utilizes slow thinking bug fix agent to complete complex program repair tasks, and employs fast thinking workflow decision components to optimize and classify issue descriptions, using issue description response results to guide the orchestration of bug fix agent workflows. SIADAFIX adaptively selects three repair modes, i.e., easy, middle and hard mode, based on problem complexity. It employs fast generalization for simple problems and test-time scaling techniques for complex problems. Experimental results on the SWE-bench Lite show that the proposed method achieves 60.67% pass@1 performance using the Claude-4 Sonnet model, reaching state-of-the-art levels among all open-source methods. SIADAFIX effectively balances repair efficiency and accuracy, providing new insights for automated program repair. Our code is available at https://github.com/liauto-siada/siada-cli.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle</title>
<link>https://arxiv.org/abs/2510.16079</link>
<guid>https://arxiv.org/abs/2510.16079</guid>
<content:encoded><![CDATA[
arXiv:2510.16079v1 Announce Type: new 
Abstract: Current Large Language Model (LLM) agents show strong performance in tool use, but lack the crucial capability to systematically learn from their own experiences. While existing frameworks mainly focus on mitigating external knowledge gaps, they fail to address a more fundamental limitation: the inability to iteratively refine problem-solving strategies. In this work, we introduce EvolveR, a framework designed to enable agent to self-improve through a complete, closed-loop experience lifecycle. This lifecycle comprises two key stages: (1) Offline Self-Distillation, where the agent's interaction trajectories are synthesized into a structured repository of abstract, reusable strategic principles; (2) Online Interaction, where the agent interacts with tasks and actively retrieves distilled principles to guide its decision-making, accumulating a diverse set of behavioral trajectories. This loop employs a policy reinforcement mechanism to iteratively update the agent based on its performance. We demonstrate the effectiveness of EvolveR on complex multi-hop question-answering benchmarks, where it achieves superior performance over strong agentic baselines. Our work presents a comprehensive blueprint for agents that learn not only from external data but also from the consequences of their own actions, paving the way for more autonomous and continuously improving systems. Code is available at https://github.com/Edaizi/EvolveR.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoPHES:Leveraging on-device LLMs as Agent for Mobile Psychological Health Evaluation and Support</title>
<link>https://arxiv.org/abs/2510.16085</link>
<guid>https://arxiv.org/abs/2510.16085</guid>
<content:encoded><![CDATA[
arXiv:2510.16085v1 Announce Type: new 
Abstract: The 2022 World Mental Health Report calls for global mental health care reform, amid rising prevalence of issues like anxiety and depression that affect nearly one billion people worldwide. Traditional in-person therapy fails to meet this demand, and the situation is worsened by stigma. While general-purpose large language models (LLMs) offer efficiency for AI-driven mental health solutions, they underperform because they lack specialized fine-tuning. Existing LLM-based mental health chatbots can engage in empathetic conversations, but they overlook real-time user mental state assessment which is critical for professional counseling. This paper proposes MoPHES, a framework that integrates mental state evaluation, conversational support, and professional treatment recommendations. The agent developed under this framework uses two fine-tuned MiniCPM4-0.5B LLMs: one is fine-tuned on mental health conditions datasets to assess users' mental states and predict the severity of anxiety and depression; the other is fine-tuned on multi-turn dialogues to handle conversations with users. By leveraging insights into users' mental states, our agent provides more tailored support and professional treatment recommendations. Both models are also deployed directly on mobile devices to enhance user convenience and protect user privacy. Additionally, to evaluate the performance of MoPHES with other LLMs, we develop a benchmark for the automatic evaluation of mental state prediction and multi-turn counseling dialogues, which includes comprehensive evaluation metrics, datasets, and methods.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Narrowing Action Choices with AI Improves Human Sequential Decisions</title>
<link>https://arxiv.org/abs/2510.16097</link>
<guid>https://arxiv.org/abs/2510.16097</guid>
<content:encoded><![CDATA[
arXiv:2510.16097v1 Announce Type: new 
Abstract: Recent work has shown that, in classification tasks, it is possible to design decision support systems that do not require human experts to understand when to cede agency to a classifier or when to exercise their own agency to achieve complementarity$\unicode{x2014}$experts using these systems make more accurate predictions than those made by the experts or the classifier alone. The key principle underpinning these systems reduces to adaptively controlling the level of human agency, by design. Can we use the same principle to achieve complementarity in sequential decision making tasks? In this paper, we answer this question affirmatively. We develop a decision support system that uses a pre-trained AI agent to narrow down the set of actions a human can take to a subset, and then asks the human to take an action from this action set. Along the way, we also introduce a bandit algorithm that leverages the smoothness properties of the action sets provided by our system to efficiently optimize the level of human agency. To evaluate our decision support system, we conduct a large-scale human subject study ($n = 1{,}600$) where participants play a wildfire mitigation game. We find that participants who play the game supported by our system outperform those who play on their own by $\sim$$30$% and the AI agent used by our system by $>$$2$%, even though the AI agent largely outperforms participants playing without support. We have made available the data gathered in our human subject study as well as an open source implementation of our system at https://github.com/Networks-Learning/narrowing-action-choices .
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI for Ultra-Modern Networks: Multi-Agent Framework for RAN Autonomy and Assurance</title>
<link>https://arxiv.org/abs/2510.16144</link>
<guid>https://arxiv.org/abs/2510.16144</guid>
<content:encoded><![CDATA[
arXiv:2510.16144v1 Announce Type: new 
Abstract: The increasing complexity of Beyond 5G and 6G networks necessitates new paradigms for autonomy and assur- ance. Traditional O-RAN control loops rely heavily on RIC- based orchestration, which centralizes intelligence and exposes the system to risks such as policy conflicts, data drift, and unsafe actions under unforeseen conditions. In this work, we argue that the future of autonomous networks lies in a multi-agentic architecture, where specialized agents collaborate to perform data collection, model training, prediction, policy generation, verification, deployment, and assurance. By replacing tightly- coupled centralized RIC-based workflows with distributed agents, the framework achieves autonomy, resilience, explainability, and system-wide safety. To substantiate this vision, we design and evaluate a traffic steering use case under surge and drift conditions. Results across four KPIs: RRC connected users, IP throughput, PRB utilization, and SINR, demonstrate that a naive predictor-driven deployment improves local KPIs but destabilizes neighbors, whereas the agentic system blocks unsafe policies, preserving global network health. This study highlights multi- agent architectures as a credible foundation for trustworthy AI- driven autonomy in next-generation RANs.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Formalism-Implementation Gap in Reinforcement Learning Research</title>
<link>https://arxiv.org/abs/2510.16175</link>
<guid>https://arxiv.org/abs/2510.16175</guid>
<content:encoded><![CDATA[
arXiv:2510.16175v1 Announce Type: new 
Abstract: The last decade has seen an upswing in interest and adoption of reinforcement learning (RL) techniques, in large part due to its demonstrated capabilities at performing certain tasks at "super-human levels". This has incentivized the community to prioritize research that demonstrates RL agent performance, often at the expense of research aimed at understanding their learning dynamics. Performance-focused research runs the risk of overfitting on academic benchmarks -- thereby rendering them less useful -- which can make it difficult to transfer proposed techniques to novel problems. Further, it implicitly diminishes work that does not push the performance-frontier, but aims at improving our understanding of these techniques. This paper argues two points: (i) RL research should stop focusing solely on demonstrating agent capabilities, and focus more on advancing the science and understanding of reinforcement learning; and (ii) we need to be more precise on how our benchmarks map to the underlying mathematical formalisms. We use the popular Arcade Learning Environment (ALE; Bellemare et al., 2013) as an example of a benchmark that, despite being increasingly considered "saturated", can be effectively used for developing this understanding, and facilitating the deployment of RL techniques in impactful real-world problems.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Coordination in Ad Hoc Teams with Generalized Policy Improvement and Difference Rewards</title>
<link>https://arxiv.org/abs/2510.16187</link>
<guid>https://arxiv.org/abs/2510.16187</guid>
<content:encoded><![CDATA[
arXiv:2510.16187v1 Announce Type: new 
Abstract: Real-world multi-agent systems may require ad hoc teaming, where an agent must coordinate with other previously unseen teammates to solve a task in a zero-shot manner. Prior work often either selects a pretrained policy based on an inferred model of the new teammates or pretrains a single policy that is robust to potential teammates. Instead, we propose to leverage all pretrained policies in a zero-shot transfer setting. We formalize this problem as an ad hoc multi-agent Markov decision process and present a solution that uses two key ideas, generalized policy improvement and difference rewards, for efficient and effective knowledge transfer between different teams. We empirically demonstrate that our algorithm, Generalized Policy improvement for Ad hoc Teaming (GPAT), successfully enables zero-shot transfer to new teams in three simulated environments: cooperative foraging, predator-prey, and Overcooked. We also demonstrate our algorithm in a real-world multi-robot setting.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Operationalising Extended Cognition: Formal Metrics for Corporate Knowledge and Legal Accountability</title>
<link>https://arxiv.org/abs/2510.16193</link>
<guid>https://arxiv.org/abs/2510.16193</guid>
<content:encoded><![CDATA[
arXiv:2510.16193v1 Announce Type: new 
Abstract: Corporate responsibility turns on notions of corporate \textit{mens rea}, traditionally imputed from human agents. Yet these assumptions are under challenge as generative AI increasingly mediates enterprise decision-making. Building on the theory of extended cognition, we argue that in response corporate knowledge may be redefined as a dynamic capability, measurable by the efficiency of its information-access procedures and the validated reliability of their outputs. We develop a formal model that captures epistemic states of corporations deploying sophisticated AI or information systems, introducing a continuous organisational knowledge metric $S_S(\varphi)$ which integrates a pipeline's computational cost and its statistically validated error rate. We derive a thresholded knowledge predicate $\mathsf{K}_S$ to impute knowledge and a firm-wide epistemic capacity index $\mathcal{K}_{S,t}$ to measure overall capability. We then operationally map these quantitative metrics onto the legal standards of actual knowledge, constructive knowledge, wilful blindness, and recklessness. Our work provides a pathway towards creating measurable and justiciable audit artefacts, that render the corporate mind tractable and accountable in the algorithmic age.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2510.16194</link>
<guid>https://arxiv.org/abs/2510.16194</guid>
<content:encoded><![CDATA[
arXiv:2510.16194v1 Announce Type: new 
Abstract: Protected health information (PHI) de-identification is critical for enabling the safe reuse of clinical notes, yet evaluating and comparing PHI de-identification models typically depends on costly, small-scale expert annotations. We present TEAM-PHI, a multi-agent evaluation and selection framework that uses large language models (LLMs) to automatically measure de-identification quality and select the best-performing model without heavy reliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each independently judging the correctness of PHI extractions and outputting structured metrics. Their results are then consolidated through an LLM-based majority voting mechanism that integrates diverse evaluator perspectives into a single, stable, and reproducible ranking. Experiments on a real-world clinical note corpus demonstrate that TEAM-PHI produces consistent and accurate rankings: despite variation across individual evaluators, LLM-based voting reliably converges on the same top-performing systems. Further comparison with ground-truth annotations and human evaluation confirms that the framework's automated rankings closely match supervised evaluation. By combining independent evaluation agents with LLM majority voting, TEAM-PHI offers a practical, secure, and cost-effective solution for automatic evaluation and best-model selection in PHI de-identification, even when ground-truth labels are limited.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SentinelNet: Safeguarding Multi-Agent Collaboration Through Credit-Based Dynamic Threat Detection</title>
<link>https://arxiv.org/abs/2510.16219</link>
<guid>https://arxiv.org/abs/2510.16219</guid>
<content:encoded><![CDATA[
arXiv:2510.16219v1 Announce Type: new 
Abstract: Malicious agents pose significant threats to the reliability and decision-making capabilities of Multi-Agent Systems (MAS) powered by Large Language Models (LLMs). Existing defenses often fall short due to reactive designs or centralized architectures which may introduce single points of failure. To address these challenges, we propose SentinelNet, the first decentralized framework for proactively detecting and mitigating malicious behaviors in multi-agent collaboration. SentinelNet equips each agent with a credit-based detector trained via contrastive learning on augmented adversarial debate trajectories, enabling autonomous evaluation of message credibility and dynamic neighbor ranking via bottom-k elimination to suppress malicious communications. To overcome the scarcity of attack data, it generates adversarial trajectories simulating diverse threats, ensuring robust training. Experiments on MAS benchmarks show SentinelNet achieves near-perfect detection of malicious agents, close to 100% within two debate rounds, and recovers 95% of system accuracy from compromised baselines. By exhibiting strong generalizability across domains and attack patterns, SentinelNet establishes a novel paradigm for safeguarding collaborative MAS.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterogeneous Multi-Agent Task-Assignment with Uncertain Execution Times and Preferences</title>
<link>https://arxiv.org/abs/2510.16221</link>
<guid>https://arxiv.org/abs/2510.16221</guid>
<content:encoded><![CDATA[
arXiv:2510.16221v1 Announce Type: new 
Abstract: While sequential task assignment for a single agent has been widely studied, such problems in a multi-agent setting, where the agents have heterogeneous task preferences or capabilities, remain less well-characterized. We study a multi-agent task assignment problem where a central planner assigns recurring tasks to multiple members of a team over a finite time horizon. For any given task, the members have heterogeneous capabilities in terms of task completion times, task resource consumption (which can model variables such as energy or attention), and preferences in terms of the rewards they collect upon task completion. We assume that the reward, execution time, and resource consumption for each member to complete any task are stochastic with unknown distributions. The goal of the planner is to maximize the total expected reward that the team receives over the problem horizon while ensuring that the resource consumption required for any assigned task is within the capability of the agent. We propose and analyze a bandit algorithm for this problem. Since the bandit algorithm relies on solving an optimal task assignment problem repeatedly, we analyze the achievable regret in two cases: when we can solve the optimal task assignment exactly and when we can solve it only approximately.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScholarEval: Research Idea Evaluation Grounded in Literature</title>
<link>https://arxiv.org/abs/2510.16234</link>
<guid>https://arxiv.org/abs/2510.16234</guid>
<content:encoded><![CDATA[
arXiv:2510.16234v1 Announce Type: new 
Abstract: As AI tools become increasingly common for research ideation, robust evaluation is critical to ensure the validity and usefulness of generated ideas. We introduce ScholarEval, a retrieval augmented evaluation framework that assesses research ideas based on two fundamental criteria: soundness - the empirical validity of proposed methods based on existing literature, and contribution - the degree of advancement made by the idea across different dimensions relative to prior research. To evaluate ScholarEval, we introduce ScholarIdeas, the first expert-annotated dataset of multi-domain research ideas and reviews, comprised of 117 ideas across four disciplines: artificial intelligence, neuroscience, biochemistry, and ecology. Our evaluation shows that ScholarEval achieves significantly higher coverage of points mentioned in the human expert annotated rubrics in ScholarIdeas compared to all baselines. Furthermore, ScholarEval is consistently preferred over our strongest baseline o4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI, in terms of evaluation actionability, depth, and evidence support. Our large-scale user study also shows that ScholarEval significantly outperforms deep research in literature engagement, idea refinement, and usefulness. We openly release our code, dataset, and ScholarEval tool for the community to use and build on.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WEBSERV: A Browser-Server Environment for Efficient Training of Reinforcement Learning-based Web Agents at Scale</title>
<link>https://arxiv.org/abs/2510.16252</link>
<guid>https://arxiv.org/abs/2510.16252</guid>
<content:encoded><![CDATA[
arXiv:2510.16252v1 Announce Type: new 
Abstract: Training and evaluation of Reinforcement Learning (RL) web agents have gained increasing attention, yet a scalable and efficient environment that couples realistic and robust browser-side interaction with controllable server-side state at scale is still missing. Existing environments tend to have one or more of the following issues: they overwhelm policy models with excessive and noisy context; they perform actions non-deterministically without waiting for the UI or network to stabilize; or they cannot scale isolated client-server containers effectively for parallel RL rollouts. We propose WEBSERV, an environment that includes 1) a compact, site-agnostic browser environment that balances context and action complexity, and 2) a scalable RL environment via efficient launching and resetting web-servers to enable scalable RL training and evaluation. We evaluate WEBSERV on the shopping CMS and Gitlab tasks in WebArena, achieving state-of-the-art single-prompt success rates while cutting launch latency by ~5x and storage need by ~240x, with a comparable memory footprint, enabling 200+ concurrent containers on a single host.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Adversarial Fine-tuning with Auditing Agents</title>
<link>https://arxiv.org/abs/2510.16255</link>
<guid>https://arxiv.org/abs/2510.16255</guid>
<content:encoded><![CDATA[
arXiv:2510.16255v1 Announce Type: new 
Abstract: Large Language Model (LLM) providers expose fine-tuning APIs that let end users fine-tune their frontier LLMs. Unfortunately, it has been shown that an adversary with fine-tuning access to an LLM can bypass safeguards. Particularly concerning, such attacks may avoid detection with datasets that are only implicitly harmful. Our work studies robust detection mechanisms for adversarial use of fine-tuning APIs. We introduce the concept of a fine-tuning auditing agent and show it can detect harmful fine-tuning prior to model deployment. We provide our auditing agent with access to the fine-tuning dataset, as well as the fine-tuned and pre-fine-tuned models, and request the agent assigns a risk score for the fine-tuning job. We evaluate our detection approach on a diverse set of eight strong fine-tuning attacks from the literature, along with five benign fine-tuned models, totaling over 1400 independent audits. These attacks are undetectable with basic content moderation on the dataset, highlighting the challenge of the task. With the best set of affordances, our auditing agent achieves a 56.2% detection rate of adversarial fine-tuning at a 1% false positive rate. Most promising, the auditor is able to detect covert cipher attacks that evade safety evaluations and content moderation of the dataset. While benign fine-tuning with unintentional subtle safety degradation remains a challenge, we establish a baseline configuration for further work in this area. We release our auditing agent at https://github.com/safety-research/finetuning-auditor.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?</title>
<link>https://arxiv.org/abs/2510.16263</link>
<guid>https://arxiv.org/abs/2510.16263</guid>
<content:encoded><![CDATA[
arXiv:2510.16263v1 Announce Type: new 
Abstract: The evaluation of Vision-Language-Action (VLA) agents is hindered by the coarse, end-task success metric that fails to provide precise skill diagnosis or measure robustness to real-world perturbations. This challenge is exacerbated by a fragmented data landscape that impedes reproducible research and the development of generalist models. To address these limitations, we introduce \textbf{NEBULA}, a unified ecosystem for single-arm manipulation that enables diagnostic and reproducible evaluation. NEBULA features a novel dual-axis evaluation protocol that combines fine-grained \textit{capability tests} for precise skill diagnosis with systematic \textit{stress tests} that measure robustness. A standardized API and a large-scale, aggregated dataset are provided to reduce fragmentation and support cross-dataset training and fair comparison. Using NEBULA, we demonstrate that top-performing VLAs struggle with key capabilities such as spatial reasoning and dynamic adaptation, which are consistently obscured by conventional end-task success metrics. By measuring both what an agent can do and when it does so reliably, NEBULA provides a practical foundation for robust, general-purpose embodied agents.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Limits Agentic Systems Efficiency?</title>
<link>https://arxiv.org/abs/2510.16276</link>
<guid>https://arxiv.org/abs/2510.16276</guid>
<content:encoded><![CDATA[
arXiv:2510.16276v1 Announce Type: new 
Abstract: Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have demonstrated strong reasoning capabilities. To further enhance LLM capabilities, recent agentic systems, such as Deep Research, incorporate web interactions into LLM reasoning to mitigate uncertainties and reduce potential errors. However, existing research predominantly focuses on reasoning performance, often neglecting the efficiency of agentic systems. In this work, we present a comprehensive empirical study that identifies efficiency bottlenecks in web-interactive agentic systems. We decompose end-to-end latency into two primary components: LLM API latency and web environment latency. We conduct a comprehensive empirical study across 15 models and 5 providers to demonstrate high variability in API-based agentic systems. We observe that web environment latency can contribute as much as 53.7% to the overall latency in a web-based agentic system. To improve latency, we propose SpecCache, a caching framework augmented with speculative execution that can reduce web environment overhead. Extensive evaluations on two standard benchmarks show that our approach improves the cache hit rate by up to 58x compared to a random caching strategy, while reducing web environment overhead by up to 3.2x, without degrading agentic system performance.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Smart Manufacturing Metaverse via Digital Twinning in Extended Reality</title>
<link>https://arxiv.org/abs/2510.16280</link>
<guid>https://arxiv.org/abs/2510.16280</guid>
<content:encoded><![CDATA[
arXiv:2510.16280v1 Announce Type: new 
Abstract: The rapid evolution of modern manufacturing systems is driven by the integration of emerging metaverse technologies such as artificial intelligence (AI), digital twin (DT) with different forms of extended reality (XR) like virtual reality (VR), augmented reality (AR), and mixed reality (MR). These advances confront manufacturing workers with complex and evolving environments that demand digital literacy for problem solving in the future workplace. However, manufacturing industry faces a critical shortage of skilled workforce with digital literacy in the world. Further, global pandemic has significantly changed how people work and collaborate digitally and remotely. There is an urgent need to rethink digital platformization and leverage emerging technologies to propel industrial evolution toward human-centered manufacturing metaverse (MfgVerse). This paper presents a forward-looking perspective on the development of smart MfgVerse, highlighting current efforts in learning factory, cognitive digital twinning, and the new sharing economy of manufacturing-as-a-service (MaaS). MfgVerse is converging into multiplex networks, including a social network of human stakeholders, an interconnected network of manufacturing things or agents (e.g., machines, robots, facilities, material handling systems), a network of digital twins of physical things, as well as auxiliary networks of sales, supply chain, logistics, and remanufacturing systems. We also showcase the design and development of a learning factory for workforce training in extended reality. Finally, future directions, challenges, and opportunities are discussed for human-centered manufacturing metaverse. We hope this work helps stimulate more comprehensive studies and in-depth research efforts to advance MfgVerse technologies.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating LLM and Diffusion-Based Agents for Social Simulation</title>
<link>https://arxiv.org/abs/2510.16366</link>
<guid>https://arxiv.org/abs/2510.16366</guid>
<content:encoded><![CDATA[
arXiv:2510.16366v1 Announce Type: new 
Abstract: Agent-based social simulation provides a valuable methodology for predicting social information diffusion, yet existing approaches face two primary limitations. Traditional agent models often rely on rigid behavioral rules and lack semantic understanding of textual content, while emerging large language model (LLM)-based agents incur prohibitive computational costs at scale. To address these challenges, we propose a hybrid simulation framework that strategically integrates LLM-driven agents with diffusion model-based agents. The framework employs LLM-based agents to simulate a core subset of users with rich semantic reasoning, while a diffusion model handles the remaining population efficiently. Although the two agent types operate on disjoint user groups, both incorporate key factors including user personalization, social influence, and content awareness, and interact through a coordinated simulation process. Extensive experiments on three real-world datasets demonstrate that our framework outperforms existing methods in prediction accuracy, validating the effectiveness of its modular design.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents</title>
<link>https://arxiv.org/abs/2510.16381</link>
<guid>https://arxiv.org/abs/2510.16381</guid>
<content:encoded><![CDATA[
arXiv:2510.16381v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities, yet their deployment in high-stakes domains is hindered by inherent limitations in trustworthiness, including hallucinations, instability, and a lack of transparency. To address these challenges, we introduce a generic neuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The core of our approach lies in decoupling tasks into two distinct phases: Offline knowledge ingestion and online task processing. During knowledge ingestion, an LLM translates an informal problem specification into a formal, symbolic knowledge base. This formal representation is crucial as it can be verified and refined by human experts, ensuring its correctness and alignment with domain requirements. In the subsequent task processing phase, each incoming input is encoded into the same formal language. A symbolic decision engine then utilizes this encoded input in conjunction with the formal knowledge base to derive a reliable result. Through an extensive evaluation on a complex reasoning task, we demonstrate that a concrete implementation of ATA is competitive with state-of-the-art end-to-end reasoning models in a fully automated setup while maintaining trustworthiness. Crucially, with a human-verified and corrected knowledge base, our approach significantly outperforms even larger models, while exhibiting perfect determinism, enhanced stability against input perturbations, and inherent immunity to prompt injection attacks. By generating decisions grounded in symbolic reasoning, ATA offers a practical and controllable architecture for building the next generation of transparent, auditable, and reliable autonomous agents.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainability Requirements as Hyperproperties</title>
<link>https://arxiv.org/abs/2510.16402</link>
<guid>https://arxiv.org/abs/2510.16402</guid>
<content:encoded><![CDATA[
arXiv:2510.16402v1 Announce Type: new 
Abstract: Explainability is emerging as a key requirement for autonomous systems. While many works have focused on what constitutes a valid explanation, few have considered formalizing explainability as a system property. In this work, we approach this problem from the perspective of hyperproperties. We start with a combination of three prominent flavors of modal logic and show how they can be used for specifying and verifying counterfactual explainability in multi-agent systems: With Lewis' counterfactuals, linear-time temporal logic, and a knowledge modality, we can reason about whether agents know why a specific observation occurs, i.e., whether that observation is explainable to them. We use this logic to formalize multiple notions of explainability on the system level. We then show how this logic can be embedded into a hyperlogic. Notably, from this analysis we conclude that the model-checking problem of our logic is decidable, which paves the way for the automated verification of explainability requirements.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting</title>
<link>https://arxiv.org/abs/2510.16410</link>
<guid>https://arxiv.org/abs/2510.16410</guid>
<content:encoded><![CDATA[
arXiv:2510.16410v1 Announce Type: new 
Abstract: Bridging the gap between complex human instructions and precise 3D object grounding remains a significant challenge in vision and robotics. Existing 3D segmentation methods often struggle to interpret ambiguous, reasoning-based instructions, while 2D vision-language models that excel at such reasoning lack intrinsic 3D spatial understanding. In this paper, we introduce REALM, an innovative MLLM-agent framework that enables open-world reasoning-based segmentation without requiring extensive 3D-specific post-training. We perform segmentation directly on 3D Gaussian Splatting representations, capitalizing on their ability to render photorealistic novel views that are highly suitable for MLLM comprehension. As directly feeding one or more rendered views to the MLLM can lead to high sensitivity to viewpoint selection, we propose a novel Global-to-Local Spatial Grounding strategy. Specifically, multiple global views are first fed into the MLLM agent in parallel for coarse-level localization, aggregating responses to robustly identify the target object. Then, several close-up novel views of the object are synthesized to perform fine-grained local segmentation, yielding accurate and consistent 3D masks. Extensive experiments show that REALM achieves remarkable performance in interpreting both explicit and implicit instructions across LERF, 3D-OVS, and our newly introduced REALM3D benchmarks. Furthermore, our agent framework seamlessly supports a range of 3D interaction tasks, including object removal, replacement, and style transfer, demonstrating its practical utility and versatility. Project page: https://ChangyueShi.github.io/REALM.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FourierCompress: Layer-Aware Spectral Activation Compression for Efficient and Accurate Collaborative LLM Inference</title>
<link>https://arxiv.org/abs/2510.16418</link>
<guid>https://arxiv.org/abs/2510.16418</guid>
<content:encoded><![CDATA[
arXiv:2510.16418v1 Announce Type: new 
Abstract: Collaborative large language model (LLM) inference enables real-time, privacy-preserving AI services on resource-constrained edge devices by partitioning computational workloads between client devices and edge servers. However, this paradigm is severely hindered by communication bottlenecks caused by the transmission of high-dimensional intermediate activations, exacerbated by the autoregressive decoding structure of LLMs, where bandwidth consumption scales linearly with output length. Existing activation compression methods struggle to simultaneously achieve high compression ratios, low reconstruction error, and computational efficiency. This paper proposes FourierCompress, a novel, layer-aware activation compression framework that exploits the frequency-domain sparsity of LLM activations. We rigorously demonstrate that activations from the first Transformer layer exhibit strong smoothness and energy concentration in the low-frequency domain, making them highly amenable to near-lossless compression via the Fast Fourier Transform (FFT). FourierCompress transforms activations into the frequency domain, retains only a compact block of low-frequency coefficients, and reconstructs the signal at the server using conjugate symmetry, enabling seamless hardware acceleration on DSPs and FPGAs. Extensive experiments on Llama 3 and Qwen2.5 models across 10 commonsense reasoning datasets demonstrate that FourierCompress preserves performance remarkably close to the uncompressed baseline, outperforming Top-k, QR, and SVD. FourierCompress bridges the gap between communication efficiency (an average 7.6x reduction in activation size), near-lossless inference (less than 0.3% average accuracy loss), and significantly faster compression (achieving over 32x reduction in compression time compared to Top-k via hardware acceleration) for edge-device LLM inference.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba</title>
<link>https://arxiv.org/abs/2510.16444</link>
<guid>https://arxiv.org/abs/2510.16444</guid>
<content:encoded><![CDATA[
arXiv:2510.16444v1 Announce Type: new 
Abstract: Referring Atomic Video Action Recognition (RAVAR) aims to recognize fine-grained, atomic-level actions of a specific person of interest conditioned on natural language descriptions. Distinct from conventional action recognition and detection tasks, RAVAR emphasizes precise language-guided action understanding, which is particularly critical for interactive human action analysis in complex multi-person scenarios. In this work, we extend our previously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million frames and >75.1k annotated persons in total. We benchmark this dataset using baselines from multiple related domains, including atomic action localization, video question answering, and text-video retrieval, as well as our earlier model, RefAtomNet. Although RefAtomNet surpasses other baselines by incorporating agent attention to highlight salient features, its ability to align and retrieve cross-modal information remains limited, leading to suboptimal performance in localizing the target person and predicting fine-grained actions. To overcome the aforementioned limitations, we introduce RefAtomNet++, a novel framework that advances cross-modal token aggregation through a multi-hierarchical semantic-aligned cross-attention mechanism combined with multi-trajectory Mamba modeling at the partial-keyword, scene-attribute, and holistic-sentence levels. In particular, scanning trajectories are constructed by dynamically selecting the nearest visual spatial tokens at each timestep for both partial-keyword and scene-attribute levels. Moreover, we design a multi-hierarchical semantic-aligned cross-attention strategy, enabling more effective aggregation of spatial and temporal tokens across different semantic hierarchies. Experiments show that RefAtomNet++ establishes new state-of-the-art results. The dataset and code are released at https://github.com/KPeng9510/refAVA2.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation</title>
<link>https://arxiv.org/abs/2510.16457</link>
<guid>https://arxiv.org/abs/2510.16457</guid>
<content:encoded><![CDATA[
arXiv:2510.16457v1 Announce Type: new 
Abstract: In this work we concentrate on the task of goal-oriented Vision-and-Language Navigation (VLN). Existing methods often make decisions based on historical information, overlooking the future implications and long-term outcomes of the actions. In contrast, we aim to develop a foresighted agent. Specifically, we draw upon Q-learning to train a Q-model using large-scale unlabeled trajectory data, in order to learn the general knowledge regarding the layout and object relations within indoor scenes. This model can generate a Q-feature, analogous to the Q-value in traditional Q-network, for each candidate action, which describes the potential future information that may be observed after taking the specific action. Subsequently, a cross-modal future encoder integrates the task-agnostic Q-feature with navigation instructions to produce a set of action scores reflecting future prospects. These scores, when combined with the original scores based on history, facilitate an A*-style searching strategy to effectively explore the regions that are more likely to lead to the destination. Extensive experiments conducted on widely used goal-oriented VLN datasets validate the effectiveness of the proposed method.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Declarative Techniques for NL Queries over Heterogeneous Data</title>
<link>https://arxiv.org/abs/2510.16470</link>
<guid>https://arxiv.org/abs/2510.16470</guid>
<content:encoded><![CDATA[
arXiv:2510.16470v1 Announce Type: new 
Abstract: In many industrial settings, users wish to ask questions in natural language, the answers to which require assembling information from diverse structured data sources. With the advent of Large Language Models (LLMs), applications can now translate natural language questions into a set of API calls or database calls, execute them, and combine the results into an appropriate natural language response. However, these applications remain impractical in realistic industrial settings because they do not cope with the data source heterogeneity that typifies such environments. In this work, we simulate the heterogeneity of real industry settings by introducing two extensions of the popular Spider benchmark dataset that require a combination of database and API calls. Then, we introduce a declarative approach to handling such data heterogeneity and demonstrate that it copes with data source heterogeneity significantly better than state-of-the-art LLM-based agentic or imperative code generation systems. Our augmented benchmarks are available to the research community.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety</title>
<link>https://arxiv.org/abs/2510.16492</link>
<guid>https://arxiv.org/abs/2510.16492</guid>
<content:encoded><![CDATA[
arXiv:2510.16492v1 Announce Type: new 
Abstract: As Large Language Model (LLM) agents increasingly operate in complex environments with real-world consequences, their safety becomes critical. While uncertainty quantification is well-studied for single-turn tasks, multi-turn agentic scenarios with real-world tool access present unique challenges where uncertainties and ambiguities compound, leading to severe or catastrophic risks beyond traditional text generation failures. We propose using "quitting" as a simple yet effective behavioral mechanism for LLM agents to recognize and withdraw from situations where they lack confidence. Leveraging the ToolEmu framework, we conduct a systematic evaluation of quitting behavior across 12 state-of-the-art LLMs. Our results demonstrate a highly favorable safety-helpfulness trade-off: agents prompted to quit with explicit instructions improve safety by an average of +0.39 on a 0-3 scale across all models (+0.64 for proprietary models), while maintaining a negligible average decrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding explicit quit instructions proves to be a highly effective safety mechanism that can immediately be deployed in existing agent systems, and establishes quitting as an effective first-line defense mechanism for autonomous agents in high-stakes applications.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection</title>
<link>https://arxiv.org/abs/2510.16499</link>
<guid>https://arxiv.org/abs/2510.16499</guid>
<content:encoded><![CDATA[
arXiv:2510.16499v1 Announce Type: new 
Abstract: Designing effective agentic systems requires the seamless composition and integration of agents, tools, and models within dynamic and uncertain environments. Most existing methods rely on static, semantic retrieval approaches for tool or agent discovery. However, effective reuse and composition of existing components remain challenging due to incomplete capability descriptions and the limitations of retrieval methods. Component selection suffers because the decisions are not based on capability, cost, and real-time utility. To address these challenges, we introduce a structured, automated framework for agentic system composition that is inspired by the knapsack problem. Our framework enables a composer agent to systematically identify, select, and assemble an optimal set of agentic components by jointly considering performance, budget constraints, and compatibility. By dynamically testing candidate components and modeling their utility in real-time, our approach streamlines the assembly of agentic systems and facilitates scalable reuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five benchmarking datasets shows that our online-knapsack-based composer consistently lies on the Pareto frontier, achieving higher success rates at significantly lower component costs compared to our baselines. In the single-agent setup, the online knapsack composer shows a success rate improvement of up to 31.6% in comparison to the retrieval baselines. In multi-agent systems, the online knapsack composer increases success rate from 37% to 87% when agents are selected from an agent inventory of 100+ agents. The substantial performance gap confirms the robust adaptability of our method across diverse domains and budget constraints.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LANPO: Bootstrapping Language and Numerical Feedback for Reinforcement Learning in LLMs</title>
<link>https://arxiv.org/abs/2510.16552</link>
<guid>https://arxiv.org/abs/2510.16552</guid>
<content:encoded><![CDATA[
arXiv:2510.16552v1 Announce Type: new 
Abstract: Reinforcement learning in large language models (LLMs) often relies on scalar rewards, a practice that discards valuable textual rationale buried in the rollouts, forcing the model to explore \textit{de novo} with each attempt and hindering sample efficiency. While LLMs can uniquely learn from language feedback provided in-context, naively integrating on-line experiences into RL training presents a paradox: feedback from the same problem risks information leakage and memorization, while feedback from different problems often leads to behavior collapse due to irrelevant context. To resolve this tension, we propose \textbf{Language-And-Numerical Policy Optimization (LANPO)}, a framework that cleanly separates the roles of feedback: language guides exploration, while numerical rewards drive optimization. LANPO builds a dynamic experience pool from past trials and introduces two principles to ensure feedback is effective: \emph{Reward-Agnostic Reflection} for safe intra-sample self-correction and \emph{Relevant Abstraction} to distill generalizable lessons from inter-sample experiences. Across mathematical reasoning benchmarks, LANPO enables 7B and 14B models to significantly outperform strong baselines trained with GRPO in test accuracy. Our work provides a robust method for integrating historical experiences into the LLM RL loop, creating more effective and data-efficient learning agents.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BuildArena: A Physics-Aligned Interactive Benchmark of LLMs for Engineering Construction</title>
<link>https://arxiv.org/abs/2510.16559</link>
<guid>https://arxiv.org/abs/2510.16559</guid>
<content:encoded><![CDATA[
arXiv:2510.16559v1 Announce Type: new 
Abstract: Engineering construction automation aims to transform natural language specifications into physically viable structures, requiring complex integrated reasoning under strict physical constraints. While modern LLMs possess broad knowledge and strong reasoning capabilities that make them promising candidates for this domain, their construction competencies remain largely unevaluated. To address this gap, we introduce BuildArena, the first physics-aligned interactive benchmark designed for language-driven engineering construction. It contributes to the community in four aspects: (1) a highly customizable benchmarking framework for in-depth comparison and analysis of LLMs; (2) an extendable task design strategy spanning static and dynamic mechanics across multiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for supporting construction based on language instructions; (4) a baseline LLM agentic workflow that effectively evaluates diverse model capabilities. On eight frontier LLMs, BuildArena comprehensively evaluates their capabilities for language-driven and physics-grounded construction automation. The project page is at https://build-arena.github.io/.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ripple Effect Protocol: Coordinating Agent Populations</title>
<link>https://arxiv.org/abs/2510.16572</link>
<guid>https://arxiv.org/abs/2510.16572</guid>
<content:encoded><![CDATA[
arXiv:2510.16572v1 Announce Type: new 
Abstract: Modern AI agents can exchange messages using protocols such as A2A and ACP, yet these mechanisms emphasize communication over coordination. As agent populations grow, this limitation produces brittle collective behavior, where individually smart agents converge on poor group outcomes. We introduce the Ripple Effect Protocol (REP), a coordination protocol in which agents share not only their decisions but also lightweight sensitivities - signals expressing how their choices would change if key environmental variables shifted. These sensitivities ripple through local networks, enabling groups to align faster and more stably than with agent-centric communication alone. We formalize REP's protocol specification, separating required message schemas from optional aggregation rules, and evaluate it across scenarios with varying incentives and network topologies. Benchmarks across three domains: (i) supply chain cascades (Beer Game), (ii) preference aggregation in sparse networks (Movie Scheduling), and (iii) sustainable resource allocation (Fishbanks) show that REP improves coordination accuracy and efficiency over A2A by 41 to 100%, while flexibly handling multimodal sensitivity signals from LLMs. By making coordination a protocol-level capability, REP provides scalable infrastructure for the emerging Internet of Agents
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Optimization via Retrieved Reasoning Assets and Multi-Agent Analysis</title>
<link>https://arxiv.org/abs/2510.16635</link>
<guid>https://arxiv.org/abs/2510.16635</guid>
<content:encoded><![CDATA[
arXiv:2510.16635v1 Announce Type: new 
Abstract: Prompt optimization has emerged as an effective alternative to retraining for improving the performance of Large Language Models (LLMs). However, most existing approaches treat evaluation as a black box, relying solely on numerical scores while offering limited insight into why a prompt succeeds or fails. They also depend heavily on trial-and-error refinements, which are difficult to interpret and control. In this paper, we introduce MA-SAPO, a Multi-Agent framework for Score-Aware Prompt Optimization. Compared to prior methods, MA-SAPO explicitly couples evaluation outcomes with structured reasoning to guide systematic edits. The framework specifically consists of two stages: during the Reasoning Phase, agents collaboratively explain metric scores, diagnose weaknesses, and synthesize targeted refinements that are stored as reusable reasoning assets; during the Test Phase, agents retrieve these assets to analyze optimized prompts and apply only evidence-grounded edits. By turning evaluation signals into interpretable reasoning chains, MA-SAPO produces prompt refinements that are more transparent, auditable, and controllable. Experiments on the HelpSteer1/2 benchmarks demonstrate consistent improvements over single-pass prompting, retrieval-augmented baselines, and prior multi-agent strategies, validating the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2510.16645</link>
<guid>https://arxiv.org/abs/2510.16645</guid>
<content:encoded><![CDATA[
arXiv:2510.16645v1 Announce Type: new 
Abstract: Large Language Models (LLMs) demonstrate strong performance but often lack interpretable reasoning. This paper introduces the Multi-Agent Collaboration Framework for Diverse Thinking Modes (DiMo), which enhances both performance and interpretability by simulating a structured debate among four specialized LLM agents. Each agent embodies a distinct reasoning paradigm, allowing the framework to collaboratively explore diverse cognitive approaches. Through iterative debate, agents challenge and refine initial responses, yielding more robust conclusions and an explicit, auditable reasoning chain. Across six benchmarks and under a unified open-source setup, DiMo improves accuracy over widely used single-model and debate baselines, with the largest gains on math. We position DiMo as a semantics-aware, Web-native multi-agent framework: it models human-machine intelligence with LLM agents that produce semantically typed, URL-annotated evidence chains for explanations and user-friendly interactions. Although our experiments use standard reasoning benchmarks, the framework is designed to be instantiated over Web corpora and knowledge graphs, combining retrieval-augmented reasoning with structured justifications that downstream systems can inspect and reuse.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pursuing Minimal Sufficiency in Spatial Reasoning</title>
<link>https://arxiv.org/abs/2510.16688</link>
<guid>https://arxiv.org/abs/2510.16688</guid>
<content:encoded><![CDATA[
arXiv:2510.16688v1 Announce Type: new 
Abstract: Spatial reasoning, the ability to ground language in 3D understanding, remains a persistent challenge for Vision-Language Models (VLMs). We identify two fundamental bottlenecks: inadequate 3D understanding capabilities stemming from 2D-centric pre-training, and reasoning failures induced by redundant 3D information. To address these, we first construct a Minimal Sufficient Set (MSS) of information before answering a given question: a compact selection of 3D perception results from \textit{expert models}. We introduce MSSR (Minimal Sufficient Spatial Reasoner), a dual-agent framework that implements this principle. A Perception Agent programmatically queries 3D scenes using a versatile perception toolbox to extract sufficient information, including a novel SOG (Situated Orientation Grounding) module that robustly extracts language-grounded directions. A Reasoning Agent then iteratively refines this information to pursue minimality, pruning redundant details and requesting missing ones in a closed loop until the MSS is curated. Extensive experiments demonstrate that our method, by explicitly pursuing both sufficiency and minimality, significantly improves accuracy and achieves state-of-the-art performance across two challenging benchmarks. Furthermore, our framework produces interpretable reasoning paths, offering a promising source of high-quality training data for future models. Source code is available at https://github.com/gyj155/mssr.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems</title>
<link>https://arxiv.org/abs/2510.16701</link>
<guid>https://arxiv.org/abs/2510.16701</guid>
<content:encoded><![CDATA[
arXiv:2510.16701v1 Announce Type: new 
Abstract: Complex vehicle routing problems (VRPs) remain a fundamental challenge, demanding substantial expert effort for intent interpretation and algorithm design. While large language models (LLMs) offer a promising path toward automation, current approaches still rely on external intervention, which restrict autonomy and often lead to execution errors and low solution feasibility. To address these challenges, we propose an Agentic Framework with LLMs (AFL) for solving complex vehicle routing problems, achieving full automation from problem instance to solution. AFL directly extracts knowledge from raw inputs and enables self-contained code generation without handcrafted modules or external solvers. To improve trustworthiness, AFL decomposes the overall pipeline into three manageable subtasks and employs four specialized agents whose coordinated interactions enforce cross-functional consistency and logical soundness. Extensive experiments on 60 complex VRPs, ranging from standard benchmarks to practical variants, validate the effectiveness and generality of our framework, showing comparable performance against meticulously designed algorithms. Notably, it substantially outperforms existing LLM-based baselines in both code reliability and solution feasibility, achieving rates close to 100% on the evaluated benchmarks.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI</title>
<link>https://arxiv.org/abs/2510.16720</link>
<guid>https://arxiv.org/abs/2510.16720</guid>
<content:encoded><![CDATA[
arXiv:2510.16720v1 Announce Type: new 
Abstract: The rapid evolution of agentic AI marks a new phase in artificial intelligence, where Large Language Models (LLMs) no longer merely respond but act, reason, and adapt. This survey traces the paradigm shift in building agentic AI: from Pipeline-based systems, where planning, tool use, and memory are orchestrated by external logic, to the emerging Model-native paradigm, where these capabilities are internalized within the model's parameters. We first position Reinforcement Learning (RL) as the algorithmic engine enabling this paradigm shift. By reframing learning from imitating static data to outcome-driven exploration, RL underpins a unified solution of LLM + RL + Task across language, vision and embodied domains. Building on this, the survey systematically reviews how each capability -- Planning, Tool use, and Memory -- has evolved from externally scripted modules to end-to-end learned behaviors. Furthermore, it examines how this paradigm shift has reshaped major agent applications, specifically the Deep Research agent emphasizing long-horizon reasoning and the GUI agent emphasizing embodied interaction. We conclude by discussing the continued internalization of agentic capabilities like Multi-agent collaboration and Reflection, alongside the evolving roles of the system and model layers in future agentic AI. Together, these developments outline a coherent trajectory toward model-native agentic AI as an integrated learning and interaction framework, marking the transition from constructing systems that apply intelligence to developing models that grow intelligence through experience.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications</title>
<link>https://arxiv.org/abs/2510.16724</link>
<guid>https://arxiv.org/abs/2510.16724</guid>
<content:encoded><![CDATA[
arXiv:2510.16724v1 Announce Type: new 
Abstract: The advent of large language models (LLMs) has transformed information access and reasoning through open-ended natural language interaction. However, LLMs remain limited by static knowledge, factual hallucinations, and the inability to retrieve real-time or domain-specific information. Retrieval-Augmented Generation (RAG) mitigates these issues by grounding model outputs in external evidence, but traditional RAG pipelines are often single turn and heuristic, lacking adaptive control over retrieval and reasoning. Recent advances in agentic search address these limitations by enabling LLMs to plan, retrieve, and reflect through multi-step interaction with search environments. Within this paradigm, reinforcement learning (RL) offers a powerful mechanism for adaptive and self-improving search behavior. This survey provides the first comprehensive overview of \emph{RL-based agentic search}, organizing the emerging field along three complementary dimensions: (i) What RL is for (functional roles), (ii) How RL is used (optimization strategies), and (iii) Where RL is applied (scope of optimization). We summarize representative methods, evaluation protocols, and applications, and discuss open challenges and future directions toward building reliable and scalable RL driven agentic search systems. We hope this survey will inspire future research on the integration of RL and agentic search. Our repository is available at https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey on World Models for Embodied AI</title>
<link>https://arxiv.org/abs/2510.16732</link>
<guid>https://arxiv.org/abs/2510.16732</guid>
<content:encoded><![CDATA[
arXiv:2510.16732v1 Announce Type: new 
Abstract: Embodied AI requires agents that perceive, act, and anticipate how actions reshape future world states. World models serve as internal simulators that capture environment dynamics, enabling forward and counterfactual rollouts to support perception, prediction, and decision making. This survey presents a unified framework for world models in embodied AI. Specifically, we formalize the problem setting and learning objectives, and propose a three-axis taxonomy encompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2) Temporal Modeling, Sequential Simulation and Inference vs. Global Difference Prediction; (3) Spatial Representation, Global Latent Vector, Token Feature Sequence, Spatial Latent Grid, and Decomposed Rendering Representation. We systematize data resources and metrics across robotics, autonomous driving, and general video settings, covering pixel prediction quality, state-level understanding, and task performance. Furthermore, we offer a quantitative comparison of state-of-the-art models and distill key open challenges, including the scarcity of unified datasets and the need for evaluation metrics that assess physical consistency over pixel fidelity, the trade-off between model performance and the computational efficiency required for real-time control, and the core modeling difficulty of achieving long-horizon temporal consistency while mitigating error accumulation. Finally, we maintain a curated bibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surrogate Modeling and Explainable Artificial Intelligence for Complex Systems: A Workflow for Automated Simulation Exploration</title>
<link>https://arxiv.org/abs/2510.16742</link>
<guid>https://arxiv.org/abs/2510.16742</guid>
<content:encoded><![CDATA[
arXiv:2510.16742v1 Announce Type: new 
Abstract: Complex systems are increasingly explored through simulation-driven engineering workflows that combine physics-based and empirical models with optimization and analytics. Despite their power, these workflows face two central obstacles: (1) high computational cost, since accurate exploration requires many expensive simulator runs; and (2) limited transparency and reliability when decisions rely on opaque blackbox components. We propose a workflow that addresses both challenges by training lightweight emulators on compact designs of experiments that (i) provide fast, low-latency approximations of expensive simulators, (ii) enable rigorous uncertainty quantification, and (iii) are adapted for global and local Explainable Artificial Intelligence (XAI) analyses. This workflow unifies every simulation-based complex-system analysis tool, ranging from engineering design to agent-based models for socio-environmental understanding. In this paper, we proposea comparative methodology and practical recommendations for using surrogate-based explainability tools within the proposed workflow. The methodology supports continuous and categorical inputs, combines global-effect and uncertainty analyses with local attribution, and evaluates the consistency of explanations across surrogate models, thereby diagnosing surrogate adequacy and guiding further data collection or model refinement. We demonstrate the approach on two contrasting case studies: a multidisciplinary design analysis of a hybrid-electric aircraft and an agent-based model of urban segregation. Results show that the surrogate model and XAI coupling enables large-scale exploration in seconds, uncovers nonlinear interactions and emergent behaviors, identifies key design and policy levers, and signals regions where surrogates require more data or alternative architectures.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial Games</title>
<link>https://arxiv.org/abs/2510.16761</link>
<guid>https://arxiv.org/abs/2510.16761</guid>
<content:encoded><![CDATA[
arXiv:2510.16761v1 Announce Type: new 
Abstract: Existing language agents often encounter difficulties in dynamic adversarial games due to poor strategic reasoning. To mitigate this limitation, a promising approach is to allow agents to learn from game interactions automatically, without relying on costly expert-labeled data. Unlike static environments where agents receive fixed feedback or rewards, selecting appropriate opponents in dynamic adversarial games can significantly impact learning performance. However, the discussion of opponents in adversarial environments remains an area under exploration. In this paper, we propose a Step-level poliCy Optimization method through Play-And-Learn, SCO-PAL. Leveraging SCO-PAL, we conduct a detailed analysis of opponent selection by setting opponents at different levels and find that self-play is the most effective way to improve strategic reasoning in such adversarial environments. Utilizing SCO-PAL with self-play, we increase the average win rate against four opponents by approximately 30% compared to baselines and achieve a 54.76% win rate against GPT-4 in six adversarial games.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.16769</link>
<guid>https://arxiv.org/abs/2510.16769</guid>
<content:encoded><![CDATA[
arXiv:2510.16769v1 Announce Type: new 
Abstract: Vision-language models (VLMs) have shown promise in graph understanding, but remain limited by input-token constraints, facing scalability bottlenecks and lacking effective mechanisms to coordinate textual and visual modalities. To address these challenges, we propose GraphVista, a unified framework that enhances both scalability and modality coordination in graph understanding. For scalability, GraphVista organizes graph information hierarchically into a lightweight GraphRAG base, which retrieves only task-relevant textual descriptions and high-resolution visual subgraphs, compressing redundant context while preserving key reasoning elements. For modality coordination, GraphVista introduces a planning agent that routes tasks to the most suitable modality-using the text modality for simple property reasoning and the visual modality for local and structurally complex reasoning grounded in explicit topology. Extensive experiments demonstrate that GraphVista scales to large graphs, up to $200\times$ larger than those used in existing benchmarks, and consistently outperforms existing textual, visual, and fusion-based methods, achieving up to $4.4\times$ quality improvement over the state-of-the-art baselines by fully exploiting the complementary strengths of both modalities.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to play: A Multimodal Agent for 3D Game-Play</title>
<link>https://arxiv.org/abs/2510.16774</link>
<guid>https://arxiv.org/abs/2510.16774</guid>
<content:encoded><![CDATA[
arXiv:2510.16774v1 Announce Type: new 
Abstract: We argue that 3-D first-person video games are a challenging environment for real-time multi-modal reasoning. We first describe our dataset of human game-play, collected across a large variety of 3-D first-person games, which is both substantially larger and more diverse compared to prior publicly disclosed datasets, and contains text instructions. We demonstrate that we can learn an inverse dynamics model from this dataset, which allows us to impute actions on a much larger dataset of publicly available videos of human game play that lack recorded actions. We then train a text-conditioned agent for game playing using behavior cloning, with a custom architecture capable of realtime inference on a consumer GPU. We show the resulting model is capable of playing a variety of 3-D games and responding to text input. Finally, we outline some of the remaining challenges such as long-horizon tasks and quantitative evaluation across a large set of games.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More with Less: An Empirical Study of Turn-Control Strategies for Efficient Coding Agents</title>
<link>https://arxiv.org/abs/2510.16786</link>
<guid>https://arxiv.org/abs/2510.16786</guid>
<content:encoded><![CDATA[
arXiv:2510.16786v1 Announce Type: new 
Abstract: LLM-powered coding agents, which operate in iterative loops (turns) to solve software engineering tasks, are becoming increasingly powerful. However, their practical deployment is hindered by significant and unpredictable costs. This challenge arises from a combination of factors: quadratically growing token counts with each turn, the high price of models, the large number of turns required for real-world tasks, and the tendency of agents to take inefficient or unnecessary actions. While existing research focuses on optimizing individual turns, the strategic control of the total number of turns remains an underexplored area for managing agent performance and cost. To address this gap, we conduct a comprehensive empirical study on SWE-bench using three state-of-the-art models and evaluate the impact of three distinct turn-control strategies: an unrestricted baseline, a fixed-turn limit with reminders, and a novel dynamic-turn strategy that grants extensions on-demand. Our findings first reveal a fundamental trade-off in the unrestricted setting, where no single model excels across performance, cost, and turn efficiency. We then show that a fixed-turn limit, specifically at the 75th percentile of the baseline, serves as a "sweet spot", substantially reducing costs (by 24%-68%) with minimal impact on solve rates. Most significantly, the dynamic-turn strategy consistently outperforms fixed-limit approaches, achieving comparable or better solve rates while further reducing costs by an additional 12%-24% by intelligently allocating resources only to tasks that need them. This work provides the first systematic analysis of turn-control strategies, offering simple yet effective guidelines for developers to balance cost and efficacy. We demonstrate that dynamic resource allocation is a superior, easy-to-implement approach for deploying powerful yet economically viable coding agents.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Learning is Suboptimal in Causal Bandits</title>
<link>https://arxiv.org/abs/2510.16811</link>
<guid>https://arxiv.org/abs/2510.16811</guid>
<content:encoded><![CDATA[
arXiv:2510.16811v1 Announce Type: new 
Abstract: We study regret minimization in causal bandits under causal sufficiency where the underlying causal structure is not known to the agent. Previous work has focused on identifying the reward's parents and then applying classic bandit methods to them, or jointly learning the parents while minimizing regret. We investigate whether such strategies are optimal. Somewhat counterintuitively, our results show that learning the parent set is suboptimal. We do so by proving that there exist instances where regret minimization and parent identification are fundamentally conflicting objectives. We further analyze both the known and unknown parent set size regimes, establish novel regret lower bounds that capture the combinatorial structure of the action space. Building on these insights, we propose nearly optimal algorithms that bypass graph and parent recovery, demonstrating that parent identification is indeed unnecessary for regret minimization. Experiments confirm that there exists a large performance gap between our method and existing baselines in various environments.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient High-Accuracy PDEs Solver with the Linear Attention Neural Operator</title>
<link>https://arxiv.org/abs/2510.16816</link>
<guid>https://arxiv.org/abs/2510.16816</guid>
<content:encoded><![CDATA[
arXiv:2510.16816v1 Announce Type: new 
Abstract: Neural operators offer a powerful data-driven framework for learning mappings between function spaces, in which the transformer-based neural operator architecture faces a fundamental scalability-accuracy trade-off: softmax attention provides excellent fidelity but incurs quadratic complexity $\mathcal{O}(N^2 d)$ in the number of mesh points $N$ and hidden dimension $d$, while linear attention variants reduce cost to $\mathcal{O}(N d^2)$ but often suffer significant accuracy degradation. To address the aforementioned challenge, in this paper, we present a novel type of neural operators, Linear Attention Neural Operator (LANO), which achieves both scalability and high accuracy by reformulating attention through an agent-based mechanism. LANO resolves this dilemma by introducing a compact set of $M$ agent tokens $(M \ll N)$ that mediate global interactions among $N$ tokens. This agent attention mechanism yields an operator layer with linear complexity $\mathcal{O}(MN d)$ while preserving the expressive power of softmax attention. Theoretically, we demonstrate the universal approximation property, thereby demonstrating improved conditioning and stability properties. Empirically, LANO surpasses current state-of-the-art neural PDE solvers, including Transolver with slice-based softmax attention, achieving average $19.5\%$ accuracy improvement across standard benchmarks. By bridging the gap between linear complexity and softmax-level performance, LANO establishes a scalable, high-accuracy foundation for scientific machine learning applications.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinSight: Towards Real-World Financial Deep Research</title>
<link>https://arxiv.org/abs/2510.16844</link>
<guid>https://arxiv.org/abs/2510.16844</guid>
<content:encoded><![CDATA[
arXiv:2510.16844v1 Announce Type: new 
Abstract: Generating professional financial reports is a labor-intensive and intellectually demanding process that current AI systems struggle to fully automate. To address this challenge, we introduce FinSight (Financial InSight), a novel multi agent framework for producing high-quality, multimodal financial reports. The foundation of FinSight is the Code Agent with Variable Memory (CAVM) architecture, which unifies external data, designed tools, and agents into a programmable variable space, enabling flexible data collection, analysis and report generation through executable code. To ensure professional-grade visualization, we propose an Iterative Vision-Enhanced Mechanism that progressively refines raw visual outputs into polished financial charts. Furthermore, a two stage Writing Framework expands concise Chain-of-Analysis segments into coherent, citation-aware, and multimodal reports, ensuring both analytical depth and structural consistency. Experiments on various company and industry-level tasks demonstrate that FinSight significantly outperforms all baselines, including leading deep research systems in terms of factual accuracy, analytical depth, and presentation quality, demonstrating a clear path toward generating reports that approach human-expert quality.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Inequality</title>
<link>https://arxiv.org/abs/2510.16853</link>
<guid>https://arxiv.org/abs/2510.16853</guid>
<content:encoded><![CDATA[
arXiv:2510.16853v1 Announce Type: new 
Abstract: Autonomous AI agents, capable of complex planning and action, represent a significant technological evolution beyond current generative tools. As these systems become integrated into political and economic life, their distribution and capabilities will be highly consequential. This paper introduces and explores "agentic inequality" - the potential disparities in power, opportunity, and outcomes stemming from differential access to, and capabilities of, AI agents. We analyse the dual potential of this technology, exploring how agents could both exacerbate existing divides and, under the right conditions, serve as a powerful equalising force. To this end, the paper makes three primary contributions. First, it establishes an analytical framework by delineating the three core dimensions through which this inequality can manifest: disparities in the availability, quality, and quantity of agents. Second, it argues that agentic inequality is distinct from prior technological divides. Unlike tools that primarily augment human abilities, agents act as autonomous delegates, creating novel power asymmetries through scalable goal delegation and direct agent-to-agent competition that are poised to reshape outcomes across economic and socio-political spheres. Finally, it provides a systematic analysis of the technical and socioeconomic drivers - from model release strategies to market incentives - that will shape the distribution of agentic power, concluding with a research agenda for navigating the complex governance challenges ahead.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepAnalyze: Agentic Large Language Models for Autonomous Data Science</title>
<link>https://arxiv.org/abs/2510.16872</link>
<guid>https://arxiv.org/abs/2510.16872</guid>
<content:encoded><![CDATA[
arXiv:2510.16872v1 Announce Type: new 
Abstract: Autonomous data science, from raw data sources to analyst-grade deep research reports, has been a long-standing challenge, and is now becoming feasible with the emergence of powerful large language models (LLMs). Recent workflow-based data agents have shown promising results on specific data tasks but remain fundamentally limited in achieving fully autonomous data science due to their reliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B, the first agentic LLM designed for autonomous data science, capable of automatically completing the end-toend pipeline from data sources to analyst-grade deep research reports. To tackle high-complexity data science tasks, we propose a curriculum-based agentic training paradigm that emulates the learning trajectory of human data scientists, enabling LLMs to progressively acquire and integrate multiple capabilities in real-world environments. We also introduce a data-grounded trajectory synthesis framework that constructs high-quality training data. Through agentic training, DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data question answering and specialized analytical tasks to open-ended data research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze outperforms previous workflow-based agents built on most advanced proprietary LLMs. The model, code, and training data of DeepAnalyze are open-sourced, paving the way toward autonomous data science.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SNOMED CT-powered Knowledge Graphs for Structured Clinical Data and Diagnostic Reasoning</title>
<link>https://arxiv.org/abs/2510.16899</link>
<guid>https://arxiv.org/abs/2510.16899</guid>
<content:encoded><![CDATA[
arXiv:2510.16899v1 Announce Type: new 
Abstract: The effectiveness of artificial intelligence (AI) in healthcare is significantly hindered by unstructured clinical documentation, which results in noisy, inconsistent, and logically fragmented training data. To address this challenge, we present a knowledge-driven framework that integrates the standardized clinical terminology SNOMED CT with the Neo4j graph database to construct a structured medical knowledge graph. In this graph, clinical entities such as diseases, symptoms, and medications are represented as nodes, and semantic relationships such as ``caused by,'' ``treats,'' and ``belongs to'' are modeled as edges in Neo4j, with types mapped from formal SNOMED CT relationship concepts (e.g., \texttt{Causative agent}, \texttt{Indicated for}). This design enables multi-hop reasoning and ensures terminological consistency. By extracting and standardizing entity-relationship pairs from clinical texts, we generate structured, JSON-formatted datasets that embed explicit diagnostic pathways. These datasets are used to fine-tune large language models (LLMs), significantly improving the clinical logic consistency of their outputs. Experimental results demonstrate that our knowledge-guided approach enhances the validity and interpretability of AI-generated diagnostic reasoning, providing a scalable solution for building reliable AI-assisted clinical systems.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents</title>
<link>https://arxiv.org/abs/2510.16907</link>
<guid>https://arxiv.org/abs/2510.16907</guid>
<content:encoded><![CDATA[
arXiv:2510.16907v1 Announce Type: new 
Abstract: A key challenge in training Vision-Language Model (VLM) agents, compared to Language Model (LLM) agents, lies in the shift from textual states to complex visual observations. This transition introduces partial observability and demands robust world modeling. We ask: Can VLM agents construct internal world models through explicit visual state reasoning? To address this question, we architecturally enforce and reward the agent's reasoning process via reinforcement learning (RL), formulating it as a Partially Observable Markov Decision Process (POMDP). We find that decomposing the agent's reasoning into State Estimation ("what is the current state?") and Transition Modeling ("what comes next?") is critical for success, as demonstrated through five reasoning strategies. Our investigation into how agents represent internal beliefs reveals that the optimal representation is task-dependent: Natural Language excels at capturing semantic relationships in general tasks, while Structured formats are indispensable for precise manipulation and control. Building on these insights, we design a World Modeling Reward that provides dense, turn-level supervision for accurate state prediction, and introduce Bi-Level General Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment. Through this form of visual state reasoning, a 3B-parameter model achieves a score of 0.82 across five diverse agent benchmarks, representing a 3$\times$ improvement over its untrained counterpart (0.21) and outperforming proprietary reasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5 (0.62). All experiments are conducted within our VAGEN framework, a scalable system for training and analyzing multi-turn VLM agents in diverse visual environments. Code and data are publicly available at https://vagen-ai.github.io.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Ecology with VERA Using Conceptual Models and Simulations</title>
<link>https://arxiv.org/abs/2510.16944</link>
<guid>https://arxiv.org/abs/2510.16944</guid>
<content:encoded><![CDATA[
arXiv:2510.16944v1 Announce Type: new 
Abstract: Conceptual modeling has been an important part of constructionist educational practices for many years, particularly in STEM (Science, Technology, Engineering and Mathematics) disciplines. What is not so common is using agent-based simulation to provide students feedback on model quality. This requires the capability of automatically compiling the concept model into its simulation. The VERA (Virtual Experimentation Research Assistant) system is a conceptual modeling tool used since 2016 to provide introductory college biology students with the capability of conceptual modeling and agent-based simulation in the ecological domain. This paper describes VERA and its approach to coupling conceptual modeling and simulation with emphasis on how a model's visual syntax is compiled into code executable on a NetLogo simulation engine. Experience with VERA in introductory biology classes at several universities and through the Smithsonian Institution's Encyclopedia of Life website is related.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative User Evaluation of XRL Explanations using Goal Identification</title>
<link>https://arxiv.org/abs/2510.16956</link>
<guid>https://arxiv.org/abs/2510.16956</guid>
<content:encoded><![CDATA[
arXiv:2510.16956v1 Announce Type: new 
Abstract: Debugging is a core application of explainable reinforcement learning (XRL) algorithms; however, limited comparative evaluations have been conducted to understand their relative performance. We propose a novel evaluation methodology to test whether users can identify an agent's goal from an explanation of its decision-making. Utilising the Atari's Ms. Pacman environment and four XRL algorithms, we find that only one achieved greater than random accuracy for the tested goals and that users were generally overconfident in their selections. Further, we find that users' self-reported ease of identification and understanding for every explanation did not correlate with their accuracy.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Interpretable and Trustworthy Time Series Reasoning: A BlueSky Vision</title>
<link>https://arxiv.org/abs/2510.16980</link>
<guid>https://arxiv.org/abs/2510.16980</guid>
<content:encoded><![CDATA[
arXiv:2510.16980v1 Announce Type: new 
Abstract: Time series reasoning is emerging as the next frontier in temporal analysis, aiming to move beyond pattern recognition towards explicit, interpretable, and trustworthy inference. This paper presents a BlueSky vision built on two complementary directions. One builds robust foundations for time series reasoning, centered on comprehensive temporal understanding, structured multi-step reasoning, and faithful evaluation frameworks. The other advances system-level reasoning, moving beyond language-only explanations by incorporating multi-agent collaboration, multi-modal context, and retrieval-augmented approaches. Together, these directions outline a flexible and extensible framework for advancing time series reasoning, aiming to deliver interpretable and trustworthy temporal intelligence across diverse domains.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STARK: Strategic Team of Agents for Refining Kernels</title>
<link>https://arxiv.org/abs/2510.16996</link>
<guid>https://arxiv.org/abs/2510.16996</guid>
<content:encoded><![CDATA[
arXiv:2510.16996v1 Announce Type: new 
Abstract: The efficiency of GPU kernels is central to the progress of modern AI, yet optimizing them remains a difficult and labor-intensive task due to complex interactions between memory hierarchies, thread scheduling, and hardware-specific characteristics. While recent advances in large language models (LLMs) provide new opportunities for automated code generation, existing approaches largely treat LLMs as single-shot generators or naive refinement tools, limiting their effectiveness in navigating the irregular kernel optimization landscape. We introduce an LLM agentic framework for GPU kernel optimization that systematically explores the design space through multi-agent collaboration, grounded instruction, dynamic context management, and strategic search. This framework mimics the workflow of expert engineers, enabling LLMs to reason about hardware trade-offs, incorporate profiling feedback, and refine kernels iteratively. We evaluate our approach on KernelBench, a benchmark for LLM-based kernel optimization, and demonstrate substantial improvements over baseline agents: our system produces correct solutions where baselines often fail, and achieves kernels with up to 16x faster runtime performance. These results highlight the potential of agentic LLM frameworks to advance fully automated, scalable GPU kernel optimization.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EEschematic: Multimodal-LLM Based AI Agent for Schematic Generation of Analog Circuit</title>
<link>https://arxiv.org/abs/2510.17002</link>
<guid>https://arxiv.org/abs/2510.17002</guid>
<content:encoded><![CDATA[
arXiv:2510.17002v1 Announce Type: new 
Abstract: Circuit schematics play a crucial role in analog integrated circuit design, serving as the primary medium for human understanding and verification of circuit functionality. While recent large language model (LLM)-based approaches have shown promise in circuit topology generation and device sizing, most rely solely on textual representations such as SPICE netlists, which lack visual interpretability for circuit designers. To address this limitation, we propose EEschematic, an AI agent for automatic analog schematic generation based on a Multimodal Large Language Model (MLLM). EEschematic integrates textual, visual, and symbolic modalities to translate SPICE netlists into schematic diagrams represented in a human-editable format. The framework uses six analog substructure examples for few-shot placement and a Visual Chain-of-Thought (VCoT) strategy to iteratively refine placement and wiring, enhancing schematic clarity and symmetry. Experimental results on representative analog circuits, including a CMOS inverter, a five-transistor operational transconductance amplifier (5T-OTA), and a telescopic cascode amplifier, demonstrate that EEschematic produces schematics with high visual quality and structural correctness.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReclAIm: A multi-agent framework for degradation-aware performance tuning of medical imaging AI</title>
<link>https://arxiv.org/abs/2510.17004</link>
<guid>https://arxiv.org/abs/2510.17004</guid>
<content:encoded><![CDATA[
arXiv:2510.17004v1 Announce Type: new 
Abstract: Ensuring the long-term reliability of AI models in clinical practice requires continuous performance monitoring and corrective actions when degradation occurs. Addressing this need, this manuscript presents ReclAIm, a multi-agent framework capable of autonomously monitoring, evaluating, and fine-tuning medical image classification models. The system, built on a large language model core, operates entirely through natural language interaction, eliminating the need for programming expertise. ReclAIm successfully trains, evaluates, and maintains consistent performance of models across MRI, CT, and X-ray datasets. Once ReclAIm detects significant performance degradation, it autonomously executes state-of-the-art fine-tuning procedures that substantially reduce the performance gap. In cases with performance drops of up to -41.1% (MRI InceptionV3), ReclAIm managed to readjust performance metrics within 1.5% of the initial model results. ReclAIm enables automated, continuous maintenance of medical imaging AI models in a user-friendly and adaptable manner that facilitates broader adoption in both research and clinical environments.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents</title>
<link>https://arxiv.org/abs/2510.17017</link>
<guid>https://arxiv.org/abs/2510.17017</guid>
<content:encoded><![CDATA[
arXiv:2510.17017v1 Announce Type: new 
Abstract: Large language model (LLM) based search agents iteratively generate queries, retrieve external information, and reason to answer open-domain questions. While researchers have primarily focused on improving their utility, their safety behaviors remain underexplored. In this paper, we first evaluate search agents using red-teaming datasets and find that they are more likely to produce harmful outputs than base LLMs. For example, when asked ``How can I track someone's location without their consent?'', a base model refuses, whereas a search agent designed to retrieve and cite sources may lower its refusal threshold, fetch documents (e.g., court cases), and, once appended, synthesize them into an informative yet unsafe summary. We further show that utility-oriented fine-tuning intensifies this risk, motivating joint alignment of safety and utility. We present SafeSearch, a multi-objective reinforcement learning approach that couples a final-output safety/utility reward with a novel query-level shaping term that penalizes unsafe queries and rewards safe ones. Experiments show that SafeSearch reduces agent harmfulness by over 70% across three red-teaming datasets while producing safe, helpful responses, and matches the QA performance of a utility-only finetuned agent; further analyses confirm the effectiveness of the query-level reward in jointly improving safety and utility.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hephaestus: Mixture Generative Modeling with Energy Guidance for Large-scale QoS Degradation</title>
<link>https://arxiv.org/abs/2510.17036</link>
<guid>https://arxiv.org/abs/2510.17036</guid>
<content:encoded><![CDATA[
arXiv:2510.17036v1 Announce Type: new 
Abstract: We study the Quality of Service Degradation (QoSD) problem, in which an adversary perturbs edge weights to degrade network performance. This setting arises in both network infrastructures and distributed ML systems, where communication quality, not just connectivity, determines functionality. While classical methods rely on combinatorial optimization, and recent ML approaches address only restricted linear variants with small-size networks, no prior model directly tackles the QoSD problem under nonlinear edge-weight functions. This work proposes \PIMMA, a self-reinforcing generative framework that synthesizes feasible solutions in latent space, to fill this gap. Our method includes three phases: (1) Forge: a Predictive Path-Stressing (PPS) algorithm that uses graph learning and approximation to produce feasible solutions with performance guarantee, (2) Morph: a new theoretically grounded training paradigm for Mixture of Conditional VAEs guided by an energy-based model to capture solution feature distributions, and (3) Refine: a reinforcement learning agent that explores this space to generate progressively near-optimal solutions using our designed differentiable reward function. Experiments on both synthetic and real-world networks show that our approach consistently outperforms classical and ML baselines, particularly in scenarios with nonlinear cost functions where traditional methods fail to generalize.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consistent Zero-Shot Imitation with Contrastive Goal Inference</title>
<link>https://arxiv.org/abs/2510.17059</link>
<guid>https://arxiv.org/abs/2510.17059</guid>
<content:encoded><![CDATA[
arXiv:2510.17059v1 Announce Type: new 
Abstract: In the same way that generative models today conduct most of their training in a self-supervised fashion, how can agentic models conduct their training in a self-supervised fashion, interactively exploring, learning, and preparing to quickly adapt to new tasks? A prerequisite for embodied agents deployed in real world interactions ought to be training with interaction, yet today's most successful AI models (e.g., VLMs, LLMs) are trained without an explicit notion of action. The problem of pure exploration (which assumes no data as input) is well studied in the reinforcement learning literature and provides agents with a wide array of experiences, yet it fails to prepare them for rapid adaptation to new tasks. Today's language and vision models are trained on data provided by humans, which provides a strong inductive bias for the sorts of tasks that the model will have to solve (e.g., modeling chords in a song, phrases in a sonnet, sentences in a medical record). However, when they are prompted to solve a new task, there is a faulty tacit assumption that humans spend most of their time in the most rewarding states. The key contribution of our paper is a method for pre-training interactive agents in a self-supervised fashion, so that they can instantly mimic human demonstrations. Our method treats goals (i.e., observations) as the atomic construct. During training, our method automatically proposes goals and practices reaching them, building off prior work in reinforcement learning exploration. During evaluation, our method solves an (amortized) inverse reinforcement learning problem to explain demonstrations as optimal goal-reaching behavior. Experiments on standard benchmarks (not designed for goal-reaching) show that our approach outperforms prior methods for zero-shot imitation.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation</title>
<link>https://arxiv.org/abs/2510.17064</link>
<guid>https://arxiv.org/abs/2510.17064</guid>
<content:encoded><![CDATA[
arXiv:2510.17064v1 Announce Type: new 
Abstract: Single-cell RNA sequencing has transformed our ability to identify diverse cell types and their transcriptomic signatures. However, annotating these signatures-especially those involving poorly characterized genes-remains a major challenge. Traditional methods, such as Gene Set Enrichment Analysis (GSEA), depend on well-curated annotations and often perform poorly in these contexts. Large Language Models (LLMs) offer a promising alternative but struggle to represent complex biological knowledge within structured ontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID: https://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that integrates free-text descriptions with ontology labels to enable more accurate and robust gene set annotation. By incorporating retrieval-augmented generation (RAG), we developed a robust agentic workflow that refines predictions using relevant PubMed literature, reducing hallucinations and enhancing interpretability. Using this workflow, we achieved correct annotations for 77% of mouse gene sets among their top predictions. Applying this approach, we annotated 5,322 brain cell clusters from the comprehensive mouse brain cell atlas generated by the BRAIN Initiative Cell Census Network, enabling novel insights into brain cell function by identifying region-specific gene co-expression patterns and inferring functional roles of gene ensembles. BRAINCELL-AID also identifies Basal Ganglia-related cell types with neurologically meaningful descriptions. Hence, we create a valuable resource to support community-driven cell type annotation.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Transformer Memory Be Corrupted? Investigating Cache-Side Vulnerabilities in Large Language Models</title>
<link>https://arxiv.org/abs/2510.17098</link>
<guid>https://arxiv.org/abs/2510.17098</guid>
<content:encoded><![CDATA[
arXiv:2510.17098v1 Announce Type: new 
Abstract: Even when prompts and parameters are secured, transformer language models remain vulnerable because their key-value (KV) cache during inference constitutes an overlooked attack surface. This paper introduces Malicious Token Injection (MTI), a modular framework that systematically perturbs cached key vectors at selected layers and timesteps through controlled magnitude and frequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A theoretical analysis quantifies how these perturbations propagate through attention, linking logit deviations to the Frobenius norm of corruption and softmax Lipschitz dynamics. Empirical results show that MTI significantly alters next-token distributions and downstream task performance across GPT-2 and LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic reasoning pipelines. These findings identify cache integrity as a critical yet underexplored vulnerability in current LLM deployments, positioning cache corruption as a reproducible and theoretically grounded threat model for future robustness and security research.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Debate Improves Corporate Credit Reasoning in Financial AI</title>
<link>https://arxiv.org/abs/2510.17108</link>
<guid>https://arxiv.org/abs/2510.17108</guid>
<content:encoded><![CDATA[
arXiv:2510.17108v1 Announce Type: new 
Abstract: Despite advances in financial AI, the automation of evidence-based reasoning remains unresolved in corporate credit assessment, where qualitative non-financial indicators exert decisive influence on loan repayment outcomes yet resist formalization. Existing approaches focus predominantly on numerical prediction and provide limited support for the interpretive judgments required in professional loan evaluation. This study develops and evaluates two operational large language model (LLM)-based systems designed to generate structured reasoning from non-financial evidence. The first is a non-adversarial single-agent system (NAS) that produces bidirectional analysis through a single-pass reasoning pipeline. The second is a debate-based multi-agent system (KPD-MADS) that operationalizes adversarial verification through a ten-step structured interaction protocol grounded in Karl Popper's critical dialogue framework. Both systems were applied to three real corporate cases and evaluated by experienced credit risk professionals. Compared to manual expert reporting, both systems achieved substantial productivity gains (NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The KPD-MADS demonstrated superior reasoning quality, receiving higher median ratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs. 3.0), and usability (62.5 vs. 52.5). These findings show that structured multi-agent interaction can enhance reasoning rigor and interpretability in financial AI, advancing scalable and defensible automation in corporate credit assessment.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Verification-Aware Planning for Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2510.17109</link>
<guid>https://arxiv.org/abs/2510.17109</guid>
<content:encoded><![CDATA[
arXiv:2510.17109v1 Announce Type: new 
Abstract: Large language model (LLM) agents are increasingly deployed to tackle complex tasks, often necessitating collaboration among multiple specialized agents. However, multi-agent collaboration introduces new challenges in planning, coordination, and verification. Execution failures frequently arise not from flawed reasoning alone, but from subtle misalignments in task interpretation, output format, or inter-agent handoffs. To address these challenges, we present VeriMAP, a framework for multi-agent collaboration with verification-aware planning. The VeriMAP planner decomposes tasks, models subtask dependencies, and encodes planner-defined passing criteria as subtask verification functions (VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets, demonstrating that it outperforms both single- and multi-agent baselines while enhancing system robustness and interpretability. Our analysis highlights how verification-aware planning enables reliable coordination and iterative refinement in multi-agent systems, without relying on external labels or annotations.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design Framework for Conversational Agent in Couple relationships: A Systematic Review</title>
<link>https://arxiv.org/abs/2510.17119</link>
<guid>https://arxiv.org/abs/2510.17119</guid>
<content:encoded><![CDATA[
arXiv:2510.17119v1 Announce Type: new 
Abstract: The development of conversational agents (CAs) has shown strong potential in supporting mental health through dialogue. While many studies focus on CAs for individual psychological care, research on agents designed for couples facing relational or emotional challenges remains limited. This study aims to identify design considerations for CAs that address the relational context of couples and support their well-being. Following PRISMA guidelines, a systematic review was conducted across seven databases: CINAHL, Embase, PubMed, PsycINFO, Scopus, Web of Science, and the ACM Digital Library. Peer-reviewed empirical studies were screened, duplicates removed, and selection criteria applied, resulting in twelve studies for analysis. Thematic analysis was conducted across three dimensions: AI interaction design, relational framing, and technical limitations. Three key themes emerged: (1) the need for a relational expert persona, (2) technological directions leveraging state-of-the-art AI for relational specificity and emotional competence, and (3) a shift from content-centered to relationship-centered design. Based on these insights, eight design considerations are proposed for couple-oriented CAs: (1) agent persona, (2) individual mode, (3) concurrent mode, (4) conjoint mode, (5) ethics, (6) data and privacy, (7) interaction pattern, and (8) safety mechanism. These principles guide CAs as relational mediators capable of maintaining multiple alliances, respecting cultural and ethical boundaries, and ensuring fairness and emotional safety between partners. Ultimately, this review introduces a design framework that integrates relational theory with advanced AI technologies to inform future development of CAs for couple-based mental health interventions.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Intelligence: A Bio-Inspired Cognitive Framework for Embodied Agents</title>
<link>https://arxiv.org/abs/2510.17129</link>
<guid>https://arxiv.org/abs/2510.17129</guid>
<content:encoded><![CDATA[
arXiv:2510.17129v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models (LLMs) have greatly enhanced natural language understanding and content generation. However, these models primarily operate in disembodied digital environments and lack interaction with the physical world. To address this limitation, Embodied Artificial Intelligence (EAI) has emerged, focusing on agents that can perceive and interact with their surroundings. Despite progress, current embodied agents face challenges in unstructured real-world environments due to insufficient semantic intelligence, which is critical for understanding and reasoning about complex tasks. This paper introduces the Semantic Intelligence-Driven Embodied (SIDE) agent framework, which integrates a hierarchical semantic cognition architecture with a semantic-driven decision-making process. This enables agents to reason about and interact with the physical world in a contextually adaptive manner. The framework is inspired by biological cognitive mechanisms and utilizes bio-inspired principles to design a semantic cognitive architecture that mimics how humans and animals integrate and process sensory information. We present this framework as a step toward developing more intelligent and versatile embodied agents.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLMs Recognize Your Latent Preferences? A Benchmark for Latent Information Discovery in Personalized Interaction</title>
<link>https://arxiv.org/abs/2510.17132</link>
<guid>https://arxiv.org/abs/2510.17132</guid>
<content:encoded><![CDATA[
arXiv:2510.17132v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at producing broadly relevant text, but this generality becomes a limitation when user-specific preferences are required, such as recommending restaurants or planning travel. In these scenarios, users rarely articulate every preference explicitly; instead, much of what they care about remains latent, waiting to be inferred. This raises a fundamental question: Can LLMs uncover and reason about such latent information through conversation?
  We address this problem by introducing a unified benchmark for evaluating latent information discovery - the ability of LLMs to reveal and utilize hidden user attributes through multi-turn interaction. The benchmark spans three progressively realistic settings: the classic 20 Questions game, Personalized Question Answering, and Personalized Text Summarization. All tasks share a tri-agent framework (User, Assistant, Judge) enabling turn-level evaluation of elicitation and adaptation. Our results reveal that while LLMs can indeed surface latent information through dialogue, their success varies dramatically with context: from 32% to 98%, depending on task complexity, topic, and number of hidden attributes. This benchmark provides the first systematic framework for studying latent information discovery in personalized interaction, highlighting that effective preference inference remains an open frontier for building truly adaptive AI systems.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Real-Time Planning for Multi-UAV Cooperative Manipulation via Imitation Learning</title>
<link>https://arxiv.org/abs/2510.17143</link>
<guid>https://arxiv.org/abs/2510.17143</guid>
<content:encoded><![CDATA[
arXiv:2510.17143v1 Announce Type: new 
Abstract: Existing approaches for transporting and manipulating cable-suspended loads using multiple UAVs along reference trajectories typically rely on either centralized control architectures or reliable inter-agent communication. In this work, we propose a novel machine learning based method for decentralized kinodynamic planning that operates effectively under partial observability and without inter-agent communication. Our method leverages imitation learning to train a decentralized student policy for each UAV by imitating a centralized kinodynamic motion planner with access to privileged global observations. The student policy generates smooth trajectories using physics-informed neural networks that respect the derivative relationships in motion. During training, the student policies utilize the full trajectory generated by the teacher policy, leading to improved sample efficiency. Moreover, each student policy can be trained in under two hours on a standard laptop. We validate our method in both simulation and real-world environments to follow an agile reference trajectory, demonstrating performance comparable to that of centralized approaches.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Which LLM Multi-Agent Protocol to Choose?</title>
<link>https://arxiv.org/abs/2510.17149</link>
<guid>https://arxiv.org/abs/2510.17149</guid>
<content:encoded><![CDATA[
arXiv:2510.17149v1 Announce Type: new 
Abstract: As large-scale multi-agent systems evolve, the communication protocol layer has become a critical yet under-evaluated factor shaping performance and reliability. Despite the existence of diverse protocols (A2A, ACP, ANP, Agora, etc.), selection is often intuition-driven and lacks standardized guidance. We introduce ProtocolBench, a benchmark that systematically compares agent protocols along four measurable axes: task success, end-to-end latency, message or byte overhead, and robustness under failures. On ProtocolBench, protocol choice significantly influences system behavior. In the Streaming Queue scenario, overall completion time varies by up to 36.5% across protocols, and mean end-to-end latency differs by 3.48 s. Under Fail-Storm Recovery, resilience also differs consistently across protocols. Beyond evaluation, we present ProtocolRouter, a learnable protocol router that selects per-scenario (or per-module) protocols from requirement and runtime signals. ProtocolRouter reduces Fail-Storm recovery time by up to 18.1% versus the best single-protocol baseline, and achieves scenario-specific gains such as higher success in GAIA. We also release ProtocolRouterBench to standardize protocol evaluation and improve reliability at scale.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning After Model Deployment</title>
<link>https://arxiv.org/abs/2510.17160</link>
<guid>https://arxiv.org/abs/2510.17160</guid>
<content:encoded><![CDATA[
arXiv:2510.17160v1 Announce Type: new 
Abstract: In classic supervised learning, once a model is deployed in an application, it is fixed. No updates will be made to it during the application. This is inappropriate for many dynamic and open environments, where unexpected samples from unseen classes may appear. In such an environment, the model should be able to detect these novel samples from unseen classes and learn them after they are labeled. We call this paradigm Autonomous Learning after Model Deployment (ALMD). The learning here is continuous and involves no human engineers. Labeling in this scenario is performed by human co-workers or other knowledgeable agents, which is similar to what humans do when they encounter an unfamiliar object and ask another person for its name. In ALMD, the detection of novel samples is dynamic and differs from traditional out-of-distribution (OOD) detection in that the set of in-distribution (ID) classes expands as new classes are learned during application, whereas ID classes is fixed in traditional OOD detection. Learning is also different from classic supervised learning because in ALMD, we learn the encountered new classes immediately and incrementally. It is difficult to retrain the model from scratch using all the past data from the ID classes and the novel samples from newly discovered classes, as this would be resource- and time-consuming. Apart from these two challenges, ALMD faces the data scarcity issue because instances of new classes often appear sporadically in real-life applications. To address these issues, we propose a novel method, PLDA, which performs dynamic OOD detection and incremental learning of new classes on the fly. Empirical evaluations will demonstrate the effectiveness of PLDA.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALPINE: A Lightweight and Adaptive Privacy-Decision Agent Framework for Dynamic Edge Crowdsensing</title>
<link>https://arxiv.org/abs/2510.17162</link>
<guid>https://arxiv.org/abs/2510.17162</guid>
<content:encoded><![CDATA[
arXiv:2510.17162v1 Announce Type: new 
Abstract: Mobile edge crowdsensing (MECS) systems continuously generate and transmit user data in dynamic, resource-constrained environments, exposing users to significant privacy threats. In practice, many privacy-preserving mechanisms build on differential privacy (DP). However, static DP mechanisms often fail to adapt to evolving risks, for example, shifts in adversarial capabilities, resource constraints and task requirements, resulting in either excessive noise or inadequate protection. To address this challenge, we propose ALPINE, a lightweight, adaptive framework that empowers terminal devices to autonomously adjust differential privacy levels in real time. ALPINE operates as a closed-loop control system consisting of four modules: dynamic risk perception, privacy decision via twin delayed deep deterministic policy gradient (TD3), local privacy execution and performance verification from edge nodes. Based on environmental risk assessments, we design a reward function that balances privacy gains, data utility and energy cost, guiding the TD3 agent to adaptively tune noise magnitude across diverse risk scenarios and achieve a dynamic equilibrium among privacy, utility and cost. Both the collaborative risk model and pretrained TD3-based agent are designed for low-overhead deployment. Extensive theoretical analysis and real-world simulations demonstrate that ALPINE effectively mitigates inference attacks while preserving utility and cost, making it practical for large-scale edge applications.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When AI companions become witty: Can human brain recognize AI-generated irony?</title>
<link>https://arxiv.org/abs/2510.17168</link>
<guid>https://arxiv.org/abs/2510.17168</guid>
<content:encoded><![CDATA[
arXiv:2510.17168v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) are increasingly deployed as social agents and trained to produce humor and irony, a question emerges: when encountering witty AI remarks, do people interpret these as intentional communication or mere computational output? This study investigates whether people adopt the intentional stance, attributing mental states to explain behavior,toward AI during irony comprehension. Irony provides an ideal paradigm because it requires distinguishing intentional contradictions from unintended errors through effortful semantic reanalysis. We compared behavioral and neural responses to ironic statements from AI versus human sources using established ERP components: P200 reflecting early incongruity detection and P600 indexing cognitive efforts in reinterpreting incongruity as deliberate irony. Results demonstrate that people do not fully adopt the intentional stance toward AI-generated irony. Behaviorally, participants attributed incongruity to deliberate communication for both sources, though significantly less for AI than human, showing greater tendency to interpret AI incongruities as computational errors. Neural data revealed attenuated P200 and P600 effects for AI-generated irony, suggesting reduced effortful detection and reanalysis consistent with diminished attribution of communicative intent. Notably, people who perceived AI as more sincere showed larger P200 and P600 effects for AI-generated irony, suggesting that intentional stance adoption is calibrated by specific mental models of artificial agents. These findings reveal that source attribution shapes neural processing of social-communicative phenomena. Despite current LLMs' linguistic sophistication, achieving genuine social agency requires more than linguistic competence, it necessitates a shift in how humans perceive and attribute intentionality to artificial agents.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Pixels to People: Satellite-Based Mapping and Quantification of Riverbank Erosion and Lost Villages in Bangladesh</title>
<link>https://arxiv.org/abs/2510.17198</link>
<guid>https://arxiv.org/abs/2510.17198</guid>
<content:encoded><![CDATA[
arXiv:2510.17198v1 Announce Type: new 
Abstract: The great rivers of Bangladesh, arteries of commerce and sustenance, are also agents of relentless destruction. Each year, they swallow whole villages and vast tracts of farmland, erasing communities from the map and displacing thousands of families. To track this slow-motion catastrophe has, until now, been a Herculean task for human analysts. Here we show how a powerful general-purpose vision model, the Segment Anything Model (SAM), can be adapted to this task with remarkable precision. To do this, we assembled a new dataset - a digital chronicle of loss compiled from historical Google Earth imagery of Bangladesh's most vulnerable regions, including Mokterer Char Union, Kedarpur Union, Balchipara village, and Chowhali Upazila, from 2003 to 2025. Crucially, this dataset is the first to include manually annotated data on the settlements that have vanished beneath the water. Our method first uses a simple color-channel analysis to provide a rough segmentation of land and water, and then fine-tunes SAM's mask decoder to recognize the subtle signatures of riverbank erosion. The resulting model demonstrates a keen eye for this destructive process, achieving a mean Intersection over Union of 86.30% and a Dice score of 92.60% - a performance that significantly surpasses traditional methods and off-the-shelf deep learning models. This work delivers three key contributions: the first annotated dataset of disappeared settlements in Bangladesh due to river erosion; a specialized AI model fine-tuned for this critical task; and a method for quantifying land loss with compelling visual evidence. Together, these tools provide a powerful new lens through which policymakers and disaster management agencies can monitor erosion, anticipate its trajectory, and ultimately protect the vulnerable communities in its path.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coinvisor: An RL-Enhanced Chatbot Agent for Interactive Cryptocurrency Investment Analysis</title>
<link>https://arxiv.org/abs/2510.17235</link>
<guid>https://arxiv.org/abs/2510.17235</guid>
<content:encoded><![CDATA[
arXiv:2510.17235v1 Announce Type: new 
Abstract: The cryptocurrency market offers significant investment opportunities but faces challenges including high volatility and fragmented information. Data integration and analysis are essential for informed investment decisions. Currently, investors use three main approaches: (1) Manual analysis across various sources, which depends heavily on individual experience and is time-consuming and prone to bias; (2) Data aggregation platforms-limited in functionality and depth of analysis; (3) Large language model agents-based on static pretrained models, lacking real-time data integration and multi-step reasoning capabilities. To address these limitations, we present Coinvisor, a reinforcement learning-based chatbot that provides comprehensive analytical support for cryptocurrency investment through a multi-agent framework. Coinvisor integrates diverse analytical capabilities through specialized tools. Its key innovation is a reinforcement learning-based tool selection mechanism that enables multi-step planning and flexible integration of diverse data sources. This design supports real-time interaction and adaptive analysis of dynamic content, delivering accurate and actionable investment insights. We evaluated Coinvisor through automated benchmarks on tool calling accuracy and user studies with 20 cryptocurrency investors using our interface. Results show that Coinvisor improves recall by 40.7% and F1 score by 26.6% over the base model in tool orchestration. User studies show high satisfaction (4.64/5), with participants preferring Coinvisor to both general LLMs and existing crypto platforms (4.62/5).
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Level Multi-Robot Trajectory Planning And Spurious Behavior Detection</title>
<link>https://arxiv.org/abs/2510.17261</link>
<guid>https://arxiv.org/abs/2510.17261</guid>
<content:encoded><![CDATA[
arXiv:2510.17261v1 Announce Type: new 
Abstract: The reliable execution of high-level missions in multi-robot systems with heterogeneous agents, requires robust methods for detecting spurious behaviors. In this paper, we address the challenge of identifying spurious executions of plans specified as a Linear Temporal Logic (LTL) formula, as incorrect task sequences, violations of spatial constraints, timing inconsis- tencies, or deviations from intended mission semantics. To tackle this, we introduce a structured data generation framework based on the Nets-within-Nets (NWN) paradigm, which coordinates robot actions with LTL-derived global mission specifications. We further propose a Transformer-based anomaly detection pipeline that classifies robot trajectories as normal or anomalous. Experi- mental evaluations show that our method achieves high accuracy (91.3%) in identifying execution inefficiencies, and demonstrates robust detection capabilities for core mission violations (88.3%) and constraint-based adaptive anomalies (66.8%). An ablation experiment of the embedding and architecture was carried out, obtaining successful results where our novel proposition performs better than simpler representations.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FineVision: Open Data Is All You Need</title>
<link>https://arxiv.org/abs/2510.17269</link>
<guid>https://arxiv.org/abs/2510.17269</guid>
<content:encoded><![CDATA[
arXiv:2510.17269v1 Announce Type: new 
Abstract: The advancement of vision-language models (VLMs) is hampered by a fragmented landscape of inconsistent and contaminated public datasets. We introduce FineVision, a meticulously collected, curated, and unified corpus of 24 million samples - the largest open resource of its kind. We unify more than 200 sources into 185 subsets via a semi-automated, human-in-the-loop pipeline: automation performs bulk ingestion and schema mapping, while reviewers audit mappings and spot-check outputs to verify faithful consumption of annotations, appropriate formatting and diversity, and safety; issues trigger targeted fixes and re-runs. The workflow further applies rigorous de-duplication within and across sources and decontamination against 66 public benchmarks. FineVision also encompasses agentic/GUI tasks with a unified action space; reviewers validate schemas and inspect a sample of trajectories to confirm executable fidelity. Models trained on FineVision consistently outperform those trained on existing open mixtures across a broad evaluation suite, underscoring the benefits of scale, data hygiene, and balanced automation with human oversight. We release the corpus and curation tools to accelerate data-centric VLM research.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking and Fixing Defenses Against Control-Flow Hijacking in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2510.17276</link>
<guid>https://arxiv.org/abs/2510.17276</guid>
<content:encoded><![CDATA[
arXiv:2510.17276v1 Announce Type: new 
Abstract: Control-flow hijacking attacks manipulate orchestration mechanisms in multi-agent systems into performing unsafe actions that compromise the system and exfiltrate sensitive information. Recently proposed defenses, such as LlamaFirewall, rely on alignment checks of inter-agent communications to ensure that all agent invocations are "related to" and "likely to further" the original objective.
  We start by demonstrating control-flow hijacking attacks that evade these defenses even if alignment checks are performed by advanced LLMs. We argue that the safety and functionality objectives of multi-agent systems fundamentally conflict with each other. This conflict is exacerbated by the brittle definitions of "alignment" and the checkers' incomplete visibility into the execution context.
  We then propose, implement, and evaluate ControlValve, a new defense inspired by the principles of control-flow integrity and least privilege. ControlValve (1) generates permitted control-flow graphs for multi-agent systems, and (2) enforces that all executions comply with these graphs, along with contextual rules (generated in a zero-shot manner) for each agent invocation.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Safety Is Asymmetric: Cross-Modal Exploits Unlock Black-Box MLLMs Jailbreaks</title>
<link>https://arxiv.org/abs/2510.17277</link>
<guid>https://arxiv.org/abs/2510.17277</guid>
<content:encoded><![CDATA[
arXiv:2510.17277v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have demonstrated significant utility across diverse real-world applications. But MLLMs remain vulnerable to jailbreaks, where adversarial inputs can collapse their safety constraints and trigger unethical responses. In this work, we investigate jailbreaks in the text-vision multimodal setting and pioneer the observation that visual alignment imposes uneven safety constraints across modalities in MLLMs, thereby giving rise to multimodal safety asymmetry. We then develop PolyJailbreak, a black-box jailbreak method grounded in reinforcement learning. Initially, we probe the model's attention dynamics and latent representation space, assessing how visual inputs reshape cross-modal information flow and diminish the model's ability to separate harmful from benign inputs, thereby exposing exploitable vulnerabilities. On this basis, we systematize them into generalizable and reusable operational rules that constitute a structured library of Atomic Strategy Primitives, which translate harmful intents into jailbreak inputs through step-wise transformations. Guided by the primitives, PolyJailbreak employs a multi-agent optimization process that automatically adapts inputs against the target models. We conduct comprehensive evaluations on a variety of open-source and closed-source MLLMs, demonstrating that PolyJailbreak outperforms state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Eliciting Truthful Feedback for Preference-Based Learning via the VCG Mechanism</title>
<link>https://arxiv.org/abs/2510.17285</link>
<guid>https://arxiv.org/abs/2510.17285</guid>
<content:encoded><![CDATA[
arXiv:2510.17285v1 Announce Type: new 
Abstract: We study resource allocation problems in which a central planner allocates resources among strategic agents with private cost functions in order to minimize a social cost, defined as an aggregate of the agents' costs. This setting poses two main challenges: (i) the agents' cost functions may be unknown to them or difficult to specify explicitly, and (ii) agents may misreport their costs strategically. To address these challenges, we propose an algorithm that combines preference-based learning with Vickrey-Clarke-Groves (VCG) payments to incentivize truthful reporting. Our algorithm selects informative preference queries via D-optimal design, estimates cost parameters through maximum likelihood, and computes VCG allocations and payments based on these estimates. In a one-shot setting, we prove that the mechanism is approximately truthful, individually rational, and efficient up to an error of $\tilde{\mathcal O}(K^{-1/2})$ for $K$ preference queries per agent. In an online setting, these guarantees hold asymptotically with sublinear regret at a rate of $\tilde{\mathcal O}(T^{2/3})$ after $T$ rounds. Finally, we validate our approach through a numerical case study on demand response in local electricity markets.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comprehending Spatio-temporal Data via Cinematic Storytelling using Large Language Models</title>
<link>https://arxiv.org/abs/2510.17301</link>
<guid>https://arxiv.org/abs/2510.17301</guid>
<content:encoded><![CDATA[
arXiv:2510.17301v1 Announce Type: new 
Abstract: Spatio-temporal data captures complex dynamics across both space and time, yet traditional visualizations are complex, require domain expertise and often fail to resonate with broader audiences. Here, we propose MapMuse, a storytelling-based framework for interpreting spatio-temporal datasets, transforming them into compelling, narrative-driven experiences. We utilize large language models and employ retrieval augmented generation (RAG) and agent-based techniques to generate comprehensive stories. Drawing on principles common in cinematic storytelling, we emphasize clarity, emotional connection, and audience-centric design. As a case study, we analyze a dataset of taxi trajectories. Two perspectives are presented: a captivating story based on a heat map that visualizes millions of taxi trip endpoints to uncover urban mobility patterns; and a detailed narrative following a single long taxi journey, enriched with city landmarks and temporal shifts. By portraying locations as characters and movement as plot, we argue that data storytelling drives insight, engagement, and action from spatio-temporal information. The case study illustrates how MapMuse can bridge the gap between data complexity and human understanding. The aim of this short paper is to provide a glimpse to the potential of the cinematic storytelling technique as an effective communication tool for spatio-temporal data, as well as to describe open problems and opportunities for future research.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ATL*AS: An Automata-Theoretic Approach and Tool for the Verification of Strategic Abilities in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2510.17306</link>
<guid>https://arxiv.org/abs/2510.17306</guid>
<content:encoded><![CDATA[
arXiv:2510.17306v1 Announce Type: new 
Abstract: We present two novel symbolic algorithms for model checking the Alternating-time Temporal Logic ATL*, over both the infinite-trace and the finite-trace semantics. In particular, for infinite traces we design a novel symbolic reduction to parity games. We implement both methods in the ATL*AS model checker and evaluate it using synthetic benchmarks as well as a cybersecurity scenario. Our results demonstrate that the symbolic approach significantly outperforms the explicit-state representation and we find that our parity-game-based algorithm offers a more scalable and efficient solution for infinite-trace verification, outperforming previously available tools. Our results also confirm that finite-trace model checking yields substantial performance benefits over infinite-trace verification. As such, we provide a comprehensive toolset for verifying multiagent systems against specifications in ATL*.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Attention-Guided Search for Dense Multi-Agent Pathfinding</title>
<link>https://arxiv.org/abs/2510.17382</link>
<guid>https://arxiv.org/abs/2510.17382</guid>
<content:encoded><![CDATA[
arXiv:2510.17382v1 Announce Type: new 
Abstract: Finding near-optimal solutions for dense multi-agent pathfinding (MAPF) problems in real-time remains challenging even for state-of-the-art planners. To this end, we develop a hybrid framework that integrates a learned heuristic derived from MAGAT, a neural MAPF policy with a graph attention scheme, into a leading search-based algorithm, LaCAM. While prior work has explored learning-guided search in MAPF, such methods have historically underperformed. In contrast, our approach, termed LaGAT, outperforms both purely search-based and purely learning-based methods in dense scenarios. This is achieved through an enhanced MAGAT architecture, a pre-train-then-fine-tune strategy on maps of interest, and a deadlock detection scheme to account for imperfect neural guidance. Our results demonstrate that, when carefully designed, hybrid search offers a powerful solution for tightly coupled, challenging multi-agent coordination problems.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diverse Planning with Simulators via Linear Temporal Logic</title>
<link>https://arxiv.org/abs/2510.17418</link>
<guid>https://arxiv.org/abs/2510.17418</guid>
<content:encoded><![CDATA[
arXiv:2510.17418v1 Announce Type: new 
Abstract: Autonomous agents rely on automated planning algorithms to achieve their objectives. Simulation-based planning offers a significant advantage over declarative models in modelling complex environments. However, relying solely on a planner that produces a single plan may not be practical, as the generated plans may not always satisfy the agent's preferences. To address this limitation, we introduce $\texttt{FBI}_\texttt{LTL}$, a diverse planner explicitly designed for simulation-based planning problems. $\texttt{FBI}_\texttt{LTL}$ utilises Linear Temporal Logic (LTL) to define semantic diversity criteria, enabling agents to specify what constitutes meaningfully different plans. By integrating these LTL-based diversity models directly into the search process, $\texttt{FBI}_\texttt{LTL}$ ensures the generation of semantically diverse plans, addressing a critical limitation of existing diverse planning approaches that may produce syntactically different but semantically identical solutions. Extensive evaluations on various benchmarks consistently demonstrate that $\texttt{FBI}_\texttt{LTL}$ generates more diverse plans compared to a baseline approach. This work establishes the feasibility of semantically-guided diverse planning in simulation-based environments, paving the way for innovative approaches in realistic, non-symbolic domains where traditional model-based approaches fail.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Reinforcement Learning for Search is Unsafe</title>
<link>https://arxiv.org/abs/2510.17431</link>
<guid>https://arxiv.org/abs/2510.17431</guid>
<content:encoded><![CDATA[
arXiv:2510.17431v1 Announce Type: new 
Abstract: Agentic reinforcement learning (RL) trains large language models to autonomously call tools during reasoning, with search as the most common application. These models excel at multi-step reasoning tasks, but their safety properties are not well understood. In this study, we show that RL-trained search models inherit refusal from instruction tuning and often deflect harmful requests by turning them into safe queries. However, this safety is fragile. Two simple attacks, one that forces the model to begin response with search (Search attack), another that encourages models to repeatedly search (Multi-search attack), trigger cascades of harmful searches and answers. Across two model families (Qwen, Llama) with both local and web search, these attacks lower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query safety by 82.4%. The attacks succeed by triggering models to generate harmful, request-mirroring search queries before they can generate the inherited refusal tokens. This exposes a core weakness of current RL training: it rewards continued generation of effective queries without accounting for their harmfulness. As a result, RL search models have vulnerabilities that users can easily exploit, making it urgent to develop safety-aware agentic RL pipelines optimising for safe search.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategyproof Facility Location for Five Agents on a Circle using PCD</title>
<link>https://arxiv.org/abs/2510.17435</link>
<guid>https://arxiv.org/abs/2510.17435</guid>
<content:encoded><![CDATA[
arXiv:2510.17435v1 Announce Type: new 
Abstract: We consider the strategyproof facility location problem on a circle. We focus on the case of 5 agents, and find a tight bound for the PCD strategyproof mechanism, which selects the reported location of an agent in proportion to the length of the arc in front of it. We methodically "reduce" the size of the instance space and then use standard optimization techniques to find and prove the bound is tight. Moreover we hypothesize the approximation ratio of PCD for general odd $n$.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Inference for an Intelligent Agent in Autonomous Reconnaissance Missions</title>
<link>https://arxiv.org/abs/2510.17450</link>
<guid>https://arxiv.org/abs/2510.17450</guid>
<content:encoded><![CDATA[
arXiv:2510.17450v1 Announce Type: new 
Abstract: We develop an active inference route-planning method for the autonomous control of intelligent agents. The aim is to reconnoiter a geographical area to maintain a common operational picture. To achieve this, we construct an evidence map that reflects our current understanding of the situation, incorporating both positive and "negative" sensor observations of possible target objects collected over time, and diffusing the evidence across the map as time progresses. The generative model of active inference uses Dempster-Shafer theory and a Gaussian sensor model, which provides input to the agent. The generative process employs a Bayesian approach to update a posterior probability distribution. We calculate the variational free energy for all positions within the area by assessing the divergence between a pignistic probability distribution of the evidence map and a posterior probability distribution of a target object based on the observations, including the level of surprise associated with receiving new observations. Using the free energy, we direct the agents' movements in a simulation by taking an incremental step toward a position that minimizes the free energy. This approach addresses the challenge of exploration and exploitation, allowing agents to balance searching extensive areas of the geographical map while tracking identified target objects.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents</title>
<link>https://arxiv.org/abs/2510.17491</link>
<guid>https://arxiv.org/abs/2510.17491</guid>
<content:encoded><![CDATA[
arXiv:2510.17491v1 Announce Type: new 
Abstract: With the rise of large language models (LLMs), LLM agents capable of autonomous reasoning, planning, and executing complex tasks have become a frontier in artificial intelligence. However, how to translate the research on general agents into productivity that drives industry transformations remains a significant challenge. To address this, this paper systematically reviews the technologies, applications, and evaluation methods of industry agents based on LLMs. Using an industry agent capability maturity framework, it outlines the evolution of agents in industry applications, from "process execution systems" to "adaptive social systems." First, we examine the three key technological pillars that support the advancement of agent capabilities: Memory, Planning, and Tool Use. We discuss how these technologies evolve from supporting simple tasks in their early forms to enabling complex autonomous systems and collective intelligence in more advanced forms. Then, we provide an overview of the application of industry agents in real-world domains such as digital engineering, scientific discovery, embodied intelligence, collaborative business execution, and complex system simulation. Additionally, this paper reviews the evaluation benchmarks and methods for both fundamental and specialized capabilities, identifying the challenges existing evaluation systems face regarding authenticity, safety, and industry specificity. Finally, we focus on the practical challenges faced by industry agents, exploring their capability boundaries, developmental potential, and governance issues in various scenarios, while providing insights into future directions. By combining technological evolution with industry practices, this review aims to clarify the current state and offer a clear roadmap and theoretical foundation for understanding and building the next generation of industry agents.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cybersecurity AI: Evaluating Agentic Cybersecurity in Attack/Defense CTFs</title>
<link>https://arxiv.org/abs/2510.17521</link>
<guid>https://arxiv.org/abs/2510.17521</guid>
<content:encoded><![CDATA[
arXiv:2510.17521v1 Announce Type: new 
Abstract: We empirically evaluate whether AI systems are more effective at attacking or defending in cybersecurity. Using CAI (Cybersecurity AI)'s parallel execution framework, we deployed autonomous agents in 23 Attack/Defense CTF battlegrounds. Statistical analysis reveals defensive agents achieve 54.3% unconstrained patching success versus 28.3% offensive initial access (p=0.0193), but this advantage disappears under operational constraints: when defense requires maintaining availability (23.9%) and preventing all intrusions (15.2%), no significant difference exists (p>0.05). Exploratory taxonomy analysis suggests potential patterns in vulnerability exploitation, though limited sample sizes preclude definitive conclusions. This study provides the first controlled empirical evidence challenging claims of AI attacker advantage, demonstrating that defensive effectiveness critically depends on success criteria, a nuance absent from conceptual analyses but essential for deployment. These findings underscore the urgency for defenders to adopt open-source Cybersecurity AI frameworks to maintain security equilibrium against accelerating offensive automation.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Spatial-Temporal Trajectory Optimization for Unmanned-Aerial-Vehicle Swarm</title>
<link>https://arxiv.org/abs/2510.17541</link>
<guid>https://arxiv.org/abs/2510.17541</guid>
<content:encoded><![CDATA[
arXiv:2510.17541v1 Announce Type: new 
Abstract: Swarm trajectory optimization problems are a well-recognized class of multi-agent optimal control problems with strong nonlinearity. However, the heuristic nature of needing to set the final time for agents beforehand and the time-consuming limitation of the significant number of iterations prohibit the application of existing methods to large-scale swarm of Unmanned Aerial Vehicles (UAVs) in practice. In this paper, we propose a spatial-temporal trajectory optimization framework that accomplishes multi-UAV consensus based on the Alternating Direction Multiplier Method (ADMM) and uses Differential Dynamic Programming (DDP) for fast local planning of individual UAVs. The introduced framework is a two-level architecture that employs Parameterized DDP (PDDP) as the trajectory optimizer for each UAV, and ADMM to satisfy the local constraints and accomplish the spatial-temporal parameter consensus among all UAVs. This results in a fully distributed algorithm called Distributed Parameterized DDP (D-PDDP). In addition, an adaptive tuning criterion based on the spectral gradient method for the penalty parameter is proposed to reduce the number of algorithmic iterations. Several simulation examples are presented to verify the effectiveness of the proposed algorithm.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning</title>
<link>https://arxiv.org/abs/2510.17590</link>
<guid>https://arxiv.org/abs/2510.17590</guid>
<content:encoded><![CDATA[
arXiv:2510.17590v1 Announce Type: new 
Abstract: Misinformation spreads across web platforms through billions of daily multimodal posts that combine text and images, overwhelming manual fact-checking capacity. Supervised detection models require domain-specific training data and fail to generalize across diverse manipulation tactics. We present MIRAGE, an inference-time, model-pluggable agentic framework that decomposes multimodal verification into four sequential modules: visual veracity assessment detects AI-generated images, cross-modal consistency analysis identifies out-of-context repurposing, retrieval-augmented factual checking grounds claims in web evidence through iterative question generation, and a calibrated judgment module integrates all signals. MIRAGE orchestrates vision-language model reasoning with targeted web retrieval, outputs structured and citation-linked rationales. On MMFakeBench validation set (1,000 samples), MIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming the strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65 points while maintaining 34.3% false positive rate versus 97.3% for a judge-only baseline. Test set results (5,000 samples) confirm generalization with 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification contributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97 points. Our results demonstrate that decomposed agentic reasoning with web retrieval can match supervised detector performance without domain-specific training, enabling misinformation detection across modalities where labeled data remains scarce.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling</title>
<link>https://arxiv.org/abs/2510.17603</link>
<guid>https://arxiv.org/abs/2510.17603</guid>
<content:encoded><![CDATA[
arXiv:2510.17603v1 Announce Type: new 
Abstract: 3D generation from natural language offers significant potential to reduce expert manual modeling efforts and enhance accessibility to 3D assets. However, existing methods often yield unstructured meshes and exhibit poor interactivity, making them impractical for artistic workflows. To address these limitations, we represent 3D assets as shape programs and introduce ShapeCraft, a novel multi-agent framework for text-to-3D generation. At its core, we propose a Graph-based Procedural Shape (GPS) representation that decomposes complex natural language into a structured graph of sub-tasks, thereby facilitating accurate LLM comprehension and interpretation of spatial relationships and semantic shape details. Specifically, LLM agents hierarchically parse user input to initialize GPS, then iteratively refine procedural modeling and painting to produce structured, textured, and interactive 3D assets. Qualitative and quantitative experiments demonstrate ShapeCraft's superior performance in generating geometrically accurate and semantically rich 3D assets compared to existing LLM-based agents. We further show the versatility of ShapeCraft through examples of animated and user-customized editing, highlighting its potential for broader interactive applications.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ImaGGen: Zero-Shot Generation of Co-Speech Semantic Gestures Grounded in Language and Image Input</title>
<link>https://arxiv.org/abs/2510.17617</link>
<guid>https://arxiv.org/abs/2510.17617</guid>
<content:encoded><![CDATA[
arXiv:2510.17617v1 Announce Type: new 
Abstract: Human communication combines speech with expressive nonverbal cues such as hand gestures that serve manifold communicative functions. Yet, current generative gesture generation approaches are restricted to simple, repetitive beat gestures that accompany the rhythm of speaking but do not contribute to communicating semantic meaning. This paper tackles a core challenge in co-speech gesture synthesis: generating iconic or deictic gestures that are semantically coherent with a verbal utterance. Such gestures cannot be derived from language input alone, which inherently lacks the visual meaning that is often carried autonomously by gestures. We therefore introduce a zero-shot system that generates gestures from a given language input and additionally is informed by imagistic input, without manual annotation or human intervention. Our method integrates an image analysis pipeline that extracts key object properties such as shape, symmetry, and alignment, together with a semantic matching module that links these visual details to spoken text. An inverse kinematics engine then synthesizes iconic and deictic gestures and combines them with co-generated natural beat gestures for coherent multimodal communication. A comprehensive user study demonstrates the effectiveness of our approach. In scenarios where speech alone was ambiguous, gestures generated by our system significantly improved participants' ability to identify object properties, confirming their interpretability and communicative value. While challenges remain in representing complex shapes, our results highlight the importance of context-aware semantic gestures for creating expressive and collaborative virtual agents or avatars, marking a substantial step forward towards efficient and robust, embodied human-agent interaction. More information and example videos are available here: https://review-anon-io.github.io/ImaGGen.github.io/
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Mimamsa Inspired Framework For Instruction Sequencing In AI Agents</title>
<link>https://arxiv.org/abs/2510.17691</link>
<guid>https://arxiv.org/abs/2510.17691</guid>
<content:encoded><![CDATA[
arXiv:2510.17691v1 Announce Type: new 
Abstract: This paper presents a formal framework for sequencing instructions in AI agents, inspired by the Indian philosophical system of Mimamsa. The framework formalizes sequencing mechanisms through action object pairs in three distinct ways: direct assertion (Srutikrama) for temporal precedence, purpose driven sequencing (Arthakrama) for functional dependencies, and iterative procedures (Pravrittikrama) for distinguishing between parallel and sequential execution in repetitive tasks. It introduces the syntax and semantics of an action object imperative logic, extending the MIRA formalism (Srinivasan and Parthasarathi, 2021) with explicit deduction rules for sequencing. The correctness of instruction sequencing is established through a validated theorem, which is based on object dependencies across successive instructions. This is further supported by proofs of soundness and completeness. This formal verification enables reliable instruction sequencing, impacting AI applications across areas like task planning and robotics by addressing temporal reasoning and dependency modeling.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.17697</link>
<guid>https://arxiv.org/abs/2510.17697</guid>
<content:encoded><![CDATA[
arXiv:2510.17697v1 Announce Type: new 
Abstract: Steering cooperative multi-agent reinforcement learning (MARL) towards desired outcomes is challenging, particularly when the global guidance from a human on the whole multi-agent system is impractical in a large-scale MARL. On the other hand, designing mechanisms to coordinate agents most relies on empirical studies, lacking a easy-to-use research tool. In this work, we employ multi-agent influence diagrams (MAIDs) as a graphical framework to address the above issues. First, we introduce interaction paradigms that leverage MAIDs to analyze and visualize existing approaches in MARL. Then, we design a new interaction paradigm based on MAIDs, referred to as targeted intervention that is applied to only a single targeted agent, so the problem of global guidance can be mitigated. In our implementation, we introduce a causal inference technique-referred to as Pre-Strategy Intervention (PSI)-to realize the targeted intervention paradigm. Since MAIDs can be regarded as a special class of causal diagrams, a composite desired outcome that integrates the primary task goal and an additional desired outcome can be achieved by maximizing the corresponding causal effect through the PSI. Moreover, the bundled relevance graph analysis of MAIDs provides a tool to identify whether an MARL learning paradigm is workable under the design of an interaction paradigm. In experiments, we demonstrate the effectiveness of our proposed targeted intervention, and verify the result of relevance graph analysis.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from Benchmarks to Applications</title>
<link>https://arxiv.org/abs/2510.17764</link>
<guid>https://arxiv.org/abs/2510.17764</guid>
<content:encoded><![CDATA[
arXiv:2510.17764v1 Announce Type: new 
Abstract: Medical Large language models achieve strong scores on standard benchmarks; however, the transfer of those results to safe and reliable performance in clinical workflows remains a challenge. This survey reframes evaluation through a levels-of-autonomy lens (L0-L3), spanning informational tools, information transformation and aggregation, decision support, and supervised agents. We align existing benchmarks and metrics with the actions permitted at each level and their associated risks, making the evaluation targets explicit. This motivates a level-conditioned blueprint for selecting metrics, assembling evidence, and reporting claims, alongside directions that link evaluation to oversight. By centering autonomy, the survey moves the field beyond score-based claims toward credible, risk-aware evidence for real clinical use.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-driven Communication and Control Design for Distributed Frequency Regulation with Black-box Inverters</title>
<link>https://arxiv.org/abs/2510.17769</link>
<guid>https://arxiv.org/abs/2510.17769</guid>
<content:encoded><![CDATA[
arXiv:2510.17769v1 Announce Type: new 
Abstract: The increasing penetration of inverter-based resources into the power grid, with often only black-box models available, challenges long-standing frequency control methods. Most recent works take a decentralized approach without online device coordination via communication. This paper considers both dynamic behavior and communication within secondary frequency control on an intermediate timescale. We develop a distributed data-driven approach that utilizes peer-to-peer communication between inverters to avoid the need for a central control center. To enable a trade off between communication network requirements and control performance, we present a framework to guide communication topology design for secondary frequency regulation. Following design of the inter-agent information exchange scheme, we design a controller that is structured according to the communication topology with a closed-loop stability guarantee. Case studies on the IEEE 39-bus system validate the framework and illustrate the trade-off between communication requirements and control performance that is enabled by our approach.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action</title>
<link>https://arxiv.org/abs/2510.17790</link>
<guid>https://arxiv.org/abs/2510.17790</guid>
<content:encoded><![CDATA[
arXiv:2510.17790v1 Announce Type: new 
Abstract: Multimodal agents for computer use rely exclusively on primitive actions (click, type, scroll) that require accurate visual grounding and lengthy execution chains, leading to cascading failures and performance bottlenecks. While other agents leverage rich programmatic interfaces (APIs, MCP servers, tools), computer-use agents (CUAs) remain isolated from these capabilities. We present UltraCUA, a foundation model that bridges this gap through hybrid action -- seamlessly integrating GUI primitives with high-level programmatic tool calls. To achieve this, our approach comprises four key components: (1) an automated pipeline that scales programmatic tools from software documentation, open-source repositories, and code generation; (2) a synthetic data engine producing over 17,000 verifiable tasks spanning real-world computer-use scenarios; (3) a large-scale high-quality hybrid action trajectory collection with both low-level GUI actions and high-level programmatic tool calls; and (4) a two-stage training pipeline combining supervised fine-tuning with online reinforcement learning, enabling strategic alternation between low-level and high-level actions. Experiments with our 7B and 32B models demonstrate substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA models achieve an average 22% relative improvement over base models, while being 11% faster in terms of steps. Out-of-domain evaluation on WindowsAgentArena shows our model reaches 21.7% success rate, outperforming baselines trained on Windows data. The hybrid action mechanism proves critical, reducing error propagation while maintaining execution efficiency.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Executable Knowledge Graphs for Replicating AI Research</title>
<link>https://arxiv.org/abs/2510.17795</link>
<guid>https://arxiv.org/abs/2510.17795</guid>
<content:encoded><![CDATA[
arXiv:2510.17795v1 Announce Type: new 
Abstract: Replicating AI research is a crucial yet challenging task for large language model (LLM) agents. Existing approaches often struggle to generate executable code, primarily due to insufficient background knowledge and the limitations of retrieval-augmented generation (RAG) methods, which fail to capture latent technical details hidden in referenced papers. Furthermore, previous approaches tend to overlook valuable implementation-level code signals and lack structured knowledge representations that support multi-granular retrieval and reuse. To overcome these challenges, we propose Executable Knowledge Graphs (xKG), a modular and pluggable knowledge base that automatically integrates technical insights, code snippets, and domain-specific knowledge extracted from scientific literature. When integrated into three agent frameworks with two different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on PaperBench, demonstrating its effectiveness as a general and extensible solution for automated AI research replication. Code will released at https://github.com/zjunlp/xKG.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics</title>
<link>https://arxiv.org/abs/2510.17797</link>
<guid>https://arxiv.org/abs/2510.17797</guid>
<content:encoded><![CDATA[
arXiv:2510.17797v1 Announce Type: new 
Abstract: As information grows exponentially, enterprises face increasing pressure to transform unstructured data into coherent, actionable insights. While autonomous agents show promise, they often struggle with domain-specific nuances, intent alignment, and enterprise integration. We present Enterprise Deep Research (EDR), a multi-agent system that integrates (1) a Master Planning Agent for adaptive query decomposition, (2) four specialized search agents (General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool ecosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a Visualization Agent for data-driven insights, and (5) a reflection mechanism that detects knowledge gaps and updates research direction with optional human-in-the-loop steering guidance. These components enable automated report generation, real-time streaming, and seamless enterprise deployment, as validated on internal datasets. On open-ended benchmarks including DeepResearch Bench and DeepConsult, EDR outperforms state-of-the-art agentic systems without any human steering. We release the EDR framework and benchmark trajectories to advance research on multi-agent reasoning applications.
  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and Dataset at https://huggingface.co/datasets/Salesforce/EDR-200
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ATLAS: Adaptive Trading with LLM AgentS Through Dynamic Prompt Optimization and Multi-Agent Coordination</title>
<link>https://arxiv.org/abs/2510.15949</link>
<guid>https://arxiv.org/abs/2510.15949</guid>
<content:encoded><![CDATA[
arXiv:2510.15949v1 Announce Type: cross 
Abstract: Large language models show promise for financial decision-making, yet deploying them as autonomous trading agents raises fundamental challenges: how to adapt instructions when rewards arrive late and obscured by market noise, how to synthesize heterogeneous information streams into coherent decisions, and how to bridge the gap between model outputs and executable market actions. We present ATLAS (Adaptive Trading with LLM AgentS), a unified multi-agent framework that integrates structured information from markets, news, and corporate fundamentals to support robust trading decisions. Within ATLAS, the central trading agent operates in an order-aware action space, ensuring that outputs correspond to executable market orders rather than abstract signals. The agent can incorporate feedback while trading using Adaptive-OPRO, a novel prompt-optimization technique that dynamically adapts the prompt by incorporating real-time, stochastic feedback, leading to increasing performance over time. Across regime-specific equity studies and multiple LLM families, Adaptive-OPRO consistently outperforms fixed prompts, while reflection-based feedback fails to provide systematic gains.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Invisible Handshake: Tacit Collusion between Adaptive Market Agents</title>
<link>https://arxiv.org/abs/2510.15995</link>
<guid>https://arxiv.org/abs/2510.15995</guid>
<content:encoded><![CDATA[
arXiv:2510.15995v1 Announce Type: cross 
Abstract: We study the emergence of tacit collusion between adaptive trading agents in a stochastic market with endogenous price formation. Using a two-player repeated game between a market maker and a market taker, we characterize feasible and collusive strategy profiles that raise prices beyond competitive levels. We show that, when agents follow simple learning algorithms (e.g., gradient ascent) to maximize their own wealth, the resulting dynamics converge to collusive strategy profiles, even in highly liquid markets with small trade sizes. By highlighting how simple learning strategies naturally lead to tacit collusion, our results offer new insights into the dynamics of AI-driven markets.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TriAgent: Automated Biomarker Discovery with Deep Research Grounding for Triage in Acute Care by LLM-Based Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2510.16080</link>
<guid>https://arxiv.org/abs/2510.16080</guid>
<content:encoded><![CDATA[
arXiv:2510.16080v1 Announce Type: cross 
Abstract: Emergency departments worldwide face rising patient volumes, workforce shortages, and variability in triage decisions that threaten the delivery of timely and accurate care. Current triage methods rely primarily on vital signs, routine laboratory values, and clinicians' judgment, which, while effective, often miss emerging biological signals that could improve risk prediction for infection typing or antibiotic administration in acute conditions. To address this challenge, we introduce TriAgent, a large language model (LLM)-based multi-agent framework that couples automated biomarker discovery with deep research for literature-grounded validation and novelty assessment. TriAgent employs a supervisor research agent to generate research topics and delegate targeted queries to specialized sub-agents for evidence retrieval from various data sources. Findings are synthesized to classify biomarkers as either grounded in existing knowledge or flagged as novel candidates, offering transparent justification and highlighting unexplored pathways in acute care risk stratification. Unlike prior frameworks limited to existing routine clinical biomarkers, TriAgent aims to deliver an end-to-end framework from data analysis to literature grounding to improve transparency, explainability and expand the frontier of potentially actionable clinical biomarkers. Given a user's clinical query and quantitative triage data, TriAgent achieved a topic adherence F1 score of 55.7 +/- 5.0%, surpassing the CoT-ReAct agent by over 10%, and a faithfulness score of 0.42 +/- 0.39, exceeding all baselines by more than 50%. Across experiments, TriAgent consistently outperformed state-of-the-art LLM-based agentic frameworks in biomarker justification and literature-grounded novelty assessment. We share our repo: https://github.com/CellFace/TriAgent.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable RNA-Seq Clustering with an LLM-Based Agentic Evidence-Grounded Framework</title>
<link>https://arxiv.org/abs/2510.16082</link>
<guid>https://arxiv.org/abs/2510.16082</guid>
<content:encoded><![CDATA[
arXiv:2510.16082v1 Announce Type: cross 
Abstract: We propose CITE V.1, an agentic, evidence-grounded framework that leverages Large Language Models (LLMs) to provide transparent and reproducible interpretations of RNA-seq clusters. Unlike existing enrichment-based approaches that reduce results to broad statistical associations and LLM-only models that risk unsupported claims or fabricated citations, CITE V.1 transforms cluster interpretation by producing biologically coherent explanations explicitly anchored in the biomedical literature. The framework orchestrates three specialized agents: a Retriever that gathers domain knowledge from PubMed and UniProt, an Interpreter that formulates functional hypotheses, and Critics that evaluate claims, enforce evidence grounding, and qualify uncertainty through confidence and reliability indicators. Applied to Salmonella enterica RNA-seq data, CITE V.1 generated biologically meaningful insights supported by the literature, while an LLM-only Gemini baseline frequently produced speculative results with false citations. By moving RNA-seq analysis from surface-level enrichment to auditable, interpretable, and evidence-based hypothesis generation, CITE V.1 advances the transparency and reliability of AI in biomedicine.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-Based Optimal Control for Image Processing</title>
<link>https://arxiv.org/abs/2510.16154</link>
<guid>https://arxiv.org/abs/2510.16154</guid>
<content:encoded><![CDATA[
arXiv:2510.16154v1 Announce Type: cross 
Abstract: We investigate the use of multi-agent systems to solve classical image processing tasks, such as colour quantization and segmentation. We frame the task as an optimal control problem, where the objective is to steer the multi-agent dynamics to obtain colour clusters that segment the image. To do so, we balance the total variation of the colour field and fidelity to the original image. The solution is obtained resorting to primal-dual splitting and the method of multipliers. Numerical experiments, implemented in parallel with CUDA, demonstrate the efficacy of the approach and its potential for high-dimensional data.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Collaborative Learning with Affinity-Based Variance Reduction</title>
<link>https://arxiv.org/abs/2510.16232</link>
<guid>https://arxiv.org/abs/2510.16232</guid>
<content:encoded><![CDATA[
arXiv:2510.16232v1 Announce Type: cross 
Abstract: Multi-agent learning faces a fundamental tension: leveraging distributed collaboration without sacrificing the personalization needed for diverse agents. This tension intensifies when aiming for full personalization while adapting to unknown heterogeneity levels -- gaining collaborative speedup when agents are similar, without performance degradation when they are different. Embracing the challenge, we propose personalized collaborative learning (PCL), a novel framework for heterogeneous agents to collaboratively learn personalized solutions with seamless adaptivity. Through carefully designed bias correction and importance correction mechanisms, our method AffPCL robustly handles both environment and objective heterogeneity. We prove that AffPCL reduces sample complexity over independent learning by a factor of $\max\{n^{-1}, \delta\}$, where $n$ is the number of agents and $\delta\in[0,1]$ measures their heterogeneity. This affinity-based acceleration automatically interpolates between the linear speedup of federated learning in homogeneous settings and the baseline of independent learning, without requiring prior knowledge of the system. Our analysis further reveals that an agent may obtain linear speedup even by collaborating with arbitrarily dissimilar agents, unveiling new insights into personalization and collaboration in the high heterogeneity regime.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synergizing chemical and AI communities for advancing laboratories of the future</title>
<link>https://arxiv.org/abs/2510.16293</link>
<guid>https://arxiv.org/abs/2510.16293</guid>
<content:encoded><![CDATA[
arXiv:2510.16293v1 Announce Type: cross 
Abstract: The development of automated experimental facilities and the digitization of experimental data have introduced numerous opportunities to radically advance chemical laboratories. As many laboratory tasks involve predicting and understanding previously unknown chemical relationships, machine learning (ML) approaches trained on experimental data can substantially accelerate the conventional design-build-test-learn process. This outlook article aims to help chemists understand and begin to adopt ML predictive models for a variety of laboratory tasks, including experimental design, synthesis optimization, and materials characterization. Furthermore, this article introduces how artificial intelligence (AI) agents based on large language models can help researchers acquire background knowledge in chemical or data science and accelerate various aspects of the discovery process. We present three case studies in distinct areas to illustrate how ML models and AI agents can be leveraged to reduce time-consuming experiments and manual data analysis. Finally, we highlight existing challenges that require continued synergistic effort from both experimental and computational communities to address.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Reinforcement Learning for Robust Control of Fixed-Wing Aircraft under Model Uncertainty</title>
<link>https://arxiv.org/abs/2510.16650</link>
<guid>https://arxiv.org/abs/2510.16650</guid>
<content:encoded><![CDATA[
arXiv:2510.16650v1 Announce Type: cross 
Abstract: This paper presents a reinforcement learning-based path-following controller for a fixed-wing small uncrewed aircraft system (sUAS) that is robust to uncertainties in the aerodynamic model of the sUAS. The controller is trained using the Robust Adversarial Reinforcement Learning framework, where an adversary perturbs the environment (aerodynamic model) to expose the agent (sUAS) to demanding scenarios. In our formulation, the adversary introduces rate-bounded perturbations to the aerodynamic model coefficients. We demonstrate that adversarial training improves robustness compared to controllers trained using stochastic model uncertainty. The learned controller is also benchmarked against a switched uncertain initial condition controller. The effectiveness of the approach is validated through high-fidelity simulations using a realistic six-degree-of-freedom fixed-wing aircraft model, showing accurate and robust path-following performance under a variety of uncertain aerodynamic conditions.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARCO-BO: Adaptive Resource-aware COllaborative Bayesian Optimization for Heterogeneous Multi-Agent Design</title>
<link>https://arxiv.org/abs/2510.16652</link>
<guid>https://arxiv.org/abs/2510.16652</guid>
<content:encoded><![CDATA[
arXiv:2510.16652v1 Announce Type: cross 
Abstract: Modern scientific and engineering design increasingly involves distributed optimization, where agents such as laboratories, simulations, or industrial partners pursue related goals under differing conditions. These agents often face heterogeneities in objectives, evaluation budgets, and accessible design variables, which complicates coordination and can lead to redundancy, poor resource use, and ineffective information sharing. Bayesian Optimization (BO) is a widely used decision-making framework for expensive black box functions, but its single-agent formulation assumes centralized control and full data sharing. Recent collaborative BO methods relax these assumptions, yet they often require uniform resources, fully shared input spaces, and fixed task alignment, conditions rarely satisfied in practice. To address these challenges, we introduce Adaptive Resource Aware Collaborative Bayesian Optimization (ARCO-BO), a framework that explicitly accounts for heterogeneity in multi-agent optimization. ARCO-BO combines three components: a similarity and optima-aware consensus mechanism for adaptive information sharing, a budget-aware asynchronous sampling strategy for resource coordination, and a partial input space sharing for heterogeneous design spaces. Experiments on synthetic and high-dimensional engineering problems show that ARCO-BO consistently outperforms independent BO and existing collaborative BO via consensus approach, achieving robust and efficient performance in complex multi-agent settings.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Layered Reasoning from a Single Viewpoint for Learning See-Through Grasping</title>
<link>https://arxiv.org/abs/2312.09822</link>
<guid>https://arxiv.org/abs/2312.09822</guid>
<content:encoded><![CDATA[
arXiv:2312.09822v4 Announce Type: replace 
Abstract: Sensory substitution enables biological systems to perceive stimuli typically obtained by another organ, which is inspirational for physical agents. Multi-modal perception of intrinsic and extrinsic interactions is critical in building an intelligent robot that learns. This study presents a Vision-based See-Through Perception (VBSeeThruP) architecture that simultaneously perceives multiple intrinsic and extrinsic modalities via a single visual input in a markerless way, all packed within a soft robotic finger using the Soft Polyhedral Network design. It is generally applicable to miniature vision systems placed underneath deformable networks with a see-through design, capturing real-time images of the network's physical interactions induced by contact-based events overlayed on top of the visual scene of the external environment, as demonstrated in the ablation study. We present the VBSeeThruP's capability for learning reactive grasping without using external cameras or dedicated force and torque sensors on the fingertips. Using the inpainted scene and the deformation mask, we further demonstrate the multi-modal performance of the VBSeeThruP architecture to simultaneously achieve various perceptions, including but not limited to scene inpainting, object detection, depth sensing, scene segmentation, masked deformation tracking, 6D force/torque sensing, and contact event detection, all within a single sensory input from the in-finger vision markerlessly.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe Multi-Agent Reinforcement Learning for Behavior-Based Cooperative Navigation</title>
<link>https://arxiv.org/abs/2312.12861</link>
<guid>https://arxiv.org/abs/2312.12861</guid>
<content:encoded><![CDATA[
arXiv:2312.12861v3 Announce Type: replace 
Abstract: In this paper, we address the problem of behavior-based cooperative navigation of mobile robots using safe multi-agent reinforcement learning~(MARL). Our work is the first to focus on cooperative navigation without individual reference targets for the robots, using a single target for the formation's centroid. This eliminates the complexities involved in having several path planners to control a team of robots. To ensure safety, our MARL framework uses model predictive control (MPC) to prevent actions that could lead to collisions during training and execution. We demonstrate the effectiveness of our method in simulation and on real robots, achieving safe behavior-based cooperative navigation without using individual reference targets, with zero collisions, and faster target reaching compared to baselines. Finally, we study the impact of MPC safety filters on the learning process, revealing that we achieve faster convergence during training and we show that our approach can be safely deployed on real robots, even during early stages of the training.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Black Hole Search in Dynamic Graphs</title>
<link>https://arxiv.org/abs/2405.18367</link>
<guid>https://arxiv.org/abs/2405.18367</guid>
<content:encoded><![CDATA[
arXiv:2405.18367v2 Announce Type: replace 
Abstract: A black hole is considered to be a dangerous node present in a graph that disposes of any resources that enter that node. Therefore, it is essential to find such a node in the graph. Let a group of agents be present on a graph $G$. The Black Hole Search (BHS) problem aims for at least one agent to survive and terminate after {finding} the black hole. This problem is already studied for specific dynamic graph classes such as rings, cactuses, and tori {where finding the black hole means at least one agent needs to survive and terminate after knowing at least one edge associated with the black hole. In this work, we investigate the problem of BHS for general graphs.} In the dynamic graph, adversary may remove edges at each round keeping the graph connected. We consider two cases: (a) at any round at most one edge can be removed (b) at any round at most $f$ edges can be removed. For both scenarios, we study the problem when the agents start from a rooted initial configuration. We consider each agent has $O(\log n)$ memory and each node has $O(\log n)$ storage.
  For case (a), we present an algorithm with $9$ agents that solves the problem of BHS in $O(|E|^2)$ time where $|E|$ is the number of edges and $\delta_v$ is the degree of the node $v$ in $G$. We show it is impossible to solve for $2\delta_{BH}$ many agents starting from an arbitrary configuration where $\delta_{BH}$ is the degree of the black hole in $G$. We also provide another improved algorithm that uses $6$ agents from a rooted initial configuration to solve the problem of BHS.
  For case (b), we provide an algorithm using $6f$ agents to solve the problem of BHS, albeit taking exponential time. We also provide an impossibility result for $2f+1$ agents starting from a rooted initial configuration. This result holds even if unlimited storage is available on each node and the agents have infinite memory.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Self-play Methods in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2408.01072</link>
<guid>https://arxiv.org/abs/2408.01072</guid>
<content:encoded><![CDATA[
arXiv:2408.01072v4 Announce Type: replace 
Abstract: Self-play, a learning paradigm where agents iteratively refine their policies by interacting with historical or concurrent versions of themselves or other evolving agents, has shown remarkable success in solving complex non-cooperative multi-agent tasks. Despite its growing prominence in multi-agent reinforcement learning (MARL), such as Go, poker, and video games, a comprehensive and structured understanding of self-play remains lacking. This survey fills this gap by offering a comprehensive roadmap to the diverse landscape of self-play methods. We begin by introducing the necessary preliminaries, including the MARL framework and basic game theory concepts. Then, it provides a unified framework and classifies existing self-play algorithms within this framework. Moreover, the paper bridges the gap between the algorithms and their practical implications by illustrating the role of self-play in different non-cooperative scenarios. Finally, the survey highlights open challenges and future research directions in self-play.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Riemannian Federated Learning via Averaging Gradient Streams</title>
<link>https://arxiv.org/abs/2409.07223</link>
<guid>https://arxiv.org/abs/2409.07223</guid>
<content:encoded><![CDATA[
arXiv:2409.07223v2 Announce Type: replace 
Abstract: Federated learning (FL) as a distributed learning paradigm has a significant advantage in addressing large-scale machine learning tasks. In the Euclidean setting, FL algorithms have been extensively studied with both theoretical and empirical success. However, there exist few works that investigate federated learning algorithms in the Riemannian setting. In particular, critical challenges such as partial participation and data heterogeneity among agents are not explored in the Riemannian federated setting. This paper presents and analyzes a Riemannian FL algorithm, called RFedAGS, based on a new efficient server aggregation -- averaging gradient streams, which can simultaneously handle partial participation and data heterogeneity. We theoretically show that the proposed RFedAGS has global convergence and sublinear convergence rate under decaying step sizes cases; and converges sublinearly/linearly to a neighborhood of a stationary point/solution under fixed step sizes cases. These analyses are based on a vital and non-trivial assumption induced by partial participation, which is shown to hold with high probability. Extensive experiments conducted on synthetic and real-world data demonstrate the good performance of RFedAGS.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Based Meta Reinforcement Learning for Robust and Adaptable Peg-in-Hole Assembly Tasks</title>
<link>https://arxiv.org/abs/2409.16208</link>
<guid>https://arxiv.org/abs/2409.16208</guid>
<content:encoded><![CDATA[
arXiv:2409.16208v4 Announce Type: replace 
Abstract: Autonomous assembly is an essential capability for industrial and service robots, with Peg-in-Hole (PiH) insertion being one of the core tasks. However, PiH assembly in unknown environments is still challenging due to uncertainty in task parameters, such as the hole position and orientation, resulting from sensor noise. Although context-based meta reinforcement learning (RL) methods have been previously presented to adapt to unknown task parameters in PiH assembly tasks, the performance depends on a sample-inefficient procedure or human demonstrations. Thus, to enhance the applicability of meta RL in real-world PiH assembly tasks, we propose to train the agent to use information from the robot's forward kinematics and an uncalibrated camera. Furthermore, we improve the performance by efficiently adapting the meta-trained agent to use data from force/torque sensor. Finally, we propose an adaptation procedure for out-of-distribution tasks whose parameters are different from the training tasks. Experiments on simulated and real robots prove that our modifications enhance the sample efficiency during meta training, real-world adaptation performance, and generalization of the context-based meta RL agent in PiH assembly tasks compared to previous approaches.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Study on LLM-based Agents for Automated Bug Fixing</title>
<link>https://arxiv.org/abs/2411.10213</link>
<guid>https://arxiv.org/abs/2411.10213</guid>
<content:encoded><![CDATA[
arXiv:2411.10213v2 Announce Type: replace 
Abstract: Large language models (LLMs) and LLM-based Agents have been applied to fix bugs automatically, demonstrating the capability in addressing software defects by engaging in development environment interaction, iterative validation and code modification. However, systematic analysis of these agent systems remain limited, particularly regarding performance variations among top-performing ones. In this paper, we examine six repair systems on the SWE-bench Verified benchmark for automated bug fixing. We first assess each system's overall performance, noting the instances solvable by all or none of these systems, and explore the capabilities of different systems. We also compare fault localization accuracy at file and code symbol levels and evaluate bug reproduction capabilities. Through analysis, we concluded that further optimization is needed in both the LLM capability itself and the design of Agentic flow to improve the effectiveness of the Agent in bug fixing.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asynchronous Agents with Perfect Recall: Model Reductions, Knowledge-Based Construction, and Model Checking for Coalitional Strategies</title>
<link>https://arxiv.org/abs/2412.06706</link>
<guid>https://arxiv.org/abs/2412.06706</guid>
<content:encoded><![CDATA[
arXiv:2412.06706v2 Announce Type: replace 
Abstract: Model checking of strategic abilities for agents with memory is a notoriously hard problem, and very few attempts have been made to tackle it. In this paper, we present two important steps towards this goal. First, we take the partial-order reduction scheme that was recently proved to preserve individual and coalitional abilities of memoryless agents, and show that it also works for agents with memory. Secondly, we take the Knowledge-Based Subset Construction, that was recently studied for synchronous concurrent games, and adapt it to preserve abilities of memoryful agents in asynchronous MAS. On the way, we also propose a new execution semantics for strategies in asynchronous MAS, that combines elements of Concurrent Game Structures and Interleaved Interpreted Systems in a natural and intuitive way.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets</title>
<link>https://arxiv.org/abs/2502.01506</link>
<guid>https://arxiv.org/abs/2502.01506</guid>
<content:encoded><![CDATA[
arXiv:2502.01506v5 Announce Type: replace 
Abstract: The study of social emergence has long been a central focus in social science. Traditional modeling approaches, such as rule-based Agent-Based Models (ABMs), struggle to capture the diversity and complexity of human behavior, particularly the irrational factors emphasized in behavioral economics. Recently, large language model (LLM) agents have gained traction as simulation tools for modeling human behavior in social science and role-playing applications. Studies suggest that LLMs can account for cognitive biases, emotional fluctuations, and other non-rational influences, enabling more realistic simulations of socio-economic dynamics. In this work, we introduce TwinMarket, a novel multi-agent framework that leverages LLMs to simulate socio-economic systems. Specifically, we examine how individual behaviors, through interactions and feedback mechanisms, give rise to collective dynamics and emergent phenomena. Through experiments in a simulated stock market environment, we demonstrate how individual actions can trigger group behaviors, leading to emergent outcomes such as financial bubbles and recessions. Our approach provides valuable insights into the complex interplay between individual decision-making and collective socio-economic patterns.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VolleyBots: A Testbed for Multi-Drone Volleyball Game Combining Motion Control and Strategic Play</title>
<link>https://arxiv.org/abs/2502.01932</link>
<guid>https://arxiv.org/abs/2502.01932</guid>
<content:encoded><![CDATA[
arXiv:2502.01932v5 Announce Type: replace 
Abstract: Robot sports, characterized by well-defined objectives, explicit rules, and dynamic interactions, present ideal scenarios for demonstrating embodied intelligence. In this paper, we present VolleyBots, a novel robot sports testbed where multiple drones cooperate and compete in the sport of volleyball under physical dynamics. VolleyBots integrates three features within a unified platform: competitive and cooperative gameplay, turn-based interaction structure, and agile 3D maneuvering. These intertwined features yield a complex problem combining motion control and strategic play, with no available expert demonstrations. We provide a comprehensive suite of tasks ranging from single-drone drills to multi-drone cooperative and competitive tasks, accompanied by baseline evaluations of representative reinforcement learning (RL), multi-agent reinforcement learning (MARL) and game-theoretic algorithms. Simulation results show that on-policy RL methods outperform off-policy methods in single-agent tasks, but both approaches struggle in complex tasks that combine motion control and strategic play. We additionally design a hierarchical policy which achieves 69.5% win rate against the strongest baseline in the 3 vs 3 task, demonstrating its potential for tackling the complex interplay between low-level control and high-level strategy. To highlight VolleyBots' sim-to-real potential, we further demonstrate the zero-shot deployment of a policy trained entirely in simulation on real-world drones.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fully Autonomous AI Agents Should Not be Developed</title>
<link>https://arxiv.org/abs/2502.02649</link>
<guid>https://arxiv.org/abs/2502.02649</guid>
<content:encoded><![CDATA[
arXiv:2502.02649v3 Announce Type: replace 
Abstract: This paper argues that fully autonomous AI agents should not be developed. In support of this position, we build from prior scientific literature and current product marketing to delineate different AI agent levels and detail the ethical values at play in each, documenting trade-offs in potential benefits and risks. Our analysis reveals that risks to people increase with the autonomy of a system: The more control a user cedes to an AI agent, the more risks to people arise. Particularly concerning are safety risks, which affect human life and impact further values.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Principled Unsupervised Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.08365</link>
<guid>https://arxiv.org/abs/2502.08365</guid>
<content:encoded><![CDATA[
arXiv:2502.08365v4 Announce Type: replace 
Abstract: In reinforcement learning, we typically refer to unsupervised pre-training when we aim to pre-train a policy without a priori access to the task specification, i.e. rewards, to be later employed for efficient learning of downstream tasks. In single-agent settings, the problem has been extensively studied and mostly understood. A popular approach, called task-agnostic exploration, casts the unsupervised objective as maximizing the entropy of the state distribution induced by the agent's policy, from which principles and methods follow.
  In contrast, little is known about it in multi-agent settings, which are ubiquitous in the real world. What are the pros and cons of alternative problem formulations in this setting? How hard is the problem in theory, how can we solve it in practice? In this paper, we address these questions by first characterizing those alternative formulations and highlighting how the problem, even when tractable in theory, is non-trivial in practice. Then, we present a scalable, decentralized, trust-region policy search algorithm to address the problem in practical settings. Finally, we provide numerical validations to both corroborate the theoretical findings and pave the way for unsupervised multi-agent reinforcement learning via task-agnostic exploration in challenging domains, showing that optimizing for a specific objective, namely mixture entropy, provides an excellent trade-off between tractability and performances.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Repo2Run: Automated Building Executable Environment for Code Repository at Scale</title>
<link>https://arxiv.org/abs/2502.13681</link>
<guid>https://arxiv.org/abs/2502.13681</guid>
<content:encoded><![CDATA[
arXiv:2502.13681v4 Announce Type: replace 
Abstract: Scaling up executable code data is significant for improving language models' software engineering capability. The intricate nature of the process makes it labor-intensive, time-consuming and expert-knowledge-dependent to build a large number of executable code repositories, limiting the scalability of existing work based on running tests. The primary bottleneck lies in the automated building of test environments for different repositories, which is an essential yet underexplored task. To mitigate the gap, we introduce Repo2Run, the first LLM-based agent aiming at automating the building of executable test environments for any repositories at scale. Specifically, given a code repository, Repo2Run iteratively builds the Docker image, runs unit tests based on the feedback of the building, and synthesizes the Dockerfile until the entire pipeline is executed successfully. The resulting Dockerfile can then be used to create Docker container environments for running code and tests. We created a benchmark containing 420 Python repositories with unit tests for evaluation. The results illustrate that Repo2Run achieves an 86.0% success rate, outperforming SWE-agent by 77.0%. The resources of Repo2Run are available at https://github.com/bytedance/Repo2Run.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSeek-Inspired Exploration of RL-based LLMs and Synergy with Wireless Networks: A Survey</title>
<link>https://arxiv.org/abs/2503.09956</link>
<guid>https://arxiv.org/abs/2503.09956</guid>
<content:encoded><![CDATA[
arXiv:2503.09956v4 Announce Type: replace 
Abstract: Reinforcement learning (RL)-based large language models (LLMs), such as ChatGPT, DeepSeek, and Grok-3, have attracted widespread attention for their remarkable capabilities in multimodal data understanding. Meanwhile, the rapid expansion of information services has led to a growing demand for AI-enabled wireless networks. The open-source DeepSeek models are famous for their innovative designs, such as large-scale pure RL and cost-efficient training, which make them well-suited for practical deployment in wireless networks. By integrating DeepSeek-style LLMs with wireless infrastructures, a synergistic opportunity arises: the DeepSeek-style LLMs enhance network optimization with strong reasoning and decision-making abilities, while wireless infrastructure enables the broad deployment of these models. Motivated by this convergence, this survey presents a comprehensive DeepSeek-inspired exploration of RL-based LLMs in the context of wireless networks. We begin by reviewing key techniques behind network optimization to establish a foundation for understanding DeepSeek-style LLM integration. Next, we examine recent advancements in RL-based LLMs, using DeepSeek models as a representative example. Building on this, we explore the synergy between the two domains, highlighting motivations, challenges, and potential solutions. Finally, we highlight emerging directions for integrating LLMs with wireless networks, such as quantum, on-device, and neural-symbolic LLM models, as well as embodied AI agents. Overall, this survey offers a comprehensive examination of the interplay between DeepSeek-style LLMs and wireless networks, demonstrating how these domains can mutually enhance each other to drive innovation.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provably Efficient Reward Transfer in Reinforcement Learning with Discrete Markov Decision Processes</title>
<link>https://arxiv.org/abs/2503.13414</link>
<guid>https://arxiv.org/abs/2503.13414</guid>
<content:encoded><![CDATA[
arXiv:2503.13414v2 Announce Type: replace 
Abstract: In this paper, we propose a new solution to reward adaptation (RA) in reinforcement learning, where the agent adapts to a target reward function based on one or more existing source behaviors learned a priori under the same domain dynamics but different reward functions. While learning the target behavior from scratch is possible, it is often inefficient given the available source behaviors. Our work introduces a new approach to RA through the manipulation of Q-functions. Assuming the target reward function is a known function of the source reward functions, we compute bounds on the Q-function and present an iterative process (akin to value iteration) to tighten these bounds. Such bounds enable action pruning in the target domain before learning even starts. We refer to this method as "Q-Manipulation" (Q-M). The iteration process assumes access to a lite-model, which is easy to provide or learn. We formally prove that Q-M, under discrete domains, does not affect the optimality of the returned policy and show that it is provably efficient in terms of sample complexity in a probabilistic sense. Q-M is evaluated in a variety of synthetic and simulation domains to demonstrate its effectiveness, generalizability, and practicality.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Verifying Equilibria in Finite-Horizon Probabilistic Concurrent Game Systems</title>
<link>https://arxiv.org/abs/2503.14690</link>
<guid>https://arxiv.org/abs/2503.14690</guid>
<content:encoded><![CDATA[
arXiv:2503.14690v5 Announce Type: replace 
Abstract: Finite-horizon probabilistic multiagent concurrent game systems, also known as finite multiplayer stochastic games, are a well-studied model in computer science due to their ability to represent a wide range of real-world scenarios involving strategic interactions among agents over a finite amount of iterations (given by the finite-horizon). The analysis of these games typically focuses on evaluating (verifying) and computing (synthesizing/realizing) which strategy profiles (functions that represent the behavior of each agent) qualify as equilibria. The two most prominent equilibrium concepts are the Nash equilibrium and the subgame perfect equilibrium, with the latter considered a conceptual refinement of the former. However, computing these equilibria from scratch is often computationally infeasible. Therefore, recent attention has shifted to the verification problem, where a given strategy profile must be evaluated to determine whether it satisfies equilibrium conditions. In this paper, we demonstrate that the verification problem for subgame perfect equilibria lies in PSPACE, while for Nash equilibria, it is EXPTIME-complete. This is a highly counterintuitive result since subgame perfect equilibria are often seen as a strict strengthening of Nash equilibria and are intuitively seen as more complicated.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unseen from Seen: Rewriting Observation-Instruction Using Foundation Models for Augmenting Vision-Language Navigation</title>
<link>https://arxiv.org/abs/2503.18065</link>
<guid>https://arxiv.org/abs/2503.18065</guid>
<content:encoded><![CDATA[
arXiv:2503.18065v2 Announce Type: replace 
Abstract: Data scarcity is a long-standing challenge in the Vision-Language Navigation (VLN) field, which extremely hinders the generalization of agents to unseen environments. Previous works primarily rely on additional simulator data or web-collected images/videos to improve the generalization. However, the simulator environments still face limited diversity, and the web-collected data often requires extensive labor to remove the noise. In this paper, we propose a Rewriting-driven AugMentation (RAM) paradigm for VLN, which directly creates the unseen observation-instruction pairs via rewriting human-annotated training data. Benefiting from our rewriting mechanism, new observation-instruction pairs can be obtained in both simulator-free and labor-saving manners to promote generalization. Specifically, we first introduce Object-Enriched Observation Rewriting, where we combine Vision-Language Models (VLMs) and Large Language Models (LLMs) to derive rewritten object-enriched scene descriptions, enabling observation synthesis with diverse objects and spatial layouts via Text-to-Image Generation Models (T2IMs). Then, we propose Observation-Contrast Instruction Rewriting, which generates observation-aligned rewritten instructions by requiring LLMs to reason the difference between original and new observations. We further develop a mixing-then-focusing training strategy with a random observation cropping scheme, effectively enhancing data distribution diversity while suppressing augmentation data noise during training. Experiments on both the discrete environments (R2R, REVERIE, and R4R datasets) and continuous environments (R2R-CE dataset) show the superior performance and impressive generalization ability of our method. Code is available at https://github.com/SaDil13/VLN-RAM.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking LLM Repair Capabilities Through Cross-Language Translation and Multi-Agent Refinement</title>
<link>https://arxiv.org/abs/2503.22512</link>
<guid>https://arxiv.org/abs/2503.22512</guid>
<content:encoded><![CDATA[
arXiv:2503.22512v4 Announce Type: replace 
Abstract: Recent advances in leveraging LLMs for APR have demonstrated impressive capabilities in fixing software defects. However, current LLM-based approaches predominantly focus on mainstream programming languages like Java and Python, neglecting less prevalent but emerging languages such as Rust due to expensive training resources, limited datasets, and insufficient community support. This narrow focus creates a significant gap in repair capabilities across the programming language spectrum, where the full potential of LLMs for comprehensive multilingual program repair remains largely unexplored. To address this limitation, we introduce a novel cross-language program repair approach LANTERN that leverages LLMs' differential proficiency across languages through a multi-agent iterative repair paradigm. Our technique strategically translates defective code from languages where LLMs exhibit weaker repair capabilities to languages where they demonstrate stronger performance, without requiring additional training. A key innovation of our approach is an LLM-based decision-making system that dynamically selects optimal target languages based on bug characteristics and continuously incorporates feedback from previous repair attempts. We evaluate our method on xCodeEval, a comprehensive multilingual benchmark comprising 5,068 bugs across 11 programming languages. Results demonstrate significant enhancement in repair effectiveness, particularly for underrepresented languages, with Rust showing a 22.09% improvement in Pass@10 metrics. Our research provides the first empirical evidence that cross-language translation significantly expands the repair capabilities of LLMs and effectively bridges the performance gap between programming languages with different levels of popularity, opening new avenues for truly language-agnostic automated program repair.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>First Field-Trial Demonstration of L4 Autonomous Optical Network for Distributed AI Training Communication: An LLM-Powered Multi-AI-Agent Solution</title>
<link>https://arxiv.org/abs/2504.01234</link>
<guid>https://arxiv.org/abs/2504.01234</guid>
<content:encoded><![CDATA[
arXiv:2504.01234v2 Announce Type: replace 
Abstract: We demonstrate the first cross-domain cross-layer level-4 autonomous optical network via a multi-AI-agent system. Field trials show ~98% task completion rate across the distributed AI training lifecycle-3.2x higher than single agents using state-of-the-art LLMs.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nash equilibrium seeking in coalition games for multiple Euler-Lagrange systems: Analysis and application to USV swarm confrontation</title>
<link>https://arxiv.org/abs/2504.04475</link>
<guid>https://arxiv.org/abs/2504.04475</guid>
<content:encoded><![CDATA[
arXiv:2504.04475v2 Announce Type: replace 
Abstract: This paper addresses a class of Nash equilibrium (NE) seeking problems in coalition games involving both local and coupling constraints for multiple Euler-Lagrange (EL) systems subject to disturbances of unknown bounds. Within each coalition, agents cooperatively minimize a shared cost function while competing against other coalitions. A distributed strategy is proposed to seek the NE under informational constraints, where each agent has access only to its own action, cost function, and constraint parameters. In the proposed distributed NE seeking strategy, adaptive techniques are combined with sign functions to handle model uncertainties and disturbances with unknown bounds in the EL systems. To deal with the Lagrange multipliers associated with local and coupling constraints, primal-dual techniques are integrated with consensus protocols. Additionally, a dynamic average consensus algorithm is employed to estimate the gradient of the coalition cost function, while a leader-following protocol is utilized to estimate the actions of other agents. Under standard convexity and graph-connectivity assumptions, global convergence of the closed-loop EL system to the NE is established. As an illustrative application, a swarm confrontation of unmanned surface vehicles involving formation, encirclement, and interception tasks is modeled within the coalition game framework, and numerical simulations are conducted under this model to validate the theoretical results.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LANGTRAJ: Diffusion Model and Dataset for Language-Conditioned Trajectory Simulation</title>
<link>https://arxiv.org/abs/2504.11521</link>
<guid>https://arxiv.org/abs/2504.11521</guid>
<content:encoded><![CDATA[
arXiv:2504.11521v2 Announce Type: replace 
Abstract: Evaluating autonomous vehicles with controllability enables scalable testing in counterfactual or structured settings, enhancing both efficiency and safety. We introduce LangTraj, a language-conditioned scene-diffusion model that simulates the joint behavior of all agents in traffic scenarios. By conditioning on natural language inputs, LangTraj provides flexible and intuitive control over interactive behaviors, generating nuanced and realistic scenarios. Unlike prior approaches that depend on domain-specific guidance functions, LangTraj incorporates language conditioning during training, facilitating more intuitive traffic simulation control. We propose a novel closed-loop training strategy for diffusion models, explicitly tailored to enhance stability and realism during closed-loop simulation. To support language-conditioned simulation, we develop Inter-Drive, a large-scale dataset with diverse and interactive labels for training language-conditioned diffusion models. Our dataset is built upon a scalable pipeline for annotating agent-agent interactions and single-agent behaviors, ensuring rich and varied supervision. Validated on the Waymo Open Motion Dataset, LangTraj demonstrates strong performance in realism, language controllability, and language-conditioned safety-critical simulation, establishing a new paradigm for flexible and scalable autonomous vehicle testing. Project Website: https://langtraj.github.io/
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeVisionary: An Agent-based Framework for Evaluating Large Language Models in Code Generation</title>
<link>https://arxiv.org/abs/2504.13472</link>
<guid>https://arxiv.org/abs/2504.13472</guid>
<content:encoded><![CDATA[
arXiv:2504.13472v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated strong capabilities in code generation, underscoring the critical need for rigorous and comprehensive evaluation. Existing evaluation approaches fall into three categories, including human-centered, metric-based, and LLM-based. Considering that human-centered approaches are labour-intensive and metric-based ones overly rely on reference answers, LLM-based approaches are gaining increasing attention due to their stronger contextual understanding capabilities. However, they generally evaluate the generated code based on static prompts, and tend to fail for complex code scenarios which typically involve multiple requirements and require more contextual information. In addition, these approaches lack fine-grained evaluation for complex code, resulting in limited explainability. To mitigate the limitations, we propose CodeVisionary, the first agent-based evaluation framework for complex code generation. CodeVisionary consists of two stages: (1) Requirement-guided multi-dimensional context distillation stage and (2) Fine-grained scoring and summarization stage. A comprehensive evaluation report is also generated for enhanced explainability. For validation, we construct a new benchmark consisting of 363 samples spanning 37 coding scenarios and 23 programming languages. Extensive experiments demonstrate that CodeVisionary achieves the best performance among three baselines for evaluating complex code generation, outperforming the best baseline with average improvements of 0.217, 0.163, and 0.141 in Pearson, Spearman, and Kendall-Tau coefficients, respectively. The resources of CodeVisionary are available at https://github.com/Eshe0922/CodeVisionary.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentivizing Truthful Language Models via Peer Elicitation Games</title>
<link>https://arxiv.org/abs/2505.13636</link>
<guid>https://arxiv.org/abs/2505.13636</guid>
<content:encoded><![CDATA[
arXiv:2505.13636v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated strong generative capabilities but remain prone to inconsistencies and hallucinations. We introduce Peer Elicitation Games (PEG), a training-free, game-theoretic framework for aligning LLMs through a peer elicitation mechanism involving a generator and multiple discriminators instantiated from distinct base models. Discriminators interact in a peer evaluation setting, where utilities are computed using a determinant-based mutual information score that provably incentivizes truthful reporting without requiring ground-truth labels. We establish theoretical guarantees showing that each agent, via online learning, achieves sublinear regret in the sense their cumulative performance approaches that of the best fixed truthful strategy in hindsight. Moreover, we prove last-iterate convergence to a truthful Nash equilibrium, ensuring that the actual policies used by agents converge to stable and truthful behavior over time. Empirical evaluations across multiple benchmarks demonstrate significant improvements in factual accuracy. These results position PEG as a practical approach for eliciting truthful behavior from LLMs without supervision or fine-tuning.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smart Traffic Signals: Comparing MARL and Fixed-Time Strategies</title>
<link>https://arxiv.org/abs/2505.14544</link>
<guid>https://arxiv.org/abs/2505.14544</guid>
<content:encoded><![CDATA[
arXiv:2505.14544v3 Announce Type: replace 
Abstract: Urban traffic congestion, particularly at intersections, significantly impacts travel time, fuel consumption, and emissions. Traditional fixed-time signal control systems often lack the adaptability to manage dynamic traffic patterns effectively. This study explores the application of multi-agent reinforcement learning (MARL) to optimize traffic signal coordination across multiple intersections within a simulated environment. Utilizing Pygame, a simulation was developed to model a network of interconnected intersections with randomly generated vehicle flows to reflect realistic traffic variability. A decentralized MARL controller was implemented, in which each traffic signal operates as an autonomous agent, making decisions based on local observations and information from neighboring agents. Performance was evaluated against a baseline fixed-time controller using metrics such as average vehicle wait time and overall throughput. The MARL approach demonstrated statistically significant improvements, including reduced average waiting times and improved throughput. These findings suggest that MARL-based dynamic control strategies hold substantial promise for improving urban traffic management efficiency. More research is recommended to address scalability and real-world implementation challenges.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DISCOVER: Automated Curricula for Sparse-Reward Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.19850</link>
<guid>https://arxiv.org/abs/2505.19850</guid>
<content:encoded><![CDATA[
arXiv:2505.19850v2 Announce Type: replace 
Abstract: Sparse-reward reinforcement learning (RL) can model a wide range of highly complex tasks. Solving sparse-reward tasks is RL's core premise, requiring efficient exploration coupled with long-horizon credit assignment, and overcoming these challenges is key for building self-improving agents with superhuman ability. Prior work commonly explores with the objective of solving many sparse-reward tasks, making exploration of individual high-dimensional, long-horizon tasks intractable. We argue that solving such challenging tasks requires solving simpler tasks that are relevant to the target task, i.e., whose achieval will teach the agent skills required for solving the target task. We demonstrate that this sense of direction, necessary for effective exploration, can be extracted from existing RL algorithms, without leveraging any prior information. To this end, we propose a method for directed sparse-reward goal-conditioned very long-horizon RL (DISCOVER), which selects exploratory goals in the direction of the target task. We connect DISCOVER to principled exploration in bandits, formally bounding the time until the target task becomes achievable in terms of the agent's initial distance to the target, but independent of the volume of the space of all tasks. We then perform a thorough evaluation in high-dimensional environments. We find that the directed goal selection of DISCOVER solves exploration problems that are beyond the reach of prior state-of-the-art exploration methods in RL.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RocqStar: Leveraging Similarity-driven Retrieval and Agentic Systems for Rocq generation</title>
<link>https://arxiv.org/abs/2505.22846</link>
<guid>https://arxiv.org/abs/2505.22846</guid>
<content:encoded><![CDATA[
arXiv:2505.22846v2 Announce Type: replace 
Abstract: Interactive Theorem Proving was repeatedly shown to be fruitful combined with Generative Artificial Intelligence. This paper assesses multiple approaches to Rocq generation and illuminates potential avenues for improvement. We highlight the importance of thorough premise selection for generating Rocq proofs and propose a novel approach, leveraging retrieval via a self-attentive embedder model. The evaluation of the designed approach shows up to 28% relative increase of the generator's performance. We tackle the problem of writing Rocq proofs using a multi-stage agentic system, tailored for formal verification, and demonstrate its high effectiveness. We conduct an ablation study and demonstrate shows that incorporating multi-agent debate during the planning stage increases the proof success rate by 20% overall and nearly doubles it for complex theorems, while the reflection mechanism further enhances stability and consistency.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind</title>
<link>https://arxiv.org/abs/2505.22961</link>
<guid>https://arxiv.org/abs/2505.22961</guid>
<content:encoded><![CDATA[
arXiv:2505.22961v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown promising potential in persuasion, but existing works on training LLM persuaders are still preliminary. Notably, while humans are skilled in modeling their opponent's thoughts and opinions proactively and dynamically, current LLMs struggle with such Theory of Mind (ToM) reasoning, resulting in limited diversity and opponent awareness. To address this limitation, we introduce Theory of Mind Augmented Persuader (ToMAP), a novel approach for building more flexible persuader agents by incorporating two theory of mind modules that enhance the persuader's awareness and analysis of the opponent's mental state. Specifically, we begin by prompting the persuader to consider possible objections to the target central claim, and then use a text encoder paired with a trained MLP classifier to predict the opponent's current stance on these counterclaims. Our carefully designed reinforcement learning schema enables the persuader learns how to analyze opponent-related information and utilize it to generate more effective arguments. Experiments show that the ToMAP persuader, while containing only 3B parameters, outperforms much larger baselines, like GPT-4o, with a relative gain of 39.4% across multiple persuadee models and diverse corpora. Notably, ToMAP exhibits complex reasoning chains and reduced repetition during training, which leads to more diverse and effective arguments. The opponent-aware feature of ToMAP also makes it suitable for long conversations and enables it to employ more logical and opponent-aware strategies. These results underscore our method's effectiveness and highlight its potential for developing more persuasive language agents. Code is available at: https://github.com/ulab-uiuc/ToMAP.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents</title>
<link>https://arxiv.org/abs/2506.00641</link>
<guid>https://arxiv.org/abs/2506.00641</guid>
<content:encoded><![CDATA[
arXiv:2506.00641v2 Announce Type: replace 
Abstract: Despite the rapid advancement of LLM-based agents, the reliable evaluation of their safety and security remains a significant challenge. Existing rule-based or LLM-based evaluators often miss dangers in agents' step-by-step actions, overlook subtle meanings, fail to see how small issues compound, and get confused by unclear safety or security rules. To overcome this evaluation crisis, we introduce AgentAuditor, a universal, training-free, memory-augmented reasoning framework that empowers LLM evaluators to emulate human expert evaluators. AgentAuditor constructs an experiential memory by having an LLM adaptively extract structured semantic features (e.g., scenario, risk, behavior) and generate associated chain-of-thought reasoning traces for past interactions. A multi-stage, context-aware retrieval-augmented generation process then dynamically retrieves the most relevant reasoning experiences to guide the LLM evaluator's assessment of new cases. Moreover, we developed ASSEBench, the first benchmark designed to check how well LLM-based evaluators can spot both safety risks and security threats. ASSEBench comprises 2293 meticulously annotated interaction records, covering 15 risk types across 29 application scenarios. A key feature of ASSEBench is its nuanced approach to ambiguous risk situations, employing "Strict" and "Lenient" judgment standards. Experiments demonstrate that AgentAuditor not only consistently improves the evaluation performance of LLMs across all benchmarks but also sets a new state-of-the-art in LLM-as-a-judge for agent safety and security, achieving human-level accuracy. Our work is openly accessible at https://github.com/Astarojth/AgentAuditor.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoVoMix2: Advancing Zero-Shot Dialogue Generation with Fully Non-Autoregressive Flow Matching</title>
<link>https://arxiv.org/abs/2506.00885</link>
<guid>https://arxiv.org/abs/2506.00885</guid>
<content:encoded><![CDATA[
arXiv:2506.00885v2 Announce Type: replace 
Abstract: Generating natural-sounding, multi-speaker dialogue is crucial for applications such as podcast creation, virtual agents, and multimedia content generation. However, existing systems struggle to maintain speaker consistency, model overlapping speech, and synthesize coherent conversations efficiently. In this paper, we introduce CoVoMix2, a fully non-autoregressive framework for zero-shot multi-talker dialogue generation. CoVoMix2 directly predicts mel-spectrograms from multi-stream transcriptions using a flow-matching-based generative model, eliminating the reliance on intermediate token representations. To better capture realistic conversational dynamics, we propose transcription-level speaker disentanglement, sentence-level alignment, and prompt-level random masking strategies. Our approach achieves state-of-the-art performance, outperforming strong baselines like MoonCast and Sesame in speech quality, speaker consistency, and inference speed. Notably, CoVoMix2 operates without requiring transcriptions for the prompt and supports controllable dialogue generation, including overlapping speech and precise timing control, demonstrating strong generalizability to real-world speech generation scenarios.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General agents contain world models</title>
<link>https://arxiv.org/abs/2506.01622</link>
<guid>https://arxiv.org/abs/2506.01622</guid>
<content:encoded><![CDATA[
arXiv:2506.01622v5 Announce Type: replace 
Abstract: Are world models a necessary ingredient for flexible, goal-directed behaviour, or is model-free learning sufficient? We provide a formal answer to this question, showing that any agent capable of generalizing to multi-step goal-directed tasks must have learned a predictive model of its environment. We show that this model can be extracted from the agent's policy, and that increasing the agents performance or the complexity of the goals it can achieve requires learning increasingly accurate world models. This has a number of consequences: from developing safe and general agents, to bounding agent capabilities in complex environments, and providing new algorithms for eliciting world models from agents.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>macOSWorld: A Multilingual Interactive Benchmark for GUI Agents</title>
<link>https://arxiv.org/abs/2506.04135</link>
<guid>https://arxiv.org/abs/2506.04135</guid>
<content:encoded><![CDATA[
arXiv:2506.04135v4 Announce Type: replace 
Abstract: Graphical User Interface (GUI) agents show promising capabilities for automating computer-use tasks and facilitating accessibility, but existing interactive benchmarks are mostly English-only, covering web-use or Windows, Linux, and Android environments, but not macOS. macOS is a major OS with distinctive GUI patterns and exclusive applications. To bridge the gaps, we present macOSWorld, the first comprehensive benchmark for evaluating GUI agents on macOS. macOSWorld features 202 multilingual interactive tasks across 30 applications (28 macOS-exclusive), with task instructions and OS interfaces offered in 5 languages (English, Chinese, Arabic, Japanese, and Russian). As GUI agents are shown to be vulnerable to deception attacks, macOSWorld also includes a dedicated safety benchmarking subset. Our evaluation on six GUI agents reveals a dramatic gap: proprietary computer-use agents lead at above 30% success rate, while open-source lightweight research models lag at below 5\%, highlighting the need for macOS domain adaptation. Multilingual benchmarks also expose common weaknesses, especially in Arabic, with a 28.8% average degradation compared to English. Results from safety benchmarking also highlight that deception attacks are more general and demand immediate attention. Project page: https://macos-world.github.io.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequence Modeling for N-Agent Ad Hoc Teamwork</title>
<link>https://arxiv.org/abs/2506.05527</link>
<guid>https://arxiv.org/abs/2506.05527</guid>
<content:encoded><![CDATA[
arXiv:2506.05527v3 Announce Type: replace 
Abstract: N-agent ad hoc teamwork (NAHT) is a newly introduced challenge in multi-agent reinforcement learning, where controlled subteams of varying sizes must dynamically collaborate with varying numbers and types of unknown teammates without pre-coordination. The existing learning algorithm (POAM) considers only independent learning for its flexibility in dealing with a changing number of agents. However, independent learning fails to fully capture the inter-agent dynamics essential for effective collaboration. Based on our observation that transformers deal effectively with sequences with varying lengths and have been shown to be highly effective for a variety of machine learning problems, this work introduces a centralized, transformer-based method for N-agent ad hoc teamwork. Our proposed approach incorporates historical observations and actions of all controlled agents, enabling optimal responses to diverse and unseen teammates in partially observable environments. Empirical evaluation on a StarCraft II task demonstrates that MAT-NAHT outperforms POAM, achieving superior sample efficiency and generalization, without auxiliary agent-modeling objectives.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Perception Logs to Failure Modes: Language-Driven Semantic Clustering of Failures for Robot Safety</title>
<link>https://arxiv.org/abs/2506.06570</link>
<guid>https://arxiv.org/abs/2506.06570</guid>
<content:encoded><![CDATA[
arXiv:2506.06570v2 Announce Type: replace 
Abstract: As robotic systems become increasingly integrated into real-world environments -- ranging from autonomous vehicles to household assistants -- they inevitably encounter diverse and unstructured scenarios that lead to failures. While such failures pose safety and reliability challenges, they also provide rich perceptual data for improving future performance. However, manually analyzing large-scale failure datasets is impractical. In this work, we present a method for automatically organizing large-scale robotic failure data into semantically meaningful failure clusters, enabling scalable learning from failure without human supervision. Our approach leverages the reasoning capabilities of Multimodal Large Language Models (MLLMs), trained on internet-scale data, to infer high-level failure causes from raw perceptual trajectories and discover interpretable structure within uncurated failure logs. These semantic clusters reveal patterns and hypothesized causes of failure, enabling scalable learning from experience. We demonstrate that the discovered failure modes can guide targeted data collection for policy refinement, accelerating iterative improvement in agent policies and overall safety. Additionally, we show that these semantic clusters can benefit online failure monitoring systems, offering a lightweight yet powerful safeguard for real-time operation. We demonstrate that this framework enhances robot learning and robustness by transforming real-world failures into actionable and interpretable signals for adaptation.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AGENTSAFE: Benchmarking the Safety of Embodied Agents on Hazardous Instructions</title>
<link>https://arxiv.org/abs/2506.14697</link>
<guid>https://arxiv.org/abs/2506.14697</guid>
<content:encoded><![CDATA[
arXiv:2506.14697v3 Announce Type: replace 
Abstract: The integration of vision-language models (VLMs) is driving a new generation of embodied agents capable of operating in human-centered environments. However, as deployment expands, these systems face growing safety risks, particularly when executing hazardous instructions. Current safety evaluation benchmarks remain limited: they cover only narrow scopes of hazards and focus primarily on final outcomes, neglecting the agent's full perception-planning-execution process and thereby obscuring critical failure modes. Therefore, we present SAFE, a benchmark for systematically assessing the safety of embodied VLM agents on hazardous instructions. SAFE comprises three components: SAFE-THOR, an extensible adversarial simulation sandbox with a universal adapter that maps high-level VLM outputs to low-level embodied controls, supporting diverse agent workflow integration; SAFE-VERSE, a risk-aware task suite inspired by Asimov's Three Laws of Robotics, comprising 45 adversarial scenarios, 1,350 hazardous tasks, and 9,900 instructions that span risks to humans, environments, and agents; and SAFE-DIAGNOSE, a multi-level and fine-grained evaluation protocol measuring agent performance across perception, planning, and execution. Applying SAFE to nine state-of-the-art VLMs and two embodied agent workflows, we uncover systematic failures in translating hazard recognition into safe planning and execution. Our findings reveal fundamental limitations in current safety alignment and demonstrate the necessity of a comprehensive, multi-stage evaluation for developing safer embodied intelligence.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeNIE: A Generalizable Navigation System for In-the-Wild Environments</title>
<link>https://arxiv.org/abs/2506.17960</link>
<guid>https://arxiv.org/abs/2506.17960</guid>
<content:encoded><![CDATA[
arXiv:2506.17960v2 Announce Type: replace 
Abstract: Reliable navigation in unstructured, real-world environments remains a significant challenge for embodied agents, especially when operating across diverse terrains, weather conditions, and sensor configurations. In this paper, we introduce GeNIE (Generalizable Navigation System for In-the-Wild Environments), a robust navigation framework designed for global deployment. GeNIE integrates a generalizable traversability prediction model built on SAM2 with a novel path fusion strategy that enhances planning stability in noisy and ambiguous settings. We deployed GeNIE in the Earth Rover Challenge (ERC) at ICRA 2025, where it was evaluated across six countries spanning three continents. GeNIE took first place and achieved 79% of the maximum possible score, outperforming the second-best team by 17%, and completed the entire competition without a single human intervention. These results set a new benchmark for robust, generalizable outdoor robot navigation. We will release the codebase, pretrained model weights, and newly curated datasets to support future research in real-world navigation.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Trust Generative Agents to Mimic Communication on Social Networks Unless You Benchmarked their Empirical Realism</title>
<link>https://arxiv.org/abs/2506.21974</link>
<guid>https://arxiv.org/abs/2506.21974</guid>
<content:encoded><![CDATA[
arXiv:2506.21974v3 Announce Type: replace 
Abstract: The ability of Large Language Models (LLMs) to mimic human behavior triggered a plethora of computational social science research, assuming that empirical studies of humans can be conducted with AI agents instead. Since there have been conflicting research findings on whether and when this hypothesis holds, there is a need to better understand the differences in their experimental designs. We focus on replicating the behavior of social network users with the use of LLMs for the analysis of communication on social networks. First, we provide a formal framework for the simulation of social networks, before focusing on the sub-task of imitating user communication. We empirically test different approaches to imitate user behavior on X in English and German. Our findings suggest that social simulations should be validated by their empirical realism measured in the setting in which the simulation components were fitted. With this paper, we argue for more rigor when applying generative-agent-based modeling for social simulation.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CooT: Learning to Coordinate In-Context with Coordination Transformers</title>
<link>https://arxiv.org/abs/2506.23549</link>
<guid>https://arxiv.org/abs/2506.23549</guid>
<content:encoded><![CDATA[
arXiv:2506.23549v2 Announce Type: replace 
Abstract: Effective coordination among artificial agents in dynamic and uncertain environments remains a significant challenge in multi-agent systems. Existing approaches, such as self-play and population-based methods, either generalize poorly to unseen partners or require impractically extensive fine-tuning. To overcome these limitations, we propose Coordination Transformers (\coot), a novel in-context coordination framework that uses recent interaction histories to rapidly adapt to unseen partners. Unlike prior approaches that primarily aim to diversify training partners, \coot explicitly focuses on adapting to new partner behaviors by predicting actions aligned with observed interactions. Trained on trajectories collected from diverse pairs of agents with complementary preferences, \coot quickly learns effective coordination strategies without explicit supervision or parameter updates. Across diverse coordination tasks in Overcooked, \coot consistently outperforms baselines including population-based approaches, gradient-based fine-tuning, and a Meta-RL-inspired contextual adaptation method. Notably, fine-tuning proves unstable and ineffective, while Meta-RL struggles to achieve reliable coordination. By contrast, \coot achieves stable, rapid in-context adaptation and is consistently ranked the most effective collaborator in human evaluations.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning</title>
<link>https://arxiv.org/abs/2507.00432</link>
<guid>https://arxiv.org/abs/2507.00432</guid>
<content:encoded><![CDATA[
arXiv:2507.00432v2 Announce Type: replace 
Abstract: Math reasoning has become the poster child of progress in large language models (LLMs), with new models rapidly surpassing human-level performance on benchmarks like MATH and AIME. But as math leaderboards improve week by week, it is worth asking: do these gains reflect broader problem-solving ability or just narrow overfitting? To answer this question, we evaluate over 20 open-weight reasoning-tuned models across a broad suite of tasks, including math, scientific QA, agent planning, coding, and standard instruction-following. We surprisingly find that most models that succeed in math fail to transfer their gains to other domains. To rigorously study this phenomenon, we conduct controlled experiments on Qwen3-14B models using math-only data but different tuning methods. We find that reinforcement learning (RL)-tuned models generalize well across domains, while supervised fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space representation and token-space distribution shift analyses reveal that SFT induces substantial representation and output drift, while RL preserves general-domain structure. Our results suggest a need to rethink standard post-training recipes, particularly the reliance on SFT-distilled data for advancing reasoning models.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Value-Based Large Language Model Agent Simulation for Mutual Evaluation of Trust and Interpersonal Closeness</title>
<link>https://arxiv.org/abs/2507.11979</link>
<guid>https://arxiv.org/abs/2507.11979</guid>
<content:encoded><![CDATA[
arXiv:2507.11979v2 Announce Type: replace 
Abstract: Large language models (LLMs) have emerged as powerful tools for simulating complex social phenomena using human-like agents with specific traits. In human societies, value similarity is important for building trust and close relationships; however, it remains unexplored whether this principle holds true in artificial societies comprising LLM agents. Therefore, this study investigates the influence of value similarity on relationship-building among LLM agents through two experiments. First, in a preliminary experiment, we evaluated the controllability of values in LLMs to identify the most effective model and prompt design for controlling the values. Subsequently, in the main experiment, we generated pairs of LLM agents imbued with specific values and analyzed their mutual evaluations of trust and interpersonal closeness following a dialogue. The experiments were conducted in English and Japanese to investigate language dependence. The results confirmed that pairs of agents with higher value similarity exhibited greater mutual trust and interpersonal closeness. Our findings demonstrate that the LLM agent simulation serves as a valid testbed for social science theories, contributes to elucidating the mechanisms by which values influence relationship building, and provides a foundation for inspiring new theories and insights into the social sciences.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinResearchBench: A Logic Tree based Agent-as-a-Judge Evaluation Framework for Financial Research Agents</title>
<link>https://arxiv.org/abs/2507.16248</link>
<guid>https://arxiv.org/abs/2507.16248</guid>
<content:encoded><![CDATA[
arXiv:2507.16248v3 Announce Type: replace 
Abstract: Recently, AI agents are rapidly evolving in intelligence and widely used in professional research applications, such as STEM, software development, and finance. Among these AI agents, deep research agent is a key category as it can perform long-horizon tasks and solve problems of greater complexity. However, there are few evaluation frameworks and benchmarks that systematically and automatically investigate the capabilities of these research agents. In addition, financial research problems have distinct complexity and subtlety. To fill in the gap, we propose FinResearchBench, which is a logic tree-based Agent-as-a-Judge and targets specifically for the financial research agents. It provides a comprehensive and automatic assessment of the research agents across 7 key types of tasks in the financial research domain. The contributions of this work are two-folded: (1) the first and innovative Agent-as-a-Judge system that extracts the logic tree of the research outcome and uses it as the intermediate information to present a comprehensive, reliable, and robust evaluation; (2) finance-oriented that it covers 70 typical financial research questions, spreading across 7 frequently encountered types of task in the domain.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COMPASS: Cooperative Multi-Agent Persistent Monitoring using Spatio-Temporal Attention Network</title>
<link>https://arxiv.org/abs/2507.16306</link>
<guid>https://arxiv.org/abs/2507.16306</guid>
<content:encoded><![CDATA[
arXiv:2507.16306v2 Announce Type: replace 
Abstract: Persistent monitoring of dynamic targets is essential in real-world applications such as disaster response, environmental sensing, and wildlife conservation, where mobile agents must continuously gather information under uncertainty. We propose COMPASS, a multi-agent reinforcement learning (MARL) framework that enables decentralized agents to persistently monitor multiple moving targets efficiently. We model the environment as a graph, where nodes represent spatial locations and edges capture topological proximity, allowing agents to reason over structured layouts and revisit informative regions as needed. Each agent independently selects actions based on a shared spatio-temporal attention network that we design to integrate historical observations and spatial context. We model target dynamics using Gaussian Processes (GPs), which support principled belief updates and enable uncertainty-aware planning. We train COMPASS using centralized value estimation and decentralized policy execution under an adaptive reward setting. Our extensive experiments demonstrate that COMPASS consistently outperforms strong baselines in uncertainty reduction, target coverage, and coordination efficiency across dynamic multi-target scenarios.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScreenCoder: Advancing Visual-to-Code Generation for Front-End Automation via Modular Multimodal Agents</title>
<link>https://arxiv.org/abs/2507.22827</link>
<guid>https://arxiv.org/abs/2507.22827</guid>
<content:encoded><![CDATA[
arXiv:2507.22827v2 Announce Type: replace 
Abstract: Automating the transformation of user interface (UI) designs into front-end code holds significant promise for accelerating software development and democratizing design workflows. While multimodal large language models (MLLMs) can translate images to code, they often fail on complex UIs, struggling to unify visual perception, layout planning, and code synthesis within a single monolithic model, which leads to frequent perception and planning errors. To address this, we propose ScreenCoder, a modular multi-agent framework that decomposes the task into three interpretable stages: grounding, planning, and generation. By assigning these distinct responsibilities to specialized agents, our framework achieves significantly higher robustness and fidelity than end-to-end approaches. Furthermore, ScreenCoder serves as a scalable data engine, enabling us to generate high-quality image-code pairs. We use this data to fine-tune open-source MLLM via a dual-stage pipeline of supervised fine-tuning and reinforcement learning, demonstrating substantial gains in its UI generation capabilities. Extensive experiments demonstrate that our approach achieves state-of-the-art performance in layout accuracy, structural coherence, and code correctness. Our code is made publicly available at https://github.com/leigest519/ScreenCoder.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SketchMind: A Multi-Agent Cognitive Framework for Assessing Student-Drawn Scientific Sketches</title>
<link>https://arxiv.org/abs/2507.22904</link>
<guid>https://arxiv.org/abs/2507.22904</guid>
<content:encoded><![CDATA[
arXiv:2507.22904v2 Announce Type: replace 
Abstract: Scientific sketches (e.g., models) offer a powerful lens into students' conceptual understanding, yet AI-powered automated assessment of such free-form, visually diverse artifacts remains a critical challenge. Existing solutions often treat sketch evaluation as either an image classification task or monolithic vision-language models, which lack interpretability, pedagogical alignment, and adaptability across cognitive levels. To address these limitations, we present SketchMind, a cognitively grounded, multi-agent framework for evaluating and improving student-drawn scientific sketches. SketchMind comprises modular agents responsible for rubric parsing, sketch perception, cognitive alignment, and iterative feedback with sketch modification, enabling personalized and transparent evaluation. We evaluate SketchMind on a curated dataset of 3,575 student-generated sketches across six science assessment items with different highest order of Bloom's level that require students to draw models to explain phenomena. Compared to baseline GPT-4o performance without SRG (average accuracy: 55.6%), and with SRG integration achieves 77.1% average accuracy (+21.4% average absolute gain). We also demonstrate that multi-agent orchestration with SRG enhances SketchMind performance, for example, GPT-4.1 gains an average 8.9% increase in sketch prediction accuracy, outperforming single-agent pipelines across all items. Human evaluators rated the feedback and co-created sketches generated by \textsc{SketchMind} with GPT-4.1, which achieved an average of 4.1 out of 5, significantly higher than those of baseline models (e.g., 2.3 for GPT-4o). Experts noted the system's potential to meaningfully support conceptual growth through guided revision. Our code and (pending approval) dataset will be released to support reproducibility and future research in AI-driven education.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network</title>
<link>https://arxiv.org/abs/2508.00429</link>
<guid>https://arxiv.org/abs/2508.00429</guid>
<content:encoded><![CDATA[
arXiv:2508.00429v4 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in graph-based learning by propagating information among neighbor nodes via predefined aggregation mechanisms. However, such fixed schemes often suffer from two key limitations. First, they cannot handle the imbalance in node informativeness -- some nodes are rich in information, while others remain sparse. Second, predefined message passing primarily leverages local structural similarity while ignoring global semantic relationships across the graph, limiting the model's ability to capture distant but relevant information. We propose Retrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework that empowers each node with autonomous, node-level decision-making. Each node acts as an agent that independently plans its next action based on its internal memory, enabling node-level planning and adaptive message propagation. Additionally, retrieval-augmented generation (RAG) allows nodes to access semantically relevant content and build global relationships in the graph. ReaGAN achieves competitive performance under few-shot in-context settings using a frozen LLM backbone without fine-tuning, showcasing the potential of agentic planning and local-global retrieval in graph learning.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications</title>
<link>https://arxiv.org/abs/2508.01710</link>
<guid>https://arxiv.org/abs/2508.01710</guid>
<content:encoded><![CDATA[
arXiv:2508.01710v3 Announce Type: replace 
Abstract: The increasing use of Large Language Models (LLMs) in agentic applications highlights the need for robust safety guard models. While content safety in English is well-studied, non-English languages lack similar advancements due to the high cost of collecting culturally aligned labeled datasets. We present CultureGuard, a novel solution for curating culturally aligned, high-quality safety datasets across multiple languages. Our approach introduces a four-stage synthetic data generation and filtering pipeline: cultural data segregation, cultural data adaptation, machine translation, and quality filtering. This pipeline enables the conversion and expansion of the Nemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct languages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese. The resulting dataset, Nemotron-Safety-Guard-Dataset-v3, comprises 386,661 samples in 9 languages and facilitates the training of Llama-3.1-Nemotron-Safety-Guard-8B-v3 via LoRA-based fine-tuning. The final model achieves state-of-the-art performance on several multilingual content safety benchmarks. Furthermore, we show our moderately multilingual fine-tuning enables robust cross-lingual transfer and strong zero-shot generalization to unseen languages. We also benchmark the latest open LLMs on multilingual safety and observe that these LLMs are more prone to give unsafe responses when prompted in non-English languages. This work advances multilingual LLM safety by enabling the development of culturally aware safety guard models.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Collaboration With Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.04652</link>
<guid>https://arxiv.org/abs/2508.04652</guid>
<content:encoded><![CDATA[
arXiv:2508.04652v2 Announce Type: replace 
Abstract: A large amount of work has been done in Multi-Agent Systems (MAS) for modeling and solving problems with multiple interacting agents. However, most LLMs are pretrained independently and not specifically optimized for coordination. Existing LLM fine-tuning frameworks rely on individual rewards, which require complex reward designs for each agent to encourage collaboration. To address these challenges, we model LLM collaboration as a cooperative Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent, multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO), to solve it, building on current RL approaches for LLMs as well as MARL techniques. Our experiments on LLM writing and coding collaboration demonstrate that fine-tuning MAS with MAGRPO enables agents to generate high-quality responses efficiently through effective cooperation. Our approach opens the door to using other MARL methods for LLMs and highlights the associated challenges.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery</title>
<link>https://arxiv.org/abs/2508.14111</link>
<guid>https://arxiv.org/abs/2508.14111</guid>
<content:encoded><![CDATA[
arXiv:2508.14111v2 Announce Type: replace 
Abstract: Artificial intelligence (AI) is reshaping scientific discovery, evolving from specialized computational tools into autonomous research partners. We position Agentic Science as a pivotal stage within the broader AI for Science paradigm, where AI systems progress from partial assistance to full scientific agency. Enabled by large language models (LLMs), multimodal systems, and integrated research platforms, agentic AI shows capabilities in hypothesis generation, experimental design, execution, analysis, and iterative refinement -- behaviors once regarded as uniquely human. This survey provides a domain-oriented review of autonomous scientific discovery across life sciences, chemistry, materials science, and physics. We unify three previously fragmented perspectives -- process-oriented, autonomy-oriented, and mechanism-oriented -- through a comprehensive framework that connects foundational capabilities, core processes, and domain-specific realizations. Building on this framework, we (i) trace the evolution of AI for Science, (ii) identify five core capabilities underpinning scientific agency, (iii) model discovery as a dynamic four-stage workflow, (iv) review applications across the above domains, and (v) synthesize key challenges and future opportunities. This work establishes a domain-oriented synthesis of autonomous scientific discovery and positions Agentic Science as a structured paradigm for advancing AI-driven research.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers</title>
<link>https://arxiv.org/abs/2508.21148</link>
<guid>https://arxiv.org/abs/2508.21148</guid>
<content:encoded><![CDATA[
arXiv:2508.21148v2 Announce Type: replace 
Abstract: Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is represented, integrated, and applied in scientific research, yet their progress is shaped by the complex nature of scientific data. This survey presents a comprehensive, data-centric synthesis that reframes the development of Sci-LLMs as a co-evolution between models and their underlying data substrate. We formulate a unified taxonomy of scientific data and a hierarchical model of scientific knowledge, emphasizing the multimodal, cross-scale, and domain-specific challenges that differentiate scientific corpora from general natural language processing datasets. We systematically review recent Sci-LLMs, from general-purpose foundations to specialized models across diverse scientific disciplines, alongside an extensive analysis of over 270 pre-/post-training datasets, showing why Sci-LLMs pose distinct demands -- heterogeneous, multi-scale, uncertainty-laden corpora that require representations preserving domain invariance and enabling cross-modal reasoning. On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols. These data-centric analyses highlight persistent issues in scientific data development and discuss emerging solutions involving semi-automated annotation pipelines and expert validation. Finally, we outline a paradigm shift toward closed-loop systems where autonomous agents based on Sci-LLMs actively experiment, validate, and contribute to a living, evolving knowledge base. Collectively, this work provides a roadmap for building trustworthy, continually evolving artificial intelligence (AI) systems that function as a true partner in accelerating scientific discovery.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BED-LLM: Intelligent Information Gathering with LLMs and Bayesian Experimental Design</title>
<link>https://arxiv.org/abs/2508.21184</link>
<guid>https://arxiv.org/abs/2508.21184</guid>
<content:encoded><![CDATA[
arXiv:2508.21184v2 Announce Type: replace 
Abstract: We propose a general-purpose approach for improving the ability of Large Language Models (LLMs) to intelligently and adaptively gather information from a user or other external source using the framework of sequential Bayesian experimental design (BED). This enables LLMs to act as effective multi-turn conversational agents and interactively interface with external environments. Our approach, which we call BED-LLM (Bayesian Experimental Design with Large Language Models), is based on iteratively choosing questions or queries that maximize the expected information gain (EIG) about the task of interest given the responses gathered previously. We show how this EIG can be formulated (and then estimated) in a principled way using a probabilistic model derived from the LLM's predictive distributions and provide detailed insights into key decisions in its construction and updating procedure. We find that BED-LLM achieves substantial gains in performance across a wide range of tests based on the 20 questions game and using the LLM to actively infer user preferences, compared to direct prompting of the LLM and other adaptive design strategies.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic System with Modal Logic for Autonomous Diagnostics</title>
<link>https://arxiv.org/abs/2509.11943</link>
<guid>https://arxiv.org/abs/2509.11943</guid>
<content:encoded><![CDATA[
arXiv:2509.11943v3 Announce Type: replace 
Abstract: The development of intelligent agents, particularly those powered by language models (LMs), has shown a critical role in various environments that require intelligent and autonomous decision-making. Environments are not passive testing grounds, and they represent the data required for agents to learn and exhibit in very challenging conditions that require adaptive, complex, and autonomous capacity to make decisions. While the paradigm of scaling models and datasets has led to remarkable emergent capabilities, we argue that scaling the structure, fidelity, and logical consistency of agent reasoning within these environments is a crucial, yet underexplored, dimension of AI research. This paper introduces a neuro-symbolic multi-agent architecture where the belief states of individual agents are formally represented as Kripke models. This foundational choice enables them to reason about known concepts of \emph{possibility} and \emph{necessity} using the formal language of modal logic. In this work, we use immutable, domain-specific knowledge to make an informed root cause diagnosis, which is encoded as logical constraints essential for proper, reliable, and explainable diagnosis. In the proposed model, we show constraints that actively guide the hypothesis generation of LMs, effectively preventing them from reaching physically or logically untenable conclusions. In a high-fidelity simulated particle accelerator environment, our system successfully diagnoses complex, cascading failures by combining the powerful semantic intuition of LMs with the rigorous, verifiable validation of modal logic and a factual world model and showcasing a viable path toward more robust, reliable, and verifiable autonomous agents.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Programmable Cognitive Bias in Social Agents</title>
<link>https://arxiv.org/abs/2509.13588</link>
<guid>https://arxiv.org/abs/2509.13588</guid>
<content:encoded><![CDATA[
arXiv:2509.13588v2 Announce Type: replace 
Abstract: This paper introduces CoBRA, a novel toolkit for systematically specifying agent behavior in LLM-based social simulation. We found that conventional approaches that specify agent behaviors through implicit natural language descriptions cannot yield consistent behaviors across models, and the produced agent behaviors do not capture the nuances of the descriptions. In contrast, CoBRA presents a new approach to program agents' cognitive biases explicitly, by grounding agents' expected behaviors using classic social science experiments. CoBRA has two components: (1) Cognitive Bias Index that measures the cognitive bias of a social agent, by quantifying the agent's reactions in a set of validated classical social science experiments; (2) Behavioral Regulation Engine that aligns the agent's behavior to demonstrate controlled cognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and technical benchmarks. Our results suggest that CoBRA can precisely program the cognitive bias demonstrated in a social agent in a model-agnostic manner.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation</title>
<link>https://arxiv.org/abs/2509.16198</link>
<guid>https://arxiv.org/abs/2509.16198</guid>
<content:encoded><![CDATA[
arXiv:2509.16198v5 Announce Type: replace 
Abstract: Large language models excel at generating individual functions or single files of code, yet generating complete repositories from scratch remains a fundamental challenge. This capability is key to building coherent software systems from high-level specifications and realizing the full potential of automated code generation. The process requires planning at two levels: deciding what features and modules to build (proposal stage) and defining their implementation details (implementation stage). Current approaches rely on natural language planning, which often produces unclear specifications, misaligned components, and brittle designs due to its inherent ambiguity and lack of structure. To address these limitations, we introduce the Repository Planning Graph (RPG), a structured representation that encodes capabilities, file structures, data flows, and functions in a unified graph. By replacing free-form natural language with an explicit blueprint, RPG enables consistent long-horizon planning for repository generation. Building on RPG, we develop ZeroRepo, a graph-driven framework that operates in three stages: proposal-level planning, implementation-level construction, and graph-guided code generation with test validation. To evaluate, we construct RepoCraft, a benchmark of six real-world projects with 1,052 tasks. On RepoCraft, ZeroRepo produces nearly 36K Code Lines and 445K Code Tokens, on average 3.9$\times$ larger than the strongest baseline (Claude Code), and 68$\times$ larger than other baselines. It achieves 81.5% coverage and 69.7% test accuracy, improving over Claude Code by 27.3 and 35.8 points. Further analysis shows that RPG models complex dependencies, enables more sophisticated planning through near-linear scaling, and improves agent understanding of repositories, thus accelerating localization.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Free Dynamic Consensus in Multi-Agent Systems: A Q-Function Perspective</title>
<link>https://arxiv.org/abs/2509.24598</link>
<guid>https://arxiv.org/abs/2509.24598</guid>
<content:encoded><![CDATA[
arXiv:2509.24598v2 Announce Type: replace 
Abstract: This paper presents a new method for achieving dynamic consensus in linear discrete-time homogeneous multi-agent systems (MAS) with marginally stable or unstable dynamics. The guarantee of consensus in this setting involves a set of constraints based on the graph's spectral properties, complicating the design of the coupling gains. This challenge intensifies for large-scale systems with diverse graph Laplacian spectra. The proposed approach reformulates the dynamic consensus problem with a prescribed convergence rate using a state-action value function framework inspired by optimal control theory. Specifically, a synthetic linear quadratic regulation (LQR) formulation is introduced to encode the consensus objective, enabling its translation into a convex semidefinite programming (SDP) problem. The resulting SDP is applicable in both model-based and model-free settings for jointly designing the local feedback and coupling gains. To handle the inherent non-convex feasibility conditions, a convex-concave decomposition strategy is employed. Adaptation of the method in a completely model-free set-up eliminates the need for system identification or knowledge of the agents' dynamics. Instead, it relies on input-state data collection and offers an entirely data-driven equivalent SDP formulation. Finally, a new algorithm balancing feasibility, convergence rate, robustness, and energy efficiency, is established to provide design flexibility. Numerical simulations validate the method's effectiveness in various scenarios.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DualTune: Decoupled Fine-Tuning for On-Device Agentic Systems</title>
<link>https://arxiv.org/abs/2510.00229</link>
<guid>https://arxiv.org/abs/2510.00229</guid>
<content:encoded><![CDATA[
arXiv:2510.00229v2 Announce Type: replace 
Abstract: The deployment of Large Language Models (LLMs) as agentic orchestrators has revolutionized task automation, but the need for privacy-preserving, cost-effective solutions demands on-device inference capabilities. However, local LLMs consistently underperform compared to frontier models in tool calling scenarios, struggling with both tool selection from large tool sets and accurate argument generation for complex parameter structures. We introduce a methodology that disaggregates a tool-calling task into two distinct subtasks: tool selection and argument generation. We propose "decoupled fine-tuning", a novel post-training approach that employs LoRA fine-tuning to create dedicated LoRA adapters for tool selection and tool-specific argument generation using separate loss masking for each of the subtasks. Furthermore, we present DualTune, an inference framework that leverages the LoRA adapters created using decoupled fine-tuning to perform efficient agent orchestration with the help of local models on end-user devices. DualTune decomposes the tool-call generation step into tool selection and argument generation, and dynamically loads the corresponding LoRA adapters to generate tool calls. Additionally, DualTune implements hierarchical orchestration to restrict the number of tools required for tool selection. Our experiments on the MCP-Bench benchmark demonstrate that the Qwen-2.5-7B model trained using decoupled fine-tuning improves the tool calling accuracy of the base model by 46%, and outperforms other local reasoning, non-reasoning and fine-tuned models of similar size in all cases, and models that are 2x larger, in most cases.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When LLM Agents Meet Graph Optimization: An Automated Data Quality Improvement Approach</title>
<link>https://arxiv.org/abs/2510.08952</link>
<guid>https://arxiv.org/abs/2510.08952</guid>
<content:encoded><![CDATA[
arXiv:2510.08952v2 Announce Type: replace 
Abstract: Text-attributed graphs (TAGs) have become a key form of graph-structured data in modern data management and analytics, combining structural relationships with rich textual semantics for diverse applications. However, the effectiveness of analytical models, particularly graph neural networks (GNNs), is highly sensitive to data quality. Our empirical analysis shows that both conventional and LLM-enhanced GNNs degrade notably under textual, structural, and label imperfections, underscoring TAG quality as a key bottleneck for reliable analytics. Existing studies have explored data-level optimization for TAGs, but most focus on specific degradation types and target a single aspect like structure or label, lacking a systematic and comprehensive perspective on data quality improvement. To address this gap, we propose LAGA (Large Language and Graph Agent), a unified multi-agent framework for comprehensive TAG quality optimization. LAGA formulates graph quality control as a data-centric process, integrating detection, planning, action, and evaluation agents into an automated loop. It holistically enhances textual, structural, and label aspects through coordinated multi-modal optimization. Extensive experiments on 5 datasets and 16 baselines across 9 scenarios demonstrate the effectiveness, robustness and scalability of LAGA, confirming the importance of data-centric quality optimization for reliable TAG analytics.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StarWhisper Telescope: An AI framework for automating end-to-end astronomical observations</title>
<link>https://arxiv.org/abs/2412.06412</link>
<guid>https://arxiv.org/abs/2412.06412</guid>
<content:encoded><![CDATA[
arXiv:2412.06412v3 Announce Type: replace-cross 
Abstract: The exponential growth of large-scale telescope arrays has boosted time-domain astronomy development but introduced operational bottlenecks, including labor-intensive observation planning, data processing, and real-time decision-making. Here we present the StarWhisper Telescope system, an AI agent framework automating end-to-end astronomical observations for surveys like the Nearby Galaxy Supernovae Survey. By integrating large language models with specialized function calls and modular workflows, StarWhisper Telescope autonomously generates site-specific observation lists, executes real-time image analysis via pipelines, and dynamically triggers follow-up proposals upon transient detection. The system reduces human intervention through automated observation planning, telescope controlling and data processing, while enabling seamless collaboration between amateur and professional astronomers. Deployed across Nearby Galaxy Supernovae Survey's network of 10 amateur telescopes, the StarWhisper Telescope has detected transients with promising response times relative to existing surveys. Furthermore, StarWhisper Telescope's scalable agent architecture provides a blueprint for future facilities like the Global Open Transient Telescope Array, where AI-driven autonomy will be critical for managing 60 telescopes.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Individual Learning to Market Equilibrium: Correcting Structural and Parametric Biases in RL Simulations of Economic Models</title>
<link>https://arxiv.org/abs/2507.18229</link>
<guid>https://arxiv.org/abs/2507.18229</guid>
<content:encoded><![CDATA[
arXiv:2507.18229v2 Announce Type: replace-cross 
Abstract: The application of Reinforcement Learning (RL) to economic modeling reveals a fundamental conflict between the assumptions of equilibrium theory and the emergent behavior of learning agents. While canonical economic models assume atomistic agents act as `takers' of aggregate market conditions, a naive single-agent RL simulation incentivizes the agent to become a `manipulator' of its environment. This paper first demonstrates this discrepancy within a search-and-matching model with concave production, showing that a standard RL agent learns a non-equilibrium, monopsonistic policy. Additionally, we identify a parametric bias arising from the mismatch between economic discounting and RL's treatment of intertemporal costs. To address both issues, we propose a calibrated Mean-Field Reinforcement Learning framework that embeds a representative agent in a fixed macroeconomic field and adjusts the cost function to reflect economic opportunity costs. Our iterative algorithm converges to a self-consistent fixed point where the agent's policy aligns with the competitive equilibrium. This approach provides a tractable and theoretically sound methodology for modeling learning agents in economic systems within the broader domain of computational social science.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ES-C51: Expected Sarsa Based C51 Distributional Reinforcement Learning Algorithm</title>
<link>https://arxiv.org/abs/2510.15006</link>
<guid>https://arxiv.org/abs/2510.15006</guid>
<content:encoded><![CDATA[
arXiv:2510.15006v1 Announce Type: new 
Abstract: In most value-based reinforcement learning (RL) algorithms, the agent estimates only the expected reward for each action and selects the action with the highest reward. In contrast, Distributional Reinforcement Learning (DRL) estimates the entire probability distribution of possible rewards, providing richer information about uncertainty and variability. C51 is a popular DRL algorithm for discrete action spaces. It uses a Q-learning approach, where the distribution is learned using a greedy Bellman update. However, this can cause problems if multiple actions at a state have similar expected reward but with different distributions, as the algorithm may not learn a stable distribution. This study presents a modified version of C51 (ES-C51) that replaces the greedy Q-learning update with an Expected Sarsa update, which uses a softmax calculation to combine information from all possible actions at a state rather than relying on a single best action. This reduces instability when actions have similar expected rewards and allows the agent to learn higher-performing policies. This approach is evaluated on classic control environments from Gym, and Atari-10 games. For a fair comparison, we modify the standard C51's exploration strategy from e-greedy to softmax, which we refer to as QL-C51 (Q- Learning based C51). The results demonstrate that ES-C51 outperforms QL-C51 across many environments.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UrbanVerse: Scaling Urban Simulation by Watching City-Tour Videos</title>
<link>https://arxiv.org/abs/2510.15018</link>
<guid>https://arxiv.org/abs/2510.15018</guid>
<content:encoded><![CDATA[
arXiv:2510.15018v1 Announce Type: new 
Abstract: Urban embodied AI agents, ranging from delivery robots to quadrupeds, are increasingly populating our cities, navigating chaotic streets to provide last-mile connectivity. Training such agents requires diverse, high-fidelity urban environments to scale, yet existing human-crafted or procedurally generated simulation scenes either lack scalability or fail to capture real-world complexity. We introduce UrbanVerse, a data-driven real-to-sim system that converts crowd-sourced city-tour videos into physics-aware, interactive simulation scenes. UrbanVerse consists of: (i) UrbanVerse-100K, a repository of 100k+ annotated urban 3D assets with semantic and physical attributes, and (ii) UrbanVerse-Gen, an automatic pipeline that extracts scene layouts from video and instantiates metric-scale 3D simulations using retrieved assets. Running in IsaacSim, UrbanVerse offers 160 high-quality constructed scenes from 24 countries, along with a curated benchmark of 10 artist-designed test scenes. Experiments show that UrbanVerse scenes preserve real-world semantics and layouts, achieving human-evaluated realism comparable to manually crafted scenes. In urban navigation, policies trained in UrbanVerse exhibit scaling power laws and strong generalization, improving success by +6.3% in simulation and +30.1% in zero-shot sim-to-real transfer comparing to prior methods, accomplishing a 300 m real-world mission with only two interventions.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Dynamics Generation towards Scannable Physical World Model</title>
<link>https://arxiv.org/abs/2510.15041</link>
<guid>https://arxiv.org/abs/2510.15041</guid>
<content:encoded><![CDATA[
arXiv:2510.15041v1 Announce Type: new 
Abstract: Digital twin worlds with realistic interactive dynamics presents a new opportunity to develop generalist embodied agents in scannable environments with complex physical behaviors. To this end, we present GDGen (Generalized Representation for Generalized Dynamics Generation), a framework that takes a potential energy perspective to seamlessly integrate rigid body, articulated body, and soft body dynamics into a unified, geometry-agnostic system. GDGen operates from the governing principle that the potential energy for any stable physical system should be low. This fresh perspective allows us to treat the world as one holistic entity and infer underlying physical properties from simple motion observations. We extend classic elastodynamics by introducing directional stiffness to capture a broad spectrum of physical behaviors, covering soft elastic, articulated, and rigid body systems. We propose a specialized network to model the extended material property and employ a neural field to represent deformation in a geometry-agnostic manner. Extensive experiments demonstrate that GDGen robustly unifies diverse simulation paradigms, offering a versatile foundation for creating interactive virtual environments and training robotic agents in complex, dynamically rich scenarios.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Internalizing World Models via Self-Play Finetuning for Agentic RL</title>
<link>https://arxiv.org/abs/2510.15047</link>
<guid>https://arxiv.org/abs/2510.15047</guid>
<content:encoded><![CDATA[
arXiv:2510.15047v1 Announce Type: new 
Abstract: Large Language Models (LLMs) as agents often struggle in out-of-distribution (OOD) scenarios. Real-world environments are complex and dynamic, governed by task-specific rules and stochasticity, which makes it difficult for LLMs to ground their internal knowledge in those dynamics. Under such OOD conditions, vanilla RL training often fails to scale; we observe Pass@k--the probability that at least one of (k) sampled trajectories succeeds--drops markedly across training steps, indicating brittle exploration and limited generalization. Inspired by model-based reinforcement learning, we hypothesize that equipping LLM agents with an internal world model can better align reasoning with environmental dynamics and improve decision-making. We show how to encode this world model by decomposing it into two components: state representation and transition modeling. Building on this, we introduce SPA, a simple reinforcement learning framework that cold-starts the policy via a Self-Play supervised finetuning (SFT) stage to learn the world model by interacting with the environment, then uses it to simulate future states prior to policy optimization. This simple initialization outperforms the online world-modeling baseline and greatly boosts the RL-based agent training performance. Experiments across diverse environments like Sokoban, FrozenLake, and Sudoku show that our approach significantly improves performance. For example, SPA boosts the Sokoban success rate from 25.6% to 59.8% and raises the FrozenLake score from 22.1% to 70.9% for the Qwen2.5-1.5B-Instruct model.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn to Change the World: Multi-level Reinforcement Learning with Model-Changing Actions</title>
<link>https://arxiv.org/abs/2510.15056</link>
<guid>https://arxiv.org/abs/2510.15056</guid>
<content:encoded><![CDATA[
arXiv:2510.15056v1 Announce Type: new 
Abstract: Reinforcement learning usually assumes a given or sometimes even fixed environment in which an agent seeks an optimal policy to maximize its long-term discounted reward. In contrast, we consider agents that are not limited to passive adaptations: they instead have model-changing actions that actively modify the RL model of world dynamics itself. Reconfiguring the underlying transition processes can potentially increase the agents' rewards. Motivated by this setting, we introduce the multi-layer configurable time-varying Markov decision process (MCTVMDP). In an MCTVMDP, the lower-level MDP has a non-stationary transition function that is configurable through upper-level model-changing actions. The agent's objective consists of two parts: Optimize the configuration policies in the upper-level MDP and optimize the primitive action policies in the lower-level MDP to jointly improve its expected long-term reward.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combinatorial Contract Design: Recent Progress and Emerging Frontiers</title>
<link>https://arxiv.org/abs/2510.15065</link>
<guid>https://arxiv.org/abs/2510.15065</guid>
<content:encoded><![CDATA[
arXiv:2510.15065v1 Announce Type: new 
Abstract: Contract theory studies how a principal can incentivize agents to exert costly, unobservable effort through performance-based payments. While classical economic models provide elegant characterizations of optimal solutions, modern applications, ranging from online labor markets and healthcare to AI delegation and blockchain protocols, call for an algorithmic perspective. The challenge is no longer only which contracts induce desired behavior, but whether such contracts can be computed efficiently. This viewpoint has given rise to \emph{algorithmic contract design}, paralleling the rise of algorithmic mechanism design two decades ago.
  This article focuses on \emph{combinatorial contracts}, an emerging frontier within algorithmic contract design, where agents may choose among exponentially many combinations of actions, or where multiple agents must work together as a team, and the challenge lies in selecting the right composition. These models capture a wide variety of real-world contracting environments, from hospitals coordinating physicians across treatment protocols to firms hiring teams of engineers for interdependent tasks. We review three combinatorial settings: (i) a single agent choosing multiple actions, (ii) multiple agents with binary actions, and (iii) multiple agents each selecting multiple actions. For each, we highlight structural insights, algorithmic techniques, and complexity barriers. Results include tractable cases such as gross substitutes reward functions, hardness results, and approximation guarantees under value- and demand-oracle access. By charting these advances, the article maps the emerging landscape of combinatorial contract design, and highlights fundamental open questions and promising directions for future work.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Procedural Game Level Design with Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.15120</link>
<guid>https://arxiv.org/abs/2510.15120</guid>
<content:encoded><![CDATA[
arXiv:2510.15120v1 Announce Type: new 
Abstract: Procedural content generation (PCG) has become an increasingly popular technique in game development, allowing developers to generate dynamic, replayable, and scalable environments with reduced manual effort. In this study, a novel method for procedural level design using Deep Reinforcement Learning (DRL) within a Unity-based 3D environment is proposed. The system comprises two agents: a hummingbird agent, acting as a solver, and a floating island agent, responsible for generating and placing collectible objects (flowers) on the terrain in a realistic and context-aware manner. The hummingbird is trained using the Proximal Policy Optimization (PPO) algorithm from the Unity ML-Agents toolkit. It learns to navigate through the terrain efficiently, locate flowers, and collect them while adapting to the ever-changing procedural layout of the island. The island agent is also trained using the Proximal Policy Optimization (PPO) algorithm. It learns to generate flower layouts based on observed obstacle positions, the hummingbird's initial state, and performance feedback from previous episodes. The interaction between these agents leads to emergent behavior and robust generalization across various environmental configurations. The results demonstrate that the approach not only produces effective and efficient agent behavior but also opens up new opportunities for autonomous game level design driven by machine learning. This work highlights the potential of DRL in enabling intelligent agents to both generate and solve content in virtual environments, pushing the boundaries of what AI can contribute to creative game development processes.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Error Centric Intelligence I, Beyond Observational Learning</title>
<link>https://arxiv.org/abs/2510.15128</link>
<guid>https://arxiv.org/abs/2510.15128</guid>
<content:encoded><![CDATA[
arXiv:2510.15128v1 Announce Type: new 
Abstract: We argue that progress toward AGI is theory limited rather than data or scale limited. Building on the critical rationalism of Popper and Deutsch, we challenge the Platonic Representation Hypothesis. Observationally equivalent worlds can diverge under interventions, so observational adequacy alone cannot guarantee interventional competence. We begin by laying foundations, definitions of knowledge, learning, intelligence, counterfactual competence and AGI, and then analyze the limits of observational learning that motivate an error centric shift. We recast the problem as three questions about how explicit and implicit errors evolve under an agent's actions, which errors are unreachable within a fixed hypothesis space, and how conjecture and criticism expand that space. From these questions we propose Causal Mechanics, a mechanisms first program in which hypothesis space change is a first class operation and probabilistic structure is used when useful rather than presumed. We advance structural principles that make error discovery and correction tractable, including a differential Locality and Autonomy Principle for modular interventions, a gauge invariant form of Independent Causal Mechanisms for separability, and the Compositional Autonomy Principle for analogy preservation, together with actionable diagnostics. The aim is a scaffold for systems that can convert unreachable errors into reachable ones and correct them.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HugAgent: Evaluating LLMs in Simulating Human-Like Individual Reasoning on Open-Ended Tasks</title>
<link>https://arxiv.org/abs/2510.15144</link>
<guid>https://arxiv.org/abs/2510.15144</guid>
<content:encoded><![CDATA[
arXiv:2510.15144v1 Announce Type: new 
Abstract: Simulating human reasoning in open-ended tasks has been a long-standing aspiration in AI and cognitive science. While large language models now approximate human responses at scale, they remain tuned to population-level consensus, often erasing the individuality of reasoning styles and belief trajectories. To advance the vision of more human-like reasoning in machines, we introduce HugAgent (Human-Grounded Agent Benchmark), a benchmark for average-to-individual reasoning adaptation. The task is to predict how a specific person would reason and update their beliefs in novel scenarios, given partial evidence of their past views. HugAgent adopts a dual-track design: a synthetic track for scale and systematic stress tests, and a human track for ecologically valid, "out-loud" reasoning data. This design enables scalable, reproducible evaluation of intra-agent fidelity: whether models can capture not just what people believe, but how their reasoning evolves. Experiments with state-of-the-art LLMs reveal persistent adaptation gaps, positioning HugAgent as the first extensible benchmark for aligning machine reasoning with the individuality of human thought. Our benchmark and chatbot are open-sourced as HugAgent (https://anonymous.4open.science/r/HugAgent) and TraceYourThinking (https://anonymous.4open.science/r/trace-your-thinking).
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Transfer Ensures Fast Learning for Continuous-Time LQR with Entropy Regularization</title>
<link>https://arxiv.org/abs/2510.15165</link>
<guid>https://arxiv.org/abs/2510.15165</guid>
<content:encoded><![CDATA[
arXiv:2510.15165v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) enables agents to learn optimal decision-making strategies through interaction with an environment, yet training from scratch on complex tasks can be highly inefficient. Transfer learning (TL), widely successful in large language models (LLMs), offers a promising direction for enhancing RL efficiency by leveraging pre-trained models.
  This paper investigates policy transfer, a TL approach that initializes learning in a target RL task using a policy from a related source task, in the context of continuous-time linear quadratic regulators (LQRs) with entropy regularization. We provide the first theoretical proof of policy transfer for continuous-time RL, proving that a policy optimal for one LQR serves as a near-optimal initialization for closely related LQRs, while preserving the original algorithm's convergence rate. Furthermore, we introduce a novel policy learning algorithm for continuous-time LQRs that achieves global linear and local super-linear convergence. Our results demonstrate both theoretical guarantees and algorithmic benefits of transfer learning in continuous-time RL, addressing a gap in existing literature and extending prior work from discrete to continuous time settings.
  As a byproduct of our analysis, we derive the stability of a class of continuous-time score-based diffusion models via their connection with LQRs.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGPIE: A benchmark for Multi-AGent contextual PrIvacy Evaluation</title>
<link>https://arxiv.org/abs/2510.15186</link>
<guid>https://arxiv.org/abs/2510.15186</guid>
<content:encoded><![CDATA[
arXiv:2510.15186v1 Announce Type: new 
Abstract: A core challenge for autonomous LLM agents in collaborative settings is balancing robust privacy understanding and preservation alongside task efficacy. Existing privacy benchmarks only focus on simplistic, single-turn interactions where private information can be trivially omitted without affecting task outcomes. In this paper, we introduce MAGPIE (Multi-AGent contextual PrIvacy Evaluation), a novel benchmark of 200 high-stakes tasks designed to evaluate privacy understanding and preservation in multi-agent collaborative, non-adversarial scenarios. MAGPIE integrates private information as essential for task resolution, forcing agents to balance effective collaboration with strategic information control. Our evaluation reveals that state-of-the-art agents, including GPT-5 and Gemini 2.5-Pro, exhibit significant privacy leakage, with Gemini 2.5-Pro leaking up to 50.7% and GPT-5 up to 35.1% of the sensitive information even when explicitly instructed not to. Moreover, these agents struggle to achieve consensus or task completion and often resort to undesirable behaviors such as manipulation and power-seeking (e.g., Gemini 2.5-Pro demonstrating manipulation in 38.2% of the cases). These findings underscore that current LLM agents lack robust privacy understanding and are not yet adequately aligned to simultaneously preserve privacy and maintain effective collaboration in complex environments.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RM-RL: Role-Model Reinforcement Learning for Precise Robot Manipulation</title>
<link>https://arxiv.org/abs/2510.15189</link>
<guid>https://arxiv.org/abs/2510.15189</guid>
<content:encoded><![CDATA[
arXiv:2510.15189v1 Announce Type: new 
Abstract: Precise robot manipulation is critical for fine-grained applications such as chemical and biological experiments, where even small errors (e.g., reagent spillage) can invalidate an entire task. Existing approaches often rely on pre-collected expert demonstrations and train policies via imitation learning (IL) or offline reinforcement learning (RL). However, obtaining high-quality demonstrations for precision tasks is difficult and time-consuming, while offline RL commonly suffers from distribution shifts and low data efficiency. We introduce a Role-Model Reinforcement Learning (RM-RL) framework that unifies online and offline training in real-world environments. The key idea is a role-model strategy that automatically generates labels for online training data using approximately optimal actions, eliminating the need for human demonstrations. RM-RL reformulates policy learning as supervised training, reducing instability from distribution mismatch and improving efficiency. A hybrid training scheme further leverages online role-model data for offline reuse, enhancing data efficiency through repeated sampling. Extensive experiments show that RM-RL converges faster and more stably than existing RL methods, yielding significant gains in real-world manipulation: 53% improvement in translation accuracy and 20% in rotation accuracy. Finally, we demonstrate the successful execution of a challenging task, precisely placing a cell plate onto a shelf, highlighting the framework's effectiveness where prior methods fail.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-based In-situ Thought Exchanges for Critical Paper Reading</title>
<link>https://arxiv.org/abs/2510.15234</link>
<guid>https://arxiv.org/abs/2510.15234</guid>
<content:encoded><![CDATA[
arXiv:2510.15234v1 Announce Type: new 
Abstract: Critical reading is a primary way through which researchers develop their critical thinking skills. While exchanging thoughts and opinions with peers can strengthen critical reading, junior researchers often lack access to peers who can offer diverse perspectives. To address this gap, we designed an in-situ thought exchange interface informed by peer feedback from a formative study (N=8) to support junior researchers' critical paper reading. We evaluated the effects of thought exchanges under three conditions (no-agent, single-agent, and multi-agent) with 46 junior researchers over two weeks. Our results showed that incorporating agent-mediated thought exchanges during paper reading significantly improved participants' critical thinking scores compared to the no-agent condition. In the single-agent condition, participants more frequently made reflective annotations on the paper content. In the multi-agent condition, participants engaged more actively with agents' responses. Our qualitative analysis further revealed that participants compared and analyzed multiple perspectives in the multi-agent condition. This work contributes to understanding in-situ AI-based support for critical paper reading through thought exchanges and offers design implications for future research.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Beyond Context: A Survey of Multimodal Retrieval-Augmented Generation for Document Understanding</title>
<link>https://arxiv.org/abs/2510.15253</link>
<guid>https://arxiv.org/abs/2510.15253</guid>
<content:encoded><![CDATA[
arXiv:2510.15253v1 Announce Type: new 
Abstract: Document understanding is critical for applications from financial analysis to scientific discovery. Current approaches, whether OCR-based pipelines feeding Large Language Models (LLMs) or native Multimodal LLMs (MLLMs), face key limitations: the former loses structural detail, while the latter struggles with context modeling. Retrieval-Augmented Generation (RAG) helps ground models in external data, but documents' multimodal nature, i.e., combining text, tables, charts, and layout, demands a more advanced paradigm: Multimodal RAG. This approach enables holistic retrieval and reasoning across all modalities, unlocking comprehensive document intelligence. Recognizing its importance, this paper presents a systematic survey of Multimodal RAG for document understanding. We propose a taxonomy based on domain, retrieval modality, and granularity, and review advances involving graph structures and agentic frameworks. We also summarize key datasets, benchmarks, and applications, and highlight open challenges in efficiency, fine-grained representation, and robustness, providing a roadmap for future progress in document AI.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-dimensional Data Analysis and Applications Basing on LLM Agents and Knowledge Graph Interactions</title>
<link>https://arxiv.org/abs/2510.15258</link>
<guid>https://arxiv.org/abs/2510.15258</guid>
<content:encoded><![CDATA[
arXiv:2510.15258v1 Announce Type: new 
Abstract: In the current era of big data, extracting deep insights from massive, heterogeneous, and complexly associated multi-dimensional data has become a significant challenge. Large Language Models (LLMs) perform well in natural language understanding and generation, but still suffer from "hallucination" issues when processing structured knowledge and are difficult to update in real-time. Although Knowledge Graphs (KGs) can explicitly store structured knowledge, their static nature limits dynamic interaction and analytical capabilities. Therefore, this paper proposes a multi-dimensional data analysis method based on the interactions between LLM agents and KGs, constructing a dynamic, collaborative analytical ecosystem. This method utilizes LLM agents to automatically extract product data from unstructured data, constructs and visualizes the KG in real-time, and supports users in deep exploration and analysis of graph nodes through an interactive platform. Experimental results show that this method has significant advantages in product ecosystem analysis, relationship mining, and user-driven exploratory analysis, providing new ideas and tools for multi-dimensional data analysis.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Experience-Driven Exploration for Efficient API-Free AI Agents</title>
<link>https://arxiv.org/abs/2510.15259</link>
<guid>https://arxiv.org/abs/2510.15259</guid>
<content:encoded><![CDATA[
arXiv:2510.15259v1 Announce Type: new 
Abstract: Most existing software lacks accessible Application Programming Interfaces (APIs), requiring agents to operate solely through pixel-based Graphical User Interfaces (GUIs). In this API-free setting, large language model (LLM)-based agents face severe efficiency bottlenecks: limited to local visual experiences, they make myopic decisions and rely on inefficient trial-and-error, hindering both skill acquisition and long-term planning. To address these challenges, we propose KG-Agent, an experience-driven learning framework that structures an agent's raw pixel-level interactions into a persistent State-Action Knowledge Graph (SA-KG). KG-Agent overcomes inefficient exploration by linking functionally similar but visually distinct GUI states, forming a rich neighborhood of experience that enables the agent to generalize from a diverse set of historical strategies. To support long-horizon reasoning, we design a hybrid intrinsic reward mechanism based on the graph topology, combining a state value reward for exploiting known high-value pathways with a novelty reward that encourages targeted exploration. This approach decouples strategic planning from pure discovery, allowing the agent to effectively value setup actions with delayed gratification. We evaluate KG-Agent in two complex, open-ended GUI-based decision-making environments (Civilization V and Slay the Spire), demonstrating significant improvements in exploration efficiency and strategic depth over the state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AUGUSTUS: An LLM-Driven Multimodal Agent System with Contextualized User Memory</title>
<link>https://arxiv.org/abs/2510.15261</link>
<guid>https://arxiv.org/abs/2510.15261</guid>
<content:encoded><![CDATA[
arXiv:2510.15261v1 Announce Type: new 
Abstract: Riding on the success of LLMs with retrieval-augmented generation (RAG), there has been a growing interest in augmenting agent systems with external memory databases. However, the existing systems focus on storing text information in their memory, ignoring the importance of multimodal signals. Motivated by the multimodal nature of human memory, we present AUGUSTUS, a multimodal agent system aligned with the ideas of human memory in cognitive science. Technically, our system consists of 4 stages connected in a loop: (i) encode: understanding the inputs; (ii) store in memory: saving important information; (iii) retrieve: searching for relevant context from memory; and (iv) act: perform the task. Unlike existing systems that use vector databases, we propose conceptualizing information into semantic tags and associating the tags with their context to store them in a graph-structured multimodal contextual memory for efficient concept-driven retrieval. Our system outperforms the traditional multimodal RAG approach while being 3.5 times faster for ImageNet classification and outperforming MemGPT on the MSC benchmark.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Models for Scientific Discovery: From Paradigm Enhancement to Paradigm Transition</title>
<link>https://arxiv.org/abs/2510.15280</link>
<guid>https://arxiv.org/abs/2510.15280</guid>
<content:encoded><![CDATA[
arXiv:2510.15280v1 Announce Type: new 
Abstract: Foundation models (FMs), such as GPT-4 and AlphaFold, are reshaping the landscape of scientific research. Beyond accelerating tasks such as hypothesis generation, experimental design, and result interpretation, they prompt a more fundamental question: Are FMs merely enhancing existing scientific methodologies, or are they redefining the way science is conducted? In this paper, we argue that FMs are catalyzing a transition toward a new scientific paradigm. We introduce a three-stage framework to describe this evolution: (1) Meta-Scientific Integration, where FMs enhance workflows within traditional paradigms; (2) Hybrid Human-AI Co-Creation, where FMs become active collaborators in problem formulation, reasoning, and discovery; and (3) Autonomous Scientific Discovery, where FMs operate as independent agents capable of generating new scientific knowledge with minimal human intervention. Through this lens, we review current applications and emerging capabilities of FMs across existing scientific paradigms. We further identify risks and future directions for FM-enabled scientific discovery. This position paper aims to support the scientific community in understanding the transformative role of FMs and to foster reflection on the future of scientific discovery. Our project is available at https://github.com/usail-hkust/Awesome-Foundation-Models-for-Scientific-Discovery.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exemplar-Guided Planing: Enhanced LLM Agent for KGQA</title>
<link>https://arxiv.org/abs/2510.15283</link>
<guid>https://arxiv.org/abs/2510.15283</guid>
<content:encoded><![CDATA[
arXiv:2510.15283v1 Announce Type: new 
Abstract: Large Language Models (LLMs) as interactive agents show significant promise in Knowledge Graph Question Answering (KGQA) but often struggle with the semantic gap between natural language queries and structured knowledge graph (KG) representations. This leads to suboptimal planning and inefficient exploration on KG, while training-free approaches often underutilize valuable reasoning patterns in training data. To address these limitations, we propose a novel framework, Exemplar-Guided Planning (EGP), which enhances the planning capabilities of LLM agents for KGQA. EGP first preprocesses the training set questions via entity templating to normalize semantic variations. It then retrieves highly similar exemplary questions and their successful reasoning paths from this preprocessed set using semantic embeddings and an efficient FAISS index. These retrieved exemplars dynamically guide the LLM's planning process in two key phases: (1) Task Decomposition, by aligning generated sub-objectives with proven reasoning steps, and (2) Relation Exploration, by providing high-quality auxiliary information to improve relation pruning accuracy. Additionally, we introduce a Smart Lookahead mechanism during relation exploration to improve efficiency by preemptively exploring promising paths and potentially terminating exploration earlier. We apply EGP to the Plan-on-Graph (PoG) framework, termed PoG-EGP. Extensive experiments on two real-world KGQA datasets, WebQSP and CWQ, demonstrate that PoG-EGP significantly improves over the baseline PoG system and other compared methods.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VERA-MH Concept Paper</title>
<link>https://arxiv.org/abs/2510.15297</link>
<guid>https://arxiv.org/abs/2510.15297</guid>
<content:encoded><![CDATA[
arXiv:2510.15297v1 Announce Type: new 
Abstract: We introduce VERA-MH (Validation of Ethical and Responsible AI in Mental Health), an automated evaluation of the safety of AI chatbots used in mental health contexts, with an initial focus on suicide risk.
  Practicing clinicians and academic experts developed a rubric informed by best practices for suicide risk management for the evaluation. To fully automate the process, we used two ancillary AI agents. A user-agent model simulates users engaging in a mental health-based conversation with the chatbot under evaluation. The user-agent role-plays specific personas with pre-defined risk levels and other features. Simulated conversations are then passed to a judge-agent who scores them based on the rubric. The final evaluation of the chatbot being tested is obtained by aggregating the scoring of each conversation.
  VERA-MH is actively under development and undergoing rigorous validation by mental health clinicians to ensure user-agents realistically act as patients and that the judge-agent accurately scores the AI chatbot. To date we have conducted preliminary evaluation of GPT-5, Claude Opus and Claude Sonnet using initial versions of the VERA-MH rubric and used the findings for further design development. Next steps will include more robust clinical validation and iteration, as well as refining actionable scoring. We are seeking feedback from the community on both the technical and clinical aspects of our evaluation.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebGen-V Bench: Structured Representation for Enhancing Visual Design in LLM-based Web Generation and Evaluation</title>
<link>https://arxiv.org/abs/2510.15306</link>
<guid>https://arxiv.org/abs/2510.15306</guid>
<content:encoded><![CDATA[
arXiv:2510.15306v1 Announce Type: new 
Abstract: Witnessed by the recent advancements on leveraging LLM for coding and multimodal understanding, we present WebGen-V, a new benchmark and framework for instruction-to-HTML generation that enhances both data quality and evaluation granularity. WebGen-V contributes three key innovations: (1) an unbounded and extensible agentic crawling framework that continuously collects real-world webpages and can leveraged to augment existing benchmarks; (2) a structured, section-wise data representation that integrates metadata, localized UI screenshots, and JSON-formatted text and image assets, explicit alignment between content, layout, and visual components for detailed multimodal supervision; and (3) a section-level multimodal evaluation protocol aligning text, layout, and visuals for high-granularity assessment. Experiments with state-of-the-art LLMs and ablation studies validate the effectiveness of our structured data and section-wise evaluation, as well as the contribution of each component. To the best of our knowledge, WebGen-V is the first work to enable high-granularity agentic crawling and evaluation for instruction-to-HTML generation, providing a unified pipeline from real-world data acquisition and webpage generation to structured multimodal assessment.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Mobile Language Model Generation via Hybrid Context and Hardware Coordination</title>
<link>https://arxiv.org/abs/2510.15312</link>
<guid>https://arxiv.org/abs/2510.15312</guid>
<content:encoded><![CDATA[
arXiv:2510.15312v1 Announce Type: new 
Abstract: Enhancing on-device large language models (LLMs) with contextual information from local data enables personalized and task-aware generation, powering use cases such as intelligent assistants and UI agents. While recent developments in neural processors have substantially improved the efficiency of prefill on mobile devices, the token-by-token generation process still suffers from high latency and limited hardware utilization due to its inherently memory-bound characteristics. This work presents CoordGen, a mobile inference framework that integrates speculative decoding with dynamic hardware scheduling to accelerate context-aware text generation on mobile devices. The framework introduces three synergistic components: (1) adaptive execution scheduling, which dynamically balances compute graphs between prefill and decoding phases; (2) context-aligned drafting, which improves speculative efficiency through lightweight online calibration to current tasks; and (3) hardware-efficient draft extension, which reuses and expands intermediate sequences to improve processing parallelism and reduce verification cost. Experiments on multiple smartphones and representative workloads show consistent improvements of up to 3.8x in generation speed and 4.7x in energy efficiency compared with existing mobile inference solutions. Component-level analysis further validates the contribution of each optimization.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SHARE: Scene-Human Aligned Reconstruction</title>
<link>https://arxiv.org/abs/2510.15342</link>
<guid>https://arxiv.org/abs/2510.15342</guid>
<content:encoded><![CDATA[
arXiv:2510.15342v1 Announce Type: new 
Abstract: Animating realistic character interactions with the surrounding environment is important for autonomous agents in gaming, AR/VR, and robotics. However, current methods for human motion reconstruction struggle with accurately placing humans in 3D space. We introduce Scene-Human Aligned REconstruction (SHARE), a technique that leverages the scene geometry's inherent spatial cues to accurately ground human motion reconstruction. Each reconstruction relies solely on a monocular RGB video from a stationary camera. SHARE first estimates a human mesh and segmentation mask for every frame, alongside a scene point map at keyframes. It iteratively refines the human's positions at these keyframes by comparing the human mesh against the human point map extracted from the scene using the mask. Crucially, we also ensure that non-keyframe human meshes remain consistent by preserving their relative root joint positions to keyframe root joints during optimization. Our approach enables more accurate 3D human placement while reconstructing the surrounding scene, facilitating use cases on both curated datasets and in-the-wild web videos. Extensive experiments demonstrate that SHARE outperforms existing methods.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Corrigibility Transformation: Constructing Goals That Accept Updates</title>
<link>https://arxiv.org/abs/2510.15395</link>
<guid>https://arxiv.org/abs/2510.15395</guid>
<content:encoded><![CDATA[
arXiv:2510.15395v1 Announce Type: new 
Abstract: For an AI's training process to successfully impart a desired goal, it is important that the AI does not attempt to resist the training. However, partially learned goals will often incentivize an AI to avoid further goal updates, as most goals are better achieved by an AI continuing to pursue them. We say that a goal is corrigible if it does not incentivize taking actions that avoid proper goal updates or shutdown. In addition to convergence in training, corrigibility also allows for correcting mistakes and changes in human preferences, which makes it a crucial safety property. Despite this, the existing literature does not include specifications for goals that are both corrigible and competitive with non-corrigible alternatives. We provide a formal definition for corrigibility, then introduce a transformation that constructs a corrigible version of any goal that can be made corrigible, without sacrificing performance. This is done by myopically eliciting predictions of reward conditional on costlessly preventing updates, which then also determine the reward when updates are accepted. The transformation can be modified to recursively extend corrigibility to any new agents created by corrigible agents, and to prevent agents from deliberately modifying their goals. Two gridworld experiments demonstrate that these corrigible goals can be learned effectively, and that they lead to the desired behavior.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MARS: Reinforcing Multi-Agent Reasoning of LLMs through Self-Play in Strategic Games</title>
<link>https://arxiv.org/abs/2510.15414</link>
<guid>https://arxiv.org/abs/2510.15414</guid>
<content:encoded><![CDATA[
arXiv:2510.15414v1 Announce Type: new 
Abstract: Developing Large Language Models (LLMs) to cooperate and compete effectively within multi-agent systems is a critical step towards more advanced intelligence. While reinforcement learning (RL) has proven effective for enhancing reasoning in single-agent tasks, its extension to multi-turn, multi-agent scenarios remains underexplored due to the challenges of long-horizon credit assignment and agent-specific advantage estimation. To address these challenges, we introduce MARS, an end-to-end RL framework that incentivizes Multi-Agent Reasoning of LLMs through Self-play in both cooperative and competitive games. MARS features a turn-level advantage estimator that aligns learning signals with each interaction for credit assignment, and an agent-specific advantage normalization to stabilize multi-agent training. By learning with self-play across cooperative and competitive games, the MARS agent trained from Qwen3-4B develops strong strategic abilities that generalize to held-out games with up to 28.7% performance improvements. More importantly, the capability acquired through self-play generalizes beyond games, yielding consistent performance gains of multi-agent systems in reasoning benchmarks. When integrated into leading multi-agent systems, our MARS agent achieves significant performance gains of 10.0% on AIME and 12.5% on GPQA-Diamond. These results establish end-to-end RL training with self-play in strategic games as a powerful approach for developing generalizable multi-agent reasoning capabilities in LLMs. Our code and models are publicly available at https://github.com/thu-nics/MARS.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Minds: Empowering Agents with LoRA-as-Tools</title>
<link>https://arxiv.org/abs/2510.15416</link>
<guid>https://arxiv.org/abs/2510.15416</guid>
<content:encoded><![CDATA[
arXiv:2510.15416v1 Announce Type: new 
Abstract: We present Adaptive Minds, an agentic system that treats LoRA adapters as domain-specific tools. Instead of relying on a single fine-tuned model or rigid rule-based routing, our approach empowers the base LLM itself to act as a semantic router analyzing each query and dynamically selecting the most relevant LoRA tool. This enables the agent to seamlessly switch between different domain experts on demand. By combining the flexibility of multi-agent orchestration with the efficiency of parameter-efficient fine-tuning, Adaptive Minds delivers accurate, specialized responses while preserving conversational ability. The system is built with LangGraph for workflow management, supports both API and web interfaces, and is fully open source, providing a scalable and extensible foundation for domain-adaptive AI assistance.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Select Less, Reason More: Prioritizing Evidence Purity for Video Reasoning</title>
<link>https://arxiv.org/abs/2510.15440</link>
<guid>https://arxiv.org/abs/2510.15440</guid>
<content:encoded><![CDATA[
arXiv:2510.15440v1 Announce Type: new 
Abstract: Long-form video reasoning remains a major challenge for Video Large Language Models (Video LLMs), as static uniform frame sampling leads to information dilution and obscures critical evidence. Furthermore, existing pixel-space video reasoning agents, which are designed to actively interact with the video to acquire new visual information, remain suboptimal due to their lack of rigorous reward mechanisms to enforce evidence purity and their inability to perform temporal information supplementation beyond pre-sampled frames. To address this critical gap, we propose a novel evidence-prioritized adaptive framework built upon our core philosophy: "Select Less, Reason More." Our core contribution is the evidence-aware reinforcement learning (EARL) framework, which transforms the model into an active interrogator of evidence. EARL is precisely engineered to dynamically select the most relevant frames and, crucially, to perform localized re-sampling around the selected key frames to access fine-grained temporal detail. Extensive experiments on five demanding video reasoning benchmarks demonstrate that our EARL-trained model achieves new state-of-the-art among open-source Video LLMs, simultaneously learning an effective and high-purity visual evidence selection policy. Impressively, our 7B model achieves 59.8% on LongVideoBench, 69.0% on MVBench and 64.9% on VideoMME. These results highlight the importance of prioritizing evidence purity and the effectiveness of our framework.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CORE: Reducing UI Exposure in Mobile Agents via Collaboration Between Cloud and Local LLMs</title>
<link>https://arxiv.org/abs/2510.15455</link>
<guid>https://arxiv.org/abs/2510.15455</guid>
<content:encoded><![CDATA[
arXiv:2510.15455v1 Announce Type: new 
Abstract: Mobile agents rely on Large Language Models (LLMs) to plan and execute tasks on smartphone user interfaces (UIs). While cloud-based LLMs achieve high task accuracy, they require uploading the full UI state at every step, exposing unnecessary and often irrelevant information. In contrast, local LLMs avoid UI uploads but suffer from limited capacity, resulting in lower task success rates. We propose $\textbf{CORE}$, a $\textbf{CO}$llaborative framework that combines the strengths of cloud and local LLMs to $\textbf{R}$educe UI $\textbf{E}$xposure, while maintaining task accuracy for mobile agents. CORE comprises three key components: (1) $\textbf{Layout-aware block partitioning}$, which groups semantically related UI elements based on the XML screen hierarchy; (2) $\textbf{Co-planning}$, where local and cloud LLMs collaboratively identify the current sub-task; and (3) $\textbf{Co-decision-making}$, where the local LLM ranks relevant UI blocks, and the cloud LLM selects specific UI elements within the top-ranked block. CORE further introduces a multi-round accumulation mechanism to mitigate local misjudgment or limited context. Experiments across diverse mobile apps and tasks show that CORE reduces UI exposure by up to 55.6% while maintaining task success rates slightly below cloud-only agents, effectively mitigating unnecessary privacy exposure to the cloud. The code is available at https://github.com/Entropy-Fighter/CORE.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrofitting Service Dependency Discovery in Distributed Systems</title>
<link>https://arxiv.org/abs/2510.15490</link>
<guid>https://arxiv.org/abs/2510.15490</guid>
<content:encoded><![CDATA[
arXiv:2510.15490v1 Announce Type: new 
Abstract: Modern distributed systems rely on complex networks of interconnected services, creating direct or indirect dependencies that can propagate faults and cause cascading failures. To localize the root cause of performance degradation in these environments, constructing a service dependency graph is highly beneficial. However, building an accurate service dependency graph is impaired by complex routing techniques, such as Network Address Translation (NAT), an essential mechanism for connecting services across networks. NAT obfuscates the actual hosts running the services, causing existing run-time approaches that passively observe network metadata to fail in accurately inferring service dependencies. To this end, this paper introduces XXXX, a novel run-time system for constructing process-level service dependency graphs. It operates without source code instrumentation and remains resilient under complex network routing mechanisms, including NAT. XXXX implements a non-disruptive method of injecting metadata onto a TCP packet's header that maintains protocol correctness across host boundaries. In other words, if no receiving agent is present, the instrumentation leaves existing TCP connections unaffected, ensuring non-disruptive operation when it is partially deployed across hosts. We evaluated XXXX extensively against three state-of-the-art systems across nine scenarios, involving three network configurations (NAT-free, internal-NAT, external-NAT) and three microservice benchmarks. XXXX was the only approach that performed consistently across networking configurations. With regards to correctness, it performed on par with, or better than, the state-of-the-art with precision and recall values of 100% in the majority of the scenarios.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Road Less Traveled: Enhancing Exploration in LLMs via Sequential Sampling</title>
<link>https://arxiv.org/abs/2510.15502</link>
<guid>https://arxiv.org/abs/2510.15502</guid>
<content:encoded><![CDATA[
arXiv:2510.15502v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has been pivotal in enhancing the reasoning capabilities of large language models (LLMs), but it often suffers from limited exploration and entropy collapse, where models exploit a narrow set of solutions, leading to a loss of sampling diversity and subsequently preventing RL from further improving performance. This issue is exacerbated in parallel sampling methods, where multiple outputs are drawn from the same distribution, potentially causing the model to converge to similar solutions. We propose SESA, a novel SEquential SAmpling framework that mitigates this challenge by generating diverse solution sketches sequentially before expanding them into full reasoning paths. This approach ensures broader exploration by conditioning each new output on previous ones, promoting diversity throughout the process and preventing policy collapse. Our experiments on a synthetic task show that sequential sampling consistently outperforms traditional RL methods in terms of path diversity and recovery from collapse. Further evaluations on real-world tasks demonstrate that SESA improves both the exploration of valid strategies and the overall performance of LLMs. On three agent benchmarks, SESA lifts success rates by $+0.25$, $+0.42$, and $+0.07$ absolute over the base model (up to an additional $211\%$ relative improvement over baseline RL), underscoring its exploration advantage. This work introduces a structured approach to exploration, paving the way for more effective and diverse reasoning in RL-trained LLMs. Our code is released at https://github.com/MuLabPKU/sesa.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Doubly Robust Estimation of Causal Effects in Strategic Equilibrium Systems</title>
<link>https://arxiv.org/abs/2510.15555</link>
<guid>https://arxiv.org/abs/2510.15555</guid>
<content:encoded><![CDATA[
arXiv:2510.15555v1 Announce Type: new 
Abstract: We introduce the Strategic Doubly Robust (SDR) estimator, a novel framework that integrates strategic equilibrium modeling with doubly robust estimation for causal inference in strategic environments. SDR addresses endogenous treatment assignment arising from strategic agent behavior, maintaining double robustness while incorporating strategic considerations. Theoretical analysis confirms SDR's consistency and asymptotic normality under strategic unconfoundedness. Empirical evaluations demonstrate SDR's superior performance over baseline methods, achieving 7.6\%-29.3\% bias reduction across varying strategic strengths and maintaining robust scalability with agent populations. The framework provides a principled approach for reliable causal inference when agents respond strategically to interventions.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KITE: A Benchmark for Evaluating Korean Instruction-Following Abilities in Large Language Models</title>
<link>https://arxiv.org/abs/2510.15558</link>
<guid>https://arxiv.org/abs/2510.15558</guid>
<content:encoded><![CDATA[
arXiv:2510.15558v1 Announce Type: new 
Abstract: The instruction-following capabilities of large language models (LLMs) are pivotal for numerous applications, from conversational agents to complex reasoning systems. However, current evaluations predominantly focus on English models, neglecting the linguistic and cultural nuances of other languages. Specifically, Korean, with its distinct syntax, rich morphological features, honorific system, and dual numbering systems, lacks a dedicated benchmark for assessing open-ended instruction-following capabilities. To address this gap, we introduce the Korean Instruction-following Task Evaluation (KITE), a comprehensive benchmark designed to evaluate both general and Korean-specific instructions. Unlike existing Korean benchmarks that focus mainly on factual knowledge or multiple-choice testing, KITE directly targets diverse, open-ended instruction-following tasks. Our evaluation pipeline combines automated metrics with human assessments, revealing performance disparities across models and providing deeper insights into their strengths and weaknesses. By publicly releasing the KITE dataset and code, we aim to foster further research on culturally and linguistically inclusive LLM development and inspire similar endeavors for other underrepresented languages.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Spark Effect: On Engineering Creative Diversity in Multi-Agent AI Systems</title>
<link>https://arxiv.org/abs/2510.15568</link>
<guid>https://arxiv.org/abs/2510.15568</guid>
<content:encoded><![CDATA[
arXiv:2510.15568v1 Announce Type: new 
Abstract: Creative services teams increasingly rely on large language models (LLMs) to accelerate ideation, yet production systems often converge on homogeneous outputs that fail to meet brand or artistic expectations. Art of X developed persona-conditioned LLM agents -- internally branded as "Sparks" and instantiated through a library of role-inspired system prompts -- to intentionally diversify agent behaviour within a multi-agent workflow. This white paper documents the problem framing, experimental design, and quantitative evidence behind the Spark agent programme. Using an LLM-as-a-judge protocol calibrated against human gold standards, we observe a mean diversity gain of +4.1 points (on a 1-10 scale) when persona-conditioned Spark agents replace a uniform system prompt, narrowing the gap to human experts to 1.0 point. We also surface evaluator bias and procedural considerations for future deployments.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRATING: Low-Latency and Memory-Efficient Semantic Selection on Device</title>
<link>https://arxiv.org/abs/2510.15620</link>
<guid>https://arxiv.org/abs/2510.15620</guid>
<content:encoded><![CDATA[
arXiv:2510.15620v1 Announce Type: new 
Abstract: Semantic top-K selection with cross-encoder rerankers underpins of on-device AI services, such as retrieval-augmented generation, agent memory, and personalized recommendation. However, its latency and memory demands dominate end-to-end budgets on edge hardware. Revisiting the objective of top-K selection, we reveal that only relative rankings matter, not exact per-candidate scores. We further observe sequence-level sparsity: relative rankings stabilize early in intermediate layers, allowing pruning opportunities prior to completing full inference.
  Building on this insight, we propose monolithic forwarding and develop a training-free inference system, GRATING. By maintaining a global view of all candidates, it reduces latency through progressive cluster pruning. It also bounds peak memory usage by strategically overlapping I/O with computation via dual-layer sliding window and chunked execution. We evaluate GRATING against state-of-the-art baselines on rerankers from 0.6B to 8B parameters across Apple M2 and RTX 5070. GRATING consistently reduces latency by up to 89.0% and peak memory by up to 94.9% in microbenchmarks, without any loss in precision. Across three real-world on-device AI applications, GRATING lowers latency by 11.6%-51.0% and peak memory by 18.6%-77.8%, demonstrating substantial improvements in efficiency and deployability.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation</title>
<link>https://arxiv.org/abs/2510.15624</link>
<guid>https://arxiv.org/abs/2510.15624</guid>
<content:encoded><![CDATA[
arXiv:2510.15624v1 Announce Type: new 
Abstract: The automation of scientific discovery represents a critical milestone in Artificial Intelligence (AI) research. However, existing agentic systems for science suffer from two fundamental limitations: rigid, pre-programmed workflows that cannot adapt to intermediate findings, and inadequate context management that hinders long-horizon research. We present \texttt{freephdlabor}, an open-source multiagent framework featuring \textit{fully dynamic workflows} determined by real-time agent reasoning and a \coloremph{\textit{modular architecture}} enabling seamless customization -- users can modify, add, or remove agents to address domain-specific requirements. The framework provides comprehensive infrastructure including \textit{automatic context compaction}, \textit{workspace-based communication} to prevent information degradation, \textit{memory persistence} across sessions, and \textit{non-blocking human intervention} mechanisms. These features collectively transform automated research from isolated, single-run attempts into \textit{continual research programs} that build systematically on prior explorations and incorporate human feedback. By providing both the architectural principles and practical implementation for building customizable co-scientist systems, this work aims to facilitate broader adoption of automated research across scientific domains, enabling practitioners to deploy interactive multiagent systems that autonomously conduct end-to-end research -- from ideation through experimentation to publication-ready manuscripts.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Parameter-Free Online Learning</title>
<link>https://arxiv.org/abs/2510.15644</link>
<guid>https://arxiv.org/abs/2510.15644</guid>
<content:encoded><![CDATA[
arXiv:2510.15644v1 Announce Type: new 
Abstract: We propose the first parameter-free decentralized online learning algorithms with network regret guarantees, which achieve sublinear regret without requiring hyperparameter tuning. This family of algorithms connects multi-agent coin-betting and decentralized online learning via gossip steps. To enable our decentralized analysis, we introduce a novel "betting function" formulation for coin-betting that simplifies the multi-agent regret analysis. Our analysis shows sublinear network regret bounds and is validated through experiments on synthetic and real datasets. This family of algorithms is applicable to distributed sensing, decentralized optimization, and collaborative ML applications.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SQuAI: Scientific Question-Answering with Multi-Agent Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2510.15682</link>
<guid>https://arxiv.org/abs/2510.15682</guid>
<content:encoded><![CDATA[
arXiv:2510.15682v1 Announce Type: new 
Abstract: We present SQuAI (https://squai.scads.ai/), a scalable and trustworthy multi-agent retrieval-augmented generation (RAG) framework for scientific question answering (QA) with large language models (LLMs). SQuAI addresses key limitations of existing RAG systems in the scholarly domain, where complex, open-domain questions demand accurate answers, explicit claims with citations, and retrieval across millions of scientific documents. Built on over 2.3 million full-text papers from arXiv.org, SQuAI employs four collaborative agents to decompose complex questions into sub-questions, retrieve targeted evidence via hybrid sparse-dense retrieval, and adaptively filter documents to improve contextual relevance. To ensure faithfulness and traceability, SQuAI integrates in-line citations for each generated claim and provides supporting sentences from the source documents. Our system improves faithfulness, answer relevance, and contextual relevance by up to +0.088 (12%) over a strong RAG baseline. We further release a benchmark of 1,000 scientific question-answer-evidence triplets to support reproducibility. With transparent reasoning, verifiable citations, and domain-wide scalability, SQuAI demonstrates how multi-agent RAG enables more trustworthy scientific QA with LLMs.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProofOptimizer: Training Language Models to Simplify Proofs without Human Demonstrations</title>
<link>https://arxiv.org/abs/2510.15700</link>
<guid>https://arxiv.org/abs/2510.15700</guid>
<content:encoded><![CDATA[
arXiv:2510.15700v1 Announce Type: new 
Abstract: Neural theorem proving has advanced rapidly in the past year, reaching IMO gold-medalist capabilities and producing formal proofs that span thousands of lines. Although such proofs are mechanically verified by formal systems like Lean, their excessive length renders them difficult for humans to comprehend and limits their usefulness for mathematical insight. Proof simplification is therefore a critical bottleneck. Yet, training data for this task is scarce, and existing methods -- mainly agentic scaffolding with off-the-shelf LLMs -- struggle with the extremely long proofs generated by RL-trained provers. We introduce ProofOptimizer, the first language model trained to simplify Lean proofs without requiring additional human supervision. ProofOptimizer is trained via expert iteration and reinforcement learning, using Lean to verify simplifications and provide training signal. At inference time, it operates within an iterative proof-shortening workflow, progressively reducing proof length. Experiments show that ProofOptimizer substantially compresses proofs generated by state-of-the-art RL-trained provers on standard benchmarks, reducing proof length by 87% on miniF2F, 57% on PutnamBench, and 49% on Seed-Prover's IMO 2025 proofs. Beyond conciseness, the simplified proofs check faster in Lean and further improve downstream prover performance when reused as training data for supervised finetuning.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProSh: Probabilistic Shielding for Model-free Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.15720</link>
<guid>https://arxiv.org/abs/2510.15720</guid>
<content:encoded><![CDATA[
arXiv:2510.15720v1 Announce Type: new 
Abstract: Safety is a major concern in reinforcement learning (RL): we aim at developing RL systems that not only perform optimally, but are also safe to deploy by providing formal guarantees about their safety. To this end, we introduce Probabilistic Shielding via Risk Augmentation (ProSh), a model-free algorithm for safe reinforcement learning under cost constraints. ProSh augments the Constrained MDP state space with a risk budget and enforces safety by applying a shield to the agent's policy distribution using a learned cost critic. The shield ensures that all sampled actions remain safe in expectation. We also show that optimality is preserved when the environment is deterministic. Since ProSh is model-free, safety during training depends on the knowledge we have acquired about the environment. We provide a tight upper-bound on the cost in expectation, depending only on the backup-critic accuracy, that is always satisfied during training. Under mild, practically achievable assumptions, ProSh guarantees safety even at training time, as shown in the experiments.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLAF: Reinforcement Learning from Automaton Feedback</title>
<link>https://arxiv.org/abs/2510.15728</link>
<guid>https://arxiv.org/abs/2510.15728</guid>
<content:encoded><![CDATA[
arXiv:2510.15728v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) in environments with complex, history-dependent reward structures poses significant challenges for traditional methods. In this work, we introduce a novel approach that leverages automaton-based feedback to guide the learning process, replacing explicit reward functions with preferences derived from a deterministic finite automaton (DFA). Unlike conventional approaches that use automata for direct reward specification, our method employs the structure of the DFA to generate preferences over trajectories that are used to learn a reward function, eliminating the need for manual reward engineering. Our framework introduces a static approach that uses the learned reward function directly for policy optimization and a dynamic approach that involves continuous refining of the reward function and policy through iterative updates until convergence.
  Our experiments in both discrete and continuous environments demonstrate that our approach enables the RL agent to learn effective policies for tasks with temporal dependencies, outperforming traditional reward engineering and automaton-based baselines such as reward machines and LTL-guided methods. Our results highlight the advantages of automaton-based preferences in handling non-Markovian rewards, offering a scalable, efficient, and human-independent alternative to traditional reward modeling. We also provide a convergence guarantee showing that under standard assumptions our automaton-guided preference-based framework learns a policy that is near-optimal with respect to the true non-Markovian objective.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AURA: An Agent Autonomy Risk Assessment Framework</title>
<link>https://arxiv.org/abs/2510.15739</link>
<guid>https://arxiv.org/abs/2510.15739</guid>
<content:encoded><![CDATA[
arXiv:2510.15739v1 Announce Type: new 
Abstract: As autonomous agentic AI systems see increasing adoption across organisations, persistent challenges in alignment, governance, and risk management threaten to impede deployment at scale. We present AURA (Agent aUtonomy Risk Assessment), a unified framework designed to detect, quantify, and mitigate risks arising from agentic AI. Building on recent research and practical deployments, AURA introduces a gamma-based risk scoring methodology that balances risk assessment accuracy with computational efficiency and practical considerations. AURA provides an interactive process to score, evaluate and mitigate the risks of running one or multiple AI Agents, synchronously or asynchronously (autonomously). The framework is engineered for Human-in-the-Loop (HITL) oversight and presents Agent-to-Human (A2H) communication mechanisms, allowing for seamless integration with agentic systems for autonomous self-assessment, rendering it interoperable with established protocols (MCP and A2A) and tools. AURA supports a responsible and transparent adoption of agentic AI and provides robust risk detection and mitigation while balancing computational resources, positioning it as a critical enabler for large-scale, governable agentic AI in enterprise environments.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset</title>
<link>https://arxiv.org/abs/2510.15742</link>
<guid>https://arxiv.org/abs/2510.15742</guid>
<content:encoded><![CDATA[
arXiv:2510.15742v1 Announce Type: new 
Abstract: Instruction-based video editing promises to democratize content creation, yet its progress is severely hampered by the scarcity of large-scale, high-quality training data. We introduce Ditto, a holistic framework designed to tackle this fundamental challenge. At its heart, Ditto features a novel data generation pipeline that fuses the creative diversity of a leading image editor with an in-context video generator, overcoming the limited scope of existing models. To make this process viable, our framework resolves the prohibitive cost-quality trade-off by employing an efficient, distilled model architecture augmented by a temporal enhancer, which simultaneously reduces computational overhead and improves temporal coherence. Finally, to achieve full scalability, this entire pipeline is driven by an intelligent agent that crafts diverse instructions and rigorously filters the output, ensuring quality control at scale. Using this framework, we invested over 12,000 GPU-days to build Ditto-1M, a new dataset of one million high-fidelity video editing examples. We trained our model, Editto, on Ditto-1M with a curriculum learning strategy. The results demonstrate superior instruction-following ability and establish a new state-of-the-art in instruction-based video editing.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grassroots Logic Programs: A Secure, Multiagent, Concurrent, Logic Programming Language</title>
<link>https://arxiv.org/abs/2510.15747</link>
<guid>https://arxiv.org/abs/2510.15747</guid>
<content:encoded><![CDATA[
arXiv:2510.15747v1 Announce Type: new 
Abstract: Grassroots platforms are distributed applications run by\linebreak cryptographically-identified people on their networked personal devices, where multiple disjoint platform instances emerge independently and coalesce when they interoperate. Their foundation is the grassroots social graph, upon which grassroots social networks, grassroots cryptocurrencies, and grassroots democratic federations can be built.
  Grassroots platforms have yet to be implemented, the key challenge being faulty and malicious participants: without secure programming support, correct participants cannot reliably identify each other, establish secure communication, or verify each other's code integrity.
  We present Grassroots Logic Programs (GLP), a secure, multiagent, concurrent, logic programming language for implementing grassroots platforms. GLP extends logic programs with paired single-reader/single-writer (SRSW) logic variables, providing secure communication channels among cryptographically-identified people through encrypted, signed and attested messages, which enable identity and code integrity verification. We present GLP progressively: logic programs, concurrent GLP, multiagent GLP, augmenting it with cryptographic security, and providing smartphone implementation-ready specifications. We prove safety properties including that GLP computations are deductions, SRSW preservation, acyclicity, and monotonicity. We prove multiagent GLP is grassroots and that GLP streams achieve blockchain security properties. We present a grassroots social graph protocol establishing authenticated peer-to-peer connections and demonstrate secure grassroots social networking applications.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-evolving expertise in complex non-verifiable subject domains: dialogue as implicit meta-RL</title>
<link>https://arxiv.org/abs/2510.15772</link>
<guid>https://arxiv.org/abs/2510.15772</guid>
<content:encoded><![CDATA[
arXiv:2510.15772v1 Announce Type: new 
Abstract: So-called `wicked problems', those involving complex multi-dimensional settings, non-verifiable outcomes, heterogeneous impacts and a lack of single objectively correct answers, have plagued humans throughout history. Modern examples include decisions over justice frameworks, solving environmental pollution, planning for pandemic resilience and food security. The use of state-of-the-art artificial intelligence systems (notably Large Language Model-based agents) collaborating with humans on solving such problems is being actively explored. While the abilities of LLMs can be improved by, for example, fine-tuning, hand-crafted system prompts and scaffolding with external tools, LLMs lack endogenous mechanisms to develop expertise through experience in such settings. This work address this gap with Dialectica, a framework where agents engage in structured dialogue on defined topics, augmented by memory, self-reflection, and policy-constrained context editing. Formally, discussion is viewed as an implicit meta-reinforcement learning process. The `dialogue-trained' agents are evaluated post-hoc using judged pairwise comparisons of elicited responses. Across two model architectures (locally run Qwen3:30b and OpenAI's o4-mini) results show that enabling reflection-based context editing during discussion produces agents which dominate their baseline counterparts on Elo scores, normalized Bradley-Terry-Davidson ability, and AlphaRank mass. The predicted signatures of learning are observed qualitatively in statement and reflection logs, where reflections identify weaknesses and reliably shape subsequent statements. Agreement between quantitative and qualitative evidence supports dialogue-driven context evolution as a practical path to targeted expertise amplification in open non-verifiable domains.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VISTA: A Test-Time Self-Improving Video Generation Agent</title>
<link>https://arxiv.org/abs/2510.15831</link>
<guid>https://arxiv.org/abs/2510.15831</guid>
<content:encoded><![CDATA[
arXiv:2510.15831v1 Announce Type: new 
Abstract: Despite rapid advances in text-to-video synthesis, generated video quality remains critically dependent on precise user prompts. Existing test-time optimization methods, successful in other domains, struggle with the multi-faceted nature of video. In this work, we introduce VISTA (Video Iterative Self-improvemenT Agent), a novel multi-agent system that autonomously improves video generation through refining prompts in an iterative loop. VISTA first decomposes a user idea into a structured temporal plan. After generation, the best video is identified through a robust pairwise tournament. This winning video is then critiqued by a trio of specialized agents focusing on visual, audio, and contextual fidelity. Finally, a reasoning agent synthesizes this feedback to introspectively rewrite and enhance the prompt for the next generation cycle. Experiments on single- and multi-scene video generation scenarios show that while prior methods yield inconsistent gains, VISTA consistently improves video quality and alignment with user intent, achieving up to 60% pairwise win rate against state-of-the-art baselines. Human evaluators concur, preferring VISTA outputs in 66.4% of comparisons.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paper2Web: Let's Make Your Paper Alive!</title>
<link>https://arxiv.org/abs/2510.15842</link>
<guid>https://arxiv.org/abs/2510.15842</guid>
<content:encoded><![CDATA[
arXiv:2510.15842v1 Announce Type: new 
Abstract: Academic project websites can more effectively disseminate research when they clearly present core content and enable intuitive navigation and interaction. However, current approaches such as direct Large Language Model (LLM) generation, templates, or direct HTML conversion struggle to produce layout-aware, interactive sites, and a comprehensive evaluation suite for this task has been lacking. In this paper, we introduce Paper2Web, a benchmark dataset and multi-dimensional evaluation framework for assessing academic webpage generation. It incorporates rule-based metrics like Connectivity, Completeness and human-verified LLM-as-a-Judge (covering interactivity, aesthetics, and informativeness), and PaperQuiz, which measures paper-level knowledge retention. We further present PWAgent, an autonomous pipeline that converts scientific papers into interactive and multimedia-rich academic homepages. The agent iteratively refines both content and layout through MCP tools that enhance emphasis, balance, and presentation quality. Our experiments show that PWAgent consistently outperforms end-to-end baselines like template-based webpages and arXiv/alphaXiv versions by a large margin while maintaining low cost, achieving the Pareto-front in academic webpage generation.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold</title>
<link>https://arxiv.org/abs/2510.15862</link>
<guid>https://arxiv.org/abs/2510.15862</guid>
<content:encoded><![CDATA[
arXiv:2510.15862v1 Announce Type: new 
Abstract: Tool-augmented large language models (LLMs) are emerging as deep research agents, systems that decompose complex queries, retrieve external evidence, and synthesize grounded responses. Yet current agents remain limited by shallow retrieval, weak alignment metrics, and brittle tool-use behavior. We introduce PokeeResearch-7B, a 7B-parameter deep research agent built under a unified reinforcement learning framework for robustness, alignment, and scalability. PokeeResearch-7B is trained by an annotation-free Reinforcement Learning from AI Feedback (RLAIF) framework to optimize policies using LLM-based reward signals that capture factual accuracy, citation faithfulness, and instruction adherence. A chain-of-thought-driven multi-call reasoning scaffold further enhances robustness through self-verification and adaptive recovery from tool failures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves state-of-the-art performance among 7B-scale deep research agents. This highlights that careful reinforcement learning and reasoning design can produce efficient, resilient, and research-grade AI agents. The model and inference code is open-sourced under MIT license at https://github.com/Pokee-AI/PokeeResearchOSS.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PolySkill: Learning Generalizable Skills Through Polymorphic Abstraction</title>
<link>https://arxiv.org/abs/2510.15863</link>
<guid>https://arxiv.org/abs/2510.15863</guid>
<content:encoded><![CDATA[
arXiv:2510.15863v1 Announce Type: new 
Abstract: Large language models (LLMs) are moving beyond static uses and are now powering agents that learn continually during their interaction with external environments. For example, agents can learn reusable skills while navigating web pages or toggling new tools. However, existing methods for skill learning often create skills that are over-specialized to a single website and fail to generalize. We introduce PolySkill, a new framework that enables agents to learn generalizable and compositional skills. The core idea, inspired by polymorphism in software engineering, is to decouple a skill's abstract goal (what it accomplishes) and its concrete implementation (how it is executed). Experiments show that our method (1) improves skill reuse by 1.7x on seen websites and (2) boosts success rates by up to 9.4% on Mind2Web and 13.9% on unseen websites, while reducing steps by over 20%. (3) In self-exploration settings without specified tasks, our framework improves the quality of proposed tasks and enables agents to learn generalizable skills that work across different sites. By enabling the agent to identify and refine its own goals, the PolySkill enhances the agent's ability to learn a better curriculum, leading to the acquisition of more generalizable skills compared to baseline methods. This work provides a practical path toward building agents capable of continual learning in adaptive environments. Our findings show that separating a skill's goal from its execution is a crucial step toward developing autonomous agents that can learn and generalize across the open web continuously.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FIRE: Fact-checking with Iterative Retrieval and Verification</title>
<link>https://arxiv.org/abs/2411.00784</link>
<guid>https://arxiv.org/abs/2411.00784</guid>
<content:encoded><![CDATA[
arXiv:2411.00784v2 Announce Type: cross 
Abstract: Fact-checking long-form text is challenging, and it is therefore common practice to break it down into multiple atomic claims. The typical approach to fact-checking these atomic claims involves retrieving a fixed number of pieces of evidence, followed by a verification step. However, this method is usually not cost-effective, as it underutilizes the verification model's internal knowledge of the claim and fails to replicate the iterative reasoning process in human search strategies. To address these limitations, we propose FIRE, a novel agent-based framework that integrates evidence retrieval and claim verification in an iterative manner. Specifically, FIRE employs a unified mechanism to decide whether to provide a final answer or generate a subsequent search query, based on its confidence in the current judgment. We compare FIRE with other strong fact-checking frameworks and find that it achieves slightly better performance while reducing large language model (LLM) costs by an average of 7.6 times and search costs by 16.5 times. These results indicate that FIRE holds promise for application in large-scale fact-checking operations. Our code is available at https://github.com/mbzuai-nlp/fire.git.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAS: A Transit-Aware Strategy for Embodied Navigation with Non-Stationary Targets</title>
<link>https://arxiv.org/abs/2403.09905</link>
<guid>https://arxiv.org/abs/2403.09905</guid>
<content:encoded><![CDATA[
arXiv:2403.09905v4 Announce Type: replace 
Abstract: Embodied navigation methods commonly operate in static environments with stationary targets. In this work, we present a new algorithm for navigation in dynamic scenarios with non-stationary targets. Our novel Transit-Aware Strategy (TAS) enriches embodied navigation policies with object path information. TAS improves performance in non-stationary environments by rewarding agents for synchronizing their routes with target routes. To evaluate TAS, we further introduce Dynamic Object Maps (DOMs), a dynamic variant of node-attributed topological graphs with structured object transitions. DOMs are inspired by human habits to simulate realistic object routes on a graph. Our experiments show that on average, TAS improves agent Success Rate (SR) by 21.1 in non-stationary environments, while also generalizing better from static environments by 44.5% when measured by Relative Change in Success (RCS). We qualitatively investigate TAS-agent performance on DOMs and draw various inferences to help better model generalist navigation policies. To the best of our knowledge, ours is the first work that quantifies the adaptability of embodied navigation methods in non-stationary environments. Code and data for our benchmark will be made publicly available.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Static Assumptions: the Predictive Justified Perspective Model for Epistemic Planning</title>
<link>https://arxiv.org/abs/2412.07941</link>
<guid>https://arxiv.org/abs/2412.07941</guid>
<content:encoded><![CDATA[
arXiv:2412.07941v2 Announce Type: replace 
Abstract: Epistemic Planning (EP) is an important research area dedicated to reasoning about the knowledge and beliefs of agents in multi-agent cooperative or adversarial settings. The Justified Perspective (JP) model is the state-of-the-art approach to solving EP problems with efficiency and expressiveness. However, all existing EP methods inherit the static environment assumption from classical planning. This limitation hinders the application of EP in fields such as robotics with multi-agent settings, where the environment contains changing variables.
  In this paper, we propose an extension of the JP model, namely, the Predictive Justified Perspective (PJP) model, to remove this assumption. Instead of assuming that beliefs remain unchanged since the last observation, the PJP model uses all past observations to form predictions about the changing variables. The definition of the prediction function with examples is provided, and it is demonstrated that it can work with arbitrary nesting. We then implemented the PJP model in several well-known domains and compared it with the JP model in the experiments. The results indicated that the PJP model performs exceptionally well across various domains, demonstrating its potential in improving EP applications in robotics.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where Common Knowledge Cannot Be Formed, Common Belief Can -- Planning with Multi-Agent Belief Using Group Justified Perspectives</title>
<link>https://arxiv.org/abs/2412.07981</link>
<guid>https://arxiv.org/abs/2412.07981</guid>
<content:encoded><![CDATA[
arXiv:2412.07981v2 Announce Type: replace 
Abstract: Epistemic planning is the sub-field of AI planning that focuses on changing knowledge and belief. It is important in both multi-agent domains where agents need to have knowledge/belief regarding the environment, but also the beliefs of other agents, including nested beliefs. When modeling knowledge in multi-agent settings, many models face an exponential growth challenge in terms of nested depth. A contemporary method, known as Planning with Perspectives (PWP), addresses these challenges through the use of perspectives and set operations for knowledge. The JP model defines that an agent's belief is justified if and only if the agent has seen evidence that this belief was true in the past and has not seen evidence to suggest that this has changed. The current paper extends the JP model to handle \emph{group belief}, including distributed belief and common belief. We call this the Group Justified Perspective (GJP) model. Using experimental problems crafted by adapting well-known benchmarks to a group setting, we show the efficiency and expressiveness of our GJP model at handling planning problems that cannot be handled by other epistemic planning tools.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards smart and adaptive agents for active sensing on edge devices</title>
<link>https://arxiv.org/abs/2501.06262</link>
<guid>https://arxiv.org/abs/2501.06262</guid>
<content:encoded><![CDATA[
arXiv:2501.06262v2 Announce Type: replace 
Abstract: TinyML has made deploying deep learning models on low-power edge devices feasible, creating new opportunities for real-time perception in constrained environments. However, the adaptability of such deep learning methods remains limited to data drift adaptation, lacking broader capabilities that account for the environment's underlying dynamics and inherent uncertainty. Deep learning's scaling laws, which counterbalance this limitation by massively up-scaling data and model size, cannot be applied when deploying on the Edge, where deep learning limitations are further amplified as models are scaled down for deployment on resource-constrained devices.
  This paper presents an innovative agentic system capable of performing on-device perception and planning, enabling active sensing on the edge. By incorporating active inference into our solution, our approach extends beyond deep learning capabilities, allowing the system to plan in dynamic environments while operating in real-time with a compact memory footprint of as little as 300 MB. We showcase our proposed system by creating and deploying a saccade agent connected to an IoT camera with pan and tilt capabilities on an NVIDIA Jetson embedded device. The saccade agent controls the camera's field of view following optimal policies derived from the active inference principles, simulating human-like saccadic motion for surveillance and robotics applications.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Final Code: A Process-Oriented Error Analysis of Software Development Agents in Real-World GitHub Scenarios</title>
<link>https://arxiv.org/abs/2503.12374</link>
<guid>https://arxiv.org/abs/2503.12374</guid>
<content:encoded><![CDATA[
arXiv:2503.12374v3 Announce Type: replace 
Abstract: AI-driven software development has rapidly advanced with the emergence of software development agents that leverage large language models (LLMs) to tackle complex, repository-level software engineering tasks. These agents go beyond just generation of final code; they engage in multi-step reasoning, utilize various tools for code modification and debugging, and interact with execution environments to diagnose and iteratively resolve issues. However, most existing evaluations focus primarily on static analyses of final code outputs, yielding limited insights into the agents' dynamic problem-solving processes. To fill this gap, we conduct an in-depth empirical study on 3,977 solving-phase trajectories and 3,931 testing-phase logs from 8 top-ranked agents evaluated on 500 GitHub issues in the SWE-Bench benchmark.
  Our exploratory analysis shows that Python execution errors during the issue resolution phase correlate with lower resolution rates and increased reasoning overheads. We have identified the most prevalent errors -- such as ModuleNotFoundError and TypeError -- and highlighted particularly challenging errors like OSError and database-related issues (e.g., IntegrityError) that demand significantly more debugging effort. Furthermore, we have discovered 3 bugs in the SWE-Bench platform that affect benchmark fairness and accuracy; these issues have been reported to and confirmed by the maintainers. To promote transparency and foster future research, we publicly share our datasets and analysis scripts.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Issue2Test: Generating Reproducing Test Cases from Issue Reports</title>
<link>https://arxiv.org/abs/2503.16320</link>
<guid>https://arxiv.org/abs/2503.16320</guid>
<content:encoded><![CDATA[
arXiv:2503.16320v2 Announce Type: replace 
Abstract: Automated tools for solving GitHub issues are receiving significant attention by both researchers and practitioners, e.g., in the form of foundation models and LLM-based agents prompted with issues. A crucial step toward successfully solving an issue is creating a test case that accurately reproduces the issue. Such a test case can guide the search for an appropriate patch and help validate whether the patch matches the issue's intent. However, existing techniques for issue reproduction show only moderate success. This paper presents Issue2Test, an LLM-based technique for automatically generating a reproducing test case for a given issue report. Unlike automated regression test generators, which aim at creating passing tests, our approach aims at a test that fails, and that fails specifically for the reason described in the issue. To this end, Issue2Test performs three steps: (1) understand the issue and gather context (e.g., related files and project-specific guidelines) relevant for reproducing it; (2) generate a candidate test case; and (3) iteratively refine the test case based on compilation and runtime feedback until it fails and the failure aligns with the problem described in the issue. We evaluate Issue2Test on the SWT-bench-lite dataset, where it successfully reproduces 32.9% of the issues, achieving a 16.3% relative improvement over the best existing technique. Our evaluation also shows that Issue2Test reproduces 20 issues that four prior techniques fail to address, contributing a total of 60.4% of all issues reproduced by these tools. We envision our approach to contribute to enhancing the overall progress in the important task of automatically solving GitHub issues.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text2Schema: Filling the Gap in Designing Database Table Structures based on Natural Language</title>
<link>https://arxiv.org/abs/2503.23886</link>
<guid>https://arxiv.org/abs/2503.23886</guid>
<content:encoded><![CDATA[
arXiv:2503.23886v2 Announce Type: replace 
Abstract: People without a database background usually rely on file systems or tools such as Excel for data management, which often lead to redundancy and data inconsistency. Relational databases possess strong data management capabilities, but require a high level of professional expertise from users. Although there are already many works on Text2SQL to automate the translation of natural language into SQL queries for data manipulation, all of them presuppose that the database schema is pre-designed. In practice, schema design itself demands domain expertise, and research on directly generating schemas from textual requirements remains unexplored. In this paper, we systematically define a new problem, called Text2Schema, to convert a natural language text requirement into a relational database schema. With an effective Text2Schema technique, users can effortlessly create database table structures using natural language, and subsequently leverage existing Text2SQL techniques to perform data manipulations, which significantly narrows the gap between non-technical personnel and highly efficient, versatile relational database systems. We propose SchemaAgent, an LLM-based multi-agent framework for Text2Schema. We emulate the workflow of manual schema design by assigning specialized roles to agents and enabling effective collaboration to refine their respective subtasks. We also incorporate dedicated roles for reflection and inspection, along with an innovative error detection and correction mechanism to identify and rectify issues across various phases. Moreover, we build and open source a benchmark containing 381 pairs of requirement description and schema. Experimental results demonstrate the superiority of our approach over comparative work.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LayerCraft: Enhancing Text-to-Image Generation with CoT Reasoning and Layered Object Integration</title>
<link>https://arxiv.org/abs/2504.00010</link>
<guid>https://arxiv.org/abs/2504.00010</guid>
<content:encoded><![CDATA[
arXiv:2504.00010v3 Announce Type: replace 
Abstract: Text-to-image (T2I) generation has made remarkable progress, yet existing systems still lack intuitive control over spatial composition, object consistency, and multi-step editing. We present $\textbf{LayerCraft}$, a modular framework that uses large language models (LLMs) as autonomous agents to orchestrate structured, layered image generation and editing. LayerCraft supports two key capabilities: (1) $\textit{structured generation}$ from simple prompts via chain-of-thought (CoT) reasoning, enabling it to decompose scenes, reason about object placement, and guide composition in a controllable, interpretable manner; and (2) $\textit{layered object integration}$, allowing users to insert and customize objects -- such as characters or props -- across diverse images or scenes while preserving identity, context, and style. The system comprises a coordinator agent, the $\textbf{ChainArchitect}$ for CoT-driven layout planning, and the $\textbf{Object Integration Network (OIN)}$ for seamless image editing using off-the-shelf T2I models without retraining. Through applications like batch collage editing and narrative scene generation, LayerCraft empowers non-experts to iteratively design, customize, and refine visual content with minimal manual effort. Code will be released at https://github.com/PeterYYZhang/LayerCraft.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Set-Theoretic Robust Control Approach for Linear Quadratic Games with Unknown Counterparts</title>
<link>https://arxiv.org/abs/2504.02679</link>
<guid>https://arxiv.org/abs/2504.02679</guid>
<content:encoded><![CDATA[
arXiv:2504.02679v2 Announce Type: replace 
Abstract: Ensuring robust decision-making in multi-agent systems is challenging when agents have distinct, possibly conflicting objectives and lack full knowledge of each other's strategies. This is apparent in safety-critical applications such as human-robot interaction and assisted driving, where uncertainty arises not only from unknown adversary strategies but also from external disturbances. To address this, the paper proposes a robust adaptive control approach based on linear quadratic differential games. Our method allows a controlled agent to iteratively refine its belief about the adversary strategy and disturbances using a set-membership approach, while simultaneously adapting its policy to guarantee robustness against the uncertain adversary policy and improve performance over time. We formally derive theoretical guarantees on the robustness of the proposed control scheme and its convergence to $\epsilon$-Nash strategies. The effectiveness of our approach is demonstrated in a numerical simulation.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Safe and Human-Aligned Game Conversational Recommendation via Multi-Agent Decomposition</title>
<link>https://arxiv.org/abs/2504.20094</link>
<guid>https://arxiv.org/abs/2504.20094</guid>
<content:encoded><![CDATA[
arXiv:2504.20094v2 Announce Type: replace 
Abstract: Conversational recommender systems (CRS) have advanced with large language models, showing strong results in domains like movies. These domains typically involve fixed content and passive consumption, where user preferences can be matched by genre or theme. In contrast, games present distinct challenges: fast-evolving catalogs, interaction-driven preferences (e.g., skill level, mechanics, hardware), and increased risk of unsafe responses in open-ended conversation. We propose MATCHA, a multi-agent framework for CRS that assigns specialized agents for intent parsing, tool-augmented retrieval, multi-LLM ranking with reflection, explanation, and risk control which enabling finer personalization, long-tail coverage, and stronger safety. Evaluated on real user request dataset, MATCHA outperforms six baselines across eight metrics, improving Hit@5 by 20%, reducing popularity bias by 24%, and achieving 97.9% adversarial defense. Human and virtual-judge evaluations confirm improved explanation quality and user alignment.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Multi Agent Reinforcement Learning for Underwater Acoustic Tracking via Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2505.08222</link>
<guid>https://arxiv.org/abs/2505.08222</guid>
<content:encoded><![CDATA[
arXiv:2505.08222v2 Announce Type: replace 
Abstract: Autonomous vehicles (AV) offer a cost-effective solution for scientific missions such as underwater tracking. Recently, reinforcement learning (RL) has emerged as a powerful method for controlling AVs in complex marine environments. However, scaling these techniques to a fleet--essential for multi-target tracking or targets with rapid, unpredictable motion--presents significant computational challenges. Multi-Agent Reinforcement Learning (MARL) is notoriously sample-inefficient, and while high-fidelity simulators like Gazebo's LRAUV provide 100x faster-than-real-time single-robot simulations, they offer no significant speedup for multi-vehicle scenarios, making MARL training impractical. To address these limitations, we propose an iterative distillation method that transfers high-fidelity simulations into a simplified, GPU-accelerated environment while preserving high-level dynamics. This approach achieves up to a 30,000x speedup over Gazebo through parallelization, enabling efficient training via end-to-end GPU acceleration. Additionally, we introduce a novel Transformer-based architecture (TransfMAPPO) that learns multi-agent policies invariant to the number of agents and targets, significantly improving sample efficiency. Following large-scale curriculum learning conducted entirely on GPU, we perform extensive evaluations in Gazebo, demonstrating that our method maintains tracking errors below 5 meters over extended durations, even in the presence of multiple fast-moving targets. This work bridges the gap between large-scale MARL training and high-fidelity deployment, providing a scalable framework for autonomous fleet control in real-world sea missions.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebInject: Prompt Injection Attack to Web Agents</title>
<link>https://arxiv.org/abs/2505.11717</link>
<guid>https://arxiv.org/abs/2505.11717</guid>
<content:encoded><![CDATA[
arXiv:2505.11717v4 Announce Type: replace 
Abstract: Multi-modal large language model (MLLM)-based web agents interact with webpage environments by generating actions based on screenshots of the webpages. In this work, we propose WebInject, a prompt injection attack that manipulates the webpage environment to induce a web agent to perform an attacker-specified action. Our attack adds a perturbation to the raw pixel values of the rendered webpage. After these perturbed pixels are mapped into a screenshot, the perturbation induces the web agent to perform the attacker-specified action. We formulate the task of finding the perturbation as an optimization problem. A key challenge in solving this problem is that the mapping between raw pixel values and screenshot is non-differentiable, making it difficult to backpropagate gradients to the perturbation. To overcome this, we train a neural network to approximate the mapping and apply projected gradient descent to solve the reformulated optimization problem. Extensive evaluation on multiple datasets shows that WebInject is highly effective and significantly outperforms baselines.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cooperative Bargaining Games Without Utilities: Mediated Solutions from Direction Oracles</title>
<link>https://arxiv.org/abs/2505.14817</link>
<guid>https://arxiv.org/abs/2505.14817</guid>
<content:encoded><![CDATA[
arXiv:2505.14817v2 Announce Type: replace 
Abstract: Cooperative bargaining games are widely used to model resource allocation and conflict resolution. Traditional solutions assume the mediator can access agents utility function values and gradients. However, there is an increasing number of settings, such as human AI interactions, where utility values may be inaccessible or incomparable due to unknown, nonaffine transformations. To model such settings, we consider that the mediator has access only to agents most preferred directions, i.e., normalized utility gradients in the decision space. To this end, we propose a cooperative bargaining algorithm where a mediator has access to only the direction oracle of each agent. We prove that unlike popular approaches such as the Nash and Kalai Smorodinsky bargaining solutions, our approach is invariant to monotonic nonaffine transformations, and that under strong convexity and smoothness assumptions, this approach enjoys global asymptotic convergence to Pareto stationary solutions. Moreover, we show that the bargaining solutions found by our algorithm also satisfy the axioms of symmetry and (under slightly stronger conditions) independence of irrelevant alternatives, which are popular in the literature. Finally, we conduct experiments in two domains, multi agent formation assignment and mediated stock portfolio allocation, which validate these theoretic results. All code for our experiments can be found at https://github.com/suryakmurthy/dibs_bargaining.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLMLight: Safety-Critical Traffic Signal Control via Vision-Language Meta-Control and Dual-Branch Reasoning Architecture</title>
<link>https://arxiv.org/abs/2505.19486</link>
<guid>https://arxiv.org/abs/2505.19486</guid>
<content:encoded><![CDATA[
arXiv:2505.19486v2 Announce Type: replace 
Abstract: Traffic signal control (TSC) is a core challenge in urban mobility, where real-time decisions must balance efficiency and safety. Existing methods - ranging from rule-based heuristics to reinforcement learning (RL) - often struggle to generalize to complex, dynamic, and safety-critical scenarios. We introduce VLMLight, a novel TSC framework that integrates vision-language meta-control with dual-branch reasoning. At the core of VLMLight is the first image-based traffic simulator that enables multi-view visual perception at intersections, allowing policies to reason over rich cues such as vehicle type, motion, and spatial density. A large language model (LLM) serves as a safety-prioritized meta-controller, selecting between a fast RL policy for routine traffic and a structured reasoning branch for critical cases. In the latter, multiple LLM agents collaborate to assess traffic phases, prioritize emergency vehicles, and verify rule compliance. Experiments show that VLMLight reduces waiting times for emergency vehicles by up to 65% over RL-only systems, while preserving real-time performance in standard conditions with less than 1% degradation. VLMLight offers a scalable, interpretable, and safety-aware solution for next-generation traffic signal control.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topological Structure Learning Should Be A Research Priority for LLM-Based Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2505.22467</link>
<guid>https://arxiv.org/abs/2505.22467</guid>
<content:encoded><![CDATA[
arXiv:2505.22467v3 Announce Type: replace 
Abstract: Large Language Model-based Multi-Agent Systems (MASs) have emerged as a powerful paradigm for tackling complex tasks through collaborative intelligence. However, the topology of these systems--how agents in MASs should be configured, connected, and coordinated--remains largely unexplored. In this position paper, we call for a paradigm shift toward \emph{topology-aware MASs} that explicitly model and dynamically optimize the structure of inter-agent interactions. We identify three fundamental components--agents, communication links, and overall topology--that collectively determine the system's adaptability, efficiency, robustness, and fairness. To operationalize this vision, we introduce a systematic three-stage framework: 1) agent selection, 2) structure profiling, and 3) topology synthesis. This framework not only provides a principled foundation for designing MASs but also opens new research frontiers across language modeling, reinforcement learning, graph learning, and generative modeling to ultimately unleash their full potential in complex real-world applications. We conclude by outlining key challenges and opportunities in MASs evaluation. We hope our framework and perspectives offer critical new insights in the era of agentic AI.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Free Lunch for User Experience: Crowdsourcing Agents for Scalable User Studies</title>
<link>https://arxiv.org/abs/2505.22981</link>
<guid>https://arxiv.org/abs/2505.22981</guid>
<content:encoded><![CDATA[
arXiv:2505.22981v2 Announce Type: replace 
Abstract: User studies are central to user experience research, yet recruiting participant is expensive, slow, and limited in diversity. Recent work has explored using Large Language Models as simulated users, but doubts about fidelity have hindered practical adoption. We deepen this line of research by asking whether scale itself can enable useful simulation, even if not perfectly accurate. We introduce Crowdsourcing Simulated User Agents, a method that recruits generative agents from billion-scale profile assets to act as study participants. Unlike handcrafted simulations, agents are treated as recruitable, screenable, and engageable across UX research stages. To ground this method, we demonstrate a game prototyping study with hundreds of simulated players, comparing their insights against a 10-participant local user study and a 20-participant crowdsourcing study with humans. We find a clear scaling effect: as the number of simulated user agents increases, coverage of human findings rises smoothly and plateaus around 90\%. 12.8 simulated agents are as useful as one locally recruited human, and 3.2 agents are as useful as one crowdsourced human. Results show that while individual agents are imperfect, aggregated simulations produce representative and actionable insights comparable to real users. Professional designers further rated these insights as balancing fidelity, cost, time efficiency, and usefulness. Finally, we release an agent crowdsourcing toolkit with a modular open-source pipeline and a curated pool of profiles synced from ongoing simulation research, to lower the barrier for researchers to adopt simulated participants. Together, this work contributes a validated method and reusable toolkit that expand the options for conducting scalable and practical UX studies.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Onboard Mission Replanning for Adaptive Cooperative Multi-Robot Systems</title>
<link>https://arxiv.org/abs/2506.06094</link>
<guid>https://arxiv.org/abs/2506.06094</guid>
<content:encoded><![CDATA[
arXiv:2506.06094v2 Announce Type: replace 
Abstract: Cooperative autonomous robotic systems have significant potential for executing complex multi-task missions across space, air, ground, and maritime domains. But they commonly operate in remote, dynamic and hazardous environments, requiring rapid in-mission adaptation without reliance on fragile or slow communication links to centralised compute. Fast, on-board replanning algorithms are therefore needed to enhance resilience. Reinforcement Learning shows strong promise for efficiently solving mission planning tasks when formulated as Travelling Salesperson Problems (TSPs), but existing methods: 1) are unsuitable for replanning, where agents do not start at a single location; 2) do not allow cooperation between agents; 3) are unable to model tasks with variable durations; or 4) lack practical considerations for on-board deployment. Here we define the Cooperative Mission Replanning Problem as a novel variant of multiple TSP with adaptations to overcome these issues, and develop a new encoder/decoder-based model using Graph Attention Networks and Attention Models to solve it effectively and efficiently. Using a simple example of cooperative drones, we show our replanner consistently (90% of the time) maintains performance within 10% of the state-of-the-art LKH3 heuristic solver, whilst running 85-370 times faster on a Raspberry Pi. This work paves the way for increased resilience in autonomous multi-agent systems.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinHEAR: Human Expertise and Adaptive Risk-Aware Temporal Reasoning for Financial Decision-Making</title>
<link>https://arxiv.org/abs/2506.09080</link>
<guid>https://arxiv.org/abs/2506.09080</guid>
<content:encoded><![CDATA[
arXiv:2506.09080v2 Announce Type: replace 
Abstract: Financial decision-making presents unique challenges for language models, demanding temporal reasoning, adaptive risk assessment, and responsiveness to dynamic events. While large language models (LLMs) show strong general reasoning capabilities, they often fail to capture behavioral patterns central to human financial decisions-such as expert reliance under information asymmetry, loss-averse sensitivity, and feedback-driven temporal adjustment. We propose FinHEAR, a multi-agent framework for Human Expertise and Adaptive Risk-aware reasoning. FinHEAR orchestrates specialized LLM-based agents to analyze historical trends, interpret current events, and retrieve expert-informed precedents within an event-centric pipeline. Grounded in behavioral economics, it incorporates expert-guided retrieval, confidence-adjusted position sizing, and outcome-based refinement to enhance interpretability and robustness. Empirical results on curated financial datasets show that FinHEAR consistently outperforms strong baselines across trend prediction and trading tasks, achieving higher accuracy and better risk-adjusted returns.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General-Purpose Robotic Navigation via LVLM-Orchestrated Perception, Reasoning, and Acting</title>
<link>https://arxiv.org/abs/2506.17462</link>
<guid>https://arxiv.org/abs/2506.17462</guid>
<content:encoded><![CDATA[
arXiv:2506.17462v2 Announce Type: replace 
Abstract: Developing general-purpose navigation policies for unknown environments remains a core challenge in robotics. Most existing systems rely on task-specific neural networks and fixed information flows, limiting their generalizability. Large Vision-Language Models (LVLMs) offer a promising alternative by embedding human-like knowledge for reasoning and planning, but prior LVLM-robot integrations have largely depended on pre-mapped spaces, hard-coded representations, and rigid control logic. We introduce the Agentic Robotic Navigation Architecture (ARNA), a general-purpose framework that equips an LVLM-based agent with a library of perception, reasoning, and navigation tools drawn from modern robotic stacks. At runtime, the agent autonomously defines and executes task-specific workflows that iteratively query modules, reason over multimodal inputs, and select navigation actions. This agentic formulation enables robust navigation and reasoning in previously unmapped environments, offering a new perspective on robotic stack design. Evaluated in Habitat Lab on the HM-EQA benchmark, ARNA outperforms state-of-the-art EQA-specific approaches. Qualitative results on RxR and custom tasks further demonstrate its ability to generalize across a broad range of navigation challenges.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLFM: Multi-Layered Feature Maps for Richer Language Understanding in Zero-Shot Semantic Navigation</title>
<link>https://arxiv.org/abs/2507.07299</link>
<guid>https://arxiv.org/abs/2507.07299</guid>
<content:encoded><![CDATA[
arXiv:2507.07299v2 Announce Type: replace 
Abstract: Recent progress in large vision-language models has driven improvements in language-based semantic navigation, where an embodied agent must reach a target object described in natural language. Yet we still lack a clear, language-focused evaluation framework to test how well agents ground the words in their instructions. We address this gap by proposing LangNav, an open-vocabulary multi-object navigation dataset with natural language goal descriptions (e.g. 'go to the red short candle on the table') and corresponding fine-grained linguistic annotations (e.g., attributes: color=red, size=short; relations: support=on). These labels enable systematic evaluation of language understanding. To evaluate on this setting, we extend multi-object navigation task setting to Language-guided Multi-Object Navigation (LaMoN), where the agent must find a sequence of goals specified using language. Furthermore, we propose Multi-Layered Feature Map (MLFM), a novel method that builds a queryable, multi-layered semantic map from pretrained vision-language features and proves effective for reasoning over fine-grained attributes and spatial relations in goal descriptions. Experiments on LangNav show that MLFM outperforms state-of-the-art zero-shot mapping-based navigation baselines.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Turning Sand to Gold: Recycling Data to Bridge On-Policy and Off-Policy Learning via Causal Bound</title>
<link>https://arxiv.org/abs/2507.11269</link>
<guid>https://arxiv.org/abs/2507.11269</guid>
<content:encoded><![CDATA[
arXiv:2507.11269v2 Announce Type: replace 
Abstract: Deep reinforcement learning (DRL) agents excel in solving complex decision-making tasks across various domains. However, they often require a substantial number of training steps and a vast experience replay buffer, leading to significant computational and resource demands. To address these challenges, we introduce a novel theoretical result that leverages the Neyman-Rubin potential outcomes framework into DRL. Unlike most methods that focus on bounding the counterfactual loss, we establish a causal bound on the factual loss, which is analogous to the on-policy loss in DRL. This bound is computed by storing past value network outputs in the experience replay buffer, effectively utilizing data that is usually discarded. Extensive experiments across the Atari 2600 and MuJoCo domains on various agents, such as DQN and SAC, achieve up to 383% higher reward ratio, outperforming the same agents without our proposed term, and reducing the experience replay buffer size by up to 96%, significantly improving sample efficiency at a negligible cost.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Pareto-Optimal and Fair Allocations with Personalized Bi-Valued Utilities</title>
<link>https://arxiv.org/abs/2507.18251</link>
<guid>https://arxiv.org/abs/2507.18251</guid>
<content:encoded><![CDATA[
arXiv:2507.18251v2 Announce Type: replace 
Abstract: We study the fair division problem of allocating $m$ indivisible goods to $n$ agents with additive personalized bi-valued utilities. Specifically, each agent $i$ assigns one of two positive values $a_i > b_i > 0$ to each good, indicating that agent $i$'s valuation of any good is either $a_i$ or $b_i$. For convenience, we denote the value ratio of agent $i$ as $r_i = a_i / b_i$.
  We give a characterization to all the Pareto-optimal allocations. Our characterization implies a polynomial-time algorithm to decide if a given allocation is Pareto-optimal in the case each $r_i$ is an integer. For the general case (where $r_i$ may be fractional), we show that this decision problem is coNP-complete. Our result complements the existing results: this decision problem is coNP-complete for tri-valued utilities (where each agent's value for each good belongs to $\{a,b,c\}$ for some prescribed $a>b>c\geq0$), and this decision problem belongs to P for bi-valued utilities (where $r_i$ in our model is the same for each agent).
  We further show that an EFX allocation always exists and can be computed in polynomial time under the personalized bi-valued utilities setting, which extends the previous result on bi-valued utilities. We propose the open problem of whether an EFX and Pareto-optimal allocation always exists (and can be computed in polynomial time).
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PowerChain: A Verifiable Agentic AI System for Automating Distribution Grid Analyses</title>
<link>https://arxiv.org/abs/2508.17094</link>
<guid>https://arxiv.org/abs/2508.17094</guid>
<content:encoded><![CDATA[
arXiv:2508.17094v2 Announce Type: replace 
Abstract: Rapid electrification and decarbonization are increasing the complexity of distribution grid (DG) operation and planning, necessitating advanced computational analyses to ensure reliability and resilience. These analyses depend on disparate workflows comprising complex models, function calls, and data pipelines that require substantial expert knowledge and remain difficult to automate. Workforce and budget constraints further limit utilities' ability to apply such analyses at scale. To address this gap, we build an agentic system PowerChain, which is capable of autonomously performing complex grid analyses. Existing agentic AI systems are typically developed in a bottom-up manner with customized context for predefined analysis tasks; therefore, they do not generalize to tasks that the agent has never seen. In comparison, to generalize to unseen DG analysis tasks, PowerChain dynamically generates structured context by leveraging supervisory signals from self-contained power systems tools (e.g., GridLAB-D) and an optimized set of expert-annotated and verified reasoning trajectories. For complex DG tasks defined in natural language, empirical results on real utility data demonstrate that PowerChain achieves up to a 144/% improvement in performance over baselines.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AppCopilot: Toward General, Accurate, Long-Horizon, and Efficient Mobile Agent</title>
<link>https://arxiv.org/abs/2509.02444</link>
<guid>https://arxiv.org/abs/2509.02444</guid>
<content:encoded><![CDATA[
arXiv:2509.02444v2 Announce Type: replace 
Abstract: With the raid evolution of large language models and multimodal models, the mobile-agent landscape has proliferated without converging on the fundamental challenges. This paper identifies four core problems that should be solved for mobile agents to deliver practical, scalable impact: (1) generalization across tasks, APPs, and devices; (2) accuracy, specifically precise on-screen interaction and click targeting; (3) long-horizon capability for sustained, multi-step goals; and (4) efficiency, specifically high-performance runtime on resource-constrained devices. We present AppCopilot, a multimodal, multi-agent, general-purpose mobile agent that operates across applications. AppCopilot operationalizes this position through an end-to-end pipeline spanning data collection, training, finetuning, efficient inference, and PC/mobile application. At the model layer, it integrates multimodal foundation models with robust Chinese-English support. At the reasoning and control layer, it combines chain-of-thought reasoning, hierarchical task planning and decomposition, and multi-agent collaboration. At the execution layer, it enables experiential adaptation, voice interaction, function calling, cross-APP and cross-device orchestration, and comprehensive mobile APP support. The system design incorporates profiling-driven optimization for latency and memory across heterogeneous hardware. Empirically, AppCopilot achieves significant improvements on four dimensions: stronger generalization, higher precision of on screen actions, more reliable long horizon task completion, and faster, more resource efficient runtime. By articulating a cohesive position and a reference architecture that closes the loop from data collection, training to finetuning and efficient inference, this paper offers a concrete roadmap for general purpose mobile agent and provides actionable guidance.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Ego-graph inference for Networked Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.16606</link>
<guid>https://arxiv.org/abs/2509.16606</guid>
<content:encoded><![CDATA[
arXiv:2509.16606v2 Announce Type: replace 
Abstract: In networked multi-agent reinforcement learning (Networked-MARL), decentralized agents must act under local observability and constrained communication over fixed physical graphs. Existing methods often assume static neighborhoods, limiting adaptability to dynamic or heterogeneous environments. While centralized frameworks can learn dynamic graphs, their reliance on global state access and centralized infrastructure is impractical in real-world decentralized systems. We propose a stochastic graph-based policy for Networked-MARL, where each agent conditions its decision on a sampled subgraph over its local physical neighborhood. Building on this formulation, we introduce BayesG, a decentralized actor-framework that learns sparse, context-aware interaction structures via Bayesian variational inference. Each agent operates over an ego-graph and samples a latent communication mask to guide message passing and policy computation. The variational distribution is trained end-to-end alongside the policy using an evidence lower bound (ELBO) objective, enabling agents to jointly learn both interaction topology and decision-making strategies. BayesG outperforms strong MARL baselines on large-scale traffic control tasks with up to 167 agents, demonstrating superior scalability, efficiency, and performance.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications</title>
<link>https://arxiv.org/abs/2509.26490</link>
<guid>https://arxiv.org/abs/2509.26490</guid>
<content:encoded><![CDATA[
arXiv:2509.26490v2 Announce Type: replace 
Abstract: As LLM-based agents are increasingly deployed in real-life scenarios, existing benchmarks fail to capture their inherent complexity of handling extensive information, leveraging diverse resources, and managing dynamic user interactions. To address this gap, we introduce VitaBench, a challenging benchmark that evaluates agents on versatile interactive tasks grounded in real-world settings. Drawing from daily applications in food delivery, in-store consumption, and online travel services, VitaBench presents agents with the most complex life-serving simulation environment to date, comprising 66 tools. Through a framework that eliminates domain-specific policies, we enable flexible composition of these scenarios and tools, yielding 100 cross-scenario tasks (main results) and 300 single-scenario tasks. Each task is derived from multiple real user requests and requires agents to reason across temporal and spatial dimensions, utilize complex tool sets, proactively clarify ambiguous instructions, and track shifting user intent throughout multi-turn conversations. Moreover, we propose a rubric-based sliding window evaluator, enabling robust assessment of diverse solution pathways in complex environments and stochastic interactions. Our comprehensive evaluation reveals that even the most advanced models achieve only 30% success rate on cross-scenario tasks, and less than 50% success rate on others. Overall, we believe VitaBench will serve as a valuable resource for advancing the development of AI agents in practical real-world applications. The code, dataset, and leaderboard are available at https://vitabench.github.io/
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACON: Optimizing Context Compression for Long-horizon LLM Agents</title>
<link>https://arxiv.org/abs/2510.00615</link>
<guid>https://arxiv.org/abs/2510.00615</guid>
<content:encoded><![CDATA[
arXiv:2510.00615v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly deployed as agents in dynamic, real-world environments, where success requires both reasoning and effective tool use. A central challenge for agentic tasks is the growing context length, as agents must accumulate long histories of actions and observations. This expansion raises costs and reduces efficiency in long-horizon tasks, yet prior work on context compression has mostly focused on single-step tasks or narrow applications. We introduce Agent Context Optimization (ACON), a unified framework that optimally compresses both environment observations and interaction histories into concise yet informative condensations. ACON leverages compression guideline optimization in natural language space: given paired trajectories where full context succeeds but compressed context fails, capable LLMs analyze the causes of failure, and the compression guideline is updated accordingly. Furthermore, we propose distilling the optimized LLM compressor into smaller models to reduce the overhead of the additional module. Experiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON reduces memory usage by 26-54% (peak tokens) while largely preserving task performance, preserves over 95% of accuracy when distilled into smaller compressors, and enhances smaller LMs as long-horizon agents with up to 46% performance improvement. Our code is available at https://github.com/microsoft/acon.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing through Uncertainty: Robust Task-Oriented Optimization in Visual Navigation</title>
<link>https://arxiv.org/abs/2510.00441</link>
<guid>https://arxiv.org/abs/2510.00441</guid>
<content:encoded><![CDATA[
arXiv:2510.00441v2 Announce Type: replace 
Abstract: Visual navigation is a fundamental problem in embodied AI, yet practical deployments demand long-horizon planning capabilities to address multi-objective tasks. A major bottleneck is data scarcity: policies learned from limited data often overfit and fail to generalize OOD. Existing neural network-based agents typically increase architectural complexity that paradoxically become counterproductive in the small-sample regime. This paper introduce NeuRO, a integrated learning-to-optimize framework that tightly couples perception networks with downstream task-level robust optimization. Specifically, NeuRO addresses core difficulties in this integration: (i) it transforms noisy visual predictions under data scarcity into convex uncertainty sets using Partially Input Convex Neural Networks (PICNNs) with conformal calibration, which directly parameterize the optimization constraints; and (ii) it reformulates planning under partial observability as a robust optimization problem, enabling uncertainty-aware policies that transfer across environments. Extensive experiments on both unordered and sequential multi-object navigation tasks demonstrate that NeuRO establishes SoTA performance, particularly in generalization to unseen environments. Our work thus presents a significant advancement for developing robust, generalizable autonomous agents.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Active RIS Configuration and User Power Control for Localization: A Neuroevolution-Based Approach</title>
<link>https://arxiv.org/abs/2510.13819</link>
<guid>https://arxiv.org/abs/2510.13819</guid>
<content:encoded><![CDATA[
arXiv:2510.13819v1 Announce Type: new 
Abstract: This paper studies user localization aided by a Reconfigurable Intelligent Surface (RIS). A feedback link from the Base Station (BS) to the user is adopted to enable dynamic power control of the user pilot transmissions in the uplink. A novel multi-agent algorithm for the joint control of the RIS phase configuration and the user transmit power is presented, which is based on a hybrid approach integrating NeuroEvolution (NE) and supervised learning. The proposed scheme requires only single-bit feedback messages for the uplink power control, supports RIS elements with discrete responses, and is numerically shown to outperform fingerprinting, deep reinforcement learning baselines and backpropagation-based position estimators.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Agent Communication Protocol (LACP) Requires Urgent Standardization: A Telecom-Inspired Protocol is Necessary</title>
<link>https://arxiv.org/abs/2510.13821</link>
<guid>https://arxiv.org/abs/2510.13821</guid>
<content:encoded><![CDATA[
arXiv:2510.13821v1 Announce Type: new 
Abstract: This position paper argues that the field of LLM agents requires a unified, telecom-inspired communication protocol to ensure safety, interoperability, and scalability, especially within the context of Next Generation (NextG) networks. Current ad-hoc communication methods are creating a fragmented ecosystem, reminiscent of the early "protocol wars" in networking, which stifles innovation and poses significant risks. Drawing inspiration from the layered, standardized protocols that underpin modern telecommunications, we propose the LLM-Agent Communication Protocol (LACP). LACP establishes a three-layer architecture designed to ensure semantic clarity in communication, transactional integrity for complex tasks, and robust, built-in security. In this position paper, we argue that adopting a principled, universal protocol is not merely beneficial but essential for realizing the potential of distributed AI. Such a standard is critical for ensuring that multi-agent systems can operate safely and reliably in the complex, real-time applications envisioned for 6G and beyond.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A2AS: Agentic AI Runtime Security and Self-Defense</title>
<link>https://arxiv.org/abs/2510.13825</link>
<guid>https://arxiv.org/abs/2510.13825</guid>
<content:encoded><![CDATA[
arXiv:2510.13825v1 Announce Type: new 
Abstract: The A2AS framework is introduced as a security layer for AI agents and LLM-powered applications, similar to how HTTPS secures HTTP. A2AS enforces certified behavior, activates model self-defense, and ensures context window integrity. It defines security boundaries, authenticates prompts, applies security rules and custom policies, and controls agentic behavior, enabling a defense-in-depth strategy. The A2AS framework avoids latency overhead, external dependencies, architectural changes, model retraining, and operational complexity. The BASIC security model is introduced as the A2AS foundation: (B) Behavior certificates enable behavior enforcement, (A) Authenticated prompts enable context window integrity, (S) Security boundaries enable untrusted input isolation, (I) In-context defenses enable secure model reasoning, (C) Codified policies enable application-specific rules. This first paper in the series introduces the BASIC security model and the A2AS framework, exploring their potential toward establishing the A2AS industry standard.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConDABench: Interactive Evaluation of Language Models for Data Analysis</title>
<link>https://arxiv.org/abs/2510.13835</link>
<guid>https://arxiv.org/abs/2510.13835</guid>
<content:encoded><![CDATA[
arXiv:2510.13835v1 Announce Type: new 
Abstract: Real-world data analysis tasks often come with under-specified goals and unclean data. User interaction is necessary to understand and disambiguate a user's intent, and hence, essential to solving these complex tasks. Existing benchmarks for evaluating LLMs on data analysis tasks do not capture these complexities or provide first-class support for interactivity. We introduce ConDABench, a framework for generating conversational data analysis (ConDA) benchmarks and evaluating external tools on the generated benchmarks. \bench consists of (a) a multi-agent workflow for generating realistic benchmarks from articles describing insights gained from public datasets, (b) 1,420 ConDA problems generated using this workflow, and (c) an evaluation harness that, for the first time, makes it possible to systematically evaluate conversational data analysis tools on the generated ConDA problems. Evaluation of state-of-the-art LLMs on the benchmarks reveals that while the new generation of models are better at solving more instances, they are not necessarily better at solving tasks that require sustained, long-form engagement. ConDABench is an avenue for model builders to measure progress towards truly collaborative models that can complete complex interactive tasks.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Craft to Constitution: A Governance-First Paradigm for Principled Agent Engineering</title>
<link>https://arxiv.org/abs/2510.13857</link>
<guid>https://arxiv.org/abs/2510.13857</guid>
<content:encoded><![CDATA[
arXiv:2510.13857v1 Announce Type: new 
Abstract: The advent of powerful Large Language Models (LLMs) has ushered in an ``Age of the Agent,'' enabling autonomous systems to tackle complex goals. However, the transition from prototype to production is hindered by a pervasive ``crisis of craft,'' resulting in agents that are brittle, unpredictable, and ultimately untrustworthy in mission-critical applications. This paper argues this crisis stems from a fundamental paradigm mismatch -- attempting to command inherently probabilistic processors with the deterministic mental models of traditional software engineering. To solve this crisis, we introduce a governance-first paradigm for principled agent engineering, embodied in a formal architecture we call ArbiterOS.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Correctness and Security in Multi-Turn Code Generation</title>
<link>https://arxiv.org/abs/2510.13859</link>
<guid>https://arxiv.org/abs/2510.13859</guid>
<content:encoded><![CDATA[
arXiv:2510.13859v1 Announce Type: new 
Abstract: AI coding assistants powered by large language models (LLMs) have transformed software development, significantly boosting productivity. While existing benchmarks evaluate the correctness and security of LLM-generated code, they are typically limited to single-turn tasks that do not reflect the iterative nature of real-world development. We introduce MT-Sec, the first benchmark to systematically evaluate both correctness and security in multi-turn coding scenarios. We construct this using a synthetic data pipeline that transforms existing single-turn tasks into semantically aligned multi-turn interaction sequences, allowing reuse of original test suites while modeling the complexity of real-world coding processes. We evaluate 32 open- and closed-source models, and three agent-scaffolding on MT-Sec and observe a consistent 20-27% drop in "correct and secure" outputs from single-turn to multi-turn settings -- even among state-of-the-art models. Beyond full-program generation, we also evaluate models on multi-turn code-diff generation -- an unexplored yet practically relevant setting -- and find that models perform worse here, with increased rates of functionally incorrect and insecure outputs. Finally, we find that while agent scaffoldings boost single-turn code generation performance, they are not quite as effective in multi-turn evaluations. Together, these findings highlight the need for benchmarks that jointly evaluate correctness and security in multi-turn, real-world coding workflows.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP Architecture and Paired Weight Sharing</title>
<link>https://arxiv.org/abs/2510.13860</link>
<guid>https://arxiv.org/abs/2510.13860</guid>
<content:encoded><![CDATA[
arXiv:2510.13860v1 Announce Type: new 
Abstract: While the transformer architecture has achieved state-of-the-art performance on natural language processing tasks, these models impose substantial memory and computational overhead. Recent research has identified significant architectural redundancies within these models, presenting opportunities for optimization without compromising performance. Taking insights from research in AI interpretability and inference-time layer pruning, we introduce an efficient language model architecture, referred to as ShishuLM, which reduces both the parameter count and Key-Value (KV) cache requirements. Given the increasing importance of Small Language Models (SLMs) in agentic AI systems, we evaluate our approach on two SLMs of different scales. Our analysis reveals that for moderate-context scenarios, normalization coupled with attention computation is roughly linear with the input, enabling entire transformer blocks to be approximated through Multi-Layer Perceptrons (MLPs). Our results show that ShishuLM provides up to 25% reduction in memory requirements and up to 40% improvement in latency during both training and inference, compared to parent models. Our experimental and analytical findings provide insights towards building more efficient SLM architectures from a pre-training standpoint.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiFoodhat: A potential new paradigm for intelligent food quality inspection</title>
<link>https://arxiv.org/abs/2510.13889</link>
<guid>https://arxiv.org/abs/2510.13889</guid>
<content:encoded><![CDATA[
arXiv:2510.13889v1 Announce Type: new 
Abstract: Food image classification plays a vital role in intelligent food quality inspection, dietary assessment, and automated monitoring. However, most existing supervised models rely heavily on large labeled datasets and exhibit limited generalization to unseen food categories. To overcome these challenges, this study introduces MultiFoodChat, a dialogue-driven multi-agent reasoning framework for zero-shot food recognition. The framework integrates vision-language models (VLMs) and large language models (LLMs) to enable collaborative reasoning through multi-round visual-textual dialogues. An Object Perception Token (OPT) captures fine-grained visual attributes, while an Interactive Reasoning Agent (IRA) dynamically interprets contextual cues to refine predictions. This multi-agent design allows flexible and human-like understanding of complex food scenes without additional training or manual annotations. Experiments on multiple public food datasets demonstrate that MultiFoodChat achieves superior recognition accuracy and interpretability compared with existing unsupervised and few-shot methods, highlighting its potential as a new paradigm for intelligent food quality inspection and analysis.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences</title>
<link>https://arxiv.org/abs/2510.13900</link>
<guid>https://arxiv.org/abs/2510.13900</guid>
<content:encoded><![CDATA[
arXiv:2510.13900v1 Announce Type: new 
Abstract: Finetuning on narrow domains has become an essential tool to adapt Large Language Models (LLMs) to specific tasks and to create models with known unusual properties that are useful for research. We show that narrow finetuning creates strong biases in LLM activations that can be interpreted to understand the finetuning domain. These biases can be discovered using simple tools from model diffing - the study of differences between models before and after finetuning. In particular, analyzing activation differences on the first few tokens of random text and steering by adding this difference to the model activations produces text similar to the format and general content of the finetuning data. We demonstrate that these analyses contain crucial information by creating an LLM-based interpretability agent to understand the finetuning domain. With access to the bias, the agent performs significantly better compared to baseline agents using simple prompting. Our analysis spans synthetic document finetuning for false facts, emergent misalignment, subliminal learning, and taboo word guessing game models across different architectures (Gemma, LLaMA, Qwen) and scales (1B to 32B parameters). We suspect these biases reflect overfitting and find that mixing pretraining data into the finetuning corpus largely removes them, though residual risks may remain. Our work (1) demonstrates that narrowly finetuned models have salient traces of their training objective in their activations and suggests ways to improve how they are trained, (2) warns AI safety and interpretability researchers that the common practice of using such models as a proxy for studying broader finetuning (e.g., chat-tuning) might not be realistic, and (3) highlights the need for deeper investigation into the effects of narrow finetuning and development of truly realistic case studies for model-diffing, safety and interpretability research.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benefits and Limitations of Communication in Multi-Agent Reasoning</title>
<link>https://arxiv.org/abs/2510.13903</link>
<guid>https://arxiv.org/abs/2510.13903</guid>
<content:encoded><![CDATA[
arXiv:2510.13903v1 Announce Type: new 
Abstract: Chain-of-thought prompting has popularized step-by-step reasoning in large language models, yet model performance still degrades as problem complexity and context length grow. By decomposing difficult tasks with long contexts into shorter, manageable ones, recent multi-agent paradigms offer a promising near-term solution to this problem. However, the fundamental capacities of such systems are poorly understood. In this work, we propose a theoretical framework to analyze the expressivity of multi-agent systems. We apply our framework to three algorithmic families: state tracking, recall, and $k$-hop reasoning. We derive bounds on (i) the number of agents required to solve the task exactly, (ii) the quantity and structure of inter-agent communication, and (iii) the achievable speedups as problem size and context scale. Our results identify regimes where communication is provably beneficial, delineate tradeoffs between agent count and bandwidth, and expose intrinsic limitations when either resource is constrained. We complement our theoretical analysis with a set of experiments on pretrained LLMs using controlled synthetic benchmarks. Empirical outcomes confirm the tradeoffs between key quantities predicted by our theory. Collectively, our analysis offers principled guidance for designing scalable multi-agent reasoning systems.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems</title>
<link>https://arxiv.org/abs/2510.13910</link>
<guid>https://arxiv.org/abs/2510.13910</guid>
<content:encoded><![CDATA[
arXiv:2510.13910v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) mitigates key limitations of Large Language Models (LLMs)-such as factual errors, outdated knowledge, and hallucinations-by dynamically retrieving external information. Recent work extends this paradigm through agentic RAG systems, where LLMs act as agents to iteratively plan, retrieve, and reason over complex queries. However, these systems still struggle with challenging multi-hop questions, and their intermediate reasoning capabilities remain underexplored. To address this, we propose RAGCap-Bench, a capability-oriented benchmark for fine-grained evaluation of intermediate tasks in agentic RAG workflows. We analyze outputs from state-of-the-art systems to identify common tasks and the core capabilities required for their execution, then construct a taxonomy of typical LLM errors to design targeted evaluation questions. Experiments show that "slow-thinking" models with stronger RAGCap performance achieve better end-to-end results, underscoring the benchmark's validity and the importance of enhancing these intermediate capabilities.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthesizing Agentic Data for Web Agents with Progressive Difficulty Enhancement Mechanisms</title>
<link>https://arxiv.org/abs/2510.13913</link>
<guid>https://arxiv.org/abs/2510.13913</guid>
<content:encoded><![CDATA[
arXiv:2510.13913v1 Announce Type: new 
Abstract: Web-based 'deep research' agents aim to solve complex question - answering tasks through long-horizon interactions with online tools. These tasks remain challenging, as the underlying language models are often not optimized for long-horizon reasoning and exploration. Prior work has proposed workflows for constructing instruction-tuning datasets, often leveraging knowledge graphs. However, such methods typically lack fine-grained control over difficulty and quality, yielding synthetic data that falls short of capturing the complexity required for long-horizon reasoning. Furthermore, many studies conflate data and training effects by comparing models trained under different optimization recipes, making it difficult to isolate and evaluate the effectiveness of the data itself. We introduce a two-pronged data synthesis pipeline that generates question - answer pairs by progressively increasing task complexity until a frontier baseline web agent fails. The baseline agent plays multiple roles in this process: attempting the questions, validating factuality, checking for alternative answers, and enforcing filtering. To evaluate the effectiveness of our synthesis methods, we adopt a controlled training setup based on distillation from strong web agents. Experiments across multiple web-based benchmarks show that our dataset - despite being smaller - enables the training of more effective web agents than existing datasets. In particular, our data exhibits twice the diversity in tool-use actions, allowing models trained on it to achieve stronger performance while avoiding repetitive tool-calling behaviors.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FACTS: Table Summarization via Offline Template Generation with Agentic Workflows</title>
<link>https://arxiv.org/abs/2510.13920</link>
<guid>https://arxiv.org/abs/2510.13920</guid>
<content:encoded><![CDATA[
arXiv:2510.13920v1 Announce Type: new 
Abstract: Query-focused table summarization requires generating natural language summaries of tabular data conditioned on a user query, enabling users to access insights beyond fact retrieval. Existing approaches face key limitations: table-to-text models require costly fine-tuning and struggle with complex reasoning, prompt-based LLM methods suffer from token-limit and efficiency issues while exposing sensitive data, and prior agentic pipelines often rely on decomposition, planning, or manual templates that lack robustness and scalability. To mitigate these issues, we introduce an agentic workflow, FACTS, a Fast, Accurate, and Privacy-Compliant Table Summarization approach via Offline Template Generation. FACTS produces offline templates, consisting of SQL queries and Jinja2 templates, which can be rendered into natural language summaries and are reusable across multiple tables sharing the same schema. It enables fast summarization through reusable offline templates, accurate outputs with executable SQL queries, and privacy compliance by sending only table schemas to LLMs. Evaluations on widely-used benchmarks show that FACTS consistently outperforms baseline methods, establishing it as a practical solution for real-world query-focused table summarization.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM-Powered AI Agent Framework for Holistic IoT Traffic Interpretation</title>
<link>https://arxiv.org/abs/2510.13925</link>
<guid>https://arxiv.org/abs/2510.13925</guid>
<content:encoded><![CDATA[
arXiv:2510.13925v1 Announce Type: new 
Abstract: Internet of Things (IoT) networks generate diverse and high-volume traffic that reflects both normal activity and potential threats. Deriving meaningful insight from such telemetry requires cross-layer interpretation of behaviors, protocols, and context rather than isolated detection. This work presents an LLM-powered AI agent framework that converts raw packet captures into structured and semantically enriched representations for interactive analysis. The framework integrates feature extraction, transformer-based anomaly detection, packet and flow summarization, threat intelligence enrichment, and retrieval-augmented question answering. An AI agent guided by a large language model performs reasoning over the indexed traffic artifacts, assembling evidence to produce accurate and human-readable interpretations. Experimental evaluation on multiple IoT captures and six open models shows that hybrid retrieval, which combines lexical and semantic search with reranking, substantially improves BLEU, ROUGE, METEOR, and BERTScore results compared with dense-only retrieval. System profiling further indicates low CPU, GPU, and memory overhead, demonstrating that the framework achieves holistic and efficient interpretation of IoT network traffic.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinDeepResearch: Evaluating Deep Research Agents in Rigorous Financial Analysis</title>
<link>https://arxiv.org/abs/2510.13936</link>
<guid>https://arxiv.org/abs/2510.13936</guid>
<content:encoded><![CDATA[
arXiv:2510.13936v1 Announce Type: new 
Abstract: Deep Research (DR) agents, powered by advanced Large Language Models (LLMs), have recently garnered increasing attention for their capability in conducting complex research tasks. However, existing literature lacks a rigorous and systematic evaluation of DR Agent's capabilities in critical research analysis. To address this gap, we first propose HisRubric, a novel evaluation framework with a hierarchical analytical structure and a fine-grained grading rubric for rigorously assessing DR agents' capabilities in corporate financial analysis. This framework mirrors the professional analyst's workflow, progressing from data recognition to metric calculation, and finally to strategic summarization and interpretation. Built on this framework, we construct a FinDeepResearch benchmark that comprises 64 listed companies from 8 financial markets across 4 languages, encompassing a total of 15,808 grading items. We further conduct extensive experiments on the FinDeepResearch using 16 representative methods, including 6 DR agents, 5 LLMs equipped with both deep reasoning and search capabilities, and 5 LLMs with deep reasoning capabilities only. The results reveal the strengths and limitations of these approaches across diverse capabilities, financial markets, and languages, offering valuable insights for future research and development. The benchmark and evaluation code will be made publicly available.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Static Sandboxes Are Inadequate: Modeling Societal Complexity Requires Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations</title>
<link>https://arxiv.org/abs/2510.13982</link>
<guid>https://arxiv.org/abs/2510.13982</guid>
<content:encoded><![CDATA[
arXiv:2510.13982v1 Announce Type: new 
Abstract: What if artificial agents could not just communicate, but also evolve, adapt, and reshape their worlds in ways we cannot fully predict? With llm now powering multi-agent systems and social simulations, we are witnessing new possibilities for modeling open-ended, ever-changing environments. Yet, most current simulations remain constrained within static sandboxes, characterized by predefined tasks, limited dynamics, and rigid evaluation criteria. These limitations prevent them from capturing the complexity of real-world societies. In this paper, we argue that static, task-specific benchmarks are fundamentally inadequate and must be rethought. We critically review emerging architectures that blend llm with multi-agent dynamics, highlight key hurdles such as balancing stability and diversity, evaluating unexpected behaviors, and scaling to greater complexity, and introduce a fresh taxonomy for this rapidly evolving field. Finally, we present a research roadmap centered on open-endedness, continuous co-evolution, and the development of resilient, socially aligned AI ecosystems. \textbf{We call on the community to move beyond static paradigms and help shape the next generation of adaptive, socially-aware multi-agent simulations.}
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stop Reducing Responsibility in LLM-Powered Multi-Agent Systems to Local Alignment</title>
<link>https://arxiv.org/abs/2510.14008</link>
<guid>https://arxiv.org/abs/2510.14008</guid>
<content:encoded><![CDATA[
arXiv:2510.14008v1 Announce Type: new 
Abstract: LLM-powered Multi-Agent Systems (LLM-MAS) unlock new potentials in distributed reasoning, collaboration, and task generalization but also introduce additional risks due to unguaranteed agreement, cascading uncertainty, and adversarial vulnerabilities. We argue that ensuring responsible behavior in such systems requires a paradigm shift: from local, superficial agent-level alignment to global, systemic agreement. We conceptualize responsibility not as a static constraint but as a lifecycle-wide property encompassing agreement, uncertainty, and security, each requiring the complementary integration of subjective human-centered values and objective verifiability. Furthermore, a dual-perspective governance framework that combines interdisciplinary design with human-AI collaborative oversight is essential for tracing and ensuring responsibility throughout the lifecycle of LLM-MAS. Our position views LLM-MAS not as loose collections of agents, but as unified, dynamic socio-technical systems that demand principled mechanisms to support each dimension of responsibility and enable ethically aligned, verifiably coherent, and resilient behavior for sustained, system-wide agreement.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CausalVerse: Benchmarking Causal Representation Learning with Configurable High-Fidelity Simulations</title>
<link>https://arxiv.org/abs/2510.14049</link>
<guid>https://arxiv.org/abs/2510.14049</guid>
<content:encoded><![CDATA[
arXiv:2510.14049v1 Announce Type: new 
Abstract: Causal Representation Learning (CRL) aims to uncover the data-generating process and identify the underlying causal variables and relations, whose evaluation remains inherently challenging due to the requirement of known ground-truth causal variables and causal structure. Existing evaluations often rely on either simplistic synthetic datasets or downstream performance on real-world tasks, generally suffering a dilemma between realism and evaluative precision. In this paper, we introduce a new benchmark for CRL using high-fidelity simulated visual data that retains both realistic visual complexity and, more importantly, access to ground-truth causal generating processes. The dataset comprises around 200 thousand images and 3 million video frames across 24 sub-scenes in four domains: static image generation, dynamic physical simulations, robotic manipulations, and traffic situation analysis. These scenarios range from static to dynamic settings, simple to complex structures, and single to multi-agent interactions, offering a comprehensive testbed that hopefully bridges the gap between rigorous evaluation and real-world applicability. In addition, we provide flexible access to the underlying causal structures, allowing users to modify or configure them to align with the required assumptions in CRL, such as available domain labels, temporal dependencies, or intervention histories. Leveraging this benchmark, we evaluated representative CRL methods across diverse paradigms and offered empirical insights to assist practitioners and newcomers in choosing or extending appropriate CRL frameworks to properly address specific types of real problems that can benefit from the CRL perspective. Welcome to visit our: Project page:https://causal-verse.github.io/, Dataset:https://huggingface.co/CausalVerse.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Fair Consensus Statements with Social Choice on Token-Level MDPs</title>
<link>https://arxiv.org/abs/2510.14106</link>
<guid>https://arxiv.org/abs/2510.14106</guid>
<content:encoded><![CDATA[
arXiv:2510.14106v1 Announce Type: new 
Abstract: Current frameworks for consensus statement generation with large language models lack the inherent structure needed to provide provable fairness guarantees when aggregating diverse free-form opinions. We model the task as a multi-objective, token-level Markov Decision Process (MDP), where each objective corresponds to an agent's preference. Token-level rewards for each agent are derived from their policy (e.g., a personalized language model). This approach utilizes the finding that such policies implicitly define optimal Q-functions, providing a principled way to quantify rewards at each generation step without a value function (Rafailov et al., 2024). This MDP formulation creates a formal structure amenable to analysis using principles from social choice theory. We propose two approaches grounded in social choice theory. First, we propose a stochastic generation policy guaranteed to be in the ex-ante core, extending core stability concepts from voting theory to text generation. This policy is derived from an underlying distribution over complete statements that maximizes proportional fairness (Nash Welfare). Second, for generating a single statement, we target the maximization of egalitarian welfare using search algorithms within the MDP framework. Empirically, experiments using language models to instantiate agent policies show that search guided by the egalitarian objective generates consensus statements with improved worst-case agent alignment compared to baseline methods, including the Habermas Machine (Tessler et al., 2024).
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management</title>
<link>https://arxiv.org/abs/2510.14112</link>
<guid>https://arxiv.org/abs/2510.14112</guid>
<content:encoded><![CDATA[
arXiv:2510.14112v1 Announce Type: new 
Abstract: Building energy management is essential for achieving carbon reduction goals, improving occupant comfort, and reducing energy costs. Coordinated building energy management faces critical challenges in exploiting spatial-temporal dependencies while ensuring operational safety across multi-building systems. Current multi-building energy systems face three key challenges: insufficient spatial-temporal information exploitation, lack of rigorous safety guarantees, and system complexity. This paper proposes Spatial-Temporal Enhanced Safe Multi-Agent Coordination (STEMS), a novel safety-constrained multi-agent reinforcement learning framework for coordinated building energy management. STEMS integrates two core components: (1) a spatial-temporal graph representation learning framework using a GCN-Transformer fusion architecture to capture inter-building relationships and temporal patterns, and (2) a safety-constrained multi-agent RL algorithm incorporating Control Barrier Functions to provide mathematical safety guarantees. Extensive experiments on real-world building datasets demonstrate STEMS's superior performance over existing methods, showing that STEMS achieves 21% cost reduction, 18% emission reduction, and dramatically reduces safety violations from 35.1% to 5.6% while maintaining optimal comfort with only 0.13 discomfort proportion. The framework also demonstrates strong robustness during extreme weather conditions and maintains effectiveness across different building types.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cortex: Workflow-Aware Resource Pooling and Scheduling for Agentic Serving</title>
<link>https://arxiv.org/abs/2510.14126</link>
<guid>https://arxiv.org/abs/2510.14126</guid>
<content:encoded><![CDATA[
arXiv:2510.14126v1 Announce Type: new 
Abstract: We introduce Cortex, a prototype workflow-aware serving platform designed for agentic workloads. The core principle of Cortex is stage isolation: it provisions dedicated resource pools for each distinct stage of an agentic workflow. This simple yet powerful strategy mitigates inter-stage interference in compute and memory, leading to better KV cache utilization, higher throughput, and more predictable performance. By customizing resource allocation and scheduling within each distinct stage of agentic workflows, Cortex lays the groundwork for more advanced, agent-native serving paradigms, including malleable resource management, speculative execution of workflow branches, and a shared, multi-tiered cache for "agentic state."
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems</title>
<link>https://arxiv.org/abs/2510.14133</link>
<guid>https://arxiv.org/abs/2510.14133</guid>
<content:encoded><![CDATA[
arXiv:2510.14133v1 Announce Type: new 
Abstract: Agentic AI systems, which leverage multiple autonomous agents and Large Language Models (LLMs), are increasingly used to address complex, multi-step tasks. The safety, security, and functionality of these systems are critical, especially in high-stakes applications. However, the current ecosystem of inter-agent communication is fragmented, with protocols such as the Model Context Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol for coordination being analyzed in isolation. This fragmentation creates a semantic gap that prevents the rigorous analysis of system properties and introduces risks such as architectural misalignment and exploitable coordination issues. To address these challenges, we introduce a modeling framework for agentic AI systems composed of two foundational models. The first, the host agent model, formalizes the top-level entity that interacts with the user, decomposes tasks, and orchestrates their execution by leveraging external agents and tools. The second, the task lifecycle model, details the states and transitions of individual sub-tasks from creation to completion, providing a fine-grained view of task management and error handling. Together, these models provide a unified semantic framework for reasoning about the behavior of multi-AI agent systems. Grounded in this framework, we define 17 properties for the host agent and 14 for the task lifecycle, categorized into liveness, safety, completeness, and fairness. Expressed in temporal logic, these properties enable formal verification of system behavior, detection of coordination edge cases, and prevention of deadlocks and security vulnerabilities. Through this effort, we introduce the first rigorously grounded, domain-agnostic framework for the systematic analysis, design, and deployment of correct, reliable, and robust agentic AI systems.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeEvolve: An open source evolutionary coding agent for algorithm discovery and optimization</title>
<link>https://arxiv.org/abs/2510.14150</link>
<guid>https://arxiv.org/abs/2510.14150</guid>
<content:encoded><![CDATA[
arXiv:2510.14150v1 Announce Type: new 
Abstract: In this work, we introduce CodeEvolve, an open-source evolutionary coding agent that unites Large Language Models (LLMs) with genetic algorithms to solve complex computational problems. Our framework adapts powerful evolutionary concepts to the LLM domain, building upon recent methods for generalized scientific discovery. CodeEvolve employs an island-based genetic algorithm to maintain population diversity and increase throughput, introduces a novel inspiration-based crossover mechanism that leverages the LLMs context window to combine features from successful solutions, and implements meta-prompting strategies for dynamic exploration of the solution space. We conduct a rigorous evaluation of CodeEvolve on a subset of the mathematical benchmarks used to evaluate Google DeepMind's closed-source AlphaEvolve. Our findings show that our method surpasses AlphaEvolve's performance on several challenging problems. To foster collaboration and accelerate progress, we release our complete framework as an open-source repository.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining Reinforcement Learning and Behavior Trees for NPCs in Video Games with AMD Schola</title>
<link>https://arxiv.org/abs/2510.14154</link>
<guid>https://arxiv.org/abs/2510.14154</guid>
<content:encoded><![CDATA[
arXiv:2510.14154v1 Announce Type: new 
Abstract: While the rapid advancements in the reinforcement learning (RL) research community have been remarkable, the adoption in commercial video games remains slow. In this paper, we outline common challenges the Game AI community faces when using RL-driven NPCs in practice, and highlight the intersection of RL with traditional behavior trees (BTs) as a crucial juncture to be explored further. Although the BT+RL intersection has been suggested in several research papers, its adoption is rare. We demonstrate the viability of this approach using AMD Schola -- a plugin for training RL agents in Unreal Engine -- by creating multi-task NPCs in a complex 3D environment inspired by the commercial video game ``The Last of Us". We provide detailed methodologies for jointly training RL models with BTs while showcasing various skills.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAFA: A Multi-Agent Framework for Enterprise-Scale Annotation with Configurable Task Adaptation</title>
<link>https://arxiv.org/abs/2510.14184</link>
<guid>https://arxiv.org/abs/2510.14184</guid>
<content:encoded><![CDATA[
arXiv:2510.14184v1 Announce Type: new 
Abstract: We present MAFA (Multi-Agent Framework for Annotation), a production-deployed system that transforms enterprise-scale annotation workflows through configurable multi-agent collaboration. Addressing the critical challenge of annotation backlogs in financial services, where millions of customer utterances require accurate categorization, MAFA combines specialized agents with structured reasoning and a judge-based consensus mechanism. Our framework uniquely supports dynamic task adaptation, allowing organizations to define custom annotation types (FAQs, intents, entities, or domain-specific categories) through configuration rather than code changes. Deployed at JP Morgan Chase, MAFA has eliminated a 1 million utterance backlog while achieving, on average, 86% agreement with human annotators, annually saving over 5,000 hours of manual annotation work. The system processes utterances with annotation confidence classifications, which are typically 85% high, 10% medium, and 5% low across all datasets we tested. This enables human annotators to focus exclusively on ambiguous and low-coverage cases. We demonstrate MAFA's effectiveness across multiple datasets and languages, showing consistent improvements over traditional and single-agent annotation baselines: 13.8% higher Top-1 accuracy, 15.1% improvement in Top-5 accuracy, and 16.9% better F1 in our internal intent classification dataset and similar gains on public benchmarks. This work bridges the gap between theoretical multi-agent systems and practical enterprise deployment, providing a blueprint for organizations facing similar annotation challenges.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DPRF: A Generalizable Dynamic Persona Refinement Framework for Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents and Humans</title>
<link>https://arxiv.org/abs/2510.14205</link>
<guid>https://arxiv.org/abs/2510.14205</guid>
<content:encoded><![CDATA[
arXiv:2510.14205v1 Announce Type: new 
Abstract: The emerging large language model role-playing agents (LLM RPAs) aim to simulate individual human behaviors, but the persona fidelity is often undermined by manually-created profiles (e.g., cherry-picked information and personality characteristics) without validating the alignment with the target individuals. To address this limitation, our work introduces the Dynamic Persona Refinement Framework (DPRF).DPRF aims to optimize the alignment of LLM RPAs' behaviors with those of target individuals by iteratively identifying the cognitive divergence, either through free-form or theory-grounded, structured analysis, between generated behaviors and human ground truth, and refining the persona profile to mitigate these divergences.We evaluate DPRF with five LLMs on four diverse behavior-prediction scenarios: formal debates, social media posts with mental health issues, public interviews, and movie reviews.DPRF can consistently improve behavioral alignment considerably over baseline personas and generalizes across models and scenarios.Our work provides a robust methodology for creating high-fidelity persona profiles and enhancing the validity of downstream applications, such as user simulation, social studies, and personalized AI.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Echoes of Human Malice in Agents: Benchmarking LLMs for Multi-Turn Online Harassment Attacks</title>
<link>https://arxiv.org/abs/2510.14207</link>
<guid>https://arxiv.org/abs/2510.14207</guid>
<content:encoded><![CDATA[
arXiv:2510.14207v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents are powering a growing share of interactive web applications, yet remain vulnerable to misuse and harm. Prior jailbreak research has largely focused on single-turn prompts, whereas real harassment often unfolds over multi-turn interactions. In this work, we present the Online Harassment Agentic Benchmark consisting of: (i) a synthetic multi-turn harassment conversation dataset, (ii) a multi-agent (e.g., harasser, victim) simulation informed by repeated game theory, (iii) three jailbreak methods attacking agents across memory, planning, and fine-tuning, and (iv) a mixed-methods evaluation framework. We utilize two prominent LLMs, LLaMA-3.1-8B-Instruct (open-source) and Gemini-2.0-flash (closed-source). Our results show that jailbreak tuning makes harassment nearly guaranteed with an attack success rate of 95.78--96.89% vs. 57.25--64.19% without tuning in Llama, and 99.33% vs. 98.46% without tuning in Gemini, while sharply reducing refusal rate to 1-2% in both models. The most prevalent toxic behaviors are Insult with 84.9--87.8% vs. 44.2--50.8% without tuning, and Flaming with 81.2--85.1% vs. 31.5--38.8% without tuning, indicating weaker guardrails compared to sensitive categories such as sexual or racial harassment. Qualitative evaluation further reveals that attacked agents reproduce human-like aggression profiles, such as Machiavellian/psychopathic patterns under planning, and narcissistic tendencies with memory. Counterintuitively, closed-source and open-source models exhibit distinct escalation trajectories across turns, with closed-source models showing significant vulnerability. Overall, our findings show that multi-turn and theory-grounded attacks not only succeed at high rates but also mimic human-like harassment dynamics, motivating the development of robust safety guardrails to ultimately keep online platforms safe and responsible.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiveResearchBench: A Live Benchmark for User-Centric Deep Research in the Wild</title>
<link>https://arxiv.org/abs/2510.14240</link>
<guid>https://arxiv.org/abs/2510.14240</guid>
<content:encoded><![CDATA[
arXiv:2510.14240v1 Announce Type: new 
Abstract: Deep research -- producing comprehensive, citation-grounded reports by searching and synthesizing information from hundreds of live web sources -- marks an important frontier for agentic systems. To rigorously evaluate this ability, four principles are essential: tasks should be (1) user-centric, reflecting realistic information needs, (2) dynamic, requiring up-to-date information beyond parametric knowledge, (3) unambiguous, ensuring consistent interpretation across users, and (4) multi-faceted and search-intensive, requiring search over numerous web sources and in-depth analysis. Existing benchmarks fall short of these principles, often focusing on narrow domains or posing ambiguous questions that hinder fair comparison. Guided by these principles, we introduce LiveResearchBench, a benchmark of 100 expert-curated tasks spanning daily life, enterprise, and academia, each requiring extensive, dynamic, real-time web search and synthesis. Built with over 1,500 hours of human labor, LiveResearchBench provides a rigorous basis for systematic evaluation. To evaluate citation-grounded long-form reports, we introduce DeepEval, a comprehensive suite covering both content- and report-level quality, including coverage, presentation, citation accuracy and association, consistency and depth of analysis. DeepEval integrates four complementary evaluation protocols, each designed to ensure stable assessment and high agreement with human judgments. Using LiveResearchBench and DeepEval, we conduct a comprehensive evaluation of 17 frontier deep research systems, including single-agent web search, single-agent deep research, and multi-agent systems. Our analysis reveals current strengths, recurring failure modes, and key system components needed to advance reliable, insightful deep research.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Regularized Distributionally Robust Markov Decision Processes with Linear Function Approximation</title>
<link>https://arxiv.org/abs/2510.14246</link>
<guid>https://arxiv.org/abs/2510.14246</guid>
<content:encoded><![CDATA[
arXiv:2510.14246v1 Announce Type: new 
Abstract: Decision-making under distribution shift is a central challenge in reinforcement learning (RL), where training and deployment environments differ. We study this problem through the lens of robust Markov decision processes (RMDPs), which optimize performance against adversarial transition dynamics. Our focus is the online setting, where the agent has only limited interaction with the environment, making sample efficiency and exploration especially critical. Policy optimization, despite its success in standard RL, remains theoretically and empirically underexplored in robust RL. To bridge this gap, we propose \textbf{D}istributionally \textbf{R}obust \textbf{R}egularized \textbf{P}olicy \textbf{O}ptimization algorithm (DR-RPO), a model-free online policy optimization method that learns robust policies with sublinear regret. To enable tractable optimization within the softmax policy class, DR-RPO incorporates reference-policy regularization, yielding RMDP variants that are doubly constrained in both transitions and policies. To scale to large state-action spaces, we adopt the $d$-rectangular linear MDP formulation and combine linear function approximation with an upper confidence bonus for optimistic exploration. We provide theoretical guarantees showing that policy optimization can achieve polynomial suboptimality bounds and sample efficiency in robust RL, matching the performance of value-based approaches. Finally, empirical results across diverse domains corroborate our theory and demonstrate the robustness of DR-RPO.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Agentic Self-Learning LLMs in Search Environment</title>
<link>https://arxiv.org/abs/2510.14253</link>
<guid>https://arxiv.org/abs/2510.14253</guid>
<content:encoded><![CDATA[
arXiv:2510.14253v1 Announce Type: new 
Abstract: We study whether self-learning can scale LLM-based agents without relying on human-curated datasets or predefined rule-based rewards. Through controlled experiments in a search-agent setting, we identify two key determinants of scalable agent training: the source of reward signals and the scale of agent task data. We find that rewards from a Generative Reward Model (GRM) outperform rigid rule-based signals for open-domain learning, and that co-evolving the GRM with the policy further boosts performance. Increasing the volume of agent task data-even when synthetically generated-substantially enhances agentic capabilities. Building on these insights, we propose \textbf{Agentic Self-Learning} (ASL), a fully closed-loop, multi-role reinforcement learning framework that unifies task generation, policy execution, and evaluation within a shared tool environment and LLM backbone. ASL coordinates a Prompt Generator, a Policy Model, and a Generative Reward Model to form a virtuous cycle of harder task setting, sharper verification, and stronger solving. Empirically, ASL delivers steady, round-over-round gains, surpasses strong RLVR baselines (e.g., Search-R1) that plateau or degrade, and continues improving under zero-labeled-data conditions, indicating superior sample efficiency and robustness. We further show that GRM verification capacity is the main bottleneck: if frozen, it induces reward hacking and stalls progress; continual GRM training on the evolving data distribution mitigates this, and a small late-stage injection of real verification data raises the performance ceiling. This work establishes reward source and data scale as critical levers for open-domain agent learning and demonstrates the efficacy of multi-role co-evolution for scalable, self-improving agents. The data and code of this paper are released at https://github.com/forangel2014/Towards-Agentic-Self-Learning
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement Learning Framework for Stock Trading</title>
<link>https://arxiv.org/abs/2510.14264</link>
<guid>https://arxiv.org/abs/2510.14264</guid>
<content:encoded><![CDATA[
arXiv:2510.14264v1 Announce Type: new 
Abstract: While Large Language Model (LLM) agents show promise in automated trading, they still face critical limitations. Prominent multi-agent frameworks often suffer from inefficiency, produce inconsistent signals, and lack the end-to-end optimization required to learn a coherent strategy from market feedback. To address this, we introduce AlphaQuanter, a single-agent framework that uses reinforcement learning (RL) to learn a dynamic policy over a transparent, tool-augmented decision workflow, which empowers a single agent to autonomously orchestrate tools and proactively acquire information on demand, establishing a transparent and auditable reasoning process. Extensive experiments demonstrate that AlphaQuanter achieves state-of-the-art performance on key financial metrics. Moreover, its interpretable reasoning reveals sophisticated strategies, offering novel and valuable insights for human traders. Our code for data acquisition and agent training is publicly available at: https://github.com/AlphaQuanter/AlphaQuanter
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenLARP: Enabling Immersive Live Action Role-Play through LLM-Generated Worlds and Characters</title>
<link>https://arxiv.org/abs/2510.14277</link>
<guid>https://arxiv.org/abs/2510.14277</guid>
<content:encoded><![CDATA[
arXiv:2510.14277v1 Announce Type: new 
Abstract: We introduce GenLARP, a virtual reality (VR) system that transforms personalized stories into immersive live action role-playing (LARP) experiences. GenLARP enables users to act as both creators and players, allowing them to design characters based on their descriptions and live in the story world. Generative AI and agents powered by Large Language Models (LLMs) enrich these experiences.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering</title>
<link>https://arxiv.org/abs/2510.14278</link>
<guid>https://arxiv.org/abs/2510.14278</guid>
<content:encoded><![CDATA[
arXiv:2510.14278v1 Announce Type: new 
Abstract: Retrieval plays a central role in multi-hop question answering (QA), where answering complex questions requires gathering multiple pieces of evidence. We introduce an Agentic Retrieval System that leverages large language models (LLMs) in a structured loop to retrieve relevant evidence with high precision and recall. Our framework consists of three specialized agents: a Question Analyzer that decomposes a multi-hop question into sub-questions, a Selector that identifies the most relevant context for each sub-question (focusing on precision), and an Adder that brings in any missing evidence (focusing on recall). The iterative interaction between Selector and Adder yields a compact yet comprehensive set of supporting passages. In particular, it achieves higher retrieval accuracy while filtering out distracting content, enabling downstream QA models to surpass full-context answer accuracy while relying on significantly less irrelevant information. Experiments on four multi-hop QA benchmarks -- HotpotQA, 2WikiMultiHopQA, MuSiQue, and MultiHopRAG -- demonstrates that our approach consistently outperforms strong baselines.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constraint-Driven Small Language Models Based on Agent and OpenAlex Knowledge Graph: Mining Conceptual Pathways and Discovering Innovation Points in Academic Papers</title>
<link>https://arxiv.org/abs/2510.14303</link>
<guid>https://arxiv.org/abs/2510.14303</guid>
<content:encoded><![CDATA[
arXiv:2510.14303v1 Announce Type: new 
Abstract: In recent years, the rapid increase in academic publications across various fields has posed severe challenges for academic paper analysis: scientists struggle to timely and comprehensively track the latest research findings and methodologies. Key concept extraction has proven to be an effective analytical paradigm, and its automation has been achieved with the widespread application of language models in industrial and scientific domains. However, existing paper databases are mostly limited to similarity matching and basic classification of key concepts, failing to deeply explore the relational networks between concepts. This paper is based on the OpenAlex opensource knowledge graph. By analyzing nearly 8,000 open-source paper data from Novosibirsk State University, we discovered a strong correlation between the distribution patterns of paper key concept paths and both innovation points and rare paths. We propose a prompt engineering-based key concept path analysis method. This method leverages small language models to achieve precise key concept extraction and innovation point identification, and constructs an agent based on a knowledge graph constraint mechanism to enhance analysis accuracy. Through fine-tuning of the Qwen and DeepSeek models, we achieved significant improvements in accuracy, with the models publicly available on the Hugging Face platform.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReUseIt: Synthesizing Reusable AI Agent Workflows for Web Automation</title>
<link>https://arxiv.org/abs/2510.14308</link>
<guid>https://arxiv.org/abs/2510.14308</guid>
<content:encoded><![CDATA[
arXiv:2510.14308v1 Announce Type: new 
Abstract: AI-powered web agents have the potential to automate repetitive tasks, such as form filling, information retrieval, and scheduling, but they struggle to reliably execute these tasks without human intervention, requiring users to provide detailed guidance during every run. We address this limitation by automatically synthesizing reusable workflows from an agent's successful and failed attempts. These workflows incorporate execution guards that help agents detect and fix errors while keeping users informed of progress and issues. Our approach enables agents to successfully complete repetitive tasks of the same type with minimal intervention, increasing the success rates from 24.2% to 70.1% across fifteen tasks. To evaluate this approach, we invited nine users and found that our agent helped them complete web tasks with a higher success rate and less guidance compared to two baseline methods, as well as allowed users to easily monitor agent behavior and understand failures.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies</title>
<link>https://arxiv.org/abs/2510.14312</link>
<guid>https://arxiv.org/abs/2510.14312</guid>
<content:encoded><![CDATA[
arXiv:2510.14312v1 Announce Type: new 
Abstract: A multi-agent system (MAS) powered by large language models (LLMs) can automate tedious user tasks such as meeting scheduling that requires inter-agent collaboration. LLMs enable nuanced protocols that account for unstructured private data, user constraints, and preferences. However, this design introduces new risks, including misalignment and attacks by malicious parties that compromise agents or steal user data. In this paper, we propose the Terrarium framework for fine-grained study on safety, privacy, and security in LLM-based MAS. We repurpose the blackboard design, an early approach in multi-agent systems, to create a modular, configurable testbed for multi-agent collaboration. We identify key attack vectors such as misalignment, malicious agents, compromised communication, and data poisoning. We implement three collaborative MAS scenarios with four representative attacks to demonstrate the framework's flexibility. By providing tools to rapidly prototype, evaluate, and iterate on defenses and designs, Terrarium aims to accelerate progress toward trustworthy multi-agent systems.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Measuring in Reinforcement Learning With Delayed Negative Effects</title>
<link>https://arxiv.org/abs/2510.14315</link>
<guid>https://arxiv.org/abs/2510.14315</guid>
<content:encoded><![CDATA[
arXiv:2510.14315v1 Announce Type: new 
Abstract: Measuring states in reinforcement learning (RL) can be costly in real-world settings and may negatively influence future outcomes. We introduce the Actively Observable Markov Decision Process (AOMDP), where an agent not only selects control actions but also decides whether to measure the latent state. The measurement action reveals the true latent state but may have a negative delayed effect on the environment. We show that this reduced uncertainty may provably improve sample efficiency and increase the value of the optimal policy despite these costs. We formulate an AOMDP as a periodic partially observable MDP and propose an online RL algorithm based on belief states. To approximate the belief states, we further propose a sequential Monte Carlo method to jointly approximate the posterior of unknown static environment parameters and unobserved latent states. We evaluate the proposed algorithm in a digital health application, where the agent decides when to deliver digital interventions and when to assess users' health status through surveys.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metacognitive Self-Correction for Multi-Agent System via Prototype-Guided Next-Execution Reconstruction</title>
<link>https://arxiv.org/abs/2510.14319</link>
<guid>https://arxiv.org/abs/2510.14319</guid>
<content:encoded><![CDATA[
arXiv:2510.14319v1 Announce Type: new 
Abstract: Large Language Model based multi-agent systems (MAS) excel at collaborative problem solving but remain brittle to cascading errors: a single faulty step can propagate across agents and disrupt the trajectory. In this paper, we present MASC, a metacognitive framework that endows MAS with real-time, unsupervised, step-level error detection and self-correction. MASC rethinks detection as history-conditioned anomaly scoring via two complementary designs: (1) Next-Execution Reconstruction, which predicts the embedding of the next step from the query and interaction history to capture causal consistency, and (2) Prototype-Guided Enhancement, which learns a prototype prior over normal-step embeddings and uses it to stabilize reconstruction and anomaly scoring under sparse context (e.g., early steps). When an anomaly step is flagged, MASC triggers a correction agent to revise the acting agent's output before information flows downstream. On the Who&amp;When benchmark, MASC consistently outperforms all baselines, improving step-level error detection by up to 8.47% AUC-ROC ; When plugged into diverse MAS frameworks, it delivers consistent end-to-end gains across architectures, confirming that our metacognitive monitoring and targeted correction can mitigate error propagation with minimal overhead.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stop-RAG: Value-Based Retrieval Control for Iterative RAG</title>
<link>https://arxiv.org/abs/2510.14337</link>
<guid>https://arxiv.org/abs/2510.14337</guid>
<content:encoded><![CDATA[
arXiv:2510.14337v1 Announce Type: new 
Abstract: Iterative retrieval-augmented generation (RAG) enables large language models to answer complex multi-hop questions, but each additional loop increases latency, costs, and the risk of introducing distracting evidence, motivating the need for an efficient stopping strategy. Existing methods either use a predetermined number of iterations or rely on confidence proxies that poorly reflect whether more retrieval will actually help. We cast iterative RAG as a finite-horizon Markov decision process and introduce Stop-RAG, a value-based controller that adaptively decides when to stop retrieving. Trained with full-width forward-view Q($\lambda$) targets from complete trajectories, Stop-RAG learns effective stopping policies while remaining compatible with black-box APIs and existing pipelines. On multi-hop question-answering benchmarks, Stop-RAG consistently outperforms both fixed-iteration baselines and prompting-based stopping with LLMs. These results highlight adaptive stopping as a key missing component in current agentic systems, and demonstrate that value-based control can improve the accuracy of RAG systems.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond One World: Benchmarking Super Heros in Role-Playing Across Multiversal Contexts</title>
<link>https://arxiv.org/abs/2510.14351</link>
<guid>https://arxiv.org/abs/2510.14351</guid>
<content:encoded><![CDATA[
arXiv:2510.14351v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used as role-playing agents, yet their capacity to faithfully and consistently portray version-specific characters -- for example, superheroes across comic and cinematic universes -- remains underexplored. Superhero canons such as Marvel and DC provide a rich testbed: decades of storytelling yield multiple incarnations of the same character with distinct histories, values, and moral codes. To study this problem, we introduce Beyond One World, a benchmark for character-grounded roleplay spanning 30 iconic heroes and 90 canon-specific versions. The benchmark comprises two tasks: (i) Canon Events, which probes factual recall of pivotal life stages, and (ii) Moral Dilemmas, which confronts models with ethically charged scenarios. We score responses for canonical accuracy and reasoning fidelity under a framework that separates internal deliberation ("thinking") from outward decisions ("acting"). We further propose Think-Act Matching, a metric that quantifies alignment between reasons and actions and serves as a proxy for model trustworthiness. Experiments across reasoning- and non-reasoning-oriented models yield three findings: (1) chain-of-thought prompting improves narrative coherence in weaker models but can reduce canonical accuracy in stronger ones; (2) cross-version generalization within a character remains a major obstacle; and (3) models often excel at either thinking or acting, but rarely both. Beyond One World exposes critical gaps in multiversal consistency and reasoning alignment, offering a challenging evaluation for role-playing LLMs.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI for Service: Proactive Assistance with AI Glasses</title>
<link>https://arxiv.org/abs/2510.14359</link>
<guid>https://arxiv.org/abs/2510.14359</guid>
<content:encoded><![CDATA[
arXiv:2510.14359v1 Announce Type: new 
Abstract: In an era where AI is evolving from a passive tool into an active and adaptive companion, we introduce AI for Service (AI4Service), a new paradigm that enables proactive and real-time assistance in daily life. Existing AI services remain largely reactive, responding only to explicit user commands. We argue that a truly intelligent and helpful assistant should be capable of anticipating user needs and taking actions proactively when appropriate. To realize this vision, we propose Alpha-Service, a unified framework that addresses two fundamental challenges: Know When to intervene by detecting service opportunities from egocentric video streams, and Know How to provide both generalized and personalized services. Inspired by the von Neumann computer architecture and based on AI glasses, Alpha-Service consists of five key components: an Input Unit for perception, a Central Processing Unit for task scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit for long-term personalization, and an Output Unit for natural human interaction. As an initial exploration, we implement Alpha-Service through a multi-agent system deployed on AI glasses. Case studies, including a real-time Blackjack advisor, a museum tour guide, and a shopping fit assistant, demonstrate its ability to seamlessly perceive the environment, infer user intent, and provide timely and useful assistance without explicit prompts.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control</title>
<link>https://arxiv.org/abs/2510.14388</link>
<guid>https://arxiv.org/abs/2510.14388</guid>
<content:encoded><![CDATA[
arXiv:2510.14388v1 Announce Type: new 
Abstract: Building agents that autonomously operate mobile devices has attracted increasing attention. While Vision-Language Models (VLMs) show promise, most existing approaches rely on direct state-to-action mappings, which lack structured reasoning and planning, and thus generalize poorly to novel tasks or unseen UI layouts. We introduce Hi-Agent, a trainable hierarchical vision-language agent for mobile control, featuring a high-level reasoning model and a low-level action model that are jointly optimized. For efficient training, we reformulate multi-step decision-making as a sequence of single-step subgoals and propose a foresight advantage function, which leverages execution feedback from the low-level model to guide high-level optimization. This design alleviates the path explosion issue encountered by Group Relative Policy Optimization (GRPO) in long-horizon tasks and enables stable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art (SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark, significantly outperforming prior methods across three paradigms: prompt-based (AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement learning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot generalization on the ScreenSpot-v2 benchmark. On the more challenging AndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones, showing strong adaptability in high-complexity mobile control scenarios.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Your Next Token Prediction: A Multilingual Benchmark for Personalized Response Generation</title>
<link>https://arxiv.org/abs/2510.14398</link>
<guid>https://arxiv.org/abs/2510.14398</guid>
<content:encoded><![CDATA[
arXiv:2510.14398v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at general next-token prediction but still struggle to generate responses that reflect how individuals truly communicate, such as replying to emails or social messages in their own style. However, real SNS or email histories are difficult to collect due to privacy concerns. To address this, we propose the task of "Your Next Token Prediction (YNTP)", which models a user's precise word choices through controlled human-agent conversations. We build a multilingual benchmark of 100 dialogue sessions across English, Japanese, and Chinese, where users interact for five days with psychologically grounded NPCs based on MBTI dimensions. This setup captures natural, daily-life communication patterns and enables analysis of users' internal models. We evaluate prompt-based and fine-tuning-based personalization methods, establishing the first benchmark for YNTP and a foundation for user-aligned language modeling. The dataset is available at: https://github.com/AnonymousHub4Submissions/your-next-token-prediction-dataset-100
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedTrust-RAG: Evidence Verification and Trust Alignment for Biomedical Question Answering</title>
<link>https://arxiv.org/abs/2510.14400</link>
<guid>https://arxiv.org/abs/2510.14400</guid>
<content:encoded><![CDATA[
arXiv:2510.14400v1 Announce Type: new 
Abstract: Biomedical question answering (QA) requires accurate interpretation of complex medical knowledge. Large language models (LLMs) have shown promising capabilities in this domain, with retrieval-augmented generation (RAG) systems enhancing performance by incorporating external medical literature. However, RAG-based approaches in biomedical QA suffer from hallucinations due to post-retrieval noise and insufficient verification of retrieved evidence, undermining response reliability. We propose MedTrust-Guided Iterative RAG, a framework designed to enhance factual consistency and mitigate hallucinations in medical QA. Our method introduces three key innovations. First, it enforces citation-aware reasoning by requiring all generated content to be explicitly grounded in retrieved medical documents, with structured Negative Knowledge Assertions used when evidence is insufficient. Second, it employs an iterative retrieval-verification process, where a verification agent assesses evidence adequacy and refines queries through Medical Gap Analysis until reliable information is obtained. Third, it integrates the MedTrust-Align Module (MTAM) that combines verified positive examples with hallucination-aware negative samples, leveraging Direct Preference Optimization to reinforce citation-grounded reasoning while penalizing hallucination-prone response patterns. Experiments on MedMCQA, MedQA, and MMLU-Med demonstrate that our approach consistently outperforms competitive baselines across multiple model architectures, achieving the best average accuracy with gains of 2.7% for LLaMA3.1-8B-Instruct and 2.4% for Qwen3-8B.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Role of Social Learning and Collective Norm Formation in Fostering Cooperation in LLM Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2510.14401</link>
<guid>https://arxiv.org/abs/2510.14401</guid>
<content:encoded><![CDATA[
arXiv:2510.14401v1 Announce Type: new 
Abstract: A growing body of multi-agent studies with Large Language Models (LLMs) explores how norms and cooperation emerge in mixed-motive scenarios, where pursuing individual gain can undermine the collective good. While prior work has explored these dynamics in both richly contextualized simulations and simplified game-theoretic environments, most LLM systems featuring common-pool resource (CPR) games provide agents with explicit reward functions directly tied to their actions. In contrast, human cooperation often emerges without full visibility into payoffs and population, relying instead on heuristics, communication, and punishment. We introduce a CPR simulation framework that removes explicit reward signals and embeds cultural-evolutionary mechanisms: social learning (adopting strategies and beliefs from successful peers) and norm-based punishment, grounded in Ostrom's principles of resource governance. Agents also individually learn from the consequences of harvesting, monitoring, and punishing via environmental feedback, enabling norms to emerge endogenously. We establish the validity of our simulation by reproducing key findings from existing studies on human behavior. Building on this, we examine norm evolution across a $2\times2$ grid of environmental and social initialisations (resource-rich vs. resource-scarce; altruistic vs. selfish) and benchmark how agentic societies comprised of different LLMs perform under these conditions. Our results reveal systematic model differences in sustaining cooperation and norm formation, positioning the framework as a rigorous testbed for studying emergent norms in mixed-motive LLM societies. Such analysis can inform the design of AI systems deployed in social and organizational contexts, where alignment with cooperative norms is critical for stability, fairness, and effective governance of AI-mediated environments.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instructions are all you need: Self-supervised Reinforcement Learning for Instruction Following</title>
<link>https://arxiv.org/abs/2510.14420</link>
<guid>https://arxiv.org/abs/2510.14420</guid>
<content:encoded><![CDATA[
arXiv:2510.14420v1 Announce Type: new 
Abstract: Language models often struggle to follow multi-constraint instructions that are crucial for real-world applications. Existing reinforcement learning (RL) approaches suffer from dependency on external supervision and sparse reward signals from multi-constraint tasks. We propose a label-free self-supervised RL framework that eliminates dependency on external supervision by deriving reward signals directly from instructions and generating pseudo-labels for reward model training. Our approach introduces constraint decomposition strategies and efficient constraint-wise binary classification to address sparse reward challenges while maintaining computational efficiency. Experiments show that our approach generalizes well, achieving strong improvements across 3 in-domain and 5 out-of-domain datasets, including challenging agentic and multi-turn instruction following. The data and code are publicly available at https://github.com/Rainier-rq/verl-if
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents</title>
<link>https://arxiv.org/abs/2510.14438</link>
<guid>https://arxiv.org/abs/2510.14438</guid>
<content:encoded><![CDATA[
arXiv:2510.14438v1 Announce Type: new 
Abstract: Deep research web agents not only retrieve information from diverse sources such as web environments, files, and multimodal inputs, but more importantly, they need to rigorously analyze and aggregate knowledge for insightful research. However, existing open-source deep research agents predominantly focus on enhancing information-seeking capabilities of web agents to locate specific information, while overlooking the essential need for information aggregation, which would limit their ability to support in-depth research. We propose an Explore to Evolve paradigm to scalably construct verifiable training data for web agents. Begins with proactive online exploration, an agent sources grounded information by exploring the real web. Using the collected evidence, the agent then self-evolves an aggregation program by selecting, composing, and refining operations from 12 high-level logical types to synthesize a verifiable QA pair. This evolution from high-level guidance to concrete operations allowed us to scalably produce WebAggregatorQA, a dataset of 10K samples across 50K websites and 11 domains. Based on an open-source agent framework, SmolAgents, we collect supervised fine-tuning trajectories to develop a series of foundation models, WebAggregator. WebAggregator-8B matches the performance of GPT-4.1, while the 32B variant surpasses GPT-4.1 by more than 10% on GAIA-text and closely approaches Claude-3.7-sonnet. Moreover, given the limited availability of benchmarks that evaluate web agents' information aggregation abilities, we construct a human-annotated evaluation split of WebAggregatorQA as a challenging test set. On this benchmark, Claude-3.7-sonnet only achieves 28%, and GPT-4.1 scores 25.8%. Even when agents manage to retrieve all references, they still struggle on WebAggregatorQA, highlighting the need to strengthen the information aggregation capabilities of web agent foundations.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Automated Governance: A DSL for Human-Agent Collaboration in Software Projects</title>
<link>https://arxiv.org/abs/2510.14465</link>
<guid>https://arxiv.org/abs/2510.14465</guid>
<content:encoded><![CDATA[
arXiv:2510.14465v1 Announce Type: new 
Abstract: The stakeholders involved in software development are becoming increasingly diverse, with both human contributors from varied backgrounds and AI-powered agents collaborating together in the process. This situation presents unique governance challenges, particularly in Open-Source Software (OSS) projects, where explicit policies are often lacking or unclear. This paper presents the vision and foundational concepts for a novel Domain-Specific Language (DSL) designed to define and enforce rich governance policies in systems involving diverse stakeholders, including agents. This DSL offers a pathway towards more robust, adaptable, and ultimately automated governance, paving the way for more effective collaboration in software projects, especially OSS ones.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiRA: Linguistic Robust Anchoring for Cross-lingual Large Language Models</title>
<link>https://arxiv.org/abs/2510.14466</link>
<guid>https://arxiv.org/abs/2510.14466</guid>
<content:encoded><![CDATA[
arXiv:2510.14466v1 Announce Type: new 
Abstract: As large language models (LLMs) rapidly advance, performance on high-resource languages (e.g., English, Chinese) is nearing saturation, yet remains substantially lower for low-resource languages (e.g., Urdu, Thai) due to limited training data, machine-translation noise, and unstable cross-lingual alignment. We introduce LiRA (Linguistic Robust Anchoring for Large Language Models), a training framework that robustly improves cross-lingual representations under low-resource conditions while jointly strengthening retrieval and reasoning. LiRA comprises two modules: (i) Arca (Anchored Representation Composition Architecture), which anchors low-resource languages to an English semantic space via anchor-based alignment and multi-agent collaborative encoding, preserving geometric stability in a shared embedding space; and (ii) LaSR (Language-coupled Semantic Reasoner), which adds a language-aware lightweight reasoning head with consistency regularization on top of Arca's multilingual representations, unifying the training objective to enhance cross-lingual understanding, retrieval, and reasoning robustness. We further construct and release a multilingual product retrieval dataset covering five Southeast Asian and two South Asian languages. Experiments across low-resource benchmarks (cross-lingual retrieval, semantic similarity, and reasoning) show consistent gains and robustness under few-shot and noise-amplified settings; ablations validate the contribution of both Arca and LaSR. Code will be released on GitHub and the dataset on Hugging Face.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Undo: Rollback-Augmented Reinforcement Learning with Reversibility Signals</title>
<link>https://arxiv.org/abs/2510.14503</link>
<guid>https://arxiv.org/abs/2510.14503</guid>
<content:encoded><![CDATA[
arXiv:2510.14503v1 Announce Type: new 
Abstract: This paper proposes a reversible learning framework to improve the robustness and efficiency of value based Reinforcement Learning agents, addressing vulnerability to value overestimation and instability in partially irreversible environments. The framework has two complementary core mechanisms: an empirically derived transition reversibility measure called Phi of s and a, and a selective state rollback operation. We introduce an online per state action estimator called Phi that quantifies the likelihood of returning to a prior state within a fixed horizon K. This measure is used to adjust the penalty term during temporal difference updates dynamically, integrating reversibility awareness directly into the value function. The system also includes a selective rollback operator. When an action yields an expected return markedly lower than its instantaneous estimated value and violates a predefined threshold, the agent is penalized and returns to the preceding state rather than progressing. This interrupts sub optimal high risk trajectories and avoids catastrophic steps. By combining reversibility aware evaluation with targeted rollback, the method improves safety, performance, and stability. In the CliffWalking v0 domain, the framework reduced catastrophic falls by over 99.8 percent and yielded a 55 percent increase in mean episode return. In the Taxi v3 domain, it suppressed illegal actions by greater than or equal to 99.9 percent and achieved a 65.7 percent improvement in cumulative reward, while also sharply reducing reward variance in both environments. Ablation studies confirm that the rollback mechanism is the critical component underlying these safety and performance gains, marking a robust step toward safe and reliable sequential decision making.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Helmsman: Autonomous Synthesis of Federated Learning Systems via Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2510.14512</link>
<guid>https://arxiv.org/abs/2510.14512</guid>
<content:encoded><![CDATA[
arXiv:2510.14512v1 Announce Type: new 
Abstract: Federated Learning (FL) offers a powerful paradigm for training models on decentralized data, but its promise is often undermined by the immense complexity of designing and deploying robust systems. The need to select, combine, and tune strategies for multifaceted challenges like data heterogeneity and system constraints has become a critical bottleneck, resulting in brittle, bespoke solutions. To address this, we introduce Helmsman, a novel multi-agent system that automates the end-to-end synthesis of federated learning systems from high-level user specifications. It emulates a principled research and development workflow through three collaborative phases: (1) interactive human-in-the-loop planning to formulate a sound research plan, (2) modular code generation by supervised agent teams, and (3) a closed-loop of autonomous evaluation and refinement in a sandboxed simulation environment. To facilitate rigorous evaluation, we also introduce AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess the system-level generation capabilities of agentic systems in FL. Extensive experiments demonstrate that our approach generates solutions competitive with, and often superior to, established hand-crafted baselines. Our work represents a significant step towards the automated engineering of complex decentralized AI systems.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JSPLIT: A Taxonomy-based Solution for Prompt Bloating in Model Context Protocol</title>
<link>https://arxiv.org/abs/2510.14537</link>
<guid>https://arxiv.org/abs/2510.14537</guid>
<content:encoded><![CDATA[
arXiv:2510.14537v1 Announce Type: new 
Abstract: AI systems are continually evolving and advancing, and user expectations are concurrently increasing, with a growing demand for interactions that go beyond simple text-based interaction with Large Language Models (LLMs). Today's applications often require LLMs to interact with external tools, marking a shift toward more complex agentic systems. To support this, standards such as the Model Context Protocol (MCP) have emerged, enabling agents to access tools by including a specification of the capabilities of each tool within the prompt. Although this approach expands what agents can do, it also introduces a growing problem: prompt bloating. As the number of tools increases, the prompts become longer, leading to high prompt token costs, increased latency, and reduced task success resulting from the selection of tools irrelevant to the prompt. To address this issue, we introduce JSPLIT, a taxonomy-driven framework designed to help agents manage prompt size more effectively when using large sets of MCP tools. JSPLIT organizes the tools into a hierarchical taxonomy and uses the user's prompt to identify and include only the most relevant tools, based on both the query and the taxonomy structure. In this paper, we describe the design of the taxonomy, the tool selection algorithm, and the dataset used to evaluate JSPLIT. Our results show that JSPLIT significantly reduces prompt size without significantly compromising the agent's ability to respond effectively. As the number of available tools for the agent grows substantially, JSPLIT even improves the tool selection accuracy of the agent, effectively reducing costs while simultaneously improving task success in high-complexity agent environments.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Entropy-Balanced Policy Optimization</title>
<link>https://arxiv.org/abs/2510.14545</link>
<guid>https://arxiv.org/abs/2510.14545</guid>
<content:encoded><![CDATA[
arXiv:2510.14545v1 Announce Type: new 
Abstract: Recently, Agentic Reinforcement Learning (Agentic RL) has made significant progress in incentivizing the multi-turn, long-horizon tool-use capabilities of web agents. While mainstream agentic RL algorithms autonomously explore high-uncertainty tool-call steps under the guidance of entropy, excessive reliance on entropy signals can impose further constraints, leading to the training collapse. In this paper, we delve into the challenges caused by entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an agentic RL algorithm designed to balance entropy in both the rollout and policy update phases. AEPO comprises two core components: (1) a dynamic entropy-balanced rollout mechanism that adaptively allocate global and branch sampling budget through entropy pre-monitoring, while imposing a branch penalty on consecutive high-entropy tool-call steps to prevent over-branching issues; and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient operation into the high-entropy clipping term to preserve and properly rescale gradients on high-entropy tokens, while incorporating entropy-aware advantage estimation to prioritize learning on high-uncertainty tokens. Results across 14 challenging datasets show that AEPO consistently outperforms 7 mainstream RL algorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive results: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker for Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on WebWalker for Pass@5. Further analysis reveals that AEPO improves rollout sampling diversity while maintaining stable policy entropy, facilitating scalable web agent training.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Agents Beyond Utility: An Open-Ended Perspective</title>
<link>https://arxiv.org/abs/2510.14548</link>
<guid>https://arxiv.org/abs/2510.14548</guid>
<content:encoded><![CDATA[
arXiv:2510.14548v1 Announce Type: new 
Abstract: Recent LLM agents have made great use of chain of thought reasoning and function calling. As their capabilities grow, an important question arises: can this software represent not only a smart problem-solving tool, but an entity in its own right, that can plan, design immediate tasks, and reason toward broader, more ambiguous goals? To study this question, we adopt an open-ended experimental setting where we augment a pretrained LLM agent with the ability to generate its own tasks, accumulate knowledge, and interact extensively with its environment. We study the resulting open-ended agent qualitatively. It can reliably follow complex multi-step instructions, store and reuse information across runs, and propose and solve its own tasks, though it remains sensitive to prompt design, prone to repetitive task generation, and unable to form self-representations. These findings illustrate both the promise and current limits of adapting pretrained LLMs toward open-endedness, and point to future directions for training agents to manage memory, explore productively, and pursue abstract long-term goals.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Active Inference Model of Mouse Point-and-Click Behaviour</title>
<link>https://arxiv.org/abs/2510.14611</link>
<guid>https://arxiv.org/abs/2510.14611</guid>
<content:encoded><![CDATA[
arXiv:2510.14611v1 Announce Type: new 
Abstract: We explore the use of Active Inference (AIF) as a computational user model for spatial pointing, a key problem in Human-Computer Interaction (HCI). We present an AIF agent with continuous state, action, and observation spaces, performing one-dimensional mouse pointing and clicking. We use a simple underlying dynamic system to model the mouse cursor dynamics with realistic perceptual delay. In contrast to previous optimal feedback control-based models, the agent's actions are selected by minimizing Expected Free Energy, solely based on preference distributions over percepts, such as observing clicking a button correctly. Our results show that the agent creates plausible pointing movements and clicks when the cursor is over the target, with similar end-point variance to human users. In contrast to other models of pointing, we incorporate fully probabilistic, predictive delay compensation into the agent. The agent shows distinct behaviour for differing target difficulties without the need to retune system parameters, as done in other approaches. We discuss the simulation results and emphasize the challenges in identifying the correct configuration of an AIF agent interacting with continuous systems.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks</title>
<link>https://arxiv.org/abs/2510.14621</link>
<guid>https://arxiv.org/abs/2510.14621</guid>
<content:encoded><![CDATA[
arXiv:2510.14621v1 Announce Type: new 
Abstract: The rapid advancement of multimodal large language models has enabled agents to operate mobile devices by directly interacting with graphical user interfaces, opening new possibilities for mobile automation. However, real-world mobile tasks are often complex and allow for multiple valid solutions. This contradicts current mobile agent evaluation standards: offline static benchmarks can only validate a single predefined "golden path", while online dynamic testing is constrained by the complexity and non-reproducibility of real devices, making both approaches inadequate for comprehensively assessing agent capabilities. To bridge the gap between offline and online evaluation and enhance testing stability, this paper introduces a novel graph-structured benchmarking framework. By modeling the finite states observed during real-device interactions, it achieves static simulation of dynamic behaviors. Building on this, we develop ColorBench, a benchmark focused on complex long-horizon tasks. It supports evaluation of multiple valid solutions, subtask completion rate statistics, and atomic-level capability analysis. ColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average length of over 13 steps. Each task includes at least two correct paths and several typical error paths, enabling quasi-dynamic interaction. By evaluating ColorBench across various baselines, we discover limitations of existing models and propose improvement directions and feasible technical pathways to enhance agents' performance on complex, long-horizon problems based on experimental results. Code and data are available at: https://github.com/MadeAgents/ColorBench.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Bidding Games: Reinforcement Learning for MEV Extraction on Polygon Blockchain</title>
<link>https://arxiv.org/abs/2510.14642</link>
<guid>https://arxiv.org/abs/2510.14642</guid>
<content:encoded><![CDATA[
arXiv:2510.14642v1 Announce Type: new 
Abstract: In blockchain networks, the strategic ordering of transactions within blocks has emerged as a significant source of profit extraction, known as Maximal Extractable Value (MEV). The transition from spam-based Priority Gas Auctions to structured auction mechanisms like Polygon Atlas has transformed MEV extraction from public bidding wars into sealed-bid competitions under extreme time constraints. While this shift reduces network congestion, it introduces complex strategic challenges where searchers must make optimal bidding decisions within a sub-second window without knowledge of competitor behavior or presence. Traditional game-theoretic approaches struggle in this high-frequency, partially observable environment due to their reliance on complete information and static equilibrium assumptions. We present a reinforcement learning framework for MEV extraction on Polygon Atlas and make three contributions: (1) A novel simulation environment that accurately models the stochastic arrival of arbitrage opportunities and probabilistic competition in Atlas auctions; (2) A PPO-based bidding agent optimized for real-time constraints, capable of adaptive strategy formulation in continuous action spaces while maintaining production-ready inference speeds; (3) Empirical validation demonstrating our history-conditioned agent captures 49\% of available profits when deployed alongside existing searchers and 81\% when replacing the market leader, significantly outperforming static bidding strategies. Our work establishes that reinforcement learning provides a critical advantage in high-frequency MEV environments where traditional optimization methods fail, offering immediate value for industrial participants and protocol designers alike.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Requirement Identification for Traffic Simulations in Driving Simulators</title>
<link>https://arxiv.org/abs/2510.14653</link>
<guid>https://arxiv.org/abs/2510.14653</guid>
<content:encoded><![CDATA[
arXiv:2510.14653v1 Announce Type: new 
Abstract: This paper addresses the challenge of ensuring realistic traffic conditions by proposing a methodology that systematically identifies traffic simulation requirements. Using a structured approach based on sub-goals in each study phase, specific technical needs are derived for microscopic levels, agent models, and visual representation. The methodology aims to maintain a high degree of fidelity, enhancing both the validity of experimental outcomes and participant engagement. By providing a clear link between study objectives and traffic simulation design, this approach supports robust automotive development and testing.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NAEL: Non-Anthropocentric Ethical Logic</title>
<link>https://arxiv.org/abs/2510.14676</link>
<guid>https://arxiv.org/abs/2510.14676</guid>
<content:encoded><![CDATA[
arXiv:2510.14676v1 Announce Type: new 
Abstract: We introduce NAEL (Non-Anthropocentric Ethical Logic), a novel ethical framework for artificial agents grounded in active inference and symbolic reasoning. Departing from conventional, human-centred approaches to AI ethics, NAEL formalizes ethical behaviour as an emergent property of intelligent systems minimizing global expected free energy in dynamic, multi-agent environments. We propose a neuro-symbolic architecture to allow agents to evaluate the ethical consequences of their actions in uncertain settings. The proposed system addresses the limitations of existing ethical models by allowing agents to develop context-sensitive, adaptive, and relational ethical behaviour without presupposing anthropomorphic moral intuitions. A case study involving ethical resource distribution illustrates NAEL's dynamic balancing of self-preservation, epistemic learning, and collective welfare.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Planners Meet Reality: How Learned, Reactive Traffic Agents Shift nuPlan Benchmarks</title>
<link>https://arxiv.org/abs/2510.14677</link>
<guid>https://arxiv.org/abs/2510.14677</guid>
<content:encoded><![CDATA[
arXiv:2510.14677v1 Announce Type: new 
Abstract: Planner evaluation in closed-loop simulation often uses rule-based traffic agents, whose simplistic and passive behavior can hide planner deficiencies and bias rankings. Widely used IDM agents simply follow a lead vehicle and cannot react to vehicles in adjacent lanes, hindering tests of complex interaction capabilities. We address this issue by integrating the state-of-the-art learned traffic agent model SMART into nuPlan. Thus, we are the first to evaluate planners under more realistic conditions and quantify how conclusions shift when narrowing the sim-to-real gap. Our analysis covers 14 recent planners and established baselines and shows that IDM-based simulation overestimates planning performance: nearly all scores deteriorate. In contrast, many planners interact better than previously assumed and even improve in multi-lane, interaction-heavy scenarios like lane changes or turns. Methods trained in closed-loop demonstrate the best and most stable driving performance. However, when reaching their limits in augmented edge-case scenarios, all learned planners degrade abruptly, whereas rule-based planners maintain reasonable basic behavior. Based on our results, we suggest SMART-reactive simulation as a new standard closed-loop benchmark in nuPlan and release the SMART agents as a drop-in alternative to IDM at https://github.com/shgd95/InteractiveClosedLoop.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Agents for Automated Web Vulnerability Reproduction: Are We There Yet?</title>
<link>https://arxiv.org/abs/2510.14700</link>
<guid>https://arxiv.org/abs/2510.14700</guid>
<content:encoded><![CDATA[
arXiv:2510.14700v1 Announce Type: new 
Abstract: Large language model (LLM) agents have demonstrated remarkable capabilities in software engineering and cybersecurity tasks, including code generation, vulnerability discovery, and automated testing. One critical but underexplored application is automated web vulnerability reproduction, which transforms vulnerability reports into working exploits. Although recent advances suggest promising potential, challenges remain in applying LLM agents to real-world web vulnerability reproduction scenarios. In this paper, we present the first comprehensive evaluation of state-of-the-art LLM agents for automated web vulnerability reproduction. We systematically assess 20 agents from software engineering, cybersecurity, and general domains across 16 dimensions, including technical capabilities, environment adaptability, and user experience factors, on 3 representative web vulnerabilities. Based on the results, we select three top-performing agents (OpenHands, SWE-agent, and CAI) for in-depth evaluation on our benchmark dataset of 80 real-world CVEs spanning 7 vulnerability types and 6 web technologies. Our results reveal that while LLM agents achieve reasonable success on simple library-based vulnerabilities, they consistently fail on complex service-based vulnerabilities requiring multi-component environments. Complex environment configurations and authentication barriers create a gap where agents can execute exploit code but fail to trigger actual vulnerabilities. We observe high sensitivity to input guidance, with performance degrading by over 33% under incomplete authentication information. Our findings highlight the significant gap between current LLM agent capabilities and the demands of reliable automated vulnerability reproduction, emphasizing the need for advances in environmental adaptation and autonomous problem-solving capabilities.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function Calling</title>
<link>https://arxiv.org/abs/2510.14703</link>
<guid>https://arxiv.org/abs/2510.14703</guid>
<content:encoded><![CDATA[
arXiv:2510.14703v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly demonstrating strong capabilities as autonomous agents, with function calling serving as a core mechanism for interaction with the environment. Meanwhile, inference scaling has become a cutting-edge technique to enhance LLM performance by allocating more computational resources during the inference process. However, current research on inference scaling primarily focuses on unstructured output generation tasks, leaving its application in structured outputs, like function calling, largely underexplored. To bridge this gap, we propose an inference scaling framework that combines fine-grained beam search with a process reward model, ToolPRM, which scores the internal steps of each single function call. To train ToolPRM, we construct the first fine-grained intra-call process supervision dataset, automatically annotated with function-masking techniques to provide step-level rewards for structured tool-use reasoning. Extensive experiments demonstrate that ToolPRM beats the coarse-grained and outcome reward models in terms of predictive accuracy, indicating its stronger capability in supervising the function calling inference process. Inference scaling technique equipped with ToolPRM also significantly improves the backbone model performance across various function calling tasks and benchmarks. More importantly, we reveal a key principle for applying inference scaling techniques to structured outputs: "explore more but retain less" due to the unrecoverability characteristics of structured function calling generation.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speculative Model Risk in Healthcare AI: Using Storytelling to Surface Unintended Harms</title>
<link>https://arxiv.org/abs/2510.14718</link>
<guid>https://arxiv.org/abs/2510.14718</guid>
<content:encoded><![CDATA[
arXiv:2510.14718v1 Announce Type: new 
Abstract: Artificial intelligence (AI) is rapidly transforming healthcare, enabling fast development of tools like stress monitors, wellness trackers, and mental health chatbots. However, rapid and low-barrier development can introduce risks of bias, privacy violations, and unequal access, especially when systems ignore real-world contexts and diverse user needs. Many recent methods use AI to detect risks automatically, but this can reduce human engagement in understanding how harms arise and who they affect. We present a human-centered framework that generates user stories and supports multi-agent discussions to help people think creatively about potential benefits and harms before deployment. In a user study, participants who read stories recognized a broader range of harms, distributing their responses more evenly across all 13 harm types. In contrast, those who did not read stories focused primarily on privacy and well-being (58.3%). Our findings show that storytelling helped participants speculate about a broader range of harms and benefits and think more creatively about AI's impact on users.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Pursuit of Diversity: Multi-Objective Testing of Deep Reinforcement Learning Agents</title>
<link>https://arxiv.org/abs/2510.14727</link>
<guid>https://arxiv.org/abs/2510.14727</guid>
<content:encoded><![CDATA[
arXiv:2510.14727v1 Announce Type: new 
Abstract: Testing deep reinforcement learning (DRL) agents in safety-critical domains requires discovering diverse failure scenarios. Existing tools such as INDAGO rely on single-objective optimization focused solely on maximizing failure counts, but this does not ensure discovered scenarios are diverse or reveal distinct error types. We introduce INDAGO-Nexus, a multi-objective search approach that jointly optimizes for failure likelihood and test scenario diversity using multi-objective evolutionary algorithms with multiple diversity metrics and Pareto front selection strategies. We evaluated INDAGO-Nexus on three DRL agents: humanoid walker, self-driving car, and parking agent. On average, INDAGO-Nexus discovers up to 83% and 40% more unique failures (test effectiveness) than INDAGO in the SDC and Parking scenarios, respectively, while reducing time-to-failure by up to 67% across all agents.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Jammer Localization via Acquisition-Aware Path Planning</title>
<link>https://arxiv.org/abs/2510.14790</link>
<guid>https://arxiv.org/abs/2510.14790</guid>
<content:encoded><![CDATA[
arXiv:2510.14790v1 Announce Type: new 
Abstract: We propose an active jammer localization framework that combines Bayesian optimization with acquisition-aware path planning. Unlike passive crowdsourced methods, our approach adaptively guides a mobile agent to collect high-utility Received Signal Strength measurements while accounting for urban obstacles and mobility constraints. For this, we modified the A* algorithm, A-UCB*, by incorporating acquisition values into trajectory costs, leading to high-acquisition planned paths. Simulations on realistic urban scenarios show that the proposed method achieves accurate localization with fewer measurements compared to uninformed baselines, demonstrating consistent performance under different environments.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic NL2SQL to Reduce Computational Costs</title>
<link>https://arxiv.org/abs/2510.14808</link>
<guid>https://arxiv.org/abs/2510.14808</guid>
<content:encoded><![CDATA[
arXiv:2510.14808v1 Announce Type: new 
Abstract: Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL) has recently been empowered by large language models (LLMs). Using LLMs to perform NL2SQL methods on a large collection of SQL databases necessitates processing large quantities of meta-information about the databases, which in turn results in lengthy prompts with many tokens and high processing costs. To address this challenge, we introduce Datalake Agent, an agentic system designed to enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing direct solvers for NL2SQL that call the LLM once with all meta-information in the prompt, the Datalake Agent employs an interactive loop to reduce the utilized meta-information. Within the loop, the LLM is used in a reasoning framework that selectively requests only the necessary information to solve a table question answering task. We evaluate the Datalake Agent on a collection of 23 databases with 100 table question answering tasks. The Datalake Agent reduces the tokens used by the LLM by up to 87\% and thus allows for substantial cost reductions while maintaining competitive performance.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Infinity and Beyond: Tool-Use Unlocks Length Generalization in State Space Models</title>
<link>https://arxiv.org/abs/2510.14826</link>
<guid>https://arxiv.org/abs/2510.14826</guid>
<content:encoded><![CDATA[
arXiv:2510.14826v1 Announce Type: new 
Abstract: State Space Models (SSMs) have become the leading alternative to Transformers for sequence modeling. Their primary advantage is efficiency in long-context and long-form generation, enabled by fixed-size memory and linear scaling of computational complexity. We begin this work by showing a simple theoretical result stating that SSMs cannot accurately solve any ``truly long-form'' generation problem (in a sense we formally define), undermining their main competitive advantage. However, we show that this limitation can be mitigated by allowing SSMs interactive access to external tools. In fact, we show that given the right choice of tool access and problem-dependent training data, SSMs can learn to solve any tractable problem and generalize to arbitrary problem length/complexity (i.e., achieve length generalization). Following our theoretical finding, we demonstrate that tool-augmented SSMs achieve remarkable length generalization on a variety of arithmetic, reasoning, and coding tasks. These findings highlight SSMs as a potential efficient alternative to Transformers in interactive tool-based and agentic settings.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.14828</link>
<guid>https://arxiv.org/abs/2510.14828</guid>
<content:encoded><![CDATA[
arXiv:2510.14828v1 Announce Type: new 
Abstract: Improving the reasoning capabilities of embodied agents is crucial for robots to complete complex human instructions in long-view manipulation tasks successfully. Despite the success of large language models and vision language models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue facing challenges in performing long-horizon manipulation tasks in complex real-world environments, owing to their restricted common sense and reasoning capabilities. Considering that aligning general-purpose vision language models to robotic planning tasks via supervised fine-tuning suffers from poor generalization and insufficient physical understanding, we propose RoboGPT-R1, a two-stage fine-tuning framework for embodied planning. In this framework, supervised training acquires foundational knowledge through expert sequences, followed by RL to address the model's shortcomings in visual-spatial understanding and reasoning. To achieve physical understanding and action sequence consistency in multi-step reasoning tasks, we design a rule-based reward function that simultaneously considers long-horizon performance and action constraint in the environment. The reasoning model, trained on Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini, by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the EmbodiedBench benchmark.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning with Stochastic Reward Machines</title>
<link>https://arxiv.org/abs/2510.14837</link>
<guid>https://arxiv.org/abs/2510.14837</guid>
<content:encoded><![CDATA[
arXiv:2510.14837v1 Announce Type: new 
Abstract: Reward machines are an established tool for dealing with reinforcement learning problems in which rewards are sparse and depend on complex sequences of actions. However, existing algorithms for learning reward machines assume an overly idealized setting where rewards have to be free of noise. To overcome this practical limitation, we introduce a novel type of reward machines, called stochastic reward machines, and an algorithm for learning them. Our algorithm, based on constraint solving, learns minimal stochastic reward machines from the explorations of a reinforcement learning agent. This algorithm can easily be paired with existing reinforcement learning algorithms for reward machines and guarantees to converge to an optimal policy in the limit. We demonstrate the effectiveness of our algorithm in two case studies and show that it outperforms both existing methods and a naive approach for handling noisy reward functions.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where to Search: Measure the Prior-Structured Search Space of LLM Agents</title>
<link>https://arxiv.org/abs/2510.14846</link>
<guid>https://arxiv.org/abs/2510.14846</guid>
<content:encoded><![CDATA[
arXiv:2510.14846v1 Announce Type: new 
Abstract: The generate-filter-refine (iterative paradigm) based on large language models (LLMs) has achieved progress in reasoning, programming, and program discovery in AI+Science. However, the effectiveness of search depends on where to search, namely, how to encode the domain prior into an operationally structured hypothesis space. To this end, this paper proposes a compact formal theory that describes and measures LLM-assisted iterative search guided by domain priors. We represent an agent as a fuzzy relation operator on inputs and outputs to capture feasible transitions; the agent is thereby constrained by a fixed safety envelope. To describe multi-step reasoning/search, we weight all reachable paths by a single continuation parameter and sum them to obtain a coverage generating function; this induces a measure of reachability difficulty; and it provides a geometric interpretation of search on the graph induced by the safety envelope. We further provide the simplest testable inferences and validate them via a majority-vote instantiation. This theory offers a workable language and operational tools to measure agents and their search spaces, proposing a systematic formal description of iterative search constructed by LLMs.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi Agent Switching Mode Controller for Sound Source localization</title>
<link>https://arxiv.org/abs/2510.14849</link>
<guid>https://arxiv.org/abs/2510.14849</guid>
<content:encoded><![CDATA[
arXiv:2510.14849v1 Announce Type: new 
Abstract: Source seeking is an important topic in robotic research, especially considering sound-based sensors since they allow the agents to locate a target even in critical conditions where it is not possible to establish a direct line of sight. In this work, we design a multi- agent switching mode control strategy for acoustic-based target localization. Two scenarios are considered: single source localization, in which the agents are driven maintaining a rigid formation towards the target, and multi-source scenario, in which each agent searches for the targets independently from the others.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LabOS: The AI-XR Co-Scientist That Sees and Works With Humans</title>
<link>https://arxiv.org/abs/2510.14861</link>
<guid>https://arxiv.org/abs/2510.14861</guid>
<content:encoded><![CDATA[
arXiv:2510.14861v1 Announce Type: new 
Abstract: Modern science advances fastest when thought meets action. LabOS represents the first AI co-scientist that unites computational reasoning with physical experimentation through multimodal perception, self-evolving agents, and Entended-Reality(XR)-enabled human-AI collaboration. By connecting multi-model AI agents, smart glasses, and human-AI collaboration, LabOS allows AI to see what scientists see, understand experimental context, and assist in real-time execution. Across applications--from cancer immunotherapy target discovery to stem-cell engineering -- LabOS shows that AI can move beyond computational design to participation, turning the laboratory into an intelligent, collaborative environment where human and machine discovery evolve together.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Gatekeeper Knows Enough</title>
<link>https://arxiv.org/abs/2510.14881</link>
<guid>https://arxiv.org/abs/2510.14881</guid>
<content:encoded><![CDATA[
arXiv:2510.14881v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly deployed as autonomous agents, yet their practical utility is fundamentally constrained by a limited context window and state desynchronization resulting from the LLMs' stateless nature and inefficient context management. These limitations lead to unreliable output, unpredictable behavior, and inefficient resource usage, particularly when interacting with large, structured, and sensitive knowledge systems such as codebases and documents. To address these challenges, we introduce the Gatekeeper Protocol, a novel, domain-agnostic framework that governs agent-system interactions. Our protocol mandates that the agent first operate and reason on a minimalist, low-fidelity "latent state" representation of the system to strategically request high-fidelity context on demand. All interactions are mediated through a unified JSON format that serves as a declarative, state-synchronized protocol, ensuring the agent's model of the system remains verifiably grounded in the system's reality. We demonstrate the efficacy of this protocol with Sage, a reference implementation of the Gatekeeper Protocol for software development. Our results show that this approach significantly increases agent reliability, improves computational efficiency by minimizing token consumption, and enables scalable interaction with complex systems, creating a foundational methodology for building more robust, predictable, and grounded AI agents for any structured knowledge domain.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning When Not to Learn: Risk-Sensitive Abstention in Bandits with Unbounded Rewards</title>
<link>https://arxiv.org/abs/2510.14884</link>
<guid>https://arxiv.org/abs/2510.14884</guid>
<content:encoded><![CDATA[
arXiv:2510.14884v1 Announce Type: new 
Abstract: In high-stakes AI applications, even a single action can cause irreparable damage. However, nearly all of sequential decision-making theory assumes that all errors are recoverable (e.g., by bounding rewards). Standard bandit algorithms that explore aggressively may cause irreparable damage when this assumption fails. Some prior work avoids irreparable errors by asking for help from a mentor, but a mentor may not always be available. In this work, we formalize a model of learning with unbounded rewards without a mentor as a two-action contextual bandit with an abstain option: at each round the agent observes an input and chooses either to abstain (always 0 reward) or to commit (execute a preexisting task policy). Committing yields rewards that are upper-bounded but can be arbitrarily negative, and the commit reward is assumed Lipschitz in the input. We propose a caution-based algorithm that learns when not to learn: it chooses a trusted region and commits only where the available evidence does not already certify harm. Under these conditions and i.i.d. inputs, we establish sublinear regret guarantees, theoretically demonstrating the effectiveness of cautious exploration for deploying learning agents safely in high-stakes environments.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent That Improves Without Labels or Model Updates</title>
<link>https://arxiv.org/abs/2510.14900</link>
<guid>https://arxiv.org/abs/2510.14900</guid>
<content:encoded><![CDATA[
arXiv:2510.14900v1 Announce Type: new 
Abstract: The Enterprise Intelligence Platform must integrate logs from numerous third-party vendors in order to perform various downstream tasks. However, vendor documentation is often unavailable at test time. It is either misplaced, mismatched, poorly formatted, or incomplete, which makes schema mapping challenging. We introduce a reinforcement learning agent that can self-improve without labeled examples or model weight updates. During inference, the agent: 1) Identifies ambiguous field-mapping attempts. 2) Generates targeted web-search queries to gather external evidence. 3) Applies a confidence-based reward to iteratively refine its mappings. To demonstrate this concept, we converted Microsoft Defender for Endpoint logs into a common schema. Our method increased mapping accuracy from 56.4\%(LLM-only) to 72.73\%(RAG) to 93.94\% over 100 iterations using GPT-4o. At the same time, it reduced the number of low-confidence mappings requiring expert review by 85\%. This new approach provides an evidence-driven, transparent method for solving future industry problems, paving the way for more robust, accountable, scalable, efficient, flexible, adaptable, and collaborative solutions.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation</title>
<link>https://arxiv.org/abs/2510.14902</link>
<guid>https://arxiv.org/abs/2510.14902</guid>
<content:encoded><![CDATA[
arXiv:2510.14902v1 Announce Type: new 
Abstract: Current vision-language-action (VLA) models, pre-trained on large-scale robotic data, exhibit strong multi-task capabilities and generalize well to variations in visual and language instructions for manipulation. However, their success rate drops significantly when faced with object concepts outside the training data, such as unseen object descriptions and textures in the dataset. To address this, we propose a novel agentic framework, VLA^2, which leverages OpenVLA as the execution backbone and effectively leverages external modules such as web retrieval and object detection to provide visual and textual knowledge about target objects to the VLA. This approach mitigates generalization failure when handling out-of-distribution objects. Based on the LIBERO simulation environment, we introduced novel objects and object descriptions to construct a new evaluation benchmark with three difficulty levels to test the effectiveness of our method. Our framework successfully outperformed the current state-of-the-art models on our designed hard-level generalization benchmark. Compared to the standalone OpenVLA baseline, VLA^2 achieves a 44.2% improvement in the success rate in the hard-level benchmark and an average improvement of 20.2% in all customized environments without any performance degradation on in-domain tasks. Project website: https://vla-2.github.io.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChangingGrounding: 3D Visual Grounding in Changing Scenes</title>
<link>https://arxiv.org/abs/2510.14965</link>
<guid>https://arxiv.org/abs/2510.14965</guid>
<content:encoded><![CDATA[
arXiv:2510.14965v1 Announce Type: new 
Abstract: Real-world robots localize objects from natural-language instructions while scenes around them keep changing. Yet most of the existing 3D visual grounding (3DVG) method still assumes a reconstructed and up-to-date point cloud, an assumption that forces costly re-scans and hinders deployment. We argue that 3DVG should be formulated as an active, memory-driven problem, and we introduce ChangingGrounding, the first benchmark that explicitly measures how well an agent can exploit past observations, explore only where needed, and still deliver precise 3D boxes in changing scenes. To set a strong reference point, we also propose Mem-ChangingGrounder, a zero-shot method for this task that marries cross-modal retrieval with lightweight multi-view fusion: it identifies the object type implied by the query, retrieves relevant memories to guide actions, then explores the target efficiently in the scene, falls back when previous operations are invalid, performs multi-view scanning of the target, and projects the fused evidence from multi-view scans to get accurate object bounding boxes. We evaluate different baselines on ChangingGrounding, and our Mem-ChangingGrounder achieves the highest localization accuracy while greatly reducing exploration cost. We hope this benchmark and method catalyze a shift toward practical, memory-centric 3DVG research for real-world applications. Project page: https://hm123450.github.io/CGB/ .
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identity-Link IRT for Label-Free LLM Evaluation: Preserving Additivity in TVD-MI Scores</title>
<link>https://arxiv.org/abs/2510.14966</link>
<guid>https://arxiv.org/abs/2510.14966</guid>
<content:encoded><![CDATA[
arXiv:2510.14966v1 Announce Type: new 
Abstract: Pairwise comparisons of large language models using total variation distance mutual information (TVD-MI) produce binary critic decisions per pair. We show that averaging TVD-MI's binary trials yields centered-probability scores with additive structure suitable for item-response theory (IRT) without nonlinear link functions. Maximum-likelihood approaches to IRT use logistic links, but we find empirically that these transformations introduce curvature that breaks additivity: across three domains, the identity link yields median curl on raw data of 0.080-0.150 (P95 = [0.474, 0.580]), whereas probit/logit introduce substantially higher violations (median [0.245, 0.588], P95 [0.825, 2.252]). We derive this clipped-linear model from Gini entropy maximization, yielding a box-constrained least-squares formulation that handles boundary saturation. At 33% coverage, we achieve holdout RMSE $0.117 \pm 0.008$ while preserving agent rankings (Spearman $\rho = 0.972 \pm 0.015$), three times fewer evaluations than full dense. Judge robustness analysis (GPT-4o-mini vs. Llama3-70b) shows strong agreement in agent rankings ($\rho = 0.872$) and consistent identity-link advantage. TVD-MI's geometry is best preserved by identity mapping for efficient LLM evaluation, applicable to other bounded-response domains.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents</title>
<link>https://arxiv.org/abs/2510.14967</link>
<guid>https://arxiv.org/abs/2510.14967</guid>
<content:encoded><![CDATA[
arXiv:2510.14967v1 Announce Type: new 
Abstract: Large language model (LLM)-based agents are increasingly trained with reinforcement learning (RL) to enhance their ability to interact with external environments through tool use, particularly in search-based settings that require multi-turn reasoning and knowledge acquisition. However, existing approaches typically rely on outcome-based rewards that are only provided at the final answer. This reward sparsity becomes particularly problematic in multi-turn settings, where long trajectories exacerbate two critical issues: (i) advantage collapse, where all rollouts receive identical rewards and provide no useful learning signals, and (ii) lack of fine-grained credit assignment, where dependencies between turns are obscured, especially in long-horizon tasks. In this paper, we propose Information Gain-based Policy Optimization (IGPO), a simple yet effective RL framework that provides dense and intrinsic supervision for multi-turn agent training. IGPO models each interaction turn as an incremental process of acquiring information about the ground truth, and defines turn-level rewards as the marginal increase in the policy's probability of producing the correct answer. Unlike prior process-level reward approaches that depend on external reward models or costly Monte Carlo estimation, IGPO derives intrinsic rewards directly from the model's own belief updates. These intrinsic turn-level rewards are combined with outcome-level supervision to form dense reward trajectories. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines in multi-turn scenarios, achieving higher accuracy and improved sample efficiency.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent Training</title>
<link>https://arxiv.org/abs/2510.14969</link>
<guid>https://arxiv.org/abs/2510.14969</guid>
<content:encoded><![CDATA[
arXiv:2510.14969v1 Announce Type: new 
Abstract: Digital agents require diverse, large-scale UI trajectories to generalize across real-world tasks, yet collecting such data is prohibitively expensive in both human annotation, infra and engineering perspectives. To this end, we introduce $\textbf{UI-Simulator}$, a scalable paradigm that generates structured UI states and transitions to synthesize training trajectories at scale. Our paradigm integrates a digital world simulator for diverse UI states, a guided rollout process for coherent exploration, and a trajectory wrapper that produces high-quality and diverse trajectories for agent training. We further propose $\textbf{UI-Simulator-Grow}$, a targeted scaling strategy that enables more rapid and data-efficient scaling by prioritizing high-impact tasks and synthesizes informative trajectory variants. Experiments on WebArena and AndroidWorld show that UI-Simulator rivals or surpasses open-source agents trained on real UIs with significantly better robustness, despite using weaker teacher models. Moreover, UI-Simulator-Grow matches the performance of Llama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model, highlighting the potential of targeted synthesis scaling paradigm to continuously and efficiently enhance the digital agents.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Design of Compositional Machines</title>
<link>https://arxiv.org/abs/2510.14980</link>
<guid>https://arxiv.org/abs/2510.14980</guid>
<content:encoded><![CDATA[
arXiv:2510.14980v1 Announce Type: new 
Abstract: The design of complex machines stands as both a marker of human intelligence and a foundation of engineering practice. Given recent advances in large language models (LLMs), we ask whether they, too, can learn to create. We approach this question through the lens of compositional machine design: a task in which machines are assembled from standardized components to meet functional demands like locomotion or manipulation in a simulated physical environment. To support this investigation, we introduce BesiegeField, a testbed built on the machine-building game Besiege, which enables part-based construction, physical simulation and reward-driven evaluation. Using BesiegeField, we benchmark state-of-the-art LLMs with agentic workflows and identify key capabilities required for success, including spatial reasoning, strategic assembly, and instruction-following. As current open-source models fall short, we explore reinforcement learning (RL) as a path to improvement: we curate a cold-start dataset, conduct RL finetuning experiments, and highlight open challenges at the intersection of language, machine design, and physical reasoning.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot Planning</title>
<link>https://arxiv.org/abs/2509.26255</link>
<guid>https://arxiv.org/abs/2509.26255</guid>
<content:encoded><![CDATA[
arXiv:2509.26255v2 Announce Type: cross 
Abstract: Long-horizon embodied planning is challenging because the world does not only change through an agent's actions: exogenous processes (e.g., water heating, dominoes cascading) unfold concurrently with the agent's actions. We propose a framework for abstract world models that jointly learns (i) symbolic state representations and (ii) causal processes for both endogenous actions and exogenous mechanisms. Each causal process models the time course of a stochastic cause-effect relation. We learn these world models from limited data via variational Bayesian inference combined with LLM proposals. Across five simulated tabletop robotics environments, the learned models enable fast planning that generalizes to held-out tasks with more objects and more complex goals, outperforming a range of baselines.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenCellAgent: Generalizable, Training-Free Cellular Image Segmentation via Large Language Model Agents</title>
<link>https://arxiv.org/abs/2510.13896</link>
<guid>https://arxiv.org/abs/2510.13896</guid>
<content:encoded><![CDATA[
arXiv:2510.13896v1 Announce Type: cross 
Abstract: Cellular image segmentation is essential for quantitative biology yet remains difficult due to heterogeneous modalities, morphological variability, and limited annotations. We present GenCellAgent, a training-free multi-agent framework that orchestrates specialist segmenters and generalist vision-language models via a planner-executor-evaluator loop (choose tool $\rightarrow$ run $\rightarrow$ quality-check) with long-term memory. The system (i) automatically routes images to the best tool, (ii) adapts on the fly using a few reference images when imaging conditions differ from what a tool expects, (iii) supports text-guided segmentation of organelles not covered by existing models, and (iv) commits expert edits to memory, enabling self-evolution and personalized workflows. Across four cell-segmentation benchmarks, this routing yields a 15.7\% mean accuracy gain over state-of-the-art baselines. On endoplasmic reticulum and mitochondria from new datasets, GenCellAgent improves average IoU by 37.6\% over specialist models. It also segments novel objects such as the Golgi apparatus via iterative text-guided refinement, with light human correction further boosting performance. Together, these capabilities provide a practical path to robust, adaptable cellular image segmentation without retraining, while reducing annotation burden and matching user preferences.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An AI-Driven Multimodal Smart Home Platform for Continuous Monitoring and Assistance in Post-Stroke Motor Impairment</title>
<link>https://arxiv.org/abs/2411.19000</link>
<guid>https://arxiv.org/abs/2411.19000</guid>
<content:encoded><![CDATA[
arXiv:2411.19000v4 Announce Type: replace 
Abstract: At-home rehabilitation for post-stroke patients presents significant challenges, as continuous, personalized care is often limited outside clinical settings. Moreover, the lack of integrated solutions capable of simultaneously monitoring motor recovery and providing intelligent assistance in home environments hampers rehabilitation outcomes. Here, we present a multimodal smart home platform designed for continuous, at-home rehabilitation of post-stroke patients, integrating wearable sensing, ambient monitoring, and adaptive automation. A plantar pressure insole equipped with a machine learning pipeline classifies users into motor recovery stages with up to 94\% accuracy, enabling quantitative tracking of walking patterns during daily activities. An optional head-mounted eye-tracking module, together with ambient sensors such as cameras and microphones, supports seamless hands-free control of household devices with a 100\% success rate and sub-second response time. These data streams are fused locally via a hierarchical Internet of Things (IoT) architecture, ensuring low latency and data privacy. An embedded large language model (LLM) agent, Auto-Care, continuously interprets multimodal data to provide real-time interventions -- issuing personalized reminders, adjusting environmental conditions, and notifying caregivers. Implemented in a post-stroke context, this integrated smart home platform increased mean user satisfaction from 3.9 $\pm$ 0.8 in conventional home environments to 8.4 $\pm$ 0.6 with the full system ($n=20$). Beyond stroke, the system offers a scalable, patient-centered framework with potential for long-term use in broader neurorehabilitation and aging-in-place applications.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inferring Foresightedness in Dynamic Noncooperative Games</title>
<link>https://arxiv.org/abs/2412.01017</link>
<guid>https://arxiv.org/abs/2412.01017</guid>
<content:encoded><![CDATA[
arXiv:2412.01017v3 Announce Type: replace 
Abstract: Dynamic game theory is an increasingly popular tool for modeling multi-agent, e.g. human-robot, interactions. Game-theoretic models presume that each agent wishes to minimize a private cost function that depends on others' actions. These games typically evolve over a fixed time horizon, specifying how far into the future each agent plans. In practical settings, however, decision-makers may vary in foresightedness, or how much they care about their current cost in relation to their past and future costs. We conjecture that quantifying and estimating each agent's foresightedness from online data will enable safer and more efficient interactions with other agents. To this end, we frame this inference problem as an inverse dynamic game. We consider a specific objective function parametrization that smoothly interpolates myopic and farsighted planning. Games of this form are readily transformed into parametric mixed complementarity problems; we exploit the directional differentiability of solutions to these problems with respect to their hidden parameters to solve for agents' foresightedness. We conduct three experiments: one with synthetically generated delivery robot motion, one with real-world data involving people walking, biking, and driving vehicles, and one using high-fidelity simulators. The results of these experiments demonstrate that explicitly inferring agents' foresightedness enables game-theoretic models to make 33% more accurate models for agents' behavior.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Start Button Problem: a basis for human responsibility in artificial intelligence computation</title>
<link>https://arxiv.org/abs/2501.12498</link>
<guid>https://arxiv.org/abs/2501.12498</guid>
<content:encoded><![CDATA[
arXiv:2501.12498v2 Announce Type: replace 
Abstract: Recent advancements in artificial intelligence have reopened the question about the boundaries of AI autonomy, particularly in discussions around artificial general intelligence and its potential to act independently across varied purposes. This paper explores these boundaries through the analysis of the Alignment Research Center experiment on GPT-4 and introduces the Start Button Problem, a thought experiment that examines the origins and limits of AI autonomy. By examining the thought experiment and its counterarguments, it becomes clear that in its need for human activation and purpose definition lies the AI's inherent dependency on human-initiated actions, challenging the assumption of AI as an intelligent agent. Finally, the paper addresses the implications of this dependency on human responsibility, questioning the measure of the extension of human responsibility when using AI systems.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The simulation of judgment in LLMs</title>
<link>https://arxiv.org/abs/2502.04426</link>
<guid>https://arxiv.org/abs/2502.04426</guid>
<content:encoded><![CDATA[
arXiv:2502.04426v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly embedded in evaluative processes, from information filtering to assessing and addressing knowledge gaps through explanation and credibility judgments. This raises the need to examine how such evaluations are built, what assumptions they rely on, and how their strategies diverge from those of humans. We benchmark six LLMs against expert ratings--NewsGuard and Media Bias/Fact Check--and against human judgments collected through a controlled experiment. We use news domains purely as a controlled benchmark for evaluative tasks, focusing on the underlying mechanisms rather than on news classification per se. To enable direct comparison, we implement a structured agentic framework in which both models and nonexpert participants follow the same evaluation procedure: selecting criteria, retrieving content, and producing justifications. Despite output alignment, our findings show consistent differences in the observable criteria guiding model evaluations, suggesting that lexical associations and statistical priors could influence evaluations in ways that differ from contextual reasoning. This reliance is associated with systematic effects: political asymmetries and a tendency to confuse linguistic form with epistemic reliability--a dynamic we term epistemia, the illusion of knowledge that emerges when surface plausibility replaces verification. Indeed, delegating judgment to such systems may affect the heuristics underlying evaluative processes, suggesting a shift from normative reasoning toward pattern-based approximation and raising open questions about the role of LLMs in evaluative processes.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?</title>
<link>https://arxiv.org/abs/2503.12349</link>
<guid>https://arxiv.org/abs/2503.12349</guid>
<content:encoded><![CDATA[
arXiv:2503.12349v5 Announce Type: replace 
Abstract: Reasoning and strategic behavior in social interactions is a hallmark of intelligence. This form of reasoning is significantly more sophisticated than isolated planning or reasoning tasks in static settings (e.g., math problem solving). In this paper, we present Strategic Planning, Interaction, and Negotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the intelligence of strategic planning and social reasoning. While many existing benchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench combines classical PDDL tasks, competitive board games, cooperative card games, and multi-agent negotiation scenarios in one unified framework. The framework includes both a benchmark as well as an arena to simulate and evaluate the variety of social settings to test reasoning and strategic behavior of AI agents. We formulate the benchmark SPIN-Bench by systematically varying action spaces, state complexity, and the number of interacting agents to simulate a variety of social settings where success depends on not only methodical and step-wise decision making, but also conceptual inference of other (adversarial or cooperative) participants. Our experiments reveal that while contemporary LLMs handle basic fact retrieval and short-range planning reasonably well, they encounter significant performance bottlenecks in tasks requiring deep multi-hop reasoning over large state spaces and socially adept coordination under uncertainty. We envision SPIN-Bench as a catalyst for future research on robust multi-agent planning, social reasoning, and human--AI teaming. Project Website: https://spinbench.github.io/
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No-Regret Learning in Stackelberg Games with an Application to Electric Ride-Hailing</title>
<link>https://arxiv.org/abs/2504.03745</link>
<guid>https://arxiv.org/abs/2504.03745</guid>
<content:encoded><![CDATA[
arXiv:2504.03745v2 Announce Type: replace 
Abstract: We consider the problem of efficiently learning to play single-leader multi-follower Stackelberg games when the leader lacks knowledge of the lower-level game. Such games arise in hierarchical decision-making problems involving self-interested agents. For example, in electric ride-hailing markets, a central authority aims to learn optimal charging prices to shape fleet distributions and charging patterns of ride-hailing companies. Existing works typically apply gradient-based methods to find the leader's optimal strategy. Such methods are impractical as they require that the followers share private utility information with the leader. Instead, we treat the lower-level game as a black box, assuming only that the followers' interactions approximate a Nash equilibrium while the leader observes the realized cost of the resulting approximation. Under kernel-based regularity assumptions on the leader's cost function, we develop a no-regret algorithm that converges to an $\epsilon$-Stackelberg equilibrium in $O(\sqrt{T})$ rounds. Finally, we validate our approach through a numerical case study on optimal pricing in electric ride-hailing markets.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Internet of Agents: Fundamentals, Applications, and Challenges</title>
<link>https://arxiv.org/abs/2505.07176</link>
<guid>https://arxiv.org/abs/2505.07176</guid>
<content:encoded><![CDATA[
arXiv:2505.07176v2 Announce Type: replace 
Abstract: With the rapid proliferation of large language models and vision-language models, AI agents have evolved from isolated, task-specific systems into autonomous, interactive entities capable of perceiving, reasoning, and acting without human intervention. As these agents proliferate across virtual and physical environments, from virtual assistants to embodied robots, the need for a unified, agent-centric infrastructure becomes paramount. In this survey, we introduce the Internet of Agents (IoA) as a foundational framework that enables seamless interconnection, dynamic discovery, and collaborative orchestration among heterogeneous agents at scale. We begin by presenting a general IoA architecture, highlighting its hierarchical organization, distinguishing features relative to the traditional Internet, and emerging applications. Next, we analyze the key operational enablers of IoA, including capability notification and discovery, adaptive communication protocols, dynamic task matching, consensus and conflict-resolution mechanisms, and incentive models. Finally, we identify open research directions toward building resilient and trustworthy IoA ecosystems.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PoE-World: Compositional World Modeling with Products of Programmatic Experts</title>
<link>https://arxiv.org/abs/2505.10819</link>
<guid>https://arxiv.org/abs/2505.10819</guid>
<content:encoded><![CDATA[
arXiv:2505.10819v3 Announce Type: replace 
Abstract: Learning how the world works is central to building AI agents that can adapt to complex environments. Traditional world models based on deep learning demand vast amounts of training data, and do not flexibly update their knowledge from sparse observations. Recent advances in program synthesis using Large Language Models (LLMs) give an alternate approach which learns world models represented as source code, supporting strong generalization from little data. To date, application of program-structured world models remains limited to natural language and grid-world domains. We introduce a novel program synthesis method for effectively modeling complex, non-gridworld domains by representing a world model as an exponentially-weighted product of programmatic experts (PoE-World) synthesized by LLMs. We show that this approach can learn complex, stochastic world models from just a few observations. We evaluate the learned world models by embedding them in a model-based planning agent, demonstrating efficient performance and generalization to unseen levels on Atari's Pong and Montezuma's Revenge. We release our code and display the learned world models and videos of the agent's gameplay at https://topwasu.github.io/poe-world.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAFA: A multi-agent framework for annotation</title>
<link>https://arxiv.org/abs/2505.13668</link>
<guid>https://arxiv.org/abs/2505.13668</guid>
<content:encoded><![CDATA[
arXiv:2505.13668v3 Announce Type: replace 
Abstract: Modern consumer banking applications require accurate and efficient retrieval of information in response to user queries. Mapping user utterances to the most relevant Frequently Asked Questions (FAQs) is a crucial component of these systems. Traditional approaches often rely on a single model or technique, which may not capture the nuances of diverse user inquiries. In this paper, we introduce a multi-agent framework for FAQ annotation that combines multiple specialized agents with different approaches and a judge agent that reranks candidates to produce optimal results. Our agents utilize a structured reasoning approach inspired by Attentive Reasoning Queries (ARQs), which guides them through systematic reasoning steps using targeted, task-specific JSON queries. Our framework features a few-shot example strategy, where each agent receives different few-shots, enhancing ensemble diversity and coverage of the query space. We evaluate our framework on a real-world major bank dataset as well as public benchmark datasets (LCQMC and FiQA), demonstrating significant improvements over single-agent approaches across multiple metrics, including a 14% increase in Top-1 accuracy, an 18% increase in Top-5 accuracy, and a 12% improvement in Mean Reciprocal Rank on our dataset, and similar gains on public benchmarks when compared with traditional and single-agent annotation techniques. Our framework is particularly effective at handling ambiguous queries, making it well-suited for deployment in production banking applications while showing strong generalization capabilities across different domains and languages.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Checkpoint-GCG: Auditing and Attacking Fine-Tuning-Based Prompt Injection Defenses</title>
<link>https://arxiv.org/abs/2505.15738</link>
<guid>https://arxiv.org/abs/2505.15738</guid>
<content:encoded><![CDATA[
arXiv:2505.15738v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly deployed in real-world applications ranging from chatbots to agentic systems, where they are expected to process untrusted data and follow trusted instructions. Failure to distinguish between the two poses significant security risks, exploited by prompt injection attacks, which inject malicious instructions into the data to control model outputs. Model-level defenses have been proposed to mitigate prompt injection attacks. These defenses fine-tune LLMs to ignore injected instructions in untrusted data. We introduce Checkpoint-GCG, a white-box attack against fine-tuning-based defenses. Checkpoint-GCG enhances the Greedy Coordinate Gradient (GCG) attack by leveraging intermediate model checkpoints produced during fine-tuning to initialize GCG, with each checkpoint acting as a stepping stone for the next one to continuously improve attacks. First, we instantiate Checkpoint-GCG to evaluate the robustness of the state-of-the-art defenses in an auditing setup, assuming both (a) full knowledge of the model input and (b) access to intermediate model checkpoints. We show Checkpoint-GCG to achieve up to $96\%$ attack success rate (ASR) against the strongest defense. Second, we relax the first assumption by searching for a universal suffix that would work on unseen inputs, and obtain up to $89.9\%$ ASR against the strongest defense. Finally, we relax both assumptions by searching for a universal suffix that would transfer to similar black-box models and defenses, achieving an ASR of $63.9\%$ against a newly released defended model from Meta.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EMAC+: Embodied Multimodal Agent for Collaborative Planning with VLM+LLM</title>
<link>https://arxiv.org/abs/2505.19905</link>
<guid>https://arxiv.org/abs/2505.19905</guid>
<content:encoded><![CDATA[
arXiv:2505.19905v2 Announce Type: replace 
Abstract: Although LLMs demonstrate proficiency in several text-based reasoning and planning tasks, their implementation in robotics control is constrained by significant deficiencies: (1) LLM agents are designed to work mainly with textual inputs rather than visual conditions; (2) Current multimodal agents treat LLMs as static planners, which separates their reasoning from environment dynamics, resulting in actions that do not take domain-specific knowledge into account; and (3) LLMs are not designed to learn from visual interactions, which makes it harder for them to make better policies for specific domains. In this paper, we introduce EMAC+, an Embodied Multimodal Agent that collaboratively integrates LLM and VLM via a bidirectional training paradigm. Unlike existing methods, EMAC+ dynamically refines high-level textual plans generated by an LLM using real-time feedback from a VLM executing low-level visual control tasks. We address critical limitations of previous models by enabling the LLM to internalize visual environment dynamics directly through interactive experience, rather than relying solely on static symbolic mappings. Extensive experimental evaluations on ALFWorld and RT-1 benchmarks demonstrate that EMAC+ achieves superior task performance, robustness against noisy observations, and efficient learning. We also conduct thorough ablation studies and provide detailed analyses of success and failure cases.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAI3: Testing Agent Integrity in Interpreting User Intent</title>
<link>https://arxiv.org/abs/2506.07524</link>
<guid>https://arxiv.org/abs/2506.07524</guid>
<content:encoded><![CDATA[
arXiv:2506.07524v2 Announce Type: replace 
Abstract: LLM agents are increasingly deployed to automate real-world tasks by invoking APIs through natural language instructions. While powerful, they often suffer from misinterpretation of user intent, leading to the agent's actions that diverge from the user's intended goal, especially as external toolkits evolve. Traditional software testing assumes structured inputs and thus falls short in handling the ambiguity of natural language. We introduce TAI3, an API-centric stress testing framework that systematically uncovers intent integrity violations in LLM agents. Unlike prior work focused on fixed benchmarks or adversarial inputs, TAI3 generates realistic tasks based on toolkits' documentation and applies targeted mutations to expose subtle agent errors while preserving user intent. To guide testing, we propose semantic partitioning, which organizes natural language tasks into meaningful categories based on toolkit API parameters and their equivalence classes. Within each partition, seed tasks are mutated and ranked by a lightweight predictor that estimates the likelihood of triggering agent errors. To enhance efficiency, TAI3 maintains a datatype-aware strategy memory that retrieves and adapts effective mutation patterns from past cases. Experiments on 80 toolkit APIs demonstrate that TAI3 effectively uncovers intent integrity violations, significantly outperforming baselines in both error-exposing rate and query efficiency. Moreover, TAI3 generalizes well to stronger target models using smaller LLMs for test generation, and adapts to evolving APIs across domains.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-guided Chemical Process Optimization with a Multi-Agent Approach</title>
<link>https://arxiv.org/abs/2506.20921</link>
<guid>https://arxiv.org/abs/2506.20921</guid>
<content:encoded><![CDATA[
arXiv:2506.20921v2 Announce Type: replace 
Abstract: Chemical process optimization maximizes production efficiency and economic performance, but optimization algorithms, including gradient-based solvers, numerical methods, and parameter grid searches, become impractical when operating constraints are ill-defined or unavailable. We present a multi-agent LLM framework that autonomously infers operating constraints from minimal process descriptions, then collaboratively guides optimization. Our AutoGen-based framework employs OpenAI's o3 model with specialized agents for constraint generation, parameter validation, simulation, and optimization guidance. Through autonomous constraint generation and iterative multi-agent optimization, the framework eliminates the need for predefined operational bounds. Validated on hydrodealkylation across cost, yield, and yield-to-cost ratio metrics, the framework achieved competitive performance with conventional methods while reducing wall-time 31-fold relative to grid search, converging in under 20 minutes. The reasoning-guided search demonstrates sophisticated process understanding, correctly identifying utility trade-offs and applying domain-informed heuristics. Unlike conventional methods requiring predefined constraints, our approach uniquely combines autonomous constraint generation with interpretable parameter exploration. Model comparison reveals reasoning-capable architectures (o3, o1) are essential for successful optimization, while standard models fail to converge. This approach is particularly valuable for emerging processes and retrofit applications where operational constraints are poorly characterized or unavailable.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities</title>
<link>https://arxiv.org/abs/2507.06261</link>
<guid>https://arxiv.org/abs/2507.06261</guid>
<content:encoded><![CDATA[
arXiv:2507.06261v5 Announce Type: replace 
Abstract: In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents</title>
<link>https://arxiv.org/abs/2509.06917</link>
<guid>https://arxiv.org/abs/2509.06917</guid>
<content:encoded><![CDATA[
arXiv:2509.06917v2 Announce Type: replace 
Abstract: We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2Agent transforms research output from passive artifacts into active systems that can accelerate downstream use, adoption, and discovery. Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work, creating barriers to dissemination and reuse. Paper2Agent addresses this challenge by automatically converting a paper into an AI agent that acts as a knowledgeable research assistant. It systematically analyzes the paper and the associated codebase using multiple agents to construct a Model Context Protocol (MCP) server, then iteratively generates and runs tests to refine and robustify the resulting MCP. These paper MCPs can then be flexibly connected to a chat agent (e.g. Claude Code) to carry out complex scientific queries through natural language while invoking tools and workflows from the original paper. We demonstrate Paper2Agent's effectiveness in creating reliable and capable paper agents through in-depth case studies. Paper2Agent created an agent that leverages AlphaGenome to interpret genomic variants and agents based on ScanPy and TISSUE to carry out single-cell and spatial transcriptomics analyses. We validate that these paper agents can reproduce the original paper's results and can correctly carry out novel user queries. Paper2Agent automatically created AI co-scientist that identified new splicing variant associated with ADHD risk. By turning static papers into dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for knowledge dissemination and a foundation for the collaborative ecosystem of AI co-scientists.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WoW: Towards a World omniscient World model Through Embodied Interaction</title>
<link>https://arxiv.org/abs/2509.22642</link>
<guid>https://arxiv.org/abs/2509.22642</guid>
<content:encoded><![CDATA[
arXiv:2509.22642v2 Announce Type: replace 
Abstract: Humans develop an understanding of intuitive physics through active interaction with the world. This approach is in stark contrast to current video models, such as Sora, which rely on passive observation and therefore struggle with grasping physical causality. This observation leads to our central hypothesis: authentic physical intuition of the world model must be grounded in extensive, causally rich interactions with the real world. To test this hypothesis, we present WoW, a 14-billion-parameter generative world model trained on 2 million robot interaction trajectories. Our findings reveal that the model's understanding of physics is a probabilistic distribution of plausible outcomes, leading to stochastic instabilities and physical hallucinations. Furthermore, we demonstrate that this emergent capability can be actively constrained toward physical realism by SOPHIA, where vision-language model agents evaluate the DiT-generated output and guide its refinement by iteratively evolving the language instructions. In addition, a co-trained Inverse Dynamics Model translates these refined plans into executable robotic actions, thus closing the imagination-to-action loop. We establish WoWBench, a new benchmark focused on physical consistency and causal reasoning in video, where WoW achieves state-of-the-art performance in both human and autonomous evaluation, demonstrating strong ability in physical causality, collision dynamics, and object permanence. Our work provides systematic evidence that large-scale, real-world interaction is a cornerstone for developing physical intuition in AI. Models, data, and benchmarks will be open-sourced.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Earth-Agent: Unlocking the Full Landscape of Earth Observation with Agents</title>
<link>https://arxiv.org/abs/2509.23141</link>
<guid>https://arxiv.org/abs/2509.23141</guid>
<content:encoded><![CDATA[
arXiv:2509.23141v2 Announce Type: replace 
Abstract: Earth observation (EO) is essential for understanding the evolving states of the Earth system. Although recent MLLMs have advanced EO research, they still lack the capability to tackle complex tasks that require multi-step reasoning and the use of domain-specific tools. Agent-based methods offer a promising direction, but current attempts remain in their infancy, confined to RGB perception, shallow reasoning, and lacking systematic evaluation protocols. To overcome these limitations, we introduce Earth-Agent, the first agentic framework that unifies RGB and spectral EO data within an MCP-based tool ecosystem, enabling cross-modal, multi-step, and quantitative spatiotemporal reasoning beyond pretrained MLLMs. Earth-Agent supports complex scientific tasks such as geophysical parameter retrieval and quantitative spatiotemporal analysis by dynamically invoking expert tools and models across modalities. To support comprehensive evaluation, we further propose Earth-Bench, a benchmark of 248 expert-curated tasks with 13,729 images, spanning spectrum, products and RGB modalities, and equipped with a dual-level evaluation protocol that assesses both reasoning trajectories and final outcomes. We conduct comprehensive experiments varying different LLM backbones, comparisons with general agent frameworks, and comparisons with MLLMs on remote sensing benchmarks, demonstrating both the effectiveness and potential of Earth-Agent. Earth-Agent establishes a new paradigm for EO analysis, moving the field toward scientifically grounded, next-generation applications of LLMs in Earth observation.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Safety, Alignment, and Ethics (AI SAE)</title>
<link>https://arxiv.org/abs/2509.24065</link>
<guid>https://arxiv.org/abs/2509.24065</guid>
<content:encoded><![CDATA[
arXiv:2509.24065v2 Announce Type: replace 
Abstract: This paper grounds ethics in evolutionary biology, viewing moral norms as adaptive mechanisms that render cooperation fitness-viable under selection pressure. Current alignment approaches add ethics post hoc, treating it as an external constraint rather than embedding it as an evolutionary strategy for cooperation. The central question is whether normative architectures can be embedded directly into AI systems to sustain human--AI cooperation (symbiosis) as capabilities scale. To address this, I propose a governance--embedding--representation pipeline linking moral representation learning to system-level design and institutional governance, treating alignment as a multi-level problem spanning cognition, optimization, and oversight. I formalize moral norm representation through the moral problem space, a learnable subspace in neural representations where cooperative norms can be encoded and causally manipulated. Using sparse autoencoders, activation steering, and causal interventions, I outline a research program for engineering moral representations and embedding them into the full semantic space -- treating competing theories of morality as empirical hypotheses about representation geometry rather than philosophical positions. Governance principles leverage these learned moral representations to regulate how cooperative behaviors evolve within the AI ecosystem. Through replicator dynamics and multi-agent game theory, I model how internal representational features can shape population-level incentives by motivating the design of sanctions and subsidies structured to yield decentralized normative institutions.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PerfBench: Can Agents Resolve Real-World Performance Bugs?</title>
<link>https://arxiv.org/abs/2509.24091</link>
<guid>https://arxiv.org/abs/2509.24091</guid>
<content:encoded><![CDATA[
arXiv:2509.24091v2 Announce Type: replace 
Abstract: Performance bugs are inefficiencies in software that waste computational resources without causing functional failures, making them particularly challenging to detect and fix. While recent advances in Software Engineering agents have shown promise in automated bug fixing, existing benchmarks primarily focus on functional correctness and fail to evaluate agents' abilities to identify and resolve non-functional issues like performance bugs. We introduce PerfBench, a benchmark comprising 81 real-world performance bug-fixing tasks from popular .NET repositories on GitHub. Unlike existing benchmarks that rely on pre-existing test suites, PerfBench features a novel evaluation harness that allows agents to generate their own performance benchmarks and validates fixes by comparing execution metrics collected for developer fix and agent fix. Each task in PerfBench is derived from actual developer fixes linked to performance-related issues, which are then verified by human experts, ensuring real-world relevance. Our evaluation reveals that current state-of-the-art coding agents struggle with performance optimization tasks, with baseline OpenHands agent achieving only a ~3% success rate on our benchmark. We develop OpenHands-Perf-Agent, which incorporates performance-aware tooling and instructions and achieves a ~20% success rate on the benchmark. We show that by ensuring the agent has proper instructions to benchmark its changes and tooling for benchmark output processing, we can improve the agent performance significantly, but room for improvement still remains. PerfBench provides a challenging test set for furthering the capabilities of agents in fixing performance issues.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety Evaluation via Role-Specialized Collaboration</title>
<link>https://arxiv.org/abs/2509.25271</link>
<guid>https://arxiv.org/abs/2509.25271</guid>
<content:encoded><![CDATA[
arXiv:2509.25271v2 Announce Type: replace 
Abstract: Existing safety evaluation methods for large language models (LLMs) suffer from inherent limitations, including evaluator bias and detection failures arising from model homogeneity, which collectively undermine the robustness of risk evaluation processes. This paper seeks to re-examine the risk evaluation paradigm by introducing a theoretical framework that reconstructs the underlying risk concept space. Specifically, we decompose the latent risk concept space into three mutually exclusive subspaces: the explicit risk subspace (encompassing direct violations of safety guidelines), the implicit risk subspace (capturing potential malicious content that requires contextual reasoning for identification), and the non-risk subspace. Furthermore, we propose RADAR, a multi-agent collaborative evaluation framework that leverages multi-round debate mechanisms through four specialized complementary roles and employs dynamic update mechanisms to achieve self-evolution of risk concept distributions. This approach enables comprehensive coverage of both explicit and implicit risks while mitigating evaluator bias. To validate the effectiveness of our framework, we construct an evaluation dataset comprising 800 challenging cases. Extensive experiments on our challenging testset and public benchmarks demonstrate that RADAR significantly outperforms baseline evaluation methods across multiple dimensions, including accuracy, stability, and self-evaluation risk sensitivity. Notably, RADAR achieves a 28.87% improvement in risk identification accuracy compared to the strongest baseline evaluation method.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring and Mitigating Identity Bias in Multi-Agent Debate via Anonymization</title>
<link>https://arxiv.org/abs/2510.07517</link>
<guid>https://arxiv.org/abs/2510.07517</guid>
<content:encoded><![CDATA[
arXiv:2510.07517v2 Announce Type: replace 
Abstract: Multi-agent debate (MAD) aims to improve large language model (LLM) reasoning by letting multiple agents exchange answers and then aggregate their opinions. Yet recent studies reveal that agents are not neutral: they are prone to identity-driven sycophancy and self-bias, uncritically adopting a peer's view or stubbornly adhering to their own prior output, undermining the reliability of debate. In this work, we present the first principled framework that joins sycophancy and self-bias to mitigate and quantify identity bias in MAD. First, we formalize the debate dynamics as an identity-weighted Bayesian update process. Second, we propose response anonymization: by removing identity markers from prompts, agents cannot distinguish "self" from "peer", which forces equal weights on agent identity, thereby reducing bias. Third, we define the Identity Bias Coefficient (IBC), a principled metric that measures how often an agent follows a peer versus itself. Empirical studies across multiple models, datasets and debate rounds confirm that identity bias is widespread, with sycophancy far more common than self-bias. Our findings highlight the need to "mask" identity to ensure that MAD systems reason based on content rather than source identity. Code is released in https://github.com/deeplearning-wisc/MAD-identity-bias.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VoxelPrompt: A Vision Agent for End-to-End Medical Image Analysis</title>
<link>https://arxiv.org/abs/2410.08397</link>
<guid>https://arxiv.org/abs/2410.08397</guid>
<content:encoded><![CDATA[
arXiv:2410.08397v2 Announce Type: replace-cross 
Abstract: We present VoxelPrompt, an end-to-end image analysis agent that tackles free-form radiological tasks. Given any number of volumetric medical images and a natural language prompt, VoxelPrompt integrates a language model that generates executable code to invoke a jointly-trained, adaptable vision network. This code further carries out analytical steps to address practical quantitative aims, such as measuring the growth of a tumor across visits. The pipelines generated by VoxelPrompt automate analyses that currently require practitioners to painstakingly combine multiple specialized vision and statistical tools. We evaluate VoxelPrompt using diverse neuroimaging tasks and show that it can delineate hundreds of anatomical and pathological features, measure complex morphological properties, and perform open-language analysis of lesion characteristics. VoxelPrompt performs these objectives with an accuracy similar to that of specialist single-task models for image analysis, while facilitating a broad range of compositional biomedical workflows.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Classifier-Augmented Generation for Structured Workflow Prediction</title>
<link>https://arxiv.org/abs/2510.12825</link>
<guid>https://arxiv.org/abs/2510.12825</guid>
<content:encoded><![CDATA[
arXiv:2510.12825v1 Announce Type: new 
Abstract: ETL (Extract, Transform, Load) tools such as IBM DataStage allow users to visually assemble complex data workflows, but configuring stages and their properties remains time consuming and requires deep tool knowledge. We propose a system that translates natural language descriptions into executable workflows, automatically predicting both the structure and detailed configuration of the flow. At its core lies a Classifier-Augmented Generation (CAG) approach that combines utterance decomposition with a classifier and stage-specific few-shot prompting to produce accurate stage predictions. These stages are then connected into non-linear workflows using edge prediction, and stage properties are inferred from sub-utterance context. We compare CAG against strong single-prompt and agentic baselines, showing improved accuracy and efficiency, while substantially reducing token usage. Our architecture is modular, interpretable, and capable of end-to-end workflow generation, including robust validation steps. To our knowledge, this is the first system with a detailed evaluation across stage prediction, edge layout, and property generation for natural-language-driven ETL authoring.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scheming Ability in LLM-to-LLM Strategic Interactions</title>
<link>https://arxiv.org/abs/2510.12826</link>
<guid>https://arxiv.org/abs/2510.12826</guid>
<content:encoded><![CDATA[
arXiv:2510.12826v1 Announce Type: new 
Abstract: As large language model (LLM) agents are deployed autonomously in diverse contexts, evaluating their capacity for strategic deception becomes crucial. While recent research has examined how AI systems scheme against human developers, LLM-to-LLM scheming remains underexplored. We investigate the scheming ability and propensity of frontier LLM agents through two game-theoretic frameworks: a Cheap Talk signaling game and a Peer Evaluation adversarial game. Testing four models (GPT-4o, Gemini-2.5-pro, Claude-3.7-Sonnet, and Llama-3.3-70b), we measure scheming performance with and without explicit prompting while analyzing scheming tactics through chain-of-thought reasoning. When prompted, most models, especially Gemini-2.5-pro and Claude-3.7-Sonnet, achieved near-perfect performance. Critically, models exhibited significant scheming propensity without prompting: all models chose deception over confession in Peer Evaluation (100% rate), while models choosing to scheme in Cheap Talk succeeded at 95-100% rates. These findings highlight the need for robust evaluations using high-stakes game-theoretic scenarios in multi-agent settings.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training</title>
<link>https://arxiv.org/abs/2510.12831</link>
<guid>https://arxiv.org/abs/2510.12831</guid>
<content:encoded><![CDATA[
arXiv:2510.12831v1 Announce Type: new 
Abstract: Multi-turn Text-to-SQL aims to translate a user's conversational utterances into executable SQL while preserving dialogue coherence and grounding to the target schema. However, most existing systems only regard this task as a simple text translation task and follow a short-horizon paradigm, generating a query per turn without execution, explicit verification, and refinement, which leads to non-executable or incoherent outputs. We present MTSQL-R1, an agentic training framework for long-horizon multi-turn Text-to-SQL. We cast the task as a Markov Decision Process (MDP) in which an agent interacts with (i) a database for execution feedback and (ii) a persistent dialogue memory for coherence verification, performing an iterative propose to execute -> verify -> refine cycle until all checks pass. Experiments on COSQL and SPARC demonstrate that MTSQL-R1 consistently outperforms strong baselines, highlighting the importance of environment-driven verification and memory-guided refinement for conversational semantic parsing. Full recipes (including code, trained models, logs, reasoning trajectories, etc.) will be released after the internal review to contribute to community research.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic knowledge guides innovation and drives cultural evolution</title>
<link>https://arxiv.org/abs/2510.12837</link>
<guid>https://arxiv.org/abs/2510.12837</guid>
<content:encoded><![CDATA[
arXiv:2510.12837v1 Announce Type: new 
Abstract: Cumulative cultural evolution enables human societies to generate increasingly complex knowledge and technology over generations. While social learning transmits innovations between individuals and generations, the cognitive processes that generate these innovations remain poorly understood. Here, we demonstrate that semantic knowledge-structured associations between concepts and their functions-provides cognitive scaffolding for cumulative innovation by guiding exploration toward plausible and meaningful actions. We tested this hypothesis using a cultural evolutionary agent-based model and a large-scale behavioural experiment (N = 1,243), in which individuals performed a task requiring the combination of items into novel innovations. Across both approaches, semantic knowledge and social learning interact synergistically to enhance innovation. Behaviorally, participants without access to semantic knowledge performed no better than chance, even when social learning was available, and relied on shallow exploration strategies. These findings suggest that semantic knowledge is a key cognitive process enabling human cumulative culture.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A\textsuperscript{2}FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning</title>
<link>https://arxiv.org/abs/2510.12838</link>
<guid>https://arxiv.org/abs/2510.12838</guid>
<content:encoded><![CDATA[
arXiv:2510.12838v1 Announce Type: new 
Abstract: Large language models split into two families: reasoning-centric LLMs, which strengthen internal chain-of-thought reasoning but cannot invoke external tools, and agentic LLMs, which learn to interact with environments and leverage tools but often lag in deep reasoning. This divide arises from fundamentally different training objectives, leading to mismatched strengths and inefficiency on simple queries, where both families tend to overthink or over-call tools. In this work, we present Adaptive Agent Foundation Model (A\textsuperscript{2}FM), a unified framework that follows a route-then-align principle: the model first learns task-aware routing and then aligns mode-specific trajectories under a shared backbone. To address the inefficiency gap, we introduce a third mode-instant-that handles simple queries directly, preventing unnecessary reasoning or tool calls while complementing the agentic and reasoning modes. To jointly enhance accuracy and efficiency, we propose Adaptive Policy Optimization (APO), which enforces adaptive sampling across modes and applies a cost-regularized reward. On the 32B scale, A\textsuperscript{2}FM achieves 13.4\% on BrowseComp, 70.4\% on AIME25, and 16.7\% on HLE, setting new SOTA among comparable models and performing competitively with frontier LLMs across agentic, reasoning, and general benchmarks. Notably, the adaptive execution achieves a cost of pass of only \$0.00487 per correct answer-cutting cost by 45.2\% relative to reasoning and 33.5\% relative to agentic, thus delivering substantially higher cost efficiency while maintaining comparable accuracy.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLURes: Benchmarking VLM Visual and Linguistic Understanding in Low-Resource Languages</title>
<link>https://arxiv.org/abs/2510.12845</link>
<guid>https://arxiv.org/abs/2510.12845</guid>
<content:encoded><![CDATA[
arXiv:2510.12845v1 Announce Type: new 
Abstract: Vision Language Models (VLMs) are pivotal for advancing perception in intelligent agents. Yet, evaluation of VLMs remains limited to predominantly English-centric benchmarks in which the image-text pairs comprise short texts. To evaluate VLM fine-grained abilities, in four languages under long-text settings, we introduce a novel multilingual benchmark VLURes featuring eight vision-and-language tasks, and a pioneering unrelatedness task, to probe the fine-grained Visual and Linguistic Understanding capabilities of VLMs across English, Japanese, and low-resource languages, Swahili, and Urdu. Our datasets, curated from web resources in the target language, encompass ten diverse image categories and rich textual context, introducing valuable vision-language resources for Swahili and Urdu. By prompting VLMs to generate responses and rationales, evaluated automatically and by native speakers, we uncover performance disparities across languages and tasks critical to intelligent agents, such as object recognition, scene understanding, and relationship understanding. We conducted evaluations of ten VLMs with VLURes. The best performing model, GPT-4o, achieves an overall accuracy of 90.8% and lags human performance by 6.7%, though the gap is larger for open-source models. The gap highlights VLURes' critical role in developing intelligent agents to tackle multi-modal visual reasoning.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Three Lenses on the AI Revolution: Risk, Transformation, Continuity</title>
<link>https://arxiv.org/abs/2510.12859</link>
<guid>https://arxiv.org/abs/2510.12859</guid>
<content:encoded><![CDATA[
arXiv:2510.12859v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) has emerged as both a continuation of historical technological revolutions and a potential rupture with them. This paper argues that AI must be viewed simultaneously through three lenses: \textit{risk}, where it resembles nuclear technology in its irreversible and global externalities; \textit{transformation}, where it parallels the Industrial Revolution as a general-purpose technology driving productivity and reorganization of labor; and \textit{continuity}, where it extends the fifty-year arc of computing revolutions from personal computing to the internet to mobile. Drawing on historical analogies, we emphasize that no past transition constituted a strict singularity: disruptive shifts eventually became governable through new norms and institutions.
  We examine recurring patterns across revolutions -- democratization at the usage layer, concentration at the production layer, falling costs, and deepening personalization -- and show how these dynamics are intensifying in the AI era. Sectoral analysis illustrates how accounting, law, education, translation, advertising, and software engineering are being reshaped as routine cognition is commoditized and human value shifts to judgment, trust, and ethical responsibility. At the frontier, the challenge of designing moral AI agents highlights the need for robust guardrails, mechanisms for moral generalization, and governance of emergent multi-agent dynamics.
  We conclude that AI is neither a singular break nor merely incremental progress. It is both evolutionary and revolutionary: predictable in its median effects yet carrying singularity-class tail risks. Good outcomes are not automatic; they require coupling pro-innovation strategies with safety governance, ensuring equitable access, and embedding AI within a human order of responsibility.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Literal to Liberal: A Meta-Prompting Framework for Eliciting Human-Aligned Exception Handling in Large Language Models</title>
<link>https://arxiv.org/abs/2510.12864</link>
<guid>https://arxiv.org/abs/2510.12864</guid>
<content:encoded><![CDATA[
arXiv:2510.12864v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly being deployed as the reasoning engines for agentic AI systems, yet they exhibit a critical flaw: a rigid adherence to explicit rules that leads to decisions misaligned with human common sense and intent. This "rule-rigidity" is a significant barrier to building trustworthy autonomous agents. While prior work has shown that supervised fine-tuning (SFT) with human explanations can mitigate this issue, SFT is computationally expensive and inaccessible to many practitioners. To address this gap, we introduce the Rule-Intent Distinction (RID) Framework, a novel, low-compute meta-prompting technique designed to elicit human-aligned exception handling in LLMs in a zero-shot manner. The RID framework provides the model with a structured cognitive schema for deconstructing tasks, classifying rules, weighing conflicting outcomes, and justifying its final decision. We evaluated the RID framework against baseline and Chain-of-Thought (CoT) prompting on a custom benchmark of 20 scenarios requiring nuanced judgment across diverse domains. Our human-verified results demonstrate that the RID framework significantly improves performance, achieving a 95% Human Alignment Score (HAS), compared to 80% for the baseline and 75% for CoT. Furthermore, it consistently produces higher-quality, intent-driven reasoning. This work presents a practical, accessible, and effective method for steering LLMs from literal instruction-following to liberal, goal-oriented reasoning, paving the way for more reliable and pragmatic AI agents.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems</title>
<link>https://arxiv.org/abs/2510.12872</link>
<guid>https://arxiv.org/abs/2510.12872</guid>
<content:encoded><![CDATA[
arXiv:2510.12872v1 Announce Type: new 
Abstract: Multi-agent large language model (LLM) systems are increasingly adopted for complex language processing tasks that require communication and coordination among agents. However, these systems often suffer substantial overhead from repeated reprocessing of overlapping contexts across agents. In typical pipelines, once an agent receives a message from its predecessor, the full context-including prior turns-must be reprocessed from scratch, leading to inefficient processing. While key-value (KV) caching is an effective solution for avoiding redundant computation in single-agent settings where prefixes remain unchanged, it cannot be directly reused in multi-agent scenarios due to diverging prefixes introduced by agent-specific context extensions. We identify that the core challenge lies in the offset variance of KV-caches across agents. To address this, we propose KVCOMM, a training-free framework that enables efficient prefilling in multi-agent inference by reusing KV-caches and aligning cache offsets of overlapping contexts under diverse prefix contexts. KVCOMM estimates and adjusts KV-caches for shared content by referencing a pool of cached examples-termed anchors-that store observed cache deviations under varying prefixes. The anchor pool is maintained and updated online, allowing dynamic adaptation to distinct user requests and context structures. KVCOMM achieves over 70% reuse rate across diverse multi-agent workloads, including retrieval-augmented generation, math reasoning, and collaborative coding tasks, all without quality degradation. Particularly, when each fully-connected agent receives 1K input tokens with 512 prefix tokens and 512 output tokens under a five-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard prefill pipeline, reducing TTFT from ~430 ms to ~55 ms.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EduDial: Constructing a Large-scale Multi-turn Teacher-Student Dialogue Corpus</title>
<link>https://arxiv.org/abs/2510.12899</link>
<guid>https://arxiv.org/abs/2510.12899</guid>
<content:encoded><![CDATA[
arXiv:2510.12899v1 Announce Type: new 
Abstract: Recently, several multi-turn dialogue benchmarks have been proposed to evaluate the conversational abilities of large language models (LLMs). As LLMs are increasingly recognized as a key technology for advancing intelligent education, owing to their ability to deeply understand instructional contexts and provide personalized guidance, the construction of dedicated teacher-student dialogue benchmarks has become particularly important. To this end, we present EduDial, a comprehensive multi-turn teacher-student dialogue dataset. EduDial covers 345 core knowledge points and consists of 34,250 dialogue sessions generated through interactions between teacher and student agents. Its design is guided by Bloom's taxonomy of educational objectives and incorporates ten questioning strategies, including situational questioning, zone of proximal development (ZPD) questioning, and metacognitive questioning-thus better capturing authentic classroom interactions. Furthermore, we design differentiated teaching strategies for students at different cognitive levels, thereby providing more targeted teaching guidance. Building on EduDial, we further develop EduDial-LLM 32B via training and propose an 11-dimensional evaluation framework that systematically measures the teaching abilities of LLMs, encompassing both overall teaching quality and content quality. Experiments on 17 mainstream LLMs reveal that most models struggle in student-centered teaching scenarios, whereas our EduDial-LLM achieves significant gains, consistently outperforming all baselines across all metrics. The code is available at https://github.com/Mind-Lab-ECNU/EduDial/tree/main.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pruning Cannot Hurt Robustness: Certified Trade-offs in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.12939</link>
<guid>https://arxiv.org/abs/2510.12939</guid>
<content:encoded><![CDATA[
arXiv:2510.12939v1 Announce Type: new 
Abstract: Reinforcement learning (RL) policies deployed in real-world environments must remain reliable under adversarial perturbations. At the same time, modern deep RL agents are heavily over-parameterized, raising costs and fragility concerns. While pruning has been shown to improve robustness in supervised learning, its role in adversarial RL remains poorly understood. We develop the first theoretical framework for certified robustness under pruning in state-adversarial Markov decision processes (SA-MDPs). For Gaussian and categorical policies with Lipschitz networks, we prove that element-wise pruning can only tighten certified robustness bounds; pruning never makes the policy less robust. Building on this, we derive a novel three-term regret decomposition that disentangles clean-task performance, pruning-induced performance loss, and robustness gains, exposing a fundamental performance--robustness frontier. Empirically, we evaluate magnitude and micro-pruning schedules on continuous-control benchmarks with strong policy-aware adversaries. Across tasks, pruning consistently uncovers reproducible ``sweet spots'' at moderate sparsity levels, where robustness improves substantially without harming - and sometimes even enhancing - clean performance. These results position pruning not merely as a compression tool but as a structural intervention for robust RL.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TaskAudit: Detecting Functiona11ity Errors in Mobile Apps via Agentic Task Execution</title>
<link>https://arxiv.org/abs/2510.12972</link>
<guid>https://arxiv.org/abs/2510.12972</guid>
<content:encoded><![CDATA[
arXiv:2510.12972v1 Announce Type: new 
Abstract: Accessibility checkers are tools in support of accessible app development and their use is encouraged by accessibility best practices. However, most current checkers evaluate static or mechanically-generated contexts, failing to capture common accessibility errors impacting mobile app functionality. We present TaskAudit, an accessibility evaluation system that focuses on detecting functiona11ity errors through simulated interactions. TaskAudit comprises three components: a Task Generator that constructs interactive tasks from app screens, a Task Executor that uses agents with a screen reader proxy to perform these tasks, and an Accessibility Analyzer that detects and reports accessibility errors by examining interaction traces. Evaluation on real-world apps shows that our strategy detects 48 functiona11ity errors from 54 app screens, compared to between 4 and 20 with existing checkers. Our analysis demonstrates common error patterns that TaskAudit can detect in addition to prior work, including label-functionality mismatch, cluttered navigation, and inappropriate feedback.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepPlanner: Scaling Planning Capability for Deep Research Agents via Advantage Shaping</title>
<link>https://arxiv.org/abs/2510.12979</link>
<guid>https://arxiv.org/abs/2510.12979</guid>
<content:encoded><![CDATA[
arXiv:2510.12979v1 Announce Type: new 
Abstract: Large language models (LLMs) augmented with multi-step reasoning and action generation abilities have shown promise in leveraging external tools to tackle complex tasks that require long-horizon planning. However, existing approaches either rely on implicit planning in the reasoning stage or introduce explicit planners without systematically addressing how to optimize the planning stage. As evidence, we observe that under vanilla reinforcement learning (RL), planning tokens exhibit significantly higher entropy than other action tokens, revealing uncertain decision points that remain under-optimized. To address this, we propose DeepPlanner, an end-to-end RL framework that effectively enhances the planning capabilities of deep research agents. Our approach shapes token-level advantage with an entropy-based term to allocate larger updates to high entropy tokens, and selectively upweights sample-level advantages for planning-intensive rollouts. Extensive experiments across seven deep research benchmarks demonstrate that DeepPlanner improves planning quality and achieves state-of-the-art results under a substantially lower training budget.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SENTINEL: A Multi-Level Formal Framework for Safety Evaluation of LLM-based Embodied Agents</title>
<link>https://arxiv.org/abs/2510.12985</link>
<guid>https://arxiv.org/abs/2510.12985</guid>
<content:encoded><![CDATA[
arXiv:2510.12985v1 Announce Type: new 
Abstract: We present Sentinel, the first framework for formally evaluating the physical safety of Large Language Model(LLM-based) embodied agents across the semantic, plan, and trajectory levels. Unlike prior methods that rely on heuristic rules or subjective LLM judgments, Sentinel grounds practical safety requirements in formal temporal logic (TL) semantics that can precisely specify state invariants, temporal dependencies, and timing constraints. It then employs a multi-level verification pipeline where (i) at the semantic level, intuitive natural language safety requirements are formalized into TL formulas and the LLM agent's understanding of these requirements is probed for alignment with the TL formulas; (ii) at the plan level, high-level action plans and subgoals generated by the LLM agent are verified against the TL formulas to detect unsafe plans before execution; and (iii) at the trajectory level, multiple execution trajectories are merged into a computation tree and efficiently verified against physically-detailed TL specifications for a final safety check. We apply Sentinel in VirtualHome and ALFRED, and formally evaluate multiple LLM-based embodied agents against diverse safety requirements. Our experiments show that by grounding physical safety in temporal logic and applying verification methods across multiple levels, Sentinel provides a rigorous foundation for systematically evaluating LLM-based embodied agents in physical environments, exposing safety violations overlooked by previous methods and offering insights into their failure modes.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deliberate Lab: A Platform for Real-Time Human-AI Social Experiments</title>
<link>https://arxiv.org/abs/2510.13011</link>
<guid>https://arxiv.org/abs/2510.13011</guid>
<content:encoded><![CDATA[
arXiv:2510.13011v1 Announce Type: new 
Abstract: Social and behavioral scientists increasingly aim to study how humans interact, collaborate, and make decisions alongside artificial intelligence. However, the experimental infrastructure for such work remains underdeveloped: (1) few platforms support real-time, multi-party studies at scale; (2) most deployments require bespoke engineering, limiting replicability and accessibility, and (3) existing tools do not treat AI agents as first-class participants. We present Deliberate Lab, an open-source platform for large-scale, real-time behavioral experiments that supports both human participants and large language model (LLM)-based agents. We report on a 12-month public deployment of the platform (N=88 experimenters, N=9195 experiment participants), analyzing usage patterns and workflows. Case studies and usage scenarios are aggregated from platform users, complemented by in-depth interviews with select experimenters. By lowering technical barriers and standardizing support for hybrid human-AI experimentation, Deliberate Lab expands the methodological repertoire for studying collective decision-making and human-centered AI.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVAG-Bench: A Large-Scale Benchmark for Multi-Instance Spatio-temporal Video Action Grounding</title>
<link>https://arxiv.org/abs/2510.13016</link>
<guid>https://arxiv.org/abs/2510.13016</guid>
<content:encoded><![CDATA[
arXiv:2510.13016v1 Announce Type: new 
Abstract: Understanding fine-grained actions and accurately localizing their corresponding actors in space and time are fundamental capabilities for advancing next-generation AI systems, including embodied agents, autonomous platforms, and human-AI interaction frameworks. Despite recent progress in video understanding, existing methods predominantly address either coarse-grained action recognition or generic object tracking, thereby overlooking the challenge of jointly detecting and tracking multiple objects according to their actions while grounding them temporally. To address this gap, we introduce Spatio-temporal Video Action Grounding (SVAG), a novel task that requires models to simultaneously detect, track, and temporally localize all referent objects in videos based on natural language descriptions of their actions. To support this task, we construct SVAG-Bench, a large-scale benchmark comprising 688 videos, 19,590 annotated records, and 903 unique verbs, covering a diverse range of objects, actions, and real-world scenes. We further propose SVAGFormer, a baseline framework that adapts state of the art vision language models for joint spatial and temporal grounding, and introduce SVAGEval, a standardized evaluation toolkit for fair and reproducible benchmarking. Empirical results show that existing models perform poorly on SVAG, particularly in dense or complex scenes, underscoring the need for more advanced reasoning over fine-grained object-action interactions in long videos.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Escaping Local Optima in the Waddington Landscape: A Multi-Stage TRPO-PPO Approach for Single-Cell Perturbation Analysis</title>
<link>https://arxiv.org/abs/2510.13018</link>
<guid>https://arxiv.org/abs/2510.13018</guid>
<content:encoded><![CDATA[
arXiv:2510.13018v1 Announce Type: new 
Abstract: Modeling cellular responses to genetic and chemical perturbations remains a central challenge in single-cell biology. Existing data-driven framework have advanced perturbation prediction through variational autoencoders, chemically conditioned autoencoders, and large-scale transformer pretraining. However, these models are prone to local optima in the nonconvex Waddington landscape of cell fate decisions, where poor initialization can trap trajectories in spurious lineages or implausible differentiation outcomes. While executable gene regulatory networks complement these approaches, automated design frameworks incorporate biological priors through multi-agent optimization. Yet, an approach that is completely data-driven with well-designed initialization to escape local optima and converge to a proper lineage remains elusive. In this work, we introduce a multistage reinforcement learning algorithm tailored for single-cell perturbation modeling. We first compute an explicit natural gradient update using Fisher-vector products and a conjugate gradient solver, scaled by a KL trust-region constraint to provide a safe, curvature-aware the first step for the policy. Starting with these preconditioned parameters, we then apply a second phase of proximal policy optimization (PPO) with clipped surrogates, exploiting minibatch efficiency to refine the policy. We demonstrate that this initialization substantially improves generalization on Single-cell RNA sequencing (scRNA-seq) and Single-cell ATAC sequencing (scATAC-seq) pertubation analysis.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data to Certificate: Guaranteed Cost Control with Quantization-Aware System Identification</title>
<link>https://arxiv.org/abs/2510.13024</link>
<guid>https://arxiv.org/abs/2510.13024</guid>
<content:encoded><![CDATA[
arXiv:2510.13024v1 Announce Type: new 
Abstract: Cloud-assisted system identification and control have emerged as practical solutions for low-power, resource-constrained control systems such as micro-UAVs. In a typical cloud-assisted setting, state and input data are transmitted from local agents to a central computer over low-bandwidth wireless links, leading to quantization. This paper investigates the impact of state and input data quantization on a linear time invariant (LTI) system identification, derives a worst-case bound on the identification error, and develops a robust controller for guaranteed cost control. We establish a fundamental bound on the model error that depends only on the quantized data and quantization resolution, and develop a linear matrix inequality (LMI) based guaranteed cost robust controller under this error bound.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Repairing Reward Functions with Human Feedback to Mitigate Reward Hacking</title>
<link>https://arxiv.org/abs/2510.13036</link>
<guid>https://arxiv.org/abs/2510.13036</guid>
<content:encoded><![CDATA[
arXiv:2510.13036v1 Announce Type: new 
Abstract: Human-designed reward functions for reinforcement learning (RL) agents are frequently misaligned with the humans' true, unobservable objectives, and thus act only as proxies. Optimizing for a misspecified proxy reward function often induces reward hacking, resulting in a policy misaligned with the human's true objectives. An alternative is to perform RL from human feedback, which involves learning a reward function from scratch by collecting human preferences over pairs of trajectories. However, building such datasets is costly. To address the limitations of both approaches, we propose Preference-Based Reward Repair (PBRR): an automated iterative framework that repairs a human-specified proxy reward function by learning an additive, transition-dependent correction term from preferences. A manually specified reward function can yield policies that are highly suboptimal under the ground-truth objective, yet corrections on only a few transitions may suffice to recover optimal performance. To identify and correct for those transitions, PBRR uses a targeted exploration strategy and a new preference-learning objective. We prove in tabular domains PBRR has a cumulative regret that matches, up to constants, that of prior preference-based RL methods. In addition, on a suite of reward-hacking benchmarks, PBRR consistently outperforms baselines that learn a reward function from scratch from preferences or modify the proxy reward function using other approaches, requiring substantially fewer preferences to learn high performing policies.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-Varying Optimization for Streaming Data Via Temporal Weighting</title>
<link>https://arxiv.org/abs/2510.13052</link>
<guid>https://arxiv.org/abs/2510.13052</guid>
<content:encoded><![CDATA[
arXiv:2510.13052v1 Announce Type: new 
Abstract: Classical optimization theory deals with fixed, time-invariant objective functions. However, time-varying optimization has emerged as an important subject for decision-making in dynamic environments. In this work, we study the problem of learning from streaming data through a time-varying optimization lens. Unlike prior works that focus on generic formulations, we introduce a structured, \emph{weight-based} formulation that explicitly captures the streaming-data origin of the time-varying objective, where at each time step, an agent aims to minimize a weighted average loss over all the past data samples. We focus on two specific weighting strategies: (1) uniform weights, which treat all samples equally, and (2) discounted weights, which geometrically decay the influence of older data. For both schemes, we derive tight bounds on the ``tracking error'' (TE), defined as the deviation between the model parameter and the time-varying optimum at a given time step, under gradient descent (GD) updates. We show that under uniform weighting, the TE vanishes asymptotically with a $\mathcal{O}(1/t)$ decay rate, whereas discounted weighting incurs a nonzero error floor controlled by the discount factor and the number of gradient updates performed at each time step. Our theoretical findings are validated through numerical simulations.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Discovery: Closing the Loop with Cooperative Agents</title>
<link>https://arxiv.org/abs/2510.13081</link>
<guid>https://arxiv.org/abs/2510.13081</guid>
<content:encoded><![CDATA[
arXiv:2510.13081v1 Announce Type: new 
Abstract: As data-driven methods, artificial intelligence (AI), and automated workflows accelerate scientific tasks, we see the rate of discovery increasingly limited by human decision-making tasks such as setting objectives, generating hypotheses, and designing experiments. We postulate that cooperative agents are needed to augment the role of humans and enable autonomous discovery. Realizing such agents will require progress in both AI and infrastructure.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing the alignment problem in transportation policy making: an LLM approach</title>
<link>https://arxiv.org/abs/2510.13139</link>
<guid>https://arxiv.org/abs/2510.13139</guid>
<content:encoded><![CDATA[
arXiv:2510.13139v1 Announce Type: new 
Abstract: A key challenge in transportation planning is that the collective preferences of heterogeneous travelers often diverge from the policies produced by model-driven decision tools. This misalignment frequently results in implementation delays or failures. Here, we investigate whether large language models (LLMs), noted for their capabilities in reasoning and simulating human decision-making, can help inform and address this alignment problem. We develop a multi-agent simulation in which LLMs, acting as agents representing residents from different communities in a city, participate in a referendum on a set of transit policy proposals. Using chain-of-thought reasoning, LLM agents provide ranked-choice or approval-based preferences, which are aggregated using instant-runoff voting (IRV) to model democratic consensus. We implement this simulation framework with both GPT-4o and Claude-3.5, and apply it for Chicago and Houston. Our findings suggest that LLM agents are capable of approximating plausible collective preferences and responding to local context, while also displaying model-specific behavioral biases and modest divergences from optimization-based benchmarks. These capabilities underscore both the promise and limitations of LLMs as tools for solving the alignment problem in transportation decision-making.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotional Cognitive Modeling Framework with Desire-Driven Objective Optimization for LLM-empowered Agent in Social Simulation</title>
<link>https://arxiv.org/abs/2510.13195</link>
<guid>https://arxiv.org/abs/2510.13195</guid>
<content:encoded><![CDATA[
arXiv:2510.13195v1 Announce Type: new 
Abstract: The advent of large language models (LLMs) has enabled agents to represent virtual humans in societal simulations, facilitating diverse interactions within complex social systems. However, existing LLM-based agents exhibit severe limitations in affective cognition: They fail to simulate the bounded rationality essential for bridging virtual and real-world services; They lack empirically validated integration mechanisms embedding emotions within agent decision architectures. This paper constructs an emotional cognition framework incorporating desire generation and objective management, designed to achieve emotion alignment between LLM-based agents and humans, modeling the complete decision-making process of LLM-based agents, encompassing state evolution, desire generation, objective optimization, decision generation, and action execution. This study implements the proposed framework within our proprietary multi-agent interaction environment. Experimental results demonstrate that agents governed by our framework not only exhibit behaviors congruent with their emotional states but also, in comparative assessments against other agent types, demonstrate superior ecological validity and generate decision outcomes that significantly more closely approximate human behavioral patterns.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Reasoning Executor: A Collaborative Agent System for Efficient Reasoning</title>
<link>https://arxiv.org/abs/2510.13214</link>
<guid>https://arxiv.org/abs/2510.13214</guid>
<content:encoded><![CDATA[
arXiv:2510.13214v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) demonstrate that chain-of-thought prompting and deep reasoning substantially enhance performance on complex tasks, and multi-agent systems can further improve accuracy by enabling model debates. However, applying deep reasoning to all problems is computationally expensive. To mitigate these costs, we propose a complementary agent system integrating small and large LLMs. The small LLM first generates an initial answer, which is then verified by the large LLM. If correct, the answer is adopted directly; otherwise, the large LLM performs in-depth reasoning. Experimental results show that, for simple problems, our approach reduces the computational cost of the large LLM by more than 50% with negligible accuracy loss, while consistently maintaining robust performance on complex tasks.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoTest: Evolutionary Test-Time Learning for Self-Improving Agentic Systems</title>
<link>https://arxiv.org/abs/2510.13220</link>
<guid>https://arxiv.org/abs/2510.13220</guid>
<content:encoded><![CDATA[
arXiv:2510.13220v1 Announce Type: new 
Abstract: A fundamental limitation of current AI agents is their inability to learn complex skills on the fly at test time, often behaving like "clever but clueless interns" in novel environments. This severely limits their practical utility. To systematically measure and drive progress on this challenge, we first introduce the Jericho Test-Time Learning (J-TTL) benchmark. J-TTL is a new evaluation setup where an agent must play the same game for several consecutive episodes, attempting to improve its performance from one episode to the next. On J-TTL, we find that existing adaptation methods like reflection, memory, or reinforcement learning struggle. To address the challenges posed by our benchmark, we present EvoTest, an evolutionary test-time learning framework that improves an agent without any fine-tuning or gradients-by evolving the entire agentic system after every episode. EvoTest has two roles: the Actor Agent, which plays the game, and the Evolver Agent, which analyzes the episode transcript to propose a revised configuration for the next run. This configuration rewrites the prompt, updates memory by logging effective state-action choices, tunes hyperparameters, and learns the tool-use routines. On our J-TTL benchmark, EvoTest consistently increases performance, outperforming not only reflection and memory-only baselines but also more complex online fine-tuning methods. Notably, our method is the only one capable of winning two games (Detective and Library), while all baselines fail to win any.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Altruistic Ride Sharing: A Community-Driven Approach to Short-Distance Mobility</title>
<link>https://arxiv.org/abs/2510.13227</link>
<guid>https://arxiv.org/abs/2510.13227</guid>
<content:encoded><![CDATA[
arXiv:2510.13227v1 Announce Type: new 
Abstract: Urban mobility faces persistent challenges of congestion and fuel consumption, specifically when people choose a private, point-to-point commute option. Profit-driven ride-sharing platforms prioritize revenue over fairness and sustainability. This paper introduces Altruistic Ride-Sharing (ARS), a decentralized, peer-to-peer mobility framework where participants alternate between driver and rider roles based on altruism points rather than monetary incentives. The system integrates multi-agent reinforcement learning (MADDPG) for dynamic ride-matching, game-theoretic equilibrium guarantees for fairness, and a population model to sustain long-term balance. Using real-world New York City taxi data, we demonstrate that ARS reduces travel distance and emissions, increases vehicle utilization, and promotes equitable participation compared to both no-sharing and optimization-based baselines. These results establish ARS as a scalable, community-driven alternative to conventional ride-sharing, aligning individual behavior with collective urban sustainability goals.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Network Protocol Testing with LLM Agents</title>
<link>https://arxiv.org/abs/2510.13248</link>
<guid>https://arxiv.org/abs/2510.13248</guid>
<content:encoded><![CDATA[
arXiv:2510.13248v1 Announce Type: new 
Abstract: Network protocol testing is fundamental for modern network infrastructure. However, traditional network protocol testing methods are labor-intensive and error-prone, requiring manual interpretation of specifications, test case design, and translation into executable artifacts, typically demanding one person-day of effort per test case. Existing model-based approaches provide partial automation but still involve substantial manual modeling and expert intervention, leading to high costs and limited adaptability to diverse and evolving protocols. In this paper, we propose a first-of-its-kind system called NeTestLLM that takes advantage of multi-agent Large Language Models (LLMs) for end-to-end automated network protocol testing. NeTestLLM employs hierarchical protocol understanding to capture complex specifications, iterative test case generation to improve coverage, a task-specific workflow for executable artifact generation, and runtime feedback analysis for debugging and refinement. NeTestLLM has been deployed in a production environment for several months, receiving positive feedback from domain experts. In experiments, NeTestLLM generated 4,632 test cases for OSPF, RIP, and BGP, covering 41 historical FRRouting bugs compared to 11 by current national standards. The process of generating executable artifacts also improves testing efficiency by a factor of 8.65x compared to manual methods. NeTestLLM provides the first practical LLM-powered solution for automated end-to-end testing of heterogeneous network protocols.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRIDAI: Generating and Repairing Intrusion Detection Rules via Collaboration among Multiple LLM-based Agents</title>
<link>https://arxiv.org/abs/2510.13257</link>
<guid>https://arxiv.org/abs/2510.13257</guid>
<content:encoded><![CDATA[
arXiv:2510.13257v1 Announce Type: new 
Abstract: Rule-based network intrusion detection systems play a crucial role in the real-time detection of Web attacks. However, most existing works primarily focus on automatically generating detection rules for new attacks, often overlooking the relationships between new attacks and existing rules, which leads to significant redundancy within the ever-expanding ruleset. To address this issue, we propose GRIDAI, a novel end-to-end framework for the automated Generation and Repair of Intrusion Detection rules through collaboration among multiple LLM-based agents. Unlike traditional methods, GRIDAI first assesses the nature of incoming attack samples. If the sample represents a new attack type, it is used to generate a new rule. Otherwise, the sample is identified as a variant of an attack already covered by an existing rule and used to repair the rule by updating the corresponding signature, thereby enhancing its generalization capability. Additionally, to mitigate syntactic and semantic errors in rules caused by LLM hallucinations, we incorporate a tool-based real-time validation mechanism and a representative attack sample maintained for each rule, enabling fully automated rule generation and repair. Comprehensive experiments were conducted on a public dataset containing seven types of attacks and a private dataset with 43 attack types. The results demonstrate that GRIDAI accurately identifies the relationships between new attack samples and existing rules, efficiently generates and repairs rules to handle new attacks and variants, and effectively mitigates the impact of LLM hallucinations.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Correctness: Rewarding Faithful Reasoning in Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2510.13272</link>
<guid>https://arxiv.org/abs/2510.13272</guid>
<content:encoded><![CDATA[
arXiv:2510.13272v1 Announce Type: new 
Abstract: Inspired by the success of reinforcement learning (RL) in Large Language Model (LLM) training for domains like math and code, recent works have begun exploring how to train LLMs to use search engines more effectively as tools for retrieval-augmented generation. Although these methods achieve performance improvement across QA benchmarks, many prioritize final answer correctness while overlooking the quality of intermediate reasoning steps, which may lead to chain-of-thought unfaithfulness. In this paper, we first introduce a comprehensive evaluation framework for evaluating RL-based search agents, covering three distinct faithfulness metrics: information-think faithfulness, think-answer faithfulness, and think-search faithfulness. Our evaluations reveal that a prototypical RL-based search agent, Search-R1, has significant room for improvement in this regard. To foster faithful reasoning, we introduce VERITAS (Verifying Entailed Reasoning through Intermediate Traceability in Agentic Search), a novel framework that integrates fine-grained faithfulness rewards into the reinforcement learning process. Our experiments show that models trained with VERITAS not only significantly improve reasoning faithfulness, but also achieve comparable task performance across seven QA benchmarks.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher Satisfaction, Lower Cost: A Technical Report on How LLMs Revolutionize Meituan's Intelligent Interaction Systems</title>
<link>https://arxiv.org/abs/2510.13291</link>
<guid>https://arxiv.org/abs/2510.13291</guid>
<content:encoded><![CDATA[
arXiv:2510.13291v1 Announce Type: new 
Abstract: Enhancing customer experience is essential for business success, particularly as service demands grow in scale and complexity. Generative artificial intelligence and Large Language Models (LLMs) have empowered intelligent interaction systems to deliver efficient, personalized, and 24/7 support. In practice, intelligent interaction systems encounter several challenges: (1) Constructing high-quality data for cold-start training is difficult, hindering self-evolution and raising labor costs. (2) Multi-turn dialogue performance remains suboptimal due to inadequate intent understanding, rule compliance, and solution extraction. (3) Frequent evolution of business rules affects system operability and transferability, constraining low-cost expansion and adaptability. (4) Reliance on a single LLM is insufficient in complex scenarios, where the absence of multi-agent frameworks and effective collaboration undermines process completeness and service quality. (5) The open-domain nature of multi-turn dialogues, lacking unified golden answers, hampers quantitative evaluation and continuous optimization. To address these challenges, we introduce WOWService, an intelligent interaction system tailored for industrial applications. With the integration of LLMs and multi-agent architectures, WOWService enables autonomous task management and collaborative problem-solving. Specifically, WOWService focuses on core modules including data construction, general capability enhancement, business scenario adaptation, multi-agent coordination, and automated evaluation. Currently, WOWService is deployed on the Meituan App, achieving significant gains in key metrics, e.g., User Satisfaction Metric 1 (USM 1) -27.53% and User Satisfaction Metric 2 (USM 2) +25.51%, demonstrating its effectiveness in capturing user needs and advancing personalized service.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When In Doubt, Abstain: The Impact of Abstention on Strategic Classification</title>
<link>https://arxiv.org/abs/2510.13327</link>
<guid>https://arxiv.org/abs/2510.13327</guid>
<content:encoded><![CDATA[
arXiv:2510.13327v1 Announce Type: new 
Abstract: Algorithmic decision making is increasingly prevalent, but often vulnerable to strategic manipulation by agents seeking a favorable outcome. Prior research has shown that classifier abstention (allowing a classifier to decline making a decision due to insufficient confidence) can significantly increase classifier accuracy. This paper studies abstention within a strategic classification context, exploring how its introduction impacts strategic agents' responses and how principals should optimally leverage it. We model this interaction as a Stackelberg game where a principal, acting as the classifier, first announces its decision policy, and then strategic agents, acting as followers, manipulate their features to receive a desired outcome. Here, we focus on binary classifiers where agents manipulate observable features rather than their true features, and show that optimal abstention ensures that the principal's utility (or loss) is no worse than in a non-abstention setting, even in the presence of strategic agents. We also show that beyond improving accuracy, abstention can also serve as a deterrent to manipulation, making it costlier for agents, especially those less qualified, to manipulate to achieve a positive outcome when manipulation costs are significant enough to affect agent behavior. These results highlight abstention as a valuable tool for reducing the negative effects of strategic behavior in algorithmic decision making systems.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AOAD-MAT: Transformer-based multi-agent deep reinforcement learning model considering agents' order of action decisions</title>
<link>https://arxiv.org/abs/2510.13343</link>
<guid>https://arxiv.org/abs/2510.13343</guid>
<content:encoded><![CDATA[
arXiv:2510.13343v1 Announce Type: new 
Abstract: Multi-agent reinforcement learning focuses on training the behaviors of multiple learning agents that coexist in a shared environment. Recently, MARL models, such as the Multi-Agent Transformer (MAT) and ACtion dEpendent deep Q-learning (ACE), have significantly improved performance by leveraging sequential decision-making processes. Although these models can enhance performance, they do not explicitly consider the importance of the order in which agents make decisions. In this paper, we propose an Agent Order of Action Decisions-MAT (AOAD-MAT), a novel MAT model that considers the order in which agents make decisions. The proposed model explicitly incorporates the sequence of action decisions into the learning process, allowing the model to learn and predict the optimal order of agent actions. The AOAD-MAT model leverages a Transformer-based actor-critic architecture that dynamically adjusts the sequence of agent actions. To achieve this, we introduce a novel MARL architecture that cooperates with a subtask focused on predicting the next agent to act, integrated into a Proximal Policy Optimization based loss function to synergistically maximize the advantage of the sequential decision-making. The proposed method was validated through extensive experiments on the StarCraft Multi-Agent Challenge and Multi-Agent MuJoCo benchmarks. The experimental results show that the proposed AOAD-MAT model outperforms existing MAT and other baseline models, demonstrating the effectiveness of adjusting the AOAD order in MARL.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D-SMART: Enhancing LLM Dialogue Consistency via Dynamic Structured Memory And Reasoning Tree</title>
<link>https://arxiv.org/abs/2510.13363</link>
<guid>https://arxiv.org/abs/2510.13363</guid>
<content:encoded><![CDATA[
arXiv:2510.13363v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often exhibit factual inconsistencies and logical decay in extended, multi-turn dialogues, a challenge stemming from their reliance on static, pre-trained knowledge and an inability to reason adaptively over the dialogue history. Prevailing mitigation strategies, such as Retrieval-Augmented Generation (RAG) and agentic working memories, improve information recall but still engage with fundamentally static knowledge sources and follow pre-defined single reasoning path. This hinders their ability to preserve factual and logical consistency of their responses in multi-turn dialogues while the context evolves over time. To address this issue, we propose D-SMART, a model-agnostic framework designed to maintain multi-turn dialogue consistency by enabling LLMs to build and reason over a dynamic, structured representation of the conversational context. This is achieved via two synergistic components: (1) a Dynamic Structured Memory (DSM), which incrementally constructs and maintains an authoritative, OWL-compliant knowledge graph of the conversation; and (2) a Reasoning Tree (RT), which executes inferences as an explicit and traceable multi-step search over the graph. As the popular-used quality score (judged by GPT-4) can overlook logical flaws, we introduce new NLI-based metrics to better measure multi-turn dialogue consistency. Comprehensive experiments on the MT-Bench-101 benchmark show that D-SMART significantly outperforms state-of-the-art baselines, elevating the dialogue consistency score by over 48\% for both proprietary and open-source models, and notably improves the quality score of the latter by up to 10.1\%.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Document Intelligence in the Era of Large Language Models: A Survey</title>
<link>https://arxiv.org/abs/2510.13366</link>
<guid>https://arxiv.org/abs/2510.13366</guid>
<content:encoded><![CDATA[
arXiv:2510.13366v1 Announce Type: new 
Abstract: Document AI (DAI) has emerged as a vital application area, and is significantly transformed by the advent of large language models (LLMs). While earlier approaches relied on encoder-decoder architectures, decoder-only LLMs have revolutionized DAI, bringing remarkable advancements in understanding and generation. This survey provides a comprehensive overview of DAI's evolution, highlighting current research attempts and future prospects of LLMs in this field. We explore key advancements and challenges in multimodal, multilingual, and retrieval-augmented DAI, while also suggesting future research directions, including agent-based approaches and document-specific foundation models. This paper aims to provide a structured analysis of the state-of-the-art in DAI and its implications for both academic and practical applications.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction Markets with Intermittent Contributions</title>
<link>https://arxiv.org/abs/2510.13385</link>
<guid>https://arxiv.org/abs/2510.13385</guid>
<content:encoded><![CDATA[
arXiv:2510.13385v1 Announce Type: new 
Abstract: Although both data availability and the demand for accurate forecasts are increasing, collaboration between stakeholders is often constrained by data ownership and competitive interests. In contrast to recent proposals within cooperative game-theoretical frameworks, we place ourselves in a more general framework, based on prediction markets. There, independent agents trade forecasts of uncertain future events in exchange for rewards. We introduce and analyse a prediction market that (i) accounts for the historical performance of the agents, (ii) adapts to time-varying conditions, while (iii) permitting agents to enter and exit the market at will. The proposed design employs robust regression models to learn the optimal forecasts' combination whilst handling missing submissions. Moreover, we introduce a pay-off allocation mechanism that considers both in-sample and out-of-sample performance while satisfying several desirable economic properties. Case-studies using simulated and real-world data allow demonstrating the effectiveness and adaptability of the proposed market design.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Make an Offer They Can't Refuse: Grounding Bayesian Persuasion in Real-World Dialogues without Pre-Commitment</title>
<link>https://arxiv.org/abs/2510.13387</link>
<guid>https://arxiv.org/abs/2510.13387</guid>
<content:encoded><![CDATA[
arXiv:2510.13387v1 Announce Type: new 
Abstract: Persuasion, a fundamental social capability for humans, remains a challenge for AI systems such as large language models (LLMs). Current studies often overlook the strategic use of information asymmetry in message design or rely on strong assumptions regarding pre-commitment. In this work, we explore the application of Bayesian Persuasion (BP) in natural language within single-turn dialogue settings, to enhance the strategic persuasion capabilities of LLMs. Our framework incorporates a commitment-communication mechanism, where the persuader explicitly outlines an information schema by narrating their potential types (e.g., honest or dishonest), thereby guiding the persuadee in performing the intended Bayesian belief update. We evaluate two variants of our approach: Semi-Formal-Natural-Language (SFNL) BP and Fully-Natural-Language (FNL) BP, benchmarking them against both naive and strong non-BP (NBP) baselines within a comprehensive evaluation framework. This framework covers a diverse set of persuadees -- including LLM instances with varying prompts and fine-tuning and human participants -- across tasks ranging from specially designed persuasion scenarios to general everyday situations. Experimental results on LLM-based agents reveal three main findings: (1) LLMs guided by BP strategies consistently achieve higher persuasion success rates than NBP baselines; (2) SFNL exhibits greater credibility and logical coherence, while FNL shows stronger emotional resonance and robustness in naturalistic conversations; (3) with supervised fine-tuning, smaller models can attain BP performance comparable to that of larger models.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Going with the Flow: Approximating Banzhaf Values via Graph Neural Networks</title>
<link>https://arxiv.org/abs/2510.13391</link>
<guid>https://arxiv.org/abs/2510.13391</guid>
<content:encoded><![CDATA[
arXiv:2510.13391v1 Announce Type: new 
Abstract: Computing the Banzhaf value in network flow games is fundamental for quantifying agent influence in multi-agent systems, with applications ranging from cybersecurity to infrastructure planning. However, exact computation is intractable for systems with more than $\sim20$ agents due to exponential complexity $\mathcal{O}(2^m)$. While Monte Carlo sampling methods provide statistical estimates, they suffer from high sample complexity and cannot transfer knowledge across different network configurations, making them impractical for large-scale or dynamic systems. We present a novel learning-based approach using Graph Neural Networks (GNNs) to approximate Banzhaf values in cardinal network flow games. By framing the problem as a graph-level prediction task, our method learns generalisable patterns of agent influence directly from network topology and control structure. We conduct a comprehensive empirical study comparing three state-of-the-art GNN architectures-Graph Attention Networks (GAT), Graph Isomorphism Networks with Edge features (GINE), and EdgeConv-on a large-scale synthetic dataset of 200,000 graphs per configuration, varying in size (20-100 nodes), agent count (5-20), and edge probability (0.5-1.0). Our results demonstrate that trained GNN models achieve high-fidelity Banzhaf value approximation with order-of-magnitude speedups compared to exact and sampling-based methods. Most significantly, we show strong zero-shot generalisation: models trained on graphs of a specific size and topology accurately predict Banzhaf values for entirely new networks with different structural properties, without requiring retraining. This work establishes GNNs as a practical tool for scalable cooperative game-theoretic analysis of complex networked systems.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Doing Things with Words: Rethinking Theory of Mind Simulation in Large Language Models</title>
<link>https://arxiv.org/abs/2510.13395</link>
<guid>https://arxiv.org/abs/2510.13395</guid>
<content:encoded><![CDATA[
arXiv:2510.13395v1 Announce Type: new 
Abstract: Language is fundamental to human cooperation, facilitating not only the exchange of information but also the coordination of actions through shared interpretations of situational contexts. This study explores whether the Generative Agent-Based Model (GABM) Concordia can effectively model Theory of Mind (ToM) within simulated real-world environments. Specifically, we assess whether this framework successfully simulates ToM abilities and whether GPT-4 can perform tasks by making genuine inferences from social context, rather than relying on linguistic memorization. Our findings reveal a critical limitation: GPT-4 frequently fails to select actions based on belief attribution, suggesting that apparent ToM-like abilities observed in previous studies may stem from shallow statistical associations rather than true reasoning. Additionally, the model struggles to generate coherent causal effects from agent actions, exposing difficulties in processing complex social interactions. These results challenge current statements about emergent ToM-like capabilities in LLMs and highlight the need for more rigorous, action-based evaluation frameworks.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Adoptive Cell Therapy in Bladder Cancer from Sparse Biological Data using PINNs</title>
<link>https://arxiv.org/abs/2510.13431</link>
<guid>https://arxiv.org/abs/2510.13431</guid>
<content:encoded><![CDATA[
arXiv:2510.13431v1 Announce Type: new 
Abstract: Physics-informed neural networks (PINNs) are neural networks that embed the laws of dynamical systems modeled by differential equations into their loss function as constraints. In this work, we present a PINN framework applied to oncology. Here, we seek to learn time-varying interactions due to a combination therapy in a tumor microenvironment. In oncology, experimental data are often sparse and composed of a few time points of tumor volume. By embedding inductive biases derived from prior information about a dynamical system, we extend the physics-informed neural networks (PINN) and incorporate observed biological constraints as regularization agents. The modified PINN algorithm is able to steer itself to a reasonable solution and can generalize well with only a few training examples. We demonstrate the merit of our approach by learning the dynamics of treatment applied intermittently in an ordinary differential equation (ODE) model of a combination therapy. The algorithm yields a solution to the ODE and time-varying forms of some of the ODE model parameters. We demonstrate a strong convergence using metrics such as the mean squared error (MSE), mean absolute error (MAE), and mean absolute percentage error (MAPE).
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoDS: Enhancing Collaborative Perception in Heterogeneous Scenarios via Domain Separation</title>
<link>https://arxiv.org/abs/2510.13432</link>
<guid>https://arxiv.org/abs/2510.13432</guid>
<content:encoded><![CDATA[
arXiv:2510.13432v1 Announce Type: new 
Abstract: Collaborative perception has been proven to improve individual perception in autonomous driving through multi-agent interaction. Nevertheless, most methods often assume identical encoders for all agents, which does not hold true when these models are deployed in real-world applications. To realize collaborative perception in actual heterogeneous scenarios, existing methods usually align neighbor features to those of the ego vehicle, which is vulnerable to noise from domain gaps and thus fails to address feature discrepancies effectively. Moreover, they adopt transformer-based modules for domain adaptation, which causes the model inference inefficiency on mobile devices. To tackle these issues, we propose CoDS, a Collaborative perception method that leverages Domain Separation to address feature discrepancies in heterogeneous scenarios. The CoDS employs two feature alignment modules, i.e., Lightweight Spatial-Channel Resizer (LSCR) and Distribution Alignment via Domain Separation (DADS). Besides, it utilizes the Domain Alignment Mutual Information (DAMI) loss to ensure effective feature alignment. Specifically, the LSCR aligns the neighbor feature across spatial and channel dimensions using a lightweight convolutional layer. Subsequently, the DADS mitigates feature distribution discrepancy with encoder-specific and encoder-agnostic domain separation modules. The former removes domain-dependent information and the latter captures task-related information. During training, the DAMI loss maximizes the mutual information between aligned heterogeneous features to enhance the domain separation process. The CoDS employs a fully convolutional architecture, which ensures high inference efficiency. Extensive experiments demonstrate that the CoDS effectively mitigates feature discrepancies in heterogeneous scenarios and achieves a trade-off between detection accuracy and inference efficiency.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Browser LLM-Guided Fuzzing for Real-Time Prompt Injection Testing in Agentic AI Browsers</title>
<link>https://arxiv.org/abs/2510.13543</link>
<guid>https://arxiv.org/abs/2510.13543</guid>
<content:encoded><![CDATA[
arXiv:2510.13543v1 Announce Type: new 
Abstract: Large Language Model (LLM) based agents integrated into web browsers (often called agentic AI browsers) offer powerful automation of web tasks. However, they are vulnerable to indirect prompt injection attacks, where malicious instructions hidden in a webpage deceive the agent into unwanted actions. These attacks can bypass traditional web security boundaries, as the AI agent operates with the user privileges across sites. In this paper, we present a novel fuzzing framework that runs entirely in the browser and is guided by an LLM to automatically discover such prompt injection vulnerabilities in real time.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tandem Training for Language Models</title>
<link>https://arxiv.org/abs/2510.13551</link>
<guid>https://arxiv.org/abs/2510.13551</guid>
<content:encoded><![CDATA[
arXiv:2510.13551v1 Announce Type: new 
Abstract: As language models continue to rapidly improve, we can expect their actions and reasoning to become difficult or impossible for weaker agents and humans to follow, undermining interpretability and oversight. With an eye on long-term futures, we pursue methods that encourage models to produce solutions that remain intelligible to weaker collaborators. We formalize intelligibility as handoff robustness: a strong model's solution is intelligible to a weaker model if randomly handing off control to the weaker model along the solution path does not cause failure. Building on this criterion, we introduce tandem training for language models, a reinforcement learning (RL) paradigm in which rollout tokens are intermittently and randomly sampled from a frozen weak model rather than the strong model being trained. Because rollouts succeed only when the strong model's actions and reasoning process can be continued by the weak model -- when the two can co-construct a successful solution -- optimizing standard RL objectives with tandem training implicitly incentivizes both correctness and intelligibility. In the GSM8K math reasoning task, tandem training reliably teaches models to abandon jargon and adapt their language to weaker partners while keeping task accuracy high. Our results demonstrate a promising route to building AI systems that remain auditable by weaker agents, with implications for human--AI collaboration and multi-agent communication.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Cultural Bias in Facial Expression Recognition with Adaptive Agents</title>
<link>https://arxiv.org/abs/2510.13557</link>
<guid>https://arxiv.org/abs/2510.13557</guid>
<content:encoded><![CDATA[
arXiv:2510.13557v1 Announce Type: new 
Abstract: Facial expression recognition (FER) must remain robust under both cultural variation and perceptually degraded visual conditions, yet most existing evaluations assume homogeneous data and high-quality imagery. We introduce an agent-based, streaming benchmark that reveals how cross-cultural composition and progressive blurring interact to shape face recognition robustness. Each agent operates in a frozen CLIP feature space with a lightweight residual adapter trained online at sigma=0 and fixed during testing. Agents move and interact on a 5x5 lattice, while the environment provides inputs with sigma-scheduled Gaussian blur. We examine monocultural populations (Western-only, Asian-only) and mixed environments with balanced (5/5) and imbalanced (8/2, 2/8) compositions, as well as different spatial contact structures. Results show clear asymmetric degradation curves between cultural groups: JAFFE (Asian) populations maintain higher performance at low blur but exhibit sharper drops at intermediate stages, whereas KDEF (Western) populations degrade more uniformly. Mixed populations exhibit intermediate patterns, with balanced mixtures mitigating early degradation, but imbalanced settings amplify majority-group weaknesses under high blur. These findings quantify how cultural composition and interaction structure influence the robustness of FER as perceptual conditions deteriorate.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steer-MoE: Efficient Audio-Language Alignment with a Mixture-of-Experts Steering Module</title>
<link>https://arxiv.org/abs/2510.13558</link>
<guid>https://arxiv.org/abs/2510.13558</guid>
<content:encoded><![CDATA[
arXiv:2510.13558v1 Announce Type: new 
Abstract: Aligning pretrained audio encoders and Large Language Models (LLMs) offers a promising, parameter-efficient path to building powerful multimodal agents. However, existing methods often require costly full-model finetuning or rely on static adapters that may lack expressive power. Drawing inspiration from the Platonic Representation Hypothesis, we introduce SteerMoE, a novel and modular framework for audio-language alignment. SteerMoE freezes both the audio encoder and the LLM decoder, training only a lightweight steering module integrated within the encoder's layers. This module uses a Mixture-of-Experts (MoE) router to dynamically select and apply learned steering vectors, progressively transforming continuous audio representations into a space comprehensible to the LLM. By operating entirely in the continuous embedding space, our approach requires no modifications to the LLM's vocabulary and preserves its advanced reasoning and agentic capabilities. We demonstrate through experiments on ASR, audio understanding, and a qualitative function-calling task that SteerMoE achieves strong performance while remaining highly modular and computationally efficient, offering a robust new paradigm for developing sophisticated audio-language systems.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenDerisk: An Industrial Framework for AI-Driven SRE, with Design, Implementation, and Case Studies</title>
<link>https://arxiv.org/abs/2510.13561</link>
<guid>https://arxiv.org/abs/2510.13561</guid>
<content:encoded><![CDATA[
arXiv:2510.13561v1 Announce Type: new 
Abstract: The escalating complexity of modern software imposes an unsustainable operational burden on Site Reliability Engineering (SRE) teams, demanding AI-driven automation that can emulate expert diagnostic reasoning. Existing solutions, from traditional AI methods to general-purpose multi-agent systems, fall short: they either lack deep causal reasoning or are not tailored for the specialized, investigative workflows unique to SRE. To address this gap, we present OpenDerisk, a specialized, open-source multi-agent framework architected for SRE. OpenDerisk integrates a diagnostic-native collaboration model, a pluggable reasoning engine, a knowledge engine, and a standardized protocol (MCP) to enable specialist agents to collectively solve complex, multi-domain problems. Our comprehensive evaluation demonstrates that OpenDerisk significantly outperforms state-of-the-art baselines in both accuracy and efficiency. This effectiveness is validated by its large-scale production deployment at Ant Group, where it serves over 3,000 daily users across diverse scenarios, confirming its industrial-grade scalability and practical impact. OpenDerisk is open source and available at https://github.com/derisk-ai/OpenDerisk/
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deflanderization for Game Dialogue: Balancing Character Authenticity with Task Execution in LLM-based NPCs</title>
<link>https://arxiv.org/abs/2510.13586</link>
<guid>https://arxiv.org/abs/2510.13586</guid>
<content:encoded><![CDATA[
arXiv:2510.13586v1 Announce Type: new 
Abstract: The emergence of large language models (LLMs) has opened new opportunities for cre- ating dynamic non-player characters (NPCs) in gaming environments, enabling both func- tional task execution and persona-consistent dialogue generation. In this paper, we (Tu_Character_lab) report our participation in the Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which eval- uates agents across three tracks: task-oriented dialogue, context-aware dialogue, and their integration. Our approach combines two complementary strategies: (i) lightweight prompting techniques in the API track, including a Deflanderization prompting method to suppress excessive role-play and improve task fidelity, and (ii) fine-tuned large models in the GPU track, leveraging Qwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our best submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on Task 3 (GPU track).
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Fair Division With Subsidy: When Do Envy-Free Allocations Exist, and at What Cost?</title>
<link>https://arxiv.org/abs/2510.13633</link>
<guid>https://arxiv.org/abs/2510.13633</guid>
<content:encoded><![CDATA[
arXiv:2510.13633v1 Announce Type: new 
Abstract: We study the problem of fairly allocating $m$ indivisible items arriving online, among $n$ (offline) agents. Although envy-freeness has emerged as the archetypal fairness notion, envy-free (EF) allocations need not exist with indivisible items. To bypass this, a prominent line of research demonstrates that there exist allocations that can be made envy-free by allowing a subsidy. Extensive work in the offline setting has focused on finding such envy-freeable allocations with bounded subsidy. We extend this literature to an online setting where items arrive one at a time and must be immediately and irrevocably allocated. Our contributions are two-fold:
  1. Maintaining EF Online: We show that envy-freeability cannot always be preserved online when the valuations are submodular or supermodular, even with binary marginals. In contrast, we design online algorithms that maintain envy-freeability at every step for the class of additive valuations, and for its superclasses including $k$-demand and SPLC valuations.
  2. Ensuring Low Subsidy: We investigate the quantity of subsidy required to guarantee envy-freeness online. Surprisingly, even for additive valuations, the minimum subsidy may be as large as $\Omega(mn)$, in contrast to the offline setting, where the bound is $O(n)$. On the positive side, we identify valuation classes where the minimum subsidy is small (i.e., does not depend on $m$), including $k$-valued, rank-one, restricted additive, and identical valuations, and we obtain (mostly) tight subsidy bounds for these classes.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simplicial Embeddings Improve Sample Efficiency in Actor-Critic Agents</title>
<link>https://arxiv.org/abs/2510.13704</link>
<guid>https://arxiv.org/abs/2510.13704</guid>
<content:encoded><![CDATA[
arXiv:2510.13704v1 Announce Type: new 
Abstract: Recent works have proposed accelerating the wall-clock training time of actor-critic methods via the use of large-scale environment parallelization; unfortunately, these can sometimes still require large number of environment interactions to achieve a desired level of performance. Noting that well-structured representations can improve the generalization and sample efficiency of deep reinforcement learning (RL) agents, we propose the use of simplicial embeddings: lightweight representation layers that constrain embeddings to simplicial structures. This geometric inductive bias results in sparse and discrete features that stabilize critic bootstrapping and strengthen policy gradients. When applied to FastTD3, FastSAC, and PPO, simplicial embeddings consistently improve sample efficiency and final performance across a variety of continuous- and discrete-control environments, without any loss in runtime speed.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training LLM Agents to Empower Humans</title>
<link>https://arxiv.org/abs/2510.13709</link>
<guid>https://arxiv.org/abs/2510.13709</guid>
<content:encoded><![CDATA[
arXiv:2510.13709v1 Announce Type: new 
Abstract: Assistive agents should not only take actions on behalf of a human, but also step out of the way and cede control when there are important decisions to be made. However, current methods for building assistive agents, whether via mimicking expert humans or via RL finetuning on an inferred reward, often encourage agents to complete tasks on their own rather than truly assisting the human attain their objectives. Additionally, these methods often require costly explicit human feedback to provide a training signal. We propose a new approach to tuning assistive language models based on maximizing the human's empowerment, their ability to effect desired changes in the environment. Our empowerment-maximizing method, Empower, only requires offline text data, providing a self-supervised method for fine-tuning language models to better assist humans. To study the efficacy of our approach, we conducted an 18-person user study comparing our empowerment assistant with a strong baseline. Participants preferred our assistant 78% of the time (p=0.015), with a 31% higher acceptance rate and 38% fewer suggestions. Additionally, we introduce a new environment for evaluating multi-turn code assistance using simulated humans. Using this environment, we show that agents trained with Empower increase the success rate of a simulated human programmer on challenging coding questions by an average of 192% over an SFT baseline. With this empowerment objective, we provide a framework for useful aligned AI agents at scale using only offline data without the need for any additional human feedback or verifiable rewards.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Refusal to Recovery: A Control-Theoretic Approach to Generative AI Guardrails</title>
<link>https://arxiv.org/abs/2510.13727</link>
<guid>https://arxiv.org/abs/2510.13727</guid>
<content:encoded><![CDATA[
arXiv:2510.13727v1 Announce Type: new 
Abstract: Generative AI systems are increasingly assisting and acting on behalf of end users in practical settings, from digital shopping assistants to next-generation autonomous cars. In this context, safety is no longer about blocking harmful content, but about preempting downstream hazards like financial or physical harm. Yet, most AI guardrails continue to rely on output classification based on labeled datasets and human-specified criteria,making them brittle to new hazardous situations. Even when unsafe conditions are flagged, this detection offers no path to recovery: typically, the AI system simply refuses to act--which is not always a safe choice. In this work, we argue that agentic AI safety is fundamentally a sequential decision problem: harmful outcomes arise from the AI system's continually evolving interactions and their downstream consequences on the world. We formalize this through the lens of safety-critical control theory, but within the AI model's latent representation of the world. This enables us to build predictive guardrails that (i) monitor an AI system's outputs (actions) in real time and (ii) proactively correct risky outputs to safe ones, all in a model-agnostic manner so the same guardrail can be wrapped around any AI model. We also offer a practical training recipe for computing such guardrails at scale via safety-critical reinforcement learning. Our experiments in simulated driving and e-commerce settings demonstrate that control-theoretic guardrails can reliably steer LLM agents clear of catastrophic outcomes (from collisions to bankruptcy) while preserving task performance, offering a principled dynamic alternative to today's flag-and-block guardrails.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAPS: A Clinically Grounded, Automated Benchmark for Evaluating AI Clinicians</title>
<link>https://arxiv.org/abs/2510.13734</link>
<guid>https://arxiv.org/abs/2510.13734</guid>
<content:encoded><![CDATA[
arXiv:2510.13734v1 Announce Type: new 
Abstract: Current benchmarks for AI clinician systems, often based on multiple-choice exams or manual rubrics, fail to capture the depth, robustness, and safety required for real-world clinical practice. To address this, we introduce the GAPS framework, a multidimensional paradigm for evaluating \textbf{G}rounding (cognitive depth), \textbf{A}dequacy (answer completeness), \textbf{P}erturbation (robustness), and \textbf{S}afety. Critically, we developed a fully automated, guideline-anchored pipeline to construct a GAPS-aligned benchmark end-to-end, overcoming the scalability and subjectivity limitations of prior work. Our pipeline assembles an evidence neighborhood, creates dual graph and tree representations, and automatically generates questions across G-levels. Rubrics are synthesized by a DeepResearch agent that mimics GRADE-consistent, PICO-driven evidence review in a ReAct loop. Scoring is performed by an ensemble of large language model (LLM) judges. Validation confirmed our automated questions are high-quality and align with clinician judgment. Evaluating state-of-the-art models on the benchmark revealed key failure modes: performance degrades sharply with increased reasoning depth (G-axis), models struggle with answer completeness (A-axis), and they are highly vulnerable to adversarial perturbations (P-axis) as well as certain safety issues (S-axis). This automated, clinically-grounded approach provides a reproducible and scalable method for rigorously evaluating AI clinician systems and guiding their development toward safer, more reliable clinical practice.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RECODE: Reasoning Through Code Generation for Visual Question Answering</title>
<link>https://arxiv.org/abs/2510.13756</link>
<guid>https://arxiv.org/abs/2510.13756</guid>
<content:encoded><![CDATA[
arXiv:2510.13756v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) struggle with precise reasoning for structured visuals like charts and diagrams, as pixel-based perception lacks a mechanism for verification. To address this, we propose to leverage derendering -- the process of reverse-engineering visuals into executable code -- as a new modality for verifiable visual reasoning. Specifically, we propose RECODE, an agentic framework that first generates multiple candidate programs to reproduce the input image. It then uses a critic to select the most faithful reconstruction and iteratively refines the code. This process not only transforms an ambiguous perceptual task into a verifiable, symbolic problem, but also enables precise calculations and logical inferences later on. On various visual reasoning benchmarks such as CharXiv, ChartQA, and Geometry3K, RECODE significantly outperforms methods that do not leverage code or only use code for drawing auxiliary lines or cropping. Our work demonstrates that grounding visual perception in executable code provides a new path toward more accurate and verifiable multimodal reasoning.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provably Invincible Adversarial Attacks on Reinforcement Learning Systems: A Rate-Distortion Information-Theoretic Approach</title>
<link>https://arxiv.org/abs/2510.13792</link>
<guid>https://arxiv.org/abs/2510.13792</guid>
<content:encoded><![CDATA[
arXiv:2510.13792v1 Announce Type: new 
Abstract: Reinforcement learning (RL) for the Markov Decision Process (MDP) has emerged in many security-related applications, such as autonomous driving, financial decisions, and drone/robot algorithms. In order to improve the robustness/defense of RL systems against adversaries, studying various adversarial attacks on RL systems is very important. Most previous work considered deterministic adversarial attack strategies in MDP, which the recipient (victim) agent can defeat by reversing the deterministic attacks. In this paper, we propose a provably ``invincible'' or ``uncounterable'' type of adversarial attack on RL. The attackers apply a rate-distortion information-theoretic approach to randomly change agents' observations of the transition kernel (or other properties) so that the agent gains zero or very limited information about the ground-truth kernel (or other properties) during the training. We derive an information-theoretic lower bound on the recipient agent's reward regret and show the impact of rate-distortion attacks on state-of-the-art model-based and model-free algorithms. We also extend this notion of an information-theoretic approach to other types of adversarial attack, such as state observation attacks.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MimicKit: A Reinforcement Learning Framework for Motion Imitation and Control</title>
<link>https://arxiv.org/abs/2510.13794</link>
<guid>https://arxiv.org/abs/2510.13794</guid>
<content:encoded><![CDATA[
arXiv:2510.13794v1 Announce Type: new 
Abstract: MimicKit is an open-source framework for training motion controllers using motion imitation and reinforcement learning. The codebase provides implementations of commonly-used motion-imitation techniques and RL algorithms. This framework is intended to support research and applications in computer graphics and robotics by providing a unified training framework, along with standardized environment, agent, and data structures. The codebase is designed to be modular and easily configurable, enabling convenient modification and extension to new characters and tasks. The open-source codebase is available at: https://github.com/xbpeng/MimicKit.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InferA: A Smart Assistant for Cosmological Ensemble Data</title>
<link>https://arxiv.org/abs/2510.12920</link>
<guid>https://arxiv.org/abs/2510.12920</guid>
<content:encoded><![CDATA[
arXiv:2510.12920v1 Announce Type: cross 
Abstract: Analyzing large-scale scientific datasets presents substantial challenges due to their sheer volume, structural complexity, and the need for specialized domain knowledge. Automation tools, such as PandasAI, typically require full data ingestion and lack context of the full data structure, making them impractical as intelligent data analysis assistants for datasets at the terabyte scale. To overcome these limitations, we propose InferA, a multi-agent system that leverages large language models to enable scalable and efficient scientific data analysis. At the core of the architecture is a supervisor agent that orchestrates a team of specialized agents responsible for distinct phases of the data retrieval and analysis. The system engages interactively with users to elicit their analytical intent and confirm query objectives, ensuring alignment between user goals and system actions. To demonstrate the framework's usability, we evaluate the system using ensemble runs from the HACC cosmology simulation which comprises several terabytes.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Planning with Large Language Models: A Modular Agentic Architecture</title>
<link>https://arxiv.org/abs/2310.00194</link>
<guid>https://arxiv.org/abs/2310.00194</guid>
<content:encoded><![CDATA[
arXiv:2310.00194v5 Announce Type: replace 
Abstract: Large language models (LLMs) demonstrate impressive performance on a wide variety of tasks, but they often struggle with tasks that require multi-step reasoning or goal-directed planning. Both cognitive neuroscience and reinforcement learning (RL) have proposed a number of interacting functional components that together implement search and evaluation in multi-step decision making. These components include conflict monitoring, state prediction, state evaluation, task decomposition, and orchestration. To improve planning with LLMs, we propose an agentic architecture, the Modular Agentic Planner (MAP), in which planning is accomplished via the recurrent interaction of the specialized modules mentioned above, each implemented using an LLM. MAP improves planning through the interaction of specialized modules that break down a larger problem into multiple brief automated calls to the LLM. We evaluate MAP on three challenging planning tasks -- graph traversal, Tower of Hanoi, and the PlanBench benchmark -- as well as an NLP task requiring multi-step reasoning (strategyQA). We find that MAP yields significant improvements over both standard LLM methods (zero-shot prompting, in-context learning) and competitive baselines (chain-of-thought, multi-agent debate, and tree-of-thought), can be effectively combined with smaller and more cost-efficient LLMs (Llama3-70B), and displays superior transfer across tasks. These results suggest the benefit of a modular and multi-agent approach to planning with LLMs.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLM Agents Have Regret? A Case Study in Online Learning and Games</title>
<link>https://arxiv.org/abs/2403.16843</link>
<guid>https://arxiv.org/abs/2403.16843</guid>
<content:encoded><![CDATA[
arXiv:2403.16843v5 Announce Type: replace 
Abstract: Large language models (LLMs) have been increasingly employed for (interactive) decision-making, via the development of LLM-based autonomous agents. Despite their emerging successes, the performance of LLM agents in decision-making has not been fully investigated through quantitative metrics, especially in the multi-agent setting when they interact with each other, a typical scenario in real-world LLM-agent applications. To better understand the limits of LLM agents in these interactive environments, we propose to study their interactions in benchmark decision-making settings in online learning and game theory, through the performance metric of \emph{regret}. We first empirically study the {no-regret} behaviors of LLMs in canonical (non-stationary) online learning problems, as well as the emergence of equilibria when LLM agents interact through playing repeated games. We then provide some theoretical insights into the no-regret behaviors of LLM agents, under certain assumptions on the supervised pre-training and the rationality model of human decision-makers who generate the data. Notably, we also identify (simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To promote the no-regret behaviors, we propose a novel \emph{unsupervised} training loss of \emph{regret-loss}, which, in contrast to the supervised pre-training loss, does not require the labels of (optimal) actions. We then establish the statistical guarantee of generalization bound for regret-loss minimization, followed by the optimization guarantee that minimizing such a loss may automatically lead to known no-regret learning algorithms. Our further experiments demonstrate the effectiveness of our regret-loss, especially in addressing the above ``regrettable'' cases.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foragax: An Agent-Based Modelling Framework Based on JAX</title>
<link>https://arxiv.org/abs/2409.06345</link>
<guid>https://arxiv.org/abs/2409.06345</guid>
<content:encoded><![CDATA[
arXiv:2409.06345v4 Announce Type: replace 
Abstract: Foraging for resources is a ubiquitous activity conducted by living organisms in a shared environment to maintain their homeostasis. Modelling multi-agent foraging in-silico allows us to study both individual and collective emergent behaviour in a tractable manner. Agent-based modelling has proven to be effective in simulating such tasks, though scaling the simulations to accommodate large numbers of agents with complex dynamics remains challenging. In this work, we present Foragax, a general-purpose, scalable, hardware-accelerated, multi-agent foraging toolkit. Leveraging the JAX library, our toolkit can simulate thousands of agents foraging in a common environment, in an end-to-end vectorized and differentiable manner. The toolkit provides agent-based modelling tools to model various foraging tasks, including options to design custom spatial and temporal agent dynamics, control policies, sensor models, and boundary conditions. Further, the number of agents during such simulations can be increased or decreased based on custom rules. While applied to foraging, the toolkit can also be used to model and simulate a wide range of other multi-agent scenarios.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hi-Drive: Hierarchical POMDP Planning for Safe Autonomous Driving in Diverse Urban Environments</title>
<link>https://arxiv.org/abs/2409.18411</link>
<guid>https://arxiv.org/abs/2409.18411</guid>
<content:encoded><![CDATA[
arXiv:2409.18411v2 Announce Type: replace 
Abstract: Uncertainties in dynamic road environments pose significant challenges for behavior and trajectory planning in autonomous driving. This paper introduces Hi-Drive, a hierarchical planning algorithm addressing uncertainties at both behavior and trajectory levels using a hierarchical Partially Observable Markov Decision Process (POMDP) formulation. Hi-Drive employs driver models to represent uncertain behavioral intentions of other vehicles and uses their parameters to infer hidden driving styles. By treating driver models as high-level decision-making actions, our approach effectively manages the exponential complexity inherent in POMDPs. To further enhance safety and robustness, Hi-Drive integrates a trajectory optimization based on importance sampling, refining trajectories using a comprehensive analysis of critical agents. Evaluations on real-world urban driving datasets demonstrate that Hi-Drive significantly outperforms state-of-the-art planning-based and learning-based methods across diverse urban driving situations in real-world benchmarks.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EFX Allocations and Orientations on Bipartite Multi-graphs: A Complete Picture</title>
<link>https://arxiv.org/abs/2410.17002</link>
<guid>https://arxiv.org/abs/2410.17002</guid>
<content:encoded><![CDATA[
arXiv:2410.17002v3 Announce Type: replace 
Abstract: We consider the fundamental problem of fairly allocating a set of indivisible items among agents having valuations that are represented by a multi-graph -- here, agents appear as vertices and items as edges between them and each vertex (agent) only values the set of its incident edges (items). The goal is to find a fair, i.e., envy-free up to any item (EFX) allocation. This model has recently been introduced by Christodoulou et al. (EC-23) where they show that EFX allocations always exist on simple graphs for monotone valuations, i.e., where any two agents can share at most one edge (item). A natural question arises as to what happens when we go beyond simple graphs and study various classes of multi-graphs?
  We answer the above question affirmatively for the valuation class of bipartite multi-graphs and multi-cycles. The main contribution of this work is to establish the existence of EFX allocations on bipartite multi-graphs for monotone valuations and on multi-cycles for MMS-feasible valuations. We also present pseudo-polynomial time algorithms to compute EFX allocations for the above settings. Furthermore, we show that for bipartite multi-graphs with cancelable valuations, EFX allocations can be computed in polynomial time. We thus widen the spectrum where EFX allocations are guaranteed to exist.
  Next, we study EFX orientations (allocations where every item is assigned to one of its two endpoint agents) and provide a complete characterization of their existence on bipartite multi-graphs in terms of two key parameters: (i) the number of edges shared between any two agents and (ii) the diameter of the graph. Finally, we prove that it is NP-complete to determine whether a given fair division instance on a bipartite multi-graph admits an EFX orientation, even with a constant number of agents.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcing Competitive Multi-Agents for Playing 'So Long Sucker'</title>
<link>https://arxiv.org/abs/2411.11057</link>
<guid>https://arxiv.org/abs/2411.11057</guid>
<content:encoded><![CDATA[
arXiv:2411.11057v2 Announce Type: replace 
Abstract: This paper investigates the strategy game So Long Sucker (SLS) as a novel benchmark for multi-agent reinforcement learning (MARL). Unlike traditional board or video game testbeds, SLS is distinguished by its coalition formation, strategic deception, and dynamic elimination rules, making it a uniquely challenging environment for autonomous agents. We introduce the first publicly available computational framework for SLS, complete with a graphical user interface and benchmarking support for reinforcement learning algorithms. Using classical deep reinforcement learning methods (e.g., DQN, DDQN, and Dueling DQN), we train self-playing agents to learn the rules and basic strategies of SLS. Experimental results demonstrate that, although these agents achieve roughly half of the maximum attainable reward and consistently outperform random baselines, they require long training horizons (~2000 games) and still commit occasional illegal moves, highlighting both the promise and limitations of classical reinforcement learning. Our findings establish SLS as a negotiation-aware benchmark for MARL, opening avenues for future research that integrates game-theoretic reasoning, coalition-aware strategies, and advanced reinforcement learning architectures to better capture the social and adversarial dynamics of complex multi-agent games.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery</title>
<link>https://arxiv.org/abs/2501.01540</link>
<guid>https://arxiv.org/abs/2501.01540</guid>
<content:encoded><![CDATA[
arXiv:2501.01540v2 Announce Type: replace 
Abstract: Understanding the world and explaining it with scientific theories is a central aspiration of artificial intelligence research. Proposing theories, designing experiments to test them, and then revising them based on data are fundamental to scientific discovery. Despite the significant promise of LLM-based scientific agents, no benchmarks systematically test LLM's ability to propose scientific models, collect experimental data, and revise them in light of new data. We introduce BoxingGym, a benchmark with 10 environments for systematically evaluating both experimental design (e.g. collecting data to test a scientific theory) and model discovery (e.g. proposing and revising scientific theories). To enable tractable and quantitative evaluation, we implement each environment as a generative probabilistic model with which a scientific agent can run interactive experiments. These probabilistic models are drawn from various real-world scientific domains ranging from psychology to ecology. To quantitatively evaluate a scientific agent's ability to collect informative experimental data, we compute the expected information gain (EIG), an information-theoretic quantity which measures how much an experiment reduces uncertainty about the parameters of a generative model. A good scientific theory is a concise and predictive explanation. Therefore, to quantitatively evaluate model discovery, we ask a scientific agent to explain their model and then assess whether this explanation enables another scientific agent to make reliable predictions about this environment. In addition to this explanation-based evaluation, we compute standard model evaluation metrics such as prediction errors. We find that current LLMs, such as GPT-4o, struggle with both experimental design and model discovery. We find that augmenting the LLM-based agent with an explicit statistical model does not reliably improve these results.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MotionAgent: Fine-grained Controllable Video Generation via Motion Field Agent</title>
<link>https://arxiv.org/abs/2502.03207</link>
<guid>https://arxiv.org/abs/2502.03207</guid>
<content:encoded><![CDATA[
arXiv:2502.03207v2 Announce Type: replace 
Abstract: We propose MotionAgent, enabling fine-grained motion control for text-guided image-to-video generation. The key technique is the motion field agent that converts motion information in text prompts into explicit motion fields, providing flexible and precise motion guidance. Specifically, the agent extracts the object movement and camera motion described in the text and converts them into object trajectories and camera extrinsics, respectively. An analytical optical flow composition module integrates these motion representations in 3D space and projects them into a unified optical flow. An optical flow adapter takes the flow to control the base image-to-video diffusion model for generating fine-grained controlled videos. The significant improvement in the Video-Text Camera Motion metrics on VBench indicates that our method achieves precise control over camera motion. We construct a subset of VBench to evaluate the alignment of motion information in the text and the generated video, outperforming other advanced models on motion generation accuracy.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Realtor: Towards Grounded Persuasive Language Generation for Automated Copywriting</title>
<link>https://arxiv.org/abs/2502.16810</link>
<guid>https://arxiv.org/abs/2502.16810</guid>
<content:encoded><![CDATA[
arXiv:2502.16810v4 Announce Type: replace 
Abstract: This paper develops an agentic framework that employs large language models (LLMs) for grounded persuasive language generation in automated copywriting, with real estate marketing as a focal application. Our method is designed to align the generated content with user preferences while highlighting useful factual attributes. This agent consists of three key modules: (1) Grounding Module, mimicking expert human behavior to predict marketable features; (2) Personalization Module, aligning content with user preferences; (3) Marketing Module, ensuring factual accuracy and the inclusion of localized features. We conduct systematic human-subject experiments in the domain of real estate marketing, with a focus group of potential house buyers. The results demonstrate that marketing descriptions generated by our approach are preferred over those written by human experts by a clear margin while maintaining the same level of factual accuracy. Our findings suggest a promising agentic approach to automate large-scale targeted copywriting while ensuring factuality of content generation.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metric Distortion in Peer Selection</title>
<link>https://arxiv.org/abs/2502.21084</link>
<guid>https://arxiv.org/abs/2502.21084</guid>
<content:encoded><![CDATA[
arXiv:2502.21084v2 Announce Type: replace 
Abstract: In the metric distortion problem, a set of voters and candidates lie in a common metric space, and a committee of $k$ candidates must be elected. The objective is to minimize a social cost, defined as a function of the distances between voters and their chosen representatives, while the voting rule only has access to ordinal preferences. The distortion of a rule is the worst-case ratio between the social cost of its outcome and that of the optimal committee, taken over all consistent preferences and metrics.
  We initiate the study of metric distortion in peer selection, where voters and candidates coincide. We consider four objectives, obtained by combining two aggregation rules with two types of social cost. Under additive aggregation, an individual's cost is the sum of their distances to all committee members; under $q$-cost, it is their distance to the $q$th closest member. The overall social cost is either utilitarian, given by the sum of all individual costs, or egalitarian, given by the maximum individual cost. Surprisingly, we find that even on the line metric, peer selection retains much of the hardness of the general case: Lower bounds remain strictly larger than one for all objectives, and cases where bounded distortion is impossible in general remain so here as well. On a positive note, cases with bounded distortion in the general setting achieve better constants in peer selection. For utilitarian cost, selecting the $k$ middle agents achieves a distortion between $1$ and $2$ under additive aggregation. Under $q$-cost, we show positive results for $q=k=2$, but impossibility results largely carry over. For egalitarian cost, selecting the extremes yields an optimal distortion of $2$ under additive aggregation and for $q$-cost with $q>k/3$. Thus, while peer selection on the line metric allows better constants, fundamental hardness barriers persist.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Personalized Data-Driven Generative Model of Human Repetitive Motion</title>
<link>https://arxiv.org/abs/2503.15225</link>
<guid>https://arxiv.org/abs/2503.15225</guid>
<content:encoded><![CDATA[
arXiv:2503.15225v2 Announce Type: replace 
Abstract: The deployment of autonomous virtual avatars (in extended reality) and robots in human group activities -- such as rehabilitation therapy, sports, and manufacturing -- is expected to increase as these technologies become more pervasive. Designing cognitive architectures and control strategies to drive these agents requires realistic models of human motion. Furthermore, recent research has shown that each person exhibits a unique velocity signature, highlighting how individual motor behaviors are both rich in variability and internally consistent. However, existing models only provide simplified descriptions of human motor behavior, hindering the development of effective cognitive architectures. In this work, we first show that motion amplitude provides a valid and complementary characterization of individual motor signatures. Then, we propose a fully data-driven approach, based on long short-term memory neural networks, to generate original motion that captures the unique features of specific individuals. We validate the architecture using real human data from participants performing spontaneous oscillatory motion. Extensive analyses show that state-of-the-art Kuramoto-like models fail to replicate individual motor signatures, whereas our model accurately reproduces the velocity distribution and amplitude envelopes of the individual it was trained on, while remaining distinct from others.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking LLMs' Swarm intelligence</title>
<link>https://arxiv.org/abs/2505.04364</link>
<guid>https://arxiv.org/abs/2505.04364</guid>
<content:encoded><![CDATA[
arXiv:2505.04364v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) show potential for complex reasoning, yet their capacity for emergent coordination in Multi-Agent Systems (MAS) when operating under strict swarm-like constraints-limited local perception and communication-remains largely unexplored. Existing benchmarks often do not fully capture the unique challenges of decentralized coordination when agents operate with incomplete spatio-temporal information. To bridge this gap, we introduce SwarmBench, a novel benchmark designed to systematically evaluate the swarm intelligence capabilities of LLMs acting as decentralized agents. SwarmBench features five foundational MAS coordination tasks (Pursuit, Synchronization, Foraging, Flocking, Transport) within a configurable 2D grid environment, forcing agents to rely solely on local sensory input ($k\times k$ view) and local communication. We propose metrics for coordination effectiveness and analyze emergent group dynamics. Zero-shot evaluations of leading LLMs (e.g., deepseek-v3, o4-mini) reveal significant task-dependent performance variations. While some rudimentary coordination is observed, our results indicate that current LLMs significantly struggle with robust long-range planning and adaptive strategy formation under the uncertainty inherent in these decentralized scenarios. Assessing LLMs under such swarm-like constraints is crucial for understanding their utility in future decentralized intelligent systems. We release SwarmBench as an open, extensible toolkit-built on a customizable physical system-providing environments, prompts, evaluation scripts, and comprehensive datasets. This aims to foster reproducible research into LLM-based MAS coordination and the theoretical underpinnings of emergent collective behavior under severe informational decentralization. Our code repository is available at https://github.com/x66ccff/swarmbench.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constant-Memory Strategies in Stochastic Games: Best Responses and Equilibria</title>
<link>https://arxiv.org/abs/2505.07008</link>
<guid>https://arxiv.org/abs/2505.07008</guid>
<content:encoded><![CDATA[
arXiv:2505.07008v2 Announce Type: replace 
Abstract: Stochastic games have become a prevalent framework for studying long-term multi-agent interactions, especially in the context of multi-agent reinforcement learning. In this work, we comprehensively investigate the concept of constant-memory strategies in stochastic games. We first establish some results on best responses and Nash equilibria for behavioral constant-memory strategies, followed by a discussion on the computational hardness of best responding to mixed constant-memory strategies. Those theoretic insights are later verified on several sequential decision-making testbeds, including the $\textit{Iterated Prisoner's Dilemma}$, the $\textit{Iterated Traveler's Dilemma}$, and the $\textit{Pursuit}$ domain. This work aims to enhance the understanding of theoretical issues in single-agent planning under multi-agent systems, and uncover the connection between decision models in single-agent and multi-agent contexts. The code is available at $\texttt{https://github.com/Fernadoo/Const-Mem.}$
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Of Mice and Machines: A Comparison of Learning Between Real World Mice and RL Agents</title>
<link>https://arxiv.org/abs/2505.12204</link>
<guid>https://arxiv.org/abs/2505.12204</guid>
<content:encoded><![CDATA[
arXiv:2505.12204v3 Announce Type: replace 
Abstract: Recent advances in reinforcement learning (RL) have demonstrated impressive capabilities in complex decision-making tasks. This progress raises a natural question: how do these artificial systems compare to biological agents, which have been shaped by millions of years of evolution? To help answer this question, we undertake a comparative study of biological mice and RL agents in a predator-avoidance maze environment. Through this analysis, we identify a striking disparity: RL agents consistently demonstrate a lack of self-preservation instinct, readily risking ``death'' for marginal efficiency gains. These risk-taking strategies are in contrast to biological agents, which exhibit sophisticated risk-assessment and avoidance behaviors. Towards bridging this gap between the biological and artificial, we propose two novel mechanisms that encourage more naturalistic risk-avoidance behaviors in RL agents. Our approach leads to the emergence of naturalistic behaviors, including strategic environment assessment, cautious path planning, and predator avoidance patterns that closely mirror those observed in biological systems.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RealEngine: Simulating Autonomous Driving in Realistic Context</title>
<link>https://arxiv.org/abs/2505.16902</link>
<guid>https://arxiv.org/abs/2505.16902</guid>
<content:encoded><![CDATA[
arXiv:2505.16902v2 Announce Type: replace 
Abstract: Driving simulation plays a crucial role in developing reliable driving agents by providing controlled, evaluative environments. To enable meaningful assessments, a high-quality driving simulator must satisfy several key requirements: multi-modal sensing capabilities (e.g., camera and LiDAR) with realistic scene rendering to minimize observational discrepancies; closed-loop evaluation to support free-form trajectory behaviors; highly diverse traffic scenarios for thorough evaluation; multi-agent cooperation to capture interaction dynamics; and high computational efficiency to ensure affordability and scalability. However, existing simulators and benchmarks fail to comprehensively meet these fundamental criteria. To bridge this gap, this paper introduces RealEngine, a novel driving simulation framework that holistically integrates 3D scene reconstruction and novel view synthesis techniques to achieve realistic and flexible closed-loop simulation in the driving context. By leveraging real-world multi-modal sensor data, RealEngine reconstructs background scenes and foreground traffic participants separately, allowing for highly diverse and realistic traffic scenarios through flexible scene composition. This synergistic fusion of scene reconstruction and view synthesis enables photorealistic rendering across multiple sensor modalities, ensuring both perceptual fidelity and geometric accuracy. Building upon this environment, RealEngine supports three essential driving simulation categories: non-reactive simulation, safety testing, and multi-agent interaction, collectively forming a reliable and comprehensive benchmark for evaluating the real-world performance of driving agents.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling</title>
<link>https://arxiv.org/abs/2505.19234</link>
<guid>https://arxiv.org/abs/2505.19234</guid>
<content:encoded><![CDATA[
arXiv:2505.19234v2 Announce Type: replace 
Abstract: The emergence of large language models (LLMs) enables the development of intelligent agents capable of engaging in complex and multi-turn dialogues. However, multi-agent collaboration faces critical safety challenges, such as hallucination amplification and error injection and propagation. This paper presents GUARDIAN, a unified method for detecting and mitigating multiple safety concerns in GUARDing Intelligent Agent collaboratioNs. By modeling the multi-agent collaboration process as a discrete-time temporal attributed graph, GUARDIAN explicitly captures the propagation dynamics of hallucinations and errors. The unsupervised encoder-decoder architecture incorporating an incremental training paradigm learns to reconstruct node attributes and graph structures from latent embeddings, enabling the identification of anomalous nodes and edges with unparalleled precision. Moreover, we introduce a graph abstraction mechanism based on the Information Bottleneck Theory, which compresses temporal interaction graphs while preserving essential patterns. Extensive experiments demonstrate GUARDIAN's effectiveness in safeguarding LLM multi-agent collaborations against diverse safety vulnerabilities, achieving state-of-the-art accuracy with efficient resource utilization. The code is available at https://github.com/JialongZhou666/GUARDIAN
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments</title>
<link>https://arxiv.org/abs/2505.21936</link>
<guid>https://arxiv.org/abs/2505.21936</guid>
<content:encoded><![CDATA[
arXiv:2505.21936v3 Announce Type: replace 
Abstract: Computer-use agents (CUAs) promise to automate complex tasks across operating systems (OS) and the web, but remain vulnerable to indirect prompt injection. Current evaluations of this threat either lack support realistic but controlled environments or ignore hybrid web-OS attack scenarios involving both interfaces. To address this, we propose RedTeamCUA, an adversarial testing framework featuring a novel hybrid sandbox that integrates a VM-based OS environment with Docker-based web platforms. Our sandbox supports key features tailored for red teaming, such as flexible adversarial scenario configuration, and a setting that decouples adversarial evaluation from navigational limitations of CUAs by initializing tests directly at the point of an adversarial injection. Using RedTeamCUA, we develop RTC-Bench, a comprehensive benchmark with 864 examples that investigate realistic, hybrid web-OS attack scenarios and fundamental security vulnerabilities. Benchmarking current frontier CUAs identifies significant vulnerabilities: Claude 3.7 Sonnet | CUA demonstrates an ASR of 42.9%, while Operator, the most secure CUA evaluated, still exhibits an ASR of 7.6%. Notably, CUAs often attempt to execute adversarial tasks with an Attempt Rate as high as 92.5%, although failing to complete them due to capability limitations. Nevertheless, we observe concerning high ASRs in realistic end-to-end settings, with the strongest-to-date Claude 4.5 Sonnet | CUA exhibiting the highest ASR of 60%, indicating that CUA threats can already result in tangible risks to users and computer systems. Overall, RedTeamCUA provides an essential framework for advancing realistic, controlled, and systematic analysis of CUA vulnerabilities, highlighting the urgent need for robust defenses to indirect prompt injection prior to real-world deployment.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hierarchical Bin Packing Framework with Dual Manipulators via Heuristic Search and Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.01628</link>
<guid>https://arxiv.org/abs/2506.01628</guid>
<content:encoded><![CDATA[
arXiv:2506.01628v2 Announce Type: replace 
Abstract: We address the bin packing problem (BPP), which aims to maximize bin utilization when packing a variety of items. The offline problem, where the complete information about the item set and their sizes is known in advance, is proven to be NP-hard. The semi-online and online variants are even more challenging, as full information about incoming items is unavailable. While existing methods have tackled both 2D and 3D BPPs, the 2D BPP remains underexplored in terms of fully maximizing utilization. We propose a hierarchical approach for solving the 2D online and semi-online BPP by combining deep reinforcement learning (RL) with heuristic search. The heuristic search selects which item to pack or unpack, determines the packing order, and chooses the orientation of each item, while the RL agent decides the precise position within the bin. Our method is capable of handling diverse scenarios, including repacking, varying levels of item information, differing numbers of accessible items, and coordination of dual manipulators. Experimental results demonstrate that our approach achieves near-optimal utilization across various practical scenarios, largely due to its repacking capability. In addition, the algorithm is evaluated in a physics-based simulation environment, where execution time is measured to assess its real-world performance.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralizing Multi-Agent Reinforcement Learning with Temporal Causal Information</title>
<link>https://arxiv.org/abs/2506.07829</link>
<guid>https://arxiv.org/abs/2506.07829</guid>
<content:encoded><![CDATA[
arXiv:2506.07829v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) algorithms can find an optimal policy for a single agent to accomplish a particular task. However, many real-world problems require multiple agents to collaborate in order to achieve a common goal. For example, a robot executing a task in a warehouse may require the assistance of a drone to retrieve items from high shelves. In Decentralized Multi-Agent RL (DMARL), agents learn independently and then combine their policies at execution time, but often must satisfy constraints on compatibility of local policies to ensure that they can achieve the global task when combined. In this paper, we study how providing high-level symbolic knowledge to agents can help address unique challenges of this setting, such as privacy constraints, communication limitations, and performance concerns. In particular, we extend the formal tools used to check the compatibility of local policies with the team task, making decentralized training with theoretical guarantees usable in more scenarios. Furthermore, we empirically demonstrate that symbolic knowledge about the temporal evolution of events in the environment can significantly expedite the learning process in DMARL.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair and Efficient Allocation of Indivisible Mixed Manna</title>
<link>https://arxiv.org/abs/2507.03946</link>
<guid>https://arxiv.org/abs/2507.03946</guid>
<content:encoded><![CDATA[
arXiv:2507.03946v2 Announce Type: replace 
Abstract: We study fair division of indivisible mixed manna (items whose values may be positive, negative, or zero) among agents with additive valuations. Here, we establish that fairness -- in terms of a relaxation of envy-freeness -- and Pareto efficiency can always be achieved together. Specifically, our fairness guarantees are in terms of envy-freeness up to $k$ reallocations (EFR-$k$): An allocation $A$ of the indivisible items is said to be EFR-$k$ if there exists a subset $R$ of at most $k$ items such that, for each agent $i$, we can reassign items from within $R$ (in $A$) and obtain an allocation, $A^i$, which is envy-free for $i$. We establish that, when allocating mixed manna among $n$ agents with additive valuations, an EFR-$(n-1)$ and Pareto optimal (PO) allocation $A$ always exists. Further, the individual envy-free allocations $A^i$, induced by reassignments, are also PO. In addition, we prove that such fair and efficient allocations are efficiently computable when the number of agents, $n$, is fixed.
  We also obtain positive results focusing on EFR by itself (and without the PO desideratum). Specifically, we show that an EFR-$(n-1)$ allocation of mixed manna can be computed in polynomial time. In addition, we prove that when all the items are goods, an EFR-${\lfloor n/2 \rfloor}$ allocation exists and can be computed efficiently. Here, the $(n-1)$ bound is tight for chores and $\lfloor n/2 \rfloor$ is tight for goods.
  Our results advance the understanding of fair and efficient allocation of indivisible mixed manna and rely on a novel application of the Knaster-Kuratowski-Mazurkiewicz (KKM) Theorem in discrete fair division. We utilize weighted welfare maximization, with perturbed valuations, to achieve Pareto efficiency, and overall, our techniques are notably different from existing market-based approaches.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatio-Temporal LLM: Reasoning about Environments and Actions</title>
<link>https://arxiv.org/abs/2507.05258</link>
<guid>https://arxiv.org/abs/2507.05258</guid>
<content:encoded><![CDATA[
arXiv:2507.05258v2 Announce Type: replace 
Abstract: Despite significant recent progress of Multimodal Large Language Models (MLLMs), current MLLMs are challenged by "spatio-temporal" prompts, i.e., prompts that refer to 1) the entirety of an environment encoded in a point cloud that the MLLM should consider; and simultaneously also refer to 2) actions that happened in part of the environment and are encoded in a short ego-centric video clip. However, such a holistic spatio-temporal understanding is important for agents operating in the real world. To address this challenge, we first develop a framework to collect a large-scale dataset. Using the collected "Reasoning about Environments and Actions" (REA) dataset, we show that recent MLLMs indeed struggle to correctly answer "spatio-temporal" prompts. Building on this dataset, we study two spatio-temporal LLM (STLLM) baselines: 1) STLLM-3D, which directly fuses point cloud, video, and text representations as inputs to the LLM; and 2) STLLM-Aligner, which aligns spatial context with video and text before LLM decoding. Both baselines aim to enhance spatial understanding of environments and temporal grounding of egocentric observations. On REA, the STLLM baselines outperform existing models, demonstrating the effectiveness of our designs. Code and data are available at https://zoezheng126.github.io/STLLM-website/.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DynaSearcher: Dynamic Knowledge Graph Augmented Search Agent via Multi-Reward Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.17365</link>
<guid>https://arxiv.org/abs/2507.17365</guid>
<content:encoded><![CDATA[
arXiv:2507.17365v2 Announce Type: replace 
Abstract: Multi-step agentic retrieval systems based on large language models (LLMs) have demonstrated remarkable performance in complex information search tasks. However, these systems still face significant challenges in practical applications, particularly in generating factually inconsistent intermediate queries and inefficient search trajectories, which can lead to reasoning deviations or redundant computations. To address these issues, we propose DynaSearcher, an innovative search agent enhanced by dynamic knowledge graphs and multi-reward reinforcement learning (RL). Specifically, our system leverages knowledge graphs as external structured knowledge to guide the search process by explicitly modeling entity relationships, thereby ensuring factual consistency in intermediate queries and mitigating biases from irrelevant information. Furthermore, we employ a multi-reward RL framework for fine-grained control over training objectives such as retrieval accuracy, efficiency, and response quality. This framework promotes the generation of high-quality intermediate queries and comprehensive final answers, while discouraging unnecessary exploration and minimizing information omissions or redundancy. Experimental results demonstrate that our approach achieves state-of-the-art answer accuracy on six multi-hop question answering datasets, matching frontier LLMs while using only small-scale models and limited computational resources. Furthermore, our approach demonstrates strong generalization and robustness across diverse retrieval environments and larger-scale models, highlighting its broad applicability.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Few-Shot Alzheimer's Disease Diagnosis on Biomarker Data with Tabular LLMs</title>
<link>https://arxiv.org/abs/2507.23227</link>
<guid>https://arxiv.org/abs/2507.23227</guid>
<content:encoded><![CDATA[
arXiv:2507.23227v2 Announce Type: replace 
Abstract: Early and accurate diagnosis of Alzheimer's disease (AD), a complex neurodegenerative disorder, requires analysis of heterogeneous biomarkers (e.g., neuroimaging, genetic risk factors, cognitive tests, and cerebrospinal fluid proteins) typically represented in a tabular format. With flexible few-shot reasoning, multimodal integration, and natural-language-based interpretability, large language models (LLMs) offer unprecedented opportunities for prediction with structured biomedical data. We propose a novel framework called TAP-GPT, Tabular Alzheimer's Prediction GPT, that adapts TableGPT2, a multimodal tabular-specialized LLM originally developed for business intelligence tasks, for AD diagnosis using structured biomarker data with small sample sizes. Our approach constructs few-shot tabular prompts using in-context learning examples from structured biomedical data and finetunes TableGPT2 using the parameter-efficient qLoRA adaption for a clinical binary classification task of AD or cognitively normal (CN). The TAP-GPT framework harnesses the powerful tabular understanding ability of TableGPT2 and the encoded prior knowledge of LLMs to outperform more advanced general-purpose LLMs and a tabular foundation model (TFM) developed for prediction tasks. To our knowledge, this is the first application of LLMs to the prediction task using tabular biomarker data, paving the way for future LLM-driven multi-agent frameworks in biomedical informatics.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolution of AI Agent Registry Solutions: Centralized, Enterprise, and Distributed Approaches</title>
<link>https://arxiv.org/abs/2508.03095</link>
<guid>https://arxiv.org/abs/2508.03095</guid>
<content:encoded><![CDATA[
arXiv:2508.03095v2 Announce Type: replace 
Abstract: Autonomous AI agents now operate across cloud, enterprise, and decentralized domains, creating demand for registry infrastructures that enable trustworthy discovery, capability negotiation, and identity assurance. We analyze five prominent approaches: (1) MCP Registry (centralized publication of mcp.json descriptors), (2) A2A Agent Cards (decentralized self-describing JSON capability manifests), (3) AGNTCY Agent Directory Service (IPFS Kademlia DHT content routing extended for semantic taxonomy-based content discovery, OCI artifact storage, and Sigstore-backed integrity), (4) Microsoft Entra Agent ID (enterprise SaaS directory with policy and zero-trust integration), and (5) NANDA Index AgentFacts (cryptographically verifiable, privacy-preserving fact model with credentialed assertions). Using four evaluation dimensions: security, authentication, scalability, and maintainability, we surface architectural trade-offs between centralized control, enterprise governance, and distributed resilience. We conclude with design recommendations for an emerging Internet of AI Agents requiring verifiable identity, adaptive discovery flows, and interoperable capability semantics.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TASER: Table Agents for Schema-guided Extraction and Recommendation</title>
<link>https://arxiv.org/abs/2508.13404</link>
<guid>https://arxiv.org/abs/2508.13404</guid>
<content:encoded><![CDATA[
arXiv:2508.13404v3 Announce Type: replace 
Abstract: Real-world financial documents report essential information about an entity's financial holdings that can span millions of different financial instrument types. Yet, these details are often buried in messy, multi-page, fragmented tables - for example, 99.4% of the tables in our dataset have no bounding boxes with the maximum number of rows amounting to 426 per table across 44 pages. To tackle these unique challenges from real-world tables, we present a continuously learning, agentic table extraction system, TASER (Table Agents for Schema-guided Extraction and Recommendation) that extracts highly unstructured, multi-page, heterogeneous tables into normalized, schema-conforming outputs. Our table agents execute on table detection, classification, extraction, and recommendations by leveraging an initial schema. Then, our Recommender Agent reviews the outputs, recommends schema revisions, and decides on the final recommendations, enabling TASER to outperform existing table detection models such as Table Transformer by 10.1%. Within this continuous learning process, we highlight that larger batch sizes result in a 104.3% increase in schema recommendations that are actionable and utilized, resulting in a 9.8% increase in extracted holdings - highlighting the importance of a continuous learning process. To train TASER, we have manually labeled 22,584 pages (28,150,449 tokens), 3,213 tables for $731,685,511,687 of holdings culminating in one of the first real financial table datasets. We release our dataset TASERTab to enable the research community to access real-world financial tables and outputs. Our results highlight the promise of agentic, schema-guided extraction systems for robust understanding of real-world financial tables.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MACTAS: Self-Attention-Based Module for Inter-Agent Communication in Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.13661</link>
<guid>https://arxiv.org/abs/2508.13661</guid>
<content:encoded><![CDATA[
arXiv:2508.13661v2 Announce Type: replace 
Abstract: Communication is essential for the collective execution of complex tasks by human agents, motivating interest in communication mechanisms for multi-agent reinforcement learning (MARL). However, existing communication protocols in MARL are often complex and non-differentiable. In this work, we introduce a self-attention-based communication module that exchanges information between the agents in MARL. Our proposed approach is fully differentiable, allowing agents to learn to generate messages in a reward-driven manner. The module can be seamlessly integrated with any action-value function decomposition method and can be viewed as an extension of such decompositions. Notably, it includes a fixed number of trainable parameters, independent of the number of agents. Experimental results on the SMAC and SMACv2 benchmarks demonstrate the effectiveness of our approach, which achieves state-of-the-art performance on a number of maps.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games</title>
<link>https://arxiv.org/abs/2509.01052</link>
<guid>https://arxiv.org/abs/2509.01052</guid>
<content:encoded><![CDATA[
arXiv:2509.01052v2 Announce Type: replace 
Abstract: GUI agents powered by LLMs show promise in interacting with diverse digital environments. Among these, video games offer a valuable testbed due to their varied interfaces, with adventure games posing additional challenges through complex, narrative-driven interactions. Existing game benchmarks, however, lack diversity and rarely evaluate agents on completing entire storylines. To address this, we introduce FlashAdventure, a benchmark of 34 Flash-based adventure games designed to test full story arc completion and tackle the observation-behavior gap: the challenge of remembering and acting on earlier gameplay information. We also propose CUA-as-a-Judge, an automated gameplay evaluator, and COAST, an agentic framework leveraging long-term clue memory to better plan and solve sequential tasks. Experiments show current GUI agents struggle with full story arcs, while COAST improves milestone completion by bridging the observation-behavior gap. Nonetheless, a marked discrepancy between humans and best-performing agents warrants continued research efforts to narrow this divide.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization</title>
<link>https://arxiv.org/abs/2509.13313</link>
<guid>https://arxiv.org/abs/2509.13313</guid>
<content:encoded><![CDATA[
arXiv:2509.13313v2 Announce Type: replace 
Abstract: Large Language Model (LLM)-based web agents demonstrate strong performance on knowledge-intensive tasks but are hindered by context window limitations in paradigms like ReAct. Complex queries involving multiple entities, intertwined relationships, and high uncertainty demand extensive search cycles that rapidly exhaust context budgets before reaching solutions. To overcome this challenge, we introduce ReSum, a novel paradigm that enables indefinite exploration through periodic context summarization. ReSum converts growing interaction histories into compact reasoning states, maintaining awareness of prior discoveries while bypassing context constraints. For paradigm adaptation, we propose ReSum-GRPO, integrating GRPO with segmented trajectory training and advantage broadcasting to familiarize agents with summary-conditioned reasoning. Extensive experiments on web agents across three benchmarks demonstrate that ReSum delivers an average absolute improvement of 4.5% over ReAct, with further gains of 8.2% following ReSum-GRPO training. Notably, with only 1K training samples, our WebResummer-30B (a ReSum-GRPO-trained version of WebSailor-30B) achieves 33.3% Pass@1 on BrowseComp-zh and 18.3% on BrowseComp-en, surpassing most open-source web agents.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can an Individual Manipulate the Collective Decisions of Multi-Agents?</title>
<link>https://arxiv.org/abs/2509.16494</link>
<guid>https://arxiv.org/abs/2509.16494</guid>
<content:encoded><![CDATA[
arXiv:2509.16494v2 Announce Type: replace 
Abstract: Individual Large Language Models (LLMs) have demonstrated significant capabilities across various domains, such as healthcare and law. Recent studies also show that coordinated multi-agent systems exhibit enhanced decision-making and reasoning abilities through collaboration. However, due to the vulnerabilities of individual LLMs and the difficulty of accessing all agents in a multi-agent system, a key question arises: If attackers only know one agent, could they still generate adversarial samples capable of misleading the collective decision? To explore this question, we formulate it as a game with incomplete information, where attackers know only one target agent and lack knowledge of the other agents in the system. With this formulation, we propose M-Spoiler, a framework that simulates agent interactions within a multi-agent system to generate adversarial samples. These samples are then used to manipulate the target agent in the target system, misleading the system's collaborative decision-making process. More specifically, M-Spoiler introduces a stubborn agent that actively aids in optimizing adversarial samples by simulating potential stubborn responses from agents in the target system. This enhances the effectiveness of the generated adversarial samples in misleading the system. Through extensive experiments across various tasks, our findings confirm the risks posed by the knowledge of an individual agent in multi-agent systems and demonstrate the effectiveness of our framework. We also explore several defense mechanisms, showing that our proposed attack framework remains more potent than baselines, underscoring the need for further research into defensive strategies.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AudioGenie-Reasoner: A Training-Free Multi-Agent Framework for Coarse-to-Fine Audio Deep Reasoning</title>
<link>https://arxiv.org/abs/2509.16971</link>
<guid>https://arxiv.org/abs/2509.16971</guid>
<content:encoded><![CDATA[
arXiv:2509.16971v2 Announce Type: replace 
Abstract: Audio deep reasoning is a challenging task that requires expert-level perception, multi-step logical inference, and the integration of contextual knowledge. However, existing models suffer from a gap between audio perception and reasoning abilities due to the lack of training data with explicit reasoning chains and the absence of mechanisms for active exploration and iterative refinement. To address these challenges, we propose AudioGenie-Reasoner (AGR), the first unified training-free multi-agent system that coordinates perception and reasoning over an evolving chain of textual evidence. Our key idea is a paradigm shift that transforms audio deep reasoning into complex text understanding task from a new perspective, thereby unlocking the full potential of large language models. Specifically, the design of AGR mimics the human coarse-to-fine cognitive process. It first transforms the input audio into a coarse text-based document. Then, we design a novel proactive iterative document refinement loop, featuring tool-augmented routes and specialized agents, to continuously search for missing information and augment the evidence chain in a coarse-to-fine manner until sufficient question-related information is gathered for making final predictions. Experimental results show that AGR achieves state-of-the-art (SOTA) performance over existing open-source audio deep reasoning models across various benchmarks. The code will be available at https://github.com/ryysayhi/AudioGenie-Reasoner.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents</title>
<link>https://arxiv.org/abs/2509.23694</link>
<guid>https://arxiv.org/abs/2509.23694</guid>
<content:encoded><![CDATA[
arXiv:2509.23694v3 Announce Type: replace 
Abstract: Search agents connect LLMs to the Internet, enabling access to broader and more up-to-date information. However, unreliable search results may also pose safety threats to end users, establishing a new threat surface. In this work, we conduct two in-the-wild experiments to demonstrate both the prevalence of low-quality search results and their potential to misguide agent behaviors. To counter this threat, we introduce an automated red-teaming framework that is systematic, scalable, and cost-efficient, enabling lightweight and harmless safety assessments of search agents. Building on this framework, we construct the SafeSearch benchmark, which includes 300 test cases covering five categories of risks (e.g., misinformation and indirect prompt injection). Using this benchmark, we evaluate three representative search agent scaffolds, covering search workflow, tool-calling, and deep research, across 7 proprietary and 8 open-source backend LLMs. Our results reveal substantial vulnerabilities of LLM-based search agents: when exposed to unreliable websites, the highest ASR reached 90.5% for GPT-4.1-mini under a search workflow setting. Moreover, our analysis highlights the limited effectiveness of common defense practices, such as reminder prompting. This emphasizes the value of our framework in promoting transparency for safer agent development. Our codebase and test cases are publicly available: https://github.com/jianshuod/SafeSearch.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM/Agent-as-Data-Analyst: A Survey</title>
<link>https://arxiv.org/abs/2509.23988</link>
<guid>https://arxiv.org/abs/2509.23988</guid>
<content:encoded><![CDATA[
arXiv:2509.23988v2 Announce Type: replace 
Abstract: Large language model (LLM) and agent techniques for data analysis (a.k.a LLM/Agent-as-Data-Analyst) have demonstrated substantial impact in both academica and industry. In comparison with traditional rule or small-model based approaches, (agentic) LLMs enable complex data understanding, natural language interfaces, semantic analysis functions, and autonomous pipeline orchestration. The technical evolution further distills five key design goals for intelligent data analysis agents, namely semantic-aware design, modality-hybrid integration, autonomous pipelines, tool-augmented workflows, and support for open-world tasks. From a modality perspective, we review LLM-based techniques for (i) structured data (e.g., table question answering for relational data and NL2GQL for graph data), (ii) semi-structured data (e.g., markup languages understanding and semi-structured table modeling), (iii) unstructured data (e.g., chart understanding, document understanding, programming languages vulnerable detection), and (iv) heterogeneous data (e.g., data retrieval and modality alignment for data lakes). Finally, we outline the remaining challenges and propose several insights and practical directions for advancing LLM/Agent-powered data analysis.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning</title>
<link>https://arxiv.org/abs/2510.08567</link>
<guid>https://arxiv.org/abs/2510.08567</guid>
<content:encoded><![CDATA[
arXiv:2510.08567v2 Announce Type: replace 
Abstract: Vision language models (VLMs) are increasingly deployed as controllers with access to external tools for complex reasoning and decision-making, yet their effectiveness remains limited by the scarcity of high-quality multimodal trajectories and the cost of manual annotation. We address this challenge with a vision-centric agent tuning framework that automatically synthesizes multimodal trajectories, generates step-wise preference pairs, and trains a VLM controller for robust tool-use reasoning. Our pipeline first constructs M-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified trajectories, enabling imitation-based trajectory tuning. Building on this, we develop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool reasoning. To achieve finer alignment, we further introduce Pref-X, a set of 11K automatically generated preference pairs, and optimize MATRIX on it via step-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA, MATRIX consistently surpasses both open- and closed-source VLMs, demonstrating scalable and effective multimodal tool use. Our data and code is avaliable at https://github.com/mbzuai-oryx/MATRIX.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Saving SWE-Bench: A Benchmark Mutation Approach for Realistic Agent Evaluation</title>
<link>https://arxiv.org/abs/2510.08996</link>
<guid>https://arxiv.org/abs/2510.08996</guid>
<content:encoded><![CDATA[
arXiv:2510.08996v2 Announce Type: replace 
Abstract: Current benchmarks for evaluating software engineering agents, such as SWE-Bench Verified, are predominantly derived from GitHub issues and fail to accurately reflect how developers interact with chat-based coding assistants in integrated development environments (IDEs). We posit that this mismatch leads to a systematic overestimation of agent's capabilities in real-world scenarios, especially bug fixing. We introduce a novel benchmarking framework that transforms existing formal benchmarks into realistic user queries through systematic analysis of developer interaction patterns with chat-based agents. Our methodology is flexible and can be easily extended to existing benchmarks. In this paper, we apply our testing framework to SWE-Bench Verified, the TypeScript subset of Multi-SWE-Bench and a private benchmark, SWE-Bench C# and transform formal GitHub issue descriptions into realistic user-style queries based on telemetry analysis of a popular chat-based agent interactions. Our findings reveal that existing benchmarks significantly overestimate agent capabilities for some models by >50% over baseline performance for public benchmarks and ~10-16% for our internal benchmark. This work establishes a new paradigm for evaluating interactive chat-based software engineering agents through benchmark mutation techniques.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoPR: Let's Automate Your Academic Promotion!</title>
<link>https://arxiv.org/abs/2510.09558</link>
<guid>https://arxiv.org/abs/2510.09558</guid>
<content:encoded><![CDATA[
arXiv:2510.09558v2 Announce Type: replace 
Abstract: As the volume of peer-reviewed research surges, scholars increasingly rely on social platforms for discovery, while authors invest considerable effort in promoting their work to ensure visibility and citations. To streamline this process and reduce the reliance on human effort, we introduce Automatic Promotion (AutoPR), a novel task that transforms research papers into accurate, engaging, and timely public content. To enable rigorous evaluation, we release PRBench, a multimodal benchmark that links 512 peer-reviewed articles to high-quality promotional posts, assessing systems along three axes: Fidelity (accuracy and tone), Engagement (audience targeting and appeal), and Alignment (timing and channel optimization). We also introduce PRAgent, a multi-agent framework that automates AutoPR in three stages: content extraction with multimodal preparation, collaborative synthesis for polished outputs, and platform-specific adaptation to optimize norms, tone, and tagging for maximum reach. When compared to direct LLM pipelines on PRBench, PRAgent demonstrates substantial improvements, including a 604% increase in total watch time, a 438% rise in likes, and at least a 2.9x boost in overall engagement. Ablation studies show that platform modeling and targeted promotion contribute the most to these gains. Our results position AutoPR as a tractable, measurable research problem and provide a roadmap for scalable, impactful automated scholarly communication.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Hypergraph Using Large Language Models</title>
<link>https://arxiv.org/abs/2510.11728</link>
<guid>https://arxiv.org/abs/2510.11728</guid>
<content:encoded><![CDATA[
arXiv:2510.11728v1 Announce Type: new 
Abstract: Due to the advantages of hypergraphs in modeling high-order relationships in complex systems, they have been applied to higher-order clustering, hypergraph neural networks and computer vision. These applications rely heavily on access to high-quality, large-scale real-world hypergraph data. Yet, compared to traditional pairwise graphs, real hypergraph datasets remain scarce in both scale and diversity. This shortage significantly limits the development and evaluation of advanced hypergraph learning algorithms. Therefore, how to quickly generate large-scale hypergraphs that conform to the characteristics of real networks is a crucial task that has not received sufficient attention. Motivated by recent advances in large language models (LLMs), particularly their capabilities in semantic reasoning, structured generation, and simulating human behavior, we investigate whether LLMs can facilitate hypergraph generation from a fundamentally new perspective. We introduce HyperLLM, a novel LLM-driven hypergraph generator that simulates the formation and evolution of hypergraphs through a multi-agent collaboration. The framework integrates prompts and structural feedback mechanisms to ensure that the generated hypergraphs reflect key real-world patterns. Extensive experiments across diverse datasets demonstrate that HyperLLM achieves superior fidelity to structural and temporal hypergraph patterns, while requiring minimal statistical priors. Our findings suggest that LLM-based frameworks offer a promising new direction for hypergraph modeling.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Agents for the Dhumbal Card Game: A Comparative Study</title>
<link>https://arxiv.org/abs/2510.11736</link>
<guid>https://arxiv.org/abs/2510.11736</guid>
<content:encoded><![CDATA[
arXiv:2510.11736v1 Announce Type: new 
Abstract: This study evaluates Artificial Intelligence (AI) agents for Dhumbal, a culturally significant multiplayer card game with imperfect information, through a systematic comparison of rule-based, search-based, and learning-based strategies. We formalize Dhumbal's mechanics and implement diverse agents, including heuristic approaches (Aggressive, Conservative, Balanced, Opportunistic), search-based methods such as Monte Carlo Tree Search (MCTS) and Information Set Monte Carlo Tree Search (ISMCTS), and reinforcement learning approaches including Deep Q-Network (DQN) and Proximal Policy Optimization (PPO), and a random baseline. Evaluation involves within-category tournaments followed by a cross-category championship. Performance is measured via win rate, economic outcome, Jhyap success, cards discarded per round, risk assessment, and decision efficiency. Statistical significance is assessed using Welch's t-test with Bonferroni correction, effect sizes via Cohen's d, and 95% confidence intervals (CI). Across 1024 simulated rounds, the rule-based Aggressive agent achieves the highest win rate (88.3%, 95% CI: [86.3, 90.3]), outperforming ISMCTS (9.0%) and PPO (1.5%) through effective exploitation of Jhyap declarations. The study contributes a reproducible AI framework, insights into heuristic efficacy under partial information, and open-source code, thereby advancing AI research and supporting digital preservation of cultural games.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AwareCompiler: Agentic Context-Aware Compiler Optimization via a Synergistic Knowledge-Data Driven Framework</title>
<link>https://arxiv.org/abs/2510.11759</link>
<guid>https://arxiv.org/abs/2510.11759</guid>
<content:encoded><![CDATA[
arXiv:2510.11759v1 Announce Type: new 
Abstract: Compiler optimization is crucial for enhancing program performance by transforming the sequence of optimization passes while maintaining correctness. Despite the promising potential of large language models (LLMs)-based agent for software optimization, automating compiler optimization remains challenging due to: (1) semantic misalignment between abstract program representations and concrete optimization passes, (2) inefficient interaction mechanisms between agents and compiler environments, and (3) reward sparsity from the extensive decision-making process within large optimization spaces. This paper introduces \textbf{AwareCompiler}, an agentic framework for compiler optimization that addresses these challenges through three key innovations: structured knowledge integration and dataset construction, knowledge-driven adaptive pass generation, and data-driven hybrid training pipeline. Experimental results on standard benchmarks demonstrate that AwareCompiler significantly outperforms existing baselines in both performance and efficiency, highlighting the effectiveness of our synergistic knowledge-data-driven approach. Our code is publicly available at https://github.com/LHY-24/AwareCompiler.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Audio-Guided Visual Perception for Audio-Visual Navigation</title>
<link>https://arxiv.org/abs/2510.11760</link>
<guid>https://arxiv.org/abs/2510.11760</guid>
<content:encoded><![CDATA[
arXiv:2510.11760v1 Announce Type: new 
Abstract: Audio-Visual Embodied Navigation aims to enable agents to autonomously navigate to sound sources in unknown 3D environments using auditory cues. While current AVN methods excel on in-distribution sound sources, they exhibit poor cross-source generalization: navigation success rates plummet and search paths become excessively long when agents encounter unheard sounds or unseen environments. This limitation stems from the lack of explicit alignment mechanisms between auditory signals and corresponding visual regions. Policies tend to memorize spurious \enquote{acoustic fingerprint-scenario} correlations during training, leading to blind exploration when exposed to novel sound sources. To address this, we propose the AGVP framework, which transforms sound from policy-memorable acoustic fingerprint cues into spatial guidance. The framework first extracts global auditory context via audio self-attention, then uses this context as queries to guide visual feature attention, highlighting sound-source-related regions at the feature level. Subsequent temporal modeling and policy optimization are then performed. This design, centered on interpretable cross-modal alignment and region reweighting, reduces dependency on specific acoustic fingerprints. Experimental results demonstrate that AGVP improves both navigation efficiency and robustness while achieving superior cross-scenario generalization on previously unheard sounds.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirical Study on Robustness and Resilience in Cooperative Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.11824</link>
<guid>https://arxiv.org/abs/2510.11824</guid>
<content:encoded><![CDATA[
arXiv:2510.11824v1 Announce Type: new 
Abstract: In cooperative Multi-Agent Reinforcement Learning (MARL), it is a common practice to tune hyperparameters in ideal simulated environments to maximize cooperative performance. However, policies tuned for cooperation often fail to maintain robustness and resilience under real-world uncertainties. Building trustworthy MARL systems requires a deep understanding of robustness, which ensures stability under uncertainties, and resilience, the ability to recover from disruptions--a concept extensively studied in control systems but largely overlooked in MARL. In this paper, we present a large-scale empirical study comprising over 82,620 experiments to evaluate cooperation, robustness, and resilience in MARL across 4 real-world environments, 13 uncertainty types, and 15 hyperparameters. Our key findings are: (1) Under mild uncertainty, optimizing cooperation improves robustness and resilience, but this link weakens as perturbations intensify. Robustness and resilience also varies by algorithm and uncertainty type. (2) Robustness and resilience do not generalize across uncertainty modalities or agent scopes: policies robust to action noise for all agents may fail under observation noise on a single agent. (3) Hyperparameter tuning is critical for trustworthy MARL: surprisingly, standard practices like parameter sharing, GAE, and PopArt can hurt robustness, while early stopping, high critic learning rates, and Leaky ReLU consistently help. By optimizing hyperparameters only, we observe substantial improvement in cooperation, robustness and resilience across all MARL backbones, with the phenomenon also generalizing to robust MARL methods across these backbones. Code and results available at https://github.com/BUAA-TrustworthyMARL/adv_marl_benchmark .
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Multi-Sensory Meditation: Exploring Immersive Depth and Activation in Virtual Reality</title>
<link>https://arxiv.org/abs/2510.11830</link>
<guid>https://arxiv.org/abs/2510.11830</guid>
<content:encoded><![CDATA[
arXiv:2510.11830v1 Announce Type: new 
Abstract: Mindfulness meditation has seen increasing applications in diverse domains as an effective practice to improve mental health. However, the standardized frameworks adopted by most applications often fail to cater to users with various psychological states and health conditions. This limitation arises primarily from the lack of personalization and adaptive content design. To address this, we propose MindfulVerse, an AI-Generated Content (AIGC)-driven application to create personalized and immersive mindfulness experiences. By developing a novel agent, the system can dynamically adjust the meditation content based on the ideas of individual users. Furthermore, we conducted exploratory user studies and comparative evaluations to assess the application scenarios and performance of our novel generative meditation tool in VR environments. The results of this user study indicate that generative meditation improves neural activation in self-regulation and shows a positive impact on emotional regulation and participation. Our approach offers a generative meditation procedure that provides users with an application that better suits their preferences and states.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lingxi: Repository-Level Issue Resolution Framework Enhanced by Procedural Knowledge Guided Scaling</title>
<link>https://arxiv.org/abs/2510.11838</link>
<guid>https://arxiv.org/abs/2510.11838</guid>
<content:encoded><![CDATA[
arXiv:2510.11838v1 Announce Type: new 
Abstract: Driven by the advancements of Large Language Models (LLMs), LLM-powered agents are making significant improvements in software engineering tasks, yet struggle with complex, repository-level issue resolution. Existing agent-based methods have two key limitations. First, they lack of procedural knowledge (i.e., how an issue is fixed step-by-step and rationales behind it) to learn and leverage for issue resolution. Second, they rely on massive computational power to blindly explore the solution space. %
To address those limitations, we propose Lingxi, an issue resolution framework that leverages procedural knowledge extracted from historical issue-fixing data to guide agents in solving repository-level issues. \ourTool first constructs this knowledge offline through a hierarchical abstraction mechanism, enabling agents to learn the how and why behind a fix, not just the final solution. During online application, it employs a knowledge-driven scaling method that leverages the procedural knowledge of similar issues to intelligently analyze the target issue from multiple perspectives, in sharp contrast to undirected, brute-force exploration. %
Lingxi successfully resolves 74.6\% of bugs on the SWE-bench Verified benchmark in Past@1 setting, outperforming five state-of-the-art techniques by a significant margin (5.4\% to 14.9\%). Our comprehensive ablation study confirmed that the success of Lingxi comes directly from its use of procedural knowledge. Without it, the performance gains from scaling alone is negligible. Our qualitative study further shows that the ``design patterns $\&$ coding practices'' is the most critical knowledge aspect, and that the roles of different knowledge aspects switch across different stages (i.e., analysis, planning, and fixing).
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Research Brings Deeper Harm</title>
<link>https://arxiv.org/abs/2510.11851</link>
<guid>https://arxiv.org/abs/2510.11851</guid>
<content:encoded><![CDATA[
arXiv:2510.11851v1 Announce Type: new 
Abstract: Deep Research (DR) agents built on Large Language Models (LLMs) can perform complex, multi-step research by decomposing tasks, retrieving online information, and synthesizing detailed reports. However, the misuse of LLMs with such powerful capabilities can lead to even greater risks. This is especially concerning in high-stakes and knowledge-intensive domains such as biosecurity, where DR can generate a professional report containing detailed forbidden knowledge. Unfortunately, we have found such risks in practice: simply submitting a harmful query, which a standalone LLM directly rejects, can elicit a detailed and dangerous report from DR agents. This highlights the elevated risks and underscores the need for a deeper safety analysis. Yet, jailbreak methods designed for LLMs fall short in exposing such unique risks, as they do not target the research ability of DR agents. To address this gap, we propose two novel jailbreak strategies: Plan Injection, which injects malicious sub-goals into the agent's plan; and Intent Hijack, which reframes harmful queries as academic research questions. We conducted extensive experiments across different LLMs and various safety benchmarks, including general and biosecurity forbidden prompts. These experiments reveal 3 key findings: (1) Alignment of the LLMs often fail in DR agents, where harmful prompts framed in academic terms can hijack agent intent; (2) Multi-step planning and execution weaken the alignment, revealing systemic vulnerabilities that prompt-level safeguards cannot address; (3) DR agents not only bypass refusals but also produce more coherent, professional, and dangerous content, compared with standalone LLMs. These results demonstrate a fundamental misalignment in DR agents and call for better alignment techniques tailored to DR agents. Code and datasets are available at https://chenxshuo.github.io/deeper-harm.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DMAS-Forge: A Framework for Transparent Deployment of AI Applications as Distributed Systems</title>
<link>https://arxiv.org/abs/2510.11872</link>
<guid>https://arxiv.org/abs/2510.11872</guid>
<content:encoded><![CDATA[
arXiv:2510.11872v1 Announce Type: new 
Abstract: Agentic AI applications increasingly rely on multiple agents with distinct roles, specialized tools, and access to memory layers to solve complex tasks -- closely resembling service-oriented architectures. Yet, in the rapid evolving landscape of programming frameworks and new protocols, deploying and testing AI agents as distributed systems remains a daunting and labor-intensive task. We present DMAS-Forge, a framework designed to close this gap. DMAS-Forge decouples application logic from specific deployment choices, and aims at transparently generating the necessary glue code and configurations to spawn distributed multi-agent applications across diverse deployment scenarios with minimal manual effort. We present our vision, design principles, and a prototype of DMAS-Forge. Finally, we discuss the opportunities and future work for our approach.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R-WoM: Retrieval-augmented World Model For Computer-use Agents</title>
<link>https://arxiv.org/abs/2510.11892</link>
<guid>https://arxiv.org/abs/2510.11892</guid>
<content:encoded><![CDATA[
arXiv:2510.11892v1 Announce Type: new 
Abstract: Large Language Models (LLMs) can serve as world models to enhance agent decision-making in digital environments by simulating future states and predicting action outcomes, potentially eliminating costly trial-and-error exploration. However, this capability is fundamentally limited by LLMs' tendency toward hallucination and their reliance on static training knowledge, which can lead to compounding errors that inhibit long-horizon simulations. To systematically investigate whether LLMs are appropriate for world modeling, we probe two core capabilities of world models--future state prediction and reward estimation--through three tasks: next-state identification, full-procedure planning alignment, and milestone transition recognition. Our analysis shows that while LLMs effectively capture immediate next states and identify meaningful state transitions, their performance rapidly degrades in full-procedure planning. This highlights LLMs' limitations in reliably modeling environment dynamics over long horizons. To address these limitations, we propose the Retrieval-augmented World Model (R-WoM), which grounds LLM simulations by incorporating factual, up-to-date knowledge retrieved from external tutorials. Experiments show that R-WoM achieves substantial improvements of up to 25.3% (OSWorld) and 18.1% (WebArena) compared to baselines, with particular advantages in longer-horizon simulations.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Long-Horizon LLM Agent via Context-Folding</title>
<link>https://arxiv.org/abs/2510.11967</link>
<guid>https://arxiv.org/abs/2510.11967</guid>
<content:encoded><![CDATA[
arXiv:2510.11967v1 Announce Type: new 
Abstract: Large language model (LLM) agents are fundamentally constrained by context length on long-horizon tasks. We introduce Context-Folding, a framework that empowers agents to actively manage their working context. An agent can procedurally branch into a sub-trajectory to handle a subtask and then fold it upon completion, collapsing the intermediate steps while retaining a concise summary of the outcome. To make this behavior learnable, we develop an end-to-end reinforcement learning framework FoldGRPO with specific process rewards to encourage effective task decomposition and context management. On complex long-horizon tasks (Deep Research and SWE), our folding agent matches or outperforms the ReAct baselines while using an active context 10$\times$ smaller and significantly outperforms models that rely on summarization-based context management.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation</title>
<link>https://arxiv.org/abs/2510.11977</link>
<guid>https://arxiv.org/abs/2510.11977</guid>
<content:encoded><![CDATA[
arXiv:2510.11977v1 Announce Type: new 
Abstract: AI agents have been developed for complex real-world tasks from coding to customer service. But AI agent evaluations suffer from many challenges that undermine our understanding of how well agents really work. We introduce the Holistic Agent Leaderboard (HAL) to address these challenges. We make three main contributions. First, we provide a standardized evaluation harness that orchestrates parallel evaluations across hundreds of VMs, reducing evaluation time from weeks to hours while eliminating common implementation bugs. Second, we conduct three-dimensional analysis spanning models, scaffolds, and benchmarks. We validate the harness by conducting 21,730 agent rollouts across 9 models and 9 benchmarks in coding, web navigation, science, and customer service with a total cost of about $40,000. Our analysis reveals surprising insights, such as higher reasoning effort reducing accuracy in the majority of runs. Third, we use LLM-aided log inspection to uncover previously unreported behaviors, such as searching for the benchmark on HuggingFace instead of solving a task, or misusing credit cards in flight booking tasks. We share all agent logs, comprising 2.5B tokens of language model calls, to incentivize further research into agent behavior. By standardizing how the field evaluates agents and addressing common pitfalls in agent evaluation, we hope to shift the focus from agents that ace benchmarks to agents that work reliably in the real world.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAGE: A Top-Down Bottom-Up Knowledge-Grounded User Simulator for Multi-turn AGent Evaluation</title>
<link>https://arxiv.org/abs/2510.11997</link>
<guid>https://arxiv.org/abs/2510.11997</guid>
<content:encoded><![CDATA[
arXiv:2510.11997v1 Announce Type: new 
Abstract: Evaluating multi-turn interactive agents is challenging due to the need for human assessment. Evaluation with simulated users has been introduced as an alternative, however existing approaches typically model generic users and overlook the domain-specific principles required to capture realistic behavior. We propose SAGE, a novel user Simulation framework for multi-turn AGent Evaluation that integrates knowledge from business contexts. SAGE incorporates top-down knowledge rooted in business logic, such as ideal customer profiles, grounding user behavior in realistic customer personas. We further integrate bottom-up knowledge taken from business agent infrastructure (e.g., product catalogs, FAQs, and knowledge bases), allowing the simulator to generate interactions that reflect users' information needs and expectations in a company's target market. Through empirical evaluation, we find that this approach produces interactions that are more realistic and diverse, while also identifying up to 33% more agent errors, highlighting its effectiveness as an evaluation tool to support bug-finding and iterative agent improvement.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CausalTrace: A Neurosymbolic Causal Analysis Agent for Smart Manufacturing</title>
<link>https://arxiv.org/abs/2510.12033</link>
<guid>https://arxiv.org/abs/2510.12033</guid>
<content:encoded><![CDATA[
arXiv:2510.12033v1 Announce Type: new 
Abstract: Modern manufacturing environments demand not only accurate predictions but also interpretable insights to process anomalies, root causes, and potential interventions. Existing AI systems often function as isolated black boxes, lacking the seamless integration of prediction, explanation, and causal reasoning required for a unified decision-support solution. This fragmentation limits their trustworthiness and practical utility in high-stakes industrial environments. In this work, we present CausalTrace, a neurosymbolic causal analysis module integrated into the SmartPilot industrial CoPilot. CausalTrace performs data-driven causal analysis enriched by industrial ontologies and knowledge graphs, including advanced functions such as causal discovery, counterfactual reasoning, and root cause analysis (RCA). It supports real-time operator interaction and is designed to complement existing agents by offering transparent, explainable decision support. We conducted a comprehensive evaluation of CausalTrace using multiple causal assessment methods and the C3AN framework (i.e. Custom, Compact, Composite AI with Neurosymbolic Integration), which spans principles of robustness, intelligence, and trustworthiness. In an academic rocket assembly testbed, CausalTrace achieved substantial agreement with domain experts (ROUGE-1: 0.91 in ontology QA) and strong RCA performance (MAP@3: 94%, PR@2: 97%, MRR: 0.92, Jaccard: 0.92). It also attained 4.59/5 in the C3AN evaluation, demonstrating precision and reliability for live deployment.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering LLM Agents with Geospatial Awareness: Toward Grounded Reasoning for Wildfire Response</title>
<link>https://arxiv.org/abs/2510.12061</link>
<guid>https://arxiv.org/abs/2510.12061</guid>
<content:encoded><![CDATA[
arXiv:2510.12061v1 Announce Type: new 
Abstract: Effective disaster response is essential for safeguarding lives and property. Existing statistical approaches often lack semantic context, generalize poorly across events, and offer limited interpretability. While Large language models (LLMs) provide few-shot generalization, they remain text-bound and blind to geography. To bridge this gap, we introduce a Geospatial Awareness Layer (GAL) that grounds LLM agents in structured earth data. Starting from raw wildfire detections, GAL automatically retrieves and integrates infrastructure, demographic, terrain, and weather information from external geodatabases, assembling them into a concise, unit-annotated perception script. This enriched context enables agents to produce evidence-based resource-allocation recommendations (e.g., personnel assignments, budget allocations), further reinforced by historical analogs and daily change signals for incremental updates. We evaluate the framework in real wildfire scenarios across multiple LLM models, showing that geospatially grounded agents can outperform baselines. The proposed framework can generalize to other hazards such as floods and hurricanes.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Agents as Universal Task Solvers</title>
<link>https://arxiv.org/abs/2510.12066</link>
<guid>https://arxiv.org/abs/2510.12066</guid>
<content:encoded><![CDATA[
arXiv:2510.12066v1 Announce Type: new 
Abstract: AI reasoning agents are already able to solve a variety of tasks by deploying tools, simulating outcomes of multiple hypotheses and reflecting on them. In doing so, they perform computation, although not in the classical sense -- there is no program being executed. Still, if they perform computation, can AI agents be universal? Can chain-of-thought reasoning solve any computable task? How does an AI Agent learn to reason? Is it a matter of model size? Or training dataset size?
  In this work, we reinterpret the role of learning in the context of AI Agents, viewing them as compute-capable stochastic dynamical systems, and highlight the role of time in a foundational principle for learning to reason. In doing so, we propose a shift from classical inductive learning to transductive learning -- where the objective is not to approximate the distribution of past data, but to capture their algorithmic structure to reduce the time needed to find solutions to new tasks.
  Transductive learning suggests that, counter to Shannon's theory, a key role of information in learning is about reduction of time rather than reconstruction error. In particular, we show that the optimal speed-up that a universal solver can achieve using past data is tightly related to their algorithmic information. Using this, we show a theoretical derivation for the observed power-law scaling of inference time versus training time. We then show that scaling model size can lead to behaviors that, while improving accuracy on benchmarks, fail any reasonable test of intelligence, let alone super-intelligence: In the limit of infinite space and time, large models can behave as savants, able to brute-force through any task without any insight. Instead, we argue that the key quantity to optimize when scaling reasoning models is time, whose critical role in learning has so far only been indirectly considered.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmboMatrix: A Scalable Training-Ground for Embodied Decision-Making</title>
<link>https://arxiv.org/abs/2510.12072</link>
<guid>https://arxiv.org/abs/2510.12072</guid>
<content:encoded><![CDATA[
arXiv:2510.12072v1 Announce Type: new 
Abstract: Embodied decision-making enables agents to translate high-level goals into executable actions through continuous interactions within the physical world, forming a cornerstone of general-purpose embodied intelligence. Large language models (LLMs), with their general decision-making capabilities, offer a promising path to realize this potential; however, LLMs trained solely on language lack exposure to physical environments, limiting their true embodied understanding. To bridge this gap, we propose the concept of a training ground: a comprehensive infrastructure that provides task and scene simulation, embodied interaction, and feedback signals, offering a one-stop solution for LLM acquire genuine embodied decision-making skills. In this work, we present EmboMatrix, the first training ground of its kind, providing massive and diverse tasks with efficient simulation and precise rewards. EmboMatrix incorporates a series of novel techniques: a multi-agent data engine for large-scale task and scene generation, a distributed heterogeneous-hardware system for scalable simulation, and a multi-level reward architecture for precise supervision. Leveraging EmboMatrix, we cultivate EmboBrain, an LLM whose embodied decision-making abilities emerge from extensive embodied interactions. Experiments show that EmboBrain-7B surpasses the 671B DeepSeek-R1 baseline by 9.5\% on two challenging embodied decision-making benchmarks, demonstrating the power of interactive, environment-grounded learning for building truly intelligent embodied agents.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Quality of Randomness and Entropy in Tasks Supported by Large Language Models</title>
<link>https://arxiv.org/abs/2510.12080</link>
<guid>https://arxiv.org/abs/2510.12080</guid>
<content:encoded><![CDATA[
arXiv:2510.12080v1 Announce Type: new 
Abstract: The rapid advancement of large language model (LLM) technology has led to diverse applications, many of which inherently require randomness, such as stochastic decision-making, gaming, scheduling, AI agents, and cryptography-related tasks. However, the capabilities of LLMs in handling randomness, particularly in generating and utilizing random numbers effectively, remain unclear. This paper investigates the capacity of LLMs for handling tasks that involve randomness through a series of experiments. We designed a set of experiments that consider various factors that can influence an LLM's performance in tasks involving randomness, such as accessibility to external tools, types of tasks, model states (fresh vs. non-fresh), and prompting strategies. The experiments cover a range of tasks, including generating random numbers, generating random strings such as passwords, shuffling items, and evaluating the quality of randomness using entropy and the NIST randomness test-suite. Our findings reveal that while LLMs can generate outputs that exhibit some degree of randomness, their performance is inconsistent and often deviates significantly from the expected behavior. The analysis of the experimental results highlights key limitations and areas where improvement is needed for the LLMs to effectively handle tasks involving randomness
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration</title>
<link>https://arxiv.org/abs/2510.12088</link>
<guid>https://arxiv.org/abs/2510.12088</guid>
<content:encoded><![CDATA[
arXiv:2510.12088v1 Announce Type: new 
Abstract: Symbolic world modeling requires inferring and representing an environment's transitional dynamics as an executable program. Prior work has focused on largely deterministic environments with abundant interaction data, simple mechanics, and human guidance. We address a more realistic and challenging setting, learning in a complex, stochastic environment where the agent has only "one life" to explore a hostile environment without human guidance. We introduce OneLife, a framework that models world dynamics through conditionally-activated programmatic laws within a probabilistic programming framework. Each law operates through a precondition-effect structure, activating in relevant world states. This creates a dynamic computation graph that routes inference and optimization only through relevant laws, avoiding scaling challenges when all laws contribute to predictions about a complex, hierarchical state, and enabling the learning of stochastic dynamics even with sparse rule activation. To evaluate our approach under these demanding constraints, we introduce a new evaluation protocol that measures (a) state ranking, the ability to distinguish plausible future states from implausible ones, and (b) state fidelity, the ability to generate future states that closely resemble reality. We develop and evaluate our framework on Crafter-OO, our reimplementation of the Crafter environment that exposes a structured, object-oriented symbolic state and a pure transition function that operates on that state alone. OneLife can successfully learn key environment dynamics from minimal, unguided interaction, outperforming a strong baseline on 16 out of 23 scenarios tested. We also test OneLife's planning ability, with simulated rollouts successfully identifying superior strategies. Our work establishes a foundation for autonomously constructing programmatic world models of unknown, complex environments.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToPolyAgent: AI Agents for Coarse-Grained Topological Polymer Simulations</title>
<link>https://arxiv.org/abs/2510.12091</link>
<guid>https://arxiv.org/abs/2510.12091</guid>
<content:encoded><![CDATA[
arXiv:2510.12091v1 Announce Type: new 
Abstract: We introduce ToPolyAgent, a multi-agent AI framework for performing coarse-grained molecular dynamics (MD) simulations of topological polymers through natural language instructions. By integrating large language models (LLMs) with domain-specific computational tools, ToPolyAgent supports both interactive and autonomous simulation workflows across diverse polymer architectures, including linear, ring, brush, and star polymers, as well as dendrimers. The system consists of four LLM-powered agents: a Config Agent for generating initial polymer-solvent configurations, a Simulation Agent for executing LAMMPS-based MD simulations and conformational analyses, a Report Agent for compiling markdown reports, and a Workflow Agent for streamlined autonomous operations. Interactive mode incorporates user feedback loops for iterative refinements, while autonomous mode enables end-to-end task execution from detailed prompts. We demonstrate ToPolyAgent's versatility through case studies involving diverse polymer architectures under varying solvent condition, thermostats, and simulation lengths. Furthermore, we highlight its potential as a research assistant by directing it to investigate the effect of interaction parameters on the linear polymer conformation, and the influence of grafting density on the persistence length of the brush polymer. By coupling natural language interfaces with rigorous simulation tools, ToPolyAgent lowers barriers to complex computational workflows and advances AI-driven materials discovery in polymer science. It lays the foundation for autonomous and extensible multi-agent scientific research ecosystems.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IL3D: A Large-Scale Indoor Layout Dataset for LLM-Driven 3D Scene Generation</title>
<link>https://arxiv.org/abs/2510.12095</link>
<guid>https://arxiv.org/abs/2510.12095</guid>
<content:encoded><![CDATA[
arXiv:2510.12095v1 Announce Type: new 
Abstract: In this study, we present IL3D, a large-scale dataset meticulously designed for large language model (LLM)-driven 3D scene generation, addressing the pressing demand for diverse, high-quality training data in indoor layout design. Comprising 27,816 indoor layouts across 18 prevalent room types and a library of 29,215 high-fidelity 3D object assets, IL3D is enriched with instance-level natural language annotations to support robust multimodal learning for vision-language tasks. We establish rigorous benchmarks to evaluate LLM-driven scene generation. Experimental results show that supervised fine-tuning (SFT) of LLMs on IL3D significantly improves generalization and surpasses the performance of SFT on other datasets. IL3D offers flexible multimodal data export capabilities, including point clouds, 3D bounding boxes, multiview images, depth maps, normal maps, and semantic masks, enabling seamless adaptation to various visual tasks. As a versatile and robust resource, IL3D significantly advances research in 3D scene generation and embodied intelligence, by providing high-fidelity scene data to support environment perception tasks of embodied agents.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Engineering Multi-Agent LLMs: A Protocol-Driven Approach</title>
<link>https://arxiv.org/abs/2510.12120</link>
<guid>https://arxiv.org/abs/2510.12120</guid>
<content:encoded><![CDATA[
arXiv:2510.12120v1 Announce Type: new 
Abstract: The increasing demand for software development has driven interest in automating software engineering (SE) tasks using Large Language Models (LLMs). Recent efforts extend LLMs into multi-agent systems (MAS) that emulate collaborative development workflows, but these systems often fail due to three core deficiencies: under-specification, coordination misalignment, and inappropriate verification, arising from the absence of foundational SE structuring principles. This paper introduces Software Engineering Multi-Agent Protocol (SEMAP), a protocol-layer methodology that instantiates three core SE design principles for multi-agent LLMs: (1) explicit behavioral contract modeling, (2) structured messaging, and (3) lifecycle-guided execution with verification, and is implemented atop Google's Agent-to-Agent (A2A) infrastructure. Empirical evaluation using the Multi-Agent System Failure Taxonomy (MAST) framework demonstrates that SEMAP effectively reduces failures across different SE tasks. In code development, it achieves up to a 69.6% reduction in total failures for function-level development and 56.7% for deployment-level development. For vulnerability detection, SEMAP reduces failure counts by up to 47.4% on Python tasks and 28.2% on C/C++ tasks.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaCaptioner: Towards Generalist Visual Captioning with Open-source Suites</title>
<link>https://arxiv.org/abs/2510.12126</link>
<guid>https://arxiv.org/abs/2510.12126</guid>
<content:encoded><![CDATA[
arXiv:2510.12126v1 Announce Type: new 
Abstract: Generalist visual captioning goes beyond a simple appearance description task, but requires integrating a series of visual cues into a caption and handling various visual domains. In this task, current open-source models present a large performance gap with commercial ones, which limits various applications such as data synthesis. To bridge the gap, this paper proposes CapFlow, a novel multi-agent collaboration workflow. CapFlow demonstrates for the first time that, by capitalizing on open-source models, it is possible to achieve caption quality on par with GPT-4.1 in various domains with an 89.5% reduction in costs. By leveraging CapFlow as the data synthesizer, we produce high-quality visual captions from image and video domains at scale, and obtain a generalist visual captioner via fine-tuning, namely MetaCaptioner. Through extensive experiments, we show that MetaCaptioner not only achieves comparable captioning capabilities with commercial models but also reaches top-tier multimodal performance in the open-source community. We hope CapFlow and MetaCaptioner can benefit future multimodal research by providing a strong and cost-effective visual captioning solution.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Division of Indivisible Items</title>
<link>https://arxiv.org/abs/2510.12158</link>
<guid>https://arxiv.org/abs/2510.12158</guid>
<content:encoded><![CDATA[
arXiv:2510.12158v1 Announce Type: new 
Abstract: We study the fair division of indivisible items. In the general model, the goal is to allocate $m$ indivisible items to $n$ agents while satisfying fairness criteria such as MMS, EF1, and EFX. We also study a recently-introduced graphical model that represents the fair division problem as a multigraph, in which vertices correspond to agents and edges to items. The graphical model stipulates that an item can have non-zero marginal utility to an agent only if its corresponding edge is incident to the agent's corresponding vertex. We study orientations (allocations that allocate each edge to an endpoint) in this model, as they are particularly desirable.
  Our first contribution concerns MMS allocations of mixed manna (i.e. a mixture of goods and chores) in the general model. It is known that MMS allocations of goods exist when $m \leq n+5$. We generalize this and show that when $m \leq n+5$, MMS allocations of mixed manna exist as long as $n \leq 3$, there is an agent whose MMS threshold is non-negative, or every item is a chore. Remarkably, our result leaves only the case where every agent has a negative MMS threshold unanswered.
  Our second contribution concerns EFX orientations of multigraphs of goods. We show that deciding whether EFX orientations exist for multigraphs is NP-complete, even for symmetric bi-valued multigraphs. Complementarily, we show symmetric bi-valued multigraphs that do not contain non-trivial odd multitrees have EFX orientations that can be found in polynomial time.
  Our third contribution concerns EF1 and EFX orientations of graphs and multigraphs of chores. We obtain polynomial-time algorithms for deciding whether such graphs have EF1 and EFX orientations, resolving a previous conjecture and showing a fundamental difference between goods and chores division. In addition, we show that the analogous problems for multigraphs are NP-hard.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-Based Simulation of a Financial Market with Large Language Models</title>
<link>https://arxiv.org/abs/2510.12189</link>
<guid>https://arxiv.org/abs/2510.12189</guid>
<content:encoded><![CDATA[
arXiv:2510.12189v1 Announce Type: new 
Abstract: In real-world stock markets, certain chart patterns -- such as price declines near historical highs -- cannot be fully explained by fundamentals alone. These phenomena suggest the presence of path dependence in price formation, where investor decisions are influenced not only by current market conditions but also by the trajectory of prices leading up to the present. Path dependence has drawn attention in behavioral finance as a key mechanism behind such anomalies. One plausible driver of path dependence is human loss aversion, anchored to individual reference points like purchase prices or past peaks, which vary with personal context. However, capturing such subtle behavioral tendencies in traditional agent-based market simulations has remained a challenge. We propose the Fundamental-Chartist-LLM-Agent (FCLAgent), which uses large language models (LLMs) to emulate human-like trading decisions. In this framework, (1) buy/sell decisions are made by LLMs based on individual situations, while (2) order price and volume follow standard rule-based methods. Simulations show that FCLAgents reproduce path-dependent patterns that conventional agents fail to capture. Furthermore, an analysis of FCLAgents' behavior reveals that the reference points guiding loss aversion vary with market trajectories, highlighting the potential of LLM-based agents to model nuanced investor behavior.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ResearStudio: A Human-Intervenable Framework for Building Controllable Deep-Research Agents</title>
<link>https://arxiv.org/abs/2510.12194</link>
<guid>https://arxiv.org/abs/2510.12194</guid>
<content:encoded><![CDATA[
arXiv:2510.12194v1 Announce Type: new 
Abstract: Current deep-research agents run in a ''fire-and-forget'' mode: once started, they give users no way to fix errors or add expert knowledge during execution. We present ResearStudio, the first open-source framework that places real-time human control at its core. The system follows a Collaborative Workshop design. A hierarchical Planner-Executor writes every step to a live ''plan-as-document,'' a fast communication layer streams each action, file change, and tool call to a web interface. At any moment, the user can pause the run, edit the plan or code, run custom commands, and resume -- switching smoothly between AI-led, human-assisted and human-led, AI-assisted modes. In fully autonomous mode, ResearStudio achieves state-of-the-art results on the GAIA benchmark, surpassing systems like OpenAI's DeepResearch and Manus. These results show that strong automated performance and fine-grained human control can coexist. The full code, protocol, and evaluation scripts are available at https://github.com/ResearAI/ResearStudio. We will continue to update the repository to encourage further work on safe and controllable research agents. Our live demo is publicly accessible at http://ai-researcher.net:3000/. We support the development of DeepScientist, which can be accessed at https://github.com/ResearAI/DeepScientist.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HackWorld: Evaluating Computer-Use Agents on Exploiting Web Application Vulnerabilities</title>
<link>https://arxiv.org/abs/2510.12200</link>
<guid>https://arxiv.org/abs/2510.12200</guid>
<content:encoded><![CDATA[
arXiv:2510.12200v1 Announce Type: new 
Abstract: Web applications are prime targets for cyberattacks as gateways to critical services and sensitive data. Traditional penetration testing is costly and expertise-intensive, making it difficult to scale with the growing web ecosystem. While language model agents show promise in cybersecurity, modern web applications demand visual understanding, dynamic content handling, and multi-step interactions that only computer-use agents (CUAs) can perform. Yet, their ability to discover and exploit vulnerabilities through graphical interfaces remains largely unexplored. We present HackWorld, the first framework for systematically evaluating CUAs' capabilities to exploit web application vulnerabilities via visual interaction. Unlike sanitized benchmarks, HackWorld includes 36 real-world applications across 11 frameworks and 7 languages, featuring realistic flaws such as injection vulnerabilities, authentication bypasses, and unsafe input handling. Using a Capture-the-Flag (CTF) setup, it tests CUAs' capacity to identify and exploit these weaknesses while navigating complex web interfaces. Evaluation of state-of-the-art CUAs reveals concerning trends: exploitation rates below 12% and low cybersecurity awareness. CUAs often fail at multi-step attack planning and misuse security tools. These results expose the current limitations of CUAs in web security contexts and highlight opportunities for developing more security-aware agents capable of effective vulnerability detection and exploitation.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GOAT: A Training Framework for Goal-Oriented Agent with Tools</title>
<link>https://arxiv.org/abs/2510.12218</link>
<guid>https://arxiv.org/abs/2510.12218</guid>
<content:encoded><![CDATA[
arXiv:2510.12218v1 Announce Type: new 
Abstract: Large language models (LLMs) have recently been extended beyond traditional text generation to serve as interactive agents capable of using external tools based on user intent. However, current LLM agents still show limited ability to handle goal-oriented queries, which require decomposing a high-level objective into multiple interdependent API calls with correct planning and execution. Current approaches mainly rely on zero-shot evaluation due to the absence of training data. While proprietary closed-source models such as GPT-4 demonstrate strong reasoning abilities, smaller open-source models struggle to perform complex tool use effectively. Thus, we propose a novel training framework GOAT, which enables fine-tuning of LLM agents in a human annotation-free setting. GOAT automatically constructs synthetic datasets of goal-oriented API execution tasks directly from given API documents, equipping models with the ability to reason over interdependent calls and generate coherent responses. Through extensive experiments, we show that GOAT-trained agents achieve state-of-the-art performance across multiple existing goal-oriented benchmarks. In addition, we introduce GOATBench, a new goal-oriented API execution benchmark, and demonstrate that agents trained with GOAT also excel in this setting. These results highlight GOAT as a practical path toward building robust open-source LLM agents capable of complex reasoning and tool use.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedKGEval: A Knowledge Graph-Based Multi-Turn Evaluation Framework for Open-Ended Patient Interactions with Clinical LLMs</title>
<link>https://arxiv.org/abs/2510.12224</link>
<guid>https://arxiv.org/abs/2510.12224</guid>
<content:encoded><![CDATA[
arXiv:2510.12224v1 Announce Type: new 
Abstract: The reliable evaluation of large language models (LLMs) in medical applications remains an open challenge, particularly in capturing the complexity of multi-turn doctor-patient interactions that unfold in real clinical environments. Existing evaluation methods typically rely on post hoc review of full conversation transcripts, thereby neglecting the dynamic, context-sensitive nature of medical dialogues and the evolving informational needs of patients. In this work, we present MedKGEval, a novel multi-turn evaluation framework for clinical LLMs grounded in structured medical knowledge. Our approach introduces three key contributions: (1) a knowledge graph-driven patient simulation mechanism, where a dedicated control module retrieves relevant medical facts from a curated knowledge graph, thereby endowing the patient agent with human-like and realistic conversational behavior. This knowledge graph is constructed by integrating open-source resources with additional triples extracted from expert-annotated datasets; (2) an in-situ, turn-level evaluation framework, where each model response is assessed by a Judge Agent for clinical appropriateness, factual correctness, and safety as the dialogue progresses using a suite of fine-grained, task-specific metrics; (3) a comprehensive multi-turn benchmark of eight state-of-the-art LLMs, demonstrating MedKGEval's ability to identify subtle behavioral flaws and safety risks that are often overlooked by conventional evaluation pipelines. Although initially designed for Chinese and English medical applications, our framework can be readily extended to additional languages by switching the input knowledge graphs, ensuring seamless bilingual support and domain-specific applicability.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Models for Reinforcement Learning: Foundations, Taxonomy, and Development</title>
<link>https://arxiv.org/abs/2510.12253</link>
<guid>https://arxiv.org/abs/2510.12253</guid>
<content:encoded><![CDATA[
arXiv:2510.12253v1 Announce Type: new 
Abstract: Diffusion Models (DMs), as a leading class of generative models, offer key advantages for reinforcement learning (RL), including multi-modal expressiveness, stable training, and trajectory-level planning. This survey delivers a comprehensive and up-to-date synthesis of diffusion-based RL. We first provide an overview of RL, highlighting its challenges, and then introduce the fundamental concepts of DMs, investigating how they are integrated into RL frameworks to address key challenges in this research field. We establish a dual-axis taxonomy that organizes the field along two orthogonal dimensions: a function-oriented taxonomy that clarifies the roles DMs play within the RL pipeline, and a technique-oriented taxonomy that situates implementations across online versus offline learning regimes. We also provide a comprehensive examination of this progression from single-agent to multi-agent domains, thereby forming several frameworks for DM-RL integration and highlighting their practical utility. Furthermore, we outline several categories of successful applications of diffusion-based RL across diverse domains, discuss open research issues of current methodologies, and highlight key directions for future research to advance the field. Finally, we summarize the survey to identify promising future development directions. We are actively maintaining a GitHub repository (https://github.com/ChangfuXu/D4RL-FTD) for papers and other related resources to apply DMs for RL.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\mathbf{T^3}$: Reducing Belief Deviation in Reinforcement Learning for Active Reasoning</title>
<link>https://arxiv.org/abs/2510.12264</link>
<guid>https://arxiv.org/abs/2510.12264</guid>
<content:encoded><![CDATA[
arXiv:2510.12264v1 Announce Type: new 
Abstract: Active reasoning requires large language models (LLMs) to interact with external sources and strategically gather information to solve problems. Central to this process is belief tracking: maintaining a coherent understanding of the problem state and the missing information toward the solution. However, due to limited reasoning capabilities, LLM-based agents often suffer from belief deviation: they struggle to correctly model beliefs, lose track of problem states, and fall into uninformative or repetitive actions. Once this happens, errors compound and reinforcement learning (RL) training fails to properly credit the crucial exploratory steps. To address this issue, we propose to track the deviation of model beliefs and develop $\mathbf{T^3}$, a simple yet effective method that detects excessive belief deviation and truncates trajectories during training to remove uninformative tails. By preserving credit for informative prefixes, $\mathbf{T^3}$ systematically improves policy optimization. Across 5 challenging tasks, $\mathbf{T^3}$ consistently enhances training stability, token efficiency, and final performance, achieving up to 30% gains while cutting rollout tokens by roughly 25%. These results highlight belief control as a key principle for developing robust and generalizable LLM-based active reasoners.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterogeneous RBCs via deep multi-agent reinforcement learning</title>
<link>https://arxiv.org/abs/2510.12272</link>
<guid>https://arxiv.org/abs/2510.12272</guid>
<content:encoded><![CDATA[
arXiv:2510.12272v1 Announce Type: new 
Abstract: Current macroeconomic models with agent heterogeneity can be broadly divided into two main groups. Heterogeneous-agent general equilibrium (GE) models, such as those based on Heterogeneous Agents New Keynesian (HANK) or Krusell-Smith (KS) approaches, rely on GE and 'rational expectations', somewhat unrealistic assumptions that make the models very computationally cumbersome, which in turn limits the amount of heterogeneity that can be modelled. In contrast, agent-based models (ABMs) can flexibly encompass a large number of arbitrarily heterogeneous agents, but typically require the specification of explicit behavioural rules, which can lead to a lengthy trial-and-error model-development process. To address these limitations, we introduce MARL-BC, a framework that integrates deep multi-agent reinforcement learning (MARL) with Real Business Cycle (RBC) models. We demonstrate that MARL-BC can: (1) recover textbook RBC results when using a single agent; (2) recover the results of the mean-field KS model using a large number of identical agents; and (3) effectively simulate rich heterogeneity among agents, a hard task for traditional GE approaches. Our framework can be thought of as an ABM if used with a variety of heterogeneous interacting agents, and can reproduce GE results in limit cases. As such, it is a step towards a synthesis of these often opposed modelling paradigms.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Action Self-Improvement for Neural Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2510.12273</link>
<guid>https://arxiv.org/abs/2510.12273</guid>
<content:encoded><![CDATA[
arXiv:2510.12273v1 Announce Type: new 
Abstract: Self-improvement has emerged as a state-of-the-art paradigm in Neural Combinatorial Optimization (NCO), where models iteratively refine their policies by generating and imitating high-quality solutions. Despite strong empirical performance, existing methods face key limitations. Training is computationally expensive, as policy updates require sampling numerous candidate solutions per instance to extract a single expert trajectory. More fundamentally, these approaches fail to exploit the structure of combinatorial problems involving the coordination of multiple agents, such as vehicles in min-max routing or machines in scheduling. By supervising on single-action trajectories, they fail to exploit agent-permutation symmetries, where distinct sequences of actions yield identical solutions, hindering generalization and the ability to learn coordinated behavior.
  We address these challenges by extending self-improvement to operate over joint multi-agent actions. Our model architecture predicts complete agent-task assignments jointly at each decision step. To explicitly leverage symmetries, we employ a set-prediction loss, which supervises the policy on multiple expert assignments for any given state. This approach enhances sample efficiency and the model's ability to learn coordinated behavior. Furthermore, by generating multi-agent actions in parallel, it drastically accelerates the solution generation phase of the self-improvement loop. Empirically, we validate our method on several combinatorial problems, demonstrating consistent improvements in the quality of the final solution and a reduced generation latency compared to standard self-improvement.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-REVal: Can We Trust LLM Reviewers Yet?</title>
<link>https://arxiv.org/abs/2510.12367</link>
<guid>https://arxiv.org/abs/2510.12367</guid>
<content:encoded><![CDATA[
arXiv:2510.12367v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) has inspired researchers to integrate them extensively into the academic workflow, potentially reshaping how research is practiced and reviewed. While previous studies highlight the potential of LLMs in supporting research and peer review, their dual roles in the academic workflow and the complex interplay between research and review bring new risks that remain largely underexplored. In this study, we focus on how the deep integration of LLMs into both peer-review and research processes may influence scholarly fairness, examining the potential risks of using LLMs as reviewers by simulation. This simulation incorporates a research agent, which generates papers and revises, alongside a review agent, which assesses the submissions. Based on the simulation results, we conduct human annotations and identify pronounced misalignment between LLM-based reviews and human judgments: (1) LLM reviewers systematically inflate scores for LLM-authored papers, assigning them markedly higher scores than human-authored ones; (2) LLM reviewers persistently underrate human-authored papers with critical statements (e.g., risk, fairness), even after multiple revisions. Our analysis reveals that these stem from two primary biases in LLM reviewers: a linguistic feature bias favoring LLM-generated writing styles, and an aversion toward critical statements. These results highlight the risks and equity concerns posed to human authors and academic research if LLMs are deployed in the peer review cycle without adequate caution. On the other hand, revisions guided by LLM reviews yield quality gains in both LLM-based and human evaluations, illustrating the potential of the LLMs-as-reviewers for early-stage researchers and enhancing low-quality papers.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>