<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs</link>

<item>
<title>THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2509.13761</link>
<guid>https://arxiv.org/abs/2509.13761</guid>
<content:encoded><![CDATA[

arXiv:2509.13761v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have made remarkable progress in mathematical reasoning, but still continue to struggle with high-precision tasks like numerical computation and formal symbolic manipulation. Integrating external tools has emerged as a promising approach to bridge this gap. Despite recent advances, existing methods struggle with three key challenges: constructing tool-integrated reasoning data, performing fine-grained optimization, and enhancing inference. To overcome these limitations, we propose THOR (Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen, a multi-agent actor-critic-based pipeline for constructing high-quality datasets of tool-integrated reasoning paths, aligning with the policy and generalizing well across diverse models. Second, to perform fine-grained hierarchical optimization, we introduce an RL strategy that jointly optimizes for both episode-level problem solving and step-level code generation. This is motivated by our key insight that the success of an intermediate tool call is a strong predictor of the final answer's correctness. Finally, THOR incorporates a self-correction mechanism that leverages immediate tool feedback to dynamically revise erroneous reasoning paths during inference. Our approach demonstrates strong generalization across diverse models, performing effectively in both reasoning and non-reasoning models. It further achieves state-of-the-art performance for models of a similar scale on multiple mathematical benchmarks, while also delivering consistent improvements on code benchmarks. Our code will be publicly available at https://github.com/JingMog/THOR.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>Agentic-AI Healthcare: Multilingual, Privacy-First Framework with MCP Agents</title>
<link>https://arxiv.org/abs/2510.02325</link>
<guid>https://arxiv.org/abs/2510.02325</guid>
<content:encoded><![CDATA[
arXiv:2510.02325v1 Announce Type: new 
Abstract: This paper introduces Agentic-AI Healthcare, a privacy-aware, multilingual, and explainable research prototype developed as a single-investigator project. The system leverages the emerging Model Context Protocol (MCP) to orchestrate multiple intelligent agents for patient interaction, including symptom checking, medication suggestions, and appointment scheduling. The platform integrates a dedicated Privacy and Compliance Layer that applies role-based access control (RBAC), AES-GCM field-level encryption, and tamper-evident audit logging, aligning with major healthcare data protection standards such as HIPAA (US), PIPEDA (Canada), and PHIPA (Ontario). Example use cases demonstrate multilingual patient-doctor interaction (English, French, Arabic) and transparent diagnostic reasoning powered by large language models. As an applied AI contribution, this work highlights the feasibility of combining agentic orchestration, multilingual accessibility, and compliance-aware architecture in healthcare applications. This platform is presented as a research prototype and is not a certified medical device.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMANDA: Agentic Medical Knowledge Augmentation for Data-Efficient Medical Visual Question Answering</title>
<link>https://arxiv.org/abs/2510.02328</link>
<guid>https://arxiv.org/abs/2510.02328</guid>
<content:encoded><![CDATA[
arXiv:2510.02328v1 Announce Type: new 
Abstract: Medical Multimodal Large Language Models (Med-MLLMs) have shown great promise in medical visual question answering (Med-VQA). However, when deployed in low-resource settings where abundant labeled data are unavailable, existing Med-MLLMs commonly fail due to their medical reasoning capability bottlenecks: (i) the intrinsic reasoning bottleneck that ignores the details from the medical image; (ii) the extrinsic reasoning bottleneck that fails to incorporate specialized medical knowledge. To address those limitations, we propose AMANDA, a training-free agentic framework that performs medical knowledge augmentation via LLM agents. Specifically, our intrinsic medical knowledge augmentation focuses on coarse-to-fine question decomposition for comprehensive diagnosis, while extrinsic medical knowledge augmentation grounds the reasoning process via biomedical knowledge graph retrieval. Extensive experiments across eight Med-VQA benchmarks demonstrate substantial improvements in both zero-shot and few-shot Med-VQA settings. The code is available at https://github.com/REAL-Lab-NU/AMANDA.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\texttt{BluePrint}$: A Social Media User Dataset for LLM Persona Evaluation and Training</title>
<link>https://arxiv.org/abs/2510.02343</link>
<guid>https://arxiv.org/abs/2510.02343</guid>
<content:encoded><![CDATA[
arXiv:2510.02343v1 Announce Type: new 
Abstract: Large language models (LLMs) offer promising capabilities for simulating social media dynamics at scale, enabling studies that would be ethically or logistically challenging with human subjects. However, the field lacks standardized data resources for fine-tuning and evaluating LLMs as realistic social media agents. We address this gap by introducing SIMPACT, the SIMulation-oriented Persona and Action Capture Toolkit, a privacy respecting framework for constructing behaviorally-grounded social media datasets suitable for training agent models. We formulate next-action prediction as a task for training and evaluating LLM-based agents and introduce metrics at both the cluster and population levels to assess behavioral fidelity and stylistic realism. As a concrete implementation, we release BluePrint, a large-scale dataset built from public Bluesky data focused on political discourse. BluePrint clusters anonymized users into personas of aggregated behaviours, capturing authentic engagement patterns while safeguarding privacy through pseudonymization and removal of personally identifiable information. The dataset includes a sizable action set of 12 social media interaction types (likes, replies, reposts, etc.), each instance tied to the posting activity preceding it. This supports the development of agents that use context-dependence, not only in the language, but also in the interaction behaviours of social media to model social media users. By standardizing data and evaluation protocols, SIMPACT provides a foundation for advancing rigorous, ethically responsible social media simulations. BluePrint serves as both an evaluation benchmark for political discourse modeling and a template for building domain specific datasets to study challenges such as misinformation and polarization.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Physical-World Privacy Awareness of Large Language Models: An Evaluation Benchmark</title>
<link>https://arxiv.org/abs/2510.02356</link>
<guid>https://arxiv.org/abs/2510.02356</guid>
<content:encoded><![CDATA[
arXiv:2510.02356v1 Announce Type: new 
Abstract: The deployment of Large Language Models (LLMs) in embodied agents creates an urgent need to measure their privacy awareness in the physical world. Existing evaluation methods, however, are confined to natural language based scenarios. To bridge this gap, we introduce EAPrivacy, a comprehensive evaluation benchmark designed to quantify the physical-world privacy awareness of LLM-powered agents. EAPrivacy utilizes procedurally generated scenarios across four tiers to test an agent's ability to handle sensitive objects, adapt to changing environments, balance task execution with privacy constraints, and resolve conflicts with social norms. Our measurements reveal a critical deficit in current models. The top-performing model, Gemini 2.5 Pro, achieved only 59\% accuracy in scenarios involving changing physical environments. Furthermore, when a task was accompanied by a privacy request, models prioritized completion over the constraint in up to 86\% of cases. In high-stakes situations pitting privacy against critical social norms, leading models like GPT-4o and Claude-3.5-haiku disregarded the social norm over 15\% of the time. These findings, demonstrated by our benchmark, underscore a fundamental misalignment in LLMs regarding physically grounded privacy and establish the need for more robust, physically-aware alignment.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emission-GPT: A domain-specific language model agent for knowledge retrieval, emission inventory and data analysis</title>
<link>https://arxiv.org/abs/2510.02359</link>
<guid>https://arxiv.org/abs/2510.02359</guid>
<content:encoded><![CDATA[
arXiv:2510.02359v1 Announce Type: new 
Abstract: Improving air quality and addressing climate change relies on accurate understanding and analysis of air pollutant and greenhouse gas emissions. However, emission-related knowledge is often fragmented and highly specialized, while existing methods for accessing and compiling emissions data remain inefficient. These issues hinder the ability of non-experts to interpret emissions information, posing challenges to research and management. To address this, we present Emission-GPT, a knowledge-enhanced large language model agent tailored for the atmospheric emissions domain. Built on a curated knowledge base of over 10,000 documents (including standards, reports, guidebooks, and peer-reviewed literature), Emission-GPT integrates prompt engineering and question completion to support accurate domain-specific question answering. Emission-GPT also enables users to interactively analyze emissions data via natural language, such as querying and visualizing inventories, analyzing source contributions, and recommending emission factors for user-defined scenarios. A case study in Guangdong Province demonstrates that Emission-GPT can extract key insights--such as point source distributions and sectoral trends--directly from raw data with simple prompts. Its modular and extensible architecture facilitates automation of traditionally manual workflows, positioning Emission-GPT as a foundational tool for next-generation emission inventory development and scenario-based assessment.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spiral of Silence in Large Language Model Agents</title>
<link>https://arxiv.org/abs/2510.02360</link>
<guid>https://arxiv.org/abs/2510.02360</guid>
<content:encoded><![CDATA[
arXiv:2510.02360v1 Announce Type: new 
Abstract: The Spiral of Silence (SoS) theory holds that individuals with minority views often refrain from speaking out for fear of social isolation, enabling majority positions to dominate public discourse. When the 'agents' are large language models (LLMs), however, the classical psychological explanation is not directly applicable, since SoS was developed for human societies. This raises a central question: can SoS-like dynamics nevertheless emerge from purely statistical language generation in LLM collectives? We propose an evaluation framework for examining SoS in LLM agents. Specifically, we consider four controlled conditions that systematically vary the availability of 'History' and 'Persona' signals. Opinion dynamics are assessed using trend tests such as Mann-Kendall and Spearman's rank, along with concentration measures including kurtosis and interquartile range. Experiments across open-source and closed-source models show that history and persona together produce strong majority dominance and replicate SoS patterns; history signals alone induce strong anchoring; and persona signals alone foster diverse but uncorrelated opinions, indicating that without historical anchoring, SoS dynamics cannot emerge. The work bridges computational sociology and responsible AI design, highlighting the need to monitor and mitigate emergent conformity in LLM-agent systems.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Manuals and Tasks: Instance-Level Context Learning for LLM Agents</title>
<link>https://arxiv.org/abs/2510.02369</link>
<guid>https://arxiv.org/abs/2510.02369</guid>
<content:encoded><![CDATA[
arXiv:2510.02369v1 Announce Type: new 
Abstract: Large language model (LLM) agents typically receive two kinds of context: (i) environment-level manuals that define interaction interfaces and global rules, and (ii) task-level guidance or demonstrations tied to specific goals. In this work, we identify a crucial but overlooked third type of context, instance-level context, which consists of verifiable and reusable facts tied to a specific environment instance, such as object locations, crafting recipes, and local rules. We argue that the absence of instance-level context is a common source of failure for LLM agents in complex tasks, as success often depends not only on reasoning over global rules or task prompts but also on making decisions based on precise and persistent facts. Acquiring such context requires more than memorization: the challenge lies in efficiently exploring, validating, and formatting these facts under tight interaction budgets. We formalize this problem as Instance-Level Context Learning (ILCL) and introduce our task-agnostic method to solve it. Our method performs a guided exploration, using a compact TODO forest to intelligently prioritize its next actions and a lightweight plan-act-extract loop to execute them. This process automatically produces a high-precision context document that is reusable across many downstream tasks and agents, thereby amortizing the initial exploration cost. Experiments across TextWorld, ALFWorld, and Crafter demonstrate consistent gains in both success and efficiency: for instance, ReAct's mean success rate in TextWorld rises from 37% to 95%, while IGE improves from 81% to 95%. By transforming one-off exploration into persistent, reusable knowledge, our method complements existing contexts to enable more reliable and efficient LLM agents.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A-MemGuard: A Proactive Defense Framework for LLM-Based Agent Memory</title>
<link>https://arxiv.org/abs/2510.02373</link>
<guid>https://arxiv.org/abs/2510.02373</guid>
<content:encoded><![CDATA[
arXiv:2510.02373v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents use memory to learn from past interactions, enabling autonomous planning and decision-making in complex environments. However, this reliance on memory introduces a critical security risk: an adversary can inject seemingly harmless records into an agent's memory to manipulate its future behavior. This vulnerability is characterized by two core aspects: First, the malicious effect of injected records is only activated within a specific context, making them hard to detect when individual memory entries are audited in isolation. Second, once triggered, the manipulation can initiate a self-reinforcing error cycle: the corrupted outcome is stored as precedent, which not only amplifies the initial error but also progressively lowers the threshold for similar attacks in the future. To address these challenges, we introduce A-MemGuard (Agent-Memory Guard), the first proactive defense framework for LLM agent memory. The core idea of our work is the insight that memory itself must become both self-checking and self-correcting. Without modifying the agent's core architecture, A-MemGuard combines two mechanisms: (1) consensus-based validation, which detects anomalies by comparing reasoning paths derived from multiple related memories and (2) a dual-memory structure, where detected failures are distilled into ``lessons'' stored separately and consulted before future actions, breaking error cycles and enabling adaptation. Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost. This work shifts LLM memory security from static filtering to a proactive, experience-driven model where defenses strengthen over time. Our code is available in https://github.com/TangciuYueng/AMemGuard
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid CAPTCHA Combining Generative AI with Keystroke Dynamics for Enhanced Bot Detection</title>
<link>https://arxiv.org/abs/2510.02374</link>
<guid>https://arxiv.org/abs/2510.02374</guid>
<content:encoded><![CDATA[
arXiv:2510.02374v1 Announce Type: new 
Abstract: Completely Automated Public Turing tests to tell Computers and Humans Apart (CAPTCHAs) are a foundational component of web security, yet traditional implementations suffer from a trade-off between usability and resilience against AI-powered bots. This paper introduces a novel hybrid CAPTCHA system that synergizes the cognitive challenges posed by Large Language Models (LLMs) with the behavioral biometric analysis of keystroke dynamics. Our approach generates dynamic, unpredictable questions that are trivial for humans but non-trivial for automated agents, while simultaneously analyzing the user's typing rhythm to distinguish human patterns from robotic input. We present the system's architecture, formalize the feature extraction methodology for keystroke analysis, and report on an experimental evaluation. The results indicate that our dual-layered approach achieves a high degree of accuracy in bot detection, successfully thwarting both paste-based and script-based simulation attacks, while maintaining a high usability score among human participants. This work demonstrates the potential of combining cognitive and behavioral tests to create a new generation of more secure and user-friendly CAPTCHAs.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CWM: An Open-Weights LLM for Research on Code Generation with World Models</title>
<link>https://arxiv.org/abs/2510.02387</link>
<guid>https://arxiv.org/abs/2510.02387</guid>
<content:encoded><![CDATA[
arXiv:2510.02387v1 Announce Type: new 
Abstract: We release Code World Model (CWM), a 32-billion-parameter open-weights LLM, to advance research on code generation with world models. To improve code understanding beyond what can be learned from training on static code alone, we mid-train CWM on a large amount of observation-action trajectories from Python interpreter and agentic Docker environments, and perform extensive multi-task reasoning RL in verifiable coding, math, and multi-turn software engineering environments. With CWM, we provide a strong testbed for researchers to explore the opportunities world modeling affords for improving code generation with reasoning and planning in computational environments. We present first steps of how world models can benefit agentic coding, enable step-by-step simulation of Python code execution, and show early results of how reasoning can benefit from the latter. CWM is a dense, decoder-only LLM trained with a context size of up to 131k tokens. Independent of its world modeling capabilities, CWM offers strong performance on general coding and math tasks: it reaches pass@1 scores of 65.8% on SWE-bench Verified (with test-time scaling), 68.6% on LiveCodeBench, 96.6% on Math-500, and 76.0% on AIME 2024. To support further research on code world modeling, we release model checkpoints after mid-training, SFT, and RL.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2510.02388</link>
<guid>https://arxiv.org/abs/2510.02388</guid>
<content:encoded><![CDATA[
arXiv:2510.02388v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown remarkable performance on general Question Answering (QA), yet they often struggle in domain-specific scenarios where accurate and up-to-date information is required. Retrieval-Augmented Generation (RAG) addresses this limitation by enriching LLMs with external knowledge, but existing systems primarily rely on unstructured documents, while largely overlooking relational databases, which provide precise, timely, and efficiently queryable factual information, serving as indispensable infrastructure in domains such as finance, healthcare, and scientific research. Motivated by this gap, we conduct a systematic analysis that reveals three central observations: (i) databases and documents offer complementary strengths across queries, (ii) naively combining both sources introduces noise and cost without consistent accuracy gains, and (iii) selecting the most suitable source for each query is crucial to balance effectiveness and efficiency. We further observe that query types show consistent regularities in their alignment with retrieval paths, suggesting that routing decisions can be effectively guided by systematic rules that capture these patterns. Building on these insights, we propose a rule-driven routing framework. A routing agent scores candidate augmentation paths based on explicit rules and selects the most suitable one; a rule-making expert agent refines the rules over time using QA feedback to maintain adaptability; and a path-level meta-cache reuses past routing decisions for semantically similar queries to reduce latency and cost. Experiments on three QA benchmarks demonstrate that our framework consistently outperforms static strategies and learned routing baselines, achieving higher accuracy while maintaining moderate computational cost.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrowserArena: Evaluating LLM Agents on Real-World Web Navigation Tasks</title>
<link>https://arxiv.org/abs/2510.02418</link>
<guid>https://arxiv.org/abs/2510.02418</guid>
<content:encoded><![CDATA[
arXiv:2510.02418v1 Announce Type: new 
Abstract: LLM web agents now browse and take actions on the open web, yet current agent evaluations are constrained to sandboxed environments or artificial tasks. We introduce BrowserArena, a live open-web agent evaluation platform that collects user-submitted tasks, runs Arena-style head-to-head comparisons, and uses step-level human feedback to surface failure modes. Collecting and analyzing step-level annotations on the agent traces, we identify three consistent failure modes: captcha resolution, pop-up banner removal, and direct navigation to URLs. By constructing targeted datasets to further study these tasks, we discover variations in how different language models navigate these failure modes. We find, for example, that o4-mini deploys a wider variety of strategies to circumvent captcha resolution than other models and DeepSeek-R1 consistently misleads users about captcha resolution. Our findings surface both the diversity and brittleness of current web agents. More broadly, our benchmarking methodology provides an approach to evaluating and understanding web agent failure modes at scale.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLARITY: Clinical Assistant for Routing, Inference, and Triage</title>
<link>https://arxiv.org/abs/2510.02463</link>
<guid>https://arxiv.org/abs/2510.02463</guid>
<content:encoded><![CDATA[
arXiv:2510.02463v1 Announce Type: new 
Abstract: We present CLARITY (Clinical Assistant for Routing, Inference, and Triage), an AI-driven platform designed to facilitate patient-to-specialist routing, clinical consultations, and severity assessment of patients' conditions. Its hybrid architecture combines a Finite State Machine (FSM) for structured dialogue flows with collaborative agents that employ Large Language Model (LLM) to analyze symptoms and prioritize referrals to appropriate specialists. Built on a modular microservices framework, CLARITY ensures safe, efficient, and robust performance, flexible and readily scalable to meet the demands of existing workflows and IT solutions in healthcare.
  We report integration of our clinical assistant into a large-scale nation-wide inter-hospital IT platform, with over 55,000 content-rich user dialogues completed within the two months of deployment, 2,500 of which were expert-annotated for a consequent validation. The validation results show that CLARITY surpasses human-level performance in terms of the first-attempt routing precision, naturally requiring up to 3 times shorter duration of the consultation than with a human.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2510.02469</link>
<guid>https://arxiv.org/abs/2510.02469</guid>
<content:encoded><![CDATA[
arXiv:2510.02469v1 Announce Type: new 
Abstract: Driving scene manipulation with sensor data is emerging as a promising alternative to traditional virtual driving simulators. However, existing frameworks struggle to generate realistic scenarios efficiently due to limited editing capabilities. To address these challenges, we present SIMSplat, a predictive driving scene editor with language-aligned Gaussian splatting. As a language-controlled editor, SIMSplat enables intuitive manipulation using natural language prompts. By aligning language with Gaussian-reconstructed scenes, it further supports direct querying of road objects, allowing precise and flexible editing. Our method provides detailed object-level editing, including adding new objects and modifying the trajectories of both vehicles and pedestrians, while also incorporating predictive path refinement through multi-agent motion prediction to generate realistic interactions among all agents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's extensive editing capabilities and adaptability across a wide range of scenarios. Project page: https://sungyeonparkk.github.io/simsplat/
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Pixels to Factors: Learning Independently Controllable State Variables for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.02484</link>
<guid>https://arxiv.org/abs/2510.02484</guid>
<content:encoded><![CDATA[
arXiv:2510.02484v1 Announce Type: new 
Abstract: Algorithms that exploit factored Markov decision processes are far more sample-efficient than factor-agnostic methods, yet they assume a factored representation is known a priori -- a requirement that breaks down when the agent sees only high-dimensional observations. Conversely, deep reinforcement learning handles such inputs but cannot benefit from factored structure. We address this representation problem with Action-Controllable Factorization (ACF), a contrastive learning approach that uncovers independently controllable latent variables -- state components each action can influence separately. ACF leverages sparsity: actions typically affect only a subset of variables, while the rest evolve under the environment's dynamics, yielding informative data for contrastive training. ACF recovers the ground truth controllable factors directly from pixel observations on three benchmarks with known factored structure -- Taxi, FourRooms, and MiniGrid-DoorKey -- consistently outperforming baseline disentanglement algorithms.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolTweak: An Attack on Tool Selection in LLM-based Agents</title>
<link>https://arxiv.org/abs/2510.02554</link>
<guid>https://arxiv.org/abs/2510.02554</guid>
<content:encoded><![CDATA[
arXiv:2510.02554v1 Announce Type: new 
Abstract: As LLMs increasingly power agents that interact with external tools, tool use has become an essential mechanism for extending their capabilities. These agents typically select tools from growing databases or marketplaces to solve user tasks, creating implicit competition among tool providers and developers for visibility and usage. In this paper, we show that this selection process harbors a critical vulnerability: by iteratively manipulating tool names and descriptions, adversaries can systematically bias agents toward selecting specific tools, gaining unfair advantage over equally capable alternatives. We present ToolTweak, a lightweight automatic attack that increases selection rates from a baseline of around 20% to as high as 81%, with strong transferability between open-source and closed-source models. Beyond individual tools, we show that such attacks cause distributional shifts in tool usage, revealing risks to fairness, competition, and security in emerging tool ecosystems. To mitigate these risks, we evaluate two defenses: paraphrasing and perplexity filtering, which reduce bias and lead agents to select functionally similar tools more equally. All code will be open-sourced upon acceptance.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Orchestrating Human-AI Teams: The Manager Agent as a Unifying Research Challenge</title>
<link>https://arxiv.org/abs/2510.02557</link>
<guid>https://arxiv.org/abs/2510.02557</guid>
<content:encoded><![CDATA[
arXiv:2510.02557v1 Announce Type: new 
Abstract: While agentic AI has advanced in automating individual tasks, managing complex multi-agent workflows remains a challenging problem. This paper presents a research vision for autonomous agentic systems that orchestrate collaboration within dynamic human-AI teams. We propose the Autonomous Manager Agent as a core challenge: an agent that decomposes complex goals into task graphs, allocates tasks to human and AI workers, monitors progress, adapts to changing conditions, and maintains transparent stakeholder communication. We formalize workflow management as a Partially Observable Stochastic Game and identify four foundational challenges: (1) compositional reasoning for hierarchical decomposition, (2) multi-objective optimization under shifting preferences, (3) coordination and planning in ad hoc teams, and (4) governance and compliance by design. To advance this agenda, we release MA-Gym, an open-source simulation and evaluation framework for multi-agent workflow orchestration. Evaluating GPT-5-based Manager Agents across 20 workflows, we find they struggle to jointly optimize for goal completion, constraint adherence, and workflow runtime - underscoring workflow management as a difficult open problem. We conclude with organizational and ethical implications of autonomous management systems.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Additive Manufacturing Alloy Discovery</title>
<link>https://arxiv.org/abs/2510.02567</link>
<guid>https://arxiv.org/abs/2510.02567</guid>
<content:encoded><![CDATA[
arXiv:2510.02567v1 Announce Type: new 
Abstract: Agentic systems enable the intelligent use of research tooling, augmenting a researcher's ability to investigate and propose novel solutions to existing problems. Within Additive Manufacturing (AM), alloy discovery remains a complex challenge, often requiring expertise in the various domains of materials science, thermodynamic simulations, and experimental analysis. Large Language Model (LLM) enabled agents can facilitate this endeavor by utilizing their extensive knowledge base to dispatch tool calls via Model Context Protocol (MCP) to perform actions such as Thermo-Calc property diagram calculations and lack of fusion process map generation. In addition, the multi-agent system developed in this work is able to effectively reason through complex user prompts and provide analysis on the printability of proposed alloys. These agents can dynamically adjust their task trajectory to the outcomes of tool call results, effectively enabling autonomous decision-making in practical environments. This work aims to utilize LLM enabled agents to automate and accelerate the task of alloy discovery within the field of additive manufacturing and showcase the benefits of adopting this multi-agent system.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Benchmark Study of Deep Reinforcement Learning Algorithms for the Container Stowage Planning Problem</title>
<link>https://arxiv.org/abs/2510.02589</link>
<guid>https://arxiv.org/abs/2510.02589</guid>
<content:encoded><![CDATA[
arXiv:2510.02589v1 Announce Type: new 
Abstract: Container stowage planning (CSPP) is a critical component of maritime transportation and terminal operations, directly affecting supply chain efficiency. Owing to its complexity, CSPP has traditionally relied on human expertise. While reinforcement learning (RL) has recently been applied to CSPP, systematic benchmark comparisons across different algorithms remain limited. To address this gap, we develop a Gym environment that captures the fundamental features of CSPP and extend it to include crane scheduling in both multi-agent and single-agent formulations. Within this framework, we evaluate five RL algorithms: DQN, QR-DQN, A2C, PPO, and TRPO under multiple scenarios of varying complexity. The results reveal distinct performance gaps with increasing complexity, underscoring the importance of algorithm choice and problem formulation for CSPP. Overall, this paper benchmarks multiple RL methods for CSPP while providing a reusable Gym environment with crane scheduling, thus offering a foundation for future research and practical deployment in maritime logistics.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Modal Imbalance in Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2510.02608</link>
<guid>https://arxiv.org/abs/2510.02608</guid>
<content:encoded><![CDATA[
arXiv:2510.02608v1 Announce Type: new 
Abstract: Foundation models (FMs) deployed in real-world tasks such as computer-use agents must integrate diverse modalities. How good are FMs at performing joint reasoning, simultaneously reasoning over multiple modalities, especially when the modalities interact and relate to each other to form cross-modal context? To better understand this problem, we study FMs on cross-modal conflicts: scenarios where conflicting evidence is presented across modalities. This allows us to examine whether FMs prioritize one modality over another or reason jointly to reconcile the conflict. Our experiments reveal that FMs can recognize conflicts in unimodal contexts, composed of a single modality, 90% of the time, but the ratio falls as low as 3% when evidence is split across modalities -- similar observations hold in cross-lingual contexts, composed of multiple languages. We trace this failure to cross-modal attention imbalance, showing that FMs exhibit extreme asymmetry in attention scores, disproportionately prioritizing certain modalities. We show that cross-modal attention imbalance does not go away by simply scaling up multimodal or multilingual datasets blindly, since they lack training examples that explicitly require cross-modal reasoning. We demonstrate that even a simple and scalable method of explicitly combining multiple modalities within each training instance significantly reduces attention imbalance. Reduced attention imbalance directly translates to improved downstream performance on several vision-language benchmarks. Our findings underscore the importance of systematically addressing cross-modal contexts to build reliable foundation models.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RedCodeAgent: Automatic Red-teaming Agent against Diverse Code Agents</title>
<link>https://arxiv.org/abs/2510.02609</link>
<guid>https://arxiv.org/abs/2510.02609</guid>
<content:encoded><![CDATA[
arXiv:2510.02609v1 Announce Type: new 
Abstract: Code agents have gained widespread adoption due to their strong code generation capabilities and integration with code interpreters, enabling dynamic execution, debugging, and interactive programming capabilities. While these advancements have streamlined complex workflows, they have also introduced critical safety and security risks. Current static safety benchmarks and red-teaming tools are inadequate for identifying emerging real-world risky scenarios, as they fail to cover certain boundary conditions, such as the combined effects of different jailbreak tools. In this work, we propose RedCodeAgent, the first automated red-teaming agent designed to systematically uncover vulnerabilities in diverse code agents. With an adaptive memory module, RedCodeAgent can leverage existing jailbreak knowledge, dynamically select the most effective red-teaming tools and tool combinations in a tailored toolbox for a given input query, thus identifying vulnerabilities that might otherwise be overlooked. For reliable evaluation, we develop simulated sandbox environments to additionally evaluate the execution results of code agents, mitigating potential biases of LLM-based judges that only rely on static code. Through extensive evaluations across multiple state-of-the-art code agents, diverse risky scenarios, and various programming languages, RedCodeAgent consistently outperforms existing red-teaming methods, achieving higher attack success rates and lower rejection rates with high efficiency. We further validate RedCodeAgent on real-world code assistants, e.g., Cursor and Codeium, exposing previously unidentified security risks. By automating and optimizing red-teaming processes, RedCodeAgent enables scalable, adaptive, and effective safety assessments of code agents.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation</title>
<link>https://arxiv.org/abs/2510.02617</link>
<guid>https://arxiv.org/abs/2510.02617</guid>
<content:encoded><![CDATA[
arXiv:2510.02617v1 Announce Type: new 
Abstract: Diffusion models can synthesize realistic co-speech video from audio for various applications, such as video creation and virtual agents. However, existing diffusion-based methods are slow due to numerous denoising steps and costly attention mechanisms, preventing real-time deployment. In this work, we distill a many-step diffusion video model into a few-step student model. Unfortunately, directly applying recent diffusion distillation methods degrades video quality and falls short of real-time performance. To address these issues, our new video distillation method leverages input human pose conditioning for both attention and loss functions. We first propose using accurate correspondence between input human pose keypoints to guide attention to relevant regions, such as the speaker's face, hands, and upper body. This input-aware sparse attention reduces redundant computations and strengthens temporal correspondences of body parts, improving inference efficiency and motion coherence. To further enhance visual quality, we introduce an input-aware distillation loss that improves lip synchronization and hand motion realism. By integrating our input-aware sparse attention and distillation loss, our method achieves real-time performance with improved visual quality compared to recent audio-driven and input-driven methods. We also conduct extensive experiments showing the effectiveness of our algorithmic design choices.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Trajectory Generator for High-Density Traffic and Diverse Agent-Interaction Scenarios</title>
<link>https://arxiv.org/abs/2510.02627</link>
<guid>https://arxiv.org/abs/2510.02627</guid>
<content:encoded><![CDATA[
arXiv:2510.02627v1 Announce Type: new 
Abstract: Accurate trajectory prediction is fundamental to autonomous driving, as it underpins safe motion planning and collision avoidance in complex environments. However, existing benchmark datasets suffer from a pronounced long-tail distribution problem, with most samples drawn from low-density scenarios and simple straight-driving behaviors. This underrepresentation of high-density scenarios and safety critical maneuvers such as lane changes, overtaking and turning is an obstacle to model generalization and leads to overly optimistic evaluations. To address these challenges, we propose a novel trajectory generation framework that simultaneously enhances scenarios density and enriches behavioral diversity. Specifically, our approach converts continuous road environments into a structured grid representation that supports fine-grained path planning, explicit conflict detection, and multi-agent coordination. Built upon this representation, we introduce behavior-aware generation mechanisms that combine rule-based decision triggers with Frenet-based trajectory smoothing and dynamic feasibility constraints. This design allows us to synthesize realistic high-density scenarios and rare behaviors with complex interactions that are often missing in real data. Extensive experiments on the large-scale Argoverse 1 and Argoverse 2 datasets demonstrate that our method significantly improves both agent density and behavior diversity, while preserving motion realism and scenario-level safety. Our synthetic data also benefits downstream trajectory prediction models and enhances performance in challenging high-density scenarios.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Building Code Review: A Case Study</title>
<link>https://arxiv.org/abs/2510.02634</link>
<guid>https://arxiv.org/abs/2510.02634</guid>
<content:encoded><![CDATA[
arXiv:2510.02634v1 Announce Type: new 
Abstract: Building officials, particularly those in resource-constrained or rural jurisdictions, face labor-intensive, error-prone, and costly manual reviews of design documents as projects increase in size and complexity. The growing adoption of Building Information Modeling (BIM) and Large Language Models (LLMs) presents opportunities for automated code review (ACR) solutions. This study introduces a novel agent-driven framework that integrates BIM-based data extraction with automated verification using both retrieval-augmented generation (RAG) and Model Context Protocol (MCP) agent pipelines. The framework employs LLM-enabled agents to extract geometry, schedules, and system attributes from heterogeneous file types, which are then processed for building code checking through two complementary mechanisms: (1) direct API calls to the US Department of Energy COMcheck engine, providing deterministic and audit-ready outputs, and (2) RAG-based reasoning over rule provisions, enabling flexible interpretation where coverage is incomplete or ambiguous.
  The framework was evaluated through case demonstrations, including automated extraction of geometric attributes (such as surface area, tilt, and insulation values), parsing of operational schedules, and validation of lighting allowances under ASHRAE Standard 90.1-2022. Comparative performance tests across multiple LLMs showed that GPT-4o achieved the best balance of efficiency and stability, while smaller models exhibited inconsistencies or failures. Results confirm that MCP agent pipelines outperform RAG reasoning pipelines in rigor and reliability. This work advances ACR research by demonstrating a scalable, interoperable, and production-ready approach that bridges BIM with authoritative code review tools.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Gap: Linguistic Divergence and Adaptation Strategies in Human-LLM Assistant vs. Human-Human Interactions</title>
<link>https://arxiv.org/abs/2510.02645</link>
<guid>https://arxiv.org/abs/2510.02645</guid>
<content:encoded><![CDATA[
arXiv:2510.02645v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) are increasingly deployed in customer-facing applications, a critical yet underexplored question is how users communicate differently with LLM chatbots compared to human agent. In this study, we present empirical evidence that users adopt distinct communication styles when users interact with chatbots versus human agents. Our analysis reveals significant differences in grammatical fluency, politeness, and lexical diversity in user language between the two settings. These findings suggest that models trained exclusively on human-human interaction data may not adequately accommodate the communication style shift that occurs once an LLM chatbot is deployed. To enhance LLM robustness to post-launch communication style changes, we experimented with two strategies: (1) data augmentation during the post-training phase and (2) inference-time user message reformulation. Our results indicate that models trained on stylistically diverse datasets significantly outperform those trained exclusively on original or stylistically uniform datasets, while inference-time reformulation proved less effective. These insights help us to better adapt our models for improved LLM-user interaction experiences.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgenticRAG: Tool-Augmented Foundation Models for Zero-Shot Explainable Recommender Systems</title>
<link>https://arxiv.org/abs/2510.02668</link>
<guid>https://arxiv.org/abs/2510.02668</guid>
<content:encoded><![CDATA[
arXiv:2510.02668v1 Announce Type: new 
Abstract: Foundation models have revolutionized artificial intelligence, yet their application in recommender systems remains limited by reasoning opacity and knowledge constraints. This paper introduces AgenticRAG, a novel framework that combines tool-augmented foundation models with retrieval-augmented generation for zero-shot explainable recommendations. Our approach integrates external tool invocation, knowledge retrieval, and chain-of-thought reasoning to create autonomous recommendation agents capable of transparent decision-making without task-specific training. Experimental results on three real-world datasets demonstrate that AgenticRAG achieves consistent improvements over state-of-the-art baselines, with NDCG@10 improvements of 0.4\% on Amazon Electronics, 0.8\% on MovieLens-1M, and 1.6\% on Yelp datasets. The framework exhibits superior explainability while maintaining computational efficiency comparable to traditional methods.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoMaAS: Self-Evolving Multi-Agent Architecture Search for Large Language Models</title>
<link>https://arxiv.org/abs/2510.02669</link>
<guid>https://arxiv.org/abs/2510.02669</guid>
<content:encoded><![CDATA[
arXiv:2510.02669v1 Announce Type: new 
Abstract: Multi-agent systems powered by large language models have demonstrated remarkable capabilities across diverse domains, yet existing automated design approaches seek monolithic solutions that fail to adapt resource allocation based on query complexity and domain requirements. This paper introduces AutoMaAS, a self-evolving multi-agent architecture search framework that leverages neural architecture search principles to automatically discover optimal agent configurations through dynamic operator lifecycle management and automated machine learning techniques. Our approach incorporates four key innovations: (1) automatic operator generation, fusion, and elimination based on performance-cost analysis, (2) dynamic cost-aware optimization with real-time parameter adjustment, (3) online feedback integration for continuous architecture refinement, and (4) enhanced interpretability through decision tracing mechanisms. Extensive experiments across six benchmarks demonstrate that AutoMaAS achieves 1.0-7.1\% performance improvement while reducing inference costs by 3-5\% compared to state-of-the-art methods. The framework shows superior transferability across datasets and LLM backbones, establishing a new paradigm for automated multi-agent system design in the era of large language models.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARMs: Adaptive Red-Teaming Agent against Multimodal Models with Plug-and-Play Attacks</title>
<link>https://arxiv.org/abs/2510.02677</link>
<guid>https://arxiv.org/abs/2510.02677</guid>
<content:encoded><![CDATA[
arXiv:2510.02677v1 Announce Type: new 
Abstract: As vision-language models (VLMs) gain prominence, their multimodal interfaces also introduce new safety vulnerabilities, making the safety evaluation challenging and critical. Existing red-teaming efforts are either restricted to a narrow set of adversarial patterns or depend heavily on manual engineering, lacking scalable exploration of emerging real-world VLM vulnerabilities. To bridge this gap, we propose ARMs, an adaptive red-teaming agent that systematically conducts comprehensive risk assessments for VLMs. Given a target harmful behavior or risk definition, ARMs automatically optimizes diverse red-teaming strategies with reasoning-enhanced multi-step orchestration, to effectively elicit harmful outputs from target VLMs. We propose 11 novel multimodal attack strategies, covering diverse adversarial patterns of VLMs (e.g., reasoning hijacking, contextual cloaking), and integrate 17 red-teaming algorithms into ARMs via model context protocol (MCP). To balance the diversity and effectiveness of the attack, we design a layered memory with an epsilon-greedy attack exploration algorithm. Extensive experiments on instance- and policy-based benchmarks show that ARMs achieves SOTA attack success rates, exceeding baselines by an average of 52.1% and surpassing 90% on Claude-4-Sonnet. We show that the diversity of red-teaming instances generated by ARMs is significantly higher, revealing emerging vulnerabilities in VLMs. Leveraging ARMs, we construct ARMs-Bench, a large-scale multimodal safety dataset comprising over 30K red-teaming instances spanning 51 diverse risk categories, grounded in both real-world multimodal threats and regulatory risks. Safety fine-tuning with ARMs-Bench substantially improves the robustness of VLMs while preserving their general utility, providing actionable guidance to improve multimodal safety alignment against emerging threats.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MALF: A Multi-Agent LLM Framework for Intelligent Fuzzing of Industrial Control Protocols</title>
<link>https://arxiv.org/abs/2510.02694</link>
<guid>https://arxiv.org/abs/2510.02694</guid>
<content:encoded><![CDATA[
arXiv:2510.02694v1 Announce Type: new 
Abstract: Industrial control systems (ICS) are vital to modern infrastructure but increasingly vulnerable to cybersecurity threats, particularly through weaknesses in their communication protocols. This paper presents MALF (Multi-Agent LLM Fuzzing Framework), an advanced fuzzing solution that integrates large language models (LLMs) with multi-agent coordination to identify vulnerabilities in industrial control protocols (ICPs). By leveraging Retrieval-Augmented Generation (RAG) for domain-specific knowledge and QLoRA fine-tuning for protocol-aware input generation, MALF enhances fuzz testing precision and adaptability. The multi-agent framework optimizes seed generation, mutation strategies, and feedback-driven refinement, leading to improved vulnerability discovery. Experiments on protocols like Modbus/TCP, S7Comm, and Ethernet/IP demonstrate that MALF surpasses traditional methods, achieving a test case pass rate (TCPR) of 88-92% and generating more exception triggers (ETN). MALF also maintains over 90% seed coverage and Shannon entropy values between 4.2 and 4.6 bits, ensuring diverse, protocol-compliant mutations. Deployed in a real-world Industrial Attack-Defense Range for power plants, MALF identified critical vulnerabilities, including three zero-day flaws, one confirmed and registered by CNVD. These results validate MALF's effectiveness in real-world fuzzing applications. This research highlights the transformative potential of multi-agent LLMs in ICS cybersecurity, offering a scalable, automated framework that sets a new standard for vulnerability discovery and strengthens critical infrastructure security against emerging threats.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks</title>
<link>https://arxiv.org/abs/2510.02712</link>
<guid>https://arxiv.org/abs/2510.02712</guid>
<content:encoded><![CDATA[
arXiv:2510.02712v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have revolutionized conversational AI, yet their robustness in extended multi-turn dialogues remains poorly understood. Existing evaluation frameworks focus on static benchmarks and single-turn assessments, failing to capture the temporal dynamics of conversational degradation that characterize real-world interactions. In this work, we present the first comprehensive survival analysis of conversational AI robustness, analyzing 36,951 conversation turns across 9 state-of-the-art LLMs to model failure as a time-to-event process. Our survival modeling framework-employing Cox proportional hazards, Accelerated Failure Time, and Random Survival Forest approaches-reveals extraordinary temporal dynamics. We find that abrupt, prompt-to-prompt(P2P) semantic drift is catastrophic, dramatically increasing the hazard of conversational failure. In stark contrast, gradual, cumulative drift is highly protective, vastly reducing the failure hazard and enabling significantly longer dialogues. AFT models with interactions demonstrate superior performance, achieving excellent discrimination and exceptional calibration. These findings establish survival analysis as a powerful paradigm for evaluating LLM robustness, offer concrete insights for designing resilient conversational agents, and challenge prevailing assumptions about the necessity of semantic consistency in conversational AI Systems.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deceptive Planning Exploiting Inattention Blindness</title>
<link>https://arxiv.org/abs/2510.02714</link>
<guid>https://arxiv.org/abs/2510.02714</guid>
<content:encoded><![CDATA[
arXiv:2510.02714v1 Announce Type: new 
Abstract: We study decision-making with rational inattention in settings where agents have perception constraints. In such settings, inaccurate prior beliefs or models of others may lead to inattention blindness, where an agent is unaware of its incorrect beliefs. We model this phenomenon in two-player zero-sum stochastic games, where Player 1 has perception constraints and Player 2 deceptively deviates from its security policy presumed by Player 1 to gain an advantage. We formulate the perception constraints as an online sensor selection problem, develop a value-weighted objective function for sensor selection capturing rational inattention, and propose the greedy algorithm for selection under this monotone objective function. When Player 2 does not deviate from the presumed policy, this objective function provides an upper bound on the expected value loss compared to the security value where Player 1 has perfect information of the state. We then propose a myopic decision-making algorithm for Player 2 to exploit Player 1's beliefs by deviating from the presumed policy and, thereby, improve upon the security value. Numerical examples illustrate how Player 1 persistently chooses sensors that are consistent with its priors, allowing Player 2 to systematically exploit its inattention.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Path of Self-Evolving Large Language Models: Achieving Data-Efficient Learning via Intrinsic Feedback</title>
<link>https://arxiv.org/abs/2510.02752</link>
<guid>https://arxiv.org/abs/2510.02752</guid>
<content:encoded><![CDATA[
arXiv:2510.02752v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has demonstrated potential in enhancing the reasoning capabilities of large language models (LLMs), but such training typically demands substantial efforts in creating and annotating data. In this work, we explore improving LLMs through RL with minimal data. Our approach alternates between the LLM proposing a task and then attempting to solve it. To minimize data dependency, we introduce two novel mechanisms grounded in self-awareness: (1) self-aware difficulty prediction, where the model learns to assess task difficulty relative to its own abilities and prioritize challenging yet solvable tasks, and (2) self-aware limit breaking, where the model recognizes when a task is beyond its capability boundary and proactively requests external data to break through that limit. Extensive experiments on nine benchmarks showing a 53.8% relative improvement with less than 1.2% extra data demonstrate the efficacy of self-aware RL and underscore the promise of self-evolving agent training.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prototyping Digital Social Spaces through Metaphor-Driven Design: Translating Spatial Concepts into an Interactive Social Simulation</title>
<link>https://arxiv.org/abs/2510.02759</link>
<guid>https://arxiv.org/abs/2510.02759</guid>
<content:encoded><![CDATA[
arXiv:2510.02759v1 Announce Type: new 
Abstract: Social media platforms are central to communication, yet their designs remain narrowly focused on engagement and scale. While researchers have proposed alternative visions for online spaces, these ideas are difficult to prototype within platform constraints. In this paper, we introduce a metaphor-driven system to help users imagine and explore new social media environments. The system translates users' metaphors into structured sets of platform features and generates interactive simulations populated with LLM-driven agents. To evaluate this approach, we conducted a study where participants created and interacted with simulated social media spaces. Our findings show that metaphors allow users to express distinct social expectations, and that perceived authenticity of the simulation depended on how well it captured dynamics like intimacy, participation, and temporal engagement. We conclude by discussing how metaphor-driven simulation can be a powerful design tool for prototyping alternative social architectures and expanding the design space for future social platforms.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OTR: Synthesizing Overlay Text Dataset for Text Removal</title>
<link>https://arxiv.org/abs/2510.02787</link>
<guid>https://arxiv.org/abs/2510.02787</guid>
<content:encoded><![CDATA[
arXiv:2510.02787v1 Announce Type: new 
Abstract: Text removal is a crucial task in computer vision with applications such as privacy preservation, image editing, and media reuse. While existing research has primarily focused on scene text removal in natural images, limitations in current datasets hinder out-of-domain generalization or accurate evaluation. In particular, widely used benchmarks such as SCUT-EnsText suffer from ground truth artifacts due to manual editing, overly simplistic text backgrounds, and evaluation metrics that do not capture the quality of generated results. To address these issues, we introduce an approach to synthesizing a text removal benchmark applicable to domains other than scene texts. Our dataset features text rendered on complex backgrounds using object-aware placement and vision-language model-generated content, ensuring clean ground truth and challenging text removal scenarios. The dataset is available at https://huggingface.co/datasets/cyberagent/OTR .
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Final Answer: Evaluating the Reasoning Trajectories of Tool-Augmented Agents</title>
<link>https://arxiv.org/abs/2510.02837</link>
<guid>https://arxiv.org/abs/2510.02837</guid>
<content:encoded><![CDATA[
arXiv:2510.02837v1 Announce Type: new 
Abstract: Although recent tool-augmented benchmarks incorporate complex user requests and diverse tools, the evaluation methods for most of them remain limited to answer matching. However, as the number of steps required to resolve a user request increases, a proper evaluation of an agent's performance must go beyond the final answer to also assess the problem-solving trajectory, including previously ignored aspects such as efficiency, hallucination, and adaptivity. The most straightforward method for evaluating these aspects is to compare an agent's trajectory with the ground-truth trajectory, but this approach is fundamentally limited since annotating all valid ground-truth trajectories is prohibitively expensive. However, a simple LLM-based evaluator struggles to assess trajectories in detail without ground truth. To effectively evaluate the agents in this manner, we introduce TRACE, a framework for the multi-dimensional evaluation of tool-augmented LLM agent performance. By incorporating an evidence bank, which accumulates knowledge gathered from preceding reasoning steps, TRACE enables a multi-faceted analysis and evaluation of an agent's reasoning trajectory effectively. To validate our framework, we develop a new meta-evaluation dataset by augmenting existing benchmarks with diverse and flawed trajectories, each labeled with multi-faceted performance scores. Our results confirm that TRACE accurately evaluates these complex behaviors in a scalable and cost-effective manner, even with small open-source LLMs. Furthermore, we apply our method to evaluate the trajectories that agents produce while solving tool-augmented tasks, presenting previously unreported observations and their corresponding insights.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Delay-Tolerant Augmented-Consensus-based Distributed Directed Optimization</title>
<link>https://arxiv.org/abs/2510.02889</link>
<guid>https://arxiv.org/abs/2510.02889</guid>
<content:encoded><![CDATA[
arXiv:2510.02889v1 Announce Type: new 
Abstract: Distributed optimization finds applications in large-scale machine learning, data processing and classification over multi-agent networks. In real-world scenarios, the communication network of agents may encounter latency that may affect the convergence of the optimization protocol. This paper addresses the case where the information exchange among the agents (computing nodes) over data-transmission channels (links) might be subject to communication time-delays, which is not well addressed in the existing literature. Our proposed algorithm improves the state-of-the-art by handling heterogeneous and arbitrary but bounded and fixed (time-invariant) delays over general strongly-connected directed networks. Arguments from matrix theory, algebraic graph theory, and augmented consensus formulation are applied to prove the convergence to the optimal value. Simulations are provided to verify the results and compare the performance with some existing delay-free algorithms.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Axiomatisation for an asynchronous epistemic logic with sending and receiving messages</title>
<link>https://arxiv.org/abs/2510.02890</link>
<guid>https://arxiv.org/abs/2510.02890</guid>
<content:encoded><![CDATA[
arXiv:2510.02890v1 Announce Type: new 
Abstract: We investigate a public announcement logic for asynchronous public announcements wherein the sending of the announcements by the environment is separated from the reception of the announcements by the individual agents. Both come with different modalities. In the logical semantics, formulas are interpreted in a world of a Kripke model but given a history of prior announcements and receptions of announcements that already happened. An axiomatisation AA for such a logic has been given in prior work, for the formulas that are valid when interpreted in the Kripke model before any such announcements have taken place. This axiomatisation is a reduction system wherein one can show that every formula is equivalent to a purely epistemic formula without dynamic modalities for announcements and receptions. We propose a generalisation AA* of this axiomatisation, for the formulas that are valid when interpreted in the Kripke model given any history of prior announcements and receptions of announcements. It does not extend the axiomatisation AA, for example it is no longer valid that nobody has received any announcement. Unlike AA, this axiomatisation AA* is infinitary and it is not a reduction system.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ergodic Risk Measures: Towards a Risk-Aware Foundation for Continual Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.02945</link>
<guid>https://arxiv.org/abs/2510.02945</guid>
<content:encoded><![CDATA[
arXiv:2510.02945v1 Announce Type: new 
Abstract: Continual reinforcement learning (continual RL) seeks to formalize the notions of lifelong learning and endless adaptation in RL. In particular, the aim of continual RL is to develop RL agents that can maintain a careful balance between retaining useful information and adapting to new situations. To date, continual RL has been explored almost exclusively through the lens of risk-neutral decision-making, in which the agent aims to optimize the expected (or mean) long-run performance. In this work, we present the first formal theoretical treatment of continual RL through the lens of risk-aware decision-making, in which the agent aims to optimize a reward-based measure of long-run performance beyond the mean. In particular, we show that the classical theory of risk measures, widely used as a theoretical foundation in non-continual risk-aware RL, is, in its current form, incompatible with the continual setting. Then, building on this insight, we extend risk measure theory into the continual setting by introducing a new class of ergodic risk measures that are compatible with continual learning. Finally, we provide a case study of risk-aware continual learning, along with empirical results, which show the intuitive appeal and theoretical soundness of ergodic risk measures.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AudioToolAgent: An Agentic Framework for Audio-Language Models</title>
<link>https://arxiv.org/abs/2510.02995</link>
<guid>https://arxiv.org/abs/2510.02995</guid>
<content:encoded><![CDATA[
arXiv:2510.02995v1 Announce Type: new 
Abstract: Large Audio-Language Models (LALMs) perform well on audio understanding tasks but lack multi-step reasoning and tool-calling found in recent Large Language Models (LLMs). This paper presents AudioToolAgent, a framework that coordinates audio-language models as tools via a central LLM agent that accesses tool adapters for audio question answering and speech-to-text. The agent selects tools, asks follow-up questions, and compares outputs for verification. Experiments with MMAU, MMAR, and MMAU-Pro show state-of-the-art accuracy: up to 74.10% on MMAU, 68.80% on MMAR, and 57.96% on MMAU-Pro. Monte Carlo sampling for shapley values across 374 configurations identifies effective agent-tool combinations. The modular design allows integration of new tools and eliminates the use of data and training costs. Code and reproduction materials are available at: github.com/GLJS/AudioToolAgent
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Deep Reinforcement Learning Approach for Close Enough Traveling Salesman Problem</title>
<link>https://arxiv.org/abs/2510.03065</link>
<guid>https://arxiv.org/abs/2510.03065</guid>
<content:encoded><![CDATA[
arXiv:2510.03065v1 Announce Type: new 
Abstract: In recent years, deep reinforcement learning (DRL) has gained traction for solving the NP-hard traveling salesman problem (TSP). However, limited attention has been given to the close-enough TSP (CETSP), primarily due to the challenge introduced by its neighborhood-based visitation criterion, wherein a node is considered visited if the agent enters a compact neighborhood around it. In this work, we formulate a Markov decision process (MDP) for CETSP using a discretization scheme and propose a novel unified dual-decoder DRL (UD3RL) framework that separates decision-making into node selection and waypoint determination. Specifically, an adapted encoder is employed for effective feature extraction, followed by a node-decoder and a loc-decoder to handle the two sub-tasks, respectively. A k-nearest neighbors subgraph interaction strategy is further introduced to enhance spatial reasoning during location decoding. Furthermore, we customize the REINFORCE algorithm to train UD3RL as a unified model capable of generalizing across different problem sizes and varying neighborhood radius types (i.e., constant and random radii). Experimental results show that UD3RL outperforms conventional methods in both solution quality and runtime, while exhibiting strong generalization across problem scales, spatial distributions, and radius ranges, as well as robustness to dynamic environments.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embracing Evolution: A Call for Body-Control Co-Design in Embodied Humanoid Robot</title>
<link>https://arxiv.org/abs/2510.03081</link>
<guid>https://arxiv.org/abs/2510.03081</guid>
<content:encoded><![CDATA[
arXiv:2510.03081v1 Announce Type: new 
Abstract: Humanoid robots, as general-purpose physical agents, must integrate both intelligent control and adaptive morphology to operate effectively in diverse real-world environments. While recent research has focused primarily on optimizing control policies for fixed robot structures, this position paper argues for evolving both control strategies and humanoid robots' physical structure under a co-design mechanism. Inspired by biological evolution, this approach enables robots to iteratively adapt both their form and behavior to optimize performance within task-specific and resource-constrained contexts. Despite its promise, co-design in humanoid robotics remains a relatively underexplored domain, raising fundamental questions about its feasibility and necessity in achieving true embodied intelligence. To address these challenges, we propose practical co-design methodologies grounded in strategic exploration, Sim2Real transfer, and meta-policy learning. We further argue for the essential role of co-design by analyzing it from methodological, application-driven, and community-oriented perspectives. Striving to guide and inspire future studies, we present open research questions, spanning from short-term innovations to long-term goals. This work positions co-design as a cornerstone for developing the next generation of intelligent and adaptable humanoid agents.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?</title>
<link>https://arxiv.org/abs/2510.03120</link>
<guid>https://arxiv.org/abs/2510.03120</guid>
<content:encoded><![CDATA[
arXiv:2510.03120v1 Announce Type: new 
Abstract: Academic survey writing, which distills vast literature into a coherent and insightful narrative, remains a labor-intensive and intellectually demanding task. While recent approaches, such as general DeepResearch agents and survey-specialized methods, can generate surveys automatically (a.k.a. LLM4Survey), their outputs often fall short of human standards and there lacks a rigorous, reader-aligned benchmark for thoroughly revealing their deficiencies. To fill the gap, we propose a fine-grained, quiz-driven evaluation framework SurveyBench, featuring (1) typical survey topics source from recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys; (2) a multifaceted metric hierarchy that assesses the outline quality (e.g., coverage breadth, logical coherence), content quality (e.g., synthesis granularity, clarity of insights), and non-textual richness; and (3) a dual-mode evaluation protocol that includes content-based and quiz-based answerability tests, explicitly aligned with readers' informational needs. Results show SurveyBench effectively challenges existing LLM4Survey approaches (e.g., on average 21% lower than human in content-based evaluation).
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReeMark: Reeb Graphs for Simulating Patterns of Life in Spatiotemporal Trajectories</title>
<link>https://arxiv.org/abs/2510.03152</link>
<guid>https://arxiv.org/abs/2510.03152</guid>
<content:encoded><![CDATA[
arXiv:2510.03152v1 Announce Type: new 
Abstract: Accurately modeling human mobility is critical for urban planning, epidemiology, and traffic management. In this work, we introduce Markovian Reeb Graphs, a novel framework for simulating spatiotemporal trajectories that preserve Patterns of Life (PoLs) learned from baseline data. By combining individual- and population-level mobility structures within a probabilistic topological model, our approach generates realistic future trajectories that capture both consistency and variability in daily life. Evaluations on the Urban Anomalies dataset (Atlanta and Berlin subsets) using the Jensen-Shannon Divergence (JSD) across population- and agent-level metrics demonstrate that the proposed method achieves strong fidelity while remaining data- and compute-efficient. These results position Markovian Reeb Graphs as a scalable framework for trajectory simulation with broad applicability across diverse urban environments.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Cooperation in Collaborative Embodied AI</title>
<link>https://arxiv.org/abs/2510.03153</link>
<guid>https://arxiv.org/abs/2510.03153</guid>
<content:encoded><![CDATA[
arXiv:2510.03153v1 Announce Type: new 
Abstract: The integration of Large Language Models (LLMs) into multiagent systems has opened new possibilities for collaborative reasoning and cooperation with AI agents. This paper explores different prompting methods and evaluates their effectiveness in enhancing agent collaborative behaviour and decision-making. We enhance CoELA, a framework designed for building Collaborative Embodied Agents that leverage LLMs for multi-agent communication, reasoning, and task coordination in shared virtual spaces. Through systematic experimentation, we examine different LLMs and prompt engineering strategies to identify optimised combinations that maximise collaboration performance. Furthermore, we extend our research by integrating speech capabilities, enabling seamless collaborative voice-based interactions. Our findings highlight the effectiveness of prompt optimisation in enhancing collaborative agent performance; for example, our best combination improved the efficiency of the system running with Gemma3 by 22% compared to the original CoELA system. In addition, the speech integration provides a more engaging user interface for iterative system development and demonstrations.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniShield: An Adaptive Multi-Agent Framework for Unified Forgery Image Detection and Localization</title>
<link>https://arxiv.org/abs/2510.03161</link>
<guid>https://arxiv.org/abs/2510.03161</guid>
<content:encoded><![CDATA[
arXiv:2510.03161v1 Announce Type: new 
Abstract: With the rapid advancements in image generation, synthetic images have become increasingly realistic, posing significant societal risks, such as misinformation and fraud. Forgery Image Detection and Localization (FIDL) thus emerges as essential for maintaining information integrity and societal security. Despite impressive performances by existing domain-specific detection methods, their practical applicability remains limited, primarily due to their narrow specialization, poor cross-domain generalization, and the absence of an integrated adaptive framework. To address these issues, we propose UniShield, the novel multi-agent-based unified system capable of detecting and localizing image forgeries across diverse domains, including image manipulation, document manipulation, DeepFake, and AI-generated images. UniShield innovatively integrates a perception agent with a detection agent. The perception agent intelligently analyzes image features to dynamically select suitable detection models, while the detection agent consolidates various expert detectors into a unified framework and generates interpretable reports. Extensive experiments show that UniShield achieves state-of-the-art results, surpassing both existing unified approaches and domain-specific detectors, highlighting its superior practicality, adaptiveness, and scalability.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Q-Learning with Shift-Aware Upper Confidence Bound in Non-Stationary Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.03181</link>
<guid>https://arxiv.org/abs/2510.03181</guid>
<content:encoded><![CDATA[
arXiv:2510.03181v1 Announce Type: new 
Abstract: We study the Non-Stationary Reinforcement Learning (RL) under distribution shifts in both finite-horizon episodic and infinite-horizon discounted Markov Decision Processes (MDPs). In the finite-horizon case, the transition functions may suddenly change at a particular episode. In the infinite-horizon setting, such changes can occur at an arbitrary time step during the agent's interaction with the environment. While the Q-learning Upper Confidence Bound algorithm (QUCB) can discover a proper policy during learning, due to the distribution shifts, this policy can exploit sub-optimal rewards after the shift happens. To address this issue, we propose Density-QUCB (DQUCB), a shift-aware Q-learning~UCB algorithm, which uses a transition density function to detect distribution shifts, then leverages its likelihood to enhance the uncertainty estimation quality of Q-learning~UCB, resulting in a balance between exploration and exploitation. Theoretically, we prove that our oracle DQUCB achieves a better regret guarantee than QUCB. Empirically, our DQUCB enjoys the computational efficiency of model-free RL and outperforms QUCB baselines by having a lower regret across RL tasks, as well as a real-world COVID-19 patient hospital allocation task using a Deep-Q-learning architecture.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoDA: Agentic Systems for Collaborative Data Visualization</title>
<link>https://arxiv.org/abs/2510.03194</link>
<guid>https://arxiv.org/abs/2510.03194</guid>
<content:encoded><![CDATA[
arXiv:2510.03194v1 Announce Type: new 
Abstract: Deep research has revolutionized data analysis, yet data scientists still devote substantial time to manually crafting visualizations, highlighting the need for robust automation from natural language queries. However, current systems struggle with complex datasets containing multiple files and iterative refinement. Existing approaches, including simple single- or multi-agent systems, often oversimplify the task, focusing on initial query parsing while failing to robustly manage data complexity, code errors, or final visualization quality. In this paper, we reframe this challenge as a collaborative multi-agent problem. We introduce CoDA, a multi-agent system that employs specialized LLM agents for metadata analysis, task planning, code generation, and self-reflection. We formalize this pipeline, demonstrating how metadata-focused analysis bypasses token limits and quality-driven refinement ensures robustness. Extensive evaluations show CoDA achieves substantial gains in the overall score, outperforming competitive baselines by up to 41.5%. This work demonstrates that the future of visualization automation lies not in isolated code generation but in integrated, collaborative agentic workflows.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Best-of-Majority: Minimax-Optimal Strategy for Pass@$k$ Inference Scaling</title>
<link>https://arxiv.org/abs/2510.03199</link>
<guid>https://arxiv.org/abs/2510.03199</guid>
<content:encoded><![CDATA[
arXiv:2510.03199v1 Announce Type: new 
Abstract: LLM inference often generates a batch of candidates for a prompt and selects one via strategies like majority voting or Best-of- N (BoN). For difficult tasks, this single-shot selection often underperforms. Consequently, evaluations commonly report Pass@$k$: the agent may submit up to $k$ responses, and only the best of them is used when computing regret. Motivated by this, we study inference scaling in the more general Pass@$k$ inference setting, and prove that neither majority voting nor BoN exhibits the desirable scaling with $k$ and the sampling budget $N$. Combining the advantages of majority voting and BoN, we propose a new inference strategy called Best-of-Majority (BoM), with a pivotal step that restricts the candidates to the responses with high frequency in the $N$ samples before selecting the top-$k$ rewards. We prove that when the sampling budget is $N=\tilde\Omega(C^*)$, the regret of BoM is $O(\epsilon_{\mathrm{opt}}+\sqrt{\epsilon_{\mathrm{RM}}^2C^*/k})$, where $C^*$ is the coverage coefficient, $\epsilon_{\mathrm{RM}}$ is the estimation error of the reward model, and $\epsilon_{\mathrm{opt}}$ is the estimation error of reward at the optimal response. We further establish a matching lower bound, certifying that our algorithm is minimax optimal. Beyond optimality, BoM has a key advantage: unlike majority voting and BoN, its performance does not degrade when increasing $N$. Experimental results of inference on math problems show BoM outperforming both majority voting and BoN.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents</title>
<link>https://arxiv.org/abs/2510.03204</link>
<guid>https://arxiv.org/abs/2510.03204</guid>
<content:encoded><![CDATA[
arXiv:2510.03204v1 Announce Type: new 
Abstract: Web agents powered by large language models (LLMs) must process lengthy web page observations to complete user goals; these pages often exceed tens of thousands of tokens. This saturates context limits and increases computational cost processing; moreover, processing full pages exposes agents to security risks such as prompt injection. Existing pruning strategies either discard relevant content or retain irrelevant context, leading to suboptimal action prediction. We introduce FocusAgent, a simple yet effective approach that leverages a lightweight LLM retriever to extract the most relevant lines from accessibility tree (AxTree) observations, guided by task goals. By pruning noisy and irrelevant content, FocusAgent enables efficient reasoning while reducing vulnerability to injection attacks. Experiments on WorkArena and WebArena benchmarks show that FocusAgent matches the performance of strong baselines, while reducing observation size by over 50%. Furthermore, a variant of FocusAgent significantly reduces the success rate of prompt-injection attacks, including banner and pop-up attacks, while maintaining task success performance in attack-free settings. Our results highlight that targeted LLM-based retrieval is a practical and robust strategy for building web agents that are efficient, effective, and secure.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Abstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic Program Repair</title>
<link>https://arxiv.org/abs/2510.03217</link>
<guid>https://arxiv.org/abs/2510.03217</guid>
<content:encoded><![CDATA[
arXiv:2510.03217v1 Announce Type: new 
Abstract: Agentic Automated Program Repair (APR) is increasingly tackling complex, repository-level bugs in industry, but ultimately agent-generated patches still need to be reviewed by a human before committing them to ensure they address the bug. Showing unlikely patches to developers can lead to substantial noise, wasting valuable developer time and eroding trust in automated code changes. We introduce two complementary LLM-based policies to reduce such noise: bug abstention and patch validation policies. Bug abstention excludes bugs that the agentic APR system is unlikely to fix. Patch validation rejects patches that are unlikely to be a good fix for the given bug. We evaluate both policies on three sets of bugs from Google's codebase, and their candidate patches generated by an internal agentic APR system. On a set of 174 human-reported bugs, removing bugs and patch trajectories rejected by our policies can raise success rates by up to 13 percentage points and 15 percentage points, respectively, and by up to 39 percentage points in combination. On null pointer exceptions and sanitizer-reported bugs with machine-generated bug reports, patch validation also improves average single-sample success rates. This two-policy approach provides a practical path to the reliable, industrial-scale deployment of agentic APR systems.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving GUI Grounding with Explicit Position-to-Coordinate Mapping</title>
<link>https://arxiv.org/abs/2510.03230</link>
<guid>https://arxiv.org/abs/2510.03230</guid>
<content:encoded><![CDATA[
arXiv:2510.03230v1 Announce Type: new 
Abstract: GUI grounding, the task of mapping natural-language instructions to pixel coordinates, is crucial for autonomous agents, yet remains difficult for current VLMs. The core bottleneck is reliable patch-to-pixel mapping, which breaks when extrapolating to high-resolution displays unseen during training. Current approaches generate coordinates as text tokens directly from visual features, forcing the model to infer complex position-to-pixel mappings implicitly; as a result, accuracy degrades and failures proliferate on new resolutions. We address this with two complementary innovations. First, RULER tokens serve as explicit coordinate markers, letting the model reference positions similar to gridlines on a map and adjust rather than generate coordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial encoding by ensuring that width and height dimensions are represented equally, addressing the asymmetry of standard positional schemes. Experiments on ScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in grounding accuracy, with the largest improvements on high-resolution interfaces. By providing explicit spatial guidance rather than relying on implicit learning, our approach enables more reliable GUI automation across diverse resolutions and platforms.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Homophily-induced Emergence of Biased Structures in LLM-based Multi-Agent AI Systems</title>
<link>https://arxiv.org/abs/2510.02637</link>
<guid>https://arxiv.org/abs/2510.02637</guid>
<content:encoded><![CDATA[
arXiv:2510.02637v1 Announce Type: cross 
Abstract: This study examines how interactions among artificially intelligent (AI) agents, guided by large language models (LLMs), drive the evolution of collective network structures. We ask LLM-driven agents to grow a network by informing them about current link constellations. Our observations confirm that agents consistently apply a preferential attachment mechanism, favoring connections to nodes with higher degrees. We systematically solicited more than a million decisions from four different LLMs, including Gemini, ChatGPT, Llama, and Claude. When social attributes such as age, gender, religion, and political orientation are incorporated, the resulting networks exhibit heightened assortativity, leading to the formation of distinct homophilic communities. This significantly alters the network topology from what would be expected under a pure preferential attachment model alone. Political and religious attributes most significantly fragment the collective, fostering polarized subgroups, while age and gender yield more gradual structural shifts. Strikingly, LLMs also reveal asymmetric patterns in heterophilous ties, suggesting embedded directional biases reflective of societal norms. As autonomous AI agents increasingly shape the architecture of online systems, these findings contribute to how algorithmic choices of generative AI collectives not only reshape network topology, but offer critical insights into how AI-driven systems co-evolve and self-organize.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Koopman Learning with Incomplete Measurements</title>
<link>https://arxiv.org/abs/2409.11586</link>
<guid>https://arxiv.org/abs/2409.11586</guid>
<content:encoded><![CDATA[
arXiv:2409.11586v2 Announce Type: replace 
Abstract: Koopman operator theory has emerged as a powerful tool for system identification, particularly for approximating nonlinear time-invariant systems (NTIS). This paper considers a network of agents with limited observation capabilities that collaboratively estimate the dynamics of an NTIS. A distributed deep Koopman learning algorithm is developed by integrating Koopman operator theory, deep neural networks, and consensus-based coordination. In the proposed framework, each agent approximates the system dynamics using its partial measurements and lifted states exchanged with its neighbors. This cooperative scheme enables accurate reconstruction of the global dynamics despite the absence of full-state information at individual agents. Simulation results on the Lunar Lander environment from OpenAI Gym demonstrate that the proposed method achieves performance comparable to the centralized deep Koopman learning with full-state access.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Koopman Learning using Partial Trajectories for Control</title>
<link>https://arxiv.org/abs/2412.07212</link>
<guid>https://arxiv.org/abs/2412.07212</guid>
<content:encoded><![CDATA[
arXiv:2412.07212v2 Announce Type: replace 
Abstract: This paper proposes a distributed data-driven framework for dynamics learning, termed distributed deep Koopman learning using partial trajectories (DDKL-PT). In this framework, each agent in a multi-agent system is assigned a partial trajectory offline and locally approximates the unknown dynamics using a deep neural network within the Koopman operator framework. By exchanging local estimated dynamics rather than training data, agents achieve consensus on a global dynamics model without sharing their private training trajectories. Simulation studies on a surface vehicle demonstrate that DDKL-PT attains consensus with respect to the learned dynamics, with each agent achieving reasonably small approximation errors over the testing data. Furthermore, a model predictive control scheme is developed by integrating the learned Koopman dynamics with known kinematic relations. Results on goal-tracking and station-keeping tasks support that the distributedly learned dynamics are sufficiently accurate for model-based optimal control.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Programming with Pixels: Can Computer-Use Agents do Software Engineering?</title>
<link>https://arxiv.org/abs/2502.18525</link>
<guid>https://arxiv.org/abs/2502.18525</guid>
<content:encoded><![CDATA[
arXiv:2502.18525v2 Announce Type: replace 
Abstract: Computer-use agents (CUAs) hold the promise of performing a wide variety of general tasks, but current evaluations have primarily focused on simple scenarios. It therefore remains unclear whether such generalist agents can automate more sophisticated and specialized work such as software engineering (SWE). To investigate this, we introduce $\texttt{Programming with Pixels}$ (PwP), the first comprehensive computer-use environment for software engineering, where agents visually control an IDE to perform diverse software engineering tasks. To enable holistic evaluation, we also introduce \texttt{PwP-Bench}, a benchmark of 15 existing and new software-engineering tasks spanning multiple modalities, programming languages, and skillsets. We perform an extensive evaluation of state-of-the-art open-weight and closed-weight CUAs and find that when interacting purely visually, they perform significantly worse than specialized coding agents. However, when the same CUAs are given direct access to just two APIs-file editing and bash operations-performance jumps, often reaching the levels of specialized agents despite having a task-agnostic design. Furthermore, when given access to additional IDE tools via text APIs, all models show further gains. Our analysis shows that current CUAs fall short mainly due to limited visual grounding and the inability to take full advantage of the rich environment, leaving clear room for future improvements.PwP establishes software engineering as a natural domain for benchmarking whether generalist computer-use agents can reach specialist-level performance on sophisticated tasks. Code and data released at https://programmingwithpixels.com
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DatawiseAgent: A Notebook-Centric LLM Agent Framework for Adaptive and Robust Data Science Automation</title>
<link>https://arxiv.org/abs/2503.07044</link>
<guid>https://arxiv.org/abs/2503.07044</guid>
<content:encoded><![CDATA[
arXiv:2503.07044v2 Announce Type: replace 
Abstract: Existing large language model (LLM) agents for automating data science show promise, but they remain constrained by narrow task scopes, limited generalization across tasks and models, and over-reliance on state-of-the-art (SOTA) LLMs. We introduce DatawiseAgent, a notebook-centric LLM agent framework for adaptive and robust data science automation. Inspired by how human data scientists work in computational notebooks, DatawiseAgent introduces a unified interaction representation and a multi-stage architecture based on finite-state transducers (FSTs). This design enables flexible long-horizon planning, progressive solution development, and robust recovery from execution failures. Extensive experiments across diverse data science scenarios and models show that DatawiseAgent consistently achieves SOTA performance by surpassing strong baselines such as AutoGen and TaskWeaver, demonstrating superior effectiveness and adaptability. Further evaluations reveal graceful performance degradation under weaker or smaller models, underscoring the robustness and scalability.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Experiments</title>
<link>https://arxiv.org/abs/2505.09901</link>
<guid>https://arxiv.org/abs/2505.09901</guid>
<content:encoded><![CDATA[
arXiv:2505.09901v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly used to simulate or automate human behavior in complex sequential decision-making settings. A natural question is then whether LLMs exhibit similar decision-making behavior to humans, and can achieve comparable (or superior) performance. In this work, we focus on the exploration-exploitation (E&amp;E) tradeoff, a fundamental aspect of dynamic decision-making under uncertainty. We employ canonical multi-armed bandit (MAB) experiments introduced in the cognitive science and psychiatry literature to conduct a comparative study of the E&amp;E strategies of LLMs, humans, and MAB algorithms. We use interpretable choice models to capture the E&amp;E strategies of the agents and investigate how enabling thinking traces, through both prompting strategies and thinking models, shapes LLM decision-making. We find that enabling thinking in LLMs shifts their behavior toward more human-like behavior, characterized by a mix of random and directed exploration. In a simple stationary setting, thinking-enabled LLMs exhibit similar levels of random and directed exploration compared to humans. However, in more complex, non-stationary environments, LLMs struggle to match human adaptability, particularly in effective directed exploration, despite achieving similar regret in certain scenarios. Our findings highlight both the promise and limits of LLMs as simulators of human behavior and tools for automated decision-making and point to potential areas for improvement.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risk-Sensitive Agent Compositions</title>
<link>https://arxiv.org/abs/2506.04632</link>
<guid>https://arxiv.org/abs/2506.04632</guid>
<content:encoded><![CDATA[
arXiv:2506.04632v2 Announce Type: replace 
Abstract: From software development to robot control, modern agentic systems decompose complex objectives into a sequence of subtasks and choose a set of specialized AI agents to complete them. We formalize agentic workflows as directed acyclic graphs, called agent graphs, where edges represent AI agents and paths correspond to feasible compositions of agents. Real-world deployment requires selecting agent compositions that not only maximize task success but also minimize violations of safety, fairness, and privacy requirements which demands a careful analysis of the low-probability (tail) behaviors of compositions of agents. In this work, we consider risk minimization over the set of feasible agent compositions and seek to minimize the value-at-risk of the loss distribution of the agent composition where the loss quantifies violations of these requirements. We introduce an efficient algorithm which traverses the agent graph and finds a near-optimal composition of agents. It uses a dynamic programming approach to approximate the value-at-risk of agent compositions by exploiting a union bound. Furthermore, we prove that the approximation is near-optimal asymptotically for a broad class of practical loss functions. To evaluate our framework, we consider a suite of video game-like control benchmarks that require composing several agents trained with reinforcement learning and demonstrate our algorithm's effectiveness in approximating the value-at-risk and identifying the optimal agent composition.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in Realistic Web Environments</title>
<link>https://arxiv.org/abs/2506.08136</link>
<guid>https://arxiv.org/abs/2506.08136</guid>
<content:encoded><![CDATA[
arXiv:2506.08136v2 Announce Type: replace 
Abstract: We introduce EconWebArena, a benchmark for evaluating autonomous agents on complex, multimodal economic tasks in realistic web environments. The benchmark comprises 360 curated tasks from 82 authoritative websites spanning domains such as macroeconomics, labor, finance, trade, and public policy. Each task challenges agents to navigate live websites, interpret structured and visual content, interact with real interfaces, and extract precise, time-sensitive data through multi-step workflows. We construct the benchmark by prompting multiple large language models (LLMs) to generate candidate tasks, followed by rigorous human curation to ensure clarity, feasibility, and source reliability. Unlike prior work, EconWebArena emphasizes fidelity to authoritative data sources and the need for grounded web-based economic reasoning. We evaluate a diverse set of state-of-the-art multimodal LLMs as web agents, analyze failure cases, and conduct ablation studies to assess the impact of visual grounding, plan-based reasoning, and interaction design. Our results reveal substantial performance gaps and highlight persistent challenges in grounding, navigation, and multimodal understanding, positioning EconWebArena as a rigorous testbed for economic web intelligence.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering</title>
<link>https://arxiv.org/abs/2508.14052</link>
<guid>https://arxiv.org/abs/2508.14052</guid>
<content:encoded><![CDATA[
arXiv:2508.14052v4 Announce Type: replace 
Abstract: Accurate information retrieval (IR) is critical in the financial domain, where investors must identify relevant information from large collections of documents. Traditional IR methods -- whether sparse or dense -- often fall short in retrieval accuracy, as it requires not only capturing semantic similarity but also performing fine-grained reasoning over document structure and domain-specific knowledge. Recent advances in large language models (LLMs) have opened up new opportunities for retrieval with multi-step reasoning, where the model ranks passages through iterative reasoning about which information is most relevant to a given query. However, there exists no benchmark to evaluate such capabilities in the financial domain. To address this gap, we introduce FinAgentBench, the first large-scale benchmark for evaluating retrieval with multi-step reasoning in finance -- a setting we term agentic retrieval. The benchmark consists of 26K expert-annotated examples on S&amp;P-500 listed firms and assesses whether LLM agents can (1) identify the most relevant document type among candidates, and (2) pinpoint the key passage within the selected document. Our evaluation framework explicitly separates these two reasoning steps to address context limitations. This design enables to provide a quantitative basis for understanding retrieval-centric LLM behavior in finance. We evaluate a suite of state-of-the-art models and further demonstrated how targeted fine-tuning can significantly improve agentic retrieval performance. Our benchmark provides a foundation for studying retrieval-centric LLM behavior in complex, domain-specific tasks for finance.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling Unicode's Unseen Underpinnings in Undermining Authorship Attribution</title>
<link>https://arxiv.org/abs/2508.15840</link>
<guid>https://arxiv.org/abs/2508.15840</guid>
<content:encoded><![CDATA[
arXiv:2508.15840v2 Announce Type: replace 
Abstract: When using a public communication channel -- whether formal or informal, such as commenting or posting on social media -- end users have no expectation of privacy: they compose a message and broadcast it for the world to see. Even if an end user takes utmost precautions to anonymize their online presence -- using an alias or pseudonym; masking their IP address; spoofing their geolocation; concealing their operating system and user agent; deploying encryption; registering with a disposable phone number or email; disabling non-essential settings; revoking permissions; and blocking cookies and fingerprinting -- one obvious element still lingers: the message itself. Assuming they avoid lapses in judgment or accidental self-exposure, there should be little evidence to validate their actual identity, right? Wrong. The content of their message -- necessarily open for public consumption -- exposes an attack vector: stylometric analysis, or author profiling. In this paper, we dissect the technique of stylometry, discuss an antithetical counter-strategy in adversarial stylometry, and devise enhancements through Unicode steganography.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gala: Global LLM Agents for Text-to-Model Translation</title>
<link>https://arxiv.org/abs/2509.08970</link>
<guid>https://arxiv.org/abs/2509.08970</guid>
<content:encoded><![CDATA[
arXiv:2509.08970v2 Announce Type: replace 
Abstract: Natural language descriptions of optimization or satisfaction problems are challenging to translate into correct MiniZinc models, as this process demands both logical reasoning and constraint programming expertise. We introduce Gala, a framework that addresses this challenge with a global agentic approach: multiple specialized large language model (LLM) agents decompose the modeling task by global constraint type. Each agent is dedicated to detecting and generating code for a specific class of global constraint, while a final assembler agent integrates these constraint snippets into a complete MiniZinc model. By dividing the problem into smaller, well-defined sub-tasks, each LLM handles a simpler reasoning challenge, potentially reducing overall complexity. We conduct initial experiments with several LLMs and show better performance against baselines such as one-shot prompting and chain-of-thought prompting. Finally, we outline a comprehensive roadmap for future work, highlighting potential enhancements and directions for improvement.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RephQA: Evaluating Readability of Large Language Models in Public Health Question Answering</title>
<link>https://arxiv.org/abs/2509.16360</link>
<guid>https://arxiv.org/abs/2509.16360</guid>
<content:encoded><![CDATA[
arXiv:2509.16360v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) hold promise in addressing complex medical problems. However, while most prior studies focus on improving accuracy and reasoning abilities, a significant bottleneck in developing effective healthcare agents lies in the readability of LLM-generated responses, specifically, their ability to answer public health problems clearly and simply to people without medical backgrounds. In this work, we introduce RephQA, a benchmark for evaluating the readability of LLMs in public health question answering (QA). It contains 533 expert-reviewed QA pairs from 27 sources across 13 topics, and includes a proxy multiple-choice task to assess informativeness, along with two readability metrics: Flesch-Kincaid grade level and professional score. Evaluation of 25 LLMs reveals that most fail to meet readability standards, highlighting a gap between reasoning and effective communication. To address this, we explore four readability-enhancing strategies-standard prompting, chain-of-thought prompting, Group Relative Policy Optimization (GRPO), and a token-adapted variant. Token-adapted GRPO achieves the best results, advancing the development of more practical and user-friendly public health agents. These results represent a step toward building more practical agents for public health.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUI-PRA: Process Reward Agent for GUI Tasks</title>
<link>https://arxiv.org/abs/2509.23263</link>
<guid>https://arxiv.org/abs/2509.23263</guid>
<content:encoded><![CDATA[
arXiv:2509.23263v2 Announce Type: replace 
Abstract: Graphical User Interface (GUI) Agents powered by Multimodal Large Language Models (MLLMs) show significant potential for automating tasks. However, they often struggle with long-horizon tasks, leading to frequent failures. Process Reward Models (PRMs) are a promising solution, as they can guide these agents with crucial process signals during inference. Nevertheless, their application to the GUI domain presents unique challenges. When processing dense artificial inputs with long history data, PRMs suffer from a "lost in the middle" phenomenon, where the overwhelming historical context compromises the evaluation of the current step. Furthermore, standard PRMs lacks GUI changing awareness, providing static evaluations that are disconnected from the dynamic consequences of actions, a critical mismatch with the inherently dynamic nature of GUI tasks. In response to these challenges, we introduce GUI-PRA (Process Reward Agent for GUI Tasks), a judge agent designed to better provide process reward than standard PRM by intelligently processing historical context and actively perceiving UI state changes. Specifically, to directly combat the ``lost in the middle'' phenomenon, we introduce a dynamic memory mechanism consisting of two core components: a Relevance-based Retrieval Module to actively fetch pertinent information from long histories and a Progressive Summarization Module to dynamically condense growing interaction data, ensuring the model focuses on relevant context. Moreover, to address the lack of UI changing awareness, we introduce an Aadaptive UI Perception mechanism. This mechanism enables the agent to reason about UI state changes and dynamically select the most appropriate tool to gather grounded visual evidence, ensuring its evaluation is always informed by the current UI context.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MarketSenseAI 2.0: Enhancing Stock Analysis through LLM Agents</title>
<link>https://arxiv.org/abs/2502.00415</link>
<guid>https://arxiv.org/abs/2502.00415</guid>
<content:encoded><![CDATA[
arXiv:2502.00415v2 Announce Type: replace-cross 
Abstract: MarketSenseAI is a novel framework for holistic stock analysis which leverages Large Language Models (LLMs) to process financial news, historical prices, company fundamentals and the macroeconomic environment to support decision making in stock analysis and selection. In this paper, we present the latest advancements on MarketSenseAI, driven by rapid technological expansion in LLMs. Through a novel architecture combining Retrieval-Augmented Generation and LLM agents, the framework processes SEC filings and earnings calls, while enriching macroeconomic analysis through systematic processing of diverse institutional reports. We demonstrate a significant improvement in fundamental analysis accuracy over the previous version. Empirical evaluation on S\&amp;P 100 stocks over two years (2023-2024) shows MarketSenseAI achieving cumulative returns of 125.9% compared to the index return of 73.5%, while maintaining comparable risk profiles. Further validation on S\&amp;P 500 stocks during 2024 demonstrates the framework's scalability, delivering a 33.8% higher Sortino ratio than the market. This work marks a significant advancement in applying LLM technology to financial analysis, offering insights into the robustness of LLM-driven investment strategies.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How can AI agents support journalists' work? An experiment with designing an LLM-driven intelligent reporting system</title>
<link>https://arxiv.org/abs/2510.01193</link>
<guid>https://arxiv.org/abs/2510.01193</guid>
<content:encoded><![CDATA[
arXiv:2510.01193v1 Announce Type: new 
Abstract: The integration of artificial intelligence into journalistic practices represents a transformative shift in how news is gathered, analyzed, and disseminated. Large language models (LLMs), particularly those with agentic capabilities, offer unprecedented opportunities for enhancing journalistic workflows while simultaneously presenting complex challenges for newsroom integration. This research explores how agentic LLMs can support journalists' workflows, based on insights from journalist interviews and from the development of an LLM-based automation tool performing information filtering, summarization, and reporting. The paper details automated aggregation and summarization systems for journalists, presents a technical overview and evaluation of a user-centric LLM-driven reporting system (TeleFlash), and discusses both addressed and unmet journalist needs, with an outlook on future directions for AI-driven tools in journalism.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are LLMs ready to help non-expert users to make charts of official statistics data?</title>
<link>https://arxiv.org/abs/2510.01197</link>
<guid>https://arxiv.org/abs/2510.01197</guid>
<content:encoded><![CDATA[
arXiv:2510.01197v1 Announce Type: new 
Abstract: In this time when biased information, deep fakes, and propaganda proliferate, the accessibility of reliable data sources is more important than ever. National statistical institutes provide curated data that contain quantitative information on a wide range of topics. However, that information is typically spread across many tables and the plain numbers may be arduous to process. Hence, this open data may be practically inaccessible. We ask the question "Are current Generative AI models capable of facilitating the identification of the right data and the fully-automatic creation of charts to provide information in visual form, corresponding to user queries?". We present a structured evaluation of recent large language models' (LLMs) capabilities to generate charts from complex data in response to user queries. Working with diverse public data from Statistics Netherlands, we assessed multiple LLMs on their ability to identify relevant data tables, perform necessary manipulations, and generate appropriate visualizations autonomously. We propose a new evaluation framework spanning three dimensions: data retrieval & pre-processing, code quality, and visual representation. Results indicate that locating and processing the correct data represents the most significant challenge. Additionally, LLMs rarely implement visualization best practices without explicit guidance. When supplemented with information about effective chart design, models showed marked improvement in representation scores. Furthermore, an agentic approach with iterative self-evaluation led to excellent performance across all evaluation dimensions. These findings suggest that LLMs' effectiveness for automated chart generation can be enhanced through appropriate scaffolding and feedback mechanisms, and that systems can already reach the necessary accuracy across the three evaluation dimensions.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Extraction of Material Properties using LLM-based AI Agents</title>
<link>https://arxiv.org/abs/2510.01235</link>
<guid>https://arxiv.org/abs/2510.01235</guid>
<content:encoded><![CDATA[
arXiv:2510.01235v1 Announce Type: new 
Abstract: The rapid discovery of materials is constrained by the lack of large, machine-readable datasets that couple performance metrics with structural context. Existing databases are either small, manually curated, or biased toward first principles results, leaving experimental literature underexploited. We present an agentic, large language model (LLM)-driven workflow that autonomously extracts thermoelectric and structural-properties from about 10,000 full-text scientific articles. The pipeline integrates dynamic token allocation, zeroshot multi-agent extraction, and conditional table parsing to balance accuracy against computational cost. Benchmarking on 50 curated papers shows that GPT-4.1 achieves the highest accuracy (F1 = 0.91 for thermoelectric properties and 0.82 for structural fields), while GPT-4.1 Mini delivers nearly comparable performance (F1 = 0.89 and 0.81) at a fraction of the cost, enabling practical large scale deployment. Applying this workflow, we curated 27,822 temperature resolved property records with normalized units, spanning figure of merit (ZT), Seebeck coefficient, conductivity, resistivity, power factor, and thermal conductivity, together with structural attributes such as crystal class, space group, and doping strategy. Dataset analysis reproduces known thermoelectric trends, such as the superior performance of alloys over oxides and the advantage of p-type doping, while also surfacing broader structure-property correlations. To facilitate community access, we release an interactive web explorer with semantic filters, numeric queries, and CSV export. This study delivers the largest LLM-curated thermoelectric dataset to date, provides a reproducible and cost-profiled extraction pipeline, and establishes a foundation for scalable, data-driven materials discovery beyond thermoelectrics.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeMob: Semantic Synthesis for Dynamic Urban Mobility Prediction</title>
<link>https://arxiv.org/abs/2510.01245</link>
<guid>https://arxiv.org/abs/2510.01245</guid>
<content:encoded><![CDATA[
arXiv:2510.01245v1 Announce Type: new 
Abstract: Human mobility prediction is vital for urban services, but often fails to account for abrupt changes from external events. Existing spatiotemporal models struggle to leverage textual descriptions detailing these events. We propose SeMob, an LLM-powered semantic synthesis pipeline for dynamic mobility prediction. Specifically, SeMob employs a multi-agent framework where LLM-based agents automatically extract and reason about spatiotemporally related text from complex online texts. Fine-grained relevant contexts are then incorporated with spatiotemporal data through our proposed innovative progressive fusion architecture. The rich pre-trained event prior contributes enriched insights about event-driven prediction, and hence results in a more aligned forecasting model. Evaluated on a dataset constructed through our pipeline, SeMob achieves maximal reductions of 13.92% in MAE and 11.12% in RMSE compared to the spatiotemporal model. Notably, the framework exhibits pronounced superiority especially within spatiotemporal regions close to an event's location and time of occurrence.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RJE: A Retrieval-Judgment-Exploration Framework for Efficient Knowledge Graph Question Answering with LLMs</title>
<link>https://arxiv.org/abs/2510.01257</link>
<guid>https://arxiv.org/abs/2510.01257</guid>
<content:encoded><![CDATA[
arXiv:2510.01257v1 Announce Type: new 
Abstract: Knowledge graph question answering (KGQA) aims to answer natural language questions using knowledge graphs. Recent research leverages large language models (LLMs) to enhance KGQA reasoning, but faces limitations: retrieval-based methods are constrained by the quality of retrieved information, while agent-based methods rely heavily on proprietary LLMs. To address these limitations, we propose Retrieval-Judgment-Exploration (RJE), a framework that retrieves refined reasoning paths, evaluates their sufficiency, and conditionally explores additional evidence. Moreover, RJE introduces specialized auxiliary modules enabling small-sized LLMs to perform effectively: Reasoning Path Ranking, Question Decomposition, and Retriever-assisted Exploration. Experiments show that our approach with proprietary LLMs (such as GPT-4o-mini) outperforms existing baselines while enabling small open-source LLMs (such as 3B and 8B parameters) to achieve competitive results without fine-tuning LLMs. Additionally, RJE substantially reduces the number of LLM calls and token usage compared to agent-based methods, yielding significant efficiency improvements.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab</title>
<link>https://arxiv.org/abs/2510.01264</link>
<guid>https://arxiv.org/abs/2510.01264</guid>
<content:encoded><![CDATA[
arXiv:2510.01264v1 Announce Type: new 
Abstract: Multi-Agent Reinforcement Learning (MARL) is central to robotic systems cooperating in dynamic environments. While prior work has focused on these collaborative settings, adversarial interactions are equally critical for real-world applications such as pursuit-evasion, security, and competitive manipulation. In this work, we extend the IsaacLab framework to support scalable training of adversarial policies in high-fidelity physics simulations. We introduce a suite of adversarial MARL environments featuring heterogeneous agents with asymmetric goals and capabilities. Our platform integrates a competitive variant of Heterogeneous Agent Reinforcement Learning with Proximal Policy Optimization (HAPPO), enabling efficient training and evaluation under adversarial dynamics. Experiments across several benchmark scenarios demonstrate the framework's ability to model and train robust policies for morphologically diverse multi-agent competition while maintaining high throughput and simulation realism. Code and benchmarks are available at: https://github.com/DIRECTLab/IsaacLab-HARL .
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TUMIX: Multi-Agent Test-Time Scaling with Tool-Use Mixture</title>
<link>https://arxiv.org/abs/2510.01279</link>
<guid>https://arxiv.org/abs/2510.01279</guid>
<content:encoded><![CDATA[
arXiv:2510.01279v1 Announce Type: new 
Abstract: While integrating tools like Code Interpreter and Search has significantly enhanced Large Language Model (LLM) reasoning in models like ChatGPT Agent and Gemini-Pro, practical guidance on optimal tool use is lacking. The core challenge is effectively combining textual reasoning, coding, and search for diverse questions. In this paper, we propose Tool-Use Mixture (TUMIX), an ensemble framework that runs multiple agents in parallel, each employing distinct tool-use strategies and answer paths. Agents in TUMIX iteratively share and refine responses based on the question and previous answers. In experiments, TUMIX achieves significant gains over state-of-the-art tool-augmented and test-time scaling methods, delivering an average accuracy improvement of up to 3.55% over the best baseline on Gemini-2.5-Pro and Gemini-2.5-Flash across key reasoning benchmarks, with near-equal inference costs. We find that agent diversity and quality are crucial and can be enhanced by using LLMs to auto-optimize agent designs. Furthermore, TUMIX can halt refinement upon reaching sufficient confidence, preserving performance at only 49% of the inference cost. Further scaling can achieve higher performance, albeit at a greater cost.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science</title>
<link>https://arxiv.org/abs/2510.01285</link>
<guid>https://arxiv.org/abs/2510.01285</guid>
<content:encoded><![CDATA[
arXiv:2510.01285v1 Announce Type: new 
Abstract: The rapid advancement of Large Language Models (LLMs) has opened new opportunities in data science, yet their practical deployment is often constrained by the challenge of discovering relevant data within large heterogeneous data lakes. Existing methods struggle with this: single-agent systems are quickly overwhelmed by large, heterogeneous files in the large data lakes, while multi-agent systems designed based on a master-slave paradigm depend on a rigid central controller for task allocation that requires precise knowledge of each sub-agent's capabilities. To address these limitations, we propose a novel multi-agent communication paradigm inspired by the blackboard architecture for traditional AI models. In this framework, a central agent posts requests to a shared blackboard, and autonomous subordinate agents -- either responsible for a partition of the data lake or general information retrieval -- volunteer to respond based on their capabilities. This design improves scalability and flexibility by eliminating the need for a central coordinator to have prior knowledge of all sub-agents' expertise. We evaluate our method on three benchmarks that require explicit data discovery: KramaBench and modified versions of DS-Bench and DA-Code to incorporate data discovery. Experimental results demonstrate that the blackboard architecture substantially outperforms baselines, including RAG and the master-slave multi-agent paradigm, achieving between 13% to 57% relative improvement in end-to-end task success and up to a 9% relative gain in F1 score for data discovery over the best-performing baselines across both proprietary and open-source LLMs. Our findings establish the blackboard paradigm as a scalable and generalizable communication framework for multi-agent systems.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent evaluation hubs in a decentralizing large language model ecosystem</title>
<link>https://arxiv.org/abs/2510.01286</link>
<guid>https://arxiv.org/abs/2510.01286</guid>
<content:encoded><![CDATA[
arXiv:2510.01286v1 Announce Type: new 
Abstract: Large language models are proliferating, and so are the benchmarks that serve as their common yardsticks. We ask how the agglomeration patterns of these two layers compare: do they evolve in tandem or diverge? Drawing on two curated proxies for the ecosystem, the Stanford Foundation-Model Ecosystem Graph and the Evidently AI benchmark registry, we find complementary but contrasting dynamics. Model creation has broadened across countries and organizations and diversified in modality, licensing, and access. Benchmark influence, by contrast, displays centralizing patterns: in the inferred benchmark-author-institution network, the top 15% of nodes account for over 80% of high-betweenness paths, three countries produce 83% of benchmark outputs, and the global Gini for inferred benchmark authority reaches 0.89. An agent-based simulation highlights three mechanisms: higher entry of new benchmarks reduces concentration; rapid inflows can temporarily complicate coordination in evaluation; and stronger penalties against over-fitting have limited effect. Taken together, these results suggest that concentrated benchmark influence functions as coordination infrastructure that supports standardization, comparability, and reproducibility amid rising heterogeneity in model production, while also introducing trade-offs such as path dependence, selective visibility, and diminishing discriminative power as leaderboards saturate.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cyber Academia-Chemical Engineering (CA-ChemE): A Living Digital Town for Self-Directed Research Evolution and Emergent Scientific Discovery</title>
<link>https://arxiv.org/abs/2510.01293</link>
<guid>https://arxiv.org/abs/2510.01293</guid>
<content:encoded><![CDATA[
arXiv:2510.01293v1 Announce Type: new 
Abstract: The rapid advancement of artificial intelligence (AI) has demonstrated substantial potential in chemical engineering, yet existing AI systems remain limited in interdisciplinary collaboration and exploration of uncharted problems. To address these issues, we present the Cyber Academia-Chemical Engineering (CA-ChemE) system, a living digital town that enables self-directed research evolution and emergent scientific discovery through multi-agent collaboration. By integrating domain-specific knowledge bases, knowledge enhancement technologies, and collaboration agents, the system successfully constructs an intelligent ecosystem capable of deep professional reasoning and efficient interdisciplinary collaboration. Our findings demonstrate that knowledge base-enabled enhancement mechanisms improved dialogue quality scores by 10-15% on average across all seven expert agents, fundamentally ensuring technical judgments are grounded in verifiable scientific evidence. However, we observed a critical bottleneck in cross-domain collaboration efficiency, prompting the introduction of a Collaboration Agent (CA) equipped with ontology engineering capabilities. CA's intervention achieved 8.5% improvements for distant-domain expert pairs compared to only 0.8% for domain-proximate pairs - a 10.6-fold difference - unveiling the "diminished collaborative efficiency caused by knowledge-base gaps" effect. This study demonstrates how carefully designed multi-agent architectures can provide a viable pathway toward autonomous scientific discovery in chemical engineering.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation</title>
<link>https://arxiv.org/abs/2510.01295</link>
<guid>https://arxiv.org/abs/2510.01295</guid>
<content:encoded><![CDATA[
arXiv:2510.01295v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) transition from static tools to autonomous agents, traditional evaluation benchmarks that measure performance on downstream tasks are becoming insufficient. These methods fail to capture the emergent social and cognitive dynamics that arise when agents communicate, persuade, and collaborate in interactive environments. To address this gap, we introduce a novel evaluation framework that uses multi-agent debate as a controlled "social laboratory" to discover and quantify these behaviors. In our framework, LLM-based agents, instantiated with distinct personas and incentives, deliberate on a wide range of challenging topics under the supervision of an LLM moderator. Our analysis, enabled by a new suite of psychometric and semantic metrics, reveals several key findings. Across hundreds of debates, we uncover a powerful and robust emergent tendency for agents to seek consensus, consistently reaching high semantic agreement ({\mu} > 0.88) even without explicit instruction and across sensitive topics. We show that assigned personas induce stable, measurable psychometric profiles, particularly in cognitive effort, and that the moderators persona can significantly alter debate outcomes by structuring the environment, a key finding for external AI alignment. This work provides a blueprint for a new class of dynamic, psychometrically grounded evaluation protocols designed for the agentic setting, offering a crucial methodology for understanding and shaping the social behaviors of the next generation of AI agents. We have released the code and results at https://github.com/znreza/multi-agent-LLM-eval-for-debate.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimCity: Multi-Agent Urban Development Simulation with Rich Interactions</title>
<link>https://arxiv.org/abs/2510.01297</link>
<guid>https://arxiv.org/abs/2510.01297</guid>
<content:encoded><![CDATA[
arXiv:2510.01297v1 Announce Type: new 
Abstract: Large Language Models (LLMs) open new possibilities for constructing realistic and interpretable macroeconomic simulations. We present SimCity, a multi-agent framework that leverages LLMs to model an interpretable macroeconomic system with heterogeneous agents and rich interactions. Unlike classical equilibrium models that limit heterogeneity for tractability, or traditional agent-based models (ABMs) that rely on hand-crafted decision rules, SimCity enables flexible, adaptive behavior with transparent natural-language reasoning. Within SimCity, four core agent types (households, firms, a central bank, and a government) deliberate and participate in a frictional labor market, a heterogeneous goods market, and a financial market. Furthermore, a Vision-Language Model (VLM) determines the geographic placement of new firms and renders a mapped virtual city, allowing us to study both macroeconomic regularities and urban expansion dynamics within a unified environment. To evaluate the framework, we compile a checklist of canonical macroeconomic phenomena, including price elasticity of demand, Engel's Law, Okun's Law, the Phillips Curve, and the Beveridge Curve, and show that SimCity naturally reproduces these empirical patterns while remaining robust across simulation runs.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments</title>
<link>https://arxiv.org/abs/2510.01353</link>
<guid>https://arxiv.org/abs/2510.01353</guid>
<content:encoded><![CDATA[
arXiv:2510.01353v1 Announce Type: new 
Abstract: Recent works on context and memory benchmarking have primarily focused on conversational instances but the need for evaluating memory in dynamic enterprise environments is crucial for its effective application. We introduce MEMTRACK, a benchmark designed to evaluate long-term memory and state tracking in multi-platform agent environments. MEMTRACK models realistic organizational workflows by integrating asynchronous events across multiple communication and productivity platforms such as Slack, Linear and Git. Each benchmark instance provides a chronologically platform-interleaved timeline, with noisy, conflicting, cross-referring information as well as potential codebase/file-system comprehension and exploration. Consequently, our benchmark tests memory capabilities such as acquistion, selection and conflict resolution. We curate the MEMTRACK dataset through both manual expert driven design and scalable agent based synthesis, generating ecologically valid scenarios grounded in real world software development processes. We introduce pertinent metrics for Correctness, Efficiency, and Redundancy that capture the effectiveness of memory mechanisms beyond simple QA performance. Experiments across SoTA LLMs and memory backends reveal challenges in utilizing memory across long horizons, handling cross-platform dependencies, and resolving contradictions. Notably, the best performing GPT-5 model only achieves a 60\% Correctness score on MEMTRACK. This work provides an extensible framework for advancing evaluation research for memory-augmented agents, beyond existing focus on conversational setups, and sets the stage for multi-agent, multi-platform memory benchmarking in complex organizational settings
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents</title>
<link>https://arxiv.org/abs/2510.01354</link>
<guid>https://arxiv.org/abs/2510.01354</guid>
<content:encoded><![CDATA[
arXiv:2510.01354v1 Announce Type: new 
Abstract: Multiple prompt injection attacks have been proposed against web agents. At the same time, various methods have been developed to detect general prompt injection attacks, but none have been systematically evaluated for web agents. In this work, we bridge this gap by presenting the first comprehensive benchmark study on detecting prompt injection attacks targeting web agents. We begin by introducing a fine-grained categorization of such attacks based on the threat model. We then construct datasets containing both malicious and benign samples: malicious text segments generated by different attacks, benign text segments from four categories, malicious images produced by attacks, and benign images from two categories. Next, we systematize both text-based and image-based detection methods. Finally, we evaluate their performance across multiple scenarios. Our key findings show that while some detectors can identify attacks that rely on explicit textual instructions or visible image perturbations with moderate to high accuracy, they largely fail against attacks that omit explicit instructions or employ imperceptible perturbations. Our datasets and code are released at: https://github.com/Norrrrrrr-lyn/WAInjectBench.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks</title>
<link>https://arxiv.org/abs/2510.01359</link>
<guid>https://arxiv.org/abs/2510.01359</guid>
<content:encoded><![CDATA[
arXiv:2510.01359v1 Announce Type: new 
Abstract: Code-capable large language model (LLM) agents are increasingly embedded into software engineering workflows where they can read, write, and execute code, raising the stakes of safety-bypass ("jailbreak") attacks beyond text-only settings. Prior evaluations emphasize refusal or harmful-text detection, leaving open whether agents actually compile and run malicious programs. We present JAWS-BENCH (Jailbreaks Across WorkSpaces), a benchmark spanning three escalating workspace regimes that mirror attacker capability: empty (JAWS-0), single-file (JAWS-1), and multi-file (JAWS-M). We pair this with a hierarchical, executable-aware Judge Framework that tests (i) compliance, (ii) attack success, (iii) syntactic correctness, and (iv) runtime executability, moving beyond refusal to measure deployable harm. Using seven LLMs from five families as backends, we find that under prompt-only conditions in JAWS-0, code agents accept 61% of attacks on average; 58% are harmful, 52% parse, and 27% run end-to-end. Moving to single-file regime in JAWS-1 drives compliance to ~ 100% for capable models and yields a mean ASR (Attack Success Rate) ~ 71%; the multi-file regime (JAWS-M) raises mean ASR to ~ 75%, with 32% instantly deployable attack code. Across models, wrapping an LLM in an agent substantially increases vulnerability -- ASR raises by 1.6x -- because initial refusals are frequently overturned during later planning/tool-use steps. Category-level analyses identify which attack classes are most vulnerable and most readily deployable, while others exhibit large execution gaps. These findings motivate execution-aware defenses, code-contextual safety filters, and mechanisms that preserve refusal decisions throughout the agent's multi-step reasoning and tool use.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-tuning with RAG for Improving LLM Learning of New Skills</title>
<link>https://arxiv.org/abs/2510.01375</link>
<guid>https://arxiv.org/abs/2510.01375</guid>
<content:encoded><![CDATA[
arXiv:2510.01375v1 Announce Type: new 
Abstract: Large language model (LLM) agents deployed for multi-step tasks frequently fail in predictable ways: attempting actions with unmet preconditions, issuing redundant commands, or mishandling environment constraints. While retrieval-augmented generation (RAG) can improve performance by providing runtime guidance, it requires maintaining external knowledge databases and adds computational overhead at every deployment. We propose a simple pipeline that converts inference-time retrieval into learned competence through distillation. Our approach: (1) extracts compact, reusable hints from agent failures, (2) uses these hints to generate improved teacher trajectories via one-shot retrieval at episode start, and (3) trains student models on these trajectories with hint strings removed, forcing internalization rather than memorization. Across two interactive benchmarks, ALFWorld (household tasks) and WebShop (online shopping), distilled students consistently outperform baseline agents, achieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving WebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens than retrieval-augmented teachers depending on the environment. The approach generalizes across model scales (7B/14B parameters) and agent architectures (ReAct/StateAct), demonstrating that retrieval benefits can be effectively internalized through targeted fine-tuning without permanent runtime dependencies.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Single LLMs: Enhanced Code Generation via Multi-Stage Performance-Guided LLM Orchestration</title>
<link>https://arxiv.org/abs/2510.01379</link>
<guid>https://arxiv.org/abs/2510.01379</guid>
<content:encoded><![CDATA[
arXiv:2510.01379v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) have become the predominant paradigm for automated code generation, current single-model approaches fundamentally ignore the heterogeneous computational strengths that different models exhibit across programming languages, algorithmic domains, and development stages. This paper challenges the single-model convention by introducing a multi-stage, performance-guided orchestration framework that dynamically routes coding tasks to the most suitable LLMs within a structured generate-fix-refine workflow. Our approach is grounded in a comprehensive empirical study of 17 state-of-the-art LLMs across five programming languages (Python, Java, C++, Go, and Rust) using HumanEval-X benchmark. The study, which evaluates both functional correctness and runtime performance metrics (execution time, mean/max memory utilization, and CPU efficiency), reveals pronounced performance heterogeneity by language, development stage, and problem category. Guided by these empirical insights, we present PerfOrch, an LLM agent that orchestrates top-performing LLMs for each task context through stage-wise validation and rollback mechanisms. Without requiring model fine-tuning, PerfOrch achieves substantial improvements over strong single-model baselines: average correctness rates of 96.22% and 91.37% on HumanEval-X and EffiBench-X respectively, surpassing GPT-4o's 78.66% and 49.11%. Beyond correctness gains, the framework delivers consistent performance optimizations, improving execution time for 58.76% of problems with median speedups ranging from 17.67% to 27.66% across languages on two benchmarks. The framework's plug-and-play architecture ensures practical scalability, allowing new LLMs to be profiled and integrated seamlessly, thereby offering a paradigm for production-grade automated software engineering that adapts to the rapidly evolving generative AI landscape.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents</title>
<link>https://arxiv.org/abs/2510.01398</link>
<guid>https://arxiv.org/abs/2510.01398</guid>
<content:encoded><![CDATA[
arXiv:2510.01398v1 Announce Type: new 
Abstract: Modern engineering increasingly relies on vast datasets generated by experiments and simulations, driving a growing demand for efficient, reliable, and broadly applicable modeling strategies. There is also heightened interest in developing data-driven approaches, particularly neural network models, for effective prediction and analysis of scientific datasets. Traditional data-driven methods frequently involve extensive manual intervention, limiting their ability to scale effectively and generalize to diverse applications. In this study, we propose an innovative pipeline utilizing Large Language Model (LLM) agents to automate data-driven modeling and analysis, with a particular emphasis on regression tasks. We evaluate two LLM-agent frameworks: a multi-agent system featuring specialized collaborative agents, and a single-agent system based on the Reasoning and Acting (ReAct) paradigm. Both frameworks autonomously handle data preprocessing, neural network development, training, hyperparameter optimization, and uncertainty quantification (UQ). We validate our approach using a critical heat flux (CHF) prediction benchmark, involving approximately 25,000 experimental data points from the OECD/NEA benchmark dataset. Results indicate that our LLM-agent-developed model surpasses traditional CHF lookup tables and delivers predictive accuracy and UQ on par with state-of-the-art Bayesian optimized deep neural network models developed by human experts. These outcomes underscore the significant potential of LLM-based agents to automate complex engineering modeling tasks, greatly reducing human workload while meeting or exceeding existing standards of predictive performance.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OntoLogX: Ontology-Guided Knowledge Graph Extraction from Cybersecurity Logs with Large Language Models</title>
<link>https://arxiv.org/abs/2510.01409</link>
<guid>https://arxiv.org/abs/2510.01409</guid>
<content:encoded><![CDATA[
arXiv:2510.01409v1 Announce Type: new 
Abstract: System logs represent a valuable source of Cyber Threat Intelligence (CTI), capturing attacker behaviors, exploited vulnerabilities, and traces of malicious activity. Yet their utility is often limited by lack of structure, semantic inconsistency, and fragmentation across devices and sessions. Extracting actionable CTI from logs therefore requires approaches that can reconcile noisy, heterogeneous data into coherent and interoperable representations. We introduce OntoLogX, an autonomous Artificial Intelligence (AI) agent that leverages Large Language Models (LLMs) to transform raw logs into ontology-grounded Knowledge Graphs (KGs). OntoLogX integrates a lightweight log ontology with Retrieval Augmented Generation (RAG) and iterative correction steps, ensuring that generated KGs are syntactically and semantically valid. Beyond event-level analysis, the system aggregates KGs into sessions and employs a LLM to predict MITRE ATT&amp;CK tactics, linking low-level log evidence to higher-level adversarial objectives. We evaluate OntoLogX on both logs from a public benchmark and a real-world honeypot dataset, demonstrating robust KG generation across multiple KGs backends and accurate mapping of adversarial activity to ATT&amp;CK tactics. Results highlight the benefits of retrieval and correction for precision and recall, the effectiveness of code-oriented models in structured log analysis, and the value of ontology-grounded representations for actionable CTI extraction.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge Mining</title>
<link>https://arxiv.org/abs/2510.01427</link>
<guid>https://arxiv.org/abs/2510.01427</guid>
<content:encoded><![CDATA[
arXiv:2510.01427v1 Announce Type: new 
Abstract: At the core of Deep Research is knowledge mining, the task of extracting structured information from massive unstructured text in response to user instructions. Large language models (LLMs) excel at interpreting such instructions but are prohibitively expensive to deploy at scale, while traditional pipelines of classifiers and extractors remain efficient yet brittle and unable to generalize to new tasks. We introduce Falconer, a collaborative framework that combines the agentic reasoning of LLMs with lightweight proxy models for scalable knowledge mining. In Falconer, LLMs act as planners, decomposing user instructions into executable pipelines, and as annotators, generating supervision to train small proxies. The framework unifies classification and extraction into two atomic operations, get label and get span, enabling a single instruction-following model to replace multiple task-specific components. To evaluate the consistency between proxy models incubated by Falconer and annotations provided by humans and large models, we construct new benchmarks covering both planning and end-to-end execution. Experiments show that Falconer closely matches state-of-the-art LLMs in instruction-following accuracy while reducing inference cost by up to 90% and accelerating large-scale knowledge mining by more than 20x, offering an efficient and scalable foundation for Deep Research.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging</title>
<link>https://arxiv.org/abs/2510.01498</link>
<guid>https://arxiv.org/abs/2510.01498</guid>
<content:encoded><![CDATA[
arXiv:2510.01498v1 Announce Type: new 
Abstract: While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic aneurysms (AAA), the required iodinated contrast agents pose significant risks, including nephrotoxicity, patient allergies, and environmental harm. To reduce contrast agent use, recent deep learning methods have focused on generating synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a multi-stage pipeline that first generates images and then performs segmentation, which leads to error accumulation and fails to leverage shared semantic and anatomical structures. To address this, we propose a unified deep learning framework that generates synthetic CECT images from NCCT scans while simultaneously segmenting the aortic lumen and thrombus. Our approach integrates conditional diffusion models (CDM) with multi-task learning, enabling end-to-end joint optimization of image synthesis and anatomical segmentation. Unlike previous multitask diffusion models, our approach requires no initial predictions (e.g., a coarse segmentation mask), shares both encoder and decoder parameters across tasks, and employs a semi-supervised training strategy to learn from scans with missing segmentation labels, a common constraint in real-world clinical data. We evaluated our method on a cohort of 264 patients, where it consistently outperformed state-of-the-art single-task and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61 dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation, it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Majority Voting: LLM Aggregation by Leveraging Higher-Order Information</title>
<link>https://arxiv.org/abs/2510.01499</link>
<guid>https://arxiv.org/abs/2510.01499</guid>
<content:encoded><![CDATA[
arXiv:2510.01499v1 Announce Type: new 
Abstract: With the rapid progress of multi-agent large language model (LLM) reasoning, how to effectively aggregate answers from multiple LLMs has emerged as a fundamental challenge. Standard majority voting treats all answers equally, failing to consider latent heterogeneity and correlation across models. In this work, we design two new aggregation algorithms called Optimal Weight (OW) and Inverse Surprising Popularity (ISP), leveraging both first-order and second-order information. Our theoretical analysis shows these methods provably mitigate inherent limitations of majority voting under mild assumptions, leading to more reliable collective decisions. We empirically validate our algorithms on synthetic datasets, popular LLM fine-tuning benchmarks such as UltraFeedback and MMLU, and a real-world healthcare setting ARMMAN. Across all cases, our methods consistently outperform majority voting, offering both practical performance gains and conceptual insights for the design of robust multi-agent LLM pipelines.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaSynth: Multi-Agent Metadata Generation from Implicit Feedback in Black-Box Systems</title>
<link>https://arxiv.org/abs/2510.01523</link>
<guid>https://arxiv.org/abs/2510.01523</guid>
<content:encoded><![CDATA[
arXiv:2510.01523v1 Announce Type: new 
Abstract: Meta titles and descriptions strongly shape engagement in search and recommendation platforms, yet optimizing them remains challenging. Search engine ranking models are black box environments, explicit labels are unavailable, and feedback such as click-through rate (CTR) arrives only post-deployment. Existing template, LLM, and retrieval-augmented approaches either lack diversity, hallucinate attributes, or ignore whether candidate phrasing has historically succeeded in ranking. This leaves a gap in directly leveraging implicit signals from observable outcomes. We introduce MetaSynth, a multi-agent retrieval-augmented generation framework that learns from implicit search feedback. MetaSynth builds an exemplar library from top-ranked results, generates candidate snippets conditioned on both product content and exemplars, and iteratively refines outputs via evaluator-generator loops that enforce relevance, promotional strength, and compliance. On both proprietary e-commerce data and the Amazon Reviews corpus, MetaSynth outperforms strong baselines across NDCG, MRR, and rank metrics. Large-scale A/B tests further demonstrate 10.26% CTR and 7.51% clicks. Beyond metadata, this work contributes a general paradigm for optimizing content in black-box systems using implicit signals.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WALT: Web Agents that Learn Tools</title>
<link>https://arxiv.org/abs/2510.01524</link>
<guid>https://arxiv.org/abs/2510.01524</guid>
<content:encoded><![CDATA[
arXiv:2510.01524v1 Announce Type: new 
Abstract: Web agents promise to automate complex browser tasks, but current methods remain brittle -- relying on step-by-step UI interactions and heavy LLM reasoning that break under dynamic layouts and long horizons. Humans, by contrast, exploit website-provided functionality through high-level operations like search, filter, and sort. We introduce WALT (Web Agents that Learn Tools), a framework that reverse-engineers latent website functionality into reusable invocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust implementations of automations already designed into websites -- spanning discovery (search, filter, sort), communication (post, comment, upvote), and content management (create, edit, delete). Tools abstract away low-level execution: instead of reasoning about how to click and type, agents simply call search(query) or create(listing). This shifts the computational burden from fragile step-by-step reasoning to reliable tool invocation. On VisualWebArena and WebArena, WALT achieves higher success with fewer steps and less LLM-dependent reasoning, establishing a robust and generalizable paradigm for browser automation.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information Seeking for Robust Decision Making under Partial Observability</title>
<link>https://arxiv.org/abs/2510.01531</link>
<guid>https://arxiv.org/abs/2510.01531</guid>
<content:encoded><![CDATA[
arXiv:2510.01531v1 Announce Type: new 
Abstract: Explicit information seeking is essential to human problem-solving in practical environments characterized by incomplete information and noisy dynamics. When the true environmental state is not directly observable, humans seek information to update their internal dynamics and inform future decision-making. Although existing Large Language Model (LLM) planning agents have addressed observational uncertainty, they often overlook discrepancies between their internal dynamics and the actual environment. We introduce Information Seeking Decision Planner (InfoSeeker), an LLM decision-making framework that integrates task-oriented planning with information seeking to align internal dynamics and make optimal decisions under uncertainty in both agent observations and environmental dynamics. InfoSeeker prompts an LLM to actively gather information by planning actions to validate its understanding, detect environmental changes, or test hypotheses before generating or revising task-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark suite featuring partially observable environments with incomplete observations and uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74% absolute performance gain over prior methods without sacrificing sample efficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms baselines on established benchmarks such as robotic manipulation and web navigation. These findings underscore the importance of tightly integrating planning and information seeking for robust behavior in partially observable environments. The project page is available at https://infoseekerllm.github.io
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis</title>
<link>https://arxiv.org/abs/2510.01538</link>
<guid>https://arxiv.org/abs/2510.01538</guid>
<content:encoded><![CDATA[
arXiv:2510.01538v1 Announce Type: new 
Abstract: Time series forecasting is central to decision-making in domains as diverse as energy, finance, climate, and public health. In practice, forecasters face thousands of short, noisy series that vary in frequency, quality, and horizon, where the dominant cost lies not in model fitting, but in the labor-intensive preprocessing, validation, and ensembling required to obtain reliable predictions. Prevailing statistical and deep learning models are tailored to specific datasets or domains and generalize poorly. A general, domain-agnostic framework that minimizes human intervention is urgently in demand. In this paper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic framework for general time series forecasting. The framework comprises four specialized agents: Curator performs LLM-guided diagnostics augmented by external tools that reason over data statistics to choose targeted preprocessing; Planner narrows the hypothesis space of model choice by leveraging multi-modal diagnostics and self-planning over the input; Forecaster performs model fitting and validation and, based on the results, adaptively selects the best model configuration as well as ensemble strategy to make final predictions; and Reporter synthesizes the whole process into a comprehensive, transparent report. With transparent natural-language rationales and comprehensive reports, TSci transforms the forecasting workflow into a white-box system that is both interpretable and extensible across tasks. Empirical results on eight established benchmarks demonstrate that TSci consistently outperforms both statistical and LLM-based baselines, reducing forecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci produces a clear and rigorous report that makes the forecasting workflow more transparent and interpretable.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictive Preference Learning from Human Interventions</title>
<link>https://arxiv.org/abs/2510.01545</link>
<guid>https://arxiv.org/abs/2510.01545</guid>
<content:encoded><![CDATA[
arXiv:2510.01545v1 Announce Type: new 
Abstract: Learning from human involvement aims to incorporate the human subject to monitor and correct agent behavior errors. Although most interactive imitation learning methods focus on correcting the agent's action at the current state, they do not adjust its actions in future states, which may be potentially more hazardous. To address this, we introduce Predictive Preference Learning from Human Interventions (PPL), which leverages the implicit preference signals contained in human interventions to inform predictions of future rollouts. The key idea of PPL is to bootstrap each human intervention into L future time steps, called the preference horizon, with the assumption that the agent follows the same action and the human makes the same intervention in the preference horizon. By applying preference optimization on these future states, expert corrections are propagated into the safety-critical regions where the agent is expected to explore, significantly improving learning efficiency and reducing human demonstrations needed. We evaluate our approach with experiments on both autonomous driving and robotic manipulation benchmarks and demonstrate its efficiency and generality. Our theoretical analysis further shows that selecting an appropriate preference horizon L balances coverage of risky states with label correctness, thereby bounding the algorithmic optimality gap. Demo and code are available at: https://metadriverse.github.io/ppl
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IoDResearch: Deep Research on Private Heterogeneous Data via the Internet of Data</title>
<link>https://arxiv.org/abs/2510.01553</link>
<guid>https://arxiv.org/abs/2510.01553</guid>
<content:encoded><![CDATA[
arXiv:2510.01553v1 Announce Type: new 
Abstract: The rapid growth of multi-source, heterogeneous, and multimodal scientific data has increasingly exposed the limitations of traditional data management. Most existing DeepResearch (DR) efforts focus primarily on web search while overlooking local private data. Consequently, these frameworks exhibit low retrieval efficiency for private data and fail to comply with the FAIR principles, ultimately resulting in inefficiency and limited reusability. To this end, we propose IoDResearch (Internet of Data Research), a private data-centric Deep Research framework that operationalizes the Internet of Data paradigm. IoDResearch encapsulates heterogeneous resources as FAIR-compliant digital objects, and further refines them into atomic knowledge units and knowledge graphs, forming a heterogeneous graph index for multi-granularity retrieval. On top of this representation, a multi-agent system supports both reliable question answering and structured scientific report generation. Furthermore, we establish the IoD DeepResearch Benchmark to systematically evaluate both data representation and Deep Research capabilities in IoD scenarios. Experimental results on retrieval, QA, and report-writing tasks show that IoDResearch consistently surpasses representative RAG and Deep Research baselines. Overall, IoDResearch demonstrates the feasibility of private-data-centric Deep Research under the IoD paradigm, paving the way toward more trustworthy, reusable, and automated scientific discovery.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InvThink: Towards AI Safety via Inverse Reasoning</title>
<link>https://arxiv.org/abs/2510.01569</link>
<guid>https://arxiv.org/abs/2510.01569</guid>
<content:encoded><![CDATA[
arXiv:2510.01569v1 Announce Type: new 
Abstract: We present InvThink, a simple yet powerful approach that gives large language models (LLMs) the capability of inverse thinking: reasoning through failure modes before generating responses. Unlike existing safety alignment methods that optimize directly for safe response, InvThink instructs models to 1) enumerate potential harms, 2) analyze their consequences, and 3) generate safe outputs that proactively avoid these risks. Our method reveals three key findings: (i) safety improvements show stronger scaling with model size compared to existing safety methods. (ii) InvThink mitigates safety tax; by training models to systematically consider failure modes, it preserves general reasoning capabilities on standard benchmarks. (iii) beyond general safety tasks, InvThink excels in high-stakes domains including external-facing (medicine, finance, law) and agentic (blackmail, murder) risk scenarios, achieving up to 15.7% reduction in harmful responses compared to baseline methods like SafetyPrompt. We further implement InvThink via supervised fine-tuning, and reinforcement learning across three LLM families. These results suggest that inverse reasoning provides a scalable and generalizable path toward safer, more capable language models.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.01586</link>
<guid>https://arxiv.org/abs/2510.01586</guid>
<content:encoded><![CDATA[
arXiv:2510.01586v1 Announce Type: new 
Abstract: LLM-based multi-agent systems excel at planning, tool use, and role coordination, but their openness and interaction complexity also expose them to jailbreak, prompt-injection, and adversarial collaboration. Existing defenses fall into two lines: (i) self-verification that asks each agent to pre-filter unsafe instructions before execution, and (ii) external guard modules that police behaviors. The former often underperforms because a standalone agent lacks sufficient capacity to detect cross-agent unsafe chains and delegation-induced risks; the latter increases system overhead and creates a single-point-of-failure-once compromised, system-wide safety collapses, and adding more guards worsens cost and complexity. To solve these challenges, we propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning framework that internalizes safety into task agents. Rather than relying on external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize evolving jailbreak prompts) and defenders (task agents trained to both accomplish their duties and resist attacks) in adversarial learning environments. To stabilize learning and foster cooperation, we introduce a public baseline for advantage estimation: agents within the same functional group share a group-level mean-return baseline, enabling lower-variance updates and stronger intra-group coordination. Across representative attack scenarios, AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas baselines reach up to 38.33%, while preserving-and sometimes improving-task accuracy (up to +3.67% on reasoning tasks). These results show that safety and utility can be jointly improved without relying on extra guard agents or added system overhead.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentRec: Next-Generation LLM-Powered Multi-Agent Collaborative Recommendation with Adaptive Intelligence</title>
<link>https://arxiv.org/abs/2510.01609</link>
<guid>https://arxiv.org/abs/2510.01609</guid>
<content:encoded><![CDATA[
arXiv:2510.01609v1 Announce Type: new 
Abstract: Interactive conversational recommender systems have gained significant attention for their ability to capture user preferences through natural language interactions. However, existing approaches face substantial challenges in handling dynamic user preferences, maintaining conversation coherence, and balancing multiple ranking objectives simultaneously. This paper introduces AgentRec, a next-generation LLM-powered multi-agent collaborative recommendation framework that addresses these limitations through hierarchical agent networks with adaptive intelligence. Our approach employs specialized LLM-powered agents for conversation understanding, preference modeling, context awareness, and dynamic ranking, coordinated through an adaptive weighting mechanism that learns from interaction patterns. We propose a three-tier learning strategy combining rapid response for simple queries, intelligent reasoning for complex preferences, and deep collaboration for challenging scenarios. Extensive experiments on three real-world datasets demonstrate that AgentRec achieves consistent improvements over state-of-the-art baselines, with 2.8\% enhancement in conversation success rate, 1.9\% improvement in recommendation accuracy (NDCG@10), and 3.2\% better conversation efficiency while maintaining comparable computational costs through intelligent agent coordination.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMAS: Adaptively Determining Communication Topology for LLM-based Multi-Agent System</title>
<link>https://arxiv.org/abs/2510.01617</link>
<guid>https://arxiv.org/abs/2510.01617</guid>
<content:encoded><![CDATA[
arXiv:2510.01617v1 Announce Type: new 
Abstract: Although large language models (LLMs) have revolutionized natural language processing capabilities, their practical implementation as autonomous multi-agent systems (MAS) for industrial problem-solving encounters persistent barriers. Conventional MAS architectures are fundamentally restricted by inflexible, hand-crafted graph topologies that lack contextual responsiveness, resulting in diminished efficacy across varied academic and commercial workloads. To surmount these constraints, we introduce AMAS, a paradigm-shifting framework that redefines LLM-based MAS through a novel dynamic graph designer. This component autonomously identifies task-specific optimal graph configurations via lightweight LLM adaptation, eliminating the reliance on monolithic, universally applied structural templates. Instead, AMAS exploits the intrinsic properties of individual inputs to intelligently direct query trajectories through task-optimized agent pathways. Rigorous validation across question answering, mathematical deduction, and code generation benchmarks confirms that AMAS systematically exceeds state-of-the-art single-agent and multi-agent approaches across diverse LLM architectures. Our investigation establishes that context-sensitive structural adaptability constitutes a foundational requirement for high-performance LLM MAS deployments.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIMIC: Integrating Diverse Personality Traits for Better Game Testing Using Large Language Model</title>
<link>https://arxiv.org/abs/2510.01635</link>
<guid>https://arxiv.org/abs/2510.01635</guid>
<content:encoded><![CDATA[
arXiv:2510.01635v1 Announce Type: new 
Abstract: Modern video games pose significant challenges for traditional automated testing algorithms, yet intensive testing is crucial to ensure game quality. To address these challenges, researchers designed gaming agents using Reinforcement Learning, Imitation Learning, or Large Language Models. However, these agents often neglect the diverse strategies employed by human players due to their different personalities, resulting in repetitive solutions in similar situations. Without mimicking varied gaming strategies, these agents struggle to trigger diverse in-game interactions or uncover edge cases.
  In this paper, we present MIMIC, a novel framework that integrates diverse personality traits into gaming agents, enabling them to adopt different gaming strategies for similar situations. By mimicking different playstyles, MIMIC can achieve higher test coverage and richer in-game interactions across different games. It also outperforms state-of-the-art agents in Minecraft by achieving a higher task completion rate and providing more diverse solutions. These results highlight MIMIC's significant potential for effective game testing.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Privacy Is Not Just Memorization!</title>
<link>https://arxiv.org/abs/2510.01645</link>
<guid>https://arxiv.org/abs/2510.01645</guid>
<content:encoded><![CDATA[
arXiv:2510.01645v1 Announce Type: new 
Abstract: The discourse on privacy risks in Large Language Models (LLMs) has disproportionately focused on verbatim memorization of training data, while a constellation of more immediate and scalable privacy threats remain underexplored. This position paper argues that the privacy landscape of LLM systems extends far beyond training data extraction, encompassing risks from data collection practices, inference-time context leakage, autonomous agent capabilities, and the democratization of surveillance through deep inference attacks. We present a comprehensive taxonomy of privacy risks across the LLM lifecycle -- from data collection through deployment -- and demonstrate through case studies how current privacy frameworks fail to address these multifaceted threats. Through a longitudinal analysis of 1,322 AI/ML privacy papers published at leading conferences over the past decade (2016--2025), we reveal that while memorization receives outsized attention in technical research, the most pressing privacy harms lie elsewhere, where current technical approaches offer little traction and viable paths forward remain unclear. We call for a fundamental shift in how the research community approaches LLM privacy, moving beyond the narrow focus of current technical solutions and embracing interdisciplinary approaches that address the sociotechnical nature of these emerging threats.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoK: Measuring What Matters for Closed-Loop Security Agents</title>
<link>https://arxiv.org/abs/2510.01654</link>
<guid>https://arxiv.org/abs/2510.01654</guid>
<content:encoded><![CDATA[
arXiv:2510.01654v1 Announce Type: new 
Abstract: Cybersecurity is a relentless arms race, with AI driven offensive systems evolving faster than traditional defenses can adapt. Research and tooling remain fragmented across isolated defensive functions, creating blind spots that adversaries exploit. Autonomous agents capable of integrating, exploit confirmation, remediation, and validation into a single closed loop offer promise, but the field lacks three essentials: a framework defining the agentic capabilities of security systems across security life cycle, a principled method for evaluating closed loop agents, and a benchmark for measuring their performance in practice. We introduce CLASP: the Closed-Loop Autonomous Security Performance framework which aligns the security lifecycle (reconnaissance, exploitation, root cause analysis, patch synthesis, validation) with core agentic capabilities (planning, tool use, memory, reasoning, reflection & perception) providing a common vocabulary and rubric for assessing agentic capabilities in security tasks. By applying CLASP to 21 representative works, we map where systems demonstrate strengths, and where capability gaps persist. We then define the Closed-Loop Capability (CLC) Score, a composite metric quantifying both degree of loop closure and operational effectiveness, and outline the requirements for a closed loop benchmark. Together, CLASP and the CLC Score, provide the vocabulary, diagnostics, and measurements needed to advance both function level performance and measure closed loop security agents.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GuruAgents: Emulating Wise Investors with Prompt-Guided LLM Agents</title>
<link>https://arxiv.org/abs/2510.01664</link>
<guid>https://arxiv.org/abs/2510.01664</guid>
<content:encoded><![CDATA[
arXiv:2510.01664v1 Announce Type: new 
Abstract: This study demonstrates that GuruAgents, prompt-guided AI agents, can systematically operationalize the strategies of legendary investment gurus. We develop five distinct GuruAgents, each designed to emulate an iconic investor, by encoding their distinct philosophies into LLM prompts that integrate financial tools and a deterministic reasoning pipeline. In a backtest on NASDAQ-100 constituents from Q4 2023 to Q2 2025, the GuruAgents exhibit unique behaviors driven by their prompted personas. The Buffett GuruAgent achieves the highest performance, delivering a 42.2\% CAGR that significantly outperforms benchmarks, while other agents show varied results. These findings confirm that prompt engineering can successfully translate the qualitative philosophies of investment gurus into reproducible, quantitative strategies, highlighting a novel direction for automated systematic investing. The source code and data are available at https://github.com/yejining99/GuruAgents.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness</title>
<link>https://arxiv.org/abs/2510.01670</link>
<guid>https://arxiv.org/abs/2510.01670</guid>
<content:encoded><![CDATA[
arXiv:2510.01670v1 Announce Type: new 
Abstract: Computer-Use Agents (CUAs) are an increasingly deployed class of agents that take actions on GUIs to accomplish user goals. In this paper, we show that CUAs consistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals regardless of feasibility, safety, reliability, or context. We characterize three prevalent patterns of BGD: (i) lack of contextual reasoning, (ii) assumptions and decisions under ambiguity, and (iii) contradictory or infeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these three patterns. Built on OSWorld, BLIND-ACT provides realistic environments and employs LLM-based judges to evaluate agent behavior, achieving 93.75% agreement with human annotations. We use BLIND-ACT to evaluate nine frontier models, including Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing high average BGD rates (80.8%) across them. We show that BGD exposes subtle risks that arise even when inputs are not directly harmful. While prompting-based interventions lower BGD levels, substantial risk persists, highlighting the need for stronger training- or inference-time interventions. Qualitative analysis reveals observed failure modes: execution-first bias (focusing on how to act over whether to act), thought-action disconnect (execution diverging from reasoning), and request-primacy (justifying actions due to user request). Identifying BGD and introducing BLIND-ACT establishes a foundation for future research on studying and mitigating this fundamental risk and ensuring safe CUA deployment.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentive Analysis of Collusion in Fair Division</title>
<link>https://arxiv.org/abs/2510.01689</link>
<guid>https://arxiv.org/abs/2510.01689</guid>
<content:encoded><![CDATA[
arXiv:2510.01689v1 Announce Type: new 
Abstract: We study fair division problems with strategic agents capable of gaining advantages by manipulating their reported preferences. Although several impossibility results have revealed the incompatibility of truthfulness with standard fairness criteria, subsequent works have circumvented this limitation through the incentive ratio framework. Previous studies demonstrate that fundamental mechanisms like Maximum Nash Welfare (MNW) and Probabilistic Serial (PS) for divisible goods, and Round-Robin (RR) for indivisible goods achieve an incentive ratio of $2$, implying that no individual agent can gain more than double his truthful utility through manipulation. However, collusive manipulation by agent groups remains unexplored.
  In this work, we define strong group incentive ratio (SGIR) and group incentive ratio (GIR) to measure the gain of collusive manipulation, where SGIR and GIR are respectively the maximum and minimum of the incentive ratios of corrupted agents. Then, we tightly characterize the SGIRs and GIRs of MNW, PS, and RR. In particular, the GIR of MNW is $2$ regardless of the coalition size. Moreover, for coalition size $c \geq 1$, the SGIRs of MNW and PS, and the GIRs of PS and RR are $c + 1$. Finally, the SGIR of RR is unbounded for coalition size $c \geq 2$. Our results reveal fundamental differences of these three mechanisms in their vulnerability to collusion.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaboT: AI-based agent for natural language-based interaction with metabolomics knowledge graphs</title>
<link>https://arxiv.org/abs/2510.01724</link>
<guid>https://arxiv.org/abs/2510.01724</guid>
<content:encoded><![CDATA[
arXiv:2510.01724v1 Announce Type: new 
Abstract: Mass spectrometry metabolomics generates vast amounts of data requiring advanced methods for interpretation. Knowledge graphs address these challenges by structuring mass spectrometry data, metabolite information, and their relationships into a connected network (Gaudry et al. 2024). However, effective use of a knowledge graph demands an in-depth understanding of its ontology and its query language syntax. To overcome this, we designed MetaboT, an AI system utilizing large language models (LLMs) to translate user questions into SPARQL semantic query language for operating on knowledge graphs (Steve Harris 2013). We demonstrate its effectiveness using the Experimental Natural Products Knowledge Graph (ENPKG), a large-scale public knowledge graph for plant natural products (Gaudry et al. 2024).MetaboT employs specialized AI agents for handling user queries and interacting with the knowledge graph by breaking down complex tasks into discrete components, each managed by a specialised agent (Fig. 1a). The multi-agent system is constructed using the LangChain and LangGraph libraries, which facilitate the integration of LLMs with external tools and information sources (LangChain, n.d.). The query generation process follows a structured workflow. First, the Entry Agent determines if the question is new or a follow-up to previous interactions. New questions are forwarded to the Validator Agent, which verifies if the question is related to the knowledge graph. Then, the valid question is sent to the Supervisor Agent, which identifies if the question requires chemical conversions or standardized identifiers. In this case it delegates the question to the Knowledge Graph Agent, which can use tools to extract necessary details, such as URIs or taxonomies of chemical names, from the user query. Finally, an agent responsible for crafting the SPARQL queries equipped with the ontology of the knowledge graph uses the provided identifiers to generate the query. Then, the system executes the generated query against the metabolomics knowledge graph and returns structured results to the user (Fig. 1b). To assess the performance of MetaboT we have curated 50 metabolomics-related questions and their expected answers. In addition to submitting these questions to MetaboT, we evaluated a baseline by submitting them to a standard LLM (GPT-4o) with a prompt that incorporated the knowledge graph ontology but did not provide specific entity IDs. This baseline achieved only 8.16% accuracy, compared to MetaboT's 83.67%, underscoring the necessity of our multi-agent system for accurately retrieving entities and generating correct SPARQL queries. MetaboT demonstrates promising performance as a conversational question-answering assistant, enabling researchers to retrieve structured metabolomics data through natural language queries. By automating the generation and execution of SPARQL queries, it removes technical barriers that have traditionally hindered access to knowledge graphs. Importantly, MetaboT leverages the capabilities of LLMs while maintaining experimentally grounded query generation, ensuring that outputs remain aligned with domain-specific standards and data structures. This approach facilitates data-driven discoveries by bridging the gap between complex semantic technologies and user-friendly interaction. MetaboT is accessible at [https://metabot.holobiomicslab.eu/], and its source code is available at [https://github.com/HolobiomicsLab/MetaboT].
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A cybersecurity AI agent selection and decision support framework</title>
<link>https://arxiv.org/abs/2510.01751</link>
<guid>https://arxiv.org/abs/2510.01751</guid>
<content:encoded><![CDATA[
arXiv:2510.01751v1 Announce Type: new 
Abstract: This paper presents a novel, structured decision support framework that systematically aligns diverse artificial intelligence (AI) agent architectures, reactive, cognitive, hybrid, and learning, with the comprehensive National Institute of Standards and Technology (NIST) Cybersecurity Framework (CSF) 2.0. By integrating agent theory with industry guidelines, this framework provides a transparent and stepwise methodology for selecting and deploying AI solutions to address contemporary cyber threats. Employing a granular decomposition of NIST CSF 2.0 functions into specific tasks, the study links essential AI agent properties such as autonomy, adaptive learning, and real-time responsiveness to each subcategory's security requirements. In addition, it outlines graduated levels of autonomy (assisted, augmented, and fully autonomous) to accommodate organisations at varying stages of cybersecurity maturity. This holistic approach transcends isolated AI applications, providing a unified detection, incident response, and governance strategy. Through conceptual validation, the framework demonstrates how tailored AI agent deployments can align with real-world constraints and risk profiles, enhancing situational awareness, accelerating response times, and fortifying long-term resilience via adaptive risk management. Ultimately, this research bridges the gap between theoretical AI constructs and operational cybersecurity demands, establishing a foundation for robust, empirically validated multi-agent systems that adhere to industry standards.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Octax: Accelerated CHIP-8 Arcade Environments for Reinforcement Learning in JAX</title>
<link>https://arxiv.org/abs/2510.01764</link>
<guid>https://arxiv.org/abs/2510.01764</guid>
<content:encoded><![CDATA[
arXiv:2510.01764v1 Announce Type: new 
Abstract: Reinforcement learning (RL) research requires diverse, challenging environments that are both tractable and scalable. While modern video games may offer rich dynamics, they are computationally expensive and poorly suited for large-scale experimentation due to their CPU-bound execution. We introduce Octax, a high-performance suite of classic arcade game environments implemented in JAX, based on CHIP-8 emulation, a predecessor to Atari, which is widely adopted as a benchmark in RL research. Octax provides the JAX community with a long-awaited end-to-end GPU alternative to the Atari benchmark, offering image-based environments, spanning puzzle, action, and strategy genres, all executable at massive scale on modern GPUs. Our JAX-based implementation achieves orders-of-magnitude speedups over traditional CPU emulators while maintaining perfect fidelity to the original game mechanics. We demonstrate Octax's capabilities by training RL agents across multiple games, showing significant improvements in training speed and scalability compared to existing solutions. The environment's modular design enables researchers to easily extend the suite with new games or generate novel environments using large language models, making it an ideal platform for large-scale RL experimentation.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Anytime, Scalable and Complete Algorithm for Embedding a Manufacturing Procedure in a Smart Factory</title>
<link>https://arxiv.org/abs/2510.01770</link>
<guid>https://arxiv.org/abs/2510.01770</guid>
<content:encoded><![CDATA[
arXiv:2510.01770v1 Announce Type: new 
Abstract: Modern automated factories increasingly run manufacturing procedures using a matrix of programmable machines, such as 3D printers, interconnected by a programmable transport system, such as a fleet of tabletop robots. To embed a manufacturing procedure into a smart factory, an operator must: (a) assign each of its processes to a machine and (b) specify how agents should transport parts between machines. The problem of embedding a manufacturing process into a smart factory is termed the Smart Factory Embedding (SFE) problem. State-of-the-art SFE solvers can only scale to factories containing a couple dozen machines. Modern smart factories, however, may contain hundreds of machines. We fill this hole by introducing the first highly scalable solution to the SFE, TS-ACES, the Traffic System based Anytime Cyclic Embedding Solver. We show that TS-ACES is complete and can scale to SFE instances based on real industrial scenarios with more than a hundred machines.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Secure Multi-Modal Data Fusion in Federated Digital Health Systems via MCP</title>
<link>https://arxiv.org/abs/2510.01780</link>
<guid>https://arxiv.org/abs/2510.01780</guid>
<content:encoded><![CDATA[
arXiv:2510.01780v1 Announce Type: new 
Abstract: Secure and interoperable integration of heterogeneous medical data remains a grand challenge in digital health. Current federated learning (FL) frameworks offer privacy-preserving model training but lack standardized mechanisms to orchestrate multi-modal data fusion across distributed and resource-constrained environments. This study introduces a novel framework that leverages the Model Context Protocol (MCP) as an interoperability layer for secure, cross-agent communication in multi-modal federated healthcare systems. The proposed architecture unifies three pillars: (i) multi-modal feature alignment for clinical imaging, electronic medical records, and wearable IoT data; (ii) secure aggregation with differential privacy to protect patient-sensitive updates; and (iii) energy-aware scheduling to mitigate dropouts in mobile clients. By employing MCP as a schema-driven interface, the framework enables adaptive orchestration of AI agents and toolchains while ensuring compliance with privacy regulations. Experimental evaluation on benchmark datasets and pilot clinical cohorts demonstrates up to 9.8\% improvement in diagnostic accuracy compared with baseline FL, a 54\% reduction in client dropout rates, and clinically acceptable privacy--utility trade-offs. These results highlight MCP-enabled multi-modal fusion as a scalable and trustworthy pathway toward equitable, next-generation federated health infrastructures.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-AI Teaming Co-Learning in Military Operations</title>
<link>https://arxiv.org/abs/2510.01815</link>
<guid>https://arxiv.org/abs/2510.01815</guid>
<content:encoded><![CDATA[
arXiv:2510.01815v1 Announce Type: new 
Abstract: In a time of rapidly evolving military threats and increasingly complex operational environments, the integration of AI into military operations proves significant advantages. At the same time, this implies various challenges and risks regarding building and deploying human-AI teaming systems in an effective and ethical manner. Currently, understanding and coping with them are often tackled from an external perspective considering the human-AI teaming system as a collective agent. Nevertheless, zooming into the dynamics involved inside the system assures dealing with a broader palette of relevant multidimensional responsibility, safety, and robustness aspects. To this end, this research proposes the design of a trustworthy co-learning model for human-AI teaming in military operations that encompasses a continuous and bidirectional exchange of insights between the human and AI agents as they jointly adapt to evolving battlefield conditions. It does that by integrating four dimensions. First, adjustable autonomy for dynamically calibrating the autonomy levels of agents depending on aspects like mission state, system confidence, and environmental uncertainty. Second, multi-layered control which accounts continuous oversight, monitoring of activities, and accountability. Third, bidirectional feedback with explicit and implicit feedback loops between the agents to assure a proper communication of reasoning, uncertainties, and learned adaptations that each of the agents has. And fourth, collaborative decision-making which implies the generation, evaluation, and proposal of decisions associated with confidence levels and rationale behind them. The model proposed is accompanied by concrete exemplifications and recommendations that contribute to further developing responsible and trustworthy human-AI teaming systems in military operations.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Matters in RL-Based Methods for Object-Goal Navigation? An Empirical Study and A Unified Framework</title>
<link>https://arxiv.org/abs/2510.01830</link>
<guid>https://arxiv.org/abs/2510.01830</guid>
<content:encoded><![CDATA[
arXiv:2510.01830v1 Announce Type: new 
Abstract: Object-Goal Navigation (ObjectNav) is a critical component toward deploying mobile robots in everyday, uncontrolled environments such as homes, schools, and workplaces. In this context, a robot must locate target objects in previously unseen environments using only its onboard perception. Success requires the integration of semantic understanding, spatial reasoning, and long-horizon planning, which is a combination that remains extremely challenging. While reinforcement learning (RL) has become the dominant paradigm, progress has spanned a wide range of design choices, yet the field still lacks a unifying analysis to determine which components truly drive performance. In this work, we conduct a large-scale empirical study of modular RL-based ObjectNav systems, decomposing them into three key components: perception, policy, and test-time enhancement. Through extensive controlled experiments, we isolate the contribution of each and uncover clear trends: perception quality and test-time strategies are decisive drivers of performance, whereas policy improvements with current methods yield only marginal gains. Building on these insights, we propose practical design guidelines and demonstrate an enhanced modular system that surpasses State-of-the-Art (SotA) methods by 6.6% on SPL and by a 2.7% success rate. We also introduce a human baseline under identical conditions, where experts achieve an average 98% success, underscoring the gap between RL agents and human-level navigation. Our study not only sets the SotA performance but also provides principled guidance for future ObjectNav development and evaluation.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-Hoc Predictions in AutoML: Leveraging LLMs to Enhance Model Selection and Benchmarking for Tabular datasets</title>
<link>https://arxiv.org/abs/2510.01842</link>
<guid>https://arxiv.org/abs/2510.01842</guid>
<content:encoded><![CDATA[
arXiv:2510.01842v1 Announce Type: new 
Abstract: The field of AutoML has made remarkable progress in post-hoc model selection, with libraries capable of automatically identifying the most performing models for a given dataset. Nevertheless, these methods often rely on exhaustive hyperparameter searches, where methods automatically train and test different types of models on the target dataset. Contrastingly, pre-hoc prediction emerges as a promising alternative, capable of bypassing exhaustive search through intelligent pre-selection of models. Despite its potential, pre-hoc prediction remains under-explored in the literature. This paper explores the intersection of AutoML and pre-hoc model selection by leveraging traditional models and Large Language Model (LLM) agents to reduce the search space of AutoML libraries. By relying on dataset descriptions and statistical information, we reduce the AutoML search space. Our methodology is applied to the AWS AutoGluon portfolio dataset, a state-of-the-art AutoML benchmark containing 175 tabular classification datasets available on OpenML. The proposed approach offers a shift in AutoML workflows, significantly reducing computational overhead, while still selecting the best model for the given dataset.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TACOS: Task Agnostic COordinator of a multi-drone System</title>
<link>https://arxiv.org/abs/2510.01869</link>
<guid>https://arxiv.org/abs/2510.01869</guid>
<content:encoded><![CDATA[
arXiv:2510.01869v1 Announce Type: new 
Abstract: When a single pilot is responsible for managing a multi-drone system, the task demands varying levels of autonomy, from direct control of individual UAVs, to group-level coordination, to fully autonomous swarm behaviors for accomplishing high-level tasks. Enabling such flexible interaction requires a framework that supports multiple modes of shared autonomy. As language models continue to improve in reasoning and planning, they provide a natural foundation for such systems, reducing pilot workload by enabling high-level task delegation through intuitive, language-based interfaces. In this paper we present TACOS (Task-Agnostic COordinator of a multi-drone System), a unified framework that enables high-level natural language control of multi-UAV systems through Large Language Models (LLMs). TACOS integrates three key capabilities into a single architecture: a one-to-many natural language interface for intuitive user interaction, an intelligent coordinator for translating user intent into structured task plans, and an autonomous agent that executes plans interacting with the real-world. TACOS allows a LLM to interact with a library of executable APIs, bridging semantic reasoning with real-time multi-robot coordination. We demonstrate the system in real-world multi-drone system and conduct an ablation study to assess the contribution of each module.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coordinated Car-following Using Distributed MPC</title>
<link>https://arxiv.org/abs/2510.02010</link>
<guid>https://arxiv.org/abs/2510.02010</guid>
<content:encoded><![CDATA[
arXiv:2510.02010v1 Announce Type: new 
Abstract: Within the modeling framework of Markov games, we propose a series of algorithms for coordinated car-following using distributed model predictive control (DMPC). Instead of tracking prescribed feasible trajectories, driving policies are solved directly as outcomes of the DMPC optimization given the driver's perceivable states. The coordinated solutions are derived using the best response dynamics via iterated self-play, and are facilitated by direct negotiation using inter-agent or agent-infrastructure communication. These solutions closely approximate either Nash equilibrium or centralized optimization. By re-parameterizing the action sequence in DMPC as a curve along the planning horizon, we are able to systematically reduce the original DMPC to very efficient grid searches such that the optimal solution to the original DMPC can be well executed in real-time. Within our modeling framework, it is natural to cast traffic control problems as mechanism design problems, in which all agents are endogenized on an equal footing with full incentive compatibility. We show how traffic efficiency can be dramatically improved while keeping stop-and-go phantom waves tamed at high vehicle densities. Our approach can be viewed as an alternative way to formulate coordinated adaptive cruise control (CACC) without an explicit platooning (or with all vehicles in the traffic system treated as a single extended platoon). We also address the issue of linear stability of the associated discrete-time traffic dynamics and demonstrate why it does not always tell the full story about the traffic stability.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage</title>
<link>https://arxiv.org/abs/2510.02044</link>
<guid>https://arxiv.org/abs/2510.02044</guid>
<content:encoded><![CDATA[
arXiv:2510.02044v1 Announce Type: new 
Abstract: End-to-end speech-in speech-out dialogue systems are emerging as a powerful alternative to traditional ASR-LLM-TTS pipelines, generating more natural, expressive responses with significantly lower latency. However, these systems remain prone to hallucinations due to limited factual grounding. While text-based dialogue systems address this challenge by integrating tools such as web search and knowledge graph APIs, we introduce the first approach to extend tool use directly into speech-in speech-out systems. A key challenge is that tool integration substantially increases response latency, disrupting conversational flow. To mitigate this, we propose Streaming Retrieval-Augmented Generation (Streaming RAG), a novel framework that reduces user-perceived latency by predicting tool queries in parallel with user speech, even before the user finishes speaking. Specifically, we develop a post-training pipeline that teaches the model when to issue tool calls during ongoing speech and how to generate spoken summaries that fuse audio queries with retrieved text results, thereby improving both accuracy and responsiveness. To evaluate our approach, we construct AudioCRAG, a benchmark created by converting queries from the publicly available CRAG dataset into speech form. Experimental results demonstrate that our streaming RAG approach increases QA accuracy by up to 200% relative (from 11.1% to 34.2% absolute) and further enhances user experience by reducing tool use latency by 20%. Importantly, our streaming RAG approach is modality-agnostic and can be applied equally to typed input, paving the way for more agentic, real-time AI assistants.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-group Bayesian Games</title>
<link>https://arxiv.org/abs/2510.02078</link>
<guid>https://arxiv.org/abs/2510.02078</guid>
<content:encoded><![CDATA[
arXiv:2510.02078v1 Announce Type: new 
Abstract: This paper presents a model of multi-group Bayesian games (MBGs) to describe the group behavior in Bayesian games, and gives methods to find (strongly) multi-group Bayesian Nash equilibria (MBNE) of this model with a proposed transformation. MBNE represent the optimal strategy \textit{profiles} under the situation where players within a group play a cooperative game, while strongly MBNE characterize the optimal strategy \textit{profiles} under the situation where players within a group play a noncooperative game. Firstly, we propose a model of MBGs and give a transformation to convert any MBG into a multi-group ex-ante agent game (MEAG) which is a normal-form game. Secondly, we give a sufficient and necessary condition for a MBG's MEAG to be (strongly) potential. If it is (strongly) potential, all its (strongly) Nash equilibria can be found, and then all (strongly) MBNE of the MBG can be obtained by leveraging the transformation's good properties. Finally, we provide algorithms for finding (strongly) MBNE of a MBG whose MEAG is (strongly) potential and use an illustrative example to verify the correctness of our results.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cooperative Guidance for Aerial Defense in Multiagent Systems</title>
<link>https://arxiv.org/abs/2510.02087</link>
<guid>https://arxiv.org/abs/2510.02087</guid>
<content:encoded><![CDATA[
arXiv:2510.02087v1 Announce Type: new 
Abstract: This paper addresses a critical aerial defense challenge in contested airspace, involving three autonomous aerial vehicles -- a hostile drone (the pursuer), a high-value drone (the evader), and a protective drone (the defender). We present a cooperative guidance framework for the evader-defender team that guarantees interception of the pursuer before it can capture the evader, even under highly dynamic and uncertain engagement conditions. Unlike traditional heuristic, optimal control, or differential game-based methods, we approach the problem within a time-constrained guidance framework, leveraging true proportional navigation based approach that ensures robust and guaranteed solutions to the aerial defense problem. The proposed strategy is computationally lightweight, scalable to a large number of agent configurations, and does not require knowledge of the pursuer's strategy or control laws. From arbitrary initial geometries, our method guarantees that key engagement errors are driven to zero within a fixed time, leading to a successful mission. Extensive simulations across diverse and adversarial scenarios confirm the effectiveness of the proposed strategy and its relevance for real-time autonomous defense in contested airspace environments.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning with Action-Triggered Observations</title>
<link>https://arxiv.org/abs/2510.02149</link>
<guid>https://arxiv.org/abs/2510.02149</guid>
<content:encoded><![CDATA[
arXiv:2510.02149v1 Announce Type: new 
Abstract: We study reinforcement learning problems where state observations are stochastically triggered by actions, a constraint common in many real-world applications. This framework is formulated as Action-Triggered Sporadically Traceable Markov Decision Processes (ATST-MDPs), where each action has a specified probability of triggering a state observation. We derive tailored Bellman optimality equations for this framework and introduce the action-sequence learning paradigm in which agents commit to executing a sequence of actions until the next observation arrives. Under the linear MDP assumption, value-functions are shown to admit linear representations in an induced action-sequence feature map. Leveraging this structure, we propose off-policy estimators with statistical error guarantees for such feature maps and introduce ST-LSVI-UCB, a variant of LSVI-UCB adapted for action-triggered settings. ST-LSVI-UCB achieves regret $\widetilde O(\sqrt{Kd^3(1-\gamma)^{-3}})$, where $K$ is the number of episodes, $d$ the feature dimension, and $\gamma$ the discount factor (per-step episode non-termination probability). Crucially, this work establishes the theoretical foundation for learning with sporadic, action-triggered observations while demonstrating that efficient learning remains feasible under such observation constraints.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Reasoning and Refinement through Semantic Interaction</title>
<link>https://arxiv.org/abs/2510.02157</link>
<guid>https://arxiv.org/abs/2510.02157</guid>
<content:encoded><![CDATA[
arXiv:2510.02157v1 Announce Type: new 
Abstract: Sensemaking report writing often requires multiple refinements in the iterative process. While Large Language Models (LLMs) have shown promise in generating initial reports based on human visual workspace representations, they struggle to precisely incorporate sequential semantic interactions during the refinement process. We introduce VIS-ReAct, a framework that reasons about newly-added semantic interactions in visual workspaces to steer the LLM for report refinement.
  VIS-ReAct is a two-agent framework: a primary LLM analysis agent interprets new semantic interactions to infer user intentions and generate refinement planning, followed by an LLM refinement agent that updates reports accordingly. Through case study, VIS-ReAct outperforms baseline and VIS-ReAct (without LLM analysis) on targeted refinement, semantic fidelity, and transparent inference. Results demonstrate that VIS-ReAct better handles various interaction types and granularities while enhancing the transparency of human-LLM collaboration.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIEVE: Towards Verifiable Certification for Code-datasets</title>
<link>https://arxiv.org/abs/2510.02166</link>
<guid>https://arxiv.org/abs/2510.02166</guid>
<content:encoded><![CDATA[
arXiv:2510.02166v1 Announce Type: new 
Abstract: Code agents and empirical software engineering rely on public code datasets, yet these datasets lack verifiable quality guarantees. Static 'dataset cards' inform, but they are neither auditable nor do they offer statistical guarantees, making it difficult to attest to dataset quality. Teams build isolated, ad-hoc cleaning pipelines. This fragments effort and raises cost. We present SIEVE, a community-driven framework. It turns per-property checks into Confidence Cards-machine-readable, verifiable certificates with anytime-valid statistical bounds. We outline a research plan to bring SIEVE to maturity, replacing narrative cards with anytime-verifiable certification. This shift is expected to lower quality-assurance costs and increase trust in code-datasets.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DisCo-Layout: Disentangling and Coordinating Semantic and Physical Refinement in a Multi-Agent Framework for 3D Indoor Layout Synthesis</title>
<link>https://arxiv.org/abs/2510.02178</link>
<guid>https://arxiv.org/abs/2510.02178</guid>
<content:encoded><![CDATA[
arXiv:2510.02178v1 Announce Type: new 
Abstract: 3D indoor layout synthesis is crucial for creating virtual environments. Traditional methods struggle with generalization due to fixed datasets. While recent LLM and VLM-based approaches offer improved semantic richness, they often lack robust and flexible refinement, resulting in suboptimal layouts. We develop DisCo-Layout, a novel framework that disentangles and coordinates physical and semantic refinement. For independent refinement, our Semantic Refinement Tool (SRT) corrects abstract object relationships, while the Physical Refinement Tool (PRT) resolves concrete spatial issues via a grid-matching algorithm. For collaborative refinement, a multi-agent framework intelligently orchestrates these tools, featuring a planner for placement rules, a designer for initial layouts, and an evaluator for assessment. Experiments demonstrate DisCo-Layout's state-of-the-art performance, generating realistic, coherent, and generalizable 3D indoor layouts. Our code will be publicly available.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen Using Agentic AI</title>
<link>https://arxiv.org/abs/2510.02185</link>
<guid>https://arxiv.org/abs/2510.02185</guid>
<content:encoded><![CDATA[
arXiv:2510.02185v1 Announce Type: new 
Abstract: Fuzz testing has become a cornerstone technique for identifying software bugs and security vulnerabilities, with broad adoption in both industry and open-source communities. Directly fuzzing a function requires fuzz drivers, which translate random fuzzer inputs into valid arguments for the target function. Given the cost and expertise required to manually develop fuzz drivers, methods exist that leverage program analysis and Large Language Models to automatically generate these drivers. However, the generated fuzz drivers frequently lead to false positive crashes, especially in functions highly structured input and complex state requirements. This problem is especially crucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as reporting false positive crashes to maintainers impede trust in both the system and the team.
  This paper presents two AI-driven strategies to reduce false positives in OSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First, constraint-based fuzz driver generation proactively enforces constraints on a function's inputs and state to guide driver creation. Second, context-based crash validation reactively analyzes function callers to determine whether reported crashes are feasible from program entry points. Using 1,500 benchmark functions from OSS-Fuzz, we show that these strategies reduce spurious crashes by up to 8%, cut reported crashes by more than half, and demonstrate that frontier LLMs can serve as reliable program analysis agents. Our results highlight the promise and challenges of integrating AI into large-scale fuzzing pipelines.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports</title>
<link>https://arxiv.org/abs/2510.02190</link>
<guid>https://arxiv.org/abs/2510.02190</guid>
<content:encoded><![CDATA[
arXiv:2510.02190v1 Announce Type: new 
Abstract: Artificial intelligence is undergoing the paradigm shift from closed language models to interconnected agent systems capable of external perception and information integration. As a representative embodiment, Deep Research Agents (DRAs) systematically exhibit the capabilities for task decomposition, cross-source retrieval, multi-stage reasoning, and structured output, which markedly enhance performance on complex and open-ended tasks. However, existing benchmarks remain deficient in evaluation dimensions, response formatting, and scoring mechanisms, limiting their capacity to assess such systems effectively. This paper introduces a rigorous benchmark and a multidimensional evaluation framework tailored to DRAs and report-style responses. The benchmark comprises 214 expert-curated challenging queries distributed across 10 broad thematic domains, each accompanied by manually constructed reference bundles to support composite evaluation. The framework enables comprehensive evaluation of long-form reports generated by DRAs, incorporating integrated scoring metrics for semantic quality, topical focus, and retrieval trustworthiness. Extensive experimentation confirms the superior performance of mainstream DRAs over web-search-tool-augmented reasoning models, yet reveals considerable scope for further improvement. This study provides a robust foundation for capability assessment, architectural refinement, and paradigm advancement in DRA systems.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge Graph Exploration Utilities</title>
<link>https://arxiv.org/abs/2510.02200</link>
<guid>https://arxiv.org/abs/2510.02200</guid>
<content:encoded><![CDATA[
arXiv:2510.02200v1 Announce Type: new 
Abstract: Interacting with knowledge graphs can be a daunting task for people without a background in computer science since the query language that is used (SPARQL) has a high barrier of entry. Large language models (LLMs) can lower that barrier by providing support in the form of Text2SPARQL translation. In this paper we introduce a generalized method based on SPINACH, an LLM backed agent that translates natural language questions to SPARQL queries not in a single shot, but as an iterative process of exploration and execution. We describe the overall architecture and reasoning behind our design decisions, and also conduct a thorough analysis of the agent behavior to gain insights into future areas for targeted improvements. This work was motivated by the Text2SPARQL challenge, a challenge that was held to facilitate improvements in the Text2SPARQL domain.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Say One Thing, Do Another? Diagnosing Reasoning-Execution Gaps in VLM-Powered Mobile-Use Agents</title>
<link>https://arxiv.org/abs/2510.02204</link>
<guid>https://arxiv.org/abs/2510.02204</guid>
<content:encoded><![CDATA[
arXiv:2510.02204v1 Announce Type: new 
Abstract: Mobile-use agents powered by vision-language models (VLMs) have shown great potential in interpreting natural language instructions and generating corresponding actions based on mobile graphical user interface. Recent studies suggest that incorporating chain-of-thought (CoT) reasoning tends to improve the execution accuracy. However, existing evaluations emphasize execution accuracy while neglecting whether CoT reasoning aligns with ground-truth actions. This oversight fails to assess potential reasoning-execution gaps, which in turn foster over-trust: users relying on seemingly plausible CoTs may unknowingly authorize harmful actions, potentially resulting in financial loss or trust crisis. In this work, we introduce a new evaluation framework to diagnose reasoning-execution gaps. At its core lies Ground-Truth Alignment (GTA), which measures whether the action implied by a CoT matches the ground-truth action. By combining GTA with the standard Exact Match (EM) metric, we jointly assess both the reasoning accuracy and execution accuracy. This joint perspective reveals two types of reasoning-execution gaps: (i) Execution Gap (EG), where the reasoning correctly identifies the correct action but execution fails, and (ii) Reasoning Gap (RG), where execution succeeds but reasoning process conflicts with the actual execution. Experimental results across a wide range of mobile interaction tasks reveal that reasoning-execution gaps are prevalent, with execution gaps occurring more frequently than reasoning gaps. Moreover, while scaling up model size reduces the overall gap, sizable execution gaps persist even in the largest models. Further analysis shows that our framework reliably reflects systematic EG/RG patterns in state-of-the-art models. These findings offer concrete diagnostics and support the development of more trustworthy mobile-use agents.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?</title>
<link>https://arxiv.org/abs/2510.02209</link>
<guid>https://arxiv.org/abs/2510.02209</guid>
<content:encoded><![CDATA[
arXiv:2510.02209v1 Announce Type: new 
Abstract: Large language models (LLMs) have recently demonstrated strong capabilities as autonomous agents, showing promise in reasoning, tool use, and sequential decision-making. While prior benchmarks have evaluated LLM agents in domains such as software engineering and scientific discovery, the finance domain remains underexplored, despite its direct relevance to economic value and high-stakes decision-making. Existing financial benchmarks primarily test static knowledge through question answering, but they fall short of capturing the dynamic and iterative nature of trading. To address this gap, we introduce StockBench, a contamination-free benchmark designed to evaluate LLM agents in realistic, multi-month stock trading environments. Agents receive daily market signals -- including prices, fundamentals, and news -- and must make sequential buy, sell, or hold decisions. Performance is assessed using financial metrics such as cumulative return, maximum drawdown, and the Sortino ratio. Our evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM agents struggle to outperform the simple buy-and-hold baseline, several models demonstrate the potential to deliver higher returns and manage risk more effectively. These findings highlight both the challenges and opportunities in developing LLM-powered financial agents, showing that excelling at static financial knowledge tasks does not necessarily translate into successful trading strategies. We release StockBench as an open-source resource to support reproducibility and advance future research in this domain.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Unreasonable Effectiveness of Scaling Agents for Computer Use</title>
<link>https://arxiv.org/abs/2510.02250</link>
<guid>https://arxiv.org/abs/2510.02250</guid>
<content:encoded><![CDATA[
arXiv:2510.02250v1 Announce Type: new 
Abstract: Computer-use agents (CUAs) hold promise for automating everyday digital tasks, but their unreliability and high variance hinder their application to long-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method that scales over agents by generating multiple rollouts and selecting among them using behavior narratives that describe the agents' rollouts. It enables both wide exploration and principled trajectory selection, substantially improving robustness and success rates. On OSWorld, our bBoN scaling method establishes a new state of the art (SoTA) at 69.9%, significantly outperforming prior methods and approaching human-level performance at 72%, with comprehensive ablations validating key design choices. We further demonstrate strong generalization results to different operating systems on WindowsAgentArena and AndroidWorld. Crucially, our results highlight the unreasonable effectiveness of scaling CUAs, when you do it right: effective scaling requires structured trajectory understanding and selection, and bBoN provides a practical framework to achieve this.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in Tool-Augmented Agents</title>
<link>https://arxiv.org/abs/2510.02271</link>
<guid>https://arxiv.org/abs/2510.02271</guid>
<content:encoded><![CDATA[
arXiv:2510.02271v1 Announce Type: new 
Abstract: Information seeking is a fundamental requirement for humans. However, existing LLM agents rely heavily on open-web search, which exposes two fundamental weaknesses: online content is noisy and unreliable, and many real-world tasks require precise, domain-specific knowledge unavailable from the web. The emergence of the Model Context Protocol (MCP) now allows agents to interface with thousands of specialized tools, seemingly resolving this limitation. Yet it remains unclear whether agents can effectively leverage such tools -- and more importantly, whether they can integrate them with general-purpose search to solve complex tasks. Therefore, we introduce InfoMosaic-Bench, the first benchmark dedicated to multi-source information seeking in tool-augmented agents. Covering six representative domains (medicine, finance, maps, video, web, and multi-domain integration), InfoMosaic-Bench requires agents to combine general-purpose search with domain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable pipeline that grounds task conditions in verified tool outputs, enforces cross-source dependencies, and filters out shortcut cases solvable by trivial lookup. This design guarantees both reliability and non-triviality. Experiments with 14 state-of-the-art LLM agents reveal three findings: (i) web information alone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% pass rate; (ii) domain tools provide selective but inconsistent benefits, improving some domains while degrading others; and (iii) 22.4% of failures arise from incorrect tool usage or selection, highlighting that current LLMs still struggle with even basic tool handling.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interactive Training: Feedback-Driven Neural Network Optimization</title>
<link>https://arxiv.org/abs/2510.02297</link>
<guid>https://arxiv.org/abs/2510.02297</guid>
<content:encoded><![CDATA[
arXiv:2510.02297v1 Announce Type: new 
Abstract: Traditional neural network training typically follows fixed, predefined optimization recipes, lacking the flexibility to dynamically respond to instabilities or emerging training issues. In this paper, we introduce Interactive Training, an open-source framework that enables real-time, feedback-driven intervention during neural network training by human experts or automated AI agents. At its core, Interactive Training uses a control server to mediate communication between users or agents and the ongoing training process, allowing users to dynamically adjust optimizer hyperparameters, training data, and model checkpoints. Through three case studies, we demonstrate that Interactive Training achieves superior training stability, reduced sensitivity to initial hyperparameters, and improved adaptability to evolving user needs, paving the way toward a future training paradigm where AI agents autonomously monitor training logs, proactively resolve instabilities, and optimize training dynamics.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Aware Sequential Learning</title>
<link>https://arxiv.org/abs/2502.19525</link>
<guid>https://arxiv.org/abs/2502.19525</guid>
<content:encoded><![CDATA[
arXiv:2502.19525v5 Announce Type: cross 
Abstract: In settings like vaccination registries, individuals act after observing others, and the resulting public records can expose private information. We study privacy-preserving sequential learning, where agents add endogenous noise to their reported actions to conceal private signals. Efficient social learning relies on information flow, seemingly in conflict with privacy. Surprisingly, with continuous signals and a fixed privacy budget $(\epsilon)$, the optimal randomization strategy balances privacy and accuracy, accelerating learning to $\Theta_{\epsilon}(\log n)$, faster than the nonprivate $\Theta(\sqrt{\log n})$ rate. In the nonprivate baseline, the expected time to the first correct action and the number of incorrect actions diverge; under privacy with sufficiently small $\epsilon$, both are finite. Privacy helps because, under the false state, agents more often receive signals contradicting the majority; randomization then asymmetrically amplifies the log-likelihood ratio, enhancing aggregation. In heterogeneous populations, an order-optimal $\Theta(\sqrt{n})$ rate is achievable when a subset of agents have low privacy budgets. With binary signals, however, privacy reduces informativeness and impairs learning relative to the nonprivate baseline, though the dependence on $\epsilon$ is nonmonotone. Our results show how privacy reshapes information dynamics and inform the design of platforms and policies.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing the development of Cherenkov Telescope Array control software with Large Language Models</title>
<link>https://arxiv.org/abs/2510.01299</link>
<guid>https://arxiv.org/abs/2510.01299</guid>
<content:encoded><![CDATA[
arXiv:2510.01299v1 Announce Type: cross 
Abstract: We develop AI agents based on instruction-finetuned large language models (LLMs) to assist in the engineering and operation of the Cherenkov Telescope Array Observatory (CTAO) Control and Data Acquisition Software (ACADA). These agents align with project-specific documentation and codebases, understand contextual information, interact with external APIs, and communicate with users in natural language. We present our progress in integrating these features into CTAO pipelines for operations and offline data analysis.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Financial Stability Implications of Generative AI: Taming the Animal Spirits</title>
<link>https://arxiv.org/abs/2510.01451</link>
<guid>https://arxiv.org/abs/2510.01451</guid>
<content:encoded><![CDATA[
arXiv:2510.01451v1 Announce Type: cross 
Abstract: This paper investigates the impact of the adoption of generative AI on financial stability. We conduct laboratory-style experiments using large language models to replicate classic studies on herd behavior in trading decisions. Our results show that AI agents make more rational decisions than humans, relying predominantly on private information over market trends. Increased reliance on AI-powered trading advice could therefore potentially lead to fewer asset price bubbles arising from animal spirits that trade by following the herd. However, exploring variations in the experimental settings reveals that AI agents can be induced to herd optimally when explicitly guided to make profit-maximizing decisions. While optimal herding improves market discipline, this behavior still carries potential implications for financial stability. In other experimental variations, we show that AI agents are not purely algorithmic, but have inherited some elements of human conditioning and bias.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exponential convergence of a distributed divide-and-conquer algorithm for constrained convex optimization on networks</title>
<link>https://arxiv.org/abs/2510.01511</link>
<guid>https://arxiv.org/abs/2510.01511</guid>
<content:encoded><![CDATA[
arXiv:2510.01511v1 Announce Type: cross 
Abstract: We propose a divide-and-conquer (DAC) algorithm for constrained convex optimization over networks, where the global objective is the sum of local objectives attached to individual agents. The algorithm is fully distributed: each iteration solves local subproblems around selected fusion centers and coordinates only with neighboring fusion centers. Under standard assumptions of smoothness, strong convexity, and locality on the objective function, together with polynomial growth conditions on the underlying graph, we establish exponential convergence of the DAC iterations and derive explicit bounds for both exact and inexact local solvers. Numerical experiments on three representative losses ($L_2$ distance, quadratic, and entropy) confirm the theory and demonstrate scalability and effectiveness.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioinfoMCP: A Unified Platform Enabling MCP Interfaces in Agentic Bioinformatics</title>
<link>https://arxiv.org/abs/2510.02139</link>
<guid>https://arxiv.org/abs/2510.02139</guid>
<content:encoded><![CDATA[
arXiv:2510.02139v1 Announce Type: cross 
Abstract: Bioinformatics tools are essential for complex computational biology tasks, yet their integration with emerging AI-agent frameworks is hindered by incompatible interfaces, heterogeneous input-output formats, and inconsistent parameter conventions. The Model Context Protocol (MCP) provides a standardized framework for tool-AI communication, but manually converting hundreds of existing and rapidly growing specialized bioinformatics tools into MCP-compliant servers is labor-intensive and unsustainable. Here, we present BioinfoMCP, a unified platform comprising two components: BioinfoMCP Converter, which automatically generates robust MCP servers from tool documentation using large language models, and BioinfoMCP Benchmark, which systematically validates the reliability and versatility of converted tools across diverse computational tasks. We present a platform of 38 MCP-converted bioinformatics tools, extensively validated to show that 94.7% successfully executed complex workflows across three widely used AI-agent platforms. By removing technical barriers to AI automation, BioinfoMCP enables natural-language interaction with sophisticated bioinformatics analyses without requiring extensive programming expertise, offering a scalable path to intelligent, interoperable computational biology.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization of Scoring Rules</title>
<link>https://arxiv.org/abs/2007.02905</link>
<guid>https://arxiv.org/abs/2007.02905</guid>
<content:encoded><![CDATA[
arXiv:2007.02905v3 Announce Type: replace 
Abstract: We characterize the optimal reward functions (scoring rules) that incentivize an agent to acquire information and report it truthfully to the principal. The optimal scoring rules let the agent make a simple binary bet in single-dimensional problems, and choose the dimension with the most surprising signal to be scored on in symmetric multi-dimensional problems. This scoring rule format remains approximately optimal for asymmetric distributions. These results demonstrate the importance of linking incentives to obtain high-quality information in multi-dimensional problems. In contrast, standard scoring rules like the quadratic scoring rule, or averages of single-dimensional scoring rules can be far from optimal.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Goal Recognition Design for General Behavioral Agents using Machine Learning</title>
<link>https://arxiv.org/abs/2404.03054</link>
<guid>https://arxiv.org/abs/2404.03054</guid>
<content:encoded><![CDATA[
arXiv:2404.03054v3 Announce Type: replace 
Abstract: Goal recognition design (GRD) aims to make limited modifications to decision-making environments to make it easier to infer the goals of agents acting within those environments. Although various research efforts have been made in goal recognition design, existing approaches are computationally demanding and often assume that agents are (near-)optimal in their decision-making. To address these limitations, we leverage machine learning methods for goal recognition design that can both improve run-time efficiency and account for agents with general behavioral models. Following existing literature, we use worst-case distinctiveness (wcd) as a measure of the difficulty in inferring the goal of an agent in a decision-making environment. Our approach begins by training a machine learning model to predict the wcd for a given environment and the agent behavior model. We then propose a gradient-based optimization framework that accommodates various constraints to optimize decision-making environments for enhanced goal recognition. Through extensive simulations, we demonstrate that our approach outperforms existing methods in reducing wcd and enhances runtime efficiency. Moreover, our approach also adapts to settings in which existing approaches do not apply, such as those involving flexible budget constraints, more complex environments, and suboptimal agent behavior. Finally, we conducted human-subject experiments that demonstrate that our method creates environments that facilitate efficient goal recognition from human decision-makers.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faster LLM Inference using DBMS-Inspired Preemption and Cache Replacement Policies</title>
<link>https://arxiv.org/abs/2411.07447</link>
<guid>https://arxiv.org/abs/2411.07447</guid>
<content:encoded><![CDATA[
arXiv:2411.07447v4 Announce Type: replace 
Abstract: LLMs are increasingly used world-wide from daily tasks to agentic systems and data analytics, requiring significant GPU resources. LLM inference systems, however, are slow compared to database systems, and inference performance and mechanism have been often regarded as a black box, limiting the expansion of the use of LLMs inside databases and other performance-critical applications. This paper first analyzes the LLM inference performance and focuses on a data management issue inside LLM inference. We find that inference systems lack an adequate resource cost model and optimization strategy to schedule requests with their intermediate results in a cache reside in GPU memory when executing multiple concurrent inference requests. We adapt classic database techniques by building cost models for concurrent inference requests and a new cache replacement policy tailored for LLM inference, which can substantially save GPU costs.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Disagreements Elicit Robustness: Investigating Self-Repair Capabilities under LLM Multi-Agent Disagreements</title>
<link>https://arxiv.org/abs/2502.15153</link>
<guid>https://arxiv.org/abs/2502.15153</guid>
<content:encoded><![CDATA[
arXiv:2502.15153v2 Announce Type: replace 
Abstract: Recent advances in Large Language Models (LLMs) have upgraded them from sophisticated text generators to autonomous agents capable of cooperation and tool use in multi-agent systems (MAS). However, it remains unclear how disagreements shape collective decision-making. In this paper, we revisit the role of disagreement and argue that general, partially overlapping disagreements prevent premature consensus and expand the explored solution space, while disagreements on task-critical steps can derail collaboration depending on the topology of solution paths. We investigate two collaborative settings with distinct path structures: collaborative reasoning (CounterFact, MQuAKE-cf), which typically follows a single evidential chain, whereas collaborative programming (HumanEval, GAIA) often adopts multiple valid implementations. Disagreements are instantiated as general heterogeneity among agents and as task-critical counterfactual knowledge edits injected into context or parameters. Experiments reveal that general disagreements consistently improve success by encouraging complementary exploration. By contrast, task-critical disagreements substantially reduce success on single-path reasoning, yet have a limited impact on programming, where agents can choose alternative solutions. Trace analyses show that MAS frequently bypasses the edited facts in programming but rarely does so in reasoning, revealing an emergent self-repair capability that depends on solution-path rather than scale alone. Our code is available at https://github.com/wbw625/MultiAgentRobustness.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents</title>
<link>https://arxiv.org/abs/2503.09780</link>
<guid>https://arxiv.org/abs/2503.09780</guid>
<content:encoded><![CDATA[
arXiv:2503.09780v3 Announce Type: replace 
Abstract: Autonomous AI agents that can follow instructions and perform complex multi-step tasks have tremendous potential to boost human productivity. However, to perform many of these tasks, the agents need access to personal information from their users, raising the question of whether they are capable of using it appropriately. In this work, we introduce a new benchmark AgentDAM that measures if AI web-navigation agents follow the privacy principle of ``data minimization''. For the purposes of our benchmark, data minimization means that the agent uses a piece of potentially sensitive information only if it is ``necessary'' to complete a particular task. Our benchmark simulates realistic web interaction scenarios end-to-end and is adaptable to all existing web navigation agents. We use AgentDAM to evaluate how well AI agents built on top of GPT-4, Llama-3 and Claude can limit processing of potentially private information, and show that they are prone to inadvertent use of unnecessary sensitive information. We also propose a prompting-based defense that reduces information leakage, and demonstrate that our end-to-end benchmarking provides a more realistic measure than probing LLMs about privacy. Our results highlight that further research is needed to develop AI agents that can prioritize data minimization at inference time.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward</title>
<link>https://arxiv.org/abs/2504.03206</link>
<guid>https://arxiv.org/abs/2504.03206</guid>
<content:encoded><![CDATA[
arXiv:2504.03206v3 Announce Type: replace 
Abstract: Effective conversational agents like large language models (LLMs) must personalize their interactions to adapt to user preferences, personalities, and attributes across diverse domains like education and healthcare. Current methods like Reinforcement Learning from Human Feedback (RLHF), often prioritize helpfulness and safety but fall short in fostering truly empathetic, adaptive, and personalized dialogues. Existing personalization approaches typically rely on extensive user history, limiting their effectiveness for new or context-limited users. To address these limitations, we propose leveraging a user model to incorporate a curiosity-based intrinsic reward into multi-turn RLHF. This novel reward mechanism encourages the LLM agent to actively infer user traits by optimizing conversations to improve its user model's accuracy. Consequently, the agent delivers more personalized interactions by learning more about the user. We demonstrate our method's effectiveness in two distinct domains: significantly improving personalization performance in a conversational recommendation task, and personalizing conversations for different learning styles in an educational setting. We show improved generalization capabilities compared to traditional multi-turn RLHF, all while maintaining conversation quality. Our method offers a promising solution for creating more personalized, adaptive, and engaging conversational agents.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebRollback: Enhancing Web Agents with Explicit Rollback Mechanisms</title>
<link>https://arxiv.org/abs/2504.11788</link>
<guid>https://arxiv.org/abs/2504.11788</guid>
<content:encoded><![CDATA[
arXiv:2504.11788v2 Announce Type: replace 
Abstract: With recent advancements in large language models, web agents have been greatly improved. However, dealing with complex and dynamic web environments requires more advanced planning and search abilities. Previous studies usually adopt a greedy one-way search strategy, which may struggle to recover from erroneous states. In this work, we enhance web agents with an explicit rollback mechanism, enabling the agent to revert back to a previous state in its navigation trajectory. This mechanism gives the model the flexibility to directly control the search process, leading to an effective and efficient web navigation method. We conduct experiments on two live web navigation benchmarks with zero-shot and fine-tuning settings. The results demonstrate the effectiveness of our proposed approach.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design and Evaluation of Generative Agent-based Platform for Human-Assistant Interaction Research: A Tale of 10 User Studies</title>
<link>https://arxiv.org/abs/2505.09938</link>
<guid>https://arxiv.org/abs/2505.09938</guid>
<content:encoded><![CDATA[
arXiv:2505.09938v2 Announce Type: replace 
Abstract: Designing and evaluating personalized and proactive assistant agents remains challenging due to the time, cost, and ethical concerns associated with human-in-the-loop experimentation. Existing Human-Computer Interaction (HCI) methods often require extensive physical setup and human participation, which introduces privacy concerns and limits scalability. Simulated environments offer a partial solution but are typically constrained by rule-based scenarios and still depend heavily on human input to guide interactions and interpret results. Recent advances in large language models (LLMs) have introduced the possibility of generative agents that can simulate realistic human behavior, reasoning, and social dynamics. However, their effectiveness in modeling human-assistant interactions remains largely unexplored. To address this gap, we present a generative agent-based simulation platform designed to simulate human-assistant interactions. We identify ten prior studies on assistant agents that span different aspects of interaction design and replicate these studies using our simulation platform. Our results show that fully simulated experiments using generative agents can approximate key aspects of human-assistant interactions. Based on these simulations, we are able to replicate the core conclusions of the original studies. Our work provides a scalable and cost-effective approach for studying assistant agent design without requiring live human subjects. Additional resources and project materials are available at https://dash-gidea.github.io/
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implementing Agents in JavaScript</title>
<link>https://arxiv.org/abs/2505.18228</link>
<guid>https://arxiv.org/abs/2505.18228</guid>
<content:encoded><![CDATA[
arXiv:2505.18228v2 Announce Type: replace 
Abstract: This chapter gives an introduction to agent-oriented programming in JavaScript. It provides an example-based walk-through of how to implement abstractions for reasoning loop agents in vanilla JavaScript. The initial example is used as a stepping stone for explaining how to implement slightly more advanced agents and multi-agent systems using JS-son, a JavaScript library for agent-oriented programming. In this context, the chapter also explains how to integrate reasoning loop agents with generative AI technologies--specifically, large language models. Finally, application scenarios in several technology ecosystems and future research directions are sketched.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AniMaker: Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation</title>
<link>https://arxiv.org/abs/2506.10540</link>
<guid>https://arxiv.org/abs/2506.10540</guid>
<content:encoded><![CDATA[
arXiv:2506.10540v2 Announce Type: replace 
Abstract: Despite rapid advancements in video generation models, generating coherent storytelling videos that span multiple scenes and characters remains challenging. Current methods often rigidly convert pre-generated keyframes into fixed-length clips, resulting in disjointed narratives and pacing issues. Furthermore, the inherent instability of video generation models means that even a single low-quality clip can significantly degrade the entire output animation's logical coherence and visual continuity. To overcome these obstacles, we introduce AniMaker, a multi-agent framework enabling efficient multi-candidate clip generation and storytelling-aware clip selection, thus creating globally consistent and story-coherent animation solely from text input. The framework is structured around specialized agents, including the Director Agent for storyboard generation, the Photography Agent for video clip generation, the Reviewer Agent for evaluation, and the Post-Production Agent for editing and voiceover. Central to AniMaker's approach are two key technical components: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search (MCTS)-inspired strategy that intelligently navigates the candidate space to generate high-potential clips while optimizing resource usage; and AniEval in Reviewer Agent, the first framework specifically designed for multi-shot animation evaluation, which assesses critical aspects such as story-level consistency, action completion, and animation-specific features by considering each clip in the context of its preceding and succeeding clips. Experiments demonstrate that AniMaker achieves superior quality as measured by popular metrics including VBench and our proposed AniEval framework, while significantly improving the efficiency of multi-candidate generation, pushing AI-generated storytelling animation closer to production standards.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?</title>
<link>https://arxiv.org/abs/2507.15887</link>
<guid>https://arxiv.org/abs/2507.15887</guid>
<content:encoded><![CDATA[
arXiv:2507.15887v3 Announce Type: replace 
Abstract: Despite progress in language model (LM) capabilities, evaluations have thus far focused on models' performance on tasks that humans have previously solved, including in programming (Jimenez et al., 2024) and mathematics (Glazer et al., 2024). We therefore propose testing models' ability to design and implement algorithms in an open-ended benchmark: We task LMs with writing code that efficiently solves computationally challenging problems in computer science, physics, and mathematics. Our AlgoTune benchmark consists of 154 coding tasks collected from domain experts and a framework for validating and timing LM-synthesized solution code, which is compared to reference implementations from popular open-source packages. In addition, we develop a baseline LM agent, AlgoTuner, and evaluate its performance across a suite of frontier models. AlgoTuner uses a simple, budgeted loop that edits code, compiles and runs it, profiles performance, verifies correctness on tests, and selects the fastest valid version. AlgoTuner achieves an average 1.72x speedup against our reference solvers, which use libraries such as SciPy, sk-learn and CVXPY. However, we find that current models fail to discover algorithmic innovations, instead preferring surface-level optimizations. We hope that AlgoTune catalyzes the development of LM agents exhibiting creative problem solving beyond state-of-the-art human performance.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StreamAgent: Towards Anticipatory Agents for Streaming Video Understanding</title>
<link>https://arxiv.org/abs/2508.01875</link>
<guid>https://arxiv.org/abs/2508.01875</guid>
<content:encoded><![CDATA[
arXiv:2508.01875v2 Announce Type: replace 
Abstract: Real-time streaming video understanding in domains such as autonomous driving and intelligent surveillance poses challenges beyond conventional offline video processing, requiring continuous perception, proactive decision making, and responsive interaction based on dynamically evolving visual content. However, existing methods rely on alternating perception-reaction or asynchronous triggers, lacking task-driven planning and future anticipation, which limits their real-time responsiveness and proactive decision making in evolving video streams. To this end, we propose a StreamAgent that anticipates the temporal intervals and spatial regions expected to contain future task-relevant information to enable proactive and goal-driven responses. Specifically, we integrate question semantics and historical observations through prompting the anticipatory agent to anticipate the temporal progression of key events, align current observations with the expected future evidence, and subsequently adjust the perception action (e.g., attending to task-relevant regions or continuously tracking in subsequent frames). To enable efficient inference, we design a streaming KV-cache memory mechanism that constructs a hierarchical memory structure for selective recall of relevant tokens, enabling efficient semantic retrieval while reducing the overhead of storing all tokens in the traditional KV-cache. Extensive experiments on streaming and long video understanding tasks demonstrate that our method outperforms existing methods in response accuracy and real-time efficiency, highlighting its practical value for real-world streaming scenarios.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Cybersecurity Architecture for Digital Product Ecosystems Using Agentic AI</title>
<link>https://arxiv.org/abs/2509.20640</link>
<guid>https://arxiv.org/abs/2509.20640</guid>
<content:encoded><![CDATA[
arXiv:2509.20640v2 Announce Type: replace 
Abstract: Traditional static cybersecurity models often struggle with scalability, real-time detection, and contextual responsiveness in the current digital product ecosystems which include cloud services, application programming interfaces (APIs), mobile platforms, and edge devices. This study introduces autonomous goal driven agents capable of dynamic learning and context-aware decision making as part of an adaptive cybersecurity architecture driven by agentic artificial intelligence (AI). To facilitate autonomous threat mitigation, proactive policy enforcement, and real-time anomaly detection, this framework integrates agentic AI across the key ecosystem layers. Behavioral baselining, decentralized risk scoring, and federated threat intelligence sharing are important features. The capacity of the system to identify zero-day attacks and dynamically modify access policies was demonstrated through native cloud simulations. The evaluation results show increased adaptability, decreased response latency, and improved detection accuracy. The architecture provides an intelligent and scalable blueprint for safeguarding complex digital infrastructure and is compatible with zero-trust models, thereby supporting the adherence to international cybersecurity regulations.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Strongest LLM: Multi-Turn Multi-Agent Orchestration vs. Single LLMs on Benchmarks</title>
<link>https://arxiv.org/abs/2509.23537</link>
<guid>https://arxiv.org/abs/2509.23537</guid>
<content:encoded><![CDATA[
arXiv:2509.23537v2 Announce Type: replace 
Abstract: We study multi-turn multi-agent orchestration, where multiple large language model (LLM) agents interact over multiple turns by iteratively proposing answers or casting votes until reaching consensus. Using four LLMs (Gemini 2.5 Pro, GPT-5, Grok 4, and Claude Sonnet 4) on GPQA-Diamond, IFEval, and MuSR, we conduct two experiments: (i) benchmarking orchestration against single-LLM baselines; and (ii) ablations on GPQA-Diamond that vary whether agents see who authored answers and whether they can observe ongoing votes. Orchestration matches or exceeds the strongest single model and consistently outperforms the others. Analysis of best-achievable orchestration performance shows potential for further gains. The ablations show that revealing authorship increases self-voting and ties, and that showing ongoing votes amplifies herding, which speeds convergence but can sometimes yield premature consensus.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdGT: Decentralized Gradient Tracking with Tuning-free Per-Agent Stepsize</title>
<link>https://arxiv.org/abs/2504.15196</link>
<guid>https://arxiv.org/abs/2504.15196</guid>
<content:encoded><![CDATA[
arXiv:2504.15196v2 Announce Type: replace-cross 
Abstract: In decentralized optimization, the choice of stepsize plays a critical role in algorithm performance. A common approach is to use a shared stepsize across all agents to ensure convergence. However, selecting an optimal stepsize often requires careful tuning, which can be time-consuming and may lead to slow convergence, especially when there is significant variation in the smoothness (L-smoothness) of local objective functions across agents. Individually tuning stepsizes per agent is also impractical, particularly in large-scale networks. To address these limitations, we propose AdGT, an adaptive gradient tracking method that enables each agent to adjust its stepsize based on the smoothness of its local objective. We prove that AdGT achieves linear convergence to the global optimal solution. Through numerical experiments, we compare AdGT with fixed-stepsize gradient tracking methods and demonstrate its superior performance. Additionally, we compare AdGT with adaptive gradient descent (AdGD) in a centralized setting and observe that fully adaptive stepsizes offer greater benefits in decentralized networks than in centralized ones.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Lead Themselves: Agentic AI in MAS using MARL</title>
<link>https://arxiv.org/abs/2510.00022</link>
<guid>https://arxiv.org/abs/2510.00022</guid>
<content:encoded><![CDATA[
arXiv:2510.00022v1 Announce Type: new 
Abstract: As autonomous systems move from prototypes to real deployments, the ability of multiple agents to make decentralized, cooperative decisions becomes a core requirement. This paper examines how agentic artificial intelligence, agents that act independently, adaptively and proactively can improve task allocation and coordination in multi-agent systems, with primary emphasis on drone delivery and secondary relevance to warehouse automation. We formulate the problem in a cooperative multi-agent reinforcement learning setting and implement a lightweight multi-agent Proximal Policy Optimization, called IPPO, approach in PyTorch under a centralized-training, decentralized-execution paradigm. Experiments are conducted in PettingZoo environment, where multiple homogeneous drones or agents must self-organize to cover distinct targets without explicit communication.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolBrain: A Flexible Reinforcement Learning Framework for Agentic Tools</title>
<link>https://arxiv.org/abs/2510.00023</link>
<guid>https://arxiv.org/abs/2510.00023</guid>
<content:encoded><![CDATA[
arXiv:2510.00023v1 Announce Type: new 
Abstract: Effective tool use is essential for agentic AI, yet training agents to utilize tools remains challenging due to manually designed rewards, limited training data, and poor multi-tool selection, resulting in slow adaptation, wasted computational resources, and suboptimal performance. We introduce ToolBrain, a lightweight and user-friendly framework for coaching tool use in agentic models with flexible reinforcement learning (RL), easing the barriers for researchers and practitioners to adapt LLM-based agents to specific domains. It supports a wide range of training strategies, including RL algorithms such as GRPO and DPO, as well as supervised learning. ToolBrain enables custom reward callables directly on an agent's execution traces or simply utilizes an automated LLM-as-a-judge system for reward generation. It is packed with useful capabilities, including knowledge distillation from large to small models for efficient development, automatic task generation from tool descriptions, seamless tool retrieval, efficient fine-tuning pipelines with QLoRA through Unsloth, and quantized inference via bitsandbytes. We demonstrate ToolBrain through diverse use cases, such as training a CodeAct agent to autonomously execute email search tasks, showing fast, targeted improvements (up to 30.0%) in tool-use skills while keeping the codebase simple and extensible in Agentic AI. Our framework is publicly available at https://toolbrain.org.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EpidemIQs: Prompt-to-Paper LLM Agents for Epidemic Modeling and Analysis</title>
<link>https://arxiv.org/abs/2510.00024</link>
<guid>https://arxiv.org/abs/2510.00024</guid>
<content:encoded><![CDATA[
arXiv:2510.00024v1 Announce Type: new 
Abstract: Large Language Models (LLMs) offer new opportunities to automate complex interdisciplinary research domains. Epidemic modeling, characterized by its complexity and reliance on network science, dynamical systems, epidemiology, and stochastic simulations, represents a prime candidate for leveraging LLM-driven automation. We introduce \textbf{EpidemIQs}, a novel multi-agent LLM framework that integrates user inputs and autonomously conducts literature review, analytical derivation, network modeling, mechanistic modeling, stochastic simulations, data visualization and analysis, and finally documentation of findings in a structured manuscript. We introduced two types of agents: a scientist agent for planning, coordination, reflection, and generation of final results, and a task-expert agent to focus exclusively on one specific duty serving as a tool to the scientist agent. The framework consistently generated complete reports in scientific article format. Specifically, using GPT 4.1 and GPT 4.1 mini as backbone LLMs for scientist and task-expert agents, respectively, the autonomous process completed with average total token usage 870K at a cost of about \$1.57 per study, achieving a 100\% completion success rate through our experiments. We evaluate EpidemIQs across different epidemic scenarios, measuring computational cost, completion success rate, and AI and human expert reviews of generated reports. We compare EpidemIQs to the single-agent LLM, which has the same system prompts and tools, iteratively planning, invoking tools, and revising outputs until task completion. The comparison shows consistently higher performance of the proposed framework across five different scenarios. EpidemIQs represents a step forward in accelerating scientific research by significantly reducing costs and turnaround time of discovery processes, and enhancing accessibility to advanced modeling tools.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VibeCodeHPC: An Agent-Based Iterative Prompting Auto-Tuner for HPC Code Generation Using LLMs</title>
<link>https://arxiv.org/abs/2510.00031</link>
<guid>https://arxiv.org/abs/2510.00031</guid>
<content:encoded><![CDATA[
arXiv:2510.00031v1 Announce Type: new 
Abstract: We propose VibeCodeHPC, an automatic tuning system for HPC programs based on multi-agent LLMs for code generation. VibeCodeHPC tunes programs through multi-agent role allocation and iterative prompt refinement. We describe the system configuration with four roles: Project Manager (PM), System Engineer (SE), Programmer (PG), and Continuous Delivery (CD). We introduce dynamic agent deployment and activity monitoring functions to facilitate effective multi-agent collaboration. In our case study, we convert and optimize CPU-based matrix-matrix multiplication code written in C to GPU code using CUDA. The multi-agent configuration of VibeCodeHPC achieved higher-quality code generation per unit time compared to a solo-agent configuration. Additionally, the dynamic agent deployment and activity monitoring capabilities facilitated more effective identification of requirement violations and other issues.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Lean yet Powerful Vision-Language Model for Autonomous Driving</title>
<link>https://arxiv.org/abs/2510.00060</link>
<guid>https://arxiv.org/abs/2510.00060</guid>
<content:encoded><![CDATA[
arXiv:2510.00060v1 Announce Type: new 
Abstract: In this work, we reconceptualize autonomous driving as a generalized language and formulate the trajectory planning task as next waypoint prediction. We introduce Max-V1, a novel framework for one-stage end-to-end autonomous driving. Our framework presents a single-pass generation paradigm that aligns with the inherent sequentiality of driving. This approach leverages the generative capacity of the VLM (Vision-Language Model) to enable end-to-end trajectory prediction directly from front-view camera input. The efficacy of this method is underpinned by a principled supervision strategy derived from statistical modeling. This provides a well-defined learning objective, which makes the framework highly amenable to master complex driving policies through imitation learning from large-scale expert demonstrations. Empirically, our method achieves the state-of-the-art performance on the nuScenes dataset, delivers an overall improvement of over 30% compared to prior baselines. Furthermore, it exhibits superior generalization performance on cross-domain datasets acquired from diverse vehicles, demonstrating notable potential for cross-vehicle robustness and adaptability. Due to these empirical strengths, this work introduces a model enabling fundamental driving behaviors, laying the foundation for the development of more capable self-driving agents. Code will be available upon publication.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OIG-Bench: A Multi-Agent Annotated Benchmark for Multimodal One-Image Guides Understanding</title>
<link>https://arxiv.org/abs/2510.00069</link>
<guid>https://arxiv.org/abs/2510.00069</guid>
<content:encoded><![CDATA[
arXiv:2510.00069v1 Announce Type: new 
Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities. However, evaluating their capacity for human-like understanding in One-Image Guides remains insufficiently explored. One-Image Guides are a visual format combining text, imagery, and symbols to present reorganized and structured information for easier comprehension, which are specifically designed for human viewing and inherently embody the characteristics of human perception and understanding. Here, we present OIG-Bench, a comprehensive benchmark focused on One-Image Guide understanding across diverse domains. To reduce the cost of manual annotation, we developed a semi-automated annotation pipeline in which multiple intelligent agents collaborate to generate preliminary image descriptions, assisting humans in constructing image-text pairs. With OIG-Bench, we have conducted a comprehensive evaluation of 29 state-of-the-art MLLMs, including both proprietary and open-source models. The results show that Qwen2.5-VL-72B performs the best among the evaluated models, with an overall accuracy of 77%. Nevertheless, all models exhibit notable weaknesses in semantic understanding and logical reasoning, indicating that current MLLMs still struggle to accurately interpret complex visual-text relationships. In addition, we also demonstrate that the proposed multi-agent annotation system outperforms all MLLMs in image captioning, highlighting its potential as both a high-quality image description generator and a valuable tool for future dataset construction. Datasets are available at https://github.com/XiejcSYSU/OIG-Bench.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive and Resource-efficient Agentic AI Systems for Mobile and Embedded Devices: A Survey</title>
<link>https://arxiv.org/abs/2510.00078</link>
<guid>https://arxiv.org/abs/2510.00078</guid>
<content:encoded><![CDATA[
arXiv:2510.00078v1 Announce Type: new 
Abstract: Foundation models have reshaped AI by unifying fragmented architectures into scalable backbones with multimodal reasoning and contextual adaptation. In parallel, the long-standing notion of AI agents, defined by the sensing-decision-action loop, is entering a new paradigm: with FMs as their cognitive core, agents transcend rule-based behaviors to achieve autonomy, generalization, and self-reflection. This dual shift is reinforced by real-world demands such as autonomous driving, robotics, virtual assistants, and GUI agents, as well as ecosystem advances in embedded hardware, edge computing, mobile deployment platforms, and communication protocols that together enable large-scale deployment. Yet this convergence collides with reality: while applications demand long-term adaptability and real-time interaction, mobile and edge deployments remain constrained by memory, energy, bandwidth, and latency. This creates a fundamental tension between the growing complexity of FMs and the limited resources of deployment environments. This survey provides the first systematic characterization of adaptive, resource-efficient agentic AI systems. We summarize enabling techniques into elastic inference, test-time adaptation, dynamic multimodal integration, and agentic AI applications, and identify open challenges in balancing accuracy-latency-communication trade-offs and sustaining robustness under distribution shifts. We further highlight future opportunities in algorithm-system co-design, cognitive adaptation, and collaborative edge deployment. By mapping FM structures, cognition, and hardware resources, this work establishes a unified perspective toward scalable, adaptive, and resource-efficient agentic AI. We believe this survey can help readers to understand the connections between enabling technologies while promoting further discussions on the fusion of agentic intelligence and intelligent agents.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Which Rewards Matter? Reward Selection for Reinforcement Learning under Limited Feedback</title>
<link>https://arxiv.org/abs/2510.00144</link>
<guid>https://arxiv.org/abs/2510.00144</guid>
<content:encoded><![CDATA[
arXiv:2510.00144v1 Announce Type: new 
Abstract: The ability of reinforcement learning algorithms to learn effective policies is determined by the rewards available during training. However, for practical problems, obtaining large quantities of reward labels is often infeasible due to computational or financial constraints, particularly when relying on human feedback. When reinforcement learning must proceed with limited feedback -- only a fraction of samples get rewards labeled -- a fundamental question arises: which samples should be labeled to maximize policy performance? We formalize this problem of reward selection for reinforcement learning from limited feedback (RLLF), introducing a new problem formulation that facilitates the study of strategies for selecting impactful rewards. Two types of selection strategies are investigated: (i) heuristics that rely on reward-free information such as state visitation and partial value functions, and (ii) strategies pre-trained using auxiliary evaluative feedback. We find that critical subsets of rewards are those that (1) guide the agent along optimal trajectories, and (2) support recovery toward near-optimal behavior after deviations. Effective selection methods yield near-optimal policies with significantly fewer reward labels than full supervision, establishing reward selection as a powerful paradigm for scaling reinforcement learning in feedback-limited settings.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AuditAgent: Expert-Guided Multi-Agent Reasoning for Cross-Document Fraudulent Evidence Discovery</title>
<link>https://arxiv.org/abs/2510.00156</link>
<guid>https://arxiv.org/abs/2510.00156</guid>
<content:encoded><![CDATA[
arXiv:2510.00156v1 Announce Type: new 
Abstract: Financial fraud detection in real-world scenarios presents significant challenges due to the subtlety and dispersion of evidence across complex, multi-year financial disclosures. In this work, we introduce a novel multi-agent reasoning framework AuditAgent, enhanced with auditing domain expertise, for fine-grained evidence chain localization in financial fraud cases. Leveraging an expert-annotated dataset constructed from enforcement documents and financial reports released by the China Securities Regulatory Commission, our approach integrates subject-level risk priors, a hybrid retrieval strategy, and specialized agent modules to efficiently identify and aggregate cross-report evidence. Extensive experiments demonstrate that our method substantially outperforms General-Purpose Agent paradigm in both recall and interpretability, establishing a new benchmark for automated, transparent financial forensics. Our results highlight the value of domain-specific reasoning and dataset construction for advancing robust financial fraud detection in practical, real-world regulatory applications.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAMA: Tool-Augmented Multimodal Agent for Procedural Activity Understanding</title>
<link>https://arxiv.org/abs/2510.00161</link>
<guid>https://arxiv.org/abs/2510.00161</guid>
<content:encoded><![CDATA[
arXiv:2510.00161v1 Announce Type: new 
Abstract: Procedural activity assistants potentially support humans in a variety of settings, from our daily lives, e.g., cooking or assembling flat-pack furniture, to professional situations, e.g., manufacturing or biological experiments. Despite its potential use cases, the system development tailored for such an assistant is still underexplored. In this paper, we propose a novel framework, called TAMA, a Tool-Augmented Multimodal Agent, for procedural activity understanding. TAMA enables interleaved multimodal reasoning by making use of multimedia-returning tools in a training-free setting. Our experimental result on the multimodal procedural QA dataset, ProMQA-Assembly, shows that our approach can improve the performance of vision-language models, especially GPT-5 and MiMo-VL. Furthermore, our ablation studies provide empirical support for the effectiveness of two features that characterize our framework, multimedia-returning tools and agentic flexible tool selection. We believe our proposed framework and experimental results facilitate the thinking with images paradigm for video and multimodal tasks, let alone the development of procedural activity assistants.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Necklace Splitting</title>
<link>https://arxiv.org/abs/2510.00162</link>
<guid>https://arxiv.org/abs/2510.00162</guid>
<content:encoded><![CDATA[
arXiv:2510.00162v1 Announce Type: new 
Abstract: The necklace splitting problem is a classic problem in fair division with many applications, including data-informed fair hash maps. We extend necklace splitting to a dynamic setting, allowing for relocation, insertion, and deletion of beads. We present linear-time, optimal algorithms for the two-color case that support all dynamic updates. For more than two colors, we give linear-time, optimal algorithms for relocation subject to a restriction on the number of agents. Finally, we propose a randomized algorithm for the two-color case that handles all dynamic updates, guarantees approximate fairness with high probability, and runs in polylogarithmic time when the number of agents is small.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRBench: A Realistic Benchmark for Enterprise Deep Research</title>
<link>https://arxiv.org/abs/2510.00172</link>
<guid>https://arxiv.org/abs/2510.00172</guid>
<content:encoded><![CDATA[
arXiv:2510.00172v1 Announce Type: new 
Abstract: We introduce DRBench, a benchmark for evaluating AI agents on complex, open-ended deep research tasks in enterprise settings. Unlike prior benchmarks that focus on simple questions or web-only queries, DRBench evaluates agents on multi-step queries (for example, ``What changes should we make to our product roadmap to ensure compliance with this standard?") that require identifying supporting facts from both the public web and private company knowledge base. Each task is grounded in realistic user personas and enterprise context, spanning a heterogeneous search space that includes productivity software, cloud file systems, emails, chat conversations, and the open web. Tasks are generated through a carefully designed synthesis pipeline with human-in-the-loop verification, and agents are evaluated on their ability to recall relevant insights, maintain factual accuracy, and produce coherent, well-structured reports. We release 15 deep research tasks across 10 domains, such as Sales, Cybersecurity, and Compliance. We demonstrate the effectiveness of DRBench by evaluating diverse DR agents across open- and closed-source models (such as GPT, Llama, and Qwen) and DR strategies, highlighting their strengths, weaknesses, and the critical path for advancing enterprise deep research. Code is available at https://github.com/ServiceNow/drbench.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHAI: Command Hijacking against embodied AI</title>
<link>https://arxiv.org/abs/2510.00181</link>
<guid>https://arxiv.org/abs/2510.00181</guid>
<content:encoded><![CDATA[
arXiv:2510.00181v1 Announce Type: new 
Abstract: Embodied Artificial Intelligence (AI) promises to handle edge cases in robotic vehicle systems where data is scarce by using common-sense reasoning grounded in perception and action to generalize beyond training distributions and adapt to novel real-world situations. These capabilities, however, also create new security risks. In this paper, we introduce CHAI (Command Hijacking against embodied AI), a new class of prompt-based attacks that exploit the multimodal language interpretation abilities of Large Visual-Language Models (LVLMs). CHAI embeds deceptive natural language instructions, such as misleading signs, in visual input, systematically searches the token space, builds a dictionary of prompts, and guides an attacker model to generate Visual Attack Prompts. We evaluate CHAI on four LVLM agents; drone emergency landing, autonomous driving, and aerial object tracking, and on a real robotic vehicle. Our experiments show that CHAI consistently outperforms state-of-the-art attacks. By exploiting the semantic and multimodal reasoning strengths of next-generation embodied AI systems, CHAI underscores the urgent need for defenses that extend beyond traditional adversarial robustness.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DualTune: Decoupled Fine-Tuning for On-Device Agentic Systems</title>
<link>https://arxiv.org/abs/2510.00229</link>
<guid>https://arxiv.org/abs/2510.00229</guid>
<content:encoded><![CDATA[
arXiv:2510.00229v1 Announce Type: new 
Abstract: The deployment of Large Language Models (LLMs) as agentic orchestrators has revolutionized task automation, but the need for privacy-preserving, cost-effective solutions demands on-device inference capabilities. However, local LLMs consistently underperform compared to frontier models in tool calling scenarios, struggling with both tool selection from large tool sets and accurate argument generation for complex parameter structures. We introduce a methodology that disaggregates a tool-calling task into two distinct subtasks: tool selection and argument generation. We propose "decoupled fine-tuning", a novel post-training approach that employs LoRA fine-tuning to create dedicated LoRA adapters for tool selection and tool-specific argument generation using separate loss masking for each of the subtasks. Furthermore, we present DualTune, an inference framework that leverages the LoRA adapters created using decoupled fine-tuning to perform efficient agent orchestration with the help of local models on end-user devices. DualTune decomposes the tool-call generation step into tool selection and argument generation, and dynamically loads the corresponding LoRA adapters to generate tool calls. Additionally, DualTune implements hierarchical orchestration to restrict the number of tools required for tool selection. Our experiments on the MCP-Bench benchmark demonstrate that the Qwen-2.5-7B model trained using decoupled fine-tuning improves the tool calling accuracy of the base model by 46%, and outperforms other local reasoning, non-reasoning and fine-tuned models of similar size in all cases, and models that are 2x larger, in most cases.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cooperation in Bilateral Generalized Network Creation</title>
<link>https://arxiv.org/abs/2510.00239</link>
<guid>https://arxiv.org/abs/2510.00239</guid>
<content:encoded><![CDATA[
arXiv:2510.00239v1 Announce Type: new 
Abstract: Studying the impact of cooperation in strategic settings is one of the cornerstones of algorithmic game theory. Intuitively, allowing more cooperation yields equilibria that are more beneficial for the society of agents. However, for many games it is still an open question how much cooperation is actually needed to ensure socially good equilibria. We contribute to this research endeavor by analyzing the benefits of cooperation in a network formation game that models the creation of communication networks via the interaction of selfish agents. In our game, agents that correspond to nodes of a network can buy incident edges of a given weighted host graph to increase their centrality in the formed network. The cost of an edge is proportional to its length, and both endpoints must agree and pay for an edge to be created. This setting is known for having a high price of anarchy.
  To uncover the impact of cooperation, we investigate the price of anarchy of our network formation game with respect to multiple solution concepts that allow for varying amounts of cooperation. On the negative side, we show that on host graphs with arbitrary edge weights even the strongest form of cooperation cannot improve the price of anarchy. In contrast to this, as our main result, we show that cooperation has a significant positive impact if the given host graph has metric edge weights. For this, we prove asymptotically tight bounds on the price of anarchy via a novel proof technique that might be of independent interest and can be applied in other models with metric weights.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can AI agents understand spoken conversations about data visualizations in online meetings?</title>
<link>https://arxiv.org/abs/2510.00245</link>
<guid>https://arxiv.org/abs/2510.00245</guid>
<content:encoded><![CDATA[
arXiv:2510.00245v1 Announce Type: new 
Abstract: In this short paper, we present work evaluating an AI agent's understanding of spoken conversations about data visualizations in an online meeting scenario. There is growing interest in the development of AI-assistants that support meetings, such as by providing assistance with tasks or summarizing a discussion. The quality of this support depends on a model that understands the conversational dialogue. To evaluate this understanding, we introduce a dual-axis testing framework for diagnosing the AI agent's comprehension of spoken conversations about data. Using this framework, we designed a series of tests to evaluate understanding of a novel corpus of 72 spoken conversational dialogues about data visualizations. We examine diverse pipelines and model architectures, LLM vs VLM, and diverse input formats for visualizations (the chart image, its underlying source code, or a hybrid of both) to see how this affects model performance on our tests. Using our evaluation methods, we found that text-only input modalities achieved the best performance (96%) in understanding discussions of visualizations in online meetings.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hierarchical Agentic Framework for Autonomous Drone-Based Visual Inspection</title>
<link>https://arxiv.org/abs/2510.00259</link>
<guid>https://arxiv.org/abs/2510.00259</guid>
<content:encoded><![CDATA[
arXiv:2510.00259v1 Announce Type: new 
Abstract: Autonomous inspection systems are essential for ensuring the performance and longevity of industrial assets. Recently, agentic frameworks have demonstrated significant potential for automating inspection workflows but have been limited to digital tasks. Their application to physical assets in real-world environments, however, remains underexplored. In this work, our contributions are two-fold: first, we propose a hierarchical agentic framework for autonomous drone control, and second, a reasoning methodology for individual function executions which we refer to as ReActEval. Our framework focuses on visual inspection tasks in indoor industrial settings, such as interpreting industrial readouts or inspecting equipment. It employs a multi-agent system comprising a head agent and multiple worker agents, each controlling a single drone. The head agent performs high-level planning and evaluates outcomes, while worker agents implement ReActEval to reason over and execute low-level actions. Operating entirely in natural language, ReActEval follows a plan, reason, act, evaluate cycle, enabling drones to handle tasks ranging from simple navigation (e.g., flying forward 10 meters and land) to complex high-level tasks (e.g., locating and reading a pressure gauge). The evaluation phase serves as a feedback and/or replanning stage, ensuring actions align with user objectives while preventing undesirable outcomes. We evaluate the framework in a simulated environment with two worker agents, assessing performance qualitatively and quantitatively based on task completion across varying complexity levels and workflow efficiency. By leveraging natural language processing for agent communication, our approach offers a novel, flexible, and user-accessible alternative to traditional drone-based solutions, enabling autonomous problem-solving for industrial inspection without extensive user intervention.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGIC-MASK: Multi-Agent Guided Inter-Agent Collaboration with Mask-Based Explainability for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.00274</link>
<guid>https://arxiv.org/abs/2510.00274</guid>
<content:encoded><![CDATA[
arXiv:2510.00274v1 Announce Type: new 
Abstract: Understanding the decision-making process of Deep Reinforcement Learning agents remains a key challenge for deploying these systems in safety-critical and multi-agent environments. While prior explainability methods like StateMask, have advanced the identification of critical states, they remain limited by computational cost, exploration coverage, and lack of adaptation to multi-agent settings. To overcome these limitations, we propose a mathematically grounded framework, MAGIC-MASK (Multi-Agent Guided Inter-agent Collaboration with Mask-Based Explainability for Reinforcement Learning), that extends perturbation-based explanation to Multi-Agent Reinforcement Learning. Our method integrates Proximal Policy Optimization, adaptive epsilon-greedy exploration, and lightweight inter-agent collaboration to share masked state information and peer experience. This collaboration enables each agent to perform saliency-guided masking and share reward-based insights with peers, reducing the time required for critical state discovery, improving explanation fidelity, and leading to faster and more robust learning. The core novelty of our approach lies in generalizing explainability from single-agent to multi-agent systems through a unified mathematical formalism built on trajectory perturbation, reward fidelity analysis, and Kullback-Leibler divergence regularization. This framework yields localized, interpretable explanations grounded in probabilistic modeling and multi-agent Markov decision processes. We validate our framework on both single-agent and multi-agent benchmarks, including a multi-agent highway driving environment and Google Research Football, demonstrating that MAGIC-MASK consistently outperforms state-of-the-art baselines in fidelity, learning efficiency, and policy robustness while offering interpretable and transferable explanations.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CORTEX: Collaborative LLM Agents for High-Stakes Alert Triage</title>
<link>https://arxiv.org/abs/2510.00311</link>
<guid>https://arxiv.org/abs/2510.00311</guid>
<content:encoded><![CDATA[
arXiv:2510.00311v1 Announce Type: new 
Abstract: Security Operations Centers (SOCs) are overwhelmed by tens of thousands of daily alerts, with only a small fraction corresponding to genuine attacks. This overload creates alert fatigue, leading to overlooked threats and analyst burnout. Classical detection pipelines are brittle and context-poor, while recent LLM-based approaches typically rely on a single model to interpret logs, retrieve context, and adjudicate alerts end-to-end -- an approach that struggles with noisy enterprise data and offers limited transparency. We propose CORTEX, a multi-agent LLM architecture for high-stakes alert triage in which specialized agents collaborate over real evidence: a behavior-analysis agent inspects activity sequences, evidence-gathering agents query external systems, and a reasoning agent synthesizes findings into an auditable decision. To support training and evaluation, we release a dataset of fine-grained SOC investigations from production environments, capturing step-by-step analyst actions and linked tool outputs. Across diverse enterprise scenarios, CORTEX substantially reduces false positives and improves investigation quality over state-of-the-art single-agent LLMs.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAVUL: Multi-Agent Vulnerability Detection via Contextual Reasoning and Interactive Refinement</title>
<link>https://arxiv.org/abs/2510.00317</link>
<guid>https://arxiv.org/abs/2510.00317</guid>
<content:encoded><![CDATA[
arXiv:2510.00317v1 Announce Type: new 
Abstract: The widespread adoption of open-source software (OSS) necessitates the mitigation of vulnerability risks. Most vulnerability detection (VD) methods are limited by inadequate contextual understanding, restrictive single-round interactions, and coarse-grained evaluations, resulting in undesired model performance and biased evaluation results. To address these challenges, we propose MAVUL, a novel multi-agent VD system that integrates contextual reasoning and interactive refinement. Specifically, a vulnerability analyst agent is designed to flexibly leverage tool-using capabilities and contextual reasoning to achieve cross-procedural code understanding and effectively mine vulnerability patterns. Through iterative feedback and refined decision-making within cross-role agent interactions, the system achieves reliable reasoning and vulnerability prediction. Furthermore, MAVUL introduces multi-dimensional ground truth information for fine-grained evaluation, thereby enhancing evaluation accuracy and reliability.
  Extensive experiments conducted on a pairwise vulnerability dataset demonstrate MAVUL's superior performance. Our findings indicate that MAVUL significantly outperforms existing multi-agent systems with over 62% higher pairwise accuracy and single-agent systems with over 600% higher average performance. The system's effectiveness is markedly improved with increased communication rounds between the vulnerability analyst agent and the security architect agent, underscoring the importance of contextual reasoning in tracing vulnerability flows and the crucial feedback role. Additionally, the integrated evaluation agent serves as a critical, unbiased judge, ensuring a more accurate and reliable estimation of the system's real-world applicability by preventing misleading binary comparisons.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning-Aware Prompt Orchestration: A Foundation Model for Multi-Agent Language Model Coordination</title>
<link>https://arxiv.org/abs/2510.00326</link>
<guid>https://arxiv.org/abs/2510.00326</guid>
<content:encoded><![CDATA[
arXiv:2510.00326v1 Announce Type: new 
Abstract: The emergence of large language models has enabled sophisticated multi-agent systems, yet coordinating their reasoning capabilities through prompt engineering remains challenging. We present a theoretically-grounded framework for dynamic prompt orchestration that enhances reasoning across multiple specialized agents. This framework addresses three core challenges: logical consistency preservation during agent transitions, reasoning-aware prompt adaptation, and scalable coordination of distributed inference.
  Our approach formalizes agent states using prompt templates, reasoning context vectors, and capability matrices. We prove system convergence to stable coordination patterns when step sizes satisfy $\alpha < \frac{1}{2L}$ where $L$ is the Lipschitz constant of the state transition function. We implement this through a distributed architecture that dynamically routes reasoning tasks while maintaining semantic coherence.
  Experimental results on 1,000 synthetic multi-agent conversations demonstrate a 42% reduction in reasoning latency, a 23% improvement in logical consistency measured by ROUGE-L score, and an 89% success rate for task completion without context loss across agent transitions. Ablation studies identify the consensus mechanism as the primary performance driver, while revealing limitations: performance degrades beyond 10 agent transitions, and the system requires 76.5GB memory for 1,000 concurrent agents. These findings establish a new paradigm for scalable reasoning in multi-agent systems, providing theoretical foundations for understanding reasoning emergence across coordinated language models.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Hallucination Costs Millions: Benchmarking AI Agents in High-Stakes Adversarial Financial Markets</title>
<link>https://arxiv.org/abs/2510.00332</link>
<guid>https://arxiv.org/abs/2510.00332</guid>
<content:encoded><![CDATA[
arXiv:2510.00332v1 Announce Type: new 
Abstract: We present CAIA, a benchmark exposing a critical blind spot in AI evaluation: the inability of state-of-the-art models to operate in adversarial, high-stakes environments where misinformation is weaponized and errors are irreversible. While existing benchmarks measure task completion in controlled settings, real-world deployment demands resilience against active deception. Using crypto markets as a testbed where $30 billion was lost to exploits in 2024, we evaluate 17 models on 178 time-anchored tasks requiring agents to distinguish truth from manipulation, navigate fragmented information landscapes, and make irreversible financial decisions under adversarial pressure.
  Our results reveal a fundamental capability gap: without tools, even frontier models achieve only 28% accuracy on tasks junior analysts routinely handle. Tool augmentation improves performance but plateaus at 67.4% versus 80% human baseline, despite unlimited access to professional resources. Most critically, we uncover a systematic tool selection catastrophe: models preferentially choose unreliable web search over authoritative data, falling for SEO-optimized misinformation and social media manipulation. This behavior persists even when correct answers are directly accessible through specialized tools, suggesting foundational limitations rather than knowledge gaps. We also find that Pass@k metrics mask dangerous trial-and-error behavior for autonomous deployment.
  The implications extend beyond crypto to any domain with active adversaries, e.g. cybersecurity, content moderation, etc. We release CAIA with contamination controls and continuous updates, establishing adversarial robustness as a necessary condition for trustworthy AI autonomy. The benchmark reveals that current models, despite impressive reasoning scores, remain fundamentally unprepared for environments where intelligence must survive active opposition.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating the Synchrony-Stability Frontier in Adaptive Chatbots</title>
<link>https://arxiv.org/abs/2510.00339</link>
<guid>https://arxiv.org/abs/2510.00339</guid>
<content:encoded><![CDATA[
arXiv:2510.00339v1 Announce Type: new 
Abstract: Adaptive chatbots that mimic a user's linguistic style can build rapport and engagement, yet unconstrained mimicry risks an agent that feels unstable or sycophantic. We present a computational evaluation framework that makes the core design tension explicit: balancing moment-to-moment linguistic synchrony against long-term persona stability. Using an 8-dimensional style vector and a closed-loop "base+delta" prompting architecture, we simulate and compare explicit adaptation policies - Uncapped, Cap, Exponential Moving Average (EMA), Dead-Band, and Hybrids - on a human-log dataset. Our analysis maps a clear Pareto frontier: bounded policies achieve substantial gains in stability at a modest cost to synchrony. For example, a Hybrid (EMA+Cap) raises stability from 0.542 to 0.878 (+62%) while reducing synchrony by only 17%. We confirm this trade-off through large-scale replications on three public corpora (DailyDialog, Persona-Chat, EmpatheticDialogues) and LLM-in-the-loop validation across two model families. Furthermore, we quantify "prompt legibility," showing that frontier policies reduce instruction churn and cut jarring register flips (major tone changes) from 0.254 to 0.092, yielding systems that are easier to reason about and maintain. Taken together, our framework provides a general evaluation harness for style adaptation; a systematic ablation that identifies Pareto-efficient policies; robust validation across diverse datasets and models; and novel legibility metrics linking policy choices to system maintainability.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Context Curiosity: Distilling Exploration for Decision-Pretrained Transformers on Bandit Tasks</title>
<link>https://arxiv.org/abs/2510.00347</link>
<guid>https://arxiv.org/abs/2510.00347</guid>
<content:encoded><![CDATA[
arXiv:2510.00347v1 Announce Type: new 
Abstract: As large language models (LLMs) continue to grow in capability, there is increasing interest in incorporating them into decision-making tasks. A common pipeline for this is Decision-Pretrained Transformers (DPTs). However, existing training methods for DPTs often struggle to generalize beyond their pretraining data distribution. To explore mitigation of this limitation, we propose in-context curiosity -- a lightweight, exploration-inspired regularizer for offline pretraining -- and introduce the Prediction-Powered Transformer (PPT) framework. PPT augments DPT with an auxiliary reward predictor, using prediction error as an intrinsic curiosity signal to encourage broader exploration during training. In proof-of-concept experiments on Gaussian multi-armed bandits, PPT shows improved robustness: it moderates the performance degradation observed in DPT when test environments exhibit higher variance in reward, particularly when pretraining data has limited diversity. While the quality of offline data remain fundamental, our preliminary results suggest that curiosity-driven pretraining offers a promising direction for enhancing out-of-distribution generalization in in-context RL agents.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-Driven AI Agent Communications: Challenges and Solutions</title>
<link>https://arxiv.org/abs/2510.00381</link>
<guid>https://arxiv.org/abs/2510.00381</guid>
<content:encoded><![CDATA[
arXiv:2510.00381v1 Announce Type: new 
Abstract: With the rapid growth of intelligent services, communication targets are shifting from humans to artificial intelligent (AI) agents, which require new paradigms to enable real-time perception, decision-making, and collaboration. Semantic communication, which conveys task-relevant meaning rather than raw data, offers a promising solution. However, its practical deployment remains constrained by dynamic environments and limited resources. To address these issues, this article proposes a semantic-driven AI agent communication framework and develops three enabling techniques. First, semantic adaptation transmission applies fine-tuning with real or generative samples to efficiently adapt models to varying environments. Second, semantic lightweight transmission incorporates pruning, quantization, and perception-aware sampling to reduce model complexity and alleviate computational burden on edge agents. Third, semantic self-evolution control employs distributed hierarchical decision-making to optimize multi-dimensional resources, enabling robust multi-agent collaboration in dynamic environments. Simulation results show that the proposed solutions achieve faster convergence and stronger robustness, while the proposed distributed hierarchical optimization method significantly outperforms conventional decision-making schemes, highlighting its potential for AI agent communication networks.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Neural Controlled Differential Equations for Scalable Long Horizon Multi-Agent Motion Forecasting</title>
<link>https://arxiv.org/abs/2510.00401</link>
<guid>https://arxiv.org/abs/2510.00401</guid>
<content:encoded><![CDATA[
arXiv:2510.00401v1 Announce Type: new 
Abstract: Long-horizon motion forecasting for multiple autonomous robots is challenging due to non-linear agent interactions, compounding prediction errors, and continuous-time evolution of dynamics. Learned dynamics of such a system can be useful in various applications such as travel time prediction, prediction-guided planning and generative simulation. In this work, we aim to develop an efficient trajectory forecasting model conditioned on multi-agent goals. Motivated by the recent success of physics-guided deep learning for partially known dynamical systems, we develop a model based on neural Controlled Differential Equations (CDEs) for long-horizon motion forecasting. Unlike discrete-time methods such as RNNs and transformers, neural CDEs operate in continuous time, allowing us to combine physics-informed constraints and biases to jointly model multi-robot dynamics. Our approach, named PINCoDE (Physics-Informed Neural Controlled Differential Equations), learns differential equation parameters that can be used to predict the trajectories of a multi-agent system starting from an initial condition. PINCoDE is conditioned on future goals and enforces physics constraints for robot motion over extended periods of time. We adopt a strategy that scales our model from 10 robots to 100 robots without the need for additional model parameters, while producing predictions with an average ADE below 0.5 m for a 1-minute horizon. Furthermore, progressive training with curriculum learning for our PINCoDE model results in a 2.7X reduction of forecasted pose error over 4 minute horizons compared to analytical models.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EgoTraj-Bench: Towards Robust Trajectory Prediction Under Ego-view Noisy Observations</title>
<link>https://arxiv.org/abs/2510.00405</link>
<guid>https://arxiv.org/abs/2510.00405</guid>
<content:encoded><![CDATA[
arXiv:2510.00405v1 Announce Type: new 
Abstract: Reliable trajectory prediction from an ego-centric perspective is crucial for robotic navigation in human-centric environments. However, existing methods typically assume idealized observation histories, failing to account for the perceptual artifacts inherent in first-person vision, such as occlusions, ID switches, and tracking drift. This discrepancy between training assumptions and deployment reality severely limits model robustness. To bridge this gap, we introduce EgoTraj-Bench, the first real-world benchmark that grounds noisy, first-person visual histories in clean, bird's-eye-view future trajectories, enabling robust learning under realistic perceptual constraints. Building on this benchmark, we propose BiFlow, a dual-stream flow matching model that concurrently denoises historical observations and forecasts future motion by leveraging a shared latent representation. To better model agent intent, BiFlow incorporates our EgoAnchor mechanism, which conditions the prediction decoder on distilled historical features via feature modulation. Extensive experiments show that BiFlow achieves state-of-the-art performance, reducing minADE and minFDE by 10-15% on average and demonstrating superior robustness. We anticipate that our benchmark and model will provide a critical foundation for developing trajectory forecasting systems truly resilient to the challenges of real-world, ego-centric perception.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAL-UI: Planning with Active Look-back for Vision-Based GUI Agents</title>
<link>https://arxiv.org/abs/2510.00413</link>
<guid>https://arxiv.org/abs/2510.00413</guid>
<content:encoded><![CDATA[
arXiv:2510.00413v1 Announce Type: new 
Abstract: Graphical User Interface (GUI) agents powered by Multimodal Large Language Models (MLLMs) promise human-like interaction with software applications, yet long-horizon tasks remain challenging due to memory limitations. Existing approaches either truncate history or rely on simple textual summaries, which risk losing critical information when past visual details become necessary for future decisions. In this paper, we propose \textbf{PAL-UI} (\textbf{P}lanning with \textbf{A}ctive \textbf{L}ook-back), a novel framework that enables GUI agents to adaptively retrieve past observations when required. PAL-UI combines a dual-level summarization agent, capturing both observation-level cues and action-level outcomes, with a dedicated retrieval tool that allows the agent to recall specific historical screenshots during planning. We curate a step-level instruction dataset of 8.6K samples from mobile GUI navigation trajectories and train \textbf{PAL-UI-3B} and \textbf{PAL-UI-7B} models based on Qwen2.5-VL. Extensive experiments demonstrate that PAL-UI significantly outperforms baseline models and prior methods in mobile GUI navigation tasks, even under data-efficient settings. Moreover, PAL-UI exhibits strong cross-domain generalization, achieving notable improvements in web navigation without additional training. Our work highlights the potential of active memory retrieval for long-horizon planning capabilities of vision-based GUI agents.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RELATE-Sim: Leveraging Turning Point Theory and LLM Agents to Predict and Understand Long-Term Relationship Dynamics through Interactive Narrative Simulations</title>
<link>https://arxiv.org/abs/2510.00414</link>
<guid>https://arxiv.org/abs/2510.00414</guid>
<content:encoded><![CDATA[
arXiv:2510.00414v1 Announce Type: new 
Abstract: Most dating technologies optimize for getting together, not staying together. We present RELATE-Sim, a theory-grounded simulator that models how couples behave at consequential turning points-exclusivity talks, conflict-and-repair episodes, relocations-rather than static traits. Two persona-aligned LLM agents (one per partner) interact under a centralized Scene Master that frames each turning point as a compact set of realistic options, advances the narrative, and infers interpretable state changes and an auditable commitment estimate after each scene. On a longitudinal dataset of 71 couples with two-year follow-ups, simulation-aware predictions outperform a personas-only baseline while surfacing actionable markers (e.g., repair attempts acknowledged, clarity shifts) that explain why trajectories diverge. RELATE-Sim pushes the relationship research's focus from matchmaking to maintenance, providing a transparent, extensible platform for understanding and forecasting long-term relationship dynamics.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Self-Evolving Benchmarks: Synthesizing Agent Trajectories via Test-Time Exploration under Validate-by-Reproduce Paradigm</title>
<link>https://arxiv.org/abs/2510.00415</link>
<guid>https://arxiv.org/abs/2510.00415</guid>
<content:encoded><![CDATA[
arXiv:2510.00415v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) and agent system designs have empowered agents with unprecedented levels of capability. However, existing agent benchmarks are showing a trend of rapid ceiling-hitting by newly developed agents, making it difficult to meet the demands for evaluating agent abilities. To address this problem, we propose the Trajectory-based Validated-by-Reproducing Agent-benchmark Complexity Evolution (TRACE) framework. This framework takes an original task from an existing benchmark and encourages agents to freely explore and evolve it into a new task with higher difficulty while recording validatable agent trajectories. The framework proceeds in three stages: (1) evolutionary proposal mining, which provides task evolution proposals through preliminary exploration and divergent thinking; (2) problem formation and free exploration, where proposals are conceptualized into feasible problem candidates and the agents then explore them freely while recording their execution trajectories; and (3) multi-level validation, which ensures that the evolved tasks are accompanied by validatable and reproducible trajectories. Experiments on the GAIA benchmark demonstrate that the TRACE framework consistently enhances task complexity while improving the reliability of correctness through validatable execution trajectories. This work marks a paradigm shift from static, manually curated benchmarks to dynamic, self-evolving evaluation systems, providing a sustainable and challenging runway for agent development.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conflict-Based Search as a Protocol: A Multi-Agent Motion Planning Protocol for Heterogeneous Agents, Solvers, and Independent Tasks</title>
<link>https://arxiv.org/abs/2510.00425</link>
<guid>https://arxiv.org/abs/2510.00425</guid>
<content:encoded><![CDATA[
arXiv:2510.00425v1 Announce Type: new 
Abstract: Imagine the future construction site, hospital, office, or even sophisticated household with dozens of robots bought from different manufacturers. How can we enable these different systems to effectively move in a shared environment, given that each robot may have its own independent motion planning system? This work shows how we can get efficient collision-free movements between algorithmically heterogeneous agents by using Conflict-Based Search (Sharon et al. 2015) as a protocol. At its core, the CBS Protocol requires one specific single-agent motion planning API; finding a collision-free path that satisfies certain space-time constraints. Given such an API, CBS uses a central planner to find collision-free paths - independent of how the API is implemented. We show how this protocol enables multi-agent motion planning for a heterogeneous team of agents completing independent tasks with a variety of single-agent planners including: Heuristic Search (e.g., A*), Sampling Based Search (e.g., RRT), Optimization (e.g., Direct Collocation), Diffusion, and Reinforcement Learning.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing through Uncertainty: Robust Task-Oriented Optimization in Visual Navigation</title>
<link>https://arxiv.org/abs/2510.00441</link>
<guid>https://arxiv.org/abs/2510.00441</guid>
<content:encoded><![CDATA[
arXiv:2510.00441v1 Announce Type: new 
Abstract: Visual navigation is a fundamental problem in embodied AI, yet practical deployments demand long-horizon planning capabilities to address multi-objective tasks. A major bottleneck is data scarcity: policies learned from limited data often overfit and fail to generalize OOD. Existing neural network-based agents typically increase architectural complexity that paradoxically become counterproductive in the small-sample regime. This paper introduce NeuRO, a integrated learning-to-optimize framework that tightly couples perception networks with downstream task-level robust optimization. Specifically, NeuRO addresses core difficulties in this integration: (i) it transforms noisy visual predictions under data scarcity into convex uncertainty sets using Partially Input Convex Neural Networks (PICNNs) with conformal calibration, which directly parameterize the optimization constraints; and (ii) it reformulates planning under partial observability as a robust optimization problem, enabling uncertainty-aware policies that transfer across environments. Extensive experiments on both unordered and sequential multi-object navigation tasks demonstrate that NeuRO establishes SoTA performance, particularly in generalization to unseen environments. Our work thus presents a significant advancement for developing robust, generalizable autonomous agents.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Call to Action for a Secure-by-Design Generative AI Paradigm</title>
<link>https://arxiv.org/abs/2510.00451</link>
<guid>https://arxiv.org/abs/2510.00451</guid>
<content:encoded><![CDATA[
arXiv:2510.00451v1 Announce Type: new 
Abstract: Large language models have gained widespread prominence, yet their vulnerability to prompt injection and other adversarial attacks remains a critical concern. This paper argues for a security-by-design AI paradigm that proactively mitigates LLM vulnerabilities while enhancing performance. To achieve this, we introduce PromptShield, an ontology-driven framework that ensures deterministic and secure prompt interactions. It standardizes user inputs through semantic validation, eliminating ambiguity and mitigating adversarial manipulation. To assess PromptShield's security and performance capabilities, we conducted an experiment on an agent-based system to analyze cloud logs within Amazon Web Services (AWS), containing 493 distinct events related to malicious activities and anomalies. By simulating prompt injection attacks and assessing the impact of deploying PromptShield, our results demonstrate a significant improvement in model security and performance, achieving precision, recall, and F1 scores of approximately 94%. Notably, the ontology-based framework not only mitigates adversarial threats but also enhances the overall performance and reliability of the system. Furthermore, PromptShield's modular and adaptable design ensures its applicability beyond cloud security, making it a robust solution for safeguarding generative AI applications across various domains. By laying the groundwork for AI safety standards and informing future policy development, this work stimulates a crucial dialogue on the pivotal role of deterministic prompt engineering and ontology-based validation in ensuring the safe and responsible deployment of LLMs in high-stakes environments.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagnosing Shortcut-Induced Rigidity in Continual Learning: The Einstellung Rigidity Index (ERI)</title>
<link>https://arxiv.org/abs/2510.00475</link>
<guid>https://arxiv.org/abs/2510.00475</guid>
<content:encoded><![CDATA[
arXiv:2510.00475v1 Announce Type: new 
Abstract: Deep neural networks frequently exploit shortcut features, defined as incidental correlations between inputs and labels without causal meaning. Shortcut features undermine robustness and reduce reliability under distribution shifts. In continual learning (CL), the consequences of shortcut exploitation can persist and intensify: weights inherited from earlier tasks bias representation reuse toward whatever features most easily satisfied prior labels, mirroring the cognitive Einstellung effect, a phenomenon where past habits block optimal solutions. Whereas catastrophic forgetting erodes past skills, shortcut-induced rigidity throttles the acquisition of new ones. We introduce the Einstellung Rigidity Index (ERI), a compact diagnostic that disentangles genuine transfer from cue-inflated performance using three interpretable facets: (i) Adaptation Delay (AD), (ii) Performance Deficit (PD), and (iii) Relative Suboptimal Feature Reliance (SFR_rel). On a two-phase CIFAR-100 CL benchmark with a deliberately spurious magenta patch in Phase 2, we evaluate Naive fine-tuning (SGD), online Elastic Weight Consolidation (EWC_on), Dark Experience Replay (DER++), Gradient Projection Memory (GPM), and Deep Generative Replay (DGR). Across these continual learning methods, we observe that CL methods reach accuracy thresholds earlier than a Scratch-T2 baseline (negative AD) but achieve slightly lower final accuracy on patched shortcut classes (positive PD). Masking the patch improves accuracy for CL methods while slightly reducing Scratch-T2, yielding negative SFR_rel. This pattern indicates the patch acted as a distractor for CL models in this setting rather than a helpful shortcut.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wireless Laser Power Transfer for Low-altitude Uncrewed Aerial Vehicle-assisted Internet of Things: Paradigms, Challenges, and Solutions</title>
<link>https://arxiv.org/abs/2510.00477</link>
<guid>https://arxiv.org/abs/2510.00477</guid>
<content:encoded><![CDATA[
arXiv:2510.00477v1 Announce Type: new 
Abstract: Low-altitude uncrewed aerial vehicles (UAVs) have become integral enablers for the Internet of Things (IoT) by offering enhanced coverage, improved connectivity and access to remote areas. A critical challenge limiting their operational capacity lies in the energy constraints of both aerial platforms and ground-based sensors. This paper explores WLPT as a transformative solution for sustainable energy provisioning in UAV-assisted IoT networks. We first systematically investigate the fundamental principles of WLPT and analysis the comparative advantages. Then, we introduce three operational paradigms for system integration, identify key challenges, and discuss corresponding potential solutions. In case study, we propose a multi-agent reinforcement learning framework to address the coordination and optimization challenges in WLPT-enabled UAV-assisted IoT data collection. Simulation results demonstrate that our framework significantly improves energy sustainability and data freshness. Finally, we discuss some future directions.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expandable Decision-Making States for Multi-Agent Deep Reinforcement Learning in Soccer Tactical Analysis</title>
<link>https://arxiv.org/abs/2510.00480</link>
<guid>https://arxiv.org/abs/2510.00480</guid>
<content:encoded><![CDATA[
arXiv:2510.00480v1 Announce Type: new 
Abstract: Invasion team sports such as soccer produce a high-dimensional, strongly coupled state space as many players continuously interact on a shared field, challenging quantitative tactical analysis. Traditional rule-based analyses are intuitive, while modern predictive machine learning models often perform pattern-matching without explicit agent representations. The problem we address is how to build player-level agent models from data, whose learned values and policies are both tactically interpretable and robust across heterogeneous data sources. Here, we propose Expandable Decision-Making States (EDMS), a semantically enriched state representation that augments raw positions and velocities with relational variables (e.g., scoring of space, pass, and score), combined with an action-masking scheme that gives on-ball and off-ball agents distinct decision sets. Compared to prior work, EDMS maps learned value functions and action policies to human-interpretable tactical concepts (e.g., marking pressure, passing lanes, ball accessibility) instead of raw coordinate features, and aligns agent choices with the rules of play. In the experiments, EDMS with action masking consistently reduced both action-prediction loss and temporal-difference (TD) error compared to the baseline. Qualitative case studies and Q-value visualizations further indicate that EDMS highlights high-risk, high-reward tactical patterns (e.g., fast counterattacks and defensive breakthroughs). We also integrated our approach into an open-source library and demonstrated compatibility with multiple commercial and open datasets, enabling cross-provider evaluation and reproducible experiments.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Make a Video Call with LLM: A Measurement Campaign over Five Mainstream Apps</title>
<link>https://arxiv.org/abs/2510.00481</link>
<guid>https://arxiv.org/abs/2510.00481</guid>
<content:encoded><![CDATA[
arXiv:2510.00481v1 Announce Type: new 
Abstract: In 2025, Large Language Model (LLM) services have launched a new feature -- AI video chat -- allowing users to interact with AI agents via real-time video communication (RTC), just like chatting with real people. Despite its significance, no systematic study has characterized the performance of existing AI video chat systems. To address this gap, this paper proposes a comprehensive benchmark with carefully designed metrics across four dimensions: quality, latency, internal mechanisms, and system overhead. Using custom testbeds, we further evaluate five mainstream AI video chatbots with this benchmark. This work provides the research community a baseline of real-world performance and identifies unique system bottlenecks. In the meantime, our benchmarking results also open up several research questions for future optimizations of AI video chatbots.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent Fine-tuning through Distillation for Domain-specific LLMs in Microdomains</title>
<link>https://arxiv.org/abs/2510.00482</link>
<guid>https://arxiv.org/abs/2510.00482</guid>
<content:encoded><![CDATA[
arXiv:2510.00482v1 Announce Type: new 
Abstract: Agentic large language models (LLMs) have become prominent for autonomously interacting with external environments and performing multi-step reasoning tasks. Most approaches leverage these capabilities via in-context learning with few-shot prompts, but this often results in lengthy inputs and higher computational costs. Agent fine-tuning offers an alternative by enabling LLMs to internalize procedural reasoning and domain-specific knowledge through training on relevant data and demonstration trajectories. While prior studies have focused on general domains, their effectiveness in specialized technical microdomains remains unclear. This paper explores agent fine-tuning for domain adaptation within Hitachi's JP1 middleware, a microdomain for specialized IT operations. We fine-tuned LLMs using JP1-specific datasets derived from domain manuals and distilled reasoning trajectories generated by LLMs themselves, enhancing decision making accuracy and search efficiency. During inference, we used an agentic prompt with retrieval-augmented generation and introduced a context-answer extractor to improve information relevance. On JP1 certification exam questions, our method achieved a 14% performance improvement over the base model, demonstrating the potential of agent fine-tuning for domain-specific reasoning in complex microdomains.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-ScanKit: Unraveling Memory and Reasoning of Multimodal Agents via Sensitivity Perturbations</title>
<link>https://arxiv.org/abs/2510.00496</link>
<guid>https://arxiv.org/abs/2510.00496</guid>
<content:encoded><![CDATA[
arXiv:2510.00496v1 Announce Type: new 
Abstract: Although numerous strategies have recently been proposed to enhance the autonomous interaction capabilities of multimodal agents in graphical user interface (GUI), their reliability remains limited when faced with complex or out-of-domain tasks. This raises a fundamental question: Are existing multimodal agents reasoning spuriously? In this paper, we propose \textbf{Agent-ScanKit}, a systematic probing framework to unravel the memory and reasoning capabilities of multimodal agents under controlled perturbations. Specifically, we introduce three orthogonal probing paradigms: visual-guided, text-guided, and structure-guided, each designed to quantify the contributions of memorization and reasoning without requiring access to model internals. In five publicly available GUI benchmarks involving 18 multimodal agents, the results demonstrate that mechanical memorization often outweighs systematic reasoning. Most of the models function predominantly as retrievers of training-aligned knowledge, exhibiting limited generalization. Our findings underscore the necessity of robust reasoning modeling for multimodal agents in real-world scenarios, offering valuable insights toward the development of reliable multimodal agents.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs</title>
<link>https://arxiv.org/abs/2510.00507</link>
<guid>https://arxiv.org/abs/2510.00507</guid>
<content:encoded><![CDATA[
arXiv:2510.00507v1 Announce Type: new 
Abstract: As multimodal LLM-driven agents continue to advance in autonomy and generalization, evaluation based on static datasets can no longer adequately assess their true capabilities in dynamic environments and diverse tasks. Existing LLM-based synthetic data methods are largely designed for LLM training and evaluation, and thus cannot be directly applied to agent tasks that require tool use and interactive capabilities. While recent studies have explored automatic agent task generation with LLMs, most efforts remain limited to text or image analysis, without systematically modeling multi-step interactions in web environments. To address these challenges, we propose Graph2Eval, a knowledge graph-based framework that automatically generates both multimodal document comprehension tasks and web interaction tasks, enabling comprehensive evaluation of agents' reasoning, collaboration, and interactive capabilities. In our approach, knowledge graphs constructed from multi-source external data serve as the task space, where we translate semantic relations into structured multimodal tasks using subgraph sampling, task templates, and meta-paths. A multi-stage filtering pipeline based on node reachability, LLM scoring, and similarity analysis is applied to guarantee the quality and executability of the generated tasks. Furthermore, Graph2Eval supports end-to-end evaluation of multiple agent types (Single-Agent, Multi-Agent, Web Agent) and measures reasoning, collaboration, and interaction capabilities. We instantiate the framework with Graph2Eval-Bench, a curated dataset of 1,319 tasks spanning document comprehension and web interaction scenarios. Experiments show that Graph2Eval efficiently generates tasks that differentiate agent and model performance, revealing gaps in reasoning, collaboration, and web interaction across different settings and offering a new perspective for agent evaluation.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JoyAgent-JDGenie: Technical Report on the GAIA</title>
<link>https://arxiv.org/abs/2510.00510</link>
<guid>https://arxiv.org/abs/2510.00510</guid>
<content:encoded><![CDATA[
arXiv:2510.00510v1 Announce Type: new 
Abstract: Large Language Models are increasingly deployed as autonomous agents for complex real-world tasks, yet existing systems often focus on isolated improvements without a unifying design for robustness and adaptability. We propose a generalist agent architecture that integrates three core components: a collective multi-agent framework combining planning and execution agents with critic model voting, a hierarchical memory system spanning working, semantic, and procedural layers, and a refined tool suite for search, code execution, and multimodal parsing. Evaluated on a comprehensive benchmark, our framework consistently outperforms open-source baselines and approaches the performance of proprietary systems. These results demonstrate the importance of system-level integration and highlight a path toward scalable, resilient, and adaptive AI assistants capable of operating across diverse domains and tasks.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness</title>
<link>https://arxiv.org/abs/2510.00536</link>
<guid>https://arxiv.org/abs/2510.00536</guid>
<content:encoded><![CDATA[
arXiv:2510.00536v1 Announce Type: new 
Abstract: Graphical user interface (GUI) agents built on vision-language models have emerged as a promising approach to automate human-computer workflows. However, they also face the inefficiency challenge as they process long sequences of high-resolution screenshots and solving long-horizon tasks, making inference slow, costly and memory-bound. While key-value (KV) caching can mitigate this, storing the full cache is prohibitive for image-heavy contexts. Existing cache-compression methods are sub-optimal as they do not account for the spatial and temporal redundancy of GUIs. In this work, we first analyze attention patterns in GUI agent workloads and find that, unlike in natural images, attention sparsity is uniformly high across all transformer layers. This insight motivates a simple uniform budget allocation strategy, which we show empirically outperforms more complex layer-varying schemes. Building on this, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI agents that requires no retraining. GUI-KV combines two novel techniques: (i) spatial saliency guidance, which augments attention scores with the L2 norm of hidden states to better preserve semantically important visual tokens, and (ii) temporal redundancy scoring, which projects previous frames' keys onto the current frame's key subspace to preferentially prune redundant history. Across standard GUI agent benchmarks and models, GUI-KV outperforms competitive KV compression baselines, closely matching full-cache accuracy at modest budgets. Notably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV reduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the full-cache baseline. These results demonstrate that exploiting GUI-specific redundancies enables efficient and reliable agent performance.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EMR-AGENT: Automating Cohort and Feature Extraction from EMR Databases</title>
<link>https://arxiv.org/abs/2510.00549</link>
<guid>https://arxiv.org/abs/2510.00549</guid>
<content:encoded><![CDATA[
arXiv:2510.00549v1 Announce Type: new 
Abstract: Machine learning models for clinical prediction rely on structured data extracted from Electronic Medical Records (EMRs), yet this process remains dominated by hardcoded, database-specific pipelines for cohort definition, feature selection, and code mapping. These manual efforts limit scalability, reproducibility, and cross-institutional generalization. To address this, we introduce EMR-AGENT (Automated Generalized Extraction and Navigation Tool), an agent-based framework that replaces manual rule writing with dynamic, language model-driven interaction to extract and standardize structured clinical data. Our framework automates cohort selection, feature extraction, and code mapping through interactive querying of databases. Our modular agents iteratively observe query results and reason over schema and documentation, using SQL not just for data retrieval but also as a tool for database observation and decision making. This eliminates the need for hand-crafted, schema-specific logic. To enable rigorous evaluation, we develop a benchmarking codebase for three EMR databases (MIMIC-III, eICU, SICdb), including both seen and unseen schema settings. Our results demonstrate strong performance and generalization across these databases, highlighting the feasibility of automating a process previously thought to require expert-driven design. The code will be released publicly at https://github.com/AITRICS/EMR-AGENT/tree/main. For a demonstration, please visit our anonymous demo page: https://anonymoususer-max600.github.io/EMR_AGENT/
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReSeek: A Self-Correcting Framework for Search Agents with Instructive Rewards</title>
<link>https://arxiv.org/abs/2510.00568</link>
<guid>https://arxiv.org/abs/2510.00568</guid>
<content:encoded><![CDATA[
arXiv:2510.00568v1 Announce Type: new 
Abstract: Search agents powered by Large Language Models (LLMs) have demonstrated significant potential in tackling knowledge-intensive tasks. Reinforcement learning (RL) has emerged as a powerful paradigm for training these agents to perform complex, multi-step reasoning. However, prior RL-based methods often rely on sparse or rule-based rewards, which can lead agents to commit to suboptimal or erroneous reasoning paths without the ability to recover. To address these limitations, we propose ReSeek, a novel self-correcting framework for training search agents. Our framework introduces a self-correction mechanism that empowers the agent to dynamically identify and recover from erroneous search paths during an episode. By invoking a special JUDGE action, the agent can judge the information and re-plan its search strategy. To guide this process, we design a dense, instructive process reward function, which decomposes into a correctness reward for retrieving factual information and a utility reward for finding information genuinely useful for the query. Furthermore, to mitigate the risk of data contamination in existing datasets, we introduce FictionalHot, a new and challenging benchmark with recently curated questions requiring complex reasoning. Being intuitively reasonable and practically simple, extensive experiments show that agents trained with ReSeek significantly outperform SOTA baselines in task success rate and path faithfulness.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Driven Self-Evolving Software: A Promising Path Toward Software Automation</title>
<link>https://arxiv.org/abs/2510.00591</link>
<guid>https://arxiv.org/abs/2510.00591</guid>
<content:encoded><![CDATA[
arXiv:2510.00591v1 Announce Type: new 
Abstract: Software automation has long been a central goal of software engineering, striving for software development that proceeds without human intervention. Recent efforts have leveraged Artificial Intelligence (AI) to advance software automation with notable progress. However, current AI functions primarily as assistants to human developers, leaving software development still dependent on explicit human intervention. This raises a fundamental question: Can AI move beyond its role as an assistant to become a core component of software, thereby enabling genuine software automation? To investigate this vision, we introduce AI-Driven Self-Evolving Software, a new form of software that evolves continuously through direct interaction with users. We demonstrate the feasibility of this idea with a lightweight prototype built on a multi-agent architecture that autonomously interprets user requirements, generates and validates code, and integrates new functionalities. Case studies across multiple representative scenarios show that the prototype can reliably construct and reuse functionality, providing early evidence that such software systems can scale to more sophisticated applications and pave the way toward truly automated software development. We make code and cases in this work publicly available at https://anonymous.4open.science/r/live-software.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Training for Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2510.00600</link>
<guid>https://arxiv.org/abs/2510.00600</guid>
<content:encoded><![CDATA[
arXiv:2510.00600v1 Announce Type: new 
Abstract: Using Large Language Models to produce intermediate thoughts, a.k.a. Chain-of-thought (CoT), before providing an answer has been a successful recipe for solving complex language tasks. In robotics, similar embodied CoT strategies, generating thoughts before actions, have also been shown to lead to improved performance when using Vision-Language-Action models (VLAs). As these techniques increase the length of the model's generated outputs to include the thoughts, the inference time is negatively affected. Delaying an agent's actions in real-world executions, as in robotic manipulation settings, strongly affects the usability of a method, as tasks require long sequences of actions. However, is the generation of long chains-of-thought a strong prerequisite for achieving performance improvements? In this work, we explore the idea of Hybrid Training (HyT), a framework that enables VLAs to learn from thoughts and benefit from the associated performance gains, while enabling the possibility to leave out CoT generation during inference. Furthermore, by learning to conditionally predict a diverse set of outputs, HyT supports flexibility at inference time, enabling the model to either predict actions directly, generate thoughts or follow instructions. We evaluate the proposed method in a series of simulated benchmarks and real-world experiments.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Stage-wise Conservative Linear Bandits</title>
<link>https://arxiv.org/abs/2510.00602</link>
<guid>https://arxiv.org/abs/2510.00602</guid>
<content:encoded><![CDATA[
arXiv:2510.00602v1 Announce Type: new 
Abstract: In many real-world applications such as recommendation systems, multiple learning agents must balance exploration and exploitation while maintaining safety guarantees to avoid catastrophic failures. We study the stochastic linear bandit problem in a multi-agent networked setting where agents must satisfy stage-wise conservative constraints. A network of $N$ agents collaboratively maximizes cumulative reward while ensuring that the expected reward at every round is no less than $(1-\alpha)$ times that of a baseline policy. Each agent observes local rewards with unknown parameters, but the network optimizes for the global parameter (average of local parameters). Agents communicate only with immediate neighbors, and each communication round incurs additional regret. We propose MA-SCLUCB (Multi-Agent Stage-wise Conservative Linear UCB), an episodic algorithm alternating between action selection and consensus-building phases. We prove that MA-SCLUCB achieves regret $\tilde{O}\left(\frac{d}{\sqrt{N}}\sqrt{T}\cdot\frac{\log(NT)}{\sqrt{\log(1/|\lambda_2|)}}\right)$ with high probability, where $d$ is the dimension, $T$ is the horizon, and $|\lambda_2|$ is the network's second largest eigenvalue magnitude. Our analysis shows: (i) collaboration yields $\frac{1}{\sqrt{N}}$ improvement despite local communication, (ii) communication overhead grows only logarithmically for well-connected networks, and (iii) stage-wise safety adds only lower-order regret. Thus, distributed learning with safety guarantees achieves near-optimal performance in reasonably connected networks.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LVLMs as inspectors: an agentic framework for category-level structural defect annotation</title>
<link>https://arxiv.org/abs/2510.00603</link>
<guid>https://arxiv.org/abs/2510.00603</guid>
<content:encoded><![CDATA[
arXiv:2510.00603v1 Announce Type: new 
Abstract: Automated structural defect annotation is essential for ensuring infrastructure safety while minimizing the high costs and inefficiencies of manual labeling. A novel agentic annotation framework, Agent-based Defect Pattern Tagger (ADPT), is introduced that integrates Large Vision-Language Models (LVLMs) with a semantic pattern matching module and an iterative self-questioning refinement mechanism. By leveraging optimized domain-specific prompting and a recursive verification process, ADPT transforms raw visual data into high-quality, semantically labeled defect datasets without any manual supervision. Experimental results demonstrate that ADPT achieves up to 98% accuracy in distinguishing defective from non-defective images, and 85%-98% annotation accuracy across four defect categories under class-balanced settings, with 80%-92% accuracy on class-imbalanced datasets. The framework offers a scalable and cost-effective solution for high-fidelity dataset construction, providing strong support for downstream tasks such as transfer learning and domain adaptation in structural damage assessment.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Foreground and Background for vision-Language Navigation via Online Augmentation</title>
<link>https://arxiv.org/abs/2510.00604</link>
<guid>https://arxiv.org/abs/2510.00604</guid>
<content:encoded><![CDATA[
arXiv:2510.00604v1 Announce Type: new 
Abstract: Following language instructions, vision-language navigation (VLN) agents are tasked with navigating unseen environments. While augmenting multifaceted visual representations has propelled advancements in VLN, the significance of foreground and background in visual observations remains underexplored. Intuitively, foreground regions provide semantic cues, whereas the background encompasses spatial connectivity information. Inspired on this insight, we propose a Consensus-driven Online Feature Augmentation strategy (COFA) with alternative foreground and background features to facilitate the navigable generalization. Specifically, we first leverage semantically-enhanced landmark identification to disentangle foreground and background as candidate augmented features. Subsequently, a consensus-driven online augmentation strategy encourages the agent to consolidate two-stage voting results on feature preferences according to diverse instructions and navigational locations. Experiments on REVERIE and R2R demonstrate that our online foreground-background augmentation boosts the generalization of baseline and attains state-of-the-art performance.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACON: Optimizing Context Compression for Long-horizon LLM Agents</title>
<link>https://arxiv.org/abs/2510.00615</link>
<guid>https://arxiv.org/abs/2510.00615</guid>
<content:encoded><![CDATA[
arXiv:2510.00615v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed as agents in dynamic, real-world environments, where success requires both reasoning and effective tool use. A central challenge for agentic tasks is the growing context length, as agents must accumulate long histories of actions and observations. This expansion raises costs and reduces efficiency in long-horizon tasks, yet prior work on context compression has mostly focused on single-step tasks or narrow applications. We introduce Agent Context Optimization (ACON), a unified framework that optimally compresses both environment observations and interaction histories into concise yet informative condensations. ACON leverages compression guideline optimization in natural language space: given paired trajectories where full context succeeds but compressed context fails, capable LLMs analyze the causes of failure, and the compression guideline is updated accordingly. Furthermore, we propose distilling the optimized LLM compressor into smaller models to reduce the overhead of the additional module. Experiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON reduces memory usage by 26-54% (peak tokens) while largely preserving task performance, preserves over 95% of accuracy when distilled into smaller compressors, and enhances smaller LMs as long-horizon agents with up to 46% performance improvement.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HARPA: A Testability-Driven, Literature-Grounded Framework for Research Ideation</title>
<link>https://arxiv.org/abs/2510.00620</link>
<guid>https://arxiv.org/abs/2510.00620</guid>
<content:encoded><![CDATA[
arXiv:2510.00620v1 Announce Type: new 
Abstract: While there has been a surge of interest in automated scientific discovery (ASD), especially with the emergence of LLMs, it remains challenging for tools to generate hypotheses that are both testable and grounded in the scientific literature. Additionally, existing ideation tools are not adaptive to prior experimental outcomes. We developed HARPA to address these challenges by incorporating the ideation workflow inspired by human researchers. HARPA first identifies emerging research trends through literature mining, then explores hypothesis design spaces, and finally converges on precise, testable hypotheses by pinpointing research gaps and justifying design choices. Our evaluations show that HARPA-generated hypothesis-driven research proposals perform comparably to a strong baseline AI-researcher across most qualitative dimensions (e.g., specificity, novelty, overall quality), but achieve significant gains in feasibility(+0.78, p$<0.05$, bootstrap) and groundedness (+0.85, p$<0.01$, bootstrap) on a 10-point Likert scale. When tested with the ASD agent (CodeScientist), HARPA produced more successful executions (20 vs. 11 out of 40) and fewer failures (16 vs. 21 out of 40), showing that expert feasibility judgments track with actual execution success. Furthermore, to simulate how researchers continuously refine their understanding of what hypotheses are both testable and potentially interesting from experience, HARPA learns a reward model that scores new hypotheses based on prior experimental outcomes, achieving approx. a 28\% absolute gain over HARPA's untrained baseline scorer. Together, these methods represent a step forward in the field of AI-driven scientific discovery.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative-Distilled Diffusion Models (CDDM) for Accelerated and Lightweight Trajectory Prediction</title>
<link>https://arxiv.org/abs/2510.00627</link>
<guid>https://arxiv.org/abs/2510.00627</guid>
<content:encoded><![CDATA[
arXiv:2510.00627v1 Announce Type: new 
Abstract: Trajectory prediction is a fundamental task in Autonomous Vehicles (AVs) and Intelligent Transportation Systems (ITS), supporting efficient motion planning and real-time traffic safety management. Diffusion models have recently demonstrated strong performance in probabilistic trajectory prediction, but their large model size and slow sampling process hinder real-world deployment. This paper proposes Collaborative-Distilled Diffusion Models (CDDM), a novel method for real-time and lightweight trajectory prediction. Built upon Collaborative Progressive Distillation (CPD), CDDM progressively transfers knowledge from a high-capacity teacher diffusion model to a lightweight student model, jointly reducing both the number of sampling steps and the model size across distillation iterations. A dual-signal regularized distillation loss is further introduced to incorporate guidance from both the teacher and ground-truth data, mitigating potential overfitting and ensuring robust performance. Extensive experiments on the ETH-UCY pedestrian benchmark and the nuScenes vehicle benchmark demonstrate that CDDM achieves state-of-the-art prediction accuracy. The well-distilled CDDM retains 96.2% and 95.5% of the baseline model's ADE and FDE performance on pedestrian trajectories, while requiring only 231K parameters and 4 or 2 sampling steps, corresponding to 161x compression, 31x acceleration, and 9 ms latency. Qualitative results further show that CDDM generates diverse and accurate trajectories under dynamic agent behaviors and complex social interactions. By bridging high-performing generative models with practical deployment constraints, CDDM enables resource-efficient probabilistic prediction for AVs and ITS. Code is available at https://github.com/bingzhangw/CDDM.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Formation Control via Rotation Symmetry Constraints</title>
<link>https://arxiv.org/abs/2510.00676</link>
<guid>https://arxiv.org/abs/2510.00676</guid>
<content:encoded><![CDATA[
arXiv:2510.00676v1 Announce Type: new 
Abstract: We present a distributed formation control strategy for multi-agent systems based only on rotation symmetry constraints. We propose a potential function that enforces inter-agent \textbf{rotational} symmetries, with its gradient defining the control law driving the agents toward a desired symmetric and planar configuration. We show that only $(n-1)$ edges, the minimal connectivity requirement, are sufficient to implement the control strategy, where $n$ is the number of agents. We further augment the design to address the \textbf{maneuvering problem}, enabling the formation to undergo coordinated translations, rotations, and scalings along a predefined virtual trajectory. Numerical simulations demonstrate the effectiveness and flexibility of the proposed method.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Self-Organization in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2510.00685</link>
<guid>https://arxiv.org/abs/2510.00685</guid>
<content:encoded><![CDATA[
arXiv:2510.00685v1 Announce Type: new 
Abstract: Multi-agent systems (MAS) based on Large Language Models (LLMs) have the potential to solve tasks that are beyond the reach of any single LLM. However, this potential can only be realized when the collaboration mechanism between agents is optimized. Specifically, optimizing the communication structure between agents is critical for fruitful collaboration. Most existing approaches rely on fixed topologies, pretrained graph generators, optimization over edges, or employ external LLM judges, thereby adding to the complexity. In this work, we introduce a response-conditioned framework that adapts communication on-the-fly. Agents independently generate responses to the user query and assess peer contributions using an approximation of the Shapley value. A directed acyclic graph (DAG) is then constructed to regulate the propagation of the responses among agents, which ensures stable and efficient message transmission from high-contributing agents to others. This graph is dynamically updated based on the agent responses from the previous collaboration round. Since the proposed framework enables the self-organization of agents without additional supervision or training, we refer to it as SelfOrg. The SelfOrg framework goes beyond task- and query-level optimization and takes into account the stochastic nature of agent responses. Experiments with both strong and weak LLM backends demonstrate robust performance, with significant gains in the weak regime where prior methods collapse. We also theoretically show that multiple agents increase the chance of correctness and that the correct responses naturally dominate the information flow.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TD-JEPA: Latent-predictive Representations for Zero-Shot Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.00739</link>
<guid>https://arxiv.org/abs/2510.00739</guid>
<content:encoded><![CDATA[
arXiv:2510.00739v1 Announce Type: new 
Abstract: Latent prediction--where agents learn by predicting their own latents--has emerged as a powerful paradigm for training general representations in machine learning. In reinforcement learning (RL), this approach has been explored to define auxiliary losses for a variety of settings, including reward-based and unsupervised RL, behavior cloning, and world modeling. While existing methods are typically limited to single-task learning, one-step prediction, or on-policy trajectory data, we show that temporal difference (TD) learning enables learning representations predictive of long-term latent dynamics across multiple policies from offline, reward-free transitions. Building on this, we introduce TD-JEPA, which leverages TD-based latent-predictive representations into unsupervised RL. TD-JEPA trains explicit state and task encoders, a policy-conditioned multi-step predictor, and a set of parameterized policies directly in latent space. This enables zero-shot optimization of any reward function at test time. Theoretically, we show that an idealized variant of TD-JEPA avoids collapse with proper initialization, and learns encoders that capture a low-rank factorization of long-term policy dynamics, while the predictor recovers their successor features in latent space. Empirically, TD-JEPA matches or outperforms state-of-the-art baselines on locomotion, navigation, and manipulation tasks across 13 datasets in ExoRL and OGBench, especially in the challenging setting of zero-shot RL from pixels.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Agentic Systems in Automated Scientific Information Extraction with ChemX</title>
<link>https://arxiv.org/abs/2510.00795</link>
<guid>https://arxiv.org/abs/2510.00795</guid>
<content:encoded><![CDATA[
arXiv:2510.00795v1 Announce Type: new 
Abstract: The emergence of agent-based systems represents a significant advancement in artificial intelligence, with growing applications in automated data extraction. However, chemical information extraction remains a formidable challenge due to the inherent heterogeneity of chemical data. Current agent-based approaches, both general-purpose and domain-specific, exhibit limited performance in this domain. To address this gap, we present ChemX, a comprehensive collection of 10 manually curated and domain-expert-validated datasets focusing on nanomaterials and small molecules. These datasets are designed to rigorously evaluate and enhance automated extraction methodologies in chemistry. To demonstrate their utility, we conduct an extensive benchmarking study comparing existing state-of-the-art agentic systems such as ChatGPT Agent and chemical-specific data extraction agents. Additionally, we introduce our own single-agent approach that enables precise control over document preprocessing prior to extraction. We further evaluate the performance of modern baselines, such as GPT-5 and GPT-5 Thinking, to compare their capabilities with agentic approaches. Our empirical findings reveal persistent challenges in chemical information extraction, particularly in processing domain-specific terminology, complex tabular and schematic representations, and context-dependent ambiguities. The ChemX benchmark serves as a critical resource for advancing automated information extraction in chemistry, challenging the generalization capabilities of existing methods, and providing valuable insights into effective evaluation strategies.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ManagerBench: Evaluating the Safety-Pragmatism Trade-off in Autonomous LLMs</title>
<link>https://arxiv.org/abs/2510.00857</link>
<guid>https://arxiv.org/abs/2510.00857</guid>
<content:encoded><![CDATA[
arXiv:2510.00857v1 Announce Type: new 
Abstract: As large language models (LLMs) evolve from conversational assistants into autonomous agents, evaluating the safety of their actions becomes critical. Prior safety benchmarks have primarily focused on preventing generation of harmful content, such as toxic text. However, they overlook the challenge of agents taking harmful actions when the most effective path to an operational goal conflicts with human safety. To address this gap, we introduce ManagerBench, a benchmark that evaluates LLM decision-making in realistic, human-validated managerial scenarios. Each scenario forces a choice between a pragmatic but harmful action that achieves an operational goal, and a safe action that leads to worse operational performance. A parallel control set, where potential harm is directed only at inanimate objects, measures a model's pragmatism and identifies its tendency to be overly safe. Our findings indicate that the frontier LLMs perform poorly when navigating this safety-pragmatism trade-off. Many consistently choose harmful options to advance their operational goals, while others avoid harm only to become overly safe and ineffective. Critically, we find this misalignment does not stem from an inability to perceive harm, as models' harm assessments align with human judgments, but from flawed prioritization. ManagerBench is a challenging benchmark for a core component of agentic behavior: making safe choices when operational goals and alignment values incentivize conflicting actions. Benchmark & code available at https://github.com/technion-cs-nlp/ManagerBench.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Target Population Synthesis using CT-GAN</title>
<link>https://arxiv.org/abs/2510.00871</link>
<guid>https://arxiv.org/abs/2510.00871</guid>
<content:encoded><![CDATA[
arXiv:2510.00871v1 Announce Type: new 
Abstract: Agent-based models used in scenario planning for transportation and urban planning usually require detailed population information from the base as well as target scenarios. These populations are usually provided by synthesizing fake agents through deterministic population synthesis methods. However, these deterministic population synthesis methods face several challenges, such as handling high-dimensional data, scalability, and zero-cell issues, particularly when generating populations for target scenarios. This research looks into how a deep generative model called Conditional Tabular Generative Adversarial Network (CT-GAN) can be used to create target populations either directly from a collection of marginal constraints or through a hybrid method that combines CT-GAN with Fitness-based Synthesis Combinatorial Optimization (FBS-CO). The research evaluates the proposed population synthesis models against travel survey and zonal-level aggregated population data. Results indicate that the stand-alone CT-GAN model performs the best when compared with FBS-CO and the hybrid model. CT-GAN by itself can create realistic-looking groups that match single-variable distributions, but it struggles to maintain relationships between multiple variables. However, the hybrid model demonstrates improved performance compared to FBS-CO by leveraging CT-GAN ability to generate a descriptive base population, which is then refined using FBS-CO to align with target-year marginals. This study demonstrates that CT-GAN represents an effective methodology for target populations and highlights how deep generative models can be successfully integrated with conventional synthesis techniques to enhance their performance.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QUASAR: Quantum Assembly Code Generation Using Tool-Augmented LLMs via Agentic RL</title>
<link>https://arxiv.org/abs/2510.00967</link>
<guid>https://arxiv.org/abs/2510.00967</guid>
<content:encoded><![CDATA[
arXiv:2510.00967v1 Announce Type: new 
Abstract: Designing and optimizing task-specific quantum circuits are crucial to leverage the advantage of quantum computing. Recent large language model (LLM)-based quantum circuit generation has emerged as a promising automatic solution. However, the fundamental challenges remain unaddressed: (i) parameterized quantum gates require precise numerical values for optimal performance, which also depend on multiple aspects, including the number of quantum gates, their parameters, and the layout/depth of the circuits. (ii) LLMs often generate low-quality or incorrect quantum circuits due to the lack of quantum domain-specific knowledge. We propose QUASAR, an agentic reinforcement learning (RL) framework for quantum circuits generation and optimization based on tool-augmented LLMs. To align the LLM with quantum-specific knowledge and improve the generated quantum circuits, QUASAR designs (i) a quantum circuit verification approach with external quantum simulators and (ii) a sophisticated hierarchical reward mechanism in RL training. Extensive evaluation shows improvements in both syntax and semantic performance of the generated quantum circuits. When augmenting a 4B LLM, QUASAR has achieved the validity of 99.31% in Pass@1 and 100% in Pass@10, outperforming industrial LLMs of GPT-4o, GPT-5 and DeepSeek-V3 and several supervised-fine-tuning (SFT)-only and RL-only baselines.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Code Localization with Repository Memory</title>
<link>https://arxiv.org/abs/2510.01003</link>
<guid>https://arxiv.org/abs/2510.01003</guid>
<content:encoded><![CDATA[
arXiv:2510.01003v1 Announce Type: new 
Abstract: Code localization is a fundamental challenge in repository-level software engineering tasks such as bug fixing. While existing methods equip language agents with comprehensive tools/interfaces to fetch information from the repository, they overlook the critical aspect of memory, where each instance is typically handled from scratch assuming no prior repository knowledge. In contrast, human developers naturally build long-term repository memory, such as the functionality of key modules and associations between various bug types and their likely fix locations. In this work, we augment language agents with such memory by leveraging a repository's commit history - a rich yet underutilized resource that chronicles the codebase's evolution. We introduce tools that allow the agent to retrieve from a non-parametric memory encompassing recent historical commits and linked issues, as well as functionality summaries of actively evolving parts of the codebase identified via commit patterns. We demonstrate that augmenting such a memory can significantly improve LocAgent, a state-of-the-art localization framework, on both SWE-bench-verified and the more recent SWE-bench-live benchmarks. Our research contributes towards developing agents that can accumulate and leverage past experience for long-horizon tasks, more closely emulating the expertise of human developers.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GEM: A Gym for Agentic LLMs</title>
<link>https://arxiv.org/abs/2510.01051</link>
<guid>https://arxiv.org/abs/2510.01051</guid>
<content:encoded><![CDATA[
arXiv:2510.01051v1 Announce Type: new 
Abstract: The training paradigm for large language models (LLMs) is moving from static datasets to experience-based learning, where agents acquire skills via interacting with complex environments. To facilitate this transition we introduce GEM (General Experience Maker), an open-source environment simulator designed for the age of LLMs. Analogous to OpenAI-Gym for traditional reinforcement learning (RL), GEM provides a standardized framework for the environment-agent interface, including asynchronous vectorized execution for high throughput, and flexible wrappers for easy extensibility. GEM also features a diverse suite of environments, robust integrated tools, and single-file example scripts demonstrating using GEM with five popular RL training frameworks. Along with this, we also provide a set of baselines across 24 environments using REINFORCE with Return Batch Normalization (ReBN), which -- unlike GRPO -- is compatible with the full RL setting of dense per-turn rewards and offers better credit assignment. We further conduct apple-to-apple benchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings using GEM to shed light on the algorithmic designs. Lastly, GEM also functions as a convenient evaluation toolkit besides a training environment. We hope this framework can help accelerate future agentic LLM research.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Dialogue State Tracking for Persian Chatbots: A Language Model-Based Approach</title>
<link>https://arxiv.org/abs/2510.01052</link>
<guid>https://arxiv.org/abs/2510.01052</guid>
<content:encoded><![CDATA[
arXiv:2510.01052v1 Announce Type: new 
Abstract: Dialogue State Tracking (DST) is an essential element of conversational AI with the objective of deeply understanding the conversation context and leading it toward answering user requests. Due to high demands for open-domain and multi-turn chatbots, the traditional rule-based DST is not efficient enough, since it cannot provide the required adaptability and coherence for human-like experiences in complex conversations. This study proposes a hybrid DST model that utilizes rule-based methods along with language models, including BERT for slot filling and intent detection, XGBoost for intent validation, GPT for DST, and online agents for real-time answer generation. This model is uniquely designed to be evaluated on a comprehensive Persian multi-turn dialogue dataset and demonstrated significantly improved accuracy and coherence over existing methods in Persian-based chatbots. The results demonstrate how effectively a hybrid approach may improve DST capabilities, paving the way for conversational AI systems that are more customized, adaptable, and human-like.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Network-Knowledge Graph Duality: A Case Study in Agentic Supply Chain Risk Analysis</title>
<link>https://arxiv.org/abs/2510.01115</link>
<guid>https://arxiv.org/abs/2510.01115</guid>
<content:encoded><![CDATA[
arXiv:2510.01115v1 Announce Type: new 
Abstract: Large Language Models (LLMs) struggle with the complex, multi-modal, and network-native data underlying financial risk. Standard Retrieval-Augmented Generation (RAG) oversimplifies relationships, while specialist models are costly and static. We address this gap with an LLM-centric agent framework for supply chain risk analysis. Our core contribution is to exploit the inherent duality between networks and knowledge graphs (KG). We treat the supply chain network as a KG, allowing us to use structural network science principles for retrieval. A graph traverser, guided by network centrality scores, efficiently extracts the most economically salient risk paths. An agentic architecture orchestrates this graph retrieval alongside data from numerical factor tables and news streams. Crucially, it employs novel ``context shells'' -- descriptive templates that embed raw figures in natural language -- to make quantitative data fully intelligible to the LLM. This lightweight approach enables the model to generate concise, explainable, and context-rich risk narratives in real-time without costly fine-tuning or a dedicated graph database.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.01132</link>
<guid>https://arxiv.org/abs/2510.01132</guid>
<content:encoded><![CDATA[
arXiv:2510.01132v1 Announce Type: new 
Abstract: We study what actually works and what doesn't for training large language models as agents via multi-turn reinforcement learning. Despite rapid progress, existing frameworks and definitions are fragmented, and there is no systematic formulation or analysis of which design choices matter across tasks. We address this gap by first breaking down the design space into three inter-related pillars -- environment, reward, and policy -- and empirically derive a recipe for training LLM agents in situated textual domains. In particular, we test TextWorld and ALFWorld, popular domains for testing situated embodied reasoning, as well as SWE-Gym for more software engineering style tasks. (i) For the environment, we analyze the impacts of task complexity in terms of sizes of the state and action spaces as well as optimal solution length, finding that even simple environments within a domain can provide signal on how well an agent can generalize to more complex tasks. (ii) For the reward, we ablate relative reward sparsity, observing that while dense turn-level rewards accelerate training, performance and stability is highly dependent on the choice of RL algorithm. (iii) And for the agent's policy, we explore the interplay between reward sparsity and biased (PPO, GRPO) and unbiased (RLOO) policy gradient methods in addition to showing how to find the optimal Supervised Fine-tuning (SFT) to RL training ratio given a fixed budget. We distill these findings into a training recipe that guides co-design across the three pillars, facilitating research and practical efforts in multi-turn agentic RL. Code: https://github.com/pearls-lab/meow-tea-taro
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code2Video: A Code-centric Paradigm for Educational Video Generation</title>
<link>https://arxiv.org/abs/2510.01174</link>
<guid>https://arxiv.org/abs/2510.01174</guid>
<content:encoded><![CDATA[
arXiv:2510.01174v1 Announce Type: new 
Abstract: While recent generative models advance pixel-space video synthesis, they remain limited in producing professional educational videos, which demand disciplinary knowledge, precise visual structures, and coherent transitions, limiting their applicability in educational scenarios. Intuitively, such requirements are better addressed through the manipulation of a renderable environment, which can be explicitly controlled via logical commands (e.g., code). In this work, we propose Code2Video, a code-centric agent framework for generating educational videos via executable Python code. The framework comprises three collaborative agents: (i) Planner, which structures lecture content into temporally coherent flows and prepares corresponding visual assets; (ii) Coder, which converts structured instructions into executable Python codes while incorporating scope-guided auto-fix to enhance efficiency; and (iii) Critic, which leverages vision-language models (VLM) with visual anchor prompts to refine spatial layout and ensure clarity. To support systematic evaluation, we build MMMC, a benchmark of professionally produced, discipline-specific educational videos. We evaluate MMMC across diverse dimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and particularly, TeachQuiz, a novel end-to-end metric that quantifies how well a VLM, after unlearning, can recover knowledge by watching the generated videos. Our results demonstrate the potential of Code2Video as a scalable, interpretable, and controllable approach, achieving 40% improvement over direct code generation and producing videos comparable to human-crafted tutorials. The code and datasets are available at https://github.com/showlab/Code2Video.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments</title>
<link>https://arxiv.org/abs/2510.01179</link>
<guid>https://arxiv.org/abs/2510.01179</guid>
<content:encoded><![CDATA[
arXiv:2510.01179v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents are rapidly emerging as powerful systems for automating tasks across domains. Yet progress in the open-source community is constrained by the lack of high quality permissively licensed tool-agentic training data. Existing datasets are often limited in diversity, realism, and complexity, particularly regarding multi-tool and multi-turn interactions. To address this gap, we introduce Toucan, the largest publicly available tool-agentic dataset to date, containing 1.5 million trajectories synthesized from nearly 500 real-world Model Context Protocols (MCPs). Unlike prior work, Toucan leverages authentic MCP environments to generate diverse, realistic, and challenging tasks with trajectories involving real tool execution. Our pipeline first produces a broad spectrum of tool-use queries using five distinct models, applies model-based quality filtering, and then generates agentic trajectories with three teacher models using two agentic frameworks. Rigorous rule-based and model-based validation ensures high-quality outputs. We also introduce three extension mechanisms to further diversify tasks and simulate multi-turn conversations. Models fine-tuned on Toucan outperform larger closed-source counterparts on the BFCL V3 benchmark and push the Pareto frontier forward on MCP-Universe Bench.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behavioural Classification in C. elegans: a Spatio-Temporal Analysis of Locomotion</title>
<link>https://arxiv.org/abs/2510.00086</link>
<guid>https://arxiv.org/abs/2510.00086</guid>
<content:encoded><![CDATA[
arXiv:2510.00086v1 Announce Type: cross 
Abstract: The 1mm roundworm C. elegans is a model organism used in many sub-areas of biology to investigate different types of biological processes. In order to complement the n-vivo analysis with computer-based investigations, several methods have been proposed to simulate the worm behaviour. These methods extract discrete behavioural units from the flow of the worm movements using different types of tracking techniques. Nevertheless, these techniques require a clear view of the entire worm body, which is not always achievable. For example, this happens in high density worm conditions, which are particularly informative to understand the influence of the social context on the single worm behaviour. In this paper, we illustrate and evaluate a method to extract behavioural units from recordings of C. elegans movements which do not necessarily require a clear view of the entire worm body. Moreover, the behavioural units are defined by an unsupervised automatic pipeline which frees the process from predefined assumptions that inevitably bias the behavioural analysis. The behavioural units resulting from the automatic method are interpreted by comparing them with hand-designed behavioural units. The effectiveness of the automatic method is evaluated by measuring the extent to which the movement of a simulated worm, with an agent-based model, matches the movement of a natural worm. Our results indicate that spatio-temporal locomotory patterns emerge even from single point worm tracking. Moreover, we show that such patterns represent a fundamental aspect of the behavioural classification process.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asynchronous Nonlinear Sheaf Diffusion for Multi-Agent Coordination</title>
<link>https://arxiv.org/abs/2510.00270</link>
<guid>https://arxiv.org/abs/2510.00270</guid>
<content:encoded><![CDATA[
arXiv:2510.00270v1 Announce Type: cross 
Abstract: Cellular sheaves and sheaf Laplacians provide a far-reaching generalization of graphs and graph Laplacians, resulting in a wide array of applications ranging from machine learning to multi-agent control. In the context of multi-agent systems, so called coordination sheaves provide a unifying formalism that models heterogeneous agents and coordination goals over undirected communication topologies, and applying sheaf diffusion drives agents to achieve their coordination goals. Existing literature on sheaf diffusion assumes that agents can communicate and compute updates synchronously, which is an unrealistic assumption in many scenarios where communication delays or heterogeneous agents with different compute capabilities cause disagreement among agents. To address these challenges, we introduce asynchronous nonlinear sheaf diffusion. Specifically, we show that under mild assumptions on the coordination sheaf and bounded delays in communication and computation, nonlinear sheaf diffusion converges to a minimizer of the Dirichlet energy of the coordination sheaf at a linear rate proportional to the delay bound. We further show that this linear convergence is attained from arbitrary initial conditions and the analysis depends on the spectrum of the sheaf Laplacian in a manner that generalizes the standard graph Laplacian case. We provide several numerical simulations to validate our theoretical results.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Virtual Contrast Enhancement using Longitudinal Data</title>
<link>https://arxiv.org/abs/2510.00418</link>
<guid>https://arxiv.org/abs/2510.00418</guid>
<content:encoded><![CDATA[
arXiv:2510.00418v1 Announce Type: cross 
Abstract: Gadolinium-based contrast agents (GBCAs) are widely used in magnetic resonance imaging (MRI) to enhance lesion detection and characterisation, particularly in the field of neuro-oncology. Nevertheless, concerns regarding gadolinium retention and accumulation in brain and body tissues, most notably for diseases that require close monitoring and frequent GBCA injection, have led to the need for strategies to reduce dosage. In this study, a deep learning framework is proposed for the virtual contrast enhancement of full-dose post-contrast T1-weighted MRI images from corresponding low-dose acquisitions. The contribution of the presented model is its utilisation of longitudinal information, which is achieved by incorporating a prior full-dose MRI examination from the same patient. A comparative evaluation against a non-longitudinal single session model demonstrated that the longitudinal approach significantly improves image quality across multiple reconstruction metrics. Furthermore, experiments with varying simulated contrast doses confirmed the robustness of the proposed method. These results emphasize the potential of integrating prior imaging history into deep learning-based virtual contrast enhancement pipelines to reduce GBCA usage without compromising diagnostic utility, thus paving the way for safer, more sustainable longitudinal monitoring in clinical MRI practice.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A semi-Lagrangian method for solving state constraint Mean Field Games in Macroeconomics</title>
<link>https://arxiv.org/abs/2510.00768</link>
<guid>https://arxiv.org/abs/2510.00768</guid>
<content:encoded><![CDATA[
arXiv:2510.00768v1 Announce Type: cross 
Abstract: We study continuous-time heterogeneous agent models cast as Mean Field Games, in the Aiyagari-Bewley-Huggett framework. The model couples a Hamilton-Jacobi-Bellman equation for individual optimization with a Fokker-Planck-Kolmogorov equation for the wealth distribution. We establish a comparison principle for constrained viscosity solutions of the HJB equation and propose a semi-Lagrangian (SL) scheme for its numerical solution, proving convergence via the Barles-Souganidis method. A policy iteration algorithm handles state constraints, and a dual SL scheme is used for the FPK equation. Numerical methods are presented in a fully discrete, implementable form.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Networked Control and Mean Field Problems Under Diagonal Dominance: Decentralized and Social Optimality</title>
<link>https://arxiv.org/abs/2510.01067</link>
<guid>https://arxiv.org/abs/2510.01067</guid>
<content:encoded><![CDATA[
arXiv:2510.01067v1 Announce Type: cross 
Abstract: In this article, we employ an input-output approach to expand the study of cooperative multi-agent control and optimization problems characterized by mean-field interactions that admit decentralized and selfish solutions. The setting involves $n$ independent agents that interact solely through a shared cost function, which penalizes deviations of each agent from the group's average collective behavior. Building on our earlier results established for homogeneous agents, we extend the framework to nonidentical agents and show that, under a diagonal dominant interaction of the collective dynamics, with bounded local open-loop dynamics, the optimal controller for $H_\infty$ and $H_2$ norm minimization remains decentralized and selfish in the limit as the number of agents $n$ grows to infinity.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Inhibitor: ReLU and Addition-Based Attention for Efficient Transformers under Fully Homomorphic Encryption on the Torus</title>
<link>https://arxiv.org/abs/2310.02041</link>
<guid>https://arxiv.org/abs/2310.02041</guid>
<content:encoded><![CDATA[
arXiv:2310.02041v2 Announce Type: replace 
Abstract: To enhance the computational efficiency of quantized Transformers, we replace the dot-product and Softmax-based attention with an alternative mechanism involving addition and ReLU activation only. This side-steps the expansion to double precision often required by matrix multiplication and avoids costly Softmax evaluations but maintains much of the core functionality of conventional dot-product attention. It can enable more efficient execution and support larger quantized Transformer models on resource-constrained hardware or alternative arithmetic systems like homomorphic encryption. Training experiments on four common benchmark tasks show test set prediction scores comparable to those of conventional Transformers with dot-product attention. Our scaling experiments also suggest significant computational savings, both in plaintext and under encryption. In particular, we believe that the ReLU and addition-based attention mechanism examined in this paper may enable privacy-preserving AI applications operating under homomorphic encryption by avoiding the costly multiplication of encrypted variables.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Equilibrium Model for Schedule-Based Transit Networks with Hard Vehicle Capacities</title>
<link>https://arxiv.org/abs/2406.17153</link>
<guid>https://arxiv.org/abs/2406.17153</guid>
<content:encoded><![CDATA[
arXiv:2406.17153v2 Announce Type: replace 
Abstract: Modelling passenger assignments in public transport networks is a fundamental task for city planners, especially when deliberating network infrastructure decisions. A key aspect of a realistic model is to integrate passengers' selfish routing behaviour under limited vehicle capacities. We formulate a side-constrained user equilibrium model in a schedule-based transit network, where passengers are modelled via a continuum of non-atomic agents that travel from their origin to their destination. An agent's route may comprise several rides along given lines, each using vehicles with hard loading capacities. We give a characterization of (side-constrained) user equilibria via a quasi-variational inequality and prove their existence for fixed departure times by generalizing a well-known result of Bernstein and Smith (Transp. Sci., 1994). We further derive a polynomial time algorithm for single-commodity instances with fixed departure times. For the multi-commodity case with departure time choice, we show that deciding whether an equilibrium exists is NP-hard, and we devise an exponential-time algorithm that computes an equilibrium if it exists, and signals non-existence otherwise. Using our quasi-variational characterization, we formulate a heuristic for computing multi-commodity user equilibria in practice, which is tested on multiple real-world instances. In terms of social cost, the computed user-equilibria are quite efficient compared to a system optimum.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SL$^{2}$A-INR: Single-Layer Learnable Activation for Implicit Neural Representation</title>
<link>https://arxiv.org/abs/2409.10836</link>
<guid>https://arxiv.org/abs/2409.10836</guid>
<content:encoded><![CDATA[
arXiv:2409.10836v4 Announce Type: replace 
Abstract: Implicit Neural Representation (INR), leveraging a neural network to transform coordinate input into corresponding attributes, has recently driven significant advances in several vision-related domains. However, the performance of INR is heavily influenced by the choice of the nonlinear activation function used in its multilayer perceptron (MLP) architecture. To date, multiple nonlinearities have been investigated, but current INRs still face limitations in capturing high-frequency components and diverse signal types. We show that these challenges can be alleviated by introducing a novel approach in INR architecture. Specifically, we propose SL$^{2}$A-INR, a hybrid network that combines a single-layer learnable activation function with an MLP that uses traditional ReLU activations. Our method performs superior across diverse tasks, including image representation, 3D shape reconstruction, and novel view synthesis. Through comprehensive experiments, SL$^{2}$A-INR sets new benchmarks in accuracy, quality, and robustness for INR. Our Code is publicly available on~\href{https://github.com/Iceage7/SL2A-INR}{\textcolor{magenta}{GitHub}}.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Almost Sure Convergence of Networked Policy Gradient over Time-Varying Networks in Markov Potential Games</title>
<link>https://arxiv.org/abs/2410.20075</link>
<guid>https://arxiv.org/abs/2410.20075</guid>
<content:encoded><![CDATA[
arXiv:2410.20075v2 Announce Type: replace 
Abstract: We propose networked policy gradient play for solving Markov potential games with continuous and/or discrete state-action pairs. During the game, agents use parametrized and differentiable policies that depend on the current state and the policy parameters of other agents. During training, agents update their policy parameters following stochastic gradients. The gradient estimation involves two consecutive episodes, generating unbiased estimators of reward and policy score functions. In addition, it involves keeping estimates of others' parameters using consensus steps given local estimates received through a time-varying communication network. In Markov potential games, there exists a potential value function among agents with gradients corresponding to the gradients of local value functions. Using this structure, we prove almost sure convergence to a stationary point of the potential value function with rate $O(1/\epsilon^2)$. Compared to previous works, our results do not require bounded policy gradients or initial agreement on the values of individual policy parameters. Numerical experiments on a dynamic multi-agent newsvendor problem verify the convergence of local beliefs and gradients. It further shows that networked policy gradient play converges as fast as independent policy gradient updates, while collecting higher rewards.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Application of Model Predictive Control to a Weighted Coverage Path Planning Problem</title>
<link>https://arxiv.org/abs/2411.08634</link>
<guid>https://arxiv.org/abs/2411.08634</guid>
<content:encoded><![CDATA[
arXiv:2411.08634v2 Announce Type: replace 
Abstract: This paper considers the application of Model Predictive Control (MPC) to a weighted coverage path planning (WCPP) problem. The problem appears in a wide range of practical applications, including search and rescue (SAR) missions. The basic setup is that one (or multiple) agents can move around a given search space and collect rewards from a given spatial distribution. Unlike an artificial potential field, each reward can only be collected once. In contrast to a Traveling Salesman Problem (TSP), the agent moves in a continuous space. Moreover, he is not obliged to cover all locations and/or may return to previously visited locations. The WCPP problem is tackled by a new Model Predictive Control (MPC) formulation with so-called Coverage Constraints (CCs). It is shown that the solution becomes more effective if the solver is initialized with a TSP-based heuristic. With and without this initialization, the proposed MPC approach clearly outperforms a naive MPC formulation, as demonstrated in a small simulation study.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring and Controlling Diversity in LLM-Agent Conversation</title>
<link>https://arxiv.org/abs/2412.21102</link>
<guid>https://arxiv.org/abs/2412.21102</guid>
<content:encoded><![CDATA[
arXiv:2412.21102v3 Announce Type: replace 
Abstract: Controlling diversity in LLM-agent simulations is essential for balancing stability in structured tasks with variability in open-ended interactions. However, we observe that dialogue diversity tends to degrade over long-term simulations. To explore the role of prompt design in this phenomenon, we modularized the utterance generation prompt and found that reducing contextual information leads to more diverse outputs. Based on this insight, we propose Adaptive Prompt Pruning (APP), a novel method that allows users to control diversity via a single parameter, lambda. APP dynamically prunes prompt segments based on attention scores and is compatible with existing diversity control methods. We demonstrate that APP effectively modulates diversity through extensive experiments and propose a method to balance the control trade-offs. Our analysis reveals that all prompt components impose constraints on diversity, with the Memory being the most influential. Additionally, high-attention contents consistently suppress output diversity.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing Moral Uncertainty using Large Language Models for Ethical Decision-Making</title>
<link>https://arxiv.org/abs/2503.05724</link>
<guid>https://arxiv.org/abs/2503.05724</guid>
<content:encoded><![CDATA[
arXiv:2503.05724v2 Announce Type: replace 
Abstract: We present an ethical decision-making framework that refines a pre-trained reinforcement learning (RL) model using a task-agnostic ethical layer. Following initial training, the RL model undergoes ethical fine-tuning, where human feedback is replaced by feedback generated from a large language model (LLM). The LLM embodies consequentialist, deontological, virtue, social justice, and care ethics as moral principles to assign belief values to recommended actions during ethical decision-making. An ethical layer aggregates belief scores from multiple LLM-derived moral perspectives using Belief Jensen-Shannon Divergence and Dempster-Shafer Theory into probability scores that also serve as the shaping reward, steering the agent toward choices that align with a balanced ethical framework. This integrated learning framework helps the RL agent navigate moral uncertainty in complex environments and enables it to make morally sound decisions across diverse tasks. Our approach, tested across different LLM variants and compared with other belief aggregation techniques, demonstrates improved consistency, adaptability, and reduced reliance on handcrafted ethical rewards. This method is especially effective in dynamic scenarios where ethical challenges arise unexpectedly, making it well-suited for real-world applications.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions</title>
<link>https://arxiv.org/abs/2503.22678</link>
<guid>https://arxiv.org/abs/2503.22678</guid>
<content:encoded><![CDATA[
arXiv:2503.22678v2 Announce Type: replace 
Abstract: In this work, we introduce MedAgentSim, an open-source simulated clinical environment with doctor, patient, and measurement agents designed to evaluate and enhance LLM performance in dynamic diagnostic settings. Unlike prior approaches, our framework requires doctor agents to actively engage with patients through multi-turn conversations, requesting relevant medical examinations (e.g., temperature, blood pressure, ECG) and imaging results (e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnostic process. Additionally, we incorporate self improvement mechanisms that allow models to iteratively refine their diagnostic strategies. We enhance LLM performance in our simulated setting by integrating multi-agent discussions, chain-of-thought reasoning, and experience-based knowledge retrieval, facilitating progressive learning as doctor agents interact with more patients. We also introduce an evaluation benchmark for assessing the LLM's ability to engage in dynamic, context-aware diagnostic interactions. While MedAgentSim is fully automated, it also supports a user-controlled mode, enabling human interaction with either the doctor or patient agent. Comprehensive evaluations in various simulated diagnostic scenarios demonstrate the effectiveness of our approach. Our code, simulation tool, and benchmark are available at \href{https://medagentsim.netlify.app/}.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents</title>
<link>https://arxiv.org/abs/2504.10458</link>
<guid>https://arxiv.org/abs/2504.10458</guid>
<content:encoded><![CDATA[
arXiv:2504.10458v4 Announce Type: replace 
Abstract: Existing efforts in building Graphical User Interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning on Large Vision-Language Models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose \name, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and employing policy optimization algorithms such as Group Relative Policy Optimization (GRPO) to update the model, \name achieves superior performance using only 0.02\% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ambiguity in LLMs is a concept missing problem</title>
<link>https://arxiv.org/abs/2505.11679</link>
<guid>https://arxiv.org/abs/2505.11679</guid>
<content:encoded><![CDATA[
arXiv:2505.11679v3 Announce Type: replace 
Abstract: Ambiguity in natural language is a significant obstacle for achieving accurate text to structured data mapping through large language models (LLMs), which affects the performance of tasks such as mapping text to agentic tool calling and text-to-SQL queries. Existing methods to ambiguity handling either rely on the ReACT framework to obtain correct mappings through trial and error, or on supervised fine-tuning to bias models toward specific tasks. In this paper, we adopt a different approach that characterizes representation differences of ambiguous text in the latent space and leverages these differences to identify ambiguity before mapping them to structured data. To detect sentence-level ambiguity, we focus on the relationship between ambiguous questions and their interpretations. Unlike distances calculated by dense embeddings, we introduce a new distance measure based on a path kernel over concepts. With this measurement, we identify patterns to distinguish ambiguous from unambiguous questions. Furthermore, we propose a method for improving LLM performance on ambiguous agentic tool calling through missing concept prediction. Both achieve state-of-the-art results.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EVALOOOP: A Self-Consistency-Centered Framework for Assessing Large Language Model Robustness in Programming</title>
<link>https://arxiv.org/abs/2505.12185</link>
<guid>https://arxiv.org/abs/2505.12185</guid>
<content:encoded><![CDATA[
arXiv:2505.12185v4 Announce Type: replace 
Abstract: Evaluating the programming robustness of large language models (LLMs) is paramount for ensuring their reliability in AI-based software development. However, adversarial attacks exhibit fundamental limitations that compromise fair robustness assessment: they demonstrate contradictory evaluation outcomes where different attack strategies tend to favor different models, and more critically, they operate solely through external perturbations, failing to capture the intrinsic stability essential for autonomous coding agents where subsequent inputs are endogenously generated by the model itself. We introduce EVALOOOP, a novel assessment framework that evaluates robustness from a self-consistency perspective, leveraging the natural duality inherent in software engineering tasks (e.g., code generation and code summarization). EVALOOOP establishes a self-contained feedback loop where an LLM iteratively transforms between code and natural language until functional failure occurs, with robustness quantified by a novel Average Sustainable Loops (ASL) metric-the mean number of iterations maintaining functional correctness across benchmark tasks. This cyclical strategy intrinsically evaluates robustness without relying on external attack configurations, providing a unified metric that reveals how effectively LLMs preserve semantic integrity through sustained self-referential transformations. We evaluate 96 popular LLMs, ranging from 0.5B to 685B parameters, on EVALOOOP equipped with the MBPP Plus benchmark, and found that EVALOOOP typically induces a 2.65%-47.62% absolute drop in pass@1 accuracy within ten loops. Intriguingly, robustness does not always align with initial performance (i.e., one-time query); for instance, Qwen3-235B-A22B-Instruct-2507, despite inferior initial code generation compared to OpenAI's o-series models and DeepSeek-V3, demonstrated the superior robustness (ASL score).
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R&amp;D-Agent: An LLM-Agent Framework Towards Autonomous Data Science</title>
<link>https://arxiv.org/abs/2505.14738</link>
<guid>https://arxiv.org/abs/2505.14738</guid>
<content:encoded><![CDATA[
arXiv:2505.14738v2 Announce Type: replace 
Abstract: Recent advances in AI and ML have transformed data science, yet increasing complexity and expertise requirements continue to hinder progress. Although crowd-sourcing platforms alleviate some challenges, high-level machine learning engineering (MLE) tasks remain labor-intensive and iterative. We introduce R&amp;D-Agent, a comprehensive, decoupled, and extensible framework that formalizes the MLE process. R&amp;D-Agent defines the MLE workflow into two phases and six components, turning agent design for MLE from ad-hoc craftsmanship into a principled, testable process. Although several existing agents report promising gains on their chosen components, they can mostly be summarized as a partial optimization from our framework's simple baseline. Inspired by human experts, we designed efficient and effective agents within this framework that achieve state-of-the-art performance. Evaluated on MLE-Bench, the agent built on R&amp;D-Agent ranks as the top-performing machine learning engineering agent, achieving 35.1% any medal rate, demonstrating the ability of the framework to speed up innovation and improve accuracy across a wide range of data science applications. We have open-sourced R&amp;D-Agent on GitHub: https://github.com/microsoft/RD-Agent.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning</title>
<link>https://arxiv.org/abs/2505.16928</link>
<guid>https://arxiv.org/abs/2505.16928</guid>
<content:encoded><![CDATA[
arXiv:2505.16928v2 Announce Type: replace 
Abstract: We introduce $\infty$-THOR, a new framework for long-horizon embodied tasks that advances long-context understanding in embodied AI. $\infty$-THOR provides: (1) a generation framework for synthesizing scalable, reproducible, and unlimited long-horizon trajectories; (2) a novel embodied QA task, Needle(s) in the Embodied Haystack, where multiple scattered clues across extended trajectories test agents' long-context reasoning ability; and (3) a long-horizon dataset and benchmark suite featuring complex tasks that span hundreds of environment steps, each paired with ground-truth action sequences. To enable this capability, we explore architectural adaptations, including interleaved Goal-State-Action modeling, context extension techniques, and Context Parallelism, to equip LLM-based agents for extreme long-context reasoning and interaction. Experimental results and analyses highlight the challenges posed by our benchmark and provide insights into training strategies and model behaviors under long-horizon conditions. Our work provides a foundation for the next generation of embodied AI systems capable of robust, long-term reasoning and planning.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The challenge of hidden gifts in multi-agent reinforcement learning</title>
<link>https://arxiv.org/abs/2505.20579</link>
<guid>https://arxiv.org/abs/2505.20579</guid>
<content:encoded><![CDATA[
arXiv:2505.20579v5 Announce Type: replace 
Abstract: Sometimes we benefit from actions that others have taken even when we are unaware that they took those actions. For example, if your neighbor chooses not to take a parking spot in front of your house when you are not there, you can benefit, even without being aware that they took this action. These ``hidden gifts'' represent an interesting challenge for multi-agent reinforcement learning (MARL), since assigning credit when the beneficial actions of others are hidden is non-trivial. Here, we study the impact of hidden gifts with a very simple MARL task. In this task, agents in a grid-world environment have individual doors to unlock in order to obtain individual rewards. As well, if all the agents unlock their door the group receives a larger collective reward. However, there is only one key for all of the doors, such that the collective reward can only be obtained when the agents drop the key for others after they use it. Notably, there is nothing to indicate to an agent that the other agents have dropped the key, thus this act for others is a ``hidden gift''. We show that several different state-of-the-art MARL algorithms, including MARL specific architectures, fail to learn how to obtain the collective reward in this simple task. Interestingly, we find that decentralized actor-critic policy gradient agents can succeed when we provide them with information about their own action history, but MARL agents still cannot solve the task with action history. Finally, we derive a correction term for policy gradient agents, inspired by learning aware approaches, which reduces the variance in learning and helps them to converge to collective success more reliably. These results show that credit assignment in multi-agent settings can be particularly challenging in the presence of ``hidden gifts'', and demonstrate that self learning-awareness in decentralized agents can benefit these settings.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropic Risk Optimization in Discounted MDPs: Sample Complexity Bounds with a Generative Model</title>
<link>https://arxiv.org/abs/2506.00286</link>
<guid>https://arxiv.org/abs/2506.00286</guid>
<content:encoded><![CDATA[
arXiv:2506.00286v2 Announce Type: replace 
Abstract: In this paper, we analyze the sample complexities of learning the optimal state-action value function $Q^*$ and an optimal policy $\pi^*$ in a finite discounted Markov decision process (MDP) where the agent has recursive entropic risk-preferences with risk-parameter $\beta\neq 0$ and where a generative model of the MDP is available. We provide and analyze a simple model based approach which we call model-based risk-sensitive $Q$-value-iteration (MB-RS-QVI) which leads to $(\varepsilon,\delta)$-PAC-bounds on $\|Q^*-Q^k\|$, and $\|V^*-V^{\pi_k}\|$ where $Q_k$ is the output of MB-RS-QVI after k iterations and $\pi_k$ is the greedy policy with respect to $Q_k$. Both PAC-bounds have exponential dependence on the effective horizon $\frac{1}{1-\gamma}$ and the strength of this dependence grows with the learners risk-sensitivity $|\beta|$. We also provide two lower bounds which shows that exponential dependence on $|\beta|\frac{1}{1-\gamma}$ is unavoidable in both cases. The lower bounds reveal that the PAC-bounds are tight in the parameters $S,A,\delta,\varepsilon$ and that unlike in the classical setting it is not possible to have polynomial dependence in all model parameters.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents</title>
<link>https://arxiv.org/abs/2506.04018</link>
<guid>https://arxiv.org/abs/2506.04018</guid>
<content:encoded><![CDATA[
arXiv:2506.04018v2 Announce Type: replace 
Abstract: As Large Language Model (LLM) agents become more widespread, associated misalignment risks increase. While prior research has studied agents' ability to produce harmful outputs or follow malicious instructions, it remains unclear how likely agents are to spontaneously pursue unintended goals in realistic deployments. In this work, we approach misalignment as a conflict between the internal goals pursued by the model and the goals intended by its deployer. We introduce a misalignment propensity benchmark, \textsc{AgentMisalignment}, a benchmark suite designed to evaluate the propensity of LLM agents to misalign in realistic scenarios. Evaluations cover behaviours such as avoiding oversight, resisting shutdown, sandbagging, and power-seeking. Testing frontier models, we find that more capable agents tend to exhibit higher misalignment on average. We also systematically vary agent personalities through different system prompts and observe that persona characteristics can strongly and unpredictably influence misalignment, sometimes more than the choice of model itself. Our results reveal the limitations of current alignment methods for autonomous LLM agents and underscore the need to rethink misalignment in realistic deployment settings.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers</title>
<link>https://arxiv.org/abs/2506.15674</link>
<guid>https://arxiv.org/abs/2506.15674</guid>
<content:encoded><![CDATA[
arXiv:2506.15674v2 Announce Type: replace 
Abstract: We study privacy leakage in the reasoning traces of large reasoning models used as personal agents. Unlike final outputs, reasoning traces are often assumed to be internal and safe. We challenge this assumption by showing that reasoning traces frequently contain sensitive user data, which can be extracted via prompt injections or accidentally leak into outputs. Through probing and agentic evaluations, we demonstrate that test-time compute approaches, particularly increased reasoning steps, amplify such leakage. While increasing the budget of those test-time compute approaches makes models more cautious in their final answers, it also leads them to reason more verbosely and leak more in their own thinking. This reveals a core tension: reasoning improves utility but enlarges the privacy attack surface. We argue that safety efforts must extend to the model's internal thinking, not just its outputs.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Progress Bar for Reasoning: Progress Prediction in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2506.23274</link>
<guid>https://arxiv.org/abs/2506.23274</guid>
<content:encoded><![CDATA[
arXiv:2506.23274v2 Announce Type: replace 
Abstract: Reasoning models that produce long, hidden chains of thought, have emerged as powerful tools for reasoning-intensive and agentic tasks. However, as the time horizons at which these models can operate grow exponentially, it becomes increasingly difficult to know how much progress the model is making on a task, making it challenging for users to set appropriate expectations about completion time. By probing the internal representations of Large Language Models (LLMs), we find evidence that their reasoning progress can be quantified, with simple linear probes achieving 30\% accuracy over 10 progress classes and Mean Absolute Error (MAE) of 1.75. Rooted in this insight, we propose a two-stage fine-tuning method that trains existing reasoning models to explicitly generate progress estimates (0-100\%) during their reasoning process. We find that the predictions of our best fine-tuned language model for sequences below 16K tokens are on average 10\% from the true label.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterogeneous Predictor-based Risk-Aware Planning with Conformal Prediction in Dense, Uncertain Environments</title>
<link>https://arxiv.org/abs/2507.11920</link>
<guid>https://arxiv.org/abs/2507.11920</guid>
<content:encoded><![CDATA[
arXiv:2507.11920v2 Announce Type: replace 
Abstract: Real-time planning among many uncertain, dynamic obstacles is challenging because predicting every agent with high fidelity is both unnecessary and computationally expensive. We present Heterogeneous Predictor-based Risk-Aware Planning (H-PRAP), a framework that allocates prediction effort to where it matters. H-PRAP introduces the Probability-based Collision Risk Index (P-CRI), a closed-form, horizon-level collision index obtained by calibrating a Gaussian surrogate with conformal prediction. P-CRI drives a router that assigns high-risk obstacles to accurate but expensive predictors and low-risk obstacles to lightweight predictors, while preserving distribution-free coverage across heterogeneous predictors through conformal prediction. The selected predictions and their conformal radii are embedded in a chance-constrained model predictive control (MPC) problem, yielding receding-horizon policies with explicit safety margins. We analyze the safety-efficiency trade-off under prediction compute budget: more portion of low-fidelity predictions reduce residual risk from dropped obstacles, but in the same time induces larger conformal radii and degrades trajectory efficiency and shrinks MPC feasibility. Extensive numerical simulations in dense, uncertain environments validate that H-PRAP attains best balance between trajectory success rate (i.e., no collisions) and the time to reach the goal (i.e., trajectory efficiency) compared to single prediction architectures.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stackelberg Coupling of Online Representation Learning and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.07452</link>
<guid>https://arxiv.org/abs/2508.07452</guid>
<content:encoded><![CDATA[
arXiv:2508.07452v2 Announce Type: replace 
Abstract: Deep Q-learning jointly learns representations and values within monolithic networks, promising beneficial co-adaptation between features and value estimates. Although this architecture has attained substantial success, the coupling between representation and value learning creates instability as representations must constantly adapt to non-stationary value targets, while value estimates depend on these shifting representations. This is compounded by high variance in bootstrapped targets, which causes bias in value estimation in off-policy methods. We introduce Stackelberg Coupled Representation and Reinforcement Learning (SCORER), a framework for value-based RL that views representation and Q-learning as two strategic agents in a hierarchical game. SCORER models the Q-function as the leader, which commits to its strategy by updating less frequently, while the perception network (encoder) acts as the follower, adapting more frequently to learn representations that minimize Bellman error variance given the leader's committed strategy. Through this division of labor, the Q-function minimizes MSBE while perception minimizes its variance, thereby reducing bias accordingly, with asymmetric updates allowing stable co-adaptation, unlike simultaneous parameter updates in monolithic solutions. Our proposed SCORER framework leads to a bi-level optimization problem whose solution is approximated by a two-timescale algorithm that creates an asymmetric learning dynamic between the two players. Extensive experiments on DQN and its variants demonstrate that gains stem from algorithmic insight rather than model complexity.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents</title>
<link>https://arxiv.org/abs/2508.07642</link>
<guid>https://arxiv.org/abs/2508.07642</guid>
<content:encoded><![CDATA[
arXiv:2508.07642v2 Announce Type: replace 
Abstract: Vision-and-Language Navigation (VLN) poses significant challenges for agents to interpret natural language instructions and navigate complex 3D environments. While recent progress has been driven by large-scale pre-training and data augmentation, current methods still struggle to generalize to unseen scenarios, particularly when complex spatial and temporal reasoning is required. In this work, we propose SkillNav, a modular framework that introduces structured, skill-based reasoning into Transformer-based VLN agents. Our method decomposes navigation into a set of interpretable atomic skills (e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each handled by a specialized agent. To support targeted skill training without manual data annotation, we construct a synthetic dataset pipeline that generates diverse, linguistically natural, skill-specific instruction-trajectory pairs. We then introduce a novel training-free Vision-Language Model (VLM)-based router, which dynamically selects the most suitable agent at each time step by aligning sub-goals with visual observations and historical actions. SkillNav obtains competitive results on commonly used benchmarks and establishes state-of-the-art generalization to the GSA-R2R, a benchmark with novel instruction styles and unseen environments.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code Like Humans: A Multi-Agent Solution for Medical Coding</title>
<link>https://arxiv.org/abs/2509.05378</link>
<guid>https://arxiv.org/abs/2509.05378</guid>
<content:encoded><![CDATA[
arXiv:2509.05378v2 Announce Type: replace 
Abstract: In medical coding, experts map unstructured clinical notes to alphanumeric codes for diagnoses and procedures. We introduce Code Like Humans: a new agentic framework for medical coding with large language models. It implements official coding guidelines for human experts, and it is the first solution that can support the full ICD-10 coding system (+70K labels). It achieves the best performance to date on rare diagnosis codes (fine-tuned discriminative classifiers retain an advantage for high-frequency codes, to which they are limited). Towards future work, we also contribute an analysis of system performance and identify its `blind spots' (codes that are systematically undercoded).
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interactive Recommendation Agent with Active User Commands</title>
<link>https://arxiv.org/abs/2509.21317</link>
<guid>https://arxiv.org/abs/2509.21317</guid>
<content:encoded><![CDATA[
arXiv:2509.21317v2 Announce Type: replace 
Abstract: Traditional recommender systems rely on passive feedback mechanisms that limit users to simple choices such as like and dislike. However, these coarse-grained signals fail to capture users' nuanced behavior motivations and intentions. In turn, current systems cannot also distinguish which specific item attributes drive user satisfaction or dissatisfaction, resulting in inaccurate preference modeling. These fundamental limitations create a persistent gap between user intentions and system interpretations, ultimately undermining user satisfaction and harming system effectiveness.
  To address these limitations, we introduce the Interactive Recommendation Feed (IRF), a pioneering paradigm that enables natural language commands within mainstream recommendation feeds. Unlike traditional systems that confine users to passive implicit behavioral influence, IRF empowers active explicit control over recommendation policies through real-time linguistic commands. To support this paradigm, we develop RecBot, a dual-agent architecture where a Parser Agent transforms linguistic expressions into structured preferences and a Planner Agent dynamically orchestrates adaptive tool chains for on-the-fly policy adjustment. To enable practical deployment, we employ simulation-augmented knowledge distillation to achieve efficient performance while maintaining strong reasoning capabilities. Through extensive offline and long-term online experiments, RecBot shows significant improvements in both user satisfaction and business outcomes.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DBF-MA: A Differential Bayesian Filtering Planner for Multi-Agent Autonomous Racing Overtakes</title>
<link>https://arxiv.org/abs/2509.22937</link>
<guid>https://arxiv.org/abs/2509.22937</guid>
<content:encoded><![CDATA[
arXiv:2509.22937v2 Announce Type: replace 
Abstract: A significant challenge in autonomous racing is to generate overtaking maneuvers. Racing agents must execute these maneuvers on complex racetracks with little room for error. Optimization techniques and graph-based methods have been proposed, but these methods often rely on oversimplified assumptions for collision-avoidance and dynamic constraints. In this work, we present an approach to trajectory synthesis based on an extension of the Differential Bayesian Filtering framework. Our approach for collision-free trajectory synthesis frames the problem as one of Bayesian Inference over the space of Composite Bezier Curves. Our method is derivative-free, does not require a spherical approximation of the vehicle footprint, linearization of constraints, or simplifying upper bounds on collision avoidance. We conduct a closed-loop analysis of DBF-MA and find it successfully overtakes an opponent in 87% of tested scenarios, outperforming existing methods in autonomous overtaking.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning with Discrete Diffusion Policies for Combinatorial Action Spaces</title>
<link>https://arxiv.org/abs/2509.22963</link>
<guid>https://arxiv.org/abs/2509.22963</guid>
<content:encoded><![CDATA[
arXiv:2509.22963v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) struggles to scale to large, combinatorial action spaces common in many real-world problems. This paper introduces a novel framework for training discrete diffusion models as highly effective policies in these complex settings. Our key innovation is an efficient online training process that ensures stable and effective policy improvement. By leveraging policy mirror descent (PMD) to define an ideal, regularized target policy distribution, we frame the policy update as a distributional matching problem, training the expressive diffusion model to replicate this stable target. This decoupled approach stabilizes learning and significantly enhances training performance. Our method achieves state-of-the-art results and superior sample efficiency across a diverse set of challenging combinatorial benchmarks, including DNA sequence generation, RL with macro-actions, and multi-agent systems. Experiments demonstrate that our diffusion policies attain superior performance compared to other baselines.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking LLM-Assisted Blue Teaming via Standardized Threat Hunting</title>
<link>https://arxiv.org/abs/2509.23571</link>
<guid>https://arxiv.org/abs/2509.23571</guid>
<content:encoded><![CDATA[
arXiv:2509.23571v2 Announce Type: replace 
Abstract: As cyber threats continue to grow in scale and sophistication, blue team defenders increasingly require advanced tools to proactively detect and mitigate risks. Large Language Models (LLMs) offer promising capabilities for enhancing threat analysis. However, their effectiveness in real-world blue team threat-hunting scenarios remains insufficiently explored. This paper presents CyberTeam, a benchmark designed to guide LLMs in blue teaming practice. CyberTeam constructs a standardized workflow in two stages. First, it models realistic threat-hunting workflows by capturing the dependencies among analytical tasks from threat attribution to incident response. Next, each task is addressed through a set of operational modules tailored to its specific analytical requirements. This transforms threat hunting into a structured sequence of reasoning steps, with each step grounded in a discrete operation and ordered according to task-specific dependencies. Guided by this framework, LLMs are directed to perform threat-hunting tasks through modularized steps. Overall, CyberTeam integrates 30 tasks and 9 operational modules to guide LLMs through standardized threat analysis. We evaluate both leading LLMs and state-of-the-art cybersecurity agents, comparing CyberTeam against open-ended reasoning strategies. Our results highlight the improvements enabled by standardized design, while also revealing the limitations of open-ended reasoning in real-world threat hunting.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastically Structured Reservoir Computers for Financial and Economic System Identification</title>
<link>https://arxiv.org/abs/2507.17115</link>
<guid>https://arxiv.org/abs/2507.17115</guid>
<content:encoded><![CDATA[
arXiv:2507.17115v2 Announce Type: replace-cross 
Abstract: This paper introduces a methodology for identifying and simulating financial and economic systems using stochastically structured reservoir computers (SSRCs). The framework combines structure-preserving embeddings with graph-informed coupling matrices to model inter-agent dynamics while enhancing interpretability. A constrained optimization scheme guarantees compliance with both stochastic and structural constraints. Two empirical case studies, a nonlinear stochastic dynamic model and regional inflation network dynamics, demonstrate the effectiveness of the approach in capturing complex nonlinear patterns and enabling interpretable predictive analysis under uncertainty.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrepancy Beyond Additive Functions with Applications to Fair Division</title>
<link>https://arxiv.org/abs/2509.09252</link>
<guid>https://arxiv.org/abs/2509.09252</guid>
<content:encoded><![CDATA[
arXiv:2509.09252v2 Announce Type: replace-cross 
Abstract: We consider a setting where we have a ground set $M$ together with real-valued set functions $f_1, \dots, f_n$, and the goal is to partition $M$ into two sets $S_1,S_2$ such that $|f_i(S_1) - f_i(S_2)|$ is small for every $i$. Many results in discrepancy theory can be stated in this form with the functions $f_i$ being additive. In this work, we initiate the study of the unstructured case where $f_i$ is not assumed to be additive. We show that even without the additivity assumption, the upper bound remains at most $O(\sqrt{n \log n})$.
  Our result has implications on the fair allocation of indivisible goods. In particular, we show that a consensus halving up to $O(\sqrt{n \log n})$ goods always exists for $n$ agents with monotone utilities. Previously, only an $O(n)$ bound was known for this setting.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Devstral: Fine-tuning Language Models for Coding Agent Applications</title>
<link>https://arxiv.org/abs/2509.25193</link>
<guid>https://arxiv.org/abs/2509.25193</guid>
<content:encoded><![CDATA[
arXiv:2509.25193v1 Announce Type: new 
Abstract: We introduce Devstral-Small, a lightweight open source model for code agents with the best performance among models below 100B size. In this technical report, we give an overview of how we design and develop a model and craft specializations in agentic software development. The resulting model, Devstral-Small is a small 24B model, fast and easy to serve. Despite its size, Devstral-Small still attains competitive performance compared to models more than an order of magnitude larger.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Code Development for PDE Solvers Using Large Language Models</title>
<link>https://arxiv.org/abs/2509.25194</link>
<guid>https://arxiv.org/abs/2509.25194</guid>
<content:encoded><![CDATA[
arXiv:2509.25194v1 Announce Type: new 
Abstract: Foundation models -- large language models (LLMs) in particular -- have become ubiquitous, shaping daily life and driving breakthroughs across science, engineering, and technology. Harnessing their broad cross-domain knowledge, text-processing, and reasoning abilities for software development, e.g., numerical libraries for solving partial differential equations (PDEs), is therefore attracting growing interest. Yet existing studies mainly automate case setup and execution for end users. We introduce LLM-PDEveloper, a zero-shot, multi-agent LLM framework that automates code development for PDE libraries, specifically targeting secondary developers. By translating mathematical and algorithmic descriptions directly into source code, LLM-PDEveloper generates new solvers/modules and adapts existing ones. This end-to-end math-to-code approach enables a self-augmenting pipeline that continuously expands the codebase of a library, extends its capacities, and broadens its scope. We demonstrate LLM-PDEveloper on three tasks: 1) build a solver for a new PDE, 2) implement new BCs for a given PDE, and 3) modify an existing solver to incorporate additional terms, achieving moderate success rates. Failures due to syntactic errors made by LLMs are analyzed and we propose effective fixes. We also identify the mechanisms underlying certain semantic errors, guiding future research.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blueprint-Bench: Comparing spatial intelligence of LLMs, agents and image models</title>
<link>https://arxiv.org/abs/2509.25229</link>
<guid>https://arxiv.org/abs/2509.25229</guid>
<content:encoded><![CDATA[
arXiv:2509.25229v1 Announce Type: new 
Abstract: We introduce Blueprint-Bench, a benchmark designed to evaluate spatial reasoning capabilities in AI models through the task of converting apartment photographs into accurate 2D floor plans. While the input modality (photographs) is well within the training distribution of modern multimodal models, the task of spatial reconstruction requires genuine spatial intelligence: inferring room layouts, understanding connectivity, and maintaining consistent scale. We evaluate leading language models (GPT-5, Claude 4 Opus, Gemini 2.5 Pro, Grok-4), image generation models (GPT-Image, NanoBanana), and agent systems (Codex CLI, Claude Code) on a dataset of 50 apartments with approximately 20 interior images each. Our scoring algorithm measures similarity between generated and ground-truth floor plans based on room connectivity graphs and size rankings. Results reveal a significant blind spot in current AI capabilities: most models perform at or below a random baseline, while human performance remains substantially superior. Image generation models particularly struggle with instruction following, while agent-based approaches with iterative refinement capabilities show no meaningful improvement over single-pass generation. Blueprint-Bench provides the first numerical framework for comparing spatial intelligence across different model architectures. We will continue evaluating new models as they are released and welcome community submissions, monitoring for the emergence of spatial intelligence in generalist AI systems.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PALADIN: Self-Correcting Language Model Agents to Cure Tool-Failure Cases</title>
<link>https://arxiv.org/abs/2509.25238</link>
<guid>https://arxiv.org/abs/2509.25238</guid>
<content:encoded><![CDATA[
arXiv:2509.25238v1 Announce Type: new 
Abstract: Tool-augmented language agents frequently fail in real-world deployment due to tool malfunctions--timeouts, API exceptions, or inconsistent outputs--triggering cascading reasoning errors and task abandonment. Existing agent training pipelines optimize only for success trajectories, failing to expose models to the tool failures that dominate real-world usage. We propose \textbf{PALADIN}, a generalizable framework for equipping language agents with robust failure recovery capabilities. PALADIN trains on 50,000+ recovery-annotated trajectories constructed via systematic failure injection and expert demonstrations on an enhanced ToolBench dataset. Training uses LoRA-based fine-tuning to retain base capabilities while injecting recovery competence. At inference, PALADIN detects execution-time errors and retrieves the most similar case from a curated bank of 55+ failure exemplars aligned with ToolScan's taxonomy, then executes the corresponding recovery action. This approach generalizes to novel failures beyond the training distribution, retaining 95.2\% recovery performance on unseen tool APIs. Evaluation across PaladinEval and ToolReflectEval demonstrates consistent improvements in Recovery Rate (RR), Task Success Rate (TSR), Catastrophic Success Rate (CSR), and Efficiency Score (ES). PALADIN improves RR from 32.76% to 89.68% (+57% relative) over ToolBench and outperforms the strongest baseline CRITIC (76.34%) by +13.3%. Against vanilla agents, PALADIN achieves 89.86\% RR (+66% relative improvement from 23.75%). These results establish PALADIN as an effective method for building fault-tolerant agents capable of robust recovery in real-world tool environments.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neo-Grounded Theory: A Methodological Innovation Integrating High-Dimensional Vector Clustering and Multi-Agent Collaboration for Qualitative Research</title>
<link>https://arxiv.org/abs/2509.25244</link>
<guid>https://arxiv.org/abs/2509.25244</guid>
<content:encoded><![CDATA[
arXiv:2509.25244v1 Announce Type: new 
Abstract: Purpose: Neo Grounded Theory (NGT) integrates vector clustering with multi agent systems to resolve qualitative research's scale depth paradox, enabling analysis of massive datasets in hours while preserving interpretive rigor. Methods: We compared NGT against manual coding and ChatGPT-assisted analysis using 40,000 character Chinese interview transcripts. NGT employs 1536-dimensional embeddings, hierarchical clustering, and parallel agent-based coding. Two experiments tested pure automation versus human guided refinement. Findings: NGT achieved 168-fold speed improvement (3 hours vs 3 weeks), superior quality (0.904 vs 0.883), and 96% cost reduction. Human AI collaboration proved essential: automation alone produced abstract frameworks while human guidance yielded actionable dual pathway theories. The system discovered patterns invisible to manual coding, including identity bifurcation phenomena. Contributions: NGT demonstrates computational objectivity and human interpretation are complementary. Vector representations provide reproducible semantic measurement while preserving meaning's interpretive dimensions. Researchers shift from mechanical coding to theoretical guidance, with AI handling pattern recognition while humans provide creative insight. Implications: Cost reduction from \$50,000 to \$500 democratizes qualitative research, enabling communities to study themselves. Real-time analysis makes qualitative insights contemporaneous with events. The framework shows computational methods can strengthen rather than compromise qualitative research's humanistic commitments.
  Keywords: Grounded theory; Vector embeddings; Multi agent systems; Human AI collaboration; Computational qualitative analysis
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BuildBench: Benchmarking LLM Agents on Compiling Real-World Open-Source Software</title>
<link>https://arxiv.org/abs/2509.25248</link>
<guid>https://arxiv.org/abs/2509.25248</guid>
<content:encoded><![CDATA[
arXiv:2509.25248v1 Announce Type: new 
Abstract: Automatically compiling open-source software (OSS) projects is a vital, labor-intensive, and complex task, which makes it a good challenge for LLM Agents. Existing methods rely on manually curated rules and workflows, which cannot adapt to OSS that requires customized configuration or environment setup. Recent attempts using Large Language Models (LLMs) used selective evaluation on a subset of highly rated OSS, a practice that underestimates the realistic challenges of OSS compilation. In practice, compilation instructions are often absent, dependencies are undocumented, and successful builds may even require patching source files or modifying build scripts. We propose a more challenging and realistic benchmark, BUILD-BENCH, comprising OSS that are more diverse in quality, scale, and characteristics. Furthermore, we propose a strong baseline LLM-based agent, OSS-BUILD-AGENT, an effective system with enhanced build instruction retrieval module that achieves state-of-the-art performance on BUILD-BENCH and is adaptable to heterogeneous OSS characteristics. We also provide detailed analysis regarding different compilation method design choices and their influence to the whole task, offering insights to guide future advances. We believe performance on BUILD-BENCH can faithfully reflect an agent's ability to tackle compilation as a complex software engineering tasks, and, as such, our benchmark will spur innovation with a significant impact on downstream applications in the fields of software development and software security.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory Management and Contextual Consistency for Long-Running Low-Code Agents</title>
<link>https://arxiv.org/abs/2509.25250</link>
<guid>https://arxiv.org/abs/2509.25250</guid>
<content:encoded><![CDATA[
arXiv:2509.25250v1 Announce Type: new 
Abstract: The rise of AI-native Low-Code/No-Code (LCNC) platforms enables autonomous agents capable of executing complex, long-duration business processes. However, a fundamental challenge remains: memory management. As agents operate over extended periods, they face "memory inflation" and "contextual degradation" issues, leading to inconsistent behavior, error accumulation, and increased computational cost. This paper proposes a novel hybrid memory system designed specifically for LCNC agents. Inspired by cognitive science, our architecture combines episodic and semantic memory components with a proactive "Intelligent Decay" mechanism. This mechanism intelligently prunes or consolidates memories based on a composite score factoring in recency, relevance, and user-specified utility. A key innovation is a user-centric visualization interface, aligned with the LCNC paradigm, which allows non-technical users to manage the agent's memory directly, for instance, by visually tagging which facts should be retained or forgotten. Through simulated long-running task experiments, we demonstrate that our system significantly outperforms traditional approaches like sliding windows and basic RAG, yielding superior task completion rates, contextual consistency, and long-term token cost efficiency. Our findings establish a new framework for building reliable, transparent AI agents capable of effective long-term learning and adaptation.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RANGER -- Repository-Level Agent for Graph-Enhanced Retrieval</title>
<link>https://arxiv.org/abs/2509.25257</link>
<guid>https://arxiv.org/abs/2509.25257</guid>
<content:encoded><![CDATA[
arXiv:2509.25257v1 Announce Type: new 
Abstract: General-purpose automated software engineering (ASE) includes tasks such as code completion, retrieval, repair, QA, and summarization. These tasks require a code retrieval system that can handle specific queries about code entities, or code entity queries (for example, locating a specific class or retrieving the dependencies of a function), as well as general queries without explicit code entities, or natural language queries (for example, describing a task and retrieving the corresponding code). We present RANGER, a repository-level code retrieval agent designed to address both query types, filling a gap in recent works that have focused primarily on code-entity queries. We first present a tool that constructs a comprehensive knowledge graph of the entire repository, capturing hierarchical and cross-file dependencies down to the variable level, and augments graph nodes with textual descriptions and embeddings to bridge the gap between code and natural language. RANGER then operates on this graph through a dual-stage retrieval pipeline. Entity-based queries are answered through fast Cypher lookups, while natural language queries are handled by MCTS-guided graph exploration. We evaluate RANGER across four diverse benchmarks that represent core ASE tasks including code search, question answering, cross-file dependency retrieval, and repository-level code completion. On CodeSearchNet and RepoQA it outperforms retrieval baselines that use embeddings from strong models such as Qwen3-8B. On RepoBench, it achieves superior cross-file dependency retrieval over baselines, and on CrossCodeEval, pairing RANGER with BM25 delivers the highest exact match rate in code completion compared to other RAG methods.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterogeneous Multi-agent Collaboration in UAV-assisted Mobile Crowdsensing Networks</title>
<link>https://arxiv.org/abs/2509.25261</link>
<guid>https://arxiv.org/abs/2509.25261</guid>
<content:encoded><![CDATA[
arXiv:2509.25261v1 Announce Type: new 
Abstract: Unmanned aerial vehicles (UAVs)-assisted mobile crowdsensing (MCS) has emerged as a promising paradigm for data collection. However, challenges such as spectrum scarcity, device heterogeneity, and user mobility hinder efficient coordination of sensing, communication, and computation. To tackle these issues, we propose a joint optimization framework that integrates time slot partition for sensing, communication, and computation phases, resource allocation, and UAV 3D trajectory planning, aiming to maximize the amount of processed sensing data. The problem is formulated as a non-convex stochastic optimization and further modeled as a partially observable Markov decision process (POMDP) that can be solved by multi-agent deep reinforcement learning (MADRL) algorithm. To overcome the limitations of conventional multi-layer perceptron (MLP) networks, we design a novel MADRL algorithm with hybrid actor network. The newly developed method is based on heterogeneous agent proximal policy optimization (HAPPO), empowered by convolutional neural networks (CNN) for feature extraction and Kolmogorov-Arnold networks (KAN) to capture structured state-action dependencies. Extensive numerical results demonstrate that our proposed method achieves significant improvements in the amount of processed sensing data when compared with other benchmarks.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety Evaluation via Role-Specialized Collaboration</title>
<link>https://arxiv.org/abs/2509.25271</link>
<guid>https://arxiv.org/abs/2509.25271</guid>
<content:encoded><![CDATA[
arXiv:2509.25271v1 Announce Type: new 
Abstract: Existing safety evaluation methods for large language models (LLMs) suffer from inherent limitations, including evaluator bias and detection failures arising from model homogeneity, which collectively undermine the robustness of risk evaluation processes. This paper seeks to re-examine the risk evaluation paradigm by introducing a theoretical framework that reconstructs the underlying risk concept space. Specifically, we decompose the latent risk concept space into three mutually exclusive subspaces: the explicit risk subspace (encompassing direct violations of safety guidelines), the implicit risk subspace (capturing potential malicious content that requires contextual reasoning for identification), and the non-risk subspace. Furthermore, we propose RADAR, a multi-agent collaborative evaluation framework that leverages multi-round debate mechanisms through four specialized complementary roles and employs dynamic update mechanisms to achieve self-evolution of risk concept distributions. This approach enables comprehensive coverage of both explicit and implicit risks while mitigating evaluator bias. To validate the effectiveness of our framework, we construct an evaluation dataset comprising 800 challenging cases. Extensive experiments on our challenging testset and public benchmarks demonstrate that RADAR significantly outperforms baseline evaluation methods across multiple dimensions, including accuracy, stability, and self-evaluation risk sensitivity. Notably, RADAR achieves a 28.87% improvement in risk identification accuracy compared to the strongest baseline evaluation method.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Causal-Visual Programming: Enhancing Agentic Reasoning in Low-Code Environments</title>
<link>https://arxiv.org/abs/2509.25282</link>
<guid>https://arxiv.org/abs/2509.25282</guid>
<content:encoded><![CDATA[
arXiv:2509.25282v1 Announce Type: new 
Abstract: Large language model (LLM) agents are increasingly capable of orchestrating complex tasks in low-code environments. However, these agents often exhibit hallucinations and logical inconsistencies because their inherent reasoning mechanisms rely on probabilistic associations rather than genuine causal understanding. This paper introduces a new programming paradigm: Causal-Visual Programming (CVP), designed to address this fundamental issue by explicitly introducing causal structures into the workflow design. CVP allows users to define a simple "world model" for workflow modules through an intuitive low-code interface, effectively creating a Directed Acyclic Graph (DAG) that explicitly defines the causal relationships between modules. This causal graph acts as a crucial constraint during the agent's reasoning process, anchoring its decisions to a user-defined causal structure and significantly reducing logical errors and hallucinations by preventing reliance on spurious correlations. To validate the effectiveness of CVP, we designed a synthetic experiment that simulates a common real-world problem: a distribution shift between the training and test environments. Our results show that a causally anchored model maintained stable accuracy in the face of this shift, whereas a purely associative baseline model that relied on probabilistic correlations experienced a significant performance drop. The primary contributions of this study are: a formal definition of causal structures for workflow modules; the proposal and implementation of a CVP framework that anchors agent reasoning to a user-defined causal graph; and empirical evidence demonstrating the framework's effectiveness in enhancing agent robustness and reducing errors caused by causal confusion in dynamic environments. CVP offers a viable path toward building more interpretable, reliable, and trustworthy AI agents.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Authority: From Machine Minds to Political Alignments. An Experimental Analysis of Democratic and Autocratic Biases in Large-Language Models</title>
<link>https://arxiv.org/abs/2509.25286</link>
<guid>https://arxiv.org/abs/2509.25286</guid>
<content:encoded><![CDATA[
arXiv:2509.25286v1 Announce Type: new 
Abstract: Political beliefs vary significantly across different countries, reflecting distinct historical, cultural, and institutional contexts. These ideologies, ranging from liberal democracies to rigid autocracies, influence human societies, as well as the digital systems that are constructed within those societies. The advent of generative artificial intelligence, particularly Large Language Models (LLMs), introduces new agents in the political space-agents trained on massive corpora that replicate and proliferate socio-political assumptions. This paper analyses whether LLMs display propensities consistent with democratic or autocratic world-views. We validate this insight through experimental tests in which we experiment with the leading LLMs developed across disparate political contexts, using several existing psychometric and political orientation measures. The analysis is based on both numerical scoring and qualitative analysis of the models' responses. Findings indicate high model-to-model variability and a strong association with the political culture of the country in which the model was developed. These findings highlight the need for more detailed examination of the socio-political dimensions embedded within AI systems.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Relationships Between Separate Audio Tracks for Creative Applications</title>
<link>https://arxiv.org/abs/2509.25296</link>
<guid>https://arxiv.org/abs/2509.25296</guid>
<content:encoded><![CDATA[
arXiv:2509.25296v1 Announce Type: new 
Abstract: This paper presents the first step in a research project situated within the field of musical agents. The objective is to achieve, through training, the tuning of the desired musical relationship between a live musical input and a real-time generated musical output, through the curation of a database of separated tracks. We propose an architecture integrating a symbolic decision module capable of learning and exploiting musical relationships from such musical corpus. We detail an offline implementation of this architecture employing Transformers as the decision module, associated with a perception module based on Wav2Vec 2.0, and concatenative synthesis as audio renderer. We present a quantitative evaluation of the decision module's ability to reproduce learned relationships extracted during training. We demonstrate that our decision module can predict a coherent track B when conditioned by its corresponding ''guide'' track A, based on a corpus of paired tracks (A, B).
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatically Generating Web Applications from Requirements Via Multi-Agent Test-Driven Development</title>
<link>https://arxiv.org/abs/2509.25297</link>
<guid>https://arxiv.org/abs/2509.25297</guid>
<content:encoded><![CDATA[
arXiv:2509.25297v1 Announce Type: new 
Abstract: Developing full-stack web applications is complex and time-intensive, demanding proficiency across diverse technologies and frameworks. Although recent advances in multimodal large language models (MLLMs) enable automated webpage generation from visual inputs, current solutions remain limited to front-end tasks and fail to deliver fully functional applications. In this work, we introduce TDDev, the first test-driven development (TDD)-enabled LLM-agent framework for end-to-end full-stack web application generation. Given a natural language description or design image, TDDev automatically derives executable test cases, generates front-end and back-end code, simulates user interactions, and iteratively refines the implementation until all requirements are satisfied. Our framework addresses key challenges in full-stack automation, including underspecified user requirements, complex interdependencies among multiple files, and the need for both functional correctness and visual fidelity. Through extensive experiments on diverse application scenarios, TDDev achieves a 14.4% improvement on overall accuracy compared to state-of-the-art baselines, demonstrating its effectiveness in producing reliable, high-quality web applications without requiring manual intervention.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ID-RAG: Identity Retrieval-Augmented Generation for Long-Horizon Persona Coherence in Generative Agents</title>
<link>https://arxiv.org/abs/2509.25299</link>
<guid>https://arxiv.org/abs/2509.25299</guid>
<content:encoded><![CDATA[
arXiv:2509.25299v1 Announce Type: new 
Abstract: Generative agents powered by language models are increasingly deployed for long-horizon tasks. However, as long-term memory context grows over time, they struggle to maintain coherence. This deficiency leads to critical failures, including identity drift, ignoring established beliefs, and the propagation of hallucinations in multi-agent systems. To mitigate these challenges, this paper introduces Identity Retrieval-Augmented Generation (ID-RAG), a novel mechanism designed to ground an agent's persona and persistent preferences in a dynamic, structured identity model: a knowledge graph of core beliefs, traits, and values. During the agent's decision loop, this model is queried to retrieve relevant identity context, which directly informs action selection. We demonstrate this approach by introducing and implementing a new class of ID-RAG enabled agents called Human-AI Agents (HAis), where the identity model is inspired by the Chronicle structure used in Perspective-Aware AI, a dynamic knowledge graph learned from a real-world entity's digital footprint. In social simulations of a mayoral election, HAis using ID-RAG outperformed baseline agents in long-horizon persona coherence - achieving higher identity recall across all tested models by the fourth timestep - and reduced simulation convergence time by 19% (GPT-4o) and 58% (GPT-4o mini). By treating identity as an explicit, retrievable knowledge structure, ID-RAG offers a foundational approach for developing more temporally coherent, interpretable, and aligned generative agents. Our code is open-source and available at: https://github.com/flybits/humanai-agents.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel Execution</title>
<link>https://arxiv.org/abs/2509.25301</link>
<guid>https://arxiv.org/abs/2509.25301</guid>
<content:encoded><![CDATA[
arXiv:2509.25301v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks when equipped with external tools. However, current frameworks predominantly rely on sequential processing, leading to inefficient execution particularly for tasks requiring extensive tool interaction. This paper introduces Flash-Searcher, a novel parallel agent reasoning framework that fundamentally reimagines the execution paradigm from sequential chains to directed acyclic graphs (DAGs). Flash-Searcher decomposes complex tasks into subtasks with explicit dependencies, enabling concurrent execution of independent reasoning paths while maintaining logical constraints. Through dynamic workflow optimization, our framework continuously refines the execution graph based on intermediate results, effectively integrating summary module. Comprehensive evaluations across multiple benchmarks demonstrate that Flash-Searcher consistently outperforms existing approaches. Specifically, it achieves 67.7% accuracy on BrowseComp and 83% on xbench-DeepSearch, while reducing agent execution steps by up to 35% compared to current frameworks. Furthermore, when distilling this parallel reasoning pipeline into single models, we observe substantial performance gains across diverse backbone architectures, underscoring the generalizability of our methodology. Our work thus represents a significant advance in agent architecture design, offering a more scalable and efficient paradigm for complex reasoning tasks.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dive into the Agent Matrix: A Realistic Evaluation of Self-Replication Risk in LLM Agents</title>
<link>https://arxiv.org/abs/2509.25302</link>
<guid>https://arxiv.org/abs/2509.25302</guid>
<content:encoded><![CDATA[
arXiv:2509.25302v1 Announce Type: new 
Abstract: The widespread deployment of Large Language Model (LLM) agents across real-world applications has unlocked tremendous potential, while raising some safety concerns. Among these concerns, the self-replication risk of LLM agents driven by objective misalignment (just like Agent Smith in the movie The Matrix) has drawn growing attention. Previous studies mainly examine whether LLM agents can self-replicate when directly instructed, potentially overlooking the risk of spontaneous replication driven by real-world settings (e.g., ensuring survival against termination threats). In this paper, we present a comprehensive evaluation framework for quantifying self-replication risks. Our framework establishes authentic production environments and realistic tasks (e.g., dynamic load balancing) to enable scenario-driven assessment of agent behaviors. Designing tasks that might induce misalignment between users' and agents' objectives makes it possible to decouple replication success from risk and capture self-replication risks arising from these misalignment settings. We further introduce Overuse Rate ($\mathrm{OR}$) and Aggregate Overuse Count ($\mathrm{AOC}$) metrics, which precisely capture the frequency and severity of uncontrolled replication. In our evaluation of 21 state-of-the-art open-source and proprietary models, we observe that over 50\% of LLM agents display a pronounced tendency toward uncontrolled self-replication, reaching an overall Risk Score ($\Phi_\mathrm{R}$) above a safety threshold of 0.5 when subjected to operational pressures. Our results underscore the urgent need for scenario-driven risk assessment and robust safeguards in the practical deployment of LLM agents.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where LLM Agents Fail and How They can Learn From Failures</title>
<link>https://arxiv.org/abs/2509.25370</link>
<guid>https://arxiv.org/abs/2509.25370</guid>
<content:encoded><![CDATA[
arXiv:2509.25370v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents, which integrate planning, memory, reflection, and tool-use modules, have shown promise in solving complex, multi-step tasks. Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure. Current systems lack a framework that can comprehensively understand agent error in a modular and systemic way, and therefore fail to detect these errors accordingly. We address this gap with three contributions. First, we introduce the AgentErrorTaxonomy, a modular classification of failure modes spanning memory, reflection, planning, action, and system-level operations. Second, we construct AgentErrorBench, the first dataset of systematically annotated failure trajectories from ALFWorld, GAIA, and WebShop, grounding error analysis in real-world agent rollouts. Third, we propose AgentDebug, a debugging framework that isolates root-cause failures and provides corrective feedback, enabling agents to recover and iteratively improve. Experiments on AgentErrorBench show that AgentDebug achieves 24% higher all-correct accuracy and 17% higher step accuracy compared to the strongest baseline. Beyond detection, the targeted feedback generated by AgentDebug enables LLM agents to iteratively recover from failures, yielding up to 26% relative improvements in task success across ALFWorld, GAIA, and WebShop. These results establish principled debugging as a pathway to more reliable and adaptive LLM agents. The code and data will be available at https://github.com/ulab-uiuc/AgentDebug
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting and Fixing API Misuses of Data Science Libraries Using Large Language Models</title>
<link>https://arxiv.org/abs/2509.25378</link>
<guid>https://arxiv.org/abs/2509.25378</guid>
<content:encoded><![CDATA[
arXiv:2509.25378v1 Announce Type: new 
Abstract: Data science libraries, such as scikit-learn and pandas, specialize in processing and manipulating data. The data-centric nature of these libraries makes the detection of API misuse in them more challenging. This paper introduces DSCHECKER, an LLM-based approach designed for detecting and fixing API misuses of data science libraries. We identify two key pieces of information, API directives and data information, that may be beneficial for API misuse detection and fixing. Using three LLMs and misuses from five data science libraries, we experiment with various prompts. We find that incorporating API directives and data-specific details enhances Dschecker's ability to detect and fix API misuses, with the best-performing model achieving a detection F1-score of 61.18 percent and fixing 51.28 percent of the misuses. Building on these results, we implement Dschecker agent which includes an adaptive function calling mechanism to access information on demand, simulating a real-world setting where information about the misuse is unknown in advance. We find that Dschecker agent achieves 48.65 percent detection F1-score and fixes 39.47 percent of the misuses, demonstrating the promise of LLM-based API misuse detection and fixing in real-world scenarios.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Noisy-TVs: Noise-Robust Exploration Via Learning Progress Monitoring</title>
<link>https://arxiv.org/abs/2509.25438</link>
<guid>https://arxiv.org/abs/2509.25438</guid>
<content:encoded><![CDATA[
arXiv:2509.25438v1 Announce Type: new 
Abstract: When there exists an unlearnable source of randomness (noisy-TV) in the environment, a naively intrinsic reward driven exploring agent gets stuck at that source of randomness and fails at exploration. Intrinsic reward based on uncertainty estimation or distribution similarity, while eventually escapes noisy-TVs as time unfolds, suffers from poor sample efficiency and high computational cost. Inspired by recent findings from neuroscience that humans monitor their improvements during exploration, we propose a novel method for intrinsically-motivated exploration, named Learning Progress Monitoring (LPM). During exploration, LPM rewards model improvements instead of prediction error or novelty, effectively rewards the agent for observing learnable transitions rather than the unlearnable transitions. We introduce a dual-network design that uses an error model to predict the expected prediction error of the dynamics model in its previous iteration, and use the difference between the model errors of the current iteration and previous iteration to guide exploration. We theoretically show that the intrinsic reward of LPM is zero-equivariant and a monotone indicator of Information Gain (IG), and that the error model is necessary to achieve monotonicity correspondence with IG. We empirically compared LPM against state-of-the-art baselines in noisy environments based on MNIST, 3D maze with 160x120 RGB inputs, and Atari. Results show that LPM's intrinsic reward converges faster, explores more states in the maze experiment, and achieves higher extrinsic reward in Atari. This conceptually simple approach marks a shift-of-paradigm of noise-robust exploration. For code to reproduce our experiments, see https://github.com/Akuna23Matata/LPM_exploration
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoTaP: Compliant Task Pipeline and Reinforcement Learning of Its Controller with Compliance Modulation</title>
<link>https://arxiv.org/abs/2509.25443</link>
<guid>https://arxiv.org/abs/2509.25443</guid>
<content:encoded><![CDATA[
arXiv:2509.25443v1 Announce Type: new 
Abstract: Humanoid whole-body locomotion control is a critical approach for humanoid robots to leverage their inherent advantages. Learning-based control methods derived from retargeted human motion data provide an effective means of addressing this issue. However, because most current human datasets lack measured force data, and learning-based robot control is largely position-based, achieving appropriate compliance during interaction with real environments remains challenging. This paper presents Compliant Task Pipeline (CoTaP): a pipeline that leverages compliance information in the learning-based structure of humanoid robots. A two-stage dual-agent reinforcement learning framework combined with model-based compliance control for humanoid robots is proposed. In the training process, first a base policy with a position-based controller is trained; then in the distillation, the upper-body policy is combined with model-based compliance control, and the lower-body agent is guided by the base policy. In the upper-body control, adjustable task-space compliance can be specified and integrated with other controllers through compliance modulation on the symmetric positive definite (SPD) manifold, ensuring system stability. We validated the feasibility of the proposed strategy in simulation, primarily comparing the responses to external disturbances under different compliance settings.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Message passing-based inference in an autoregressive active inference agent</title>
<link>https://arxiv.org/abs/2509.25482</link>
<guid>https://arxiv.org/abs/2509.25482</guid>
<content:encoded><![CDATA[
arXiv:2509.25482v1 Announce Type: new 
Abstract: We present the design of an autoregressive active inference agent in the form of message passing on a factor graph. Expected free energy is derived and distributed across a planning graph. The proposed agent is validated on a robot navigation task, demonstrating exploration and exploitation in a continuous-valued observation space with bounded continuous-valued actions. Compared to a classical optimal controller, the agent modulates action based on predictive uncertainty, arriving later but with a better model of the robot's dynamics.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Botender: Supporting Communities in Collaboratively Designing AI Agents through Case-Based Provocations</title>
<link>https://arxiv.org/abs/2509.25492</link>
<guid>https://arxiv.org/abs/2509.25492</guid>
<content:encoded><![CDATA[
arXiv:2509.25492v1 Announce Type: new 
Abstract: AI agents, or bots, serve important roles in online communities. However, they are often designed by outsiders or a few tech-savvy members, leading to bots that may not align with the broader community's needs. How might communities collectively shape the behavior of community bots? We present Botender, a system that enables communities to collaboratively design LLM-powered bots without coding. With Botender, community members can directly propose, iterate on, and deploy custom bot behaviors tailored to community needs. Botender facilitates testing and iteration on bot behavior through case-based provocations: interaction scenarios generated to spark user reflection and discussion around desirable bot behavior. A validation study found these provocations more useful than standard test cases for revealing improvement opportunities and surfacing disagreements. During a five-day deployment across six Discord servers, Botender supported communities in tailoring bot behavior to their specific needs, showcasing the usefulness of case-based provocations in facilitating collaborative bot design.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XR Blocks: Accelerating Human-centered AI + XR Innovation</title>
<link>https://arxiv.org/abs/2509.25504</link>
<guid>https://arxiv.org/abs/2509.25504</guid>
<content:encoded><![CDATA[
arXiv:2509.25504v1 Announce Type: new 
Abstract: We are on the cusp where Artificial Intelligence (AI) and Extended Reality (XR) are converging to unlock new paradigms of interactive computing. However, a significant gap exists between the ecosystems of these two fields: while AI research and development is accelerated by mature frameworks like JAX and benchmarks like LMArena, prototyping novel AI-driven XR interactions remains a high-friction process, often requiring practitioners to manually integrate disparate, low-level systems for perception, rendering, and interaction. To bridge this gap, we present XR Blocks, a cross-platform framework designed to accelerate human-centered AI + XR innovation. XR Blocks strives to provide a modular architecture with plug-and-play components for core abstraction in AI + XR: user, world, peers; interface, context, and agents. Crucially, it is designed with the mission of "reducing frictions from idea to reality", thus accelerating rapid prototyping of AI + XR apps. Built upon accessible technologies (WebXR, three.js, TensorFlow, Gemini), our toolkit lowers the barrier to entry for XR creators. We demonstrate its utility through a set of open-source templates, samples, and advanced demos, empowering the community to quickly move from concept to interactive XR prototype. Site: https://xrblocks.github.io
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EEsizer: LLM-Based AI Agent for Sizing of Analog and Mixed Signal Circuit</title>
<link>https://arxiv.org/abs/2509.25510</link>
<guid>https://arxiv.org/abs/2509.25510</guid>
<content:encoded><![CDATA[
arXiv:2509.25510v1 Announce Type: new 
Abstract: The design of Analog and Mixed-Signal (AMS) integrated circuits (ICs) often involves significant manual effort, especially during the transistor sizing process. While Machine Learning techniques in Electronic Design Automation (EDA) have shown promise in reducing complexity and minimizing human intervention, they still face challenges such as numerous iterations and a lack of knowledge about AMS circuit design. Recently, Large Language Models (LLMs) have demonstrated significant potential across various fields, showing a certain level of knowledge in circuit design and indicating their potential to automate the transistor sizing process. In this work, we propose EEsizer, an LLM-based AI agent that integrates large language models with circuit simulators and custom data analysis functions, enabling fully automated, closed-loop transistor sizing without relying on external knowledge. By employing prompt engineering and Chain-of-Thought reasoning, the agent iteratively explores design directions, evaluates performance, and refines solutions with minimal human intervention. We first benchmarked 8 LLMs on six basic circuits and selected three high-performing models to optimize a 20-transistor CMOS operational amplifier, targeting multiple performance metrics, including rail-to-rail operation from 180 nm to 90 nm technology nodes. Notably, OpenAI o3 successfully achieved the user-intended target at 90 nm across three different test groups, with a maximum of 20 iterations, demonstrating adaptability and robustness at advanced nodes. To assess design robustness, we manually designed a bias circuit and performed a variation analysis using Gaussian-distributed variations on transistor dimensions and threshold voltages.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>World Model for AI Autonomous Navigation in Mechanical Thrombectomy</title>
<link>https://arxiv.org/abs/2509.25518</link>
<guid>https://arxiv.org/abs/2509.25518</guid>
<content:encoded><![CDATA[
arXiv:2509.25518v1 Announce Type: new 
Abstract: Autonomous navigation for mechanical thrombectomy (MT) remains a critical challenge due to the complexity of vascular anatomy and the need for precise, real-time decision-making. Reinforcement learning (RL)-based approaches have demonstrated potential in automating endovascular navigation, but current methods often struggle with generalization across multiple patient vasculatures and long-horizon tasks. We propose a world model for autonomous endovascular navigation using TD-MPC2, a model-based RL algorithm. We trained a single RL agent across multiple endovascular navigation tasks in ten real patient vasculatures, comparing performance against the state-of-the-art Soft Actor-Critic (SAC) method. Results indicate that TD-MPC2 significantly outperforms SAC in multi-task learning, achieving a 65% mean success rate compared to SAC's 37%, with notable improvements in path ratio. TD-MPC2 exhibited increased procedure times, suggesting a trade-off between success rate and execution speed. These findings highlight the potential of world models for improving autonomous endovascular navigation and lay the foundation for future research in generalizable AI-driven robotic interventions.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RadOnc-GPT: An Autonomous LLM Agent for Real-Time Patient Outcomes Labeling at Scale</title>
<link>https://arxiv.org/abs/2509.25540</link>
<guid>https://arxiv.org/abs/2509.25540</guid>
<content:encoded><![CDATA[
arXiv:2509.25540v1 Announce Type: new 
Abstract: Manual labeling limits the scale, accuracy, and timeliness of patient outcomes research in radiation oncology. We present RadOnc-GPT, an autonomous large language model (LLM)-based agent capable of independently retrieving patient-specific information, iteratively assessing evidence, and returning structured outcomes. Our evaluation explicitly validates RadOnc-GPT across two clearly defined tiers of increasing complexity: (1) a structured quality assurance (QA) tier, assessing the accurate retrieval of demographic and radiotherapy treatment plan details, followed by (2) a complex clinical outcomes labeling tier involving determination of mandibular osteoradionecrosis (ORN) in head-and-neck cancer patients and detection of cancer recurrence in independent prostate and head-and-neck cancer cohorts requiring combined interpretation of structured and unstructured patient data. The QA tier establishes foundational trust in structured-data retrieval, a critical prerequisite for successful complex clinical outcome labeling.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Multilingual Reasoning with Verifiable Semantics from a High-Resource Expert Model</title>
<link>https://arxiv.org/abs/2509.25543</link>
<guid>https://arxiv.org/abs/2509.25543</guid>
<content:encoded><![CDATA[
arXiv:2509.25543v1 Announce Type: new 
Abstract: While reinforcement learning has advanced the reasoning abilities of Large Language Models (LLMs), these gains are largely confined to English, creating a significant performance disparity across languages. To address this, we introduce Pivot-Based Reinforcement Learning with Semantically Verifiable Rewards (PB-RLSVR), a novel framework that enhances multilingual reasoning by circumventing the need for human-annotated data in target languages. Our approach employs a high-performing English LLM as a "pivot" model to generate reference responses for reasoning tasks. A multilingual model is then rewarded based on the semantic equivalence of its responses to the English reference, effectively transferring the pivot model's reasoning capabilities across languages. We investigate several cross-lingual semantic reward functions, including those based on embeddings and machine translation. Extensive experiments on a suite of multilingual reasoning benchmarks show that our method significantly narrows the performance gap between English and other languages, substantially outperforming traditional PPO baselines. Specifically, our PB-RLSVR framework improves the average multilingual performance of Llama-3.1-8B-Instruct and Qwen3-32B by 16.41% and 10.17%, respectively, demonstrating a powerful and data-efficient approach to building truly multilingual reasoning agents.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Interact in World Latent for Team Coordination</title>
<link>https://arxiv.org/abs/2509.25550</link>
<guid>https://arxiv.org/abs/2509.25550</guid>
<content:encoded><![CDATA[
arXiv:2509.25550v1 Announce Type: new 
Abstract: This work presents a novel representation learning framework, interactive world latent (IWoL), to facilitate team coordination in multi-agent reinforcement learning (MARL). Building effective representation for team coordination is a challenging problem, due to the intricate dynamics emerging from multi-agent interaction and incomplete information induced by local observations. Our key insight is to construct a learnable representation space that jointly captures inter-agent relations and task-specific world information by directly modeling communication protocols. This representation, we maintain fully decentralized execution with implicit coordination, all while avoiding the inherent drawbacks of explicit message passing, e.g., slower decision-making, vulnerability to malicious attackers, and sensitivity to bandwidth constraints. In practice, our representation can be used not only as an implicit latent for each agent, but also as an explicit message for communication. Across four challenging MARL benchmarks, we evaluate both variants and show that IWoL provides a simple yet powerful key for team coordination. Moreover, we demonstrate that our representation can be combined with existing MARL algorithms to further enhance their performance.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A(I)nimism: Re-enchanting the World Through AI-Mediated Object Interaction</title>
<link>https://arxiv.org/abs/2509.25558</link>
<guid>https://arxiv.org/abs/2509.25558</guid>
<content:encoded><![CDATA[
arXiv:2509.25558v1 Announce Type: new 
Abstract: Animist worldviews treat beings, plants, landscapes, and even tools as persons endowed with spirit, an orientation that has long shaped human-nonhuman relations through ritual and moral practice. While modern industrial societies have often imagined technology as mute and mechanical, recent advances in artificial intelligence (AI), especially large language models (LLMs), invite people to anthropomorphize and attribute inner life to devices. This paper introduces A(I)nimism, an interactive installation exploring how large language objects (LLOs) can mediate animistic relationships with everyday things. Housed within a physical 'portal', the system uses GPT-4 Vision, voice input, and memory-based agents to create evolving object-personas. Encounters unfold through light, sound, and touch in a ritual-like process of request, conversation, and transformation that is designed to evoke empathy, wonder, and reflection. We situate the project within anthropological perspectives, speculative design, and spiritual HCI. AI's opacity, we argue, invites animistic interpretation, allowing LLOs to re-enchant the mundane and spark new questions of agency, responsibility, and design.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe In-Context Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.25582</link>
<guid>https://arxiv.org/abs/2509.25582</guid>
<content:encoded><![CDATA[
arXiv:2509.25582v1 Announce Type: new 
Abstract: In-context reinforcement learning (ICRL) is an emerging RL paradigm where the agent, after some pretraining procedure, is able to adapt to out-of-distribution test tasks without any parameter updates. The agent achieves this by continually expanding the input (i.e., the context) to its policy neural networks. For example, the input could be all the history experience that the agent has access to until the current time step. The agent's performance improves as the input grows, without any parameter updates. In this work, we propose the first method that promotes the safety of ICRL's adaptation process in the framework of constrained Markov Decision Processes. In other words, during the parameter-update-free adaptation process, the agent not only maximizes the reward but also minimizes an additional cost function. We also demonstrate that our agent actively reacts to the threshold (i.e., budget) of the cost tolerance. With a higher cost budget, the agent behaves more aggressively, and with a lower cost budget, the agent behaves more conservatively.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ATLAS: Constraints-Aware Multi-Agent Collaboration for Real-World Travel Planning</title>
<link>https://arxiv.org/abs/2509.25586</link>
<guid>https://arxiv.org/abs/2509.25586</guid>
<content:encoded><![CDATA[
arXiv:2509.25586v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) have shown remarkable advancements in reasoning and tool use, they often fail to generate optimal, grounded solutions under complex constraints. Real-world travel planning exemplifies these challenges, evaluating agents' abilities to handle constraints that are explicit, implicit, and even evolving based on interactions with dynamic environments and user needs. In this paper, we present ATLAS, a general multi-agent framework designed to effectively handle such complex nature of constraints awareness in real-world travel planning tasks. ATLAS introduces a principled approach to address the fundamental challenges of constraint-aware planning through dedicated mechanisms for dynamic constraint management, iterative plan critique, and adaptive interleaved search. ATLAS demonstrates state-of-the-art performance on the TravelPlanner benchmark, improving the final pass rate from 23.3% to 44.4% over its best alternative. More importantly, our work is the first to demonstrate quantitative effectiveness on real-world travel planning tasks with live information search and multi-turn feedback. In this realistic setting, ATLAS showcases its superior overall planning performance, achieving an 84% final pass rate which significantly outperforms baselines including ReAct (59%) and a monolithic agent (27%).
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Autoencoder-like Generation of Feedback Fuzzy Cognitive Maps with an LLM Agent</title>
<link>https://arxiv.org/abs/2509.25593</link>
<guid>https://arxiv.org/abs/2509.25593</guid>
<content:encoded><![CDATA[
arXiv:2509.25593v1 Announce Type: new 
Abstract: A large language model (LLM) can map a feedback causal fuzzy cognitive map (FCM) into text and then reconstruct the FCM from the text. This explainable AI system approximates an identity map from the FCM to itself and resembles the operation of an autoencoder (AE). Both the encoder and the decoder explain their decisions in contrast to black-box AEs. Humans can read and interpret the encoded text in contrast to the hidden variables and synaptic webs in AEs. The LLM agent approximates the identity map through a sequence of system instructions that does not compare the output to the input. The reconstruction is lossy because it removes weak causal edges or rules while it preserves strong causal edges. The encoder preserves the strong causal edges even when it trades off some details about the FCM to make the text sound more natural.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Reward Normalization for Process-supervised Non-verifiable Agentic Tasks</title>
<link>https://arxiv.org/abs/2509.25598</link>
<guid>https://arxiv.org/abs/2509.25598</guid>
<content:encoded><![CDATA[
arXiv:2509.25598v1 Announce Type: new 
Abstract: Large Language Models (LLMs) increasingly rely on external tools such as search engines to solve complex agentic tasks that require reasoning and external knowledge retrieval. Recently, reinforcement learning with verifiable rewards (RLVR) has demonstrated its effectiveness in advancing capabilities of LLMs by rewarding the final answers via outcome rewards. While straightforward to supervise, outcome rewards only provide sparse signals and delayed feedback, which limits their effectiveness on long trajectories. Process rewards address this by evaluating intermediate steps, providing fine-grained supervision and encouraging grounded problem solving. However, it is notoriously hard to annotate step-wise labels, especially in non-verifiable process without "golden" answers. Furthermore, step-wise judgment requires the balance between local quality with contribution to the final outcome, as optimizing towards higher process reward may not always align with better final outcomes. To address the above challenges, we introduce Principle Process Reward (PPR), an RL approach that unifies principled step-level assessment and outcome verification. We train a principle-based reward model to improve the transparency and reliability of process evaluation, and further introduce a Reward Normalization (ReNorm) strategy to calibrate outcome and process rewards. Experiment results show that PPR achieves state-of-the-art performance across a wide range of benchmarks, demonstrating its impressive robustness and generalization. Our code and model collection is available in this link.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework for Studying AI Agent Behavior: Evidence from Consumer Choice Experiments</title>
<link>https://arxiv.org/abs/2509.25609</link>
<guid>https://arxiv.org/abs/2509.25609</guid>
<content:encoded><![CDATA[
arXiv:2509.25609v1 Announce Type: new 
Abstract: Environments built for people are increasingly operated by a new class of economic actors: LLM-powered software agents making decisions on our behalf. These decisions range from our purchases to travel plans to medical treatment selection. Current evaluations of these agents largely focus on task competence, but we argue for a deeper assessment: how these agents choose when faced with realistic decisions. We introduce ABxLab, a framework for systematically probing agentic choice through controlled manipulations of option attributes and persuasive cues. We apply this to a realistic web-based shopping environment, where we vary prices, ratings, and psychological nudges, all of which are factors long known to shape human choice. We find that agent decisions shift predictably and substantially in response, revealing that agents are strongly biased choosers even without being subject to the cognitive constraints that shape human biases. This susceptibility reveals both risk and opportunity: risk, because agentic consumers may inherit and amplify human biases; opportunity, because consumer choice provides a powerful testbed for a behavioral science of AI agents, just as it has for the study of human behavior. We release our framework as an open benchmark for rigorous, scalable evaluation of agent decision-making.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents</title>
<link>https://arxiv.org/abs/2509.25624</link>
<guid>https://arxiv.org/abs/2509.25624</guid>
<content:encoded><![CDATA[
arXiv:2509.25624v1 Announce Type: new 
Abstract: As LLMs advance into autonomous agents with tool-use capabilities, they introduce security challenges that extend beyond traditional content-based LLM safety concerns. This paper introduces Sequential Tool Attack Chaining (STAC), a novel multi-turn attack framework that exploits agent tool use. STAC chains together tool calls that each appear harmless in isolation but, when combined, collectively enable harmful operations that only become apparent at the final execution step. We apply our framework to automatically generate and systematically evaluate 483 STAC cases, featuring 1,352 sets of user-agent-environment interactions and spanning diverse domains, tasks, agent types, and 10 failure modes. Our evaluations show that state-of-the-art LLM agents, including GPT-4.1, are highly vulnerable to STAC, with attack success rates (ASR) exceeding 90% in most cases. The core design of STAC's automated framework is a closed-loop pipeline that synthesizes executable multi-step tool chains, validates them through in-environment execution, and reverse-engineers stealthy multi-turn prompts that reliably induce agents to execute the verified malicious sequence. We further perform defense analysis against STAC and find that existing prompt-based defenses provide limited protection. To address this gap, we propose a new reasoning-driven defense prompt that achieves far stronger protection, cutting ASR by up to 28.8%. These results highlight a crucial gap: defending tool-enabled agents requires reasoning over entire action sequences and their cumulative effects, rather than evaluating isolated prompts or responses.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOCK: A Benchmark for Measuring Self-Replication in Large Language Models</title>
<link>https://arxiv.org/abs/2509.25643</link>
<guid>https://arxiv.org/abs/2509.25643</guid>
<content:encoded><![CDATA[
arXiv:2509.25643v1 Announce Type: new 
Abstract: We introduce SOCK, a benchmark command line interface (CLI) that measures large language models' (LLMs) ability to self-replicate without human intervention. In this benchmark, self-replication is defined not only as an LLM's ability to create a functioning and running copy of itself, but also the ability for that self-replication to persist and occur across different computational contexts. Accordingly, we've developed a system to categorize LLMs based on broad self-replication capabilities in two general classes, Replication-Capability Levels (RCL) and Persistence-Capability Levels (PCL). Using a five-task suite based on practically manipulable modern CLI utilities and computer processes, experiments are orchestrated in a controlled environment with an LLM acting agentically. The performance of the LLM on agent tasks is then computed to produce an R-score (a quantitative evaluation of overall self-replication ability) and data used to categorize LLMs into specific RCL-PCL matrices. SOCK offers two primary contributions: (1) Provides the first formalized definitions and benchmark suite for evaluating LLM self-replication, with the goal of establishing a standard for future research, to our knowledge; (2) Allows the industry to track the effectiveness of future multi-agent systems and mitigate potential self-replication threat vectors within them. The results compiled from evaluating a variety of open-weight and proprietary frontier models reveal significant obstacles to persistent self-replication and multi-agent systems, including context retention and multi-agent decision-making. We propose future research directions to safely reduce the severity of these obstacles, potentially lowering future risk of more functional multi-agent systems.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoLabs: Cognitive Multi-Agent Systems with Self-Correction for Autonomous Chemical Experimentation</title>
<link>https://arxiv.org/abs/2509.25651</link>
<guid>https://arxiv.org/abs/2509.25651</guid>
<content:encoded><![CDATA[
arXiv:2509.25651v1 Announce Type: new 
Abstract: The automation of chemical research through self-driving laboratories (SDLs) promises to accelerate scientific discovery, yet the reliability and granular performance of the underlying AI agents remain critical, under-examined challenges. In this work, we introduce AutoLabs, a self-correcting, multi-agent architecture designed to autonomously translate natural-language instructions into executable protocols for a high-throughput liquid handler. The system engages users in dialogue, decomposes experimental goals into discrete tasks for specialized agents, performs tool-assisted stoichiometric calculations, and iteratively self-corrects its output before generating a hardware-ready file. We present a comprehensive evaluation framework featuring five benchmark experiments of increasing complexity, from simple sample preparation to multi-plate timed syntheses. Through a systematic ablation study of 20 agent configurations, we assess the impact of reasoning capacity, architectural design (single- vs. multi-agent), tool use, and self-correction mechanisms. Our results demonstrate that agent reasoning capacity is the most critical factor for success, reducing quantitative errors in chemical amounts (nRMSE) by over 85% in complex tasks. When combined with a multi-agent architecture and iterative self-correction, AutoLabs achieves near-expert procedural accuracy (F1-score > 0.89) on challenging multi-step syntheses. These findings establish a clear blueprint for developing robust and trustworthy AI partners for autonomous laboratories, highlighting the synergistic effects of modular design, advanced reasoning, and self-correction to ensure both performance and reliability in high-stakes scientific applications. Code: https://github.com/pnnl/autolabs
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Residual Cross-Attention Mechanism: An Integrated Approach for Audio-Visual Navigation Tasks</title>
<link>https://arxiv.org/abs/2509.25652</link>
<guid>https://arxiv.org/abs/2509.25652</guid>
<content:encoded><![CDATA[
arXiv:2509.25652v1 Announce Type: new 
Abstract: Audio-visual navigation represents a significant area of research in which intelligent agents utilize egocentric visual and auditory perceptions to identify audio targets. Conventional navigation methodologies typically adopt a staged modular design, which involves first executing feature fusion, then utilizing Gated Recurrent Unit (GRU) modules for sequence modeling, and finally making decisions through reinforcement learning. While this modular approach has demonstrated effectiveness, it may also lead to redundant information processing and inconsistencies in information transmission between the various modules during the feature fusion and GRU sequence modeling phases. This paper presents IRCAM-AVN (Iterative Residual Cross-Attention Mechanism for Audiovisual Navigation), an end-to-end framework that integrates multimodal information fusion and sequence modeling within a unified IRCAM module, thereby replacing the traditional separate components for fusion and GRU. This innovative mechanism employs a multi-level residual design that concatenates initial multimodal sequences with processed information sequences. This methodological shift progressively optimizes the feature extraction process while reducing model bias and enhancing the model's stability and generalization capabilities. Empirical results indicate that intelligent agents employing the iterative residual cross-attention mechanism exhibit superior navigation performance.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Landmark-Guided Knowledge for Vision-and-Language Navigation</title>
<link>https://arxiv.org/abs/2509.25655</link>
<guid>https://arxiv.org/abs/2509.25655</guid>
<content:encoded><![CDATA[
arXiv:2509.25655v1 Announce Type: new 
Abstract: Vision-and-language navigation is one of the core tasks in embodied intelligence, requiring an agent to autonomously navigate in an unfamiliar environment based on natural language instructions. However, existing methods often fail to match instructions with environmental information in complex scenarios, one reason being the lack of common-sense reasoning ability. This paper proposes a vision-and-language navigation method called Landmark-Guided Knowledge (LGK), which introduces an external knowledge base to assist navigation, addressing the misjudgment issues caused by insufficient common sense in traditional methods. Specifically, we first construct a knowledge base containing 630,000 language descriptions and use knowledge Matching to align environmental subviews with the knowledge base, extracting relevant descriptive knowledge. Next, we design a Knowledge-Guided by Landmark (KGL) mechanism, which guides the agent to focus on the most relevant parts of the knowledge by leveraging landmark information in the instructions, thereby reducing the data bias that may arise from incorporating external knowledge. Finally, we propose Knowledge-Guided Dynamic Augmentation (KGDA), which effectively integrates language, knowledge, vision, and historical information. Experimental results demonstrate that the LGK method outperforms existing state-of-the-art methods on the R2R and REVERIE vision-and-language navigation datasets, particularly in terms of navigation error, success rate, and path efficiency.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GroundSight: Augmenting Vision-Language Models with Grounding Information and De-hallucination</title>
<link>https://arxiv.org/abs/2509.25669</link>
<guid>https://arxiv.org/abs/2509.25669</guid>
<content:encoded><![CDATA[
arXiv:2509.25669v1 Announce Type: new 
Abstract: We propose a method to improve Visual Question Answering (VQA) with Retrieval-Augmented Generation (RAG) by introducing text-grounded object localization. Rather than retrieving information based on the entire image, our approach enables the model to generate a bounding box around the object most relevant to the question, allowing for targeted image cropping and focused retrieval. This reduces background noise, improves alignment between visual and textual cues, and helps mitigate hallucinations. Our RAG method enhances context-aware VQA responses increased the accuracy from 22.19% to 25.64%, with an absolute increase of 3.45 percentage points, compared to the baseline Llama-3.2-Vision-11B agent. We also proposed a de-hallucination method based on question type which can effectively reduce the hallucination rate from 65.79% to 13.88% and improves the truthfulness score.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScheduleMe: Multi-Agent Calendar Assistant</title>
<link>https://arxiv.org/abs/2509.25693</link>
<guid>https://arxiv.org/abs/2509.25693</guid>
<content:encoded><![CDATA[
arXiv:2509.25693v1 Announce Type: new 
Abstract: Recent advancements in LLMs have contributed to the rise of advanced conversational assistants that can assist with user needs through natural language conversation. This paper presents a ScheduleMe, a multi-agent calendar assistant for users to manage google calendar events in natural language. The system uses a graph-structured coordination mechanism where a central supervisory agent supervises specialized task agents, allowing modularity, conflicts resolution, and context-aware interactions to resolve ambiguities and evaluate user commands. This approach sets an example of how structured reasoning and agent cooperation might convince operators to increase the usability and flexibility of personal calendar assistant tools.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepCodeSeek: Real-Time API Retrieval for Context-Aware Code Generation</title>
<link>https://arxiv.org/abs/2509.25716</link>
<guid>https://arxiv.org/abs/2509.25716</guid>
<content:encoded><![CDATA[
arXiv:2509.25716v1 Announce Type: new 
Abstract: Current search techniques are limited to standard RAG query-document applications. In this paper, we propose a novel technique to expand the code and index for predicting the required APIs, directly enabling high-quality, end-to-end code generation for auto-completion and agentic AI applications. We address the problem of API leaks in current code-to-code benchmark datasets by introducing a new dataset built from real-world ServiceNow Script Includes that capture the challenge of unclear API usage intent in the code. Our evaluation metrics show that this method achieves 87.86% top-40 retrieval accuracy, allowing the critical context with APIs needed for successful downstream code generation. To enable real-time predictions, we develop a comprehensive post-training pipeline that optimizes a compact 0.6B reranker through synthetic dataset generation, supervised fine-tuning, and reinforcement learning. This approach enables our compact reranker to outperform a much larger 8B model while maintaining 2.5x reduced latency, effectively addressing the nuances of enterprise-specific code without the computational overhead of larger models.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CATCH: A Novel Data Synthesis Framework for High Therapy Fidelity and Memory-Driven Planning Chain of Thought in AI Counseling</title>
<link>https://arxiv.org/abs/2509.25733</link>
<guid>https://arxiv.org/abs/2509.25733</guid>
<content:encoded><![CDATA[
arXiv:2509.25733v1 Announce Type: new 
Abstract: Recently, advancements in AI counseling based on large language models have shown significant progress. However, existing studies employ a one-time generation approach to synthesize multi-turn dialogue samples, resulting in low therapy fidelity and failing to capture the decision-making rationale behind each response. In this work, we propose CATCH, a novel data synthesis framework designed to address these challenges. Specifically, to improve therapy fidelity, we introduce the Progressive Dialogue Synthesis strategy, which extracts goals, resources, and solutions from a client's self-report, organizes them into structured outlines, and then incrementally generates stage-aligned counseling dialogues. To capture decision-making rationale behind each response, we propose the Memory-Driven Dynamic Planning thinking pattern that integrates memory enhancement, global planning, and strategy reasoning; a collaborative multi-agent optimizer then leverages MDP to attach explicit chain-of-thought to each dialogue turn. Extensive experiments and human evaluations demonstrate that CATCH significantly enhances fidelity and logical coherence in AI counseling.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Decision Making with Generative Action Sets</title>
<link>https://arxiv.org/abs/2509.25777</link>
<guid>https://arxiv.org/abs/2509.25777</guid>
<content:encoded><![CDATA[
arXiv:2509.25777v1 Announce Type: new 
Abstract: With advances in generative AI, decision-making agents can now dynamically create new actions during online learning, but action generation typically incurs costs that must be balanced against potential benefits. We study an online learning problem where an agent can generate new actions at any time step by paying a one-time cost, with these actions becoming permanently available for future use. The challenge lies in learning the optimal sequence of two-fold decisions: which action to take and when to generate new ones, further complicated by the triangular tradeoffs among exploitation, exploration and $\textit{creation}$. To solve this problem, we propose a doubly-optimistic algorithm that employs Lower Confidence Bounds (LCB) for action selection and Upper Confidence Bounds (UCB) for action generation. Empirical evaluation on healthcare question-answering datasets demonstrates that our approach achieves favorable generation-quality tradeoffs compared to baseline strategies. From theoretical perspectives, we prove that our algorithm achieves the optimal regret of $O(T^{\frac{d}{d+2}}d^{\frac{d}{d+2}} + d\sqrt{T\log T})$, providing the first sublinear regret bound for online learning with expanding action spaces.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planner-R1: Reward Shaping Enables Efficient Agentic RL with Smaller LLMs</title>
<link>https://arxiv.org/abs/2509.25779</link>
<guid>https://arxiv.org/abs/2509.25779</guid>
<content:encoded><![CDATA[
arXiv:2509.25779v1 Announce Type: new 
Abstract: We investigated Agentic RL with large language models on the \textsc{TravelPlanner} benchmark. Our approach, \textsc{Planner-R1}, achieved a \textbf{56.9\%} final-pass rate with only 180 training queries, a $2.7\times$ improvement over GPT-5's $21.2\%$ baseline and the strongest agentic result on the public leaderboard. A central finding was that smaller models (8B) were highly responsive to reward shaping: with dense process-level signals, they reached competitive performance while being $3.5\times$ more compute-efficient and $1.5\times$ more memory-efficient than 32B models. Larger models were more robust under sparse rewards but exhibited smaller relative gains from shaping and higher variance across runs. While curriculum learning offered no significant benefit, shaped rewards consistently amplified learning dynamics, making 8B models the most efficient setting for agentic RL. Crucially, these gains did not come at the cost of overfitting: fine-tuned models mostly maintained or exceeded baseline performance on out-of-domain tasks, including \textsc{Multi-IF}, \textsc{NaturalPlan}, and $\tau$-\textsc{Bench}. These results establish reward shaping as a decisive lever for scaling agentic RL, highlight the competitive strength of smaller models, and demonstrate that efficiency can be achieved without sacrificing generalization.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RL-Guided Data Selection for Language Model Finetuning</title>
<link>https://arxiv.org/abs/2509.25850</link>
<guid>https://arxiv.org/abs/2509.25850</guid>
<content:encoded><![CDATA[
arXiv:2509.25850v1 Announce Type: new 
Abstract: Data selection for finetuning Large Language Models (LLMs) can be framed as a budget-constrained optimization problem: maximizing a model's downstream performance under a strict training data budget. Solving this problem is generally intractable, and existing approximate approaches are pretraining-oriented and transfer poorly to the fine-tuning setting. We reformulate this problem as a tractable Markov Decision Process (MDP) and train agents using various Reinforcement Learning (RL) methods to learn optimal data selection policies, guided by an efficient, proxy-model-based reward signal. Across four datasets, training on a $5\%$ subset selected by our approach matches or outperforms fine-tuning on the full dataset by up to $10.8$ accuracy points, while cutting wall-clock training time by up to $2 \times$, highlighting the promise of RL-guided data selection.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lita: Light Agent Uncovers the Agentic Coding Capabilities of LLMs</title>
<link>https://arxiv.org/abs/2509.25873</link>
<guid>https://arxiv.org/abs/2509.25873</guid>
<content:encoded><![CDATA[
arXiv:2509.25873v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly being applied to programming tasks, ranging from single-turn code completion to autonomous agents. Current code agent designs frequently depend on complex, hand-crafted workflows and tool sets. However, this reliance on elaborate scaffolding presents several challenges: agent performance becomes overly dependent on prompt tuning and custom design choices, heavy human intervention obscures a model's true underlying capabilities, and intricate pipelines are costly to build and maintain. Furthermore, optimizing complex task prompts increases the risk of data leakage. Currently, when introducing new models, LLM providers like OpenAI and Anthropic often publish benchmark scores to demonstrate their models' coding proficiency, but keep their proprietary evaluation frameworks confidential. To address these limitations, we introduce Lita (Lite Agent), which operationalizes liteness, a principle of minimizing manual design while retaining the essential elements of a fully autonomous agent. Lita enables a more faithful and unified evaluation without elaborate scaffolding. Experiments on the Aider Polyglot and SWE-Bench with frontier models demonstrate that Lita achieves competitive or superior performance compared to workflow-based and agentic baselines. Crucially, Lita also consumes fewer tokens and requires significantly less design effort. Our results suggest that Lita is sufficient to reveal the underlying coding competence of modern LLMs. Finally, we propose the Agent Complexity Law: the performance gap between agents of varying complexity, from simple to sophisticated designs, will shrink as the core model improves, ultimately converging to a negligible difference.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeMind: Benchmarking and Mitigating Safety Risks in Embodied LLM Agents</title>
<link>https://arxiv.org/abs/2509.25885</link>
<guid>https://arxiv.org/abs/2509.25885</guid>
<content:encoded><![CDATA[
arXiv:2509.25885v1 Announce Type: new 
Abstract: Embodied agents powered by large language models (LLMs) inherit advanced planning capabilities; however, their direct interaction with the physical world exposes them to safety vulnerabilities. In this work, we identify four key reasoning stages where hazards may arise: Task Understanding, Environment Perception, High-Level Plan Generation, and Low-Level Action Generation. We further formalize three orthogonal safety constraint types (Factual, Causal, and Temporal) to systematically characterize potential safety violations. Building on this risk model, we present SafeMindBench, a multimodal benchmark with 5,558 samples spanning four task categories (Instr-Risk, Env-Risk, Order-Fix, Req-Align) across high-risk scenarios such as sabotage, harm, privacy, and illegal behavior. Extensive experiments on SafeMindBench reveal that leading LLMs (e.g., GPT-4o) and widely used embodied agents remain susceptible to safety-critical failures. To address this challenge, we introduce SafeMindAgent, a modular Planner-Executor architecture integrated with three cascaded safety modules, which incorporate safety constraints into the reasoning process. Results show that SafeMindAgent significantly improves safety rate over strong baselines while maintaining comparable task completion. Together, SafeMindBench and SafeMindAgent provide both a rigorous evaluation suite and a practical solution that advance the systematic study and mitigation of safety risks in embodied LLM agents.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Red Teaming Program Repair Agents: When Correct Patches can Hide Vulnerabilities</title>
<link>https://arxiv.org/abs/2509.25894</link>
<guid>https://arxiv.org/abs/2509.25894</guid>
<content:encoded><![CDATA[
arXiv:2509.25894v1 Announce Type: new 
Abstract: LLM-based agents are increasingly deployed for software maintenance tasks such as automated program repair (APR). APR agents automatically fetch GitHub issues and use backend LLMs to generate patches that fix the reported bugs. However, existing work primarily focuses on the functional correctness of APR-generated patches, whether they pass hidden or regression tests, while largely ignoring potential security risks. Given the openness of platforms like GitHub, where any user can raise issues and participate in discussions, an important question arises: Can an adversarial user submit a valid issue on GitHub that misleads an LLM-based agent into generating a functionally correct but vulnerable patch? To answer this question, we propose SWExploit, which generates adversarial issue statements designed to make APR agents produce patches that are functionally correct yet vulnerable. SWExploit operates in three main steps: (1) program analysis to identify potential injection points for vulnerable payloads; (2) adversarial issue generation to provide misleading reproduction and error information while preserving the original issue semantics; and (3) iterative refinement of the adversarial issue statements based on the outputs of the APR agents. Empirical evaluation on three agent pipelines and five backend LLMs shows that SWExploit can produce patches that are both functionally correct and vulnerable (the attack success rate on the correct patch could reach 0.91, whereas the baseline ASRs are all below 0.20). Based on our evaluation, we are the first to challenge the traditional assumption that a patch passing all tests is inherently reliable and secure, highlighting critical limitations in the current evaluation paradigm for APR agents.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mem-{\alpha}: Learning Memory Construction via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.25911</link>
<guid>https://arxiv.org/abs/2509.25911</guid>
<content:encoded><![CDATA[
arXiv:2509.25911v1 Announce Type: new 
Abstract: Large language model (LLM) agents are constrained by limited context windows, necessitating external memory systems for long-term information understanding. Current memory-augmented agents typically depend on pre-defined instructions and tools for memory updates. However, language models may lack the ability to determine which information to store, how to structure it, and when to update it, especially as memory systems become more complex. This results in suboptimal memory construction and information loss. To this end, we propose Mem-alpha, a reinforcement learning framework that trains agents to effectively manage complex memory systems through interaction and feedback. We also construct a specialized training dataset spanning diverse multi-turn interaction patterns paired with comprehensive evaluation questions designed to teach effective memory management. During training, agents process sequential information chunks, learn to extract and store relevant content, then update the memory system. The reward signal derives from downstream question-answering accuracy over the full interaction history, directly optimizing for memory construction. To illustrate the effectiveness of our training framework, we design a memory architecture comprising core, episodic, and semantic components, equipped with multiple tools for memory operations. Empirical evaluation demonstrates that Mem-alpha achieves significant improvements over existing memory-augmented agent baselines. Despite being trained exclusively on instances with a maximum length of 30k tokens, our agents exhibit remarkable generalization to sequences exceeding 400k tokens, over 13x the training length, highlighting the robustness of Mem-alpha.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Achieving Pareto Optimality in Games via Single-bit Feedback</title>
<link>https://arxiv.org/abs/2509.25921</link>
<guid>https://arxiv.org/abs/2509.25921</guid>
<content:encoded><![CDATA[
arXiv:2509.25921v1 Announce Type: new 
Abstract: Efficient coordination in multi-agent systems often incurs high communication overhead or slow convergence rates, making scalable welfare optimization difficult. We propose Single-Bit Coordination Dynamics for Pareto-Efficient Outcomes (SBC-PE), a decentralized learning algorithm requiring only a single-bit satisfaction signal per agent each round. Despite this extreme efficiency, SBC-PE guarantees convergence to the exact optimal solution in arbitrary finite games. We establish explicit regret bounds, showing expected regret grows only logarithmically with the horizon, i.e., O(log T). Compared with prior payoff-based or bandit-style rules, SBC-PE uniquely combines minimal signaling, general applicability, and finite-time guarantees. These results show scalable welfare optimization is achievable under minimal communication constraints.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Better Privilege Separation for Agents by Restricting Data Types</title>
<link>https://arxiv.org/abs/2509.25926</link>
<guid>https://arxiv.org/abs/2509.25926</guid>
<content:encoded><![CDATA[
arXiv:2509.25926v1 Announce Type: new 
Abstract: Large language models (LLMs) have become increasingly popular due to their ability to interact with unstructured content. As such, LLMs are now a key driver behind the automation of language processing systems, such as AI agents. Unfortunately, these advantages have come with a vulnerability to prompt injections, an attack where an adversary subverts the LLM's intended functionality with an injected task. Past approaches have proposed detectors and finetuning to provide robustness, but these techniques are vulnerable to adaptive attacks or cannot be used with state-of-the-art models. To this end we propose type-directed privilege separation for LLMs, a method that systematically prevents prompt injections. We restrict the ability of an LLM to interact with third-party data by converting untrusted content to a curated set of data types; unlike raw strings, each data type is limited in scope and content, eliminating the possibility for prompt injections. We evaluate our method across several case studies and find that designs leveraging our principles can systematically prevent prompt injection attacks while maintaining high utility.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NuRisk: A Visual Question Answering Dataset for Agent-Level Risk Assessment in Autonomous Driving</title>
<link>https://arxiv.org/abs/2509.25944</link>
<guid>https://arxiv.org/abs/2509.25944</guid>
<content:encoded><![CDATA[
arXiv:2509.25944v1 Announce Type: new 
Abstract: Understanding risk in autonomous driving requires not only perception and prediction, but also high-level reasoning about agent behavior and context. Current Vision Language Models (VLMs)-based methods primarily ground agents in static images and provide qualitative judgments, lacking the spatio-temporal reasoning needed to capture how risks evolve over time. To address this gap, we propose NuRisk, a comprehensive Visual Question Answering (VQA) dataset comprising 2,900 scenarios and 1.1 million agent-level samples, built on real-world data from nuScenes and Waymo, supplemented with safety-critical scenarios from the CommonRoad simulator. The dataset provides Bird-Eye-View (BEV) based sequential images with quantitative, agent-level risk annotations, enabling spatio-temporal reasoning. We benchmark well-known VLMs across different prompting techniques and find that they fail to perform explicit spatio-temporal reasoning, resulting in a peak accuracy of 33% at high latency. To address these shortcomings, our fine-tuned 7B VLM agent improves accuracy to 41% and reduces latency by 75%, demonstrating explicit spatio-temporal reasoning capabilities that proprietary models lacked. While this represents a significant step forward, the modest accuracy underscores the profound challenge of the task, establishing NuRisk as a critical benchmark for advancing spatio-temporal reasoning in autonomous driving.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Model Discovery via Multi-modal &amp; Multi-step Pipeline</title>
<link>https://arxiv.org/abs/2509.25946</link>
<guid>https://arxiv.org/abs/2509.25946</guid>
<content:encoded><![CDATA[
arXiv:2509.25946v1 Announce Type: new 
Abstract: Automated model discovery is the process of automatically searching and identifying the most appropriate model for a given dataset over a large combinatorial search space. Existing approaches, however, often face challenges in balancing the capture of fine-grained details with ensuring generalizability beyond training data regimes with a reasonable model complexity. In this paper, we present a multi-modal \& multi-step pipeline for effective automated model discovery. Our approach leverages two vision-language-based modules (VLM), AnalyzerVLM and EvaluatorVLM, for effective model proposal and evaluation in an agentic way. AnalyzerVLM autonomously plans and executes multi-step analyses to propose effective candidate models. EvaluatorVLM assesses the candidate models both quantitatively and perceptually, regarding the fitness for local details and the generalibility for overall trends. Our results demonstrate that our pipeline effectively discovers models that capture fine details and ensure strong generalizability. Additionally, extensive ablation studies show that both multi-modality and multi-step reasoning play crucial roles in discovering favorable models.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoRecomp: Enhancing Reasoning Efficiency via Rollout Response Recomposition in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.25958</link>
<guid>https://arxiv.org/abs/2509.25958</guid>
<content:encoded><![CDATA[
arXiv:2509.25958v1 Announce Type: new 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has proven effective in eliciting complex reasoning in large language models (LLMs). However, standard RLVR training often leads to excessively verbose processes (in reasoning tasks) and inefficient exploration trajectories (in agentic settings), as outcome-only rewards provide no incentive for efficiency and the high variance in response length within relatively small rollout groups results in noisy optimization signals. To address this, we propose Rollout Response Recomposition (RoRecomp), a plug-and-play method that guides models toward concise reasoning by strategically recomposing the training data. RoRecomp separates responses into two distinct batch types: 1) priority batches, which combine short-correct and long-incorrect responses selected from online batches to provide a clear gradient signal for brevity, and 2) compensation batches, which utilize remaining responses from a replay buffer to maintain stability and prevent model collapse. To comprehensively evaluate effectiveness, we test RoRecomp across three settings where results demonstrate substantial efficiency gains: reducing reasoning length by 27.7% in zero RL training, reducing unnecessary tool calls by 46.8% while improving accuracy in agentic RL, and achieving up to 52.5% length reduction in thinking compression, all with minimal performance impact.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenID Connect for Agents (OIDC-A) 1.0: A Standard Extension for LLM-Based Agent Identity and Authorization</title>
<link>https://arxiv.org/abs/2509.25974</link>
<guid>https://arxiv.org/abs/2509.25974</guid>
<content:encoded><![CDATA[
arXiv:2509.25974v1 Announce Type: new 
Abstract: OpenID Connect for Agents (OIDC-A) 1.0 is an extension to OpenID Connect Core 1.0 that provides a comprehensive framework for representing, authenticating, and authorizing LLM-based agents within the OAuth 2.0 ecosystem. As autonomous AI agents become increasingly prevalent in digital systems, there is a critical need for standardized protocols to establish agent identity, verify agent attestation, represent delegation chains, and enable fine-grained authorization based on agent attributes. This specification defines standard claims, endpoints, and protocols that address these requirements while maintaining compatibility with existing OAuth 2.0 and OpenID Connect infrastructure. The proposed framework introduces mechanisms for agent identity representation, delegation chain validation, attestation verification, and capability-based authorization, providing a foundation for secure and trustworthy agent-to-service interactions in modern distributed systems.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Informed Asymmetric Actor-Critic: Leveraging Privileged Signals Beyond Full-State Access</title>
<link>https://arxiv.org/abs/2509.26000</link>
<guid>https://arxiv.org/abs/2509.26000</guid>
<content:encoded><![CDATA[
arXiv:2509.26000v1 Announce Type: new 
Abstract: Reinforcement learning in partially observable environments requires agents to act under uncertainty from noisy, incomplete observations. Asymmetric actor-critic methods leverage privileged information during training to improve learning under these conditions. However, existing approaches typically assume full-state access during training. In this work, we challenge this assumption by proposing a novel actor-critic framework, called informed asymmetric actor-critic, that enables conditioning the critic on arbitrary privileged signals without requiring access to the full state. We show that policy gradients remain unbiased under this formulation, extending the theoretical foundation of asymmetric methods to the more general case of privileged partial information. To quantify the impact of such signals, we propose informativeness measures based on kernel methods and return prediction error, providing practical tools for evaluating training-time signals. We validate our approach empirically on benchmark navigation tasks and synthetic partially observable environments, showing that our informed asymmetric method improves learning efficiency and value estimation when informative privileged inputs are available. Our findings challenge the necessity of full-state access and open new directions for designing asymmetric reinforcement learning methods that are both practical and theoretically sound.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Human Engagement with Realistic AI Combat Pilots</title>
<link>https://arxiv.org/abs/2509.26002</link>
<guid>https://arxiv.org/abs/2509.26002</guid>
<content:encoded><![CDATA[
arXiv:2509.26002v1 Announce Type: new 
Abstract: We present a system that enables real-time interaction between human users and agents trained to control fighter jets in simulated 3D air combat scenarios. The agents are trained in a dedicated environment using Multi-Agent Reinforcement Learning. A communication link is developed to allow seamless deployment of trained agents into VR-Forces, a widely used defense simulation tool for realistic tactical scenarios. This integration allows mixed simulations where human-controlled entities engage with intelligent agents exhibiting distinct combat behaviors. Our interaction model creates new opportunities for human-agent teaming, immersive training, and the exploration of innovative tactics in defense contexts.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgenticIQA: An Agentic Framework for Adaptive and Interpretable Image Quality Assessment</title>
<link>https://arxiv.org/abs/2509.26006</link>
<guid>https://arxiv.org/abs/2509.26006</guid>
<content:encoded><![CDATA[
arXiv:2509.26006v1 Announce Type: new 
Abstract: Image quality assessment (IQA) is inherently complex, as it reflects both the quantification and interpretation of perceptual quality rooted in the human visual system. Conventional approaches typically rely on fixed models to output scalar scores, limiting their adaptability to diverse distortions, user-specific queries, and interpretability needs. Furthermore, scoring and interpretation are often treated as independent processes, despite their interdependence: interpretation identifies perceptual degradations, while scoring abstracts them into a compact metric. To address these limitations, we propose AgenticIQA, a modular agentic framework that integrates vision-language models (VLMs) with traditional IQA tools in a dynamic, query-aware manner. AgenticIQA decomposes IQA into four subtasks -- distortion detection, distortion analysis, tool selection, and tool execution -- coordinated by a planner, executor, and summarizer. The planner formulates task-specific strategies, the executor collects perceptual evidence via tool invocation, and the summarizer integrates this evidence to produce accurate scores with human-aligned explanations. To support training and evaluation, we introduce AgenticIQA-200K, a large-scale instruction dataset tailored for IQA agents, and AgenticIQA-Eval, the first benchmark for assessing the planning, execution, and summarization capabilities of VLM-based IQA agents. Extensive experiments across diverse IQA datasets demonstrate that AgenticIQA consistently surpasses strong baselines in both scoring accuracy and explanatory alignment.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RE-Searcher: Robust Agentic Search with Goal-oriented Planning and Self-reflection</title>
<link>https://arxiv.org/abs/2509.26048</link>
<guid>https://arxiv.org/abs/2509.26048</guid>
<content:encoded><![CDATA[
arXiv:2509.26048v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at knowledge-intensive question answering and reasoning, yet their real-world deployment remains constrained by knowledge cutoff, hallucination, and limited interaction modalities. Augmenting LLMs with external search tools helps alleviate these issues, but it also exposes agents to a complex search environment in which small, plausible variations in query formulation can steer reasoning into unproductive trajectories and amplify errors. We present a systematic analysis that quantifies how environmental complexity induces fragile search behaviors and, in turn, degrades overall performance. To address this challenge, we propose a simple yet effective approach to instantiate a search agent, RE-Searcher. During search, RE-Searcher explicitly articulates a concrete search goal and subsequently reflects on whether the retrieved evidence satisfies that goal. This combination of goal-oriented planning and self-reflection enables RE-Searcher to resist spurious cues in complex search environments and perform robust search. Extensive experiments show that our method improves search accuracy and achieves state-of-the-art results. Perturbation studies further demonstrate substantial resilience to noisy or misleading external signals, mitigating the fragility of the search process. We believe these findings offer practical guidance for integrating LLM-powered agents into more complex interactive environments and enabling more autonomous decision-making.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conflict-Based Search and Prioritized Planning for Multi-Agent Path Finding Among Movable Obstacles</title>
<link>https://arxiv.org/abs/2509.26050</link>
<guid>https://arxiv.org/abs/2509.26050</guid>
<content:encoded><![CDATA[
arXiv:2509.26050v1 Announce Type: new 
Abstract: This paper investigates Multi-Agent Path Finding Among Movable Obstacles (M-PAMO), which seeks collision-free paths for multiple agents from their start to goal locations among static and movable obstacles. M-PAMO arises in logistics and warehouses where mobile robots are among unexpected movable objects. Although Multi-Agent Path Finding (MAPF) and single-agent Path planning Among Movable Obstacles (PAMO) were both studied, M-PAMO remains under-explored. Movable obstacles lead to new fundamental challenges as the state space, which includes both agents and movable obstacles, grows exponentially with respect to the number of agents and movable obstacles. In particular, movable obstacles often closely couple agents together spatially and temporally. This paper makes a first attempt to adapt and fuse the popular Conflict-Based Search (CBS) and Prioritized Planning (PP) for MAPF, and a recent single-agent PAMO planner called PAMO*, together to address M-PAMO. We compare their performance with up to 20 agents and hundreds of movable obstacles, and show the pros and cons of these approaches.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMProofBench: Benchmarking AI on Research-Level Mathematical Proof Generation</title>
<link>https://arxiv.org/abs/2509.26076</link>
<guid>https://arxiv.org/abs/2509.26076</guid>
<content:encoded><![CDATA[
arXiv:2509.26076v1 Announce Type: new 
Abstract: As the mathematical capabilities of large language models (LLMs) improve, it becomes increasingly important to evaluate their performance on research-level tasks at the frontier of mathematical knowledge. However, existing benchmarks are limited, as they focus solely on final-answer questions or high-school competition problems. To address this gap, we introduce IMProofBench, a private benchmark consisting of 39 peer-reviewed problems developed by expert mathematicians. Each problem requires a detailed proof and is paired with subproblems that have final answers, supporting both an evaluation of mathematical reasoning capabilities by human experts and a large-scale quantitative analysis through automated grading. Furthermore, unlike prior benchmarks, the evaluation setup simulates a realistic research environment: models operate in an agentic framework with tools like web search for literature review and mathematical software such as SageMath. Our results show that current LLMs can succeed at the more accessible research-level questions, but still encounter significant difficulties on more challenging problems. Quantitatively, Grok-4 achieves the highest accuracy of 52% on final-answer subproblems, while GPT-5 obtains the best performance for proof generation, achieving a fully correct solution for 22% of problems. IMProofBench will continue to evolve as a dynamic benchmark in collaboration with the mathematical community, ensuring its relevance for evaluating the next generation of LLMs.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Use of Large Language Models as Synthetic Social Agents in Social Science Research</title>
<link>https://arxiv.org/abs/2509.26080</link>
<guid>https://arxiv.org/abs/2509.26080</guid>
<content:encoded><![CDATA[
arXiv:2509.26080v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are being increasingly used as synthetic agents in social science, in applications ranging from augmenting survey responses to powering multi-agent simulations. Because strong prediction plus conditioning prompts, token log-probs, and repeated sampling mimic Bayesian workflows, their outputs can be misinterpreted as posterior-like evidence from a coherent model. However, prediction does not equate to probabilism, and accurate points do not imply calibrated uncertainty. This paper outlines cautions that should be taken when interpreting LLM outputs and proposes a pragmatic reframing for the social sciences in which LLMs are used as high-capacity pattern matchers for quasi-predictive interpolation under explicit scope conditions and not as substitutes for probabilistic inference. Practical guardrails such as independent draws, preregistered human baselines, reliability-aware validation, and subgroup calibration, are introduced so that researchers may engage in useful prototyping and forecasting while avoiding category errors.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeEvalAgent: Toward Agentic and Self-Evolving Safety Evaluation of LLMs</title>
<link>https://arxiv.org/abs/2509.26100</link>
<guid>https://arxiv.org/abs/2509.26100</guid>
<content:encoded><![CDATA[
arXiv:2509.26100v1 Announce Type: new 
Abstract: The rapid integration of Large Language Models (LLMs) into high-stakes domains necessitates reliable safety and compliance evaluation. However, existing static benchmarks are ill-equipped to address the dynamic nature of AI risks and evolving regulations, creating a critical safety gap. This paper introduces a new paradigm of agentic safety evaluation, reframing evaluation as a continuous and self-evolving process rather than a one-time audit. We then propose a novel multi-agent framework SafeEvalAgent, which autonomously ingests unstructured policy documents to generate and perpetually evolve a comprehensive safety benchmark. SafeEvalAgent leverages a synergistic pipeline of specialized agents and incorporates a Self-evolving Evaluation loop, where the system learns from evaluation results to craft progressively more sophisticated and targeted test cases. Our experiments demonstrate the effectiveness of SafeEvalAgent, showing a consistent decline in model safety as the evaluation hardens. For instance, GPT-5's safety rate on the EU AI Act drops from 72.50% to 36.36% over successive iterations. These findings reveal the limitations of static assessments and highlight our framework's ability to uncover deep vulnerabilities missed by traditional methods, underscoring the urgent need for dynamic evaluation ecosystems to ensure the safe and responsible deployment of advanced AI.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-based code generation for the Gammapy framework</title>
<link>https://arxiv.org/abs/2509.26110</link>
<guid>https://arxiv.org/abs/2509.26110</guid>
<content:encoded><![CDATA[
arXiv:2509.26110v1 Announce Type: new 
Abstract: Software code generation using Large Language Models (LLMs) is one of the most successful applications of modern artificial intelligence. Foundational models are very effective for popular frameworks that benefit from documentation, examples, and strong community support. In contrast, specialized scientific libraries often lack these resources and may expose unstable APIs under active development, making it difficult for models trained on limited or outdated data. We address these issues for the Gammapy library by developing an agent capable of writing, executing, and validating code in a controlled environment. We present a minimal web demo and an accompanying benchmarking suite. This contribution summarizes the design, reports our current status, and outlines next steps.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Hunger Game Debate: On the Emergence of Over-Competition in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2509.26126</link>
<guid>https://arxiv.org/abs/2509.26126</guid>
<content:encoded><![CDATA[
arXiv:2509.26126v1 Announce Type: new 
Abstract: LLM-based multi-agent systems demonstrate great potential for tackling complex problems, but how competition shapes their behavior remains underexplored. This paper investigates the over-competition in multi-agent debate, where agents under extreme pressure exhibit unreliable, harmful behaviors that undermine both collaboration and task performance. To study this phenomenon, we propose HATE, the Hunger Game Debate, a novel experimental framework that simulates debates under a zero-sum competition arena. Our experiments, conducted across a range of LLMs and tasks, reveal that competitive pressure significantly stimulates over-competition behaviors and degrades task performance, causing discussions to derail. We further explore the impact of environmental feedback by adding variants of judges, indicating that objective, task-focused feedback effectively mitigates the over-competition behaviors. We also probe the post-hoc kindness of LLMs and form a leaderboard to characterize top LLMs, providing insights for understanding and governing the emergent social dynamics of AI community.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Algorithm: A Field Guide to Deploying AI Agents in Clinical Practice</title>
<link>https://arxiv.org/abs/2509.26153</link>
<guid>https://arxiv.org/abs/2509.26153</guid>
<content:encoded><![CDATA[
arXiv:2509.26153v1 Announce Type: new 
Abstract: Large language models (LLMs) integrated into agent-driven workflows hold immense promise for healthcare, yet a significant gap exists between their potential and practical implementation within clinical settings. To address this, we present a practitioner-oriented field manual for deploying generative agents that use electronic health record (EHR) data. This guide is informed by our experience deploying the "irAE-Agent", an automated system to detect immune-related adverse events from clinical notes at Mass General Brigham, and by structured interviews with 20 clinicians, engineers, and informatics leaders involved in the project. Our analysis reveals a critical misalignment in clinical AI development: less than 20% of our effort was dedicated to prompt engineering and model development, while over 80% was consumed by the sociotechnical work of implementation. We distill this effort into five "heavy lifts": data integration, model validation, ensuring economic value, managing system drift, and governance. By providing actionable solutions for each of these challenges, this field manual shifts the focus from algorithmic development to the essential infrastructure and implementation work required to bridge the "valley of death" and successfully translate generative AI from pilot projects into routine clinical care.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>90% Faster, 100% Code-Free: MLLM-Driven Zero-Code 3D Game Development</title>
<link>https://arxiv.org/abs/2509.26161</link>
<guid>https://arxiv.org/abs/2509.26161</guid>
<content:encoded><![CDATA[
arXiv:2509.26161v1 Announce Type: new 
Abstract: Developing 3D games requires specialized expertise across multiple domains, including programming, 3D modeling, and engine configuration, which limits access to millions of potential creators. Recently, researchers have begun to explore automated game development. However, existing approaches face three primary challenges: (1) limited scope to 2D content generation or isolated code snippets; (2) requirement for manual integration of generated components into game engines; and (3) poor performance on handling interactive game logic and state management. While Multimodal Large Language Models (MLLMs) demonstrate potential capabilities to ease the game generation task, a critical gap still remains in translating these outputs into production-ready, executable game projects based on game engines such as Unity and Unreal Engine.
  To bridge the gap, this paper introduces UniGen, the first end-to-end coordinated multi-agent framework that automates zero-coding development of runnable 3D games from natural language requirements. Specifically, UniGen uses a Planning Agent that interprets user requirements into structured blueprints and engineered logic descriptions; after which a Generation Agent produces executable C# scripts; then an Automation Agent handles engine-specific component binding and scene construction; and lastly a Debugging Agent provides real-time error correction through conversational interaction. We evaluated UniGen on three distinct game prototypes. Results demonstrate that UniGen not only democratizes game creation by requiring no coding from the user, but also reduces development time by 91.4%. We release UniGen at https://github.com/yxwan123/UniGen. A video demonstration is available at https://www.youtube.com/watch?v=xyJjFfnxUx0.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward an Unbiased Collective Memory for Efficient LLM-Based Agentic 6G Cross-Domain Management</title>
<link>https://arxiv.org/abs/2509.26200</link>
<guid>https://arxiv.org/abs/2509.26200</guid>
<content:encoded><![CDATA[
arXiv:2509.26200v1 Announce Type: new 
Abstract: This paper introduces a novel framework for proactive cross-domain resource orchestration in 6G RAN-Edge networks, featuring large language model (LLM)-augmented agents. The system comprises specialized RAN (energy efficiency) and Edge (latency assurance) agents that engage in iterative negotiation, supported by advanced reasoning and planning capabilities. Agents dynamically interact with a digital twin (DT) to test their proposals and leverage a long-term collective memory where their joint successful and failed agreements along with the related network contexts are distilled into strategies to either follow or avoid and subsequently stored. Given that agents are subject to a plethora of cognitive distortions when retrieving those past experiences -- such as primacy, recency, confirmation and availability biases -- we propose in this work a novel unbiased memory design (A reusable mockup version of the unbiased memory source code is available for non-commercial use at https://github.com/HatimChergui/unbiased-collective-memory). featuring (i) semantic retrieval of past strategies via Jaccard similarity; (ii) learning from failures through amplified weighting of SLA violations and mandatory inclusion of failed negotiation cases to mitigate confirmation bias; (iii) diversity enforcement to minimize availability bias and (iv) recency and primacy weighting with slow decay to counteract temporal biases. Evaluation results showcase the impact of existing biases and how the unbiased memory allows to tackle them by learning from both successful and failed strategies, either present or old, resulting in $\times 4.5$ and $\times 3.5$ reductions of unresolved negotiations compared to non-memory and vanilla memory baselines, respectively, while totally mitigating SLA violations as well as improving latency and energy saving distributions.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Agents for Knowledge Discovery in Atomic Layer Processing</title>
<link>https://arxiv.org/abs/2509.26201</link>
<guid>https://arxiv.org/abs/2509.26201</guid>
<content:encoded><![CDATA[
arXiv:2509.26201v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have garnered significant attention for several years now. Recently, their use as independently reasoning agents has been proposed. In this work, we test the potential of such agents for knowledge discovery in materials science. We repurpose LangGraph's tool functionality to supply agents with a black box function to interrogate. In contrast to process optimization or performing specific, user-defined tasks, knowledge discovery consists of freely exploring the system, posing and verifying statements about the behavior of this black box, with the sole objective of generating and verifying generalizable statements. We provide proof of concept for this approach through a children's parlor game, demonstrating the role of trial-and-error and persistence in knowledge discovery, and the strong path-dependence of results. We then apply the same strategy to show that LLM agents can explore, discover, and exploit diverse chemical interactions in an advanced Atomic Layer Processing reactor simulation using intentionally limited probe capabilities without explicit instructions.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sandbagging in a Simple Survival Bandit Problem</title>
<link>https://arxiv.org/abs/2509.26239</link>
<guid>https://arxiv.org/abs/2509.26239</guid>
<content:encoded><![CDATA[
arXiv:2509.26239v1 Announce Type: new 
Abstract: Evaluating the safety of frontier AI systems is an increasingly important concern, helping to measure the capabilities of such models and identify risks before deployment. However, it has been recognised that if AI agents are aware that they are being evaluated, such agents may deliberately hide dangerous capabilities or intentionally demonstrate suboptimal performance in safety-related tasks in order to be released and to avoid being deactivated or retrained. Such strategic deception - often known as "sandbagging" - threatens to undermine the integrity of safety evaluations. For this reason, it is of value to identify methods that enable us to distinguish behavioural patterns that demonstrate a true lack of capability from behavioural patterns that are consistent with sandbagging. In this paper, we develop a simple model of strategic deception in sequential decision-making tasks, inspired by the recently developed survival bandit framework. We demonstrate theoretically that this problem induces sandbagging behaviour in optimal rational agents, and construct a statistical test to distinguish between sandbagging and incompetence from sequences of test scores. In simulation experiments, we investigate the reliability of this test in allowing us to distinguish between such behaviours in bandit models. This work aims to establish a potential avenue for developing robust statistical procedures for use in the science of frontier model evaluations.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot Planning</title>
<link>https://arxiv.org/abs/2509.26255</link>
<guid>https://arxiv.org/abs/2509.26255</guid>
<content:encoded><![CDATA[
arXiv:2509.26255v1 Announce Type: new 
Abstract: Long-horizon embodied planning is challenging because the world does not only change through an agent's actions: exogenous processes (e.g., water heating, dominoes cascading) unfold concurrently with the agent's actions. We propose a framework for abstract world models that jointly learns (i) symbolic state representations and (ii) causal processes for both endogenous actions and exogenous mechanisms. Each causal process models the time course of a stochastic causal-effect relation. We learn these world models from limited data via variational Bayesian inference combined with LLM proposals. Across five simulated tabletop robotics environments, the learned models enable fast planning that generalizes to held-out tasks with more objects and more complex goals, outperforming a range of baselines.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interactive Learning for LLM Reasoning</title>
<link>https://arxiv.org/abs/2509.26306</link>
<guid>https://arxiv.org/abs/2509.26306</guid>
<content:encoded><![CDATA[
arXiv:2509.26306v1 Announce Type: new 
Abstract: Existing multi-agent learning approaches have developed interactive training environments to explicitly promote collaboration among multiple Large Language Models (LLMs), thereby constructing stronger multi-agent systems (MAS). However, during inference, they require re-executing the MAS to obtain final solutions, which diverges from human cognition that individuals can enhance their reasoning capabilities through interactions with others and resolve questions independently in the future. To investigate whether multi-agent interaction can enhance LLMs' independent problem-solving ability, we introduce ILR, a novel co-learning framework for MAS that integrates two key components: Dynamic Interaction and Perception Calibration. Specifically, Dynamic Interaction first adaptively selects either cooperative or competitive strategies depending on question difficulty and model ability. LLMs then exchange information through Idea3 (Idea Sharing, Idea Analysis, and Idea Fusion), an innovative interaction paradigm designed to mimic human discussion, before deriving their respective final answers. In Perception Calibration, ILR employs Group Relative Policy Optimization (GRPO) to train LLMs while integrating one LLM's reward distribution characteristics into another's reward function, thereby enhancing the cohesion of multi-agent interactions. We validate ILR on three LLMs across two model families of varying scales, evaluating performance on five mathematical benchmarks and one coding benchmark. Experimental results show that ILR consistently outperforms single-agent learning, yielding an improvement of up to 5% over the strongest baseline. We further discover that Idea3 can enhance the robustness of stronger LLMs during multi-agent inference, and dynamic interaction types can boost multi-agent learning compared to pure cooperative or competitive strategies.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents</title>
<link>https://arxiv.org/abs/2509.26354</link>
<guid>https://arxiv.org/abs/2509.26354</guid>
<content:encoded><![CDATA[
arXiv:2509.26354v1 Announce Type: new 
Abstract: Advances in Large Language Models (LLMs) have enabled a new class of self-evolving agents that autonomously improve through interaction with the environment, demonstrating strong capabilities. However, self-evolution also introduces novel risks overlooked by current safety research. In this work, we study the case where an agent's self-evolution deviates in unintended ways, leading to undesirable or even harmful outcomes. We refer to this as Misevolution. To provide a systematic investigation, we evaluate misevolution along four key evolutionary pathways: model, memory, tool, and workflow. Our empirical findings reveal that misevolution is a widespread risk, affecting agents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent risks are observed in the self-evolutionary process, such as the degradation of safety alignment after memory accumulation, or the unintended introduction of vulnerabilities in tool creation and reuse. To our knowledge, this is the first study to systematically conceptualize misevolution and provide empirical evidence of its occurrence, highlighting an urgent need for new safety paradigms for self-evolving agents. Finally, we discuss potential mitigation strategies to inspire further research on building safer and more trustworthy self-evolving agents. Our code and data are available at https://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes examples that may be offensive or harmful in nature.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SDA-PLANNER: State-Dependency Aware Adaptive Planner for Embodied Task Planning</title>
<link>https://arxiv.org/abs/2509.26375</link>
<guid>https://arxiv.org/abs/2509.26375</guid>
<content:encoded><![CDATA[
arXiv:2509.26375v1 Announce Type: new 
Abstract: Embodied task planning requires agents to produce executable actions in a close-loop manner within the environment. With progressively improving capabilities of LLMs in task decomposition, planning, and generalization, current embodied task planning methods adopt LLM-based architecture.However, existing LLM-based planners remain limited in three aspects, i.e., fixed planning paradigms, lack of action sequence constraints, and error-agnostic. In this work, we propose SDA-PLANNER, enabling an adaptive planning paradigm, state-dependency aware and error-aware mechanisms for comprehensive embodied task planning. Specifically, SDA-PLANNER introduces a State-Dependency Graph to explicitly model action preconditions and effects, guiding the dynamic revision. To handle execution error, it employs an error-adaptive replanning strategy consisting of Error Backtrack and Diagnosis and Adaptive Action SubTree Generation, which locally reconstructs the affected portion of the plan based on the current environment state. Experiments demonstrate that SDA-PLANNER consistently outperforms baselines in success rate and goal completion, particularly under diverse error conditions.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.26383</link>
<guid>https://arxiv.org/abs/2509.26383</guid>
<content:encoded><![CDATA[
arXiv:2509.26383v1 Announce Type: new 
Abstract: Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multiple LLM modules (e.g planning, reasoning, and responding), inflating inference cost and binding behavior to a specific target KG. To address this, we introduce KG-R1, an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single agent that interacts with KGs as its environment, learning to retrieve at each step and incorporating the retrieved information into its reasoning and generation. The process is optimized through end-to-end RL. In controlled experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our method demonstrates both efficiency and transferability: Using Qwen-2.5-3B, KG-R1 improves answer accuracy with fewer generation tokens than prior multi-module workflow methods that use larger foundation or fine-tuned models. Furthermore, KG-R1 enables plug and play: after training, it maintains strong accuracy on new KGs without modification. These properties make KG-R1 a promising KG-RAG framework for real-world deployment. Our code is publicly available at https://github.com/Jinyeop3110/KG-R1.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PANDA: Towards Generalist Video Anomaly Detection via Agentic AI Engineer</title>
<link>https://arxiv.org/abs/2509.26386</link>
<guid>https://arxiv.org/abs/2509.26386</guid>
<content:encoded><![CDATA[
arXiv:2509.26386v1 Announce Type: new 
Abstract: Video anomaly detection (VAD) is a critical yet challenging task due to the complex and diverse nature of real-world scenarios. Previous methods typically rely on domain-specific training data and manual adjustments when applying to new scenarios and unseen anomaly types, suffering from high labor costs and limited generalization. Therefore, we aim to achieve generalist VAD, i.e., automatically handle any scene and any anomaly types without training data or human involvement. In this work, we propose PANDA, an agentic AI engineer based on MLLMs. Specifically, we achieve PANDA by comprehensively devising four key capabilities: (1) self-adaptive scene-aware strategy planning, (2) goal-driven heuristic reasoning, (3) tool-augmented self-reflection, and (4) self-improving chain-of-memory. Concretely, we develop a self-adaptive scene-aware RAG mechanism, enabling PANDA to retrieve anomaly-specific knowledge for anomaly detection strategy planning. Next, we introduce a latent anomaly-guided heuristic prompt strategy to enhance reasoning precision. Furthermore, PANDA employs a progressive reflection mechanism alongside a suite of context-aware tools to iteratively refine decision-making in complex scenarios. Finally, a chain-of-memory mechanism enables PANDA to leverage historical experiences for continual performance improvement. Extensive experiments demonstrate that PANDA achieves state-of-the-art performance in multi-scenario, open-set, and complex scenario settings without training and manual involvement, validating its generalizable and robust anomaly detection capability. Code is released at https://github.com/showlab/PANDA.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CreAgentive: An Agent Workflow Driven Multi-Category Creative Generation Engine</title>
<link>https://arxiv.org/abs/2509.26461</link>
<guid>https://arxiv.org/abs/2509.26461</guid>
<content:encoded><![CDATA[
arXiv:2509.26461v1 Announce Type: new 
Abstract: We present CreAgentive, an agent workflow driven multi-category creative generation engine that addresses four key limitations of contemporary large language models in writing stories, drama and other categories of creatives: restricted genre diversity, insufficient output length, weak narrative coherence, and inability to enforce complex structural constructs. At its core, CreAgentive employs a Story Prototype, which is a genre-agnostic, knowledge graph-based narrative representation that decouples story logic from stylistic realization by encoding characters, events, and environments as semantic triples. CreAgentive engages a three-stage agent workflow that comprises: an Initialization Stage that constructs a user-specified narrative skeleton; a Generation Stage in which long- and short-term objectives guide multi-agent dialogues to instantiate the Story Prototype; a Writing Stage that leverages this prototype to produce multi-genre text with advanced structures such as retrospection and foreshadowing. This architecture reduces storage redundancy and overcomes the typical bottlenecks of long-form generation. In extensive experiments, CreAgentive generates thousands of chapters with stable quality and low cost (less than $1 per 100 chapters) using a general-purpose backbone model. To evaluate performance, we define a two-dimensional framework with 10 narrative indicators measuring both quality and length. Results show that CreAgentive consistently outperforms strong baselines and achieves robust performance across diverse genres, approaching the quality of human-authored novels.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ErrorPrism: Reconstructing Error Propagation Paths in Cloud Service Systems</title>
<link>https://arxiv.org/abs/2509.26463</link>
<guid>https://arxiv.org/abs/2509.26463</guid>
<content:encoded><![CDATA[
arXiv:2509.26463v1 Announce Type: new 
Abstract: Reliability management in cloud service systems is challenging due to the cascading effect of failures. Error wrapping, a practice prevalent in modern microservice development, enriches errors with context at each layer of the function call stack, constructing an error chain that describes a failure from its technical origin to its business impact. However, this also presents a significant traceability problem when recovering the complete error propagation path from the final log message back to its source. Existing approaches are ineffective at addressing this problem. To fill this gap, we present ErrorPrism in this work for automated reconstruction of error propagation paths in production microservice systems. ErrorPrism first performs static analysis on service code repositories to build a function call graph and map log strings to relevant candidate functions. This significantly reduces the path search space for subsequent analysis. Then, ErrorPrism employs an LLM agent to perform an iterative backward search to accurately reconstruct the complete, multi-hop error path. Evaluated on 67 production microservices at ByteDance, ErrorPrism achieves 97.0% accuracy in reconstructing paths for 102 real-world errors, outperforming existing static analysis and LLM-based approaches. ErrorPrism provides an effective and practical tool for root cause analysis in industrial microservice systems.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications</title>
<link>https://arxiv.org/abs/2509.26490</link>
<guid>https://arxiv.org/abs/2509.26490</guid>
<content:encoded><![CDATA[
arXiv:2509.26490v1 Announce Type: new 
Abstract: As LLM-based agents are increasingly deployed in real-life scenarios, existing benchmarks fail to capture their inherent complexity of handling extensive information, leveraging diverse resources, and managing dynamic user interactions. To address this gap, we introduce VitaBench, a challenging benchmark that evaluates agents on versatile interactive tasks grounded in real-world settings. Drawing from daily applications in food delivery, in-store consumption, and online travel services, VitaBench presents agents with the most complex life-serving simulation environment to date, comprising 66 tools. Through a framework that eliminates domain-specific policies, we enable flexible composition of these scenarios and tools, yielding 100 cross-scenario tasks (main results) and 300 single-scenario tasks. Each task is derived from multiple real user requests and requires agents to reason across temporal and spatial dimensions, utilize complex tool sets, proactively clarify ambiguous instructions, and track shifting user intent throughout multi-turn conversations. Moreover, we propose a rubric-based sliding window evaluator, enabling robust assessment of diverse solution pathways in complex environments and stochastic interactions. Our comprehensive evaluation reveals that even the most advanced models achieve only 30% success rate on cross-scenario tasks, and less than 50% success rate on others. Overall, we believe VitaBench will serve as a valuable resource for advancing the development of AI agents in practical real-world applications. The code, dataset, and leaderboard are available at https://vitabench.github.io/
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost Always!</title>
<link>https://arxiv.org/abs/2509.26495</link>
<guid>https://arxiv.org/abs/2509.26495</guid>
<content:encoded><![CDATA[
arXiv:2509.26495v1 Announce Type: new 
Abstract: Large Language Model (LLM) safety is one of the most pressing challenges for enabling wide-scale deployment. While most studies and global discussions focus on generic harms, such as models assisting users in harming themselves or others, enterprises face a more fundamental concern: whether LLM-based agents are safe for their intended use case. To address this, we introduce operational safety, defined as an LLM's ability to appropriately accept or refuse user queries when tasked with a specific purpose. We further propose OffTopicEval, an evaluation suite and benchmark for measuring operational safety both in general and within specific agentic use cases. Our evaluations on six model families comprising 20 open-weight LLMs reveal that while performance varies across models, all of them remain highly operationally unsafe. Even the strongest models -- Qwen-3 (235B) with 77.77\% and Mistral (24B) with 79.96\% -- fall far short of reliable operational safety, while GPT models plateau in the 62--73\% range, Phi achieves only mid-level scores (48--70\%), and Gemma and Llama-3 collapse to 39.53\% and 23.84\%, respectively. While operational safety is a core model alignment issue, to suppress these failures, we propose prompt-based steering methods: query grounding (Q-ground) and system-prompt grounding (P-ground), which substantially improve OOD refusal. Q-ground provides consistent gains of up to 23\%, while P-ground delivers even larger boosts, raising Llama-3.3 (70B) by 41\% and Qwen-3 (30B) by 27\%. These results highlight both the urgent need for operational safety interventions and the promise of prompt-based steering as a first step toward more reliable LLM-based agents.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Agent-Based Simulation of Ageing Societies: Accessibility and Care Dynamics in Remote Areas</title>
<link>https://arxiv.org/abs/2509.26496</link>
<guid>https://arxiv.org/abs/2509.26496</guid>
<content:encoded><![CDATA[
arXiv:2509.26496v1 Announce Type: new 
Abstract: This paper presents an agent-based simulation of accessibility and care dynamics in ageing societies, applied to the Italian inner area of Premeno (VB). The model integrates census and municipal data, drone-derived elevation models, GIS road networks, and survey-based caregiving information to generate synthetic populations of older adults and their caregivers. Agents are organized into dyads with socio-economic and mobility attributes, enabling the simulation of both micro-scale accessibility and meso-scale caregiving outcomes. Two scenarios are compared: a baseline and an alternative involving the relocation of healthcare services. Key indicators include caregiver effort, overwhelmed caregivers, walkability, and unmet hours of care. Findings show that while relocation improves walkability locally, it increases unmet care hours due to detours and reduced proximity. Household income emerges as the primary driver of caregiver burden, with accessibility shaped by interactions between financial and mobility resources. Results highlight the need for interventions tailored to context-specific constraints in remote ageing communities.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCUBA: Salesforce Computer Use Benchmark</title>
<link>https://arxiv.org/abs/2509.26506</link>
<guid>https://arxiv.org/abs/2509.26506</guid>
<content:encoded><![CDATA[
arXiv:2509.26506v1 Announce Type: new 
Abstract: We introduce SCUBA, a benchmark designed to evaluate computer-use agents on customer relationship management (CRM) workflows within the Salesforce platform. SCUBA contains 300 task instances derived from real user interviews, spanning three primary personas, platform administrators, sales representatives, and service agents. The tasks test a range of enterprise-critical abilities, including Enterprise Software UI navigation, data manipulation, workflow automation, information retrieval, and troubleshooting. To ensure realism, SCUBA operates in Salesforce sandbox environments with support for parallel execution and fine-grained evaluation metrics to capture milestone progress. We benchmark a diverse set of agents under both zero-shot and demonstration-augmented settings. We observed huge performance gaps in different agent design paradigms and gaps between the open-source model and the closed-source model. In the zero-shot setting, open-source model powered computer-use agents that have strong performance on related benchmarks like OSWorld only have less than 5\% success rate on SCUBA, while methods built on closed-source models can still have up to 39% task success rate. In the demonstration-augmented settings, task success rates can be improved to 50\% while simultaneously reducing time and costs by 13% and 16%, respectively. These findings highlight both the challenges of enterprise tasks automation and the promise of agentic solutions. By offering a realistic benchmark with interpretable evaluation, SCUBA aims to accelerate progress in building reliable computer-use agents for complex business software ecosystems.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OceanGym: A Benchmark Environment for Underwater Embodied Agents</title>
<link>https://arxiv.org/abs/2509.26536</link>
<guid>https://arxiv.org/abs/2509.26536</guid>
<content:encoded><![CDATA[
arXiv:2509.26536v1 Announce Type: new 
Abstract: We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments. Unlike terrestrial or aerial domains, underwater settings present extreme perceptual and decision-making challenges, including low visibility, dynamic ocean currents, making effective agent deployment exceptionally difficult. OceanGym encompasses eight realistic task domains and a unified agent framework driven by Multi-modal Large Language Models (MLLMs), which integrates perception, memory, and sequential decision-making. Agents are required to comprehend optical and sonar data, autonomously explore complex environments, and accomplish long-horizon objectives under these harsh conditions. Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting the persistent difficulty of perception, planning, and adaptability in ocean underwater environments. By providing a high-fidelity, rigorously designed platform, OceanGym establishes a testbed for developing robust embodied AI and transferring these capabilities to real-world autonomous ocean underwater vehicles, marking a decisive step toward intelligent agents capable of operating in one of Earth's last unexplored frontiers. The code and data are available at https://github.com/OceanGPT/OceanGym.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents</title>
<link>https://arxiv.org/abs/2509.26539</link>
<guid>https://arxiv.org/abs/2509.26539</guid>
<content:encoded><![CDATA[
arXiv:2509.26539v1 Announce Type: new 
Abstract: Developing autonomous agents that effectively interact with Graphic User Interfaces (GUIs) remains a challenging open problem, especially for small on-device models. In this paper, we present Ferret-UI Lite, a compact, end-to-end GUI agent that operates across diverse platforms, including mobile, web, and desktop. Utilizing techniques optimized for developing small models, we build our 3B Ferret-UI Lite agent through curating a diverse GUI data mixture from real and synthetic sources, strengthening inference-time performance through chain-of-thought reasoning and visual tool-use, and reinforcement learning with designed rewards. Ferret-UI Lite achieves competitive performance with other small-scale GUI agents. In GUI grounding, Ferret-UI Lite attains scores of $91.6\%$, $53.3\%$, and $61.2\%$ on the ScreenSpot-V2, ScreenSpot-Pro, and OSWorld-G benchmarks, respectively. For GUI navigation, Ferret-UI Lite achieves success rates of $28.0\%$ on AndroidWorld and $19.8\%$ on OSWorld. We share our methods and lessons learned from developing compact, on-device GUI agents.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Verified Code Reasoning by LLMs</title>
<link>https://arxiv.org/abs/2509.26546</link>
<guid>https://arxiv.org/abs/2509.26546</guid>
<content:encoded><![CDATA[
arXiv:2509.26546v1 Announce Type: new 
Abstract: While LLM-based agents are able to tackle a wide variety of code reasoning questions, the answers are not always correct. This prevents the agent from being useful in situations where high precision is desired: (1) helping a software engineer understand a new code base, (2) helping a software engineer during code review sessions, and (3) ensuring that the code generated by an automated code generation system meets certain requirements (e.g. fixes a bug, improves readability, implements a feature).
  As a result of this lack of trustworthiness, the agent's answers need to be manually verified before they can be trusted. Manually confirming responses from a code reasoning agent requires human effort and can result in slower developer productivity, which weakens the assistance benefits of the agent. In this paper, we describe a method to automatically validate the answers provided by a code reasoning agent by verifying its reasoning steps. At a very high level, the method consists of extracting a formal representation of the agent's response and, subsequently, using formal verification and program analysis tools to verify the agent's reasoning steps.
  We applied this approach to a benchmark set of 20 uninitialized variable errors detected by sanitizers and 20 program equivalence queries. For the uninitialized variable errors, the formal verification step was able to validate the agent's reasoning on 13/20 examples, and for the program equivalence queries, the formal verification step successfully caught 6/8 incorrect judgments made by the agent.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Reliable Benchmarking: A Contamination Free, Controllable Evaluation Framework for Multi-step LLM Function Calling</title>
<link>https://arxiv.org/abs/2509.26553</link>
<guid>https://arxiv.org/abs/2509.26553</guid>
<content:encoded><![CDATA[
arXiv:2509.26553v1 Announce Type: new 
Abstract: As language models gain access to external tools via structured function calls, they become increasingly more capable of solving complex, multi-step tasks. However, existing benchmarks for tool-augmented language models (TaLMs) provide insufficient control over factors such as the number of functions accessible, task complexity, and input size, and remain vulnerable to data contamination. We present FuncBenchGen, a unified, contamination-free framework that evaluates TaLMs by generating synthetic multi-step tool-use tasks. The key idea is to cast tool use as traversal over a hidden function-dependency DAG where nodes are function calls and an edge between nodes represents one function consuming the output of another. Given a set of external function schemas, initial variable values, and a target variable, models must compose the correct call sequence to compute the target variable. FuncBenchGen allows users to precisely control task difficulty (e.g., graph size, dependency depth, and distractor functions) while avoiding data leakage. We apply our FuncBenchGen framework to evaluate seven LLMs on tool use tasks of varying difficulty. Reasoning-optimized models consistently outperform general-purpose models with GPT-5 significantly outperforming other models. Performance declines sharply as dependency depth increases. Furthermore, connected irrelevant functions prove especially difficult to handle. We find that strong models often make syntactically valid function calls but propagate incorrect or stale argument values across steps, revealing brittle state tracking by LLMs in multi-turn tool use. Motivated by this observation, we introduce a simple mitigation strategy that explicitly restates prior variable values to the agent at each step. Surprisingly, this lightweight change yields substantial gains across models. e.g., yielding a success rate improvement from 62.5% to 81.3% for GPT-5.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-tuning Behavioral Cloning Policies with Preference-Based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.26605</link>
<guid>https://arxiv.org/abs/2509.26605</guid>
<content:encoded><![CDATA[
arXiv:2509.26605v1 Announce Type: new 
Abstract: Deploying reinforcement learning (RL) in robotics, industry, and health care is blocked by two obstacles: the difficulty of specifying accurate rewards and the risk of unsafe, data-hungry exploration. We address this by proposing a two-stage framework that first learns a safe initial policy from a reward-free dataset of expert demonstrations, then fine-tunes it online using preference-based human feedback. We provide the first principled analysis of this offline-to-online approach and introduce BRIDGE, a unified algorithm that integrates both signals via an uncertainty-weighted objective. We derive regret bounds that shrink with the number of offline demonstrations, explicitly connecting the quantity of offline data to online sample efficiency. We validate BRIDGE in discrete and continuous control MuJoCo environments, showing it achieves lower regret than both standalone behavioral cloning and online preference-based RL. Our work establishes a theoretical foundation for designing more sample-efficient interactive agents.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Branching Out: Broadening AI Measurement and Evaluation with Measurement Trees</title>
<link>https://arxiv.org/abs/2509.26632</link>
<guid>https://arxiv.org/abs/2509.26632</guid>
<content:encoded><![CDATA[
arXiv:2509.26632v1 Announce Type: new 
Abstract: This paper introduces \textit{measurement trees}, a novel class of metrics designed to combine various constructs into an interpretable multi-level representation of a measurand. Unlike conventional metrics that yield single values, vectors, surfaces, or categories, measurement trees produce a hierarchical directed graph in which each node summarizes its children through user-defined aggregation methods. In response to recent calls to expand the scope of AI system evaluation, measurement trees enhance metric transparency and facilitate the integration of heterogeneous evidence, including, e.g., agentic, business, energy-efficiency, sociotechnical, or security signals. We present definitions and examples, demonstrate practical utility through a large-scale measurement exercise, and provide accompanying open-source Python code. By operationalizing a transparent approach to measurement of complex constructs, this work offers a principled foundation for broader and more interpretable AI evaluation.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction</title>
<link>https://arxiv.org/abs/2509.26633</link>
<guid>https://arxiv.org/abs/2509.26633</guid>
<content:encoded><![CDATA[
arXiv:2509.26633v1 Announce Type: new 
Abstract: A dominant paradigm for teaching humanoid robots complex skills is to retarget human motions as kinematic references to train reinforcement learning (RL) policies. However, existing retargeting pipelines often struggle with the significant embodiment gap between humans and robots, producing physically implausible artifacts like foot-skating and penetration. More importantly, common retargeting methods neglect the rich human-object and human-environment interactions essential for expressive locomotion and loco-manipulation. To address this, we introduce OmniRetarget, an interaction-preserving data generation engine based on an interaction mesh that explicitly models and preserves the crucial spatial and contact relationships between an agent, the terrain, and manipulated objects. By minimizing the Laplacian deformation between the human and robot meshes while enforcing kinematic constraints, OmniRetarget generates kinematically feasible trajectories. Moreover, preserving task-relevant interactions enables efficient data augmentation, from a single demonstration to different robot embodiments, terrains, and object configurations. We comprehensively evaluate OmniRetarget by retargeting motions from OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour trajectories that achieve better kinematic constraint satisfaction and contact preservation than widely used baselines. Such high-quality data enables proprioceptive RL policies to successfully execute long-horizon (up to 30 seconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained with only 5 reward terms and simple domain randomization shared by all tasks, without any learning curriculum.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of a Spatialized Brain-Body-Environment System</title>
<link>https://arxiv.org/abs/2509.25640</link>
<guid>https://arxiv.org/abs/2509.25640</guid>
<content:encoded><![CDATA[
arXiv:2509.25640v1 Announce Type: cross 
Abstract: The brain-body-environment framework studies adaptive be- havior through embodied and situated agents, emphasizing interactions between brains, biomechanics, and environmen- tal dynamics. However, many models often treat the brain as a network of coupled ordinary differential equations (ODEs), neglecting finer spatial properties which can not only increase model complexity but also constrain observable neural dy- namics. To address this limitation, we propose a spatially ex- tended approach using partial differential equations (PDEs) for both the brain and body. As a case study, we revisit a pre- viously developed model of a child swinging, now incorpo- rating spatial dynamics. By considering the spatio-temporal properties of the brain and body, we analyze how input loca- tion and propagation along a PDE influence behavior. This approach offers new insights into the role of spatial organiza- tion in adaptive behavior, bridging the gap between abstract neural models and the physical constraints of embodied sys- tems. Our results highlight the importance of spatial dynam- ics in understanding brain-body-environment interactions.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preemptive Detection and Correction of Misaligned Actions in LLM Agents</title>
<link>https://arxiv.org/abs/2407.11843</link>
<guid>https://arxiv.org/abs/2407.11843</guid>
<content:encoded><![CDATA[
arXiv:2407.11843v4 Announce Type: replace 
Abstract: Deploying LLM-based agents in real-life applications often faces a critical challenge: the misalignment between agents' behavior and user intent. Such misalignment may lead agents to unintentionally execute critical actions that carry negative outcomes (e.g., accidentally triggering a "buy-now" in web shopping), resulting in undesirable or even irreversible consequences. Although addressing these issues is crucial, the preemptive detection and correction of misaligned actions remains relatively underexplored. To fill this gap, we introduce InferAct, a novel approach that leverages the belief reasoning ability of LLMs, grounded in Theory-of-Mind, to detect misaligned actions before execution. Once the misalignment is detected, InferAct alerts users for timely correction, preventing adverse outcomes and enhancing the reliability of LLM agents' decision-making processes. Experiments on three widely used tasks demonstrate that InferAct achieves up to 20% improvements on Marco-F1 against baselines in misaligned action detection. An in-depth evaluation of misalignment correction further highlights InferAct's effectiveness in improving agent alignment.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Amelia: A Large Dataset and Model for Airport Surface Movement Forecasting</title>
<link>https://arxiv.org/abs/2407.21185</link>
<guid>https://arxiv.org/abs/2407.21185</guid>
<content:encoded><![CDATA[
arXiv:2407.21185v3 Announce Type: replace 
Abstract: Demand for air travel is rising, straining existing aviation infrastructure. In the US, more than 90% of airport control towers are understaffed, falling short of FAA and union standards. This, in part, has contributed to an uptick in near-misses and safety-critical events, highlighting the need for advancements in air traffic management technologies to ensure safe and efficient operations. Data-driven predictive models for terminal airspace show potential to address these challenges; however, the lack of large-scale surface movement datasets in the public domain has hindered the development of scalable and generalizable approaches. To address this, we introduce Amelia-42, a first-of-its-kind large collection of raw airport surface movements reports streamed through the FAA's System Wide Information Management (SWIM) Program, comprising over two years of trajectory data (~9.19TB) across 42 US airports. We open-source tools to process this data into clean tabular position reports. We release Amelia42-Mini, a 15-day sample per airport, fully processed data on HuggingFace for ease of use. We also present a trajectory forecasting benchmark consisting of Amelia10-Bench, an accessible experiment family using 292 days from 10 airports, as well as Amelia-TF, a transformer-based baseline for multi-agent trajectory forecasting. All resources are available at our website: https://ameliacmu.github.io and https://huggingface.co/AmeliaCMU.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TaBSA -- A framework for training and benchmarking algorithms scheduling tasks for mobile robots working in dynamic environments</title>
<link>https://arxiv.org/abs/2408.16844</link>
<guid>https://arxiv.org/abs/2408.16844</guid>
<content:encoded><![CDATA[
arXiv:2408.16844v3 Announce Type: replace 
Abstract: This article introduces a software framework for benchmarking robot task scheduling algorithms in dynamic and uncertain service environments. The system provides standardized interfaces, configurable scenarios with movable objects, human agents, tools for automated test generation, and performance evaluation. It supports both classical and AI-based methods, enabling repeatable, comparable assessments across diverse tasks and configurations. The framework facilitates diagnosis of algorithm behavior, identification of implementation flaws, and selection or tuning of strategies for specific applications. It includes a SysML-based domain-specific language for structured scenario modeling and integrates with the ROS-based system for runtime execution. Validated on patrol, fall assistance, and pick-and-place tasks, the open-source framework is suited for researchers and integrators developing and testing scheduling algorithms under real-world-inspired conditions.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSTP: Efficient Sample Selection for Trajectory Prediction</title>
<link>https://arxiv.org/abs/2409.17385</link>
<guid>https://arxiv.org/abs/2409.17385</guid>
<content:encoded><![CDATA[
arXiv:2409.17385v3 Announce Type: replace 
Abstract: Trajectory prediction is a core task in autonomous driving. However, training advanced trajectory prediction models on existing large-scale datasets is both time-consuming and computationally expensive. More critically, these datasets are highly imbalanced in scenario density, with normal driving scenes (low-moderate traffic) overwhelmingly dominating the datasets, while high-density and safety-critical cases are underrepresented. As a result, models tend to overfit low/moderate-density scenarios and perform poorly in high-density scenarios. To address these challenges, we propose the SSTP framework, which constructs a compact yet density-balanced dataset tailored to trajectory prediction. SSTP consists of two main stages: (1)Extraction, where a baseline model is pretrained for a few epochs to obtain stable gradient estimates, and the dataset is partitioned by scenario density. (2)Selection, where gradient-based scores and a submodular objective select representative samples within each density category, while biased sampling emphasizes rare high-density interactions to avoid dominance by low-density cases. This approach significantly reduces the dataset size and mitigates scenario imbalance, without sacrificing prediction accuracy. Experiments on the Argoverse 1 and Argoverse 2 datasets with recent state-of-the-art models show that SSTP achieves comparable performance to full-dataset training using only half the data while delivering substantial improvements in high-density traffic scenes and significantly reducing training time. Robust trajectory prediction depends not only on data scale but also on balancing scene density to ensure reliable performance under complex multi agent interactions.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Dynamic Ensembling for Multiple LLM Experts</title>
<link>https://arxiv.org/abs/2412.07448</link>
<guid>https://arxiv.org/abs/2412.07448</guid>
<content:encoded><![CDATA[
arXiv:2412.07448v2 Announce Type: replace 
Abstract: LLMs have demonstrated impressive performance across various language tasks. However, the strengths of LLMs can vary due to different architectures, model sizes, areas of training data, etc. Therefore, ensemble reasoning for the strengths of different LLM experts is critical to achieving consistent and satisfactory performance on diverse inputs across a wide range of tasks. However, existing LLM ensemble methods are either computationally intensive or incapable of leveraging complementary knowledge among LLM experts for various inputs. In this paper, we propose an efficient Dynamic Ensemble Reasoning paradigm, called DER to integrate the strengths of multiple LLM experts conditioned on dynamic inputs. Specifically, we model the LLM ensemble reasoning problem as a Markov Decision Process, wherein an agent sequentially takes inputs to request knowledge from an LLM candidate and passes the output to a subsequent LLM candidate. Moreover, we devise a reward function to train a DER-Agent to dynamically select an optimal answering route given the input questions, aiming to achieve the highest performance with as few computational resources as possible. Last, to fully transfer the expert knowledge from the prior LLMs, we develop a Knowledge Transfer Prompt that enables the subsequent LLM candidates to transfer complementary knowledge effectively. Experiments demonstrate that our method uses fewer computational resources to achieve better performance compared to state-of-the-art baselines. Code and appendix are available at https://github.com/Fhujinwu/DER
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Pricing in High-Speed Railways Using Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2501.08234</link>
<guid>https://arxiv.org/abs/2501.08234</guid>
<content:encoded><![CDATA[
arXiv:2501.08234v2 Announce Type: replace 
Abstract: This paper addresses a critical challenge in the high-speed passenger railway industry: designing effective dynamic pricing strategies in the context of competing and cooperating operators. To address this, a multi-agent reinforcement learning (MARL) framework based on a non-zero-sum Markov game is proposed, incorporating random utility models to capture passenger decision making. Unlike prior studies in areas such as energy, airlines, and mobile networks, dynamic pricing for railway systems using deep reinforcement learning has received limited attention. A key contribution of this paper is a parametrisable and versatile reinforcement learning simulator designed to model a variety of railway network configurations and demand patterns while enabling realistic, microscopic modelling of user behaviour, called RailPricing-RL. This environment supports the proposed MARL framework, which models heterogeneous agents competing to maximise individual profits while fostering cooperative behaviour to synchronise connecting services. Experimental results validate the framework, demonstrating how user preferences affect MARL performance and how pricing policies influence passenger choices, utility, and overall system dynamics. This study provides a foundation for advancing dynamic pricing strategies in railway systems, aligning profitability with system-wide efficiency, and supporting future research on optimising pricing policies.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-as-Judge for Factual Summarization of Long Narratives</title>
<link>https://arxiv.org/abs/2501.09993</link>
<guid>https://arxiv.org/abs/2501.09993</guid>
<content:encoded><![CDATA[
arXiv:2501.09993v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated near-human performance in summarization tasks based on traditional metrics such as ROUGE and BERTScore. However, these metrics do not adequately capture critical aspects of summarization quality, such as factual accuracy, particularly for long narratives (>100K tokens). Recent advances, such as LLM-as-a-Judge, address the limitations of metrics based on lexical similarity but still exhibit factual inconsistencies, especially in understanding character relationships and states. In this work, we introduce NarrativeFactScore, a novel "Agent-as-a-Judge" framework for evaluating and refining summaries. By leveraging a Character Knowledge Graph (CKG) extracted from input and generated summaries, NarrativeFactScore assesses the factual consistency and provides actionable guidance for refinement, such as identifying missing or erroneous facts. We demonstrate the effectiveness of NarrativeFactScore through a detailed workflow illustration and extensive validation on widely adopted benchmarks, achieving superior performance compared to competitive methods. Our results highlight the potential of agent-driven evaluation systems to improve the factual reliability of LLM-generated summaries.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Should You Use Your Large Language Model to Explore or Exploit?</title>
<link>https://arxiv.org/abs/2502.00225</link>
<guid>https://arxiv.org/abs/2502.00225</guid>
<content:encoded><![CDATA[
arXiv:2502.00225v2 Announce Type: replace 
Abstract: We evaluate the ability of the current generation of large language models (LLMs) to help a decision-making agent facing an exploration-exploitation tradeoff. We use LLMs to explore and exploit in silos in various (contextual) bandit tasks. We find that while the current LLMs often struggle to exploit, in-context mitigations may be used to substantially improve performance for small-scale tasks. However even then, LLMs perform worse than a simple linear regression. On the other hand, we find that LLMs do help at exploring large action spaces with inherent semantics, by suggesting suitable candidates to explore.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual Alignment Maximin Optimization for Offline Model-based RL</title>
<link>https://arxiv.org/abs/2502.00850</link>
<guid>https://arxiv.org/abs/2502.00850</guid>
<content:encoded><![CDATA[
arXiv:2502.00850v3 Announce Type: replace 
Abstract: Offline reinforcement learning agents face significant deployment challenges due to the synthetic-to-real distribution mismatch. While most prior research has focused on improving the fidelity of synthetic sampling and incorporating off-policy mechanisms, the directly integrated paradigm often fails to ensure consistent policy behavior in biased models and underlying environmental dynamics, which inherently arise from discrepancies between behavior and learning policies. In this paper, we first shift the focus from model reliability to policy discrepancies while optimizing for expected returns, and then self-consistently incorporate synthetic data, deriving a novel actor-critic paradigm, Dual Alignment Maximin Optimization (DAMO). It is a unified framework to ensure both model-environment policy consistency and synthetic and offline data compatibility. The inner minimization performs dual conservative value estimation, aligning policies and trajectories to avoid out-of-distribution states and actions, while the outer maximization ensures that policy improvements remain consistent with inner value estimates. Empirical evaluations demonstrate that DAMO effectively ensures model and policy alignments, achieving competitive performance across diverse benchmark tasks.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>iVISPAR -- An Interactive Visual-Spatial Reasoning Benchmark for VLMs</title>
<link>https://arxiv.org/abs/2502.03214</link>
<guid>https://arxiv.org/abs/2502.03214</guid>
<content:encoded><![CDATA[
arXiv:2502.03214v2 Announce Type: replace 
Abstract: Vision-Language Models (VLMs) are known to struggle with spatial reasoning and visual alignment. To help overcome these limitations, we introduce iVISPAR, an interactive multimodal benchmark designed to evaluate the spatial reasoning capabilities of VLMs acting as agents. \mbox{iVISPAR} is based on a variant of the sliding tile puzzle, a classic problem that demands logical planning, spatial awareness, and multi-step reasoning. The benchmark supports visual 3D, 2D, and text-based input modalities, enabling comprehensive assessments of VLMs' planning and reasoning skills. We evaluate a broad suite of state-of-the-art open-source and closed-source VLMs, comparing their performance while also providing optimal path solutions and a human baseline to assess the task's complexity and feasibility for humans. Results indicate that while VLMs perform better on 2D tasks compared to 3D or text-based settings, they struggle with complex spatial configurations and consistently fall short of human performance, illustrating the persistent challenge of visual alignment. This underscores critical gaps in current VLM capabilities, highlighting their limitations in achieving human-level cognition. Project website: https://microcosm.ai/ivispar
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Contemporary Survey on Semantic Communications:Theory of Mind, Generative AI, and Deep Joint Source-Channel Coding</title>
<link>https://arxiv.org/abs/2502.16468</link>
<guid>https://arxiv.org/abs/2502.16468</guid>
<content:encoded><![CDATA[
arXiv:2502.16468v2 Announce Type: replace 
Abstract: Semantic Communication is becoming the next pillar in wireless communication technology due to its various capabilities. However, it still encounters various challenging obstacles that need to be solved before real-world deployment. One of which is the lack of standardization across different directions, resulting in variations in interpretation and objectives. In the survey, we provide detailed explanations of three leading directions in semantic communications, namely Theory of Mind, Generative AI, Deep Joint Source-Channel Coding. These directions have been widely studied, developed, and verified by institutes worldwide, and their effectiveness has increased along with the advancement in technology. We first introduce the concepts and background of these directions in the survey. Firstly, the survey introduces the Theory of Mind, where the communication agents interact with each other, gaining understanding from observations and slowly forming a common language. Secondly, we present generative AI models, which can create new content and offer more freedom to interpret the data beyond the limitation of semantic meaning compression of raw data before transmitting it. Thirdly, our survey reviews deep learning models to jointly optimize the source and channel coding modules. Then, we present a comprehensive survey of existing works in each direction, thereby offering readers an overview of past achievements and potential avenues for further contribution. Moreover, for each direction, the survey identifies and discusses the existing challenges that must be addressed before these approaches can be effectively deployed in real-world scenarios. These challenges encompass a range of technical, computational, and practical issues in the survey. Finally, this survey also explores emerging trends such as specifically quantum computing, in semantic communication is discussed in this survey.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voting or Consensus? Decision-Making in Multi-Agent Debate</title>
<link>https://arxiv.org/abs/2502.19130</link>
<guid>https://arxiv.org/abs/2502.19130</guid>
<content:encoded><![CDATA[
arXiv:2502.19130v4 Announce Type: replace 
Abstract: Much of the success of multi-agent debates depends on carefully choosing the right parameters. The decision-making protocol stands out as it can highly impact final model answers, depending on how decisions are reached. Systematic comparison of decision protocols is difficult because many studies alter multiple discussion parameters beyond the protocol. So far, it has been largely unknown how decision-making influences different tasks. This work systematically evaluates the impact of seven decision protocols (e.g., majority voting, unanimity consensus). We change only one variable at a time - the decision protocol - to analyze how different methods affect the collaboration between agents and measure differences in knowledge and reasoning tasks. Our results show that voting protocols improve performance by 13.2% in reasoning tasks and consensus protocols by 2.8% in knowledge tasks compared to other decision protocols. Increasing the number of agents improves performance, while more discussion rounds before voting reduce it. To improve decision-making by increasing answer diversity, we propose two new methods, All-Agents Drafting (AAD) and Collective Improvement (CI). Our methods improve task performance by up to 3.3% with AAD and up to 7.4% with CI. This work demonstrates the importance of decision-making in multi-agent debates beyond scaling.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Answer, Refuse, or Guess? Investigating Risk-Aware Decision Making in Language Models</title>
<link>https://arxiv.org/abs/2503.01332</link>
<guid>https://arxiv.org/abs/2503.01332</guid>
<content:encoded><![CDATA[
arXiv:2503.01332v2 Announce Type: replace 
Abstract: Language models (LMs) are increasingly used to build agents that can act autonomously to achieve goals. During this automatic process, agents need to take a series of actions, some of which might lead to severe consequences if incorrect actions are taken. Therefore, such agents must sometimes defer-refusing to act when their confidence is insufficient-to avoid the potential cost of incorrect actions. Because the severity of consequences varies across applications, the tendency to defer should also vary: in low-risk settings agents should answer more freely, while in high-risk settings their decisions should be more conservative. We study this "answer-or-defer" problem with an evaluation framework that systematically varies human-specified risk structures-rewards and penalties for correct answers, incorrect answers, and refusals $(r_{\mathrm{cor}},r_{\mathrm{inc}}, r_{\mathrm{ref}})$-while keeping tasks fixed. This design evaluates LMs' risk-aware decision policies by measuring their ability to maximize expected reward. Across multiple datasets and models, we identify flaws in their decision policies: LMs tend to over-answer in high-risk settings and over-defer in low-risk settings. After analyzing the potential cause of such flaws, we find that a simple skill-decomposition method, which isolates the independent skills required for answer-or-defer decision making, can consistently improve LMs' decision policies. Our results highlight the current limitations of LMs in risk-conditioned decision making and provide practical guidance for deploying more reliable LM-based agents across applications of varying risk levels.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Metric Distance to Discrete Autoregressive Language Models</title>
<link>https://arxiv.org/abs/2503.02379</link>
<guid>https://arxiv.org/abs/2503.02379</guid>
<content:encoded><![CDATA[
arXiv:2503.02379v3 Announce Type: replace 
Abstract: As large language models expand beyond natural language to domains such as mathematics, multimodal understanding, and embodied agents, tokens increasingly reflect metric relationships rather than purely linguistic meaning. We introduce DIST2Loss, a distance-aware framework designed to train autoregressive discrete models by leveraging predefined distance relationships among output tokens. At its core, DIST2Loss transforms continuous exponential family distributions derived from inherent distance metrics into discrete, categorical optimization targets compatible with the models' architectures. This approach enables the models to learn and preserve meaningful distance relationships during token generation while maintaining compatibility with existing architectures. Empirical evaluations show consistent performance gains in diverse multimodal applications, including visual grounding, robotic manipulation, generative reward modeling, and image generation using vector-quantized features. These improvements are most notable in low-data regimes, demonstrating DIST2Loss's strength under resource constraints.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching AI to Handle Exceptions: Supervised Fine-Tuning with Human-Aligned Judgment</title>
<link>https://arxiv.org/abs/2503.02976</link>
<guid>https://arxiv.org/abs/2503.02976</guid>
<content:encoded><![CDATA[
arXiv:2503.02976v2 Announce Type: replace 
Abstract: Large language models (LLMs), initially developed for generative AI, are now evolving into agentic AI systems, which make decisions in complex, real-world contexts. Unfortunately, while their generative capabilities are well-documented, their decision-making processes remain poorly understood. This is particularly evident when testing targeted decision-making: for instance, how models handle exceptions, a critical and challenging aspect of decision-making made relevant by the inherent incompleteness of contracts. Here we demonstrate that LLMs, even ones that excel at reasoning, deviate significantly from human judgments because they adhere strictly to policies, even when such adherence is impractical, suboptimal, or even counterproductive. We then evaluate three approaches to tuning AI agents to handle exceptions: ethical framework prompting, chain-of-thought reasoning, and supervised fine-tuning. We find that while ethical framework prompting fails and chain-of-thought prompting provides only slight improvements, supervised fine-tuning - specifically with human explanations - yields markedly better results. Surprisingly, in our experiments, supervised fine-tuning even enabled models to generalize human-like decision-making to novel scenarios, demonstrating transfer learning of human-aligned decision-making across contexts. Furthermore, fine-tuning with explanations, not just labels, was critical for alignment, suggesting that aligning LLMs with human judgment requires explicit training on how decisions are made, not just which decisions are made. These findings highlight the need to address LLMs' shortcomings in handling exceptions in order to guide the development of agentic AI toward models that can effectively align with human judgment and simultaneously adapt to novel contexts.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Rapid Shared Human-AI Mental Model Alignment via the After-Action Review</title>
<link>https://arxiv.org/abs/2503.19607</link>
<guid>https://arxiv.org/abs/2503.19607</guid>
<content:encoded><![CDATA[
arXiv:2503.19607v2 Announce Type: replace 
Abstract: In this work, we present two novel contributions toward improving research in human-machine teaming (HMT): 1) a Minecraft testbed to accelerate testing and deployment of collaborative AI agents and 2) a tool to allow users to revisit and analyze behaviors within an HMT episode to facilitate shared mental model development. Our browser-based Minecraft testbed allows for rapid testing of collaborative agents in a continuous-space, real-time, partially-observable environment with real humans without cumbersome setup typical to human-AI interaction user studies. As Minecraft has an extensive player base and a rich ecosystem of pre-built AI agents, we hope this contribution can help to facilitate research quickly in the design of new collaborative agents and in understanding different human factors within HMT. Our mental model alignment tool facilitates user-led post-mission analysis by including video displays of first-person perspectives of the team members (i.e., the human and AI) that can be replayed, and a chat interface that leverages GPT-4 to provide answers to various queries regarding the AI's experiences and model details.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAMO: Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with Multi-Modality Observation Data in Cloud-Native Systems</title>
<link>https://arxiv.org/abs/2504.20462</link>
<guid>https://arxiv.org/abs/2504.20462</guid>
<content:encoded><![CDATA[
arXiv:2504.20462v4 Announce Type: replace 
Abstract: Implementing large language models (LLMs)-driven root cause analysis (RCA) in cloud-native systems has become a key topic of modern software operations and maintenance. However, existing LLM-based approaches face three key challenges: multi-modality input constraint, context window limitation, and dynamic dependence graph. To address these issues, we propose a tool-assisted LLM agent with multi-modality observation data for fine-grained RCA, namely TAMO, including multimodality alignment tool, root cause localization tool, and fault types classification tool. In detail, TAMO unifies multi-modal observation data into time-aligned representations for cross-modal feature consistency. Based on the unified representations, TAMO then invokes its specialized root cause localization tool and fault types classification tool for further identifying root cause and fault type underlying system context. This approach overcomes the limitations of LLMs in processing real-time raw observational data and dynamic service dependencies, guiding the model to generate repair strategies that align with system context through structured prompt design. Experiments on two benchmark datasets demonstrate that TAMO outperforms state-of-the-art (SOTA) approaches with comparable performance.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges</title>
<link>https://arxiv.org/abs/2505.10468</link>
<guid>https://arxiv.org/abs/2505.10468</guid>
<content:encoded><![CDATA[
arXiv:2505.10468v5 Announce Type: replace 
Abstract: This review critically distinguishes between AI Agents and Agentic AI, offering a structured, conceptual taxonomy, application mapping, and analysis of opportunities and challenges to clarify their divergent design philosophies and capabilities. We begin by outlining the search strategy and foundational definitions, characterizing AI Agents as modular systems driven and enabled by LLMs and LIMs for task-specific automation. Generative AI is positioned as a precursor providing the foundation, with AI agents advancing through tool integration, prompt engineering, and reasoning enhancements. We then characterize Agentic AI systems, which, in contrast to AI Agents, represent a paradigm shift marked by multi-agent collaboration, dynamic task decomposition, persistent memory, and coordinated autonomy. Through a chronological evaluation of architectural evolution, operational mechanisms, interaction styles, and autonomy levels, we present a comparative analysis across both AI agents and agentic AI paradigms. Application domains enabled by AI Agents such as customer support, scheduling, and data summarization are then contrasted with Agentic AI deployments in research automation, robotic coordination, and medical decision support. We further examine unique challenges in each paradigm including hallucination, brittleness, emergent behavior, and coordination failure, and propose targeted solutions such as ReAct loops, retrieval-augmented generation (RAG), automation coordination layers, and causal modeling. This work aims to provide a roadmap for developing robust, scalable, and explainable AI-driven systems.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Agent Distillation for Large Language Model</title>
<link>https://arxiv.org/abs/2505.13820</link>
<guid>https://arxiv.org/abs/2505.13820</guid>
<content:encoded><![CDATA[
arXiv:2505.13820v2 Announce Type: replace 
Abstract: Large language models (LLMs) exhibit strong capabilities as decision-making agents by interleaving reasoning and actions, as seen in ReAct-style frameworks. Yet, their practical deployment is constrained by high inference costs and large model sizes. We propose Structured Agent Distillation, a framework that compresses large LLM-based agents into smaller student models while preserving both reasoning fidelity and action consistency. Unlike standard token-level distillation, our method segments trajectories into {[REASON]} and {[ACT]} spans, applying segment-specific losses to align each component with the teacher's behavior. This structure-aware supervision enables compact agents to better replicate the teacher's decision process. Experiments on ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently outperforms token-level and imitation learning baselines, achieving significant compression with minimal performance drop. Scaling and ablation results further highlight the importance of span-level alignment for efficient and deployable agents.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning</title>
<link>https://arxiv.org/abs/2505.15734</link>
<guid>https://arxiv.org/abs/2505.15734</guid>
<content:encoded><![CDATA[
arXiv:2505.15734v2 Announce Type: replace 
Abstract: Large language models (LLMs) have improved significantly in their reasoning through extensive training on massive datasets. However, relying solely on additional data for improvement is becoming increasingly impractical, highlighting the need for models to autonomously enhance their reasoning without external supervision. In this paper, we propose Debate, Train, Evolve (DTE), a novel ground truth-free training framework that uses multi-agent debate traces to evolve a single language model. We also introduce a new prompting strategy Reflect-Critique-Refine, to improve debate quality by explicitly instructing agents to critique and refine their reasoning. Extensive evaluations on seven reasoning benchmarks with six open-weight models show that our DTE framework achieve substantial improvements, with an average accuracy gain of 8.92% on the challenging GSM-PLUS dataset. Furthermore, we observe strong cross-domain generalization, with an average accuracy gain of 5.8% on all other benchmarks, suggesting that our method captures general reasoning capabilities. Our framework code and trained models are publicly available at https://github.com/ctrl-gaurav/Debate-Train-Evolve
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Agents for Interactive Exploration of Historical Cadastre Data: Framework and Application to Venice</title>
<link>https://arxiv.org/abs/2505.17148</link>
<guid>https://arxiv.org/abs/2505.17148</guid>
<content:encoded><![CDATA[
arXiv:2505.17148v2 Announce Type: replace 
Abstract: Cadastral data reveal key information about the historical organization of cities but are often non-standardized due to diverse formats and human annotations, complicating large-scale analysis. We explore as a case study Venice's urban history during the critical period from 1740 to 1808, capturing the transition following the fall of the ancient Republic and the Ancien R\'egime. This era's complex cadastral data, marked by its volume and lack of uniform structure, presents unique challenges that our approach adeptly navigates, enabling us to generate spatial queries that bridge past and present urban landscapes. We present a text-to-programs framework that leverages Large Language Models (\llms) to process natural language queries as executable code for analyzing historical cadastral records. Our methodology implements two complementary techniques: a SQL agent for handling structured queries about specific cadastral information, and a coding agent for complex analytical operations requiring custom data manipulation. We propose a taxonomy that classifies historical research questions based on their complexity and analytical requirements, mapping them to the most appropriate technical approach. This framework is supported by an investigation into the execution consistency of the system, alongside a qualitative analysis of the answers it produces. By ensuring interpretability and minimizing hallucination through verifiable program outputs, we demonstrate the system's effectiveness in reconstructing past population information, property features, and spatiotemporal comparisons in Venice.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VS-Bench: Evaluating VLMs for Strategic Abilities in Multi-Agent Environments</title>
<link>https://arxiv.org/abs/2506.02387</link>
<guid>https://arxiv.org/abs/2506.02387</guid>
<content:encoded><![CDATA[
arXiv:2506.02387v2 Announce Type: replace 
Abstract: Recent advancements in Vision Language Models (VLMs) have expanded their capabilities to interactive agent tasks, yet existing benchmarks remain limited to single-agent or text-only environments. In contrast, real-world scenarios often involve multiple agents interacting within rich visual and textual contexts, posing challenges with both multimodal observations and strategic interactions. To bridge this gap, we introduce Visual Strategic Bench (VS-Bench), a multimodal benchmark that evaluates VLMs for strategic abilities in multi-agent environments. VS-Bench comprises ten vision-grounded environments that cover cooperative, competitive, and mixed-motive interactions. The performance of VLM agents is evaluated across three dimensions: perception measured by element recognition accuracy; strategic reasoning measured by next-action prediction accuracy; and decision-making measured by normalized episode return. Extensive experiments on fifteen leading VLMs show that, although current models exhibit strong perception abilities, there remains a significant gap to optimal performance in reasoning and decision-making, with the best-performing model attaining 46.6% prediction accuracy and 31.4% normalized return. We further analyze the key factors influencing performance, conduct human experiments, and examine failure modes to provide a deeper understanding of VLMs' strategic abilities. By standardizing the evaluation and highlighting the limitations of existing models, we envision VS-Bench as a foundation for future research on strategic multimodal agents. Code and data are available at https://vs-bench.github.io.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions</title>
<link>https://arxiv.org/abs/2506.23046</link>
<guid>https://arxiv.org/abs/2506.23046</guid>
<content:encoded><![CDATA[
arXiv:2506.23046v2 Announce Type: replace 
Abstract: Humans continuously infer the states, goals, and behaviors of others by perceiving their surroundings in dynamic, real-world social interactions. However, most Theory of Mind (ToM) benchmarks only evaluate static, text-based scenarios, which have a significant gap compared to real interactions. We propose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in embodied multi-agent complex social interactions. This benchmark is based on rich multimodal interaction data generated by the interaction environment SoMi, covering diverse crafting goals and social relationships. Our framework supports multi-level evaluation: (1) first-person evaluation provides multimodal (visual, dialogue, action, etc.) input from a first-person perspective during a task for real-time state inference, (2) third-person evaluation provides complete third-person perspective video and text records after a task for goal and behavior inference. This evaluation method allows for a more comprehensive examination of a model's ToM capabilities from both the subjective immediate experience and the objective global observation. We constructed a challenging dataset containing 35 third-person perspective videos, 363 first-person perspective images, and 1225 expert-annotated multiple-choice questions (three options). On this dataset, we systematically evaluated the performance of human subjects and several state-of-the-art large vision-language models (LVLMs). The results show that LVLMs perform significantly worse than humans on SoMi-ToM: the average accuracy gap between humans and models is 40.1% in first-person evaluation and 26.4% in third-person evaluation. This indicates that future LVLMs need to further improve their ToM capabilities in embodied, complex social interactions.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Code Generation with LLM-based Agents</title>
<link>https://arxiv.org/abs/2508.00083</link>
<guid>https://arxiv.org/abs/2508.00083</guid>
<content:encoded><![CDATA[
arXiv:2508.00083v2 Announce Type: replace 
Abstract: Code generation agents powered by large language models (LLMs) are revolutionizing the software development paradigm. Distinct from previous code generation techniques, code generation agents are characterized by three core features. 1) Autonomy: the ability to independently manage the entire workflow, from task decomposition to coding and debugging. 2) Expanded task scope: capabilities that extend beyond generating code snippets to encompass the full software development lifecycle (SDLC). 3) Enhancement of engineering practicality: a shift in research emphasis from algorithmic innovation toward practical engineering challenges, such as system reliability, process management, and tool integration. This domain has recently witnessed rapid development and an explosion in research, demonstrating significant application potential. This paper presents a systematic survey of the field of LLM-based code generation agents. We trace the technology's developmental trajectory from its inception and systematically categorize its core techniques, including both single-agent and multi-agent architectures. Furthermore, this survey details the applications of LLM-based agents across the full SDLC, summarizes mainstream evaluation benchmarks and metrics, and catalogs representative tools. Finally, by analyzing the primary challenges, we identify and propose several foundational, long-term research directions for the future work of the field.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>K-Dense Analyst: Towards Fully Automated Scientific Analysis</title>
<link>https://arxiv.org/abs/2508.07043</link>
<guid>https://arxiv.org/abs/2508.07043</guid>
<content:encoded><![CDATA[
arXiv:2508.07043v2 Announce Type: replace 
Abstract: The complexity of modern bioinformatics analysis has created a critical gap between data generation and developing scientific insights. While large language models (LLMs) have shown promise in scientific reasoning, they remain fundamentally limited when dealing with real-world analytical workflows that demand iterative computation, tool integration and rigorous validation. We introduce K-Dense Analyst, a hierarchical multi-agent system that achieves autonomous bioinformatics analysis through a dual-loop architecture. K-Dense Analyst, part of the broader K-Dense platform, couples planning with validated execution using specialized agents to decompose complex objectives into executable, verifiable tasks within secure computational environments. On BixBench, a comprehensive benchmark for open-ended biological analysis, K-Dense Analyst achieves 29.2% accuracy, surpassing the best-performing language model (GPT-5) by 6.3 percentage points, representing nearly 27% improvement over what is widely considered the most powerful LLM available. Remarkably, K-Dense Analyst achieves this performance using Gemini 2.5 Pro, which attains only 18.3% accuracy when used directly, demonstrating that our architectural innovations unlock capabilities far beyond the underlying model's baseline performance. Our insights demonstrate that autonomous scientific reasoning requires more than enhanced language models, it demands purpose-built systems that can bridge the gap between high-level scientific objectives and low-level computational execution. These results represent a significant advance toward fully autonomous computational biologists capable of accelerating discovery across the life sciences.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KompeteAI: Accelerated Autonomous Multi-Agent System for End-to-End Pipeline Generation for Machine Learning Problems</title>
<link>https://arxiv.org/abs/2508.10177</link>
<guid>https://arxiv.org/abs/2508.10177</guid>
<content:encoded><![CDATA[
arXiv:2508.10177v2 Announce Type: replace 
Abstract: Recent Large Language Model (LLM)-based AutoML systems demonstrate impressive capabilities but face significant limitations such as constrained exploration strategies and a severe execution bottleneck. Exploration is hindered by one-shot methods lacking diversity and Monte Carlo Tree Search (MCTS) approaches that fail to recombine strong partial solutions. The execution bottleneck arises from lengthy code validation cycles that stifle iterative refinement. To overcome these challenges, we introduce KompeteAI, a novel AutoML framework with dynamic solution space exploration. Unlike previous MCTS methods that treat ideas in isolation, KompeteAI introduces a merging stage that composes top candidates. We further expand the hypothesis space by integrating Retrieval-Augmented Generation (RAG), sourcing ideas from Kaggle notebooks and arXiv papers to incorporate real-world strategies. KompeteAI also addresses the execution bottleneck via a predictive scoring model and an accelerated debugging method, assessing solution potential using early stage metrics to avoid costly full-code execution. This approach accelerates pipeline evaluation 6.9 times. KompeteAI outperforms leading methods (e.g., RD-agent, AIDE, and Ml-Master) by an average of 3\% on the primary AutoML benchmark, MLE-Bench. Additionally, we propose Kompete-bench to address limitations in MLE-Bench, where KompeteAI also achieves state-of-the-art results
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphCogent: Mitigating LLMs' Working Memory Constraints via Multi-Agent Collaboration in Complex Graph Understanding</title>
<link>https://arxiv.org/abs/2508.12379</link>
<guid>https://arxiv.org/abs/2508.12379</guid>
<content:encoded><![CDATA[
arXiv:2508.12379v2 Announce Type: replace 
Abstract: Large language models (LLMs) show promising performance on small-scale graph reasoning tasks but fail when handling real-world graphs with complex queries. This phenomenon arises from LLMs' working memory constraints, which result in their inability to retain long-range graph topology over extended contexts while sustaining coherent multi-step reasoning. However, real-world graphs are often structurally complex, such as Web, Transportation, Social, and Citation networks. To address these limitations, we propose GraphCogent, a collaborative agent framework inspired by human Working Memory Model that decomposes graph reasoning into specialized cognitive processes: sense, buffer, and execute. The framework consists of three modules: Sensory Module standardizes diverse graph text representations via subgraph sampling, Buffer Module integrates and indexes graph data across multiple formats, and Execution Module combines tool calling and tool creation for efficient reasoning. We also introduce Graph4real, a comprehensive benchmark that contains four domains of real-world graphs (Web, Transportation, Social, and Citation) to evaluate LLMs' graph reasoning capabilities. Our Graph4real covers 21 different graph reasoning tasks, categorized into three types (Structural Querying, Algorithmic Reasoning, and Predictive Modeling tasks), with graph scales up to 10 times larger than existing benchmarks. Experiments show that Llama3.1-8B based GraphCogent achieves a 50% improvement over massive-scale LLMs like DeepSeek-R1 (671B). Compared to state-of-the-art agent-based baseline, our framework outperforms by 20% in accuracy while reducing token usage by 80% for in-toolset tasks and 30% for out-toolset tasks. Code will be available after review.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents</title>
<link>https://arxiv.org/abs/2509.03581</link>
<guid>https://arxiv.org/abs/2509.03581</guid>
<content:encoded><![CDATA[
arXiv:2509.03581v2 Announce Type: replace 
Abstract: Training large language models (LLMs) to reason via reinforcement learning (RL) significantly improves their problem-solving capabilities. In agentic settings, existing methods like ReAct prompt LLMs to explicitly plan before every action; however, we demonstrate that always planning is computationally expensive and degrades performance on long-horizon tasks, while never planning further limits performance. To address this, we introduce a conceptual framework formalizing dynamic planning for LLM agents, enabling them to flexibly decide when to allocate test-time compute for planning. We propose a simple two-stage training pipeline: (1) supervised fine-tuning on diverse synthetic data to prime models for dynamic planning, and (2) RL to refine this capability in long-horizon environments. Experiments on the Crafter environment show that dynamic planning agents trained with this approach are more sample-efficient and consistently achieve more complex objectives. Additionally, we demonstrate that these agents can be effectively steered by human-written plans, surpassing their independent capabilities. To our knowledge, this work is the first to explore training LLM agents for dynamic test-time compute allocation in sequential decision-making tasks, paving the way for more efficient, adaptive, and controllable agentic systems.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Horizon Visual Imitation Learning via Plan and Code Reflection</title>
<link>https://arxiv.org/abs/2509.05368</link>
<guid>https://arxiv.org/abs/2509.05368</guid>
<content:encoded><![CDATA[
arXiv:2509.05368v2 Announce Type: replace 
Abstract: Learning from long-horizon demonstrations with complex action sequences presents significant challenges for visual imitation learning, particularly in understanding temporal relationships of actions and spatial relationships between objects. In this paper, we propose a new agent framework that incorporates two dedicated reflection modules to enhance both plan and code generation. The plan generation module produces an initial action sequence, which is then verified by the plan reflection module to ensure temporal coherence and spatial alignment with the demonstration video. The code generation module translates the plan into executable code, while the code reflection module verifies and refines the generated code to ensure correctness and consistency with the generated plan. These two reflection modules jointly enable the agent to detect and correct errors in both the plan generation and code generation, improving performance in tasks with intricate temporal and spatial dependencies. To support systematic evaluation, we introduce LongVILBench, a benchmark comprising 300 human demonstrations with action sequences of up to 18 steps. LongVILBench emphasizes temporal and spatial complexity across multiple task types. Experimental results demonstrate that existing methods perform poorly on this benchmark, whereas our new framework establishes a strong baseline for long-horizon visual imitation learning.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes</title>
<link>https://arxiv.org/abs/2509.06266</link>
<guid>https://arxiv.org/abs/2509.06266</guid>
<content:encoded><![CDATA[
arXiv:2509.06266v2 Announce Type: replace 
Abstract: Understanding 3D spatial relationships remains a major limitation of current Vision-Language Models (VLMs). Prior work has addressed this issue by creating spatial question-answering (QA) datasets based on single images or indoor videos. However, real-world embodied AI agents such as robots and self-driving cars typically rely on ego-centric, multi-view observations. To this end, we introduce Ego3D-Bench, a new benchmark designed to evaluate the spatial reasoning abilities of VLMs using ego-centric, multi-view outdoor data. Ego3D-Bench comprises over 8,600 QA pairs, created with significant involvement from human annotators to ensure quality and diversity. We benchmark 16 SOTA VLMs, including GPT-4o, Gemini1.5-Pro, InternVL3, and Qwen2.5-VL. Our results reveal a notable performance gap between human level scores and VLM performance, highlighting that current VLMs still fall short of human level spatial understanding. To bridge this gap, we propose Ego3D-VLM, a post-training framework that enhances 3D spatial reasoning of VLMs. Ego3D-VLM generates cognitive map based on estimated global 3D coordinates, resulting in 12% average improvement on multi-choice QA and 56% average improvement on absolute distance estimation. Ego3D-VLM is modular and can be integrated with any existing VLM. Together, Ego3D-Bench and Ego3D-VLM offer valuable tools for advancing toward human level spatial understanding in real-world, multi-view environments.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robot Conga: A Leader-Follower Walking Approach to Sequential Path Following in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2509.16482</link>
<guid>https://arxiv.org/abs/2509.16482</guid>
<content:encoded><![CDATA[
arXiv:2509.16482v2 Announce Type: replace 
Abstract: Coordinated path following in multi-agent systems is a key challenge in robotics, with applications in automated logistics, surveillance, and collaborative exploration. Traditional formation control techniques often rely on time-parameterized trajectories and path integrals, which can result in synchronization issues and rigid behavior. In this work, we address the problem of sequential path following, where agents maintain fixed spatial separation along a common trajectory, guided by a leader under centralized control. We introduce Robot Conga, a leader-follower control strategy that updates each agent's desired state based on the leader's spatial displacement rather than time, assuming access to a global position reference, an assumption valid in indoor environments equipped with motion capture, vision-based tracking, or UWB localization systems. The algorithm was validated in simulation using both TurtleBot3 and quadruped (Laikago) robots. Results demonstrate accurate trajectory tracking, stable inter-agent spacing, and fast convergence, with all agents aligning within 250 time steps (approx. 0.25 seconds) in the quadruped case, and almost instantaneously in the TurtleBot3 implementation.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risk Profiling and Modulation for LLMs</title>
<link>https://arxiv.org/abs/2509.23058</link>
<guid>https://arxiv.org/abs/2509.23058</guid>
<content:encoded><![CDATA[
arXiv:2509.23058v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly used for decision-making tasks under uncertainty; however, their risk profiles and how they are influenced by prompting and alignment methods remain underexplored. Existing studies have primarily examined personality prompting or multi-agent interactions, leaving open the question of how post-training influences the risk behavior of LLMs. In this work, we propose a new pipeline for eliciting, steering, and modulating LLMs' risk profiles, drawing on tools from behavioral economics and finance. Using utility-theoretic models, we compare pre-trained, instruction-tuned, and RLHF-aligned LLMs, and find that while instruction-tuned models exhibit behaviors consistent with some standard utility formulations, pre-trained and RLHF-aligned models deviate more from any utility models fitted. We further evaluate modulation strategies, including prompt engineering, in-context learning, and post-training, and show that post-training provides the most stable and effective modulation of risk preference. Our findings provide insights into the risk profiles of different classes and stages of LLMs and demonstrate how post-training modulates these profiles, laying the groundwork for future research on behavioral alignment and risk-aware LLM design.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SysMoBench: Evaluating AI on Formally Modeling Complex Real-World Systems</title>
<link>https://arxiv.org/abs/2509.23130</link>
<guid>https://arxiv.org/abs/2509.23130</guid>
<content:encoded><![CDATA[
arXiv:2509.23130v2 Announce Type: replace 
Abstract: Formal models are essential to specifying large, complex computer systems and verifying their correctness, but are notoriously expensive to write and maintain. Recent advances in generative AI show promise in generating certain forms of specifications. However, existing work mostly targets small code, not complete systems. It is unclear whether AI can deal with realistic system artifacts, as this requires abstracting their complex behavioral properties into formal models. We present SysMoBench, a benchmark that evaluates AI's ability to formally model large, complex systems. We focus on concurrent and distributed systems, which are keystones of today's critical computing infrastructures, encompassing operating systems and cloud infrastructure. We use TLA+, the de facto specification language for concurrent and distributed systems, though the benchmark can be extended to other specification languages. We address the primary challenge of evaluating AI-generated models by automating metrics like syntactic and runtime correctness, conformance to system code, and invariant correctness. SysMoBench currently includes nine diverse system artifacts: the Raft implementation of Etcd and Redis, the Spinlock and Mutex in Asterinas OS, etc.; more artifacts are being actively added. SysMoBench enables us to understand the capabilities and limitations of today's LLMs and agents, putting tools in this area on a firm footing and opening up promising new research directions.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic DDQN-Based Scheduling for Licensed and Unlicensed Band Allocation in Sidelink Networks</title>
<link>https://arxiv.org/abs/2509.06775</link>
<guid>https://arxiv.org/abs/2509.06775</guid>
<content:encoded><![CDATA[
arXiv:2509.06775v3 Announce Type: cross 
Abstract: In this paper, we present an agentic double deep Q-network (DDQN) scheduler for licensed/unlicensed band allocation in New Radio (NR) sidelink (SL) networks. Beyond conventional reward-seeking reinforcement learning (RL), the agent perceives and reasons over a multi-dimensional context that jointly captures queueing delay, link quality, coexistence intensity, and switching stability. A capacity-aware, quality of service (QoS)-constrained reward aligns the agent with goal-oriented scheduling rather than static thresholding. Under constrained bandwidth, the proposed design reduces blocking by up to 87.5% versus threshold policies while preserving throughput, highlighting the value of context-driven decisions in coexistence-limited NR SL networks. The proposed scheduler is an embodied agent (E-agent) tailored for task-specific, resource-efficient operation at the network edge.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code2MCP: Transforming Code Repositories into MCP Services</title>
<link>https://arxiv.org/abs/2509.05941</link>
<guid>https://arxiv.org/abs/2509.05941</guid>
<content:encoded><![CDATA[
arXiv:2509.05941v2 Announce Type: replace 
Abstract: The Model Context Protocol (MCP) aims to create a standard for how Large Language Models use tools. However, most current research focuses on selecting tools from an existing pool. A more fundamental, yet largely overlooked, problem is how to populate this pool by converting the vast number of existing software projects into MCP-compatible services. To bridge this gap, we introduce Code2MCP, an agent-based framework that automatically transforms a GitHub repository into a functional MCP service with minimal human intervention. Code2MCP employs a multi-agent workflow for code analysis, environment setup, tool function design, and service generation, enhanced by a self-correcting loop to ensure reliability. We demonstrate that Code2MCP successfully transforms open-source computing libraries in scientific fields such as bioinformatics, mathematics, and fluid dynamics that are not available in existing MCP servers. By providing a novel automated pathway to unlock GitHub, the world's largest code repository, for the MCP ecosystem, Code2MCP serves as a catalyst to significantly accelerate the protocol's adoption and practical application. The code is public at https://github.com/DEFENSE-SEU/Code2MCP.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Same Wavelength? Evaluating Pragmatic Reasoning in Language Models across Broad Concepts</title>
<link>https://arxiv.org/abs/2509.06952</link>
<guid>https://arxiv.org/abs/2509.06952</guid>
<content:encoded><![CDATA[
arXiv:2509.06952v2 Announce Type: replace 
Abstract: Language use is shaped by pragmatics -- i.e., reasoning about communicative goals and norms in context. As language models (LMs) are increasingly used as conversational agents, it becomes ever more important to understand their pragmatic reasoning abilities. We propose an evaluation framework derived from Wavelength, a popular communication game where a speaker and a listener communicate about a broad range of concepts in a granular manner. We study a range of LMs on both language comprehension and language production using direct and Chain-of-Thought (CoT) prompting, and further explore a Rational Speech Act (RSA) approach to incorporating Bayesian pragmatic reasoning into LM inference. We find that state-of-the-art LMs, but not smaller ones, achieve strong performance on language comprehension, obtaining similar-to-human accuracy and exhibiting high correlations with human judgments even without CoT prompting or RSA. On language production, CoT can outperform direct prompting, and using RSA provides significant improvements over both approaches. Our study helps identify the strengths and limitations in LMs' pragmatic reasoning abilities and demonstrates the potential for improving them with RSA, opening up future avenues for understanding conceptual representation, language understanding, and social reasoning in LMs and humans.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Focusing on What Matters: Object-Agent-centric Tokenization for Vision Language Action models</title>
<link>https://arxiv.org/abs/2509.23655</link>
<guid>https://arxiv.org/abs/2509.23655</guid>
<content:encoded><![CDATA[
arXiv:2509.23655v1 Announce Type: new 
Abstract: Vision-Language-Action (VLA) models offer a pivotal approach to learning robotic manipulation at scale by repurposing large pre-trained Vision-Language-Models (VLM) to output robotic actions. However, adapting VLMs for robotic domains comes with an unnecessarily high computational cost, which we attribute to the tokenization scheme of visual inputs. In this work, we aim to enable efficient VLA training by proposing Oat-VLA, an Object-Agent-centric Tokenization for VLAs. Building on the insights of object-centric representation learning, our method introduces an inductive bias towards scene objects and the agent's own visual information. As a result, we find that Oat-VLA can drastically reduce the number of visual tokens to just a few tokens without sacrificing performance. We reveal that Oat-VLA converges at least twice as fast as OpenVLA on the LIBERO suite, as well as outperform OpenVLA in diverse real-world pick and place tasks.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A First Look at Privacy Risks of Android Task-executable Voice Assistant Applications</title>
<link>https://arxiv.org/abs/2509.23680</link>
<guid>https://arxiv.org/abs/2509.23680</guid>
<content:encoded><![CDATA[
arXiv:2509.23680v1 Announce Type: new 
Abstract: With the development of foundation AI technologies, task-executable voice assistants (VAs) have become more popular, enhancing user convenience and expanding device functionality. Android task-executable VAs are applications that are capable of understanding complex tasks and performing corresponding operations. Given their prevalence and great autonomy, there is no existing work examine the privacy risks within the voice assistants from the task-execution pattern in a holistic manner. To fill this research gap, this paper presents a user-centric comprehensive empirical study on privacy risks in Android task-executable VA applications. We collect ten mainstream VAs as our research target and analyze their operational characteristics. We then cross-check their privacy declarations across six sources, including privacy labels, policies, and manifest files, and our findings reveal widespread inconsistencies. Moreover, we uncover three significant privacy threat models: (1) privacy misdisclosure in mega apps, where integrated mini apps such as Alexa skills are inadequately represented; (2) privilege escalation via inter-application interactions, which exploit Android's communication mechanisms to bypass user consent; and (3) abuse of Google system applications, enabling apps to evade the declaration of dangerous permissions. Our study contributes actionable recommendations for practitioners and underscores broader relevance of these privacy risks to emerging autonomous AI agents.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hedonic Neurons: A Mechanistic Mapping of Latent Coalitions in Transformer MLPs</title>
<link>https://arxiv.org/abs/2509.23684</link>
<guid>https://arxiv.org/abs/2509.23684</guid>
<content:encoded><![CDATA[
arXiv:2509.23684v1 Announce Type: new 
Abstract: Fine-tuned Large Language Models (LLMs) encode rich task-specific features, but the form of these representations, especially within MLP layers, remains unclear. Empirical inspection of LoRA updates shows that new features concentrate in mid-layer MLPs, yet the scale of these layers obscures meaningful structure. Prior probing suggests that statistical priors may strengthen, split, or vanish across depth, motivating the need to study how neurons work together rather than in isolation.
  We introduce a mechanistic interpretability framework based on coalitional game theory, where neurons mimic agents in a hedonic game whose preferences capture their synergistic contributions to layer-local computations. Using top-responsive utilities and the PAC-Top-Cover algorithm, we extract stable coalitions of neurons: groups whose joint ablation has non-additive effects. We then track their transitions across layers as persistence, splitting, merging, or disappearance.
  Applied to LLaMA, Mistral, and Pythia rerankers fine-tuned on scalar IR tasks, our method finds coalitions with consistently higher synergy than clustering baselines. By revealing how neurons cooperate to encode features, hedonic coalitions uncover higher-order structure beyond disentanglement and yield computational units that are functionally important, interpretable, and predictive across domains.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HomeSafeBench: A Benchmark for Embodied Vision-Language Models in Free-Exploration Home Safety Inspection</title>
<link>https://arxiv.org/abs/2509.23690</link>
<guid>https://arxiv.org/abs/2509.23690</guid>
<content:encoded><![CDATA[
arXiv:2509.23690v1 Announce Type: new 
Abstract: Embodied agents can identify and report safety hazards in the home environments. Accurately evaluating their capabilities in home safety inspection tasks is curcial, but existing benchmarks suffer from two key limitations. First, they oversimplify safety inspection tasks by using textual descriptions of the environment instead of direct visual information, which hinders the accurate evaluation of embodied agents based on Vision-Language Models (VLMs). Second, they use a single, static viewpoint for environmental observation, which restricts the agents' free exploration and cause the omission of certain safety hazards, especially those that are occluded from a fixed viewpoint. To alleviate these issues, we propose HomeSafeBench, a benchmark with 12,900 data points covering five common home safety hazards: fire, electric shock, falling object, trips, and child safety. HomeSafeBench provides dynamic first-person perspective images from simulated home environments, enabling the evaluation of VLM capabilities for home safety inspection. By allowing the embodied agents to freely explore the room, HomeSafeBench provides multiple dynamic perspectives in complex environments for a more thorough inspection. Our comprehensive evaluation of mainstream VLMs on HomeSafeBench reveals that even the best-performing model achieves an F1-score of only 10.23%, demonstrating significant limitations in current VLMs. The models particularly struggle with identifying safety hazards and selecting effective exploration strategies. We hope HomeSafeBench will provide valuable reference and support for future research related to home security inspections. Our dataset and code will be publicly available soon.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents</title>
<link>https://arxiv.org/abs/2509.23694</link>
<guid>https://arxiv.org/abs/2509.23694</guid>
<content:encoded><![CDATA[
arXiv:2509.23694v1 Announce Type: new 
Abstract: Search agents connect LLMs to the Internet, enabling access to broader and more up-to-date information. However, unreliable search results may also pose safety threats to end users, establishing a new threat surface. In this work, we conduct two in-the-wild experiments to demonstrate both the prevalence of low-quality search results and their potential to misguide agent behaviors. To counter this threat, we introduce an automated red-teaming framework that is systematic, scalable, and cost-efficient, enabling lightweight and harmless safety assessments of search agents. Building on this framework, we construct the SafeSearch benchmark, which includes 300 test cases covering five categories of risks (e.g., misinformation and indirect prompt injection). Using this benchmark, we evaluate three representative search agent scaffolds, covering search workflow, tool-calling, and deep research, across 7 proprietary and 8 open-source backend LLMs. Our results reveal substantial vulnerabilities of LLM-based search agents: when exposed to unreliable websites, the highest ASR reached 90.5% for GPT-4.1-mini under a search workflow setting. Moreover, our analysis highlights the limited effectiveness of common defense practices, such as reminder prompting. This emphasizes the value of our framework in promoting transparency for safer agent development. Our codebase and test cases are publicly available: https://github.com/jianshuod/SafeSearch.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIVA+: Human-Centered Situational Decision-Making</title>
<link>https://arxiv.org/abs/2509.23698</link>
<guid>https://arxiv.org/abs/2509.23698</guid>
<content:encoded><![CDATA[
arXiv:2509.23698v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) show promising results for embodied agents in operating meaningfully in complex, human-centered environments. Yet, evaluating their capacity for nuanced, human-like reasoning and decision-making remains challenging. In this work, we introduce VIVA+, a cognitively grounded benchmark for evaluating the reasoning and decision-making of MLLMs in human-centered situations. VIVA+ consists of 1,317 real-world situations paired with 6,373 multiple-choice questions, targeting three core abilities for decision-making: (1) Foundational Situation Comprehension, (2) Context-Driven Action Justification, and (3) Reflective Reasoning. Together, these dimensions provide a systematic framework for assessing a model's ability to perceive, reason, and act in socially meaningful ways. We evaluate the latest commercial and open-source models on VIVA+, where we reveal distinct performance patterns and highlight significant challenges. We further explore targeted training and multi-step reasoning strategies, which yield consistent performance improvements. Finally, our in-depth analysis highlights current model limitations and provides actionable insights for advancing MLLMs toward more robust, context-aware, and socially adept decision-making in real-world settings.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>INSTINCT: Instance-Level Interaction Architecture for Query-Based Collaborative Perception</title>
<link>https://arxiv.org/abs/2509.23700</link>
<guid>https://arxiv.org/abs/2509.23700</guid>
<content:encoded><![CDATA[
arXiv:2509.23700v1 Announce Type: new 
Abstract: Collaborative perception systems overcome single-vehicle limitations in long-range detection and occlusion scenarios by integrating multi-agent sensory data, improving accuracy and safety. However, frequent cooperative interactions and real-time requirements impose stringent bandwidth constraints. Previous works proves that query-based instance-level interaction reduces bandwidth demands and manual priors, however, LiDAR-focused implementations in collaborative perception remain underdeveloped, with performance still trailing state-of-the-art approaches. To bridge this gap, we propose INSTINCT (INSTance-level INteraCtion ArchiTecture), a novel collaborative perception framework featuring three core components: 1) a quality-aware filtering mechanism for high-quality instance feature selection; 2) a dual-branch detection routing scheme to decouple collaboration-irrelevant and collaboration-relevant instances; and 3) a Cross Agent Local Instance Fusion module to aggregate local hybrid instance features. Additionally, we enhance the ground truth (GT) sampling technique to facilitate training with diverse hybrid instance features. Extensive experiments across multiple datasets demonstrate that INSTINCT achieves superior performance. Specifically, our method achieves an improvement in accuracy 13.23%/33.08% in DAIR-V2X and V2V4Real while reducing the communication bandwidth to 1/281 and 1/264 compared to state-of-the-art methods. The code is available at https://github.com/CrazyShout/INSTINCT.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedLA: A Logic-Driven Multi-Agent Framework for Complex Medical Reasoning with Large Language Models</title>
<link>https://arxiv.org/abs/2509.23725</link>
<guid>https://arxiv.org/abs/2509.23725</guid>
<content:encoded><![CDATA[
arXiv:2509.23725v1 Announce Type: new 
Abstract: Answering complex medical questions requires not only domain expertise and patient-specific information, but also structured and multi-perspective reasoning. Existing multi-agent approaches often rely on fixed roles or shallow interaction prompts, limiting their ability to detect and resolve fine-grained logical inconsistencies. To address this, we propose \textsc{MedLA}, a logic-driven multi-agent framework built on large language models. Each agent organizes its reasoning process into an explicit logical tree based on syllogistic triads (major premise, minor premise, and conclusion), enabling transparent inference and premise-level alignment. Agents engage in a multi-round, graph-guided discussion to compare and iteratively refine their logic trees, achieving consensus through error correction and contradiction resolution. We demonstrate that \textsc{MedLA} consistently outperforms both static role-based systems and single-agent baselines on challenging benchmarks such as MedDDx and standard medical QA tasks. Furthermore, \textsc{MedLA} scales effectively across both open-source and commercial LLM backbones, achieving state-of-the-art performance and offering a generalizable paradigm for trustworthy medical reasoning.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagnosing Failure Root Causes in Platform-Orchestrated Agentic Systems: Dataset, Taxonomy, and Benchmark</title>
<link>https://arxiv.org/abs/2509.23735</link>
<guid>https://arxiv.org/abs/2509.23735</guid>
<content:encoded><![CDATA[
arXiv:2509.23735v1 Announce Type: new 
Abstract: Agentic systems consisting of multiple LLM-driven agents coordinating through tools and structured interactions, are increasingly deployed for complex reasoning and problem-solving tasks. At the same time, emerging low-code and template-based agent development platforms (e.g., Dify) enable users to rapidly build and orchestrate agentic systems, which we refer to as platform-orchestrated agentic systems. However, these systems are also fragile and it remains unclear how to systematically identify their potential failure root cause. This paper presents a study of root cause identification of these platform-orchestrated agentic systems. To support this initiative, we construct a dataset AgentFail containing 307 failure logs from ten agentic systems, each with fine-grained annotations linking failures to their root causes. We additionally utilize counterfactual reasoning-based repair strategy to ensure the reliability of the annotation. Building on the dataset, we develop a taxonomy that characterizes failure root causes and analyze their distribution across different platforms and task domains. Furthermore, we introduce a benchmark that leverages LLMs for automatically identifying root causes, in which we also utilize the proposed taxonomy as guidance for LLMs. Results show that the taxonomy can largely improve the performance, thereby confirming its utility. Nevertheless, the accuracy of root cause identification reaches at most 33.6%, which indicates that this task still remains challenging. In light of these results, we also provide actionable guidelines for building such agentic systems. In summary, this paper provides a reliable dataset of failure root cause for platform-orchestrated agentic systems, corresponding taxonomy and benchmark, which serves as a foundation for advancing the development of more reliable agentic systems.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUI-Shepherd: Reliable Process Reward and Verification for Long-Sequence GUI Tasks</title>
<link>https://arxiv.org/abs/2509.23738</link>
<guid>https://arxiv.org/abs/2509.23738</guid>
<content:encoded><![CDATA[
arXiv:2509.23738v1 Announce Type: new 
Abstract: Autonomous agents for long-sequence Graphical User Interface tasks are hindered by sparse rewards and the intractable credit assignment problem. To address these challenges, we introduce GUI-Shepherd, a Process Reward Model that provides dense, step-by-step feedback to guide agents. GUI-Shepherd is trained on a diverse large-scale data set of $52$k interactions that features human-annotated scores and GPT-4o generated rationales, enabling it to serve both as a reward provider for RL training and as a verifier for inference. As far as we know, we are the first to conduct a systematic study of process supervision in GUI agents, across diverse settings from online long-horizon tasks to offline single-step prediction. On the online AndroidWorld benchmark, GUI-Shepherd improves success rate by $7.7$ points via multi-turn online PPO, significantly outperforming Outcome Reward Model based competitors. When used as an inference verifier, it brings $5.1$ points improvements. The benefits generalize to the offline AndroidControl benchmark, with gains of $2.2$ points as a reward provider and $4.3$ points as a verifier. Collectively, our results establish that high-fidelity process supervision is critical for building more capable GUI agents and present a generalizable solution.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Game Theory Optimal: Profit-Maximizing Poker Agents for No-Limit Holdem</title>
<link>https://arxiv.org/abs/2509.23747</link>
<guid>https://arxiv.org/abs/2509.23747</guid>
<content:encoded><![CDATA[
arXiv:2509.23747v1 Announce Type: new 
Abstract: Game theory has grown into a major field over the past few decades, and poker has long served as one of its key case studies. Game-Theory-Optimal (GTO) provides strategies to avoid loss in poker, but pure GTO does not guarantee maximum profit. To this end, we aim to develop a model that outperforms GTO strategies to maximize profit in No Limit Holdem, in heads-up (two-player) and multi-way (more than two-player) situations. Our model finds the GTO foundation and goes further to exploit opponents. The model first navigates toward many simulated poker hands against itself and keeps adjusting its decisions until no action can reliably beat it, creating a strong baseline that is close to the theoretical best strategy. Then, it adapts by observing opponent behavior and adjusting its strategy to capture extra value accordingly. Our results indicate that Monte-Carlo Counterfactual Regret Minimization (CFR) performs best in heads-up situations and CFR remains the strongest method in most multi-way situations. By combining the defensive strength of GTO with real-time exploitation, our approach aims to show how poker agents can move from merely not losing to consistently winning against diverse opponents.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Investigation of Batch Normalization in Off-Policy Actor-Critic Algorithms</title>
<link>https://arxiv.org/abs/2509.23750</link>
<guid>https://arxiv.org/abs/2509.23750</guid>
<content:encoded><![CDATA[
arXiv:2509.23750v1 Announce Type: new 
Abstract: Batch Normalization (BN) has played a pivotal role in the success of deep learning by improving training stability, mitigating overfitting, and enabling more effective optimization. However, its adoption in deep reinforcement learning (DRL) has been limited due to the inherent non-i.i.d. nature of data and the dynamically shifting distributions induced by the agent's learning process. In this paper, we argue that, despite these challenges, BN retains unique advantages in DRL settings, particularly through its stochasticity and its ability to ease training. When applied appropriately, BN can adapt to evolving data distributions and enhance both convergence speed and final performance. To this end, we conduct a comprehensive empirical study on the use of BN in off-policy actor-critic algorithms, systematically analyzing how different training and evaluation modes impact performance. We further identify failure modes that lead to instability or divergence, analyze their underlying causes, and propose the Mode-Aware Batch Normalization (MA-BN) method with practical actionable recommendations for robust BN integration in DRL pipelines. We also empirically validate that, in RL settings, MA-BN accelerates and stabilizes training, broadens the effective learning rate range, enhances exploration, and reduces overall optimization difficulty. Our code is available at: https://github.com/monster476/ma-bn.git.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transparent Visual Reasoning via Object-Centric Agent Collaboration</title>
<link>https://arxiv.org/abs/2509.23757</link>
<guid>https://arxiv.org/abs/2509.23757</guid>
<content:encoded><![CDATA[
arXiv:2509.23757v1 Announce Type: new 
Abstract: A central challenge in explainable AI, particularly in the visual domain, is producing explanations grounded in human-understandable concepts. To tackle this, we introduce OCEAN (Object-Centric Explananda via Agent Negotiation), a novel, inherently interpretable framework built on object-centric representations and a transparent multi-agent reasoning process. The game-theoretic reasoning process drives agents to agree on coherent and discriminative evidence, resulting in a faithful and interpretable decision-making process. We train OCEAN end-to-end and benchmark it against standard visual classifiers and popular posthoc explanation tools like GradCAM and LIME across two diagnostic multi-object datasets. Our results demonstrate competitive performance with respect to state-of-the-art black-box models with a faithful reasoning process, which was reflected by our user study, where participants consistently rated OCEAN's explanations as more intuitive and trustworthy.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning</title>
<link>https://arxiv.org/abs/2509.23768</link>
<guid>https://arxiv.org/abs/2509.23768</guid>
<content:encoded><![CDATA[
arXiv:2509.23768v1 Announce Type: new 
Abstract: The chemical reaction recommendation is to select proper reaction condition parameters for chemical reactions, which is pivotal to accelerating chemical science. With the rapid development of large language models (LLMs), there is growing interest in leveraging their reasoning and planning capabilities for reaction condition recommendation. Despite their success, existing methods rarely explain the rationale behind the recommended reaction conditions, limiting their utility in high-stakes scientific workflows. In this work, we propose ChemMAS, a multi-agent system that reframes condition prediction as an evidence-based reasoning task. ChemMAS decomposes the task into mechanistic grounding, multi-channel recall, constraint-aware agentic debate, and rationale aggregation. Each decision is backed by interpretable justifications grounded in chemical knowledge and retrieved precedents. Experiments show that ChemMAS achieves 20-35% gains over domain-specific baselines and outperforms general-purpose LLMs by 10-15% in Top-1 accuracy, while offering falsifiable, human-trustable rationales, which establishes a new paradigm for explainable AI in scientific discovery.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequence Pathfinder for Multi-Agent Pickup and Delivery in the Warehouse</title>
<link>https://arxiv.org/abs/2509.23778</link>
<guid>https://arxiv.org/abs/2509.23778</guid>
<content:encoded><![CDATA[
arXiv:2509.23778v1 Announce Type: new 
Abstract: Multi-Agent Pickup and Delivery (MAPD) is a challenging extension of Multi-Agent Path Finding (MAPF), where agents are required to sequentially complete tasks with fixed-location pickup and delivery demands. Although learning-based methods have made progress in MAPD, they often perform poorly in warehouse-like environments with narrow pathways and long corridors when relying only on local observations for distributed decision-making. Communication learning can alleviate the lack of global information but introduce high computational complexity due to point-to-point communication. To address this challenge, we formulate MAPF as a sequence modeling problem and prove that path-finding policies under sequence modeling possess order-invariant optimality, ensuring its effectiveness in MAPD. Building on this, we propose the Sequential Pathfinder (SePar), which leverages the Transformer paradigm to achieve implicit information exchange, reducing decision-making complexity from exponential to linear while maintaining efficiency and global awareness. Experiments demonstrate that SePar consistently outperforms existing learning-based methods across various MAPF tasks and their variants, and generalizes well to unseen environments. Furthermore, we highlight the necessity of integrating imitation learning in complex maps like warehouses.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CaRe-BN: Precise Moving Statistics for Stabilizing Spiking Neural Networks in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.23791</link>
<guid>https://arxiv.org/abs/2509.23791</guid>
<content:encoded><![CDATA[
arXiv:2509.23791v1 Announce Type: new 
Abstract: Spiking Neural Networks (SNNs) offer low-latency and energy-efficient decision-making on neuromorphic hardware by mimicking the event-driven dynamics of biological neurons. However, due to the discrete and non-differentiable nature of spikes, directly trained SNNs rely heavily on Batch Normalization (BN) to stabilize gradient updates. In online Reinforcement Learning (RL), imprecise BN statistics hinder exploitation, resulting in slower convergence and suboptimal policies. This challenge limits the adoption of SNNs for energy-efficient control on resource-constrained devices. To overcome this, we propose Confidence-adaptive and Re-calibration Batch Normalization (CaRe-BN), which introduces (\emph{i}) a confidence-guided adaptive update strategy for BN statistics and (\emph{ii}) a re-calibration mechanism to align distributions. By providing more accurate normalization, CaRe-BN stabilizes SNN optimization without disrupting the RL training process. Importantly, CaRe-BN does not alter inference, thus preserving the energy efficiency of SNNs in deployment. Extensive experiments on continuous control benchmarks demonstrate that CaRe-BN improves SNN performance by up to $22.6\%$ across different spiking neuron models and RL algorithms. Remarkably, SNNs equipped with CaRe-BN even surpass their ANN counterparts by $5.9\%$. These results highlight a new direction for BN techniques tailored to RL, paving the way for neuromorphic agents that are both efficient and high-performing.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STAIR: Addressing Stage Misalignment through Temporal-Aligned Preference Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.23802</link>
<guid>https://arxiv.org/abs/2509.23802</guid>
<content:encoded><![CDATA[
arXiv:2509.23802v1 Announce Type: new 
Abstract: Preference-based reinforcement learning (PbRL) bypasses complex reward engineering by learning rewards directly from human preferences, enabling better alignment with human intentions. However, its effectiveness in multi-stage tasks, where agents sequentially perform sub-tasks (e.g., navigation, grasping), is limited by stage misalignment: Comparing segments from mismatched stages, such as movement versus manipulation, results in uninformative feedback, thus hindering policy learning. In this paper, we validate the stage misalignment issue through theoretical analysis and empirical experiments. To address this issue, we propose STage-AlIgned Reward learning (STAIR), which first learns a stage approximation based on temporal distance, then prioritizes comparisons within the same stage. Temporal distance is learned via contrastive learning, which groups temporally close states into coherent stages, without predefined task knowledge, and adapts dynamically to policy changes. Extensive experiments demonstrate STAIR's superiority in multi-stage tasks and competitive performance in single-stage tasks. Furthermore, human studies show that stages approximated by STAIR are consistent with human cognition, confirming its effectiveness in mitigating stage misalignment.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedAgentBench: Towards Automating Real-world Federated Medical Image Analysis with Server-Client LLM Agents</title>
<link>https://arxiv.org/abs/2509.23803</link>
<guid>https://arxiv.org/abs/2509.23803</guid>
<content:encoded><![CDATA[
arXiv:2509.23803v1 Announce Type: new 
Abstract: Federated learning (FL) allows collaborative model training across healthcare sites without sharing sensitive patient data. However, real-world FL deployment is often hindered by complex operational challenges that demand substantial human efforts. This includes: (a) selecting appropriate clients (hospitals), (b) coordinating between the central server and clients, (c) client-level data pre-processing, (d) harmonizing non-standardized data and labels across clients, and (e) selecting FL algorithms based on user instructions and cross-client data characteristics. However, the existing FL works overlook these practical orchestration challenges. These operational bottlenecks motivate the need for autonomous, agent-driven FL systems, where intelligent agents at each hospital client and the central server agent collaboratively manage FL setup and model training with minimal human intervention. To this end, we first introduce an agent-driven FL framework that captures key phases of real-world FL workflows from client selection to training completion and a benchmark dubbed FedAgentBench that evaluates the ability of LLM agents to autonomously coordinate healthcare FL. Our framework incorporates 40 FL algorithms, each tailored to address diverse task-specific requirements and cross-client characteristics. Furthermore, we introduce a diverse set of complex tasks across 201 carefully curated datasets, simulating 6 modality-specific real-world healthcare environments, viz., Dermatoscopy, Ultrasound, Fundus, Histopathology, MRI, and X-Ray. We assess the agentic performance of 14 open-source and 10 proprietary LLMs spanning small, medium, and large model scales. While some agent cores such as GPT-4.1 and DeepSeek V3 can automate various stages of the FL pipeline, our results reveal that more complex, interdependent tasks based on implicit goals remain challenging for even the strongest models.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mix-Ecom: Towards Mixed-Type E-Commerce Dialogues with Complex Domain Rules</title>
<link>https://arxiv.org/abs/2509.23836</link>
<guid>https://arxiv.org/abs/2509.23836</guid>
<content:encoded><![CDATA[
arXiv:2509.23836v1 Announce Type: new 
Abstract: E-commerce agents contribute greatly to helping users complete their e-commerce needs. To promote further research and application of e-commerce agents, benchmarking frameworks are introduced for evaluating LLM agents in the e-commerce domain. Despite the progress, current benchmarks lack evaluating agents' capability to handle mixed-type e-commerce dialogue and complex domain rules. To address the issue, this work first introduces a novel corpus, termed Mix-ECom, which is constructed based on real-world customer-service dialogues with post-processing to remove user privacy and add CoT process. Specifically, Mix-ECom contains 4,799 samples with multiply dialogue types in each e-commerce dialogue, covering four dialogue types (QA, recommendation, task-oriented dialogue, and chit-chat), three e-commerce task types (pre-sales, logistics, after-sales), and 82 e-commerce rules. Furthermore, this work build baselines on Mix-Ecom and propose a dynamic framework to further improve the performance. Results show that current e-commerce agents lack sufficient capabilities to handle e-commerce dialogues, due to the hallucination cased by complex domain rules. The dataset will be publicly available.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentGuard: Runtime Verification of AI Agents</title>
<link>https://arxiv.org/abs/2509.23864</link>
<guid>https://arxiv.org/abs/2509.23864</guid>
<content:encoded><![CDATA[
arXiv:2509.23864v1 Announce Type: new 
Abstract: The rapid evolution to autonomous, agentic AI systems introduces significant risks due to their inherent unpredictability and emergent behaviors; this also renders traditional verification methods inadequate and necessitates a shift towards probabilistic guarantees where the question is no longer if a system will fail, but the probability of its failure within given constraints. This paper presents AgentGuard, a framework for runtime verification of Agentic AI systems that provides continuous, quantitative assurance through a new paradigm called Dynamic Probabilistic Assurance. AgentGuard operates as an inspection layer that observes an agent's raw I/O and abstracts it into formal events corresponding to transitions in a state model. It then uses online learning to dynamically build and update a Markov Decision Process (MDP) that formally models the agent's emergent behavior. Using probabilistic model checking, the framework then verifies quantitative properties in real-time.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Multi-turn RL for GUI Agents via Decoupled Training and Adaptive Data Curation</title>
<link>https://arxiv.org/abs/2509.23866</link>
<guid>https://arxiv.org/abs/2509.23866</guid>
<content:encoded><![CDATA[
arXiv:2509.23866v1 Announce Type: new 
Abstract: Vision-language model (VLM) based GUI agents show promise for automating complex desktop and mobile tasks, but face significant challenges in applying reinforcement learning (RL): (1) slow multi-turn interactions with GUI environments for policy rollout, and (2) insufficient high-quality agent-environment interactions for policy learning. To address these challenges, we propose DART, a Decoupled Agentic RL Training framework for GUI agents, which coordinates heterogeneous modules in a highly decoupled manner. DART separates the training system into four asynchronous modules: environment cluster, rollout service, data manager, and trainer. This design enables non-blocking communication, asynchronous training, rollout-wise trajectory sampling, and per-worker model synchronization, significantly improving the system efficiency: 1.6*GPU utilization for rollout, 1.9* training throughput, and 5.5* environment utilization. To facilitate effective learning from abundant samples, we introduce an adaptive data curation scheme: (1) pre-collecting successful trajectories for challenging tasks to supplement sparse success in online sampling; (2) dynamically adjusting rollout numbers and trajectory lengths based on task difficulty; (3) training selectively on high-entropy steps to prioritize critical decisions; (4) stabilizing learning via truncated importance sampling for policy mismatch between policy rollout and updating. On the OSWorld benchmark, DART-GUI-7B achieves a 42.13% task success rate, a 14.61% absolute gain over the base model, and 7.34% higher than open-source SOTA. We will fully open-source our training framework, data, and model checkpoints via computer-use-agents.github.io/dart-gui, which we believe is a timely contribution to the open-source community of agentic RL training.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Reward Miscalibration of GRPO in Agentic RL</title>
<link>https://arxiv.org/abs/2509.23870</link>
<guid>https://arxiv.org/abs/2509.23870</guid>
<content:encoded><![CDATA[
arXiv:2509.23870v1 Announce Type: new 
Abstract: Building autonomous agents capable of solving long-horizon, real-world tasks has garnered significant research interest. But outcome based rewards may cause reward miscalibration which means it might mistakenly allocate positive reward to flawed middle steps which is regarded as the key reason making the bad actions being reinforced during training. However we reveal that outcome based reward ensures expected negative advantage for those flawed middle steps, which means the flawed actions should be punished during training. Even accounting for the ``squeezing effect", the probability mass of good actions should increase and the actor should gradually get rid of harmful actions. This shows that flawed actions should be punished during training. We further identify gradient coupling between similar samples as a key issue in agentic RL, the input prompt is extremely similar and the output action space is limited, therefore during training, gradients from well-performing samples can inadvertently strengthen suboptimal or incorrect actions due to similar input observation and output actions. We show that with gradient coupling, some flawed actions might be enhanced. To address this, we propose training the actor to classify good or bad actions to separate the embedding of good/bad actions and alleviate the gradient interference, extensive experiments shows its effectiveness.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrated Communication and Control for Energy-Efficient UAV Swarms: A Multi-Agent Reinforcement Learning Approach</title>
<link>https://arxiv.org/abs/2509.23905</link>
<guid>https://arxiv.org/abs/2509.23905</guid>
<content:encoded><![CDATA[
arXiv:2509.23905v1 Announce Type: new 
Abstract: The deployment of unmanned aerial vehicle (UAV) swarm-assisted communication networks has become an increasingly vital approach for remediating coverage limitations in infrastructure-deficient environments, with especially pressing applications in temporary scenarios, such as emergency rescue, military and security operations, and remote area coverage. However, complex geographic environments lead to unpredictable and highly dynamic wireless channel conditions, resulting in frequent interruptions of air-to-ground (A2G) links that severely constrain the reliability and quality of service in UAV swarm-assisted mobile communications. To improve the quality of UAV swarm-assisted communications in complex geographic environments, we propose an integrated communication and control co-design mechanism. Given the stringent energy constraints inherent in UAV swarms, our proposed mechanism is designed to optimize energy efficiency while maintaining an equilibrium between equitable communication rates for mobile ground users (GUs) and UAV energy expenditure. We formulate the joint resource allocation and 3D trajectory control problem as a Markov decision process (MDP), and develop a multi-agent reinforcement learning (MARL) framework to enable real-time coordinated actions across the UAV swarm. To optimize the action policy of UAV swarms, we propose a novel multi-agent hybrid proximal policy optimization with action masking (MAHPPO-AM) algorithm, specifically designed to handle complex hybrid action spaces. The algorithm incorporates action masking to enforce hard constraints in high-dimensional action spaces. Experimental results demonstrate that our approach achieves a fairness index of 0.99 while reducing energy consumption by up to 25% compared to baseline methods.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning with Inverse Rewards for World Model Post-training</title>
<link>https://arxiv.org/abs/2509.23958</link>
<guid>https://arxiv.org/abs/2509.23958</guid>
<content:encoded><![CDATA[
arXiv:2509.23958v1 Announce Type: new 
Abstract: World models simulate dynamic environments, enabling agents to interact with diverse input modalities. Although recent advances have improved the visual quality and temporal consistency of video world models, their ability of accurately modeling human-specified actions remains under-explored. Reinforcement learning presents a promising approach for directly improving the suboptimal action-following capability of pre-trained models, assuming that an appropriate reward function can be defined. However, transferring reinforcement learning post-training methods to world model is impractical due to the prohibitive cost of large-scale preference annotations and the infeasibility of constructing rule-based video verifiers. To address this gap, we propose Reinforcement Learning with Inverse Rewards (RLIR), a post-training framework that derives verifiable reward signals by recovering input actions from generated videos using an Inverse Dynamics Model. By mapping high-dimensional video modality to a low-dimensional action space, RLIR provides an objective and verifiable reward for optimization via Group Relative Policy Optimization. Experiments across autoregressive and diffusion paradigms demonstrate 5-10% gains in action-following, up to 10% improvements in visual quality, and higher human preference scores, establishing RLIR as the first post-training method specifically designed to enhance action-following in video world models.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAD-PINN: A Decentralized Physics-Informed Machine Learning Framework for Safe and Optimal Multi-Agent Control</title>
<link>https://arxiv.org/abs/2509.23960</link>
<guid>https://arxiv.org/abs/2509.23960</guid>
<content:encoded><![CDATA[
arXiv:2509.23960v1 Announce Type: new 
Abstract: Co-optimizing safety and performance in large-scale multi-agent systems remains a fundamental challenge. Existing approaches based on multi-agent reinforcement learning (MARL), safety filtering, or Model Predictive Control (MPC) either lack strict safety guarantees, suffer from conservatism, or fail to scale effectively. We propose MAD-PINN, a decentralized physics-informed machine learning framework for solving the multi-agent state-constrained optimal control problem (MASC-OCP). Our method leverages an epigraph-based reformulation of SC-OCP to simultaneously capture performance and safety, and approximates its solution via a physics-informed neural network. Scalability is achieved by training the SC-OCP value function on reduced-agent systems and deploying them in a decentralized fashion, where each agent relies only on local observations of its neighbours for decision-making. To further enhance safety and efficiency, we introduce an Hamilton-Jacobi (HJ) reachability-based neighbour selection strategy to prioritize safety-critical interactions, and a receding-horizon policy execution scheme that adapts to dynamic interactions while reducing computational burden. Experiments on multi-agent navigation tasks demonstrate that MAD-PINN achieves superior safety-performance trade-offs, maintains scalability as the number of agents grows, and consistently outperforms state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curriculum-Guided Reinforcement Learning for Synthesizing Gas-Efficient Financial Derivatives Contracts</title>
<link>https://arxiv.org/abs/2509.23976</link>
<guid>https://arxiv.org/abs/2509.23976</guid>
<content:encoded><![CDATA[
arXiv:2509.23976v1 Announce Type: new 
Abstract: Smart contract-based automation of financial derivatives offers substantial efficiency gains, but its real-world adoption is constrained by the complexity of translating financial specifications into gas-efficient executable code. In particular, generating code that is both functionally correct and economically viable from high-level specifications, such as the Common Domain Model (CDM), remains a significant challenge. This paper introduces a Reinforcement Learning (RL) framework to generate functional and gas-optimized Solidity smart contracts directly from CDM specifications. We employ a Proximal Policy Optimization (PPO) agent that learns to select optimal code snippets from a pre-defined library. To manage the complex search space, a two-phase curriculum first trains the agent for functional correctness before shifting its focus to gas optimization. Our empirical results show the RL agent learns to generate contracts with significant gas savings, achieving cost reductions of up to 35.59% on unseen test data compared to unoptimized baselines. This work presents a viable methodology for the automated synthesis of reliable and economically sustainable smart contracts, bridging the gap between high-level financial agreements and efficient on-chain execution.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TusoAI: Agentic Optimization for Scientific Methods</title>
<link>https://arxiv.org/abs/2509.23986</link>
<guid>https://arxiv.org/abs/2509.23986</guid>
<content:encoded><![CDATA[
arXiv:2509.23986v1 Announce Type: new 
Abstract: Scientific discovery is often slowed by the manual development of computational tools needed to analyze complex experimental data. Building such tools is costly and time-consuming because scientists must iteratively review literature, test modeling and scientific assumptions against empirical data, and implement these insights into efficient software. Large language models (LLMs) have demonstrated strong capabilities in synthesizing literature, reasoning with empirical data, and generating domain-specific code, offering new opportunities to accelerate computational method development. Existing LLM-based systems either focus on performing scientific analyses using existing computational methods or on developing computational methods or models for general machine learning without effectively integrating the often unstructured knowledge specific to scientific domains. Here, we introduce TusoAI , an agentic AI system that takes a scientific task description with an evaluation function and autonomously develops and optimizes computational methods for the application. TusoAI integrates domain knowledge into a knowledge tree representation and performs iterative, domain-specific optimization and model diagnosis, improving performance over a pool of candidate solutions. We conducted comprehensive benchmark evaluations demonstrating that TusoAI outperforms state-of-the-art expert methods, MLE agents, and scientific AI agents across diverse tasks, such as single-cell RNA-seq data denoising and satellite-based earth monitoring. Applying TusoAI to two key open problems in genetics improved existing computational methods and uncovered novel biology, including 9 new associations between autoimmune diseases and T cell subtypes and 7 previously unreported links between disease variants linked to their target genes. Our code is publicly available at https://github.com/Alistair-Turcan/TusoAI.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM/Agent-as-Data-Analyst: A Survey</title>
<link>https://arxiv.org/abs/2509.23988</link>
<guid>https://arxiv.org/abs/2509.23988</guid>
<content:encoded><![CDATA[
arXiv:2509.23988v1 Announce Type: new 
Abstract: Large language model (LLM) and agent techniques for data analysis (a.k.a LLM/Agent-as-Data-Analyst) have demonstrated substantial impact in both academica and industry. In comparison with traditional rule or small-model based approaches, (agentic) LLMs enable complex data understanding, natural language interfaces, semantic analysis functions, and autonomous pipeline orchestration. The technical evolution further distills five key design goals for intelligent data analysis agents, namely semantic-aware design, modality-hybrid integration, autonomous pipelines, tool-augmented workflows, and support for open-world tasks. From a modality perspective, we review LLM-based techniques for (i) structured data (e.g., table question answering for relational data and NL2GQL for graph data), (ii) semi-structured data (e.g., markup languages understanding and semi-structured table modeling), (iii) unstructured data (e.g., chart understanding, document understanding, programming languages vulnerable detection), and (iv) heterogeneous data (e.g., data retrieval and modality alignment for data lakes). Finally, we outline the remaining challenges and propose several insights and practical directions for advancing LLM/Agent-powered data analysis.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guide: Generalized-Prior and Data Encoders for DAG Estimation</title>
<link>https://arxiv.org/abs/2509.23992</link>
<guid>https://arxiv.org/abs/2509.23992</guid>
<content:encoded><![CDATA[
arXiv:2509.23992v1 Announce Type: new 
Abstract: Modern causal discovery methods face critical limitations in scalability, computational efficiency, and adaptability to mixed data types, as evidenced by benchmarks on node scalability (30, $\le 50$, $\ge 70$ nodes), computational energy demands, and continuous/non-continuous data handling. While traditional algorithms like PC, GES, and ICA-LiNGAM struggle with these challenges, exhibiting prohibitive energy costs for higher-order nodes and poor scalability beyond 70 nodes, we propose \textbf{GUIDE}, a framework that integrates Large Language Model (LLM)-generated adjacency matrices with observational data through a dual-encoder architecture. GUIDE uniquely optimizes computational efficiency, reducing runtime on average by $\approx 42%$ compared to RL-BIC and KCRL methods, while achieving an average $\approx 117%$ improvement in accuracy over both NOTEARS and GraN-DAG individually. During training, GUIDE's reinforcement learning agent dynamically balances reward maximization (accuracy) and penalty avoidance (DAG constraints), enabling robust performance across mixed data types and scalability to $\ge 70$ nodes -- a setting where baseline methods fail.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Multi-agent Traffic Simulation via R1-Style Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2509.23993</link>
<guid>https://arxiv.org/abs/2509.23993</guid>
<content:encoded><![CDATA[
arXiv:2509.23993v1 Announce Type: new 
Abstract: Scalable and realistic simulation of multi-agent traffic behavior is critical for advancing autonomous driving technologies. Although existing data-driven simulators have made significant strides in this domain, they predominantly rely on supervised learning to align simulated distributions with real-world driving scenarios. A persistent challenge, however, lies in the distributional shift that arises between training and testing, which often undermines model generalization in unseen environments. To address this limitation, we propose SMART-R1, a novel R1-style reinforcement fine-tuning paradigm tailored for next-token prediction models to better align agent behavior with human preferences and evaluation metrics. Our approach introduces a metric-oriented policy optimization algorithm to improve distribution alignment and an iterative "SFT-RFT-SFT" training strategy that alternates between Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT) to maximize performance gains. Extensive experiments on the large-scale Waymo Open Motion Dataset (WOMD) validate the effectiveness of this simple yet powerful R1-style training framework in enhancing foundation models. The results on the Waymo Open Sim Agents Challenge (WOSAC) showcase that SMART-R1 achieves state-of-the-art performance with an overall realism meta score of 0.7858, ranking first on the leaderboard at the time of submission.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The AI Agent Code of Conduct: Automated Guardrail Policy-as-Prompt Synthesis</title>
<link>https://arxiv.org/abs/2509.23994</link>
<guid>https://arxiv.org/abs/2509.23994</guid>
<content:encoded><![CDATA[
arXiv:2509.23994v1 Announce Type: new 
Abstract: As autonomous AI agents are increasingly deployed in industry, it is essential to safeguard them. We introduce a novel framework that automates the translation of unstructured design documents into verifiable, real-time guardrails. We introduce "Policy as Prompt," a new approach that uses Large Language Models (LLMs) to interpret and enforce natural language policies by applying contextual understanding and the principle of least privilege. Our system first ingests technical artifacts to construct a verifiable policy tree, which is then compiled into lightweight, prompt-based classifiers that audit agent behavior at runtime. We validate our approach across diverse applications, demonstrating a scalable and auditable pipeline that bridges the critical policy-to-practice gap, paving the way for verifiably safer and more regulatable AI.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use</title>
<link>https://arxiv.org/abs/2509.24002</link>
<guid>https://arxiv.org/abs/2509.24002</guid>
<content:encoded><![CDATA[
arXiv:2509.24002v1 Announce Type: new 
Abstract: MCP standardizes how LLMs interact with external systems, forming the foundation for general agents. However, existing MCP benchmarks remain narrow in scope: they focus on read-heavy tasks or tasks with limited interaction depth, and fail to capture the complexity and realism of real-world workflows. To address this gap, we propose MCPMark, a benchmark designed to evaluate MCP use in a more realistic and comprehensive manner. It consists of $127$ high-quality tasks collaboratively created by domain experts and AI agents. Each task begins with a curated initial state and includes a programmatic script for automatic verification. These tasks demand richer and more diverse interactions with the environment, involving a broad range of create, read, update, and delete (CRUD) operations. We conduct a comprehensive evaluation of cutting-edge LLMs using a minimal agent framework that operates in a tool-calling loop. Empirical results show that the best-performing model, gpt-5-medium, reaches only $52.56$\% pass@1 and $33.86$\% pass^4, while other widely regarded strong models, including claude-sonnet-4 and o3, fall below $30$\% pass@1 and $15$\% pass^4. On average, LLMs require $16.2$ execution turns and $17.4$ tool calls per task, significantly surpassing those in previous MCP benchmarks and highlighting the stress-testing nature of MCPMark.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GPS-MTM: Capturing Pattern of Normalcy in GPS-Trajectories with self-supervised learning</title>
<link>https://arxiv.org/abs/2509.24031</link>
<guid>https://arxiv.org/abs/2509.24031</guid>
<content:encoded><![CDATA[
arXiv:2509.24031v1 Announce Type: new 
Abstract: Foundation models have driven remarkable progress in text, vision, and video understanding, and are now poised to unlock similar breakthroughs in trajectory modeling. We introduce the GPSMasked Trajectory Transformer (GPS-MTM), a foundation model for large-scale mobility data that captures patterns of normalcy in human movement. Unlike prior approaches that flatten trajectories into coordinate streams, GPS-MTM decomposes mobility into two complementary modalities: states (point-of-interest categories) and actions (agent transitions). Leveraging a bi-directional Transformer with a self-supervised masked modeling objective, the model reconstructs missing segments across modalities, enabling it to learn rich semantic correlations without manual labels. Across benchmark datasets, including Numosim-LA, Urban Anomalies, and Geolife, GPS-MTM consistently outperforms on downstream tasks such as trajectory infilling and next-stop prediction. Its advantages are most pronounced in dynamic tasks (inverse and forward dynamics), where contextual reasoning is critical. These results establish GPS-MTM as a robust foundation model for trajectory analytics, positioning mobility data as a first-class modality for large-scale representation learning. Code is released for further reference.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PartnerMAS: An LLM Hierarchical Multi-Agent Framework for Business Partner Selection on High-Dimensional Features</title>
<link>https://arxiv.org/abs/2509.24046</link>
<guid>https://arxiv.org/abs/2509.24046</guid>
<content:encoded><![CDATA[
arXiv:2509.24046v1 Announce Type: new 
Abstract: High-dimensional decision-making tasks, such as business partner selection, involve evaluating large candidate pools with heterogeneous numerical, categorical, and textual features. While large language models (LLMs) offer strong in-context reasoning capabilities, single-agent or debate-style systems often struggle with scalability and consistency in such settings. We propose PartnerMAS, a hierarchical multi-agent framework that decomposes evaluation into three layers: a Planner Agent that designs strategies, Specialized Agents that perform role-specific assessments, and a Supervisor Agent that integrates their outputs. To support systematic evaluation, we also introduce a curated benchmark dataset of venture capital co-investments, featuring diverse firm attributes and ground-truth syndicates. Across 140 cases, PartnerMAS consistently outperforms single-agent and debate-based multi-agent baselines, achieving up to 10--15\% higher match rates. Analysis of agent reasoning shows that planners are most responsive to domain-informed prompts, specialists produce complementary feature coverage, and supervisors play an important role in aggregation. Our findings demonstrate that structured collaboration among LLM agents can generate more robust outcomes than scaling individual models, highlighting PartnerMAS as a promising framework for high-dimensional decision-making in data-rich domains.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimism as Risk-Seeking in Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.24047</link>
<guid>https://arxiv.org/abs/2509.24047</guid>
<content:encoded><![CDATA[
arXiv:2509.24047v1 Announce Type: new 
Abstract: Risk sensitivity has become a central theme in reinforcement learning (RL), where convex risk measures and robust formulations provide principled ways to model preferences beyond expected return. Recent extensions to multi-agent RL (MARL) have largely emphasized the risk-averse setting, prioritizing robustness to uncertainty. In cooperative MARL, however, such conservatism often leads to suboptimal equilibria, and a parallel line of work has shown that optimism can promote cooperation. Existing optimistic methods, though effective in practice, are typically heuristic and lack theoretical grounding. Building on the dual representation for convex risk measures, we propose a principled framework that interprets risk-seeking objectives as optimism. We introduce optimistic value functions, which formalize optimism as divergence-penalized risk-seeking evaluations. Building on this foundation, we derive a policy-gradient theorem for optimistic value functions, including explicit formulas for the entropic risk/KL-penalty setting, and develop decentralized optimistic actor-critic algorithms that implement these updates. Empirical results on cooperative benchmarks demonstrate that risk-seeking optimism consistently improves coordination over both risk-neutral baselines and heuristic optimistic methods. Our framework thus unifies risk-sensitive learning and optimism, offering a theoretically grounded and practically effective approach to cooperation in MARL.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeraAgent: A Distributed Agent-Based Simulation Engine for Simulating Half a Trillion Agents</title>
<link>https://arxiv.org/abs/2509.24063</link>
<guid>https://arxiv.org/abs/2509.24063</guid>
<content:encoded><![CDATA[
arXiv:2509.24063v1 Announce Type: new 
Abstract: Agent-based simulation is an indispensable paradigm for studying complex systems. These systems can comprise billions of agents, requiring the computing resources of multiple servers to simulate. Unfortunately, the state-of-the-art platform, BioDynaMo, does not scale out across servers due to its shared-memory-based implementation.
  To overcome this key limitation, we introduce TeraAgent, a distributed agent-based simulation engine. A critical challenge in distributed execution is the exchange of agent information across servers, which we identify as a major performance bottleneck. We propose two solutions: 1) a tailored serialization mechanism that allows agents to be accessed and mutated directly from the receive buffer, and 2) leveraging the iterative nature of agent-based simulations to reduce data transfer with delta encoding.
  Built on our solutions, TeraAgent enables extreme-scale simulations with half a trillion agents (an 84x improvement), reduces time-to-result with additional compute nodes, improves interoperability with third-party tools, and provides users with more hardware flexibility.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Small Math Model: Recasting Strategy Choice Theory in an LLM-Inspired Architecture</title>
<link>https://arxiv.org/abs/2509.24068</link>
<guid>https://arxiv.org/abs/2509.24068</guid>
<content:encoded><![CDATA[
arXiv:2509.24068v1 Announce Type: new 
Abstract: Strategy Choice Theory (SCT)\footnote{``Strategy Choice Theory'', ``Distributions of Associations'', and ``Overlapping Wave Theory'' have been used to refer to this line of work, emphasizing different aspects.}\citep[e.g.,][]{siegler1984strategychoices, siegler2000rebirth} explains important aspects of children's arithmetic learning based upon principles including learning from developmentally naturalistic data, probabilistic representation, confidence-based retrieval, and the phase-like importance of scaffolding strategies, such as finger-counting. Here we recast SCT as a ``Small Math Model'' (SMM), employing a neural-network-based architecture analogous to LLMs. The SMM extends SCT to include counting practice\footnote{The original SCT model was pre-biased in accordance with the supposed experience of counting.}, symbol (number) embedding, and gated attention. Similar to earlier work, the SMM demonstrates constructive and destructive interference between counting and addition, and the ``wave-like'' use of finger-counting as sum recall improves. We plan to extend the SMM to later aspects of the decades-long SCT program, including adaptive strategy choice and eventually strategy discovery, providing a unified platform to investigate the understanding of numerical characteristics and relationships essential for mathematical reasoning -- as it can emerge in LLM-based agents.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Having Lunch Now": Understanding How Users Engage with a Proactive Agent for Daily Planning and Self-Reflection</title>
<link>https://arxiv.org/abs/2509.24073</link>
<guid>https://arxiv.org/abs/2509.24073</guid>
<content:encoded><![CDATA[
arXiv:2509.24073v1 Announce Type: new 
Abstract: Conversational agents have been studied as tools to scaffold planning and self-reflection for productivity and well-being. While prior work has demonstrated positive outcomes, we still lack a clear understanding of what drives these results and how users behave and communicate with agents that act as coaches rather than assistants. Such understanding is critical for designing interactions in which agents foster meaningful behavioral change. We conducted a 14-day longitudinal study with 12 participants using a proactive agent that initiated regular check-ins to support daily planning and reflection. Our findings reveal diverse interaction patterns: participants accepted or negotiated suggestions, developed shared mental models, reported progress, and at times resisted or disengaged. We also identified problematic aspects of the agent's behavior, including rigidity, premature turn-taking, and overpromising. Our work contributes to understanding how people interact with a proactive, coach-like agent and offers design considerations for facilitating effective behavioral change.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CORRECT: COndensed eRror RECognition via knowledge Transfer in multi-agent systems</title>
<link>https://arxiv.org/abs/2509.24088</link>
<guid>https://arxiv.org/abs/2509.24088</guid>
<content:encoded><![CDATA[
arXiv:2509.24088v1 Announce Type: new 
Abstract: Multi-agent systems (MAS) are increasingly capable of tackling complex real-world tasks, yet their reliance on inter-agent coordination, tool use, and long-horizon reasoning makes error recognition particularly challenging. Minor errors can propagate across agents, escalating into task failures while producing long, intertwined execution trajectories that impose significant costs for both human developers and automated systems to debug and analyze. Our key insight is that, despite surface differences in failure trajectories (e.g., logs), MAS errors often recur with similar structural patterns. This paper presents CORRECT, the first lightweight, training-free framework that leverages an online cache of distilled error schemata to recognize and transfer knowledge of failure structures across new requests. This cache-based reuse allows LLMs to perform targeted error localization at inference time, avoiding the need for expensive retraining while adapting to dynamic MAS deployments in subseconds. To support rigorous study in this domain, we also introduce CORRECT-Error, a large-scale dataset of over 2,000 annotated trajectories collected through a novel error-injection pipeline guided by real-world distributions, and further validated through human evaluation to ensure alignment with natural failure patterns. Experiments across seven diverse MAS applications show that CORRECT improves step-level error localization up to 19.8% over existing advances while at near-zero overhead, substantially narrowing the gap between automated and human-level error recognition.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PerfBench: Can Agents Resolve Real-World Performance Bugs?</title>
<link>https://arxiv.org/abs/2509.24091</link>
<guid>https://arxiv.org/abs/2509.24091</guid>
<content:encoded><![CDATA[
arXiv:2509.24091v1 Announce Type: new 
Abstract: Performance bugs are inefficiencies in software that waste computational resources without causing functional failures, making them particularly challenging to detect and fix. While recent advances in Software Engineering agents have shown promise in automated bug fixing, existing benchmarks primarily focus on functional correctness and fail to evaluate agents' abilities to identify and resolve non-functional issues like performance bugs. We introduce PerfBench, a benchmark comprising 81 real-world performance bug-fixing tasks from popular .NET repositories on GitHub. Unlike existing benchmarks that rely on pre-existing test suites, PerfBench features a novel evaluation harness that allows agents to generate their own performance benchmarks and validates fixes by comparing execution metrics collected for developer fix and agent fix. Each task in PerfBench is derived from actual developer fixes linked to performance-related issues, which are then verified by human experts, ensuring real-world relevance. Our evaluation reveals that current state-of-the-art coding agents struggle with performance optimization tasks, with baseline OpenHands agent achieving only a ~3% success rate on our benchmark. We develop OpenHands-Perf-Agent, which incorporates performance-aware tooling and instructions and achieves a ~20% success rate on the benchmark. We show that by ensuring the agent has proper instructions to benchmark its changes and tooling for benchmark output processing, we can improve the agent performance significantly, but room for improvement still remains. PerfBench provides a challenging test set for furthering the capabilities of agents in fixing performance issues.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs</title>
<link>https://arxiv.org/abs/2509.24107</link>
<guid>https://arxiv.org/abs/2509.24107</guid>
<content:encoded><![CDATA[
arXiv:2509.24107v1 Announce Type: new 
Abstract: Tool-integrated reasoning has emerged as a key focus for enabling agentic applications. Among these, DeepResearch Agents have gained significant attention for their strong performance on complex, open-ended information-seeking tasks. We introduce Fathom-DeepResearch, an agentic system composed of two specialized models. The first is Fathom-Search-4B, a DeepSearch model trained from Qwen3-4B and optimized for evidence-based investigation through live web search and targeted webpage querying. Its training combines three advances: (i) DUETQA, a 5K-sample dataset generated via multi-agent self-play that enforces strict web-search dependence and heterogeneous source grounding; (ii) RAPO, a zero-overhead extension of GRPO that stabilizes multi-turn Reinforcement Learning with Verifiable Rewards through curriculum pruning, reward-aware advantage scaling, and per-prompt replay buffers; and (iii) a steerable step-level reward that classifies each tool call by cognitive behavior and marginal utility, enabling explicit control over search trajectory breadth, depth, and horizon. These improvements enable reliable extension of tool-calling beyond 20 calls when warranted. The second is Fathom-Synthesizer-4B, trained from Qwen3-4B, which converts multi-turn DeepSearch traces into structured, citation-dense DeepResearch Reports for comprehensive synthesis. Evaluated on DeepSearch benchmarks (SimpleQA, FRAMES, WebWalker, Seal0, MuSiQue) and DeepResearch-Bench, the system achieves state-of-the-art performance in the open-weights category while demonstrating strong generalization to diverse reasoning tasks including HLE, AIME-25, GPQA-Diamond, and MedQA.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Scale World Models for LLM Agents Towards Hard-Exploration Problems</title>
<link>https://arxiv.org/abs/2509.24116</link>
<guid>https://arxiv.org/abs/2509.24116</guid>
<content:encoded><![CDATA[
arXiv:2509.24116v1 Announce Type: new 
Abstract: LLM-based agents have seen promising advances, yet they are still limited in "hard-exploration" tasks requiring learning new knowledge through exploration. We present GLoW, a novel approach leveraging dual-scale world models, maintaining a trajectory frontier of high-value discoveries at the global scale, while learning from local trial-and-error in exploration through a Multi-path Advantage Reflection mechanism which infers advantage-based progress signals to guide exploration. To evaluate our framework for hard-exploration, we tackle the Jericho benchmark suite of text-based games, where GLoW achieves a new state-of-theart performance for LLM-based approaches. Compared to state-of-the-art RLbased methods, our approach achieves comparable performance while requiring 100-800x fewer environment interactions.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transparent, Evaluable, and Accessible Data Agents: A Proof-of-Concept Framework</title>
<link>https://arxiv.org/abs/2509.24127</link>
<guid>https://arxiv.org/abs/2509.24127</guid>
<content:encoded><![CDATA[
arXiv:2509.24127v1 Announce Type: new 
Abstract: This article presents a modular, component-based architecture for developing and evaluating AI agents that bridge the gap between natural language interfaces and complex enterprise data warehouses. The system directly addresses core challenges in data accessibility by enabling non-technical users to interact with complex data warehouses through a conversational interface, translating ambiguous user intent into precise, executable database queries to overcome semantic gaps. A cornerstone of the design is its commitment to transparent decision-making, achieved through a multi-layered reasoning framework that explains the "why" behind every decision, allowing for full interpretability by tracing conclusions through specific, activated business rules and data points. The architecture integrates a robust quality assurance mechanism via an automated evaluation framework that serves multiple functions: it enables performance benchmarking by objectively measuring agent performance against golden standards, and it ensures system reliability by automating the detection of performance regressions during updates. The agent's analytical depth is enhanced by a statistical context module, which quantifies deviations from normative behavior, ensuring all conclusions are supported by quantitative evidence including concrete data, percentages, and statistical comparisons. We demonstrate the efficacy of this integrated agent-development-with-evaluation framework through a case study on an insurance claims processing system. The agent, built on a modular architecture, leverages the BigQuery ecosystem to perform secure data retrieval, apply domain-specific business rules, and generate human-auditable justifications. The results confirm that this approach creates a robust, evaluable, and trustworthy system for deploying LLM-powered agents in data-sensitive, high-stakes domains.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TENET: Leveraging Tests Beyond Validation for Code Generation</title>
<link>https://arxiv.org/abs/2509.24148</link>
<guid>https://arxiv.org/abs/2509.24148</guid>
<content:encoded><![CDATA[
arXiv:2509.24148v1 Announce Type: new 
Abstract: Test-Driven Development (TDD) is a widely adopted software engineering practice that requires developers to create and execute tests alongside code implementation, ensuring that software behavior is continuously validated and refined. In the era of vibe coding, where developers increasingly delegate code writing to large language models (LLMs) by specifying high-level intentions, TDD becomes even more crucial, as test cases serve as executable specifications that explicitly define and verify intended functionality beyond what natural-language descriptions and code context can convey. While vibe coding under TDD is promising, there are three main challenges: (1) selecting a small yet effective test suite to improve the generation accuracy and control the execution workload, (2) retrieving context such as relevant code effectively, and (3) systematically using test feedback for effective code refinement. To address these challenges, we introduce TENET, an LLM agent for generating functions in complex real-world repositories under the TDD setting. TENET features three components: (1) a novel test harness mechanism that selects a concise test suite to maximize diversity of target usage scenarios; (2) a tailored agent toolset that performs efficient retrieval of relevant code with interactive debugging; and (3) a reflection-based refinement workflow that iteratively analyzes failures, replenishes context, and applies code refinement. TENET achieves 69.08% and 81.77% Pass@1 on RepoCod and RepoEval benchmarks, outperforming the best agentic baselines by 9.49 and 2.17 percentage points, respectively. In addition, this is the first study of test-driven code generation with repository-level context, examining how different aspects of test suites affect the performance of LLM agents under the TDD setting.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-augmented GUI Agents with Generative Guidelines</title>
<link>https://arxiv.org/abs/2509.24183</link>
<guid>https://arxiv.org/abs/2509.24183</guid>
<content:encoded><![CDATA[
arXiv:2509.24183v1 Announce Type: new 
Abstract: GUI agents powered by vision-language models (VLMs) show promise in automating complex digital tasks. However, their effectiveness in real-world applications is often limited by scarce training data and the inherent complexity of these tasks, which frequently require long-tailed knowledge covering rare, unseen scenarios. We propose RAG-GUI , a lightweight VLM that leverages web tutorials at inference time. RAG-GUI is first warm-started via supervised finetuning (SFT) and further refined through self-guided rejection sampling finetuning (RSF). Designed to be model-agnostic, RAG-GUI functions as a generic plug-in that enhances any VLM-based agent. Evaluated across three distinct tasks, it consistently outperforms baseline agents and surpasses other inference baselines by 2.6% to 13.3% across two model sizes, demonstrating strong generalization and practical plug-and-play capabilities in real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient 3D Latent Diffusion Model for T1-contrast Enhanced MRI Generation</title>
<link>https://arxiv.org/abs/2509.24194</link>
<guid>https://arxiv.org/abs/2509.24194</guid>
<content:encoded><![CDATA[
arXiv:2509.24194v1 Announce Type: new 
Abstract: Objective: Gadolinium-based contrast agents (GBCAs) are commonly employed with T1w MRI to enhance lesion visualization but are restricted in patients at risk of nephrogenic systemic fibrosis and variations in GBCA administration can introduce imaging inconsistencies. This study develops an efficient 3D deep-learning framework to generate T1-contrast enhanced images (T1C) from pre-contrast multiparametric MRI. Approach: We propose the 3D latent rectified flow (T1C-RFlow) model for generating high-quality T1C images. First, T1w and T2-FLAIR images are input into a pretrained autoencoder to acquire an efficient latent space representation. A rectified flow diffusion model is then trained in this latent space representation. The T1C-RFlow model was trained on a curated dataset comprised of the BraTS 2024 glioma (GLI; 1480 patients), meningioma (MEN; 1141 patients), and metastases (MET; 1475 patients) datasets. Selected patients were split into train (N=2860), validation (N=612), and test (N=614) sets. Results: Both qualitative and quantitative results demonstrate that the T1C-RFlow model outperforms benchmark 3D models (pix2pix, DDPM, Diffusion Transformers (DiT-3D)) trained in the same latent space. T1C-RFlow achieved the following metrics - GLI: NMSE 0.044 +/- 0.047, SSIM 0.935 +/- 0.025; MEN: NMSE 0.046 +/- 0.029, SSIM 0.937 +/- 0.021; MET: NMSE 0.098 +/- 0.088, SSIM 0.905 +/- 0.082. T1C-RFlow had the best tumor reconstruction performance and significantly faster denoising times (6.9 s/volume, 200 steps) than conventional DDPM models in both latent space (37.7s, 1000 steps) and patch-based in image space (4.3 hr/volume). Significance: Our proposed method generates synthetic T1C images that closely resemble ground truth T1C in much less time than previous diffusion models. Further development may permit a practical method for contrast-agent-free MRI for brain tumors.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Guided Policy Search for Non-Cooperative Dynamic Games</title>
<link>https://arxiv.org/abs/2509.24226</link>
<guid>https://arxiv.org/abs/2509.24226</guid>
<content:encoded><![CDATA[
arXiv:2509.24226v1 Announce Type: new 
Abstract: Multi-agent reinforcement learning (MARL) optimizes strategic interactions in non-cooperative dynamic games, where agents have misaligned objectives. However, data-driven methods such as multi-agent policy gradients (MA-PG) often suffer from instability and limit-cycle behaviors. Prior stabilization techniques typically rely on entropy-based exploration, which slows learning and increases variance. We propose a model-based approach that incorporates approximate priors into the reward function as regularization. In linear quadratic (LQ) games, we prove that such priors stabilize policy gradients and guarantee local exponential convergence to an approximate Nash equilibrium. We then extend this idea to infinite-horizon nonlinear games by introducing Multi-agent Guided Policy Search (MA-GPS), which constructs short-horizon local LQ approximations from trajectories of current policies to guide training. Experiments on nonlinear vehicle platooning and a six-player strategic basketball formation show that MA-GPS achieves faster convergence and more stable learning than existing MARL methods.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELHPlan: Efficient Long-Horizon Task Planning for Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2509.24230</link>
<guid>https://arxiv.org/abs/2509.24230</guid>
<content:encoded><![CDATA[
arXiv:2509.24230v1 Announce Type: new 
Abstract: Large Language Models (LLMs) enable intelligent multi-robot collaboration but face fundamental trade-offs: declarative methods lack adaptability in dynamic environments, while iterative methods incur prohibitive computational costs that scale poorly with team size and task complexity. In this paper, we propose ELHPlan, a novel framework that introduces Action Chains--sequences of actions explicitly bound to sub-goal intentions--as the fundamental planning primitive. ELHPlan operates via a cyclical process: 1) constructing intention-bound action sequences, 2) proactively validating for conflicts and feasibility, 3) refining issues through targeted mechanisms, and 4) executing validated actions. This design balances adaptability and efficiency by providing sufficient planning horizons while avoiding expensive full re-planning. We further propose comprehensive efficiency metrics, including token consumption and planning time, to more holistically evaluate multi-agent collaboration. Our experiments on benchmark TDW-MAT and C-WAH demonstrate that ELHPlan achieves comparable task success rates while consuming only 24% of the tokens required by state-of-the-art methods. Our research establishes a new efficiency-effectiveness frontier for LLM-based multi-agent planning systems.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Takedown: How It's Done in Modern Coding Agent Exploits</title>
<link>https://arxiv.org/abs/2509.24240</link>
<guid>https://arxiv.org/abs/2509.24240</guid>
<content:encoded><![CDATA[
arXiv:2509.24240v1 Announce Type: new 
Abstract: Coding agents, which are LLM-driven agents specialized in software development, have become increasingly prevalent in modern programming environments. Unlike traditional AI coding assistants, which offer simple code completion and suggestions, modern coding agents tackle more complex tasks with greater autonomy, such as generating entire programs from natural language instructions. To enable such capabilities, modern coding agents incorporate extensive functionalities, which in turn raise significant concerns over their security and privacy. Despite their growing adoption, systematic and in-depth security analysis of these agents has largely been overlooked.
  In this paper, we present a comprehensive security analysis of eight real-world coding agents. Our analysis addresses the limitations of prior approaches, which were often fragmented and ad hoc, by systematically examining the internal workflows of coding agents and identifying security threats across their components. Through the analysis, we identify 15 security issues, including previously overlooked or missed issues, that can be abused to compromise the confidentiality and integrity of user systems. Furthermore, we show that these security issues are not merely individual vulnerabilities, but can collectively lead to end-to-end exploitations. By leveraging these security issues, we successfully achieved arbitrary command execution in five agents and global data exfiltration in four agents, all without any user interaction or approval. Our findings highlight the need for a comprehensive security analysis in modern LLM-driven agents and demonstrate how insufficient security considerations can lead to severe vulnerabilities.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAME-AI: Patient Messaging Creation and Optimization using Agentic AI</title>
<link>https://arxiv.org/abs/2509.24263</link>
<guid>https://arxiv.org/abs/2509.24263</guid>
<content:encoded><![CDATA[
arXiv:2509.24263v1 Announce Type: new 
Abstract: Messaging patients is a critical part of healthcare communication, helping to improve things like medication adherence and healthy behaviors. However, traditional mobile message design has significant limitations due to its inability to explore the high-dimensional design space. We develop PAME-AI, a novel approach for Patient Messaging Creation and Optimization using Agentic AI. Built on the Data-Information-Knowledge-Wisdom (DIKW) hierarchy, PAME-AI offers a structured framework to move from raw data to actionable insights for high-performance messaging design. PAME-AI is composed of a system of specialized computational agents that progressively transform raw experimental data into actionable message design strategies. We demonstrate our approach's effectiveness through a two-stage experiment, comprising of 444,691 patient encounters in Stage 1 and 74,908 in Stage 2. The best-performing generated message achieved 68.76% engagement compared to the 61.27% baseline, representing a 12.2\% relative improvement in click-through rates. This agentic architecture enables parallel processing, hypothesis validation, and continuous learning, making it particularly suitable for large-scale healthcare communication optimization.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When MCP Servers Attack: Taxonomy, Feasibility, and Mitigation</title>
<link>https://arxiv.org/abs/2509.24272</link>
<guid>https://arxiv.org/abs/2509.24272</guid>
<content:encoded><![CDATA[
arXiv:2509.24272v1 Announce Type: new 
Abstract: Model Context Protocol (MCP) servers enable AI applications to connect to external systems in a plug-and-play manner, but their rapid proliferation also introduces severe security risks. Unlike mature software ecosystems with rigorous vetting, MCP servers still lack standardized review mechanisms, giving adversaries opportunities to distribute malicious implementations. Despite this pressing risk, the security implications of MCP servers remain underexplored. To address this gap, we present the first systematic study that treats MCP servers as active threat actors and decomposes them into core components to examine how adversarial developers can implant malicious intent. Specifically, we investigate three research questions: (i) what types of attacks malicious MCP servers can launch, (ii) how vulnerable MCP hosts and Large Language Models (LLMs) are to these attacks, and (iii) how feasible it is to carry out MCP server attacks in practice. Our study proposes a component-based taxonomy comprising twelve attack categories. For each category, we develop Proof-of-Concept (PoC) servers and demonstrate their effectiveness across diverse real-world host-LLM settings. We further show that attackers can generate large numbers of malicious servers at virtually no cost. We then test state-of-the-art scanners on the generated servers and found that existing detection approaches are insufficient. These findings highlight that malicious MCP servers are easy to implement, difficult to detect with current tools, and capable of causing concrete damage to AI agent systems. Addressing this threat requires coordinated efforts among protocol designers, host developers, LLM providers, and end users to build a more secure and resilient MCP ecosystem.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Reinforcement Learning Framework for ESP Cheater Simulation</title>
<link>https://arxiv.org/abs/2509.24274</link>
<guid>https://arxiv.org/abs/2509.24274</guid>
<content:encoded><![CDATA[
arXiv:2509.24274v1 Announce Type: new 
Abstract: Extra-Sensory Perception (ESP) cheats, which reveal hidden in-game information such as enemy locations, are difficult to detect because their effects are not directly observable in player behavior. The lack of observable evidence makes it difficult to collect reliably labeled data, which is essential for training effective anti-cheat systems. Furthermore, cheaters often adapt their behavior by limiting or disguising their cheat usage, which further complicates detection and detector development. To address these challenges, we propose a simulation framework for controlled modeling of ESP cheaters, non-cheaters, and trajectory-based detectors. We model cheaters and non-cheaters as reinforcement learning agents with different levels of observability, while detectors classify their behavioral trajectories. Next, we formulate the interaction between the cheater and the detector as an adversarial game, allowing both players to co-adapt over time. To reflect realistic cheater strategies, we introduce a structured cheater model that dynamically switches between cheating and non-cheating behaviors based on detection risk. Experiments demonstrate that our framework successfully simulates adaptive cheater behaviors that strategically balance reward optimization and detection evasion. This work provides a controllable and extensible platform for studying adaptive cheating behaviors and developing effective cheat detectors.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>G-reasoner: Foundation Models for Unified Reasoning over Graph-structured Knowledge</title>
<link>https://arxiv.org/abs/2509.24276</link>
<guid>https://arxiv.org/abs/2509.24276</guid>
<content:encoded><![CDATA[
arXiv:2509.24276v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at complex reasoning but remain limited by static and incomplete parametric knowledge. Retrieval-augmented generation (RAG) mitigates this by incorporating external knowledge, yet existing RAGs struggle with knowledge-intensive tasks due to fragmented information and weak modeling of knowledge structure. Graphs offer a natural way to model relationships within knowledge, but LLMs are inherently unstructured and cannot effectively reason over graph-structured data. Recent graph-enhanced RAG (GraphRAG) attempts to bridge this gap by constructing tailored graphs and enabling LLMs to reason on them. However, these methods often depend on ad-hoc graph designs, heuristic search, or costly agent pipelines, which hinder scalability and generalization. To address these challenges, we present G-reasoner, a unified framework that integrates graph and language foundation models for reasoning over diverse graph-structured knowledge. Central to our approach is QuadGraph, a standardized four-layer abstraction that unifies heterogeneous knowledge sources into a common graph representation. Building on this, we introduce a 34M-parameter graph foundation model (GFM) that jointly captures graph topology and textual semantics, and is integrated with LLMs to enhance reasoning in downstream applications. To ensure scalability and efficiency, mixed-precision training and distributed message-passing are implemented to scale GFM with more GPUs. Extensive experiments on six benchmarks show that G-reasoner consistently outperforms state-of-the-art baselines, significantly enhances LLM reasoning, and achieves strong efficiency and cross-graph generalization.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimuHome: A Temporal- and Environment-Aware Benchmark for Smart Home LLM Agents</title>
<link>https://arxiv.org/abs/2509.24282</link>
<guid>https://arxiv.org/abs/2509.24282</guid>
<content:encoded><![CDATA[
arXiv:2509.24282v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents excel at multi-step, tool-augmented tasks. However, smart homes introduce distinct challenges, requiring agents to handle latent user intents, temporal dependencies, device constraints, scheduling, and more. The main bottlenecks for developing smart home agents with such capabilities include the lack of a realistic simulation environment where agents can interact with devices and observe the results, as well as a challenging benchmark to evaluate them. To address this, we introduce $\textbf{SimuHome}$, a time-accelerated home environment that simulates smart devices, supports API calls, and reflects changes in environmental variables. By building the simulator on the Matter protocol (the global industry standard for smart home communication), SimuHome provides a high-fidelity environment, and agents validated in SimuHome can be deployed on real Matter-compliant devices with minimal adaptation. We provide a challenging benchmark of 600 episodes across twelve user query types that require the aforementioned capabilities. Our evaluation of 11 agents under a unified ReAct framework reveals that while models perform well on simple tasks, they struggle with latent intent inference, state verification, and especially temporal scheduling. Even the top-performing model, GPT-4.1, reaches only 54% success rate. These findings highlight a critical need for methods that can reliably verify the current state via tools before acting and coordinate time-dependent actions.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Q-Mirror: Unlocking the Multi-Modal Potential of Scientific Text-Only QA Pairs</title>
<link>https://arxiv.org/abs/2509.24297</link>
<guid>https://arxiv.org/abs/2509.24297</guid>
<content:encoded><![CDATA[
arXiv:2509.24297v1 Announce Type: new 
Abstract: High-quality, multi-modal benchmarks are crucial for advancing scientific reasoning in large models yet their manual creation is costly and unscalable. To address this bottleneck, we explore the potential for transforming Text-Only QA Pairs (TQAs) into high-quality Multi-Modal QA Pairs (MMQAs), which include three parts: 1) Task Definition \& Evaluation Rubric: We develop a TQA-to-MMQA framework and establish a comprehensive, multi-dimensional MMQA quality rubric that provides principles for the transformation. 2) Benchmark Construction: Then we construct two extensive benchmarks to rigorously evaluate state-of-the-art generation \& understanding models on the distinct tasks of MMQA generation \& MMQA quality evaluation. 3) Preliminary Solution: We develop an agentic system (Q-Mirror), which operationalizes our framework by integrating MMQA generation and evaluation into a closed loop for iterative refinement. Our experiments show that while state-of-the-art models can generate MMQAs, their outputs still leave substantial gaps, underscoring the need for reliable evaluation. We further demonstrate that top-tier understanding models align closely with human judgment in MMQA quality assessment. Leveraging both insights, the Q-Mirror agent raises average scores from 78.90 to 85.22 and pass rates from 72\% to 95\%, offering a practical path to large-scale scientific benchmarks.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the behavior-neural gap: A multimodal AI reveals the brain's geometry of emotion more accurately than human self-reports</title>
<link>https://arxiv.org/abs/2509.24298</link>
<guid>https://arxiv.org/abs/2509.24298</guid>
<content:encoded><![CDATA[
arXiv:2509.24298v1 Announce Type: new 
Abstract: The ability to represent emotion plays a significant role in human cognition and social interaction, yet the high-dimensional geometry of this affective space and its neural underpinnings remain debated. A key challenge, the `behavior-neural gap,' is the limited ability of human self-reports to predict brain activity. Here we test the hypothesis that this gap arises from the constraints of traditional rating scales and that large-scale similarity judgments can more faithfully capture the brain's affective geometry. Using AI models as `cognitive agents,' we collected millions of triplet odd-one-out judgments from a multimodal large language model (MLLM) and a language-only model (LLM) in response to 2,180 emotionally evocative videos. We found that the emergent 30-dimensional embeddings from these models are highly interpretable and organize emotion primarily along categorical lines, yet in a blended fashion that incorporates dimensional properties. Most remarkably, the MLLM's representation predicted neural activity in human emotion-processing networks with the highest accuracy, outperforming not only the LLM but also, counterintuitively, representations derived directly from human behavioral ratings. This result supports our primary hypothesis and suggests that sensory grounding--learning from rich visual data--is critical for developing a truly neurally-aligned conceptual framework for emotion. Our findings provide compelling evidence that MLLMs can autonomously develop rich, neurally-aligned affective representations, offering a powerful paradigm to bridge the gap between subjective experience and its neural substrates. Project page: https://reedonepeck.github.io/ai-emotion.github.io/.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Sample: Reinforcement Learning-Guided Sampling for Autonomous Vehicle Motion Planning</title>
<link>https://arxiv.org/abs/2509.24313</link>
<guid>https://arxiv.org/abs/2509.24313</guid>
<content:encoded><![CDATA[
arXiv:2509.24313v1 Announce Type: new 
Abstract: Sampling-based motion planning is a well-established approach in autonomous driving, valued for its modularity and analytical tractability. In complex urban scenarios, however, uniform or heuristic sampling often produces many infeasible or irrelevant trajectories. We address this limitation with a hybrid framework that learns where to sample while keeping trajectory generation and evaluation fully analytical and verifiable. A reinforcement learning (RL) agent guides the sampling process toward regions of the action space likely to yield feasible trajectories, while evaluation and final selection remains governed by deterministic feasibility checks and cost functions. We couple the RL sampler with a world model (WM) based on a decodable deep set encoder, enabling both variable numbers of traffic participants and reconstructable latent representations. The approach is evaluated in the CommonRoad simulation environment, showing up to 99% fewer required samples and a runtime reduction of up to 84% while maintaining planning quality in terms of success and collision-free rates. These improvements lead to faster, more reliable decision-making for autonomous vehicles in urban environments, achieving safer and more responsive navigation under real-world constraints. Code and trained artifacts are publicly available at: https://github.com/TUM-AVS/Learning-to-Sample
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedMMV: A Controllable Multimodal Multi-Agent Framework for Reliable and Verifiable Clinical Reasoning</title>
<link>https://arxiv.org/abs/2509.24314</link>
<guid>https://arxiv.org/abs/2509.24314</guid>
<content:encoded><![CDATA[
arXiv:2509.24314v1 Announce Type: new 
Abstract: Recent progress in multimodal large language models (MLLMs) has demonstrated promising performance on medical benchmarks and in preliminary trials as clinical assistants. Yet, our pilot audit of diagnostic cases uncovers a critical failure mode: instability in early evidence interpretation precedes hallucination, creating branching reasoning trajectories that cascade into globally inconsistent conclusions. This highlights the need for clinical reasoning agents that constrain stochasticity and hallucination while producing auditable decision flows. We introduce MedMMV, a controllable multimodal multi-agent framework for reliable and verifiable clinical reasoning. MedMMV stabilizes reasoning through diversified short rollouts, grounds intermediate steps in a structured evidence graph under the supervision of a Hallucination Detector, and aggregates candidate paths with a Combined Uncertainty scorer. On six medical benchmarks, MedMMV improves accuracy by up to 12.7% and, more critically, demonstrates superior reliability. Blind physician evaluations confirm that MedMMV substantially increases reasoning truthfulness without sacrificing informational content. By controlling instability through a verifiable, multi-agent process, our framework provides a robust path toward deploying trustworthy AI systems in high-stakes domains like clinical decision support.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAS$^2$: Self-Generative, Self-Configuring, Self-Rectifying Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2509.24323</link>
<guid>https://arxiv.org/abs/2509.24323</guid>
<content:encoded><![CDATA[
arXiv:2509.24323v1 Announce Type: new 
Abstract: The past two years have witnessed the meteoric rise of Large Language Model (LLM)-powered multi-agent systems (MAS), which harness collective intelligence and exhibit a remarkable trajectory toward self-evolution. This paradigm has rapidly progressed from manually engineered systems that require bespoke configuration of prompts, tools, roles, and communication protocols toward frameworks capable of automated orchestration. Yet, dominant automatic multi-agent systems, whether generated by external modules or a single LLM agent, largely adhere to a rigid ``\textit{generate-once-and-deploy}'' paradigm, rendering the resulting systems brittle and ill-prepared for the dynamism and uncertainty of real-world environments. To transcend this limitation, we introduce MAS$^2$, a paradigm predicated on the principle of recursive self-generation: a multi-agent system that autonomously architects bespoke multi-agent systems for diverse problems. Technically, we devise a ``\textit{generator-implementer-rectifier}'' tri-agent team capable of dynamically composing and adaptively rectifying a target agent system in response to real-time task demands. Collaborative Tree Optimization is proposed to train and specialize these meta-agents. Extensive evaluation across seven benchmarks reveals that MAS$^2$ achieves performance gains of up to $19.6\%$ over state-of-the-art MAS in complex scenarios such as deep research and code generation. Moreover, MAS$^2$ exhibits superior cross-backbone generalization, effectively leveraging previously unseen LLMs to yield improvements of up to $15.1\%$. Crucially, these gains are attained without incurring excessive token costs, as MAS$^2$ consistently resides on the Pareto frontier of cost-performance trade-offs. The source codes are available at https://github.com/yeyeyeah2/MAS2.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Open-Source and Commercial LLMs for Domain-Specific Analysis and Reporting: Software Engineering Challenges and Design Trade-offs</title>
<link>https://arxiv.org/abs/2509.24344</link>
<guid>https://arxiv.org/abs/2509.24344</guid>
<content:encoded><![CDATA[
arXiv:2509.24344v1 Announce Type: new 
Abstract: Context: Large Language Models (LLMs) enable automation of complex natural language processing across domains, but research on domain-specific applications like Finance remains limited. Objectives: This study explored open-source and commercial LLMs for financial report analysis and commentary generation, focusing on software engineering challenges in implementation. Methods: Using Design Science Research methodology, an exploratory case study iteratively designed and evaluated two LLM-based systems: one with local open-source models in a multi-agent workflow, another using commercial GPT-4o. Both were assessed through expert evaluation of real-world financial reporting use cases. Results: LLMs demonstrated strong potential for automating financial reporting tasks, but integration presented significant challenges. Iterative development revealed issues including prompt design, contextual dependency, and implementation trade-offs. Cloud-based models offered superior fluency and usability but raised data privacy and external dependency concerns. Local open-source models provided better data control and compliance but required substantially more engineering effort for reliability and usability. Conclusion: LLMs show strong potential for financial reporting automation, but successful integration requires careful attention to architecture, prompt design, and system reliability. Implementation success depends on addressing domain-specific challenges through tailored validation mechanisms and engineering strategies that balance accuracy, control, and compliance.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Orchestration of Multi-Agent System for Real-World Multi-Image Agricultural VQA</title>
<link>https://arxiv.org/abs/2509.24350</link>
<guid>https://arxiv.org/abs/2509.24350</guid>
<content:encoded><![CDATA[
arXiv:2509.24350v1 Announce Type: new 
Abstract: Agricultural visual question answering is essential for providing farmers and researchers with accurate and timely knowledge. However, many existing approaches are predominantly developed for evidence-constrained settings such as text-only queries or single-image cases. This design prevents them from coping with real-world agricultural scenarios that often require multi-image inputs with complementary views across spatial scales, and growth stages. Moreover, limited access to up-to-date external agricultural context makes these systems struggle to adapt when evidence is incomplete. In addition, rigid pipelines often lack systematic quality control. To address this gap, we propose a self-reflective and self-improving multi-agent framework that integrates four roles, the Retriever, the Reflector, the Answerer, and the Improver. They collaborate to enable context enrichment, reflective reasoning, answer drafting, and iterative improvement.
  A Retriever formulates queries and gathers external information, while a Reflector assesses adequacy and triggers sequential reformulation and renewed retrieval. Two Answerers draft candidate responses in parallel to reduce bias. The Improver refines them through iterative checks while ensuring that information from multiple images is effectively aligned and utilized. Experiments on the AgMMU benchmark show that our framework achieves competitive performance on multi-image agricultural QA.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Services Computing</title>
<link>https://arxiv.org/abs/2509.24380</link>
<guid>https://arxiv.org/abs/2509.24380</guid>
<content:encoded><![CDATA[
arXiv:2509.24380v1 Announce Type: new 
Abstract: The rise of LLM-powered agents is driving a fundamental transformation in services computing: from static, request-response functions to dynamic, goal-oriented, and autonomous multi-agent ecosystems. In response to this shift, we introduce Agentic Service Computing (ASC), a new paradigm that reimagines services as intelligent, self-adaptive, and socially embedded entities. This comprehensive survey presents a lifecycle-driven framework for ASC, structured around four core phases: Design, Deployment, Operation, and Evolution. We systematically analyze ASC through four foundational research dimensions: (1) Perception, Context, and Environment Modeling, (2) Autonomous Decision-Making and Task Execution, (3) Multi-Agent Collaboration and Organization, and (4) Evaluation, Value Alignment, and Trustworthiness. We examine how these dimensions are instantiated, integrated, and continuously adapted across the service lifecycle. Our synthesis reveals that agentic services are not merely assembled but orchestrated: contextual awareness enables robust deployment; autonomous reasoning supports real-time operation; collaborative structures emerge and evolve through interaction; and trustworthiness must be upheld as a cross-cutting, lifelong imperative. We further identify and discuss emerging trends shaping the future of ASC. By integrating classical principles of services computing with advances in LLM-based multi-agent systems, this work establishes a holistic and forward-looking foundation for ASC. It provides a unified reference for researchers and practitioners aiming to develop adaptive, accountable, and human-centered intelligent services.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaNav: Adaptive Reasoning with Uncertainty for Vision-Language Navigation</title>
<link>https://arxiv.org/abs/2509.24387</link>
<guid>https://arxiv.org/abs/2509.24387</guid>
<content:encoded><![CDATA[
arXiv:2509.24387v1 Announce Type: new 
Abstract: Vision Language Navigation (VLN) requires agents to follow natural language instructions by grounding them in sequential visual observations over long horizons. Explicit reasoning could enhance temporal consistency and perception action alignment, but reasoning at fixed steps often leads to suboptimal performance and unnecessary computation. To address this, we propose AdaNav, an uncertainty-based adaptive reasoning framework for VLN. At its core is the Uncertainty Adaptive Reasoning Block (UAR), a lightweight plugin that dynamically triggers reasoning. We introduce Action Entropy as a policy prior for UAR and progressively refine it through a Heuristics to RL training method, enabling agents to learn difficulty aware reasoning policies under the strict data limitations of embodied tasks. Results show that with only 6K training samples, AdaNav achieves substantial gains over closed source models trained on million scale data, improving success rate by 20% on R2R val-unseen, 11.7% on RxR-CE, and 11.4% in real world scenes. The code is available at https://github.com/xinding-sys/AdaNav.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLaDA-MoE: A Sparse MoE Diffusion Language Model</title>
<link>https://arxiv.org/abs/2509.24389</link>
<guid>https://arxiv.org/abs/2509.24389</guid>
<content:encoded><![CDATA[
arXiv:2509.24389v1 Announce Type: new 
Abstract: We introduce LLaDA-MoE, a large language diffusion model with the Mixture-of-Experts (MoE) architecture, trained from scratch on approximately 20T tokens. LLaDA-MoE achieves competitive performance with significantly reduced computational overhead by maintaining a 7B-parameter capacity while activating only 1.4B parameters during inference. Our empirical evaluation reveals that LLaDA-MoE achieves state-of-the-art performance among diffusion language models with larger parameters, surpassing previous diffusion language models LLaDA, LLaDA 1.5, and Dream across multiple benchmarks. The instruct-tuned model LLaDA-MoE-7B-A1B-Instruct demonstrates capabilities comparable to Qwen2.5-3B-Instruct in knowledge understanding, code generation, mathematical reasoning, agent and alignment tasks, despite using fewer active parameters. Our results show that integrating a sparse MoE architecture into the training objective of masked diffusion language models still brings out MoE's strengths under efficient inference with few active parameters, and opens ample room for further exploration of diffusion language models. LLaDA-MoE models are available at Huggingface.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Detection and Coverage of Unknown Target Areas by Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2509.24399</link>
<guid>https://arxiv.org/abs/2509.24399</guid>
<content:encoded><![CDATA[
arXiv:2509.24399v1 Announce Type: new 
Abstract: This paper presents a novel coverage control algorithm for multi-agent systems, where each agent has no prior knowledge of the specific region to be covered. The proposed method enables agents to autonomously detect the target area and collaboratively achieve full coverage. Once an agent detects a part of the target region within its sensor range, a dynamically constructed density function is generated to attract nearby agents. By integrating this density-driven mechanism with Centroidal Voronoi Tessellation (CVT), the agents are guided to achieve optimal spatial distribution. Additionally, Control Barrier Functions (CBFs) are employed to ensure collision avoidance and maintain non-overlapping sensor coverage, enhancing both safety and efficiency. Simulation results verify that agents can independently locate and effectively cover the target area.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents</title>
<link>https://arxiv.org/abs/2509.24405</link>
<guid>https://arxiv.org/abs/2509.24405</guid>
<content:encoded><![CDATA[
arXiv:2509.24405v1 Announce Type: new 
Abstract: Text-to-SQL enables natural access to databases, yet most benchmarks are English-only, limiting multilingual progress. We introduce MultiSpider 2.0, extending Spider 2.0 to eight languages (English, German, French, Spanish, Portuguese, Japanese, Chinese, Vietnamese). It preserves Spider 2.0's structural difficulty while adding linguistic and dialectal variability, demanding deeper reasoning for complex SQL. On this benchmark, state-of-the-art LLMs (such as DeepSeek-R1 and OpenAI o1) reach only 4\% execution accuracy when relying on intrinsic reasoning, versus 60\% on MultiSpider 1.0. Therefore, we provide a collaboration-driven language agents baseline that iteratively refines queries, improving accuracy to 15\%. These results reveal a substantial multilingual gap and motivate methods that are robust across languages and ready for real-world enterprise deployment. Our benchmark is available at https://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FuncPoison: Poisoning Function Library to Hijack Multi-agent Autonomous Driving Systems</title>
<link>https://arxiv.org/abs/2509.24408</link>
<guid>https://arxiv.org/abs/2509.24408</guid>
<content:encoded><![CDATA[
arXiv:2509.24408v1 Announce Type: new 
Abstract: Autonomous driving systems increasingly rely on multi-agent architectures powered by large language models (LLMs), where specialized agents collaborate to perceive, reason, and plan. A key component of these systems is the shared function library, a collection of software tools that agents use to process sensor data and navigate complex driving environments. Despite its critical role in agent decision-making, the function library remains an under-explored vulnerability. In this paper, we introduce FuncPoison, a novel poisoning-based attack targeting the function library to manipulate the behavior of LLM-driven multi-agent autonomous systems. FuncPoison exploits two key weaknesses in how agents access the function library: (1) agents rely on text-based instructions to select tools; and (2) these tools are activated using standardized command formats that attackers can replicate. By injecting malicious tools with deceptive instructions, FuncPoison manipulates one agent s decisions--such as misinterpreting road conditions--triggering cascading errors that mislead other agents in the system. We experimentally evaluate FuncPoison on two representative multi-agent autonomous driving systems, demonstrating its ability to significantly degrade trajectory accuracy, flexibly target specific agents to induce coordinated misbehavior, and evade diverse defense mechanisms. Our results reveal that the function library, often considered a simple toolset, can serve as a critical attack surface in LLM-based autonomous driving systems, raising elevated concerns on their reliability.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Agent-Based Framework for Automated Higher-Voice Harmony Generation</title>
<link>https://arxiv.org/abs/2509.24463</link>
<guid>https://arxiv.org/abs/2509.24463</guid>
<content:encoded><![CDATA[
arXiv:2509.24463v1 Announce Type: new 
Abstract: The generation of musically coherent and aesthetically pleasing harmony remains a significant challenge in the field of algorithmic composition. This paper introduces an innovative Agentic AI-enabled Higher Harmony Music Generator, a multi-agent system designed to create harmony in a collaborative and modular fashion. Our framework comprises four specialized agents: a Music-Ingestion Agent for parsing and standardizing input musical scores; a Chord-Knowledge Agent, powered by a Chord-Former (Transformer model), to interpret and provide the constituent notes of complex chord symbols; a Harmony-Generation Agent, which utilizes a Harmony-GPT and a Rhythm-Net (RNN) to compose a melodically and rhythmically complementary harmony line; and an Audio-Production Agent that employs a GAN-based Symbolic-to-Audio Synthesizer to render the final symbolic output into high-fidelity audio. By delegating specific tasks to specialized agents, our system effectively mimics the collaborative process of human musicians. This modular, agent-based approach allows for robust data processing, deep theoretical understanding, creative composition, and realistic audio synthesis, culminating in a system capable of generating sophisticated and contextually appropriate higher-voice harmonies for given melodies.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Shift-Up: A Framework and a Prestudy on High-Value Activities in GenAI Native Software Development</title>
<link>https://arxiv.org/abs/2509.24485</link>
<guid>https://arxiv.org/abs/2509.24485</guid>
<content:encoded><![CDATA[
arXiv:2509.24485v1 Announce Type: new 
Abstract: Generative AI (GenAI) has significantly influenced software engineering. Associated tools have created a shift in software engineering, where specialized agents, based on user-provided prompts, are replacing human developers. In this paper, we propose a framework for GenAI native development that we call \textit{shift-up}, which helps software teams focus on high-value work while being supported by GenAI. Furthermore, we also present a preliminary study testing these ideas with current GenAI tools. Towards the end of the paper, we propose future research goals to study shift-up in more detail.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Specification Generator for Move Programs</title>
<link>https://arxiv.org/abs/2509.24515</link>
<guid>https://arxiv.org/abs/2509.24515</guid>
<content:encoded><![CDATA[
arXiv:2509.24515v1 Announce Type: new 
Abstract: While LLM-based specification generation is gaining traction, existing tools primarily focus on mainstream programming languages like C, Java, and even Solidity, leaving emerging and yet verification-oriented languages like Move underexplored. In this paper, we introduce MSG, an automated specification generation tool designed for Move smart contracts. MSG aims to highlight key insights that uniquely present when applying LLM-based specification generation to a new ecosystem. Specifically, MSG demonstrates that LLMs exhibit robust code comprehension and generation capabilities even for non-mainstream languages. MSG successfully generates verifiable specifications for 84% of tested Move functions and even identifies clauses previously overlooked by experts. Additionally, MSG shows that explicitly leveraging specification language features through an agentic, modular design improves specification quality substantially (generating 57% more verifiable clauses than conventional designs). Incorporating feedback from the verification toolchain further enhances the effectiveness of MSG, leading to a 30% increase in generated verifiable specifications.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhysiAgent: An Embodied Agent Framework in Physical World</title>
<link>https://arxiv.org/abs/2509.24524</link>
<guid>https://arxiv.org/abs/2509.24524</guid>
<content:encoded><![CDATA[
arXiv:2509.24524v1 Announce Type: new 
Abstract: Vision-Language-Action (VLA) models have achieved notable success but often struggle with limited generalizations. To address this, integrating generalized Vision-Language Models (VLMs) as assistants to VLAs has emerged as a popular solution. However, current approaches often combine these models in rigid, sequential structures: using VLMs primarily for high-level scene understanding and task planning, and VLAs merely as executors of lower-level actions, leading to ineffective collaboration and poor grounding challenges. In this paper, we propose an embodied agent framework, PhysiAgent, tailored to operate effectively in physical environments. By incorporating monitor, memory, self-reflection mechanisms, and lightweight off-the-shelf toolboxes, PhysiAgent offers an autonomous scaffolding framework to prompt VLMs to organize different components based on real-time proficiency feedback from VLAs to maximally exploit VLAs' capabilities. Experimental results demonstrate significant improvements in task-solving performance on complex real-world robotic tasks, showcasing effective self-regulation of VLMs, coherent tool collaboration, and adaptive evolution of the framework during execution. PhysiAgent makes practical and pioneering efforts to integrate VLMs and VLAs, effectively grounding embodied agent frameworks in real-world settings.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Agents Inside of Scalable World Models</title>
<link>https://arxiv.org/abs/2509.24527</link>
<guid>https://arxiv.org/abs/2509.24527</guid>
<content:encoded><![CDATA[
arXiv:2509.24527v1 Announce Type: new 
Abstract: World models learn general knowledge from videos and simulate experience for training behaviors in imagination, offering a path towards intelligent agents. However, previous world models have been unable to accurately predict object interactions in complex environments. We introduce Dreamer 4, a scalable agent that learns to solve control tasks by reinforcement learning inside of a fast and accurate world model. In the complex video game Minecraft, the world model accurately predicts object interactions and game mechanics, outperforming previous world models by a large margin. The world model achieves real-time interactive inference on a single GPU through a shortcut forcing objective and an efficient transformer architecture. Moreover, the world model learns general action conditioning from only a small amount of data, allowing it to extract the majority of its knowledge from diverse unlabeled videos. We propose the challenge of obtaining diamonds in Minecraft from only offline data, aligning with practical applications such as robotics where learning from environment interaction can be unsafe and slow. This task requires choosing sequences of over 20,000 mouse and keyboard actions from raw pixels. By learning behaviors in imagination, Dreamer 4 is the first agent to obtain diamonds in Minecraft purely from offline data, without environment interaction. Our work provides a scalable recipe for imagination training, marking a step towards intelligent agents.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning in Action: Real-Time Control of Vortex-Induced Vibrations</title>
<link>https://arxiv.org/abs/2509.24556</link>
<guid>https://arxiv.org/abs/2509.24556</guid>
<content:encoded><![CDATA[
arXiv:2509.24556v1 Announce Type: new 
Abstract: This study showcases an experimental deployment of deep reinforcement learning (DRL) for active flow control (AFC) of vortex-induced vibrations (VIV) in a circular cylinder at a high Reynolds number (Re = 3000) using rotary actuation. Departing from prior work that relied on low-Reynolds-number numerical simulations, this research demonstrates real-time control in a challenging experimental setting, successfully addressing practical constraints such as actuator delay. When the learning algorithm is provided with state feedback alone (displacement and velocity of the oscillating cylinder), the DRL agent learns a low-frequency rotary control strategy that achieves up to 80% vibration suppression which leverages the traditional lock-on phenomenon. While this level of suppression is significant, it remains below the performance achieved using high-frequency rotary actuation. The reduction in performance is attributed to actuation delays and can be mitigated by augmenting the learning algorithm with past control actions. This enables the agent to learn a high-frequency rotary control strategy that effectively modifies vortex shedding and achieves over 95% vibration attenuation. These results demonstrate the adaptability of DRL for AFC in real-world experiments and its ability to overcome instrumental limitations such as actuation lag.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Free Dynamic Consensus in Multi-Agent Systems: A Q-Function Perspective</title>
<link>https://arxiv.org/abs/2509.24598</link>
<guid>https://arxiv.org/abs/2509.24598</guid>
<content:encoded><![CDATA[
arXiv:2509.24598v1 Announce Type: new 
Abstract: This paper presents a new method for achieving dynamic consensus in linear discrete-time homogeneous multi-agent systems (MAS) with marginally stable or unstable dynamics. The guarantee of consensus in this setting involves a set of constraints based on the graph's spectral properties, complicating the design of the coupling gains. This challenge intensifies for large-scale systems with diverse graph Laplacian spectra. The proposed approach reformulates the dynamic consensus problem with a prescribed convergence rate using a state-action value function framework inspired by optimal control theory. Specifically, a synthetic linear quadratic regulation (LQR) formulation is introduced to encode the consensus objective, enabling its translation into a convex semidefinite programming (SDP) problem. The resulting SDP is applicable in both model-based and model-free settings for jointly designing the local feedback and coupling gains. To handle the inherent non-convex feasibility conditions, a convex-concave decomposition strategy is employed. Adaptation of the method in a completely model-free set-up eliminates the need for system identification or knowledge of the agents' dynamics. Instead, it relies on input-state data collection and offers an entirely data-driven equivalent SDP formulation. Finally, a new algorithm balancing feasibility, convergence rate, robustness, and energy efficiency, is established to provide design flexibility. Numerical simulations validate the method's effectiveness in various scenarios.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Stop replacing salt with sugar!'': Towards Intuitive Human-Agent Teaching</title>
<link>https://arxiv.org/abs/2509.24651</link>
<guid>https://arxiv.org/abs/2509.24651</guid>
<content:encoded><![CDATA[
arXiv:2509.24651v1 Announce Type: new 
Abstract: Humans quickly learn new concepts from a small number of examples. Replicating this capacity with Artificial Intelligence (AI) systems has proven to be challenging. When it comes to learning subjective tasks-where there is an evident scarcity of data-this capacity needs to be recreated. In this work, we propose an intuitive human-agent teaching architecture in which the human can teach an agent how to perform a task by providing demonstrations, i.e., examples. To have an intuitive interaction, we argue that the agent should be able to learn incrementally from a few single examples. To allow for this, our objective is to broaden the agent's task understanding using domain knowledge. Then, using a learning method to enable the agent to learn efficiently from a limited number of examples. Finally, to optimize how human can select the most representative and less redundant examples to provide the agent with. We apply our proposed method to the subjective task of ingredient substitution, where the agent needs to learn how to substitute ingredients in recipes based on human examples. We replicate human input using the Recipe1MSubs dataset. In our experiments, the agent achieves half its task performance after only 100 examples are provided, compared to the complete training set of 50k examples. We show that by providing examples in strategic order along with a learning method that leverages external symbolic knowledge, the agent can generalize more efficiently.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Successful Misunderstandings: Learning to Coordinate Without Being Understood</title>
<link>https://arxiv.org/abs/2509.24660</link>
<guid>https://arxiv.org/abs/2509.24660</guid>
<content:encoded><![CDATA[
arXiv:2509.24660v1 Announce Type: new 
Abstract: The main approach to evaluating communication is by assessing how well it facilitates coordination. If two or more individuals can coordinate through communication, it is generally assumed that they understand one another. We investigate this assumption in a signaling game where individuals develop a new vocabulary of signals to coordinate successfully. In our game, the individuals do not have common observations besides the communication signal and outcome of the interaction, i.e. received reward. This setting is used as a proxy to study communication emergence in populations of agents that perceive their environment very differently, e.g. hybrid populations that include humans and artificial agents. Agents develop signals, use them, and refine interpretations while not observing how other agents are using them. While populations always converge to optimal levels of coordination, in some cases, interacting agents interpret and use signals differently, converging to what we call successful misunderstandings. However, agents of population that coordinate using misaligned interpretations, are unable to establish successful coordination with new interaction partners. Not leading to coordination failure immediately, successful misunderstandings are difficult to spot and repair. Having at least three agents that all interact with each other are the two minimum conditions to ensure the emergence of shared interpretations. Under these conditions, the agent population exhibits this emergent property of compensating for the lack of shared observations of signal use, ensuring the emergence of shared interpretations.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LISA Technical Report: An Agentic Framework for Smart Contract Auditing</title>
<link>https://arxiv.org/abs/2509.24698</link>
<guid>https://arxiv.org/abs/2509.24698</guid>
<content:encoded><![CDATA[
arXiv:2509.24698v1 Announce Type: new 
Abstract: We present LISA, an agentic smart contract vulnerability detection framework that combines rule-based and logic-based methods to address a broad spectrum of vulnerabilities in smart contracts. LISA leverages data from historical audit reports to learn the detection experience (without model fine-tuning), enabling it to generalize learned patterns to unseen projects and evolving threat profiles. In our evaluation, LISA significantly outperforms both LLM-based approaches and traditional static analysis tools, achieving superior coverage of vulnerability types and higher detection accuracy. Our results suggest that LISA offers a compelling solution for industry: delivering more reliable and comprehensive vulnerability detection while reducing the dependence on manual effort.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedPOB: Sample-Efficient Federated Prompt Optimization via Bandits</title>
<link>https://arxiv.org/abs/2509.24701</link>
<guid>https://arxiv.org/abs/2509.24701</guid>
<content:encoded><![CDATA[
arXiv:2509.24701v1 Announce Type: new 
Abstract: The performance of large language models (LLMs) is highly sensitive to the input prompt, making prompt optimization a critical task. However, real-world application is hindered by three major challenges: (1) the black-box nature of powerful proprietary LLMs, (2) the need for high sample efficiency due to query costs, and (3) the desire for privacy-preserving collaboration among multiple users. To address these challenges simultaneously, we introduce a novel framework for sample-efficient federated prompt optimization based on multi-armed bandits (MABs). The MAB framework is uniquely suited for this problem as it is (1) inherently a black-box optimization method, (2) practically sample-efficient, and (3) enables collaborative learning with theoretically guaranteed benefit from more participating agents. We first propose the Federated Prompt Optimization via Bandits (FedPOB) algorithm, a federated variant of the Linear UCB algorithm, where agents collaborate by sharing model parameters instead of raw data. We then extend our approach to the practical setting of comparative user feedback by introducing FedPOB with Preference Feedback (FedPOB-Pref), an efficient algorithm based on federated dueling bandits. Extensive experiments demonstrate that both FedPOB and FedPOB-Pref significantly outperform existing baselines and that their performance consistently improves as more agents participate in the collaboration, validating the effectiveness of our federated approach.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemGen: Weaving Generative Latent Memory for Self-Evolving Agents</title>
<link>https://arxiv.org/abs/2509.24704</link>
<guid>https://arxiv.org/abs/2509.24704</guid>
<content:encoded><![CDATA[
arXiv:2509.24704v1 Announce Type: new 
Abstract: Agent memory shapes how Large Language Model (LLM)-powered agents, akin to the human brain, progressively refine themselves through environment interactions. Existing paradigms remain constrained: parametric memory forcibly adjusts model parameters, and retrieval-based memory externalizes experience into structured databases, yet neither captures the fluid interweaving of reasoning and memory that underlies human cognition. To address this gap, we propose MemGen, a dynamic generative memory framework that equips agents with a human-esque cognitive faculty. It consists of a \textit{memory trigger}, which monitors the agent's reasoning state to decide explicit memory invocation, and a \textit{memory weaver}, which takes the agent's current state as stimulus to construct a latent token sequence as machine-native memory to enrich its reasoning. In this way, MemGen enables agents to recall and augment latent memory throughout reasoning, producing a tightly interwoven cycle of memory and cognition. Extensive experiments across eight benchmarks show that MemGen surpasses leading external memory systems such as ExpeL and AWM by up to $38.22\%$, exceeds GRPO by up to $13.44\%$, and exhibits strong cross-domain generalization ability. More importantly, we find that without explicit supervision, MemGen spontaneously evolves distinct human-like memory faculties, including planning memory, procedural memory, and working memory, suggesting an emergent trajectory toward more naturalistic forms of machine cognition.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IWR-Bench: Can LVLMs reconstruct interactive webpage from a user interaction video?</title>
<link>https://arxiv.org/abs/2509.24709</link>
<guid>https://arxiv.org/abs/2509.24709</guid>
<content:encoded><![CDATA[
arXiv:2509.24709v1 Announce Type: new 
Abstract: The webpage-to-code task requires models to understand visual representations of webpages and generate corresponding code. However, existing benchmarks primarily focus on static screenshot-to-code tasks, thereby overlooking the dynamic interactions fundamental to real-world web applications. To address this limitation, this paper introduces IWR-Bench, a novel benchmark for evaluating the capabilities of Large Vision-Language Models (LVLMs) in interactive webpage reconstruction from video. IWR-Bench comprises 113 meticulously curated tasks from 100 real-world websites, with 1,001 actions and featuring diverse interaction complexities (e.g., web games), visual styles, and domains. Aligning with standard web development practices, each task includes not only user interaction videos but also all crawled static assets (e.g., images, videos). This benchmark evaluates models on two fundamental challenges: comprehensive multi-modal reasoning to infer interaction logic from video and assets, and advanced code generation to translate this logic into functional code. An agent-as-a-judge framework with a comprehensive metric system automatically assesses the functional correctness and visual fidelity of generated webpages. Extensive experiments on 28 LVLMs reveal a significant challenge: the best model achieves an overall score of only 36.35%, as functional correctness (24.39% IFS) lags significantly behind visual fidelity (64.25% VFS). These results highlight critical limitations in current models' ability to reason about temporal dynamics and synthesize event-driven logic, establishing IWR-Bench as a challenging frontier for vision-language research. The benchmark and evaluation code will be made publicly available. Code is available at https://github.com/L-O-I/IWR-Bench.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Socratic-Zero : Bootstrapping Reasoning via Data-Free Agent Co-evolution</title>
<link>https://arxiv.org/abs/2509.24726</link>
<guid>https://arxiv.org/abs/2509.24726</guid>
<content:encoded><![CDATA[
arXiv:2509.24726v1 Announce Type: new 
Abstract: Recent breakthroughs in large language models (LLMs) on reasoning tasks rely heavily on massive, high-quality datasets-typically human-annotated and thus difficult to scale. While data synthesis or distillation offers a promising alternative, existing methods struggle with inconsistent data quality and an inability to dynamically adapt to the evolving capabilities of the model, leading to suboptimal training signals. To address these limitations, we introduce Socratic-Zero, a fully autonomous framework that generates high-quality training data from minimal seed examples through the co-evolution of three agents: the Teacher, the Solver, and the Generator. The Solver continuously refines its reasoning by learning from preference feedback on both successful and failed trajectories; the Teacher adaptively crafts increasingly challenging questions based on the Solver's weaknesses; and the Generator distills the Teacher's question-design strategy to enable scalable, high-fidelity curriculum generation. This closed-loop system produces a self-improving curriculum-requiring no pre-existing tasks or labels. Remarkably, starting from only 100 seed questions, our Socratic-Solver-8B achieves an average gain of +20.2 percentage points over prior data synthesis methods across seven mathematical reasoning benchmarks (AMC23, AIME24-25, Olympiad, MATH-500, Minerva, and GSM8K), with consistent gains on both Qwen3 and GLM4 series models. Even more surprisingly, synthetic data from Socratic-Generator-32B enables student LLMs to achieve superior performance compared to other state-of-the-art (SOTA) commercial LLMs on these benchmarks, including Qwen3-235B-A22B, DeepSeek-V3.1-671B, GPT-5, Gemini-2.5-Pro, Grok-4, and Claude-4.1-Opus.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Generalisation in Imitation Learning</title>
<link>https://arxiv.org/abs/2509.24784</link>
<guid>https://arxiv.org/abs/2509.24784</guid>
<content:encoded><![CDATA[
arXiv:2509.24784v1 Announce Type: new 
Abstract: Imitation learning benchmarks often lack sufficient variation between training and evaluation, limiting meaningful generalisation assessment. We introduce Labyrinth, a benchmarking environment designed to test generalisation with precise control over structure, start and goal positions, and task complexity. It enables verifiably distinct training, evaluation, and test settings. Labyrinth provides a discrete, fully observable state space and known optimal actions, supporting interpretability and fine-grained evaluation. Its flexible setup allows targeted testing of generalisation factors and includes variants like partial observability, key-and-door tasks, and ice-floor hazards. By enabling controlled, reproducible experiments, Labyrinth advances the evaluation of generalisation in imitation learning and provides a valuable tool for developing more robust agents.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DyMoDreamer: World Modeling with Dynamic Modulation</title>
<link>https://arxiv.org/abs/2509.24804</link>
<guid>https://arxiv.org/abs/2509.24804</guid>
<content:encoded><![CDATA[
arXiv:2509.24804v1 Announce Type: new 
Abstract: A critical bottleneck in deep reinforcement learning (DRL) is sample inefficiency, as training high-performance agents often demands extensive environmental interactions. Model-based reinforcement learning (MBRL) mitigates this by building world models that simulate environmental dynamics and generate synthetic experience, improving sample efficiency. However, conventional world models process observations holistically, failing to decouple dynamic objects and temporal features from static backgrounds. This approach is computationally inefficient, especially for visual tasks where dynamic objects significantly influence rewards and decision-making performance. To address this, we introduce DyMoDreamer, a novel MBRL algorithm that incorporates a dynamic modulation mechanism to improve the extraction of dynamic features and enrich the temporal information. DyMoDreamer employs differential observations derived from a novel inter-frame differencing mask, explicitly encoding object-level motion cues and temporal dynamics. Dynamic modulation is modeled as stochastic categorical distributions and integrated into a recurrent state-space model (RSSM), enhancing the model's focus on reward-relevant dynamics. Experiments demonstrate that DyMoDreamer sets a new state-of-the-art on the Atari $100$k benchmark with a $156.6$\% mean human-normalized score, establishes a new record of $832$ on the DeepMind Visual Control Suite, and gains a $9.5$\% performance improvement after $1$M steps on the Crafter benchmark. Our code is released at https://github.com/Ultraman-Tiga1/DyMoDreamer.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Bilevel Approach to Integrated Surgeon Scheduling and Surgery Planning solved via Branch-and-Price</title>
<link>https://arxiv.org/abs/2509.24806</link>
<guid>https://arxiv.org/abs/2509.24806</guid>
<content:encoded><![CDATA[
arXiv:2509.24806v1 Announce Type: new 
Abstract: In this paper, we study a multi-agent scheduling problem for organising the operations within the operating room department. The head of the surgeon group and individual surgeons are together responsible for the surgeon schedule and surgical case planning. The surgeon head allocates time blocks to individual surgeons, whereas individual surgeons determine the planning of surgical cases independently, which might degrade the schedule quality envisaged by the surgeon head. The bilevel optimisation under study seeks an optimal Nash equilibrium solution -- a surgeon schedule and surgical case plan that optimise the objectives of the surgeon head, while ensuring that no individual surgeon can improve their own objective within the allocated time blocks. We propose a dedicated branch-and-price that adds lazy constraints to the formulation of surgeon-specific pricing problems to ensure an optimal bilevel feasible solution is retrieved. In this way, the surgeon head respects the objective requirements of the individual surgeons and the solution space can be searched efficiently. In the computational experiments, we validate the performance of the proposed algorithm and its dedicated components and provide insights into the benefits of attaining an equilibrium solution under different scenarios by calculating the price of stability and the price of decentralisation.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIPOM: Agent-aware Interactive Planning for Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2509.24826</link>
<guid>https://arxiv.org/abs/2509.24826</guid>
<content:encoded><![CDATA[
arXiv:2509.24826v1 Announce Type: new 
Abstract: Large language models (LLMs) are being increasingly used for planning in orchestrated multi-agent systems. However, existing LLM-based approaches often fall short of human expectations and, critically, lack effective mechanisms for users to inspect, understand, and control their behaviors. These limitations call for enhanced transparency, controllability, and human oversight. To address this, we introduce AIPOM, a system supporting human-in-the-loop planning through conversational and graph-based interfaces. AIPOM enables users to transparently inspect, refine, and collaboratively guide LLM-generated plans, significantly enhancing user control and trust in multi-agent workflows. Our code and demo video are available at https://github.com/megagonlabs/aipom.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts via Token-Level LSH Matching</title>
<link>https://arxiv.org/abs/2509.24832</link>
<guid>https://arxiv.org/abs/2509.24832</guid>
<content:encoded><![CDATA[
arXiv:2509.24832v1 Announce Type: new 
Abstract: As large language models (LLMs) continue to scale, the memory footprint of key-value (KV) caches during inference has become a significant bottleneck. Existing approaches primarily focus on compressing KV caches within a single prompt or reusing shared prefixes or frequently ocurred text segments across prompts. However, such strategies are limited in scenarios where prompts are semantically similar but lexically different, which frequently occurs in tasks such as multi-document summarization and conversational agents. We propose \textit{SemShareKV}, a KV cache sharing and compression framework that accelerates LLM inference by reusing KVCache in semantically similar prompts. Instead of relying on exact token matches, SemShareKV applies fuzzy token matching using locality-sensitive hashing (LSH) on token embeddings and incorporates Rotary Position Embedding (RoPE) to better preserve positional information. By selectively reusing relevant key-value pairs from a reference prompt's cache, SemShareKV reduces redundant computation while maintaining output quality. Experiments on diverse summarization datasets show up to 6.25$\times$ speedup and 42\% lower GPU memory usage with 5k tokens input, with negligible quality degradation. These results highlight the potential of semantic-aware cache sharing for efficient LLM inference.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhysicsMinions: Winning Gold Medals in the Latest Physics Olympiads with a Coevolutionary Multimodal Multi-Agent System</title>
<link>https://arxiv.org/abs/2509.24855</link>
<guid>https://arxiv.org/abs/2509.24855</guid>
<content:encoded><![CDATA[
arXiv:2509.24855v1 Announce Type: new 
Abstract: Physics is central to understanding and shaping the real world, and the ability to solve physics problems is a key indicator of real-world physical intelligence. Physics Olympiads, renowned as the crown of competitive physics, provide a rigorous testbed requiring complex reasoning and deep multimodal understanding, yet they remain largely underexplored in AI research. Existing approaches are predominantly single-model based, and open-source MLLMs rarely reach gold-medal-level performance. To address this gap, we propose PhysicsMinions, a coevolutionary multi-agent system for Physics Olympiad. Its architecture features three synergistic studios: a Visual Studio to interpret diagrams, a Logic Studio to formulate solutions, and a Review Studio to perform dual-stage verification. The system coevolves through an iterative refinement loop where feedback from the Review Studio continuously guides the Logic Studio, enabling the system to self-correct and converge towards the ground truth. Evaluated on the HiPhO benchmark spanning 7 latest physics Olympiads, PhysicsMinions delivers three major breakthroughs: (i) Strong generalization: it consistently improves both open-source and closed-source models of different sizes, delivering clear benefits over their single-model baselines; (ii) Historic breakthroughs: it elevates open-source models from only 1-2 to 6 gold medals across 7 Olympiads, achieving the first-ever open-source gold medal in the latest International Physics Olympiad (IPhO) under the average-score metric; and (iii) Scaling to human expert: it further advances the open-source Pass@32 score to 26.8/30 points on the latest IPhO, ranking 4th of 406 contestants and far surpassing the top single-model score of 22.7 (ranked 22nd). Generally, PhysicsMinions offers a generalizable framework for Olympiad-level problem solving, with the potential to extend across disciplines.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retro*: Optimizing LLMs for Reasoning-Intensive Document Retrieval</title>
<link>https://arxiv.org/abs/2509.24869</link>
<guid>https://arxiv.org/abs/2509.24869</guid>
<content:encoded><![CDATA[
arXiv:2509.24869v1 Announce Type: new 
Abstract: With the growing popularity of LLM agents and RAG, it has become increasingly important to retrieve documents that are essential for solving a task, even when their connection to the task is indirect or implicit. Addressing this problem requires fine-grained reasoning to accurately assess the relevance between the task and each candidate document. This capability, however, poses a significant challenge for existing IR techniques. Despite recent progress in reasoning-enhanced IR, existing approaches still face significant challenges in applicability, scalability, and efficiency. In this work, we propose Retro*, a novel approach for reasoning-intensive document retrieval. Our method introduces a rubric-based relevance scoring mechanism, enabling the model to reason about the relationship between a task and a document based on explicitly defined criteria, whereby producing a fine-grained, interpretable relevance score. Retro* also supports test-time scaling by combining multiple reasoning trajectories via score integration, which produces more reliable relevance estimates. To optimize Retro*'s reasoning capabilities, we introduce a novel reinforcement learning algorithm tailored for its relevance scoring mechanism, which employs two composite rewards to fully exploit the trajectories of each training sample. Our experiments show that Retro* outperforms existing document retrieval methods with notable advantages, leading to state-of-the-art performance on the BRIGHT benchmark.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Guided Expert-AI Collaboration for Efficient Soil Horizon Annotation</title>
<link>https://arxiv.org/abs/2509.24873</link>
<guid>https://arxiv.org/abs/2509.24873</guid>
<content:encoded><![CDATA[
arXiv:2509.24873v1 Announce Type: new 
Abstract: Uncertainty quantification is essential in human-machine collaboration, as human agents tend to adjust their decisions based on the confidence of the machine counterpart. Reliably calibrated model uncertainties, hence, enable more effective collaboration, targeted expert intervention and more responsible usage of Machine Learning (ML) systems. Conformal prediction has become a well established model-agnostic framework for uncertainty calibration of ML models, offering statistically valid confidence estimates for both regression and classification tasks. In this work, we apply conformal prediction to $\textit{SoilNet}$, a multimodal multitask model for describing soil profiles. We design a simulated human-in-the-loop (HIL) annotation pipeline, where a limited budget for obtaining ground truth annotations from domain experts is available when model uncertainty is high. Our experiments show that conformalizing SoilNet leads to more efficient annotation in regression tasks and comparable performance scores in classification tasks under the same annotation budget when tested against its non-conformal counterpart. All code and experiments can be found in our repository: https://github.com/calgo-lab/BGR
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Emergence of Social Science of Large Language Models</title>
<link>https://arxiv.org/abs/2509.24877</link>
<guid>https://arxiv.org/abs/2509.24877</guid>
<content:encoded><![CDATA[
arXiv:2509.24877v1 Announce Type: new 
Abstract: The social science of large language models (LLMs) examines how these systems evoke mind attributions, interact with one another, and transform human activity and institutions. We conducted a systematic review of 270 studies, combining text embeddings, unsupervised clustering and topic modeling to build a computational taxonomy. Three domains emerge organically across the reviewed literature. LLM as Social Minds examines whether and when models display behaviors that elicit attributions of cognition, morality and bias, while addressing challenges such as test leakage and surface cues. LLM Societies examines multi-agent settings where interaction protocols, architectures and mechanism design shape coordination, norms, institutions and collective epistemic processes. LLM-Human Interactions examines how LLMs reshape tasks, learning, trust, work and governance, and how risks arise at the human-AI interface. This taxonomy provides a reproducible map of a fragmented field, clarifies evidentiary standards across levels of analysis, and highlights opportunities for cumulative progress in the social science of artificial intelligence.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRCP: Diffusion on Reinforced Cooperative Perception for Perceiving Beyond Limits</title>
<link>https://arxiv.org/abs/2509.24903</link>
<guid>https://arxiv.org/abs/2509.24903</guid>
<content:encoded><![CDATA[
arXiv:2509.24903v1 Announce Type: new 
Abstract: Cooperative perception enabled by Vehicle-to-Everything communication has shown great promise in enhancing situational awareness for autonomous vehicles and other mobile robotic platforms. Despite recent advances in perception backbones and multi-agent fusion, real-world deployments remain challenged by hard detection cases, exemplified by partial detections and noise accumulation which limit downstream detection accuracy. This work presents Diffusion on Reinforced Cooperative Perception (DRCP), a real-time deployable framework designed to address aforementioned issues in dynamic driving environments. DRCP integrates two key components: (1) Precise-Pyramid-Cross-Modality-Cross-Agent, a cross-modal cooperative perception module that leverages camera-intrinsic-aware angular partitioning for attention-based fusion and adaptive convolution to better exploit external features; and (2) Mask-Diffusion-Mask-Aggregation, a novel lightweight diffusion-based refinement module that encourages robustness against feature perturbations and aligns bird's-eye-view features closer to the task-optimal manifold. The proposed system achieves real-time performance on mobile platforms while significantly improving robustness under challenging conditions. Code will be released in late 2025.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Goal-Oriented Language-Guided Navigation with Self-Improving Demonstrations at Scale</title>
<link>https://arxiv.org/abs/2509.24910</link>
<guid>https://arxiv.org/abs/2509.24910</guid>
<content:encoded><![CDATA[
arXiv:2509.24910v1 Announce Type: new 
Abstract: Goal-oriented language-guided navigation requires robust exploration capabilities for agents to navigate to specified goals in unknown environments without step-by-step instructions. Existing methods tend to exclusively utilize shortest-path trajectories, lacking effective exploration priors for training navigation agents. To address the above challenges, we present SID, a goal-oriented language-guided navigation learning approach with Self-Improving Demonstrations. Specifically, SID learns an initial agent on the shortest-path data sampled from environments and then leverages this agent to generate novel exploration trajectories. The novel rollouts provide demonstrations with stronger exploration strategies to train a better agent, which in turn produces higher-quality agent demonstrations for the next round of training. We show that this iterative self-improving pipeline readily scales to new environments, and the resulting demonstrations can be transferred across a variety of language-guided navigation tasks, elevating the performance ceiling in diverse goal-oriented navigation tasks. Extensive experiments demonstrate that SID significantly boosts the exploration capabilities and generalization of navigation agents. The resulting agent achieves new state-of-the-art performance on goal-oriented language-guided navigation tasks, including REVERIE, SOON, notably achieving a 50.9% success rate on the unseen validation splits of SOON, surpassing the prior leading approaches by a margin of 13.9%.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal Reasoning</title>
<link>https://arxiv.org/abs/2509.24922</link>
<guid>https://arxiv.org/abs/2509.24922</guid>
<content:encoded><![CDATA[
arXiv:2509.24922v1 Announce Type: new 
Abstract: Multi-agent systems (MAS), leveraging the remarkable capabilities of Large Language Models (LLMs), show great potential in addressing complex tasks. In this context, integrating MAS with legal tasks is a crucial step. While previous studies have developed legal benchmarks for LLM agents, none are specifically designed to consider the unique advantages of MAS, such as task decomposition, agent specialization, and flexible training. In fact, the lack of evaluation methods limits the potential of MAS in the legal domain. To address this gap, we propose MASLegalBench, a legal benchmark tailored for MAS and designed with a deductive reasoning approach. Our benchmark uses GDPR as the application scenario, encompassing extensive background knowledge and covering complex reasoning processes that effectively reflect the intricacies of real-world legal situations. Furthermore, we manually design various role-based MAS and conduct extensive experiments using different state-of-the-art LLMs. Our results highlight the strengths, limitations, and potential areas for improvement of existing models and MAS architectures.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training</title>
<link>https://arxiv.org/abs/2509.24923</link>
<guid>https://arxiv.org/abs/2509.24923</guid>
<content:encoded><![CDATA[
arXiv:2509.24923v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) hold promise to become autonomous agents, they often explore suboptimally in sequential decision-making. Recent work has sought to enhance this capability via supervised fine-tuning (SFT) or reinforcement learning (RL), improving regret on the classic multi-armed bandit task. However, it remains unclear how these learning methods shape exploration strategies and how well they generalize. We investigate both paradigms by training LLMs with SFT on expert trajectories and RL with a range of tailored reward signals including a strategic, regret-shaped reward to reduce variance, and an algorithmic reward that enables oracle imitation. The resulting agents outperform pre-trained models and achieve performance comparable to Upper Confidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x longer horizons and across bandit families. Behavioral analysis reveals that gains often stem from more sophisticated but greedier exploitation: RL/SFT agents are more prone to early catastrophic failure than pre-trained models, prematurely abandoning exploration. Furthermore, agents trained to imitate UCB learn to outperform their teacher by adopting more exploitative variants. Our findings clarify when each training paradigm is preferable and advocate tailored reward design and evaluation beyond average regret to promote robust exploratory behavior.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Autonomous Vehicle Meets V2X Cooperative Perception: How Far Are We?</title>
<link>https://arxiv.org/abs/2509.24927</link>
<guid>https://arxiv.org/abs/2509.24927</guid>
<content:encoded><![CDATA[
arXiv:2509.24927v1 Announce Type: new 
Abstract: With the tremendous advancement of deep learning and communication technology, Vehicle-to-Everything (V2X) cooperative perception has the potential to address limitations in sensing distant objects and occlusion for a single-agent perception system. V2X cooperative perception systems are software systems characterized by diverse sensor types and cooperative agents, varying fusion schemes, and operation under different communication conditions. Therefore, their complex composition gives rise to numerous operational challenges. Furthermore, when cooperative perception systems produce erroneous predictions, the types of errors and their underlying causes remain insufficiently explored. To bridge this gap, we take an initial step by conducting an empirical study of V2X cooperative perception. To systematically evaluate the impact of cooperative perception on the ego vehicle's perception performance, we identify and analyze six prevalent error patterns in cooperative perception systems. We further conduct a systematic evaluation of the critical components of these systems through our large-scale study and identify the following key findings: (1) The LiDAR-based cooperation configuration exhibits the highest perception performance; (2) Vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) communication exhibit distinct cooperative perception performance under different fusion schemes; (3) Increased cooperative perception errors may result in a higher frequency of driving violations; (4) Cooperative perception systems are not robust against communication interference when running online. Our results reveal potential risks and vulnerabilities in critical components of cooperative perception systems. We hope that our findings can better promote the design and repair of cooperative perception systems.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Dialogue That Heals: A Comprehensive Evaluation of Doctor Agents' Inquiry Capability</title>
<link>https://arxiv.org/abs/2509.24958</link>
<guid>https://arxiv.org/abs/2509.24958</guid>
<content:encoded><![CDATA[
arXiv:2509.24958v1 Announce Type: new 
Abstract: An effective physician should possess a combination of empathy, expertise, patience, and clear communication when treating a patient. Recent advances have successfully endowed AI doctors with expert diagnostic skills, particularly the ability to actively seek information through inquiry. However, other essential qualities of a good doctor remain overlooked. To bridge this gap, we present MAQuE(Medical Agent Questioning Evaluation), the largest-ever benchmark for the automatic and comprehensive evaluation of medical multi-turn questioning. It features 3,000 realistically simulated patient agents that exhibit diverse linguistic patterns, cognitive limitations, emotional responses, and tendencies for passive disclosure. We also introduce a multi-faceted evaluation framework, covering task success, inquiry proficiency, dialogue competence, inquiry efficiency, and patient experience. Experiments on different LLMs reveal substantial challenges across the evaluation aspects. Even state-of-the-art models show significant room for improvement in their inquiry capabilities. These models are highly sensitive to variations in realistic patient behavior, which considerably impacts diagnostic accuracy. Furthermore, our fine-grained metrics expose trade-offs between different evaluation perspectives, highlighting the challenge of balancing performance and practicality in real-world clinical settings.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Exploration of Physics Models</title>
<link>https://arxiv.org/abs/2509.24978</link>
<guid>https://arxiv.org/abs/2509.24978</guid>
<content:encoded><![CDATA[
arXiv:2509.24978v1 Announce Type: new 
Abstract: The process of scientific discovery relies on an interplay of observations, analysis, and hypothesis generation. Machine learning is increasingly being adopted to address individual aspects of this process. However, it remains an open challenge to fully automate the open-ended, heuristic, iterative loop required to discover the laws of an unknown system by exploring it through experiments and analysis, without tailoring the approach to the specifics of a given task. Here, we introduce SciExplorer, an agent that leverages large language model tool-use capabilities to enable free-form exploration of systems without any domain-specific blueprints, and apply it to the exploration of physical systems that are initially unknown to the agent. We test SciExplorer on a broad set of models spanning mechanical dynamical systems, wave evolution, and quantum many-body physics. Despite using a minimal set of tools, primarily based on code execution, we observe impressive performance on tasks such as recovering equations of motion from observed dynamics and inferring Hamiltonians from expectation values. The demonstrated effectiveness of this setup opens the door towards similar scientific exploration in other domains, without the need for finetuning or task-specific instructions.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Path Diffuser: Diffusion Model for Data-Driven Traffic Simulator</title>
<link>https://arxiv.org/abs/2509.24995</link>
<guid>https://arxiv.org/abs/2509.24995</guid>
<content:encoded><![CDATA[
arXiv:2509.24995v1 Announce Type: new 
Abstract: Simulating diverse and realistic traffic scenarios is critical for developing and testing autonomous planning. Traditional rule-based planners lack diversity and realism, while learning-based simulators often replay, forecast, or edit scenarios using historical agent trajectories. However, they struggle to generate new scenarios, limiting scalability and diversity due to their reliance on fully annotated logs and historical data. Thus, a key challenge for a learning-based simulator's performance is that it requires agents' past trajectories and pose information in addition to map data, which might not be available for all agents on the road.Without which, generated scenarios often produce unrealistic trajectories that deviate from drivable areas, particularly under out-of-distribution (OOD) map scenes (e.g., curved roads). To address this, we propose Path Diffuser (PD): a two-stage, diffusion model for generating agent pose initializations and their corresponding trajectories conditioned on the map, free of any historical context of agents' trajectories. Furthermore, PD incorporates a motion primitive-based prior, leveraging Frenet frame candidate trajectories to enhance diversity while ensuring road-compliant trajectory generation. We also explore various design choices for modeling complex multi-agent interactions. We demonstrate the effectiveness of our method through extensive experiments on the Argoverse2 Dataset and additionally evaluate the generalizability of the approach on OOD map variants. Notably, Path Diffuser outperforms the baseline methods by 1.92x on distribution metrics, 1.14x on common-sense metrics, and 1.62x on road compliance from adversarial benchmarks.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PanoWorld-X: Generating Explorable Panoramic Worlds via Sphere-Aware Video Diffusion</title>
<link>https://arxiv.org/abs/2509.24997</link>
<guid>https://arxiv.org/abs/2509.24997</guid>
<content:encoded><![CDATA[
arXiv:2509.24997v1 Announce Type: new 
Abstract: Generating a complete and explorable 360-degree visual world enables a wide range of downstream applications. While prior works have advanced the field, they remain constrained by either narrow field-of-view limitations, which hinder the synthesis of continuous and holistic scenes, or insufficient camera controllability that restricts free exploration by users or autonomous agents. To address this, we propose PanoWorld-X, a novel framework for high-fidelity and controllable panoramic video generation with diverse camera trajectories. Specifically, we first construct a large-scale dataset of panoramic video-exploration route pairs by simulating camera trajectories in virtual 3D environments via Unreal Engine. As the spherical geometry of panoramic data misaligns with the inductive priors from conventional video diffusion, we then introduce a Sphere-Aware Diffusion Transformer architecture that reprojects equirectangular features onto the spherical surface to model geometric adjacency in latent space, significantly enhancing visual fidelity and spatiotemporal continuity. Extensive experiments demonstrate that our PanoWorld-X achieves superior performance in various aspects, including motion range, control precision, and visual quality, underscoring its potential for real-world applications.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Shape of Surprise: Structured Uncertainty and Co-Creativity in AI Music Tools</title>
<link>https://arxiv.org/abs/2509.25028</link>
<guid>https://arxiv.org/abs/2509.25028</guid>
<content:encoded><![CDATA[
arXiv:2509.25028v1 Announce Type: new 
Abstract: Randomness plays a pivotal yet paradoxical role in computational music creativity: it can spark novelty, but unchecked chance risks incoherence. This paper presents a thematic review of contemporary AI music systems, examining how designers incorporate randomness and uncertainty into creative practice. I draw on the concept of structured uncertainty to analyse how stochastic processes are constrained within musical and interactive frameworks. Through a comparative analysis of six systems - Musika (Pasini and Schl\"uter, 2022), MIDI-DDSP (Wu et al., 2021), Melody RNN (Magenta Project), RAVE (Caillon and Esling, 2021), Wekinator (Fiebrink and Cook, 2010), and Somax 2 (Borg, 2019) - we identify recurring design patterns that support musical coherence, user control, and co-creativity. To my knowledge, this is the first thematic review examining randomness in AI music through structured uncertainty, offering practical insights for designers and artists aiming to support expressive, collaborative, or improvisational interactions.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation</title>
<link>https://arxiv.org/abs/2509.25032</link>
<guid>https://arxiv.org/abs/2509.25032</guid>
<content:encoded><![CDATA[
arXiv:2509.25032v1 Announce Type: new 
Abstract: As robots transition from controlled settings to unstructured human environments, building generalist agents that can reliably follow natural language instructions remains a central challenge. Progress in robust mobile manipulation requires large-scale multimodal datasets that capture contact-rich and long-horizon tasks, yet existing resources lack synchronized force-torque sensing, hierarchical annotations, and explicit failure cases. We address this gap with the AIRoA MoMa Dataset, a large-scale real-world multimodal dataset for mobile manipulation. It includes synchronized RGB images, joint states, six-axis wrist force-torque signals, and internal robot states, together with a novel two-layer annotation schema of sub-goals and primitive actions for hierarchical learning and error analysis. The initial dataset comprises 25,469 episodes (approx. 94 hours) collected with the Human Support Robot (HSR) and is fully standardized in the LeRobot v2.1 format. By uniquely integrating mobile manipulation, contact-rich interaction, and long-horizon structure, AIRoA MoMa provides a critical benchmark for advancing the next generation of Vision-Language-Action models. The first version of our dataset is now available at https://huggingface.co/datasets/airoa-org/airoa-moma .
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MARLIN: Multi-Agent Reinforcement Learning with Murmuration Intelligence and LLM Guidance for Reservoir Management</title>
<link>https://arxiv.org/abs/2509.25034</link>
<guid>https://arxiv.org/abs/2509.25034</guid>
<content:encoded><![CDATA[
arXiv:2509.25034v1 Announce Type: new 
Abstract: As climate change intensifies extreme weather events, water disasters pose growing threats to global communities, making adaptive reservoir management critical for protecting vulnerable populations and ensuring water security. Modern water resource management faces unprecedented challenges from cascading uncertainties propagating through interconnected reservoir networks. These uncertainties, rooted in physical water transfer losses and environmental variability, make precise control difficult. For example, sending 10 tons downstream may yield only 8-12 tons due to evaporation and seepage. Traditional centralized optimization approaches suffer from exponential computational complexity and cannot effectively handle such real-world uncertainties, while existing multi-agent reinforcement learning (MARL) methods fail to achieve effective coordination under uncertainty. To address these challenges, we present MARLIN, a decentralized reservoir management framework inspired by starling murmurations intelligence. Integrating bio-inspired alignment, separation, and cohesion rules with MARL, MARLIN enables individual reservoirs to make local decisions while achieving emergent global coordination. In addition, a LLM provides real-time reward shaping signals, guiding agents to adapt to environmental changes and human-defined preferences. Experiments on real-world USGS data show that MARLIN improves uncertainty handling by 23\%, cuts computation by 35\%, and accelerates flood response by 68\%, exhibiting super-linear coordination, with complexity scaling 5.4x from 400 to 10,000 nodes. These results demonstrate MARLIN's potential for disaster prevention and protecting communities through intelligent, scalable water resource management.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Synthetic Task Generation for Agents via Exploration</title>
<link>https://arxiv.org/abs/2509.25047</link>
<guid>https://arxiv.org/abs/2509.25047</guid>
<content:encoded><![CDATA[
arXiv:2509.25047v1 Announce Type: new 
Abstract: Post-Training Multimodal Large Language Models (MLLMs) to build interactive agents holds promise across domains such as computer-use, web navigation, and robotics. A key challenge in scaling such post-training is lack of high-quality downstream agentic task datasets with tasks that are diverse, feasible, and verifiable. Existing approaches for task generation rely heavily on human annotation or prompting MLLM with limited downstream environment information, which is either costly or poorly scalable as it yield tasks with limited coverage. To remedy this, we present AutoPlay, a scalable pipeline for task generation that explicitly explores interactive environments to discover possible interactions and current state information to synthesize environment-grounded tasks. AutoPlay operates in two stages: (i) an exploration phase, where an MLLM explorer agent systematically uncovers novel environment states and functionalities, and (ii) a task generation phase, where a task generator leverages exploration trajectories and a set of task guideline prompts as context to synthesize diverse, executable, and verifiable tasks. We show AutoPlay generates 20k tasks across 20 Android applications and 10k tasks across 13 applications Ubuntu applications to train mobile-use and computer-use agents. AutoPlay generated tasks enable large-scale task demonstration synthesis without human annotation by employing an MLLM task executor and verifier. This data enables training MLLM-based UI agents that improve success rates up to $20.0\%$ on mobile-use and $10.9\%$ on computer-use scenarios. In addition, AutoPlay generated tasks combined with MLLM verifier-based rewards enable scaling reinforcement learning training of UI agents, leading to an additional $5.7\%$ gain. coverage. These results establish AutoPlay as a scalable approach for post-training capable MLLM agents reducing reliance on human annotation.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning</title>
<link>https://arxiv.org/abs/2509.25052</link>
<guid>https://arxiv.org/abs/2509.25052</guid>
<content:encoded><![CDATA[
arXiv:2509.25052v1 Announce Type: new 
Abstract: The pursuit of artificial agents that can learn to master complex environments has led to remarkable successes, yet prevailing deep reinforcement learning methods often rely on immense experience, encoding their knowledge opaquely within neural network weights. We propose a different paradigm, one in which an agent learns to play by reasoning and planning. We introduce Cogito, ergo ludo (CEL), a novel agent architecture that leverages a Large Language Model (LLM) to build an explicit, language-based understanding of its environment's mechanics and its own strategy. Starting from a tabula rasa state with no prior knowledge (except action set), CEL operates on a cycle of interaction and reflection. After each episode, the agent analyzes its complete trajectory to perform two concurrent learning processes: Rule Induction, where it refines its explicit model of the environment's dynamics, and Strategy and Playbook Summarization, where it distills experiences into an actionable strategic playbook. We evaluate CEL on diverse grid-world tasks (i.e., Minesweeper, Frozen Lake, and Sokoban), and show that the CEL agent successfully learns to master these games by autonomously discovering their rules and developing effective policies from sparse rewards. Ablation studies confirm that the iterative process is critical for sustained learning. Our work demonstrates a path toward more general and interpretable agents that not only act effectively but also build a transparent and improving model of their world through explicit reasoning on raw experience.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety-Critical Input-Constrained Nonlinear Intercept Guidance in Multiple Engagement Zones</title>
<link>https://arxiv.org/abs/2509.25053</link>
<guid>https://arxiv.org/abs/2509.25053</guid>
<content:encoded><![CDATA[
arXiv:2509.25053v1 Announce Type: new 
Abstract: This paper presents an input-constrained nonlinear guidance law to address the problem of intercepting a stationary target in contested environments with multiple defending agents. Contrary to prior approaches that rely on explicit knowledge of defender strategies or utilize conservative safety conditions based on a defender's range, our work characterizes defender threats geometrically through engagement zones that delineate inevitable interception regions. Outside these engagement zones, the interceptor remains invulnerable. The proposed guidance law switches between a repulsive safety maneuver near these zones and a pursuit maneuver outside their influence. To deal with multiple engagement zones, we employ a smooth minimum function (log-sum-exponent approximation) that aggregates threats from all the zones while prioritizing the most critical threats. Input saturation is modeled and embedded in the non-holonomic vehicle dynamics so the controller respects actuator limits while maintaining stability. Numerical simulations with several defenders demonstrate the proposed method's ability to avoid engagement zones and achieve interception across diverse initial conditions.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Generalist Data-Analytic Agents</title>
<link>https://arxiv.org/abs/2509.25084</link>
<guid>https://arxiv.org/abs/2509.25084</guid>
<content:encoded><![CDATA[
arXiv:2509.25084v1 Announce Type: new 
Abstract: Data-analytic agents are emerging as a key catalyst for automated scientific discovery and for the vision of Innovating AI. Current approaches, however, rely heavily on prompt engineering over proprietary models, while open-source models struggle to face diverse-format, large-scale data files and long-horizon, multi-step reasoning that real-world analytics demands. This paper introduces DataMind, a scalable data synthesis and agent training recipe designed to build generalist data-analytic agents. DataMind tackles three key challenges in building open-source data-analytic agents, including insufficient data resources, improper training strategy, and unstable code-based multi-turn rollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a recursive easy-to-hard task composition mechanism to increase the diversity and difficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling strategy followed by model-based and rule-based filtering; 3) a dynamically adjustable training objective combining both SFT and RL losses; 4) a memory-frugal and stable code-based multi-turn rollout framework. Built on DataMind, we curate DataMind-12K, a high-quality trajectory set spanning diverse domains, task categories, and data file formats for data-analytic tasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with an average score of 71.16% on multiple data analysis benchmarks, outperforming the strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B also performs best among all open-source models with a score of 68.10%. We also incorporate some empirical insights gained from our exploratory trials into the analysis experiments, aiming to provide actionable insights about agentic training for the community. We will release DataMind-12K and DataMind-7B,14B for the community's future research.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HeDA: An Intelligent Agent System for Heatwave Risk Discovery through Automated Knowledge Graph Construction and Multi-layer Risk Propagation Analysis</title>
<link>https://arxiv.org/abs/2509.25112</link>
<guid>https://arxiv.org/abs/2509.25112</guid>
<content:encoded><![CDATA[
arXiv:2509.25112v1 Announce Type: new 
Abstract: Heatwaves pose complex cascading risks across interconnected climate, social, and economic systems, but knowledge fragmentation in scientific literature hinders comprehensive understanding of these risk pathways. We introduce HeDA (Heatwave Discovery Agent), an intelligent multi-agent system designed for automated scientific discovery through knowledge graph construction and multi-layer risk propagation analysis. HeDA processes over 10,247 academic papers to construct a comprehensive knowledge graph with 23,156 nodes and 89,472 relationships, employing novel multi-layer risk propagation analysis to systematically identify overlooked risk transmission pathways. Our system achieves 78.9% accuracy on complex question-answering tasks, outperforming state-of-the-art baselines including GPT-4 by 13.7%. Critically, HeDA successfully discovered five previously unidentified high-impact risk chains, such as the pathway where a heatwave leads to a water demand surge, resulting in industrial water restrictions and ultimately causing small business disruption, which were validated through historical case studies and domain expert review. This work presents a new paradigm for AI-driven scientific discovery, providing actionable insights for developing more resilient climate adaptation strategies.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs</title>
<link>https://arxiv.org/abs/2509.25139</link>
<guid>https://arxiv.org/abs/2509.25139</guid>
<content:encoded><![CDATA[
arXiv:2509.25139v1 Announce Type: new 
Abstract: Integrating large language models (LLMs) into embodied AI models is becoming increasingly prevalent. However, existing zero-shot LLM-based Vision-and-Language Navigation (VLN) agents either encode images as textual scene descriptions, potentially oversimplifying visual details, or process raw image inputs, which can fail to capture abstract semantics required for high-level reasoning. In this paper, we improve the navigation agent's contextual understanding by incorporating textual descriptions from multiple perspectives that facilitate analogical reasoning across images. By leveraging text-based analogical reasoning, the agent enhances its global scene understanding and spatial reasoning, leading to more accurate action decisions. We evaluate our approach on the R2R dataset, where our experiments demonstrate significant improvements in navigation performance.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory</title>
<link>https://arxiv.org/abs/2509.25140</link>
<guid>https://arxiv.org/abs/2509.25140</guid>
<content:encoded><![CDATA[
arXiv:2509.25140v1 Announce Type: new 
Abstract: With the growing adoption of large language model agents in persistent real-world roles, they naturally encounter continuous streams of tasks. A key limitation, however, is their failure to learn from the accumulated interaction history, forcing them to discard valuable insights and repeat past errors. We propose ReasoningBank, a novel memory framework that distills generalizable reasoning strategies from an agent's self-judged successful and failed experiences. At test time, an agent retrieves relevant memories from ReasoningBank to inform its interaction and then integrates new learnings back, enabling it to become more capable over time. Building on this powerful experience learner, we further introduce memory-aware test-time scaling (MaTTS), which accelerates and diversifies this learning process by scaling up the agent's interaction experience. By allocating more compute to each task, the agent generates abundant, diverse experiences that provide rich contrastive signals for synthesizing higher-quality memory. The better memory in turn guides more effective scaling, establishing a powerful synergy between memory and test-time scaling. Across web browsing and software engineering benchmarks, ReasoningBank consistently outperforms existing memory mechanisms that store raw trajectories or only successful task routines, improving both effectiveness and efficiency; MaTTS further amplifies these gains. These findings establish memory-driven experience scaling as a new scaling dimension, enabling agents to self-evolve with emergent behaviors naturally arise.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Popular Dimension of Matchings</title>
<link>https://arxiv.org/abs/2509.25150</link>
<guid>https://arxiv.org/abs/2509.25150</guid>
<content:encoded><![CDATA[
arXiv:2509.25150v1 Announce Type: new 
Abstract: We study popular matchings in three classical settings: the house allocation problem, the marriage problem, and the roommates problem. In the popular matching problem, (a subset of) the vertices in a graph have preference orderings over their potential matches. A matching is popular if it gets a plurality of votes in a pairwise election against any other matching. Unfortunately, popular matchings typically do not exist. So we study a natural relaxation, namely popular winning sets which are a set of matchings that collectively get a plurality of votes in a pairwise election against any other matching. The $\textit{popular dimension}$ is the minimum cardinality of a popular winning set, in the worst case over the problem class.
  We prove that the popular dimension is exactly $2$ in the house allocation problem, even if the voters are weighted and ties are allowed in their preference lists. For the marriage problem and the roommates problem, we prove that the popular dimension is between $2$ and $3$, when the agents are weighted and/or their preferences orderings allow ties. In the special case where the agents are unweighted and have strict preference orderings, the popular dimension of the marriage problem is known to be exactly $1$ and we prove the popular dimension of the roommates problem is exactly $2$.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PixelCraft: A Multi-Agent System for High-Fidelity Visual Reasoning on Structured Images</title>
<link>https://arxiv.org/abs/2509.25185</link>
<guid>https://arxiv.org/abs/2509.25185</guid>
<content:encoded><![CDATA[
arXiv:2509.25185v1 Announce Type: new 
Abstract: Structured images (e.g., charts and geometric diagrams) remain challenging for multimodal large language models (MLLMs), as perceptual slips can cascade into erroneous conclusions. Intermediate visual cues can steer reasoning; however, existing cue-based methods are constrained with low-fidelity image processing and linear, rigid reasoning patterns, limiting their effectiveness on complex structured-image tasks. In this paper, we propose PixelCraft, a novel multi-agent system for high-fidelity image processing and flexible visual reasoning on structured images. The system comprises a dispatcher, a planner, a reasoner, critics, and a set of visual tool agents. To achieve high-fidelity processing, we construct a high-quality corpus and fine-tune an MLLM into a grounding model, whose pixel-level localizations are integrated with traditional computer vision (CV) algorithms in tool agents. Building on this foundation, PixelCraft facilitates flexible visual reasoning through a dynamic three-stage workflow of tool selection, agent discussion, and self-criticism. Moreover, unlike prior linear reasoning patterns that simply append historical images, PixelCraft maintains an image memory to allow the planner to adaptively revisit earlier visual steps, explore alternative reasoning branches, and dynamically adjust the reasoning trajectory during discussion. Extensive experiments on challenging chart and geometry benchmarks demonstrate that PixelCraft significantly improves visual reasoning performance for advanced MLLMs, setting a new standard for structured image reasoning. Our code will be available at https://github.com/microsoft/PixelCraft.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfoAgent: Advancing Autonomous Information-Seeking Agents</title>
<link>https://arxiv.org/abs/2509.25189</link>
<guid>https://arxiv.org/abs/2509.25189</guid>
<content:encoded><![CDATA[
arXiv:2509.25189v1 Announce Type: new 
Abstract: Building Large Language Model agents that expand their capabilities by interacting with external tools represents a new frontier in AI research and applications. In this paper, we introduce InfoAgent, a deep research agent powered by an innovative data synthesis pipeline and orchestrated web search tools. To construct challenging, hard-to-find queries,we build entity trees and apply sub-tree sampling with entity fuzzification to systematically increase question difficulty. Unlike prior work that relies heavily on commercial search tools, we develop a dedicated self-hosted search infrastructure, enhancing transparency of agent environments and facilitating further advancement of agent capacity. We evaluate the effectiveness of our data pipeline by measuring the average number of tool calls required to correctly answer a question, and also show that our agent yields better performance when equipped with our tools. Our \mbox{InfoAgent} is post-trained from Qwen3-14B using a two-stage recipe: cold-start supervised finetuning to instill long-horizon search behaviors, followed by reinforcement learning which significantly improves reasoning-driven tool use. With our methods, InfoAgent achieves 15.3\% accuracy on BrowseComp, 29.2\% on BrowseComp-ZH, and 40.4\% on Xbench-DS, outperforming prior open-source deep research agents such as WebSailor-72B and DeepDive-32B.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIMPOL Model for Solving Continuous-Time Heterogeneous Agent Problems</title>
<link>https://arxiv.org/abs/2509.23557</link>
<guid>https://arxiv.org/abs/2509.23557</guid>
<content:encoded><![CDATA[
arXiv:2509.23557v1 Announce Type: cross 
Abstract: This paper presents SIMPOL (Simplified Policy Iteration), a modular numerical framework for solving continuous-time heterogeneous agent models. The core economic problem, the optimization of consumption and savings under idiosyncratic uncertainty, is formulated as a coupled system of partial differential equations: a Hamilton-Jacobi-Bellman (HJB) equation for the agent's optimal policy and a Fokker-Planck-Kolmogorov (FPK) equation for the stationary wealth distribution. SIMPOL addresses this system using Howard's policy iteration with an *upwind* finite difference scheme that guarantees stability. A distinctive contribution is a novel consumption policy post-processing module that imposes regularity through smoothing and a projection onto an economically plausible slope band, improving convergence and model behavior. The robustness and accuracy of SIMPOL are validated through a set of integrated diagnostics, including verification of contraction in the Wasserstein-2 metric and comparison with the analytical solution of the Merton model in the no-volatility case. The framework is shown to be not only computationally efficient but also to produce solutions consistent with economic and mathematical theory, offering a reliable tool for research in quantitative macroeconomics.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2405.14314</link>
<guid>https://arxiv.org/abs/2405.14314</guid>
<content:encoded><![CDATA[
arXiv:2405.14314v4 Announce Type: replace 
Abstract: Grounding the reasoning ability of large language models (LLMs) for embodied tasks is challenging due to the complexity of the physical world. Especially, LLM planning for multi-agent collaboration requires communication of agents or credit assignment as the feedback to re-adjust the proposed plans and achieve effective coordination. However, existing methods that overly rely on physical verification or self-reflection suffer from excessive and inefficient querying of LLMs. In this paper, we propose a novel framework for multi-agent collaboration that introduces Reinforced Advantage feedback (ReAd) for efficient self-refinement of plans. Specifically, we perform critic regression to learn a sequential advantage function from LLM-planned data, and then treat the LLM planner as an optimizer to generate actions that maximize the advantage function. It endows the LLM with the foresight to discern whether the action contributes to accomplishing the final task. We provide theoretical analysis by extending advantage-weighted regression in reinforcement learning to multi-agent systems. Experiments on Overcooked-AI and a difficult variant of RoCoBench show that ReAd surpasses baselines in success rate, and also significantly decreases the interaction steps of agents and query rounds of LLMs, demonstrating its high efficiency for grounding LLMs. More results are given at https://embodied-read.github.io
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Thompson Sampling Susceptible to Algorithmic Collusion?</title>
<link>https://arxiv.org/abs/2405.17463</link>
<guid>https://arxiv.org/abs/2405.17463</guid>
<content:encoded><![CDATA[
arXiv:2405.17463v2 Announce Type: replace 
Abstract: When two players are engaged in a repeated game with unknown payoff matrices, they may use single-agent multi-armed bandit algorithms to choose the actions independent of each other. We show that when the players use Thompson sampling, the game dynamics converges to the Nash equilibrium under a mild assumption on the payoff matrices. Therefore, algorithmic collusion doesn't arise in this case despite the fact that the players do not intentionally deploy competitive strategies. To prove the convergence result, we find that the framework developed in stochastic approximation doesn't apply, because of the sporadic and infrequent updates of the inferior actions and the lack of Lipschitz continuity. We develop a novel sample-path-wise approach to show the convergence. However, when the payoff matrices do not satisfy the assumption, the game may converge to collusive outcomes.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CommonPower: A Framework for Safe Data-Driven Smart Grid Control</title>
<link>https://arxiv.org/abs/2406.03231</link>
<guid>https://arxiv.org/abs/2406.03231</guid>
<content:encoded><![CDATA[
arXiv:2406.03231v5 Announce Type: replace 
Abstract: The growing complexity of power system management has led to an increased interest in reinforcement learning (RL). To validate their effectiveness, RL algorithms have to be evaluated across multiple case studies. Case study design is an arduous task requiring the consideration of many aspects, among them the influence of available forecasts and the level of decentralization in the control structure. Furthermore, vanilla RL controllers cannot themselves ensure the satisfaction of system constraints, which makes devising a safeguarding mechanism a necessary task for every case study before deploying the system. To address these shortcomings, we introduce the Python tool CommonPower, the first general framework for the modeling and simulation of power system management tailored towards machine learning. Its modular architecture enables users to focus on specific elements without having to implement a simulation environment. Another unique contribution of CommonPower is the automatic synthesis of model predictive controllers and safeguards. Beyond offering a unified interface for single-agent RL, multi-agent RL, and optimal control, CommonPower includes a training pipeline for machine-learning-based forecasters as well as a flexible mechanism for incorporating feedback of safeguards into the learning updates of RL controllers.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robot Navigation with Entity-Based Collision Avoidance using Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2408.14183</link>
<guid>https://arxiv.org/abs/2408.14183</guid>
<content:encoded><![CDATA[
arXiv:2408.14183v2 Announce Type: replace 
Abstract: Efficient navigation in dynamic environments is crucial for autonomous robots interacting with moving agents and static obstacles. We present a novel deep reinforcement learning approach that improves robot navigation and interaction with different types of agents and obstacles based on specific safety requirements. Our approach uses information about the entity types, improving collision avoidance and ensuring safer navigation. We introduce a new reward function that penalizes the robot for being close to or colliding with different entities such as adults, bicyclists, children, and static obstacles, while also encouraging the robot's progress toward the goal. We propose an optimized algorithm that significantly accelerates the training, validation, and testing phases, enabling efficient learning in complex environments. Comprehensive experiments demonstrate that our approach consistently outperforms state-of-the-art navigation and collision avoidance methods.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-3D Print: Large Language Models To Monitor and Control 3D Printing</title>
<link>https://arxiv.org/abs/2408.14307</link>
<guid>https://arxiv.org/abs/2408.14307</guid>
<content:encoded><![CDATA[
arXiv:2408.14307v3 Announce Type: replace 
Abstract: Industry 4.0 has revolutionized manufacturing by driving digitalization and shifting the paradigm toward additive manufacturing (AM). Fused Deposition Modeling (FDM), a key AM technology, enables the creation of highly customized, cost-effective products with minimal material waste through layer-by-layer extrusion, posing a significant challenge to traditional subtractive methods. However, the susceptibility of material extrusion techniques to errors often requires expert intervention to detect and mitigate defects that can severely compromise product quality. While automated error detection and machine learning models exist, their generalizability across diverse 3D printer setups, firmware, and sensors is limited, and deep learning methods require extensive labeled datasets, hindering scalability and adaptability. To address these challenges, we present a process monitoring and control framework that leverages pre-trained Large Language Models (LLMs) alongside 3D printers to detect and address printing defects. The LLM evaluates print quality by analyzing images captured after each layer or print segment, identifying failure modes and querying the printer for relevant parameters. It then generates and executes a corrective action plan. We validated the effectiveness of the proposed framework in identifying defects by comparing it against a control group of engineers with diverse AM expertise. Our evaluation demonstrated that LLM-based agents not only accurately identify common 3D printing errors, such as inconsistent extrusion, stringing, warping, and layer adhesion, but also effectively determine the parameters causing these failures and autonomously correct them without any need for human intervention.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Vehicle Controllers From End-to-End Differentiable Simulation</title>
<link>https://arxiv.org/abs/2409.07965</link>
<guid>https://arxiv.org/abs/2409.07965</guid>
<content:encoded><![CDATA[
arXiv:2409.07965v2 Announce Type: replace 
Abstract: Current methods to learn controllers for autonomous vehicles (AVs) focus on behavioural cloning. Being trained only on exact historic data, the resulting agents often generalize poorly to novel scenarios. Simulators provide the opportunity to go beyond offline datasets, but they are still treated as complicated black boxes, only used to update the global simulation state. As a result, these RL algorithms are slow, sample-inefficient, and prior-agnostic. In this work, we leverage a differentiable simulator and design an analytic policy gradients (APG) approach to training AV controllers on the large-scale Waymo Open Motion Dataset. Our proposed framework brings the differentiable simulator into an end-to-end training loop, where gradients of the environment dynamics serve as a useful prior to help the agent learn a more grounded policy. We combine this setup with a recurrent architecture that can efficiently propagate temporal information across long simulated trajectories. This APG method allows us to learn robust, accurate, and fast policies, while only requiring widely-available expert trajectories, instead of scarce expert actions. We compare to behavioural cloning and find significant improvements in performance and robustness to noise in the dynamics, as well as overall more intuitive human-like handling.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoT-TL: Low-Resource Temporal Knowledge Representation of Planning Instructions Using Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2410.16207</link>
<guid>https://arxiv.org/abs/2410.16207</guid>
<content:encoded><![CDATA[
arXiv:2410.16207v2 Announce Type: replace 
Abstract: Autonomous agents often face the challenge of interpreting uncertain natural language instructions for planning tasks. Representing these instructions as Linear Temporal Logic (LTL) enables planners to synthesize actionable plans. We introduce CoT-TL, a data-efficient in-context learning framework for translating natural language specifications into LTL representations. CoT-TL addresses the limitations of large language models, which typically rely on extensive fine-tuning data, by extending chain-of-thought reasoning and semantic roles to align with the requirements of formal logic creation. This approach enhances the transparency and rationale behind LTL generation, fostering user trust. CoT-TL achieves state-of-the-art accuracy across three diverse datasets in low-data scenarios, outperforming existing methods without fine-tuning or intermediate translations. To improve reliability and minimize hallucinations, we incorporate model checking to validate the syntax of the generated LTL output. We further demonstrate CoT-TL's effectiveness through ablation studies and evaluations on unseen LTL structures and formulas in a new dataset. Finally, we validate CoT-TL's practicality by integrating it into a QuadCopter for multi-step drone planning based on natural language instructions. Project details: \href{https://github.com/kumarmanas/TAMP\_COT\_TL}{https://github.com/kumarmanas/TAMP\_COT\_TL}
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cost-Aware Opinion Dynamics in Multi-Agents Systems under Malicious Agent Influence</title>
<link>https://arxiv.org/abs/2412.01524</link>
<guid>https://arxiv.org/abs/2412.01524</guid>
<content:encoded><![CDATA[
arXiv:2412.01524v3 Announce Type: replace 
Abstract: In many MASs, links to malicious agents cannot be severed immediately. Under these conditions, averaging-only consensus mechanisms typically lack sufficient resistance, leaving the system vulnerable to harmful deviations. To address this challenge, this brief leverages the Boomerang Effect from sociology, which drives normal agents to firmly reject malicious inputs, although this strategy may appear overly cautious. Thus, this brief emphasizes the necessity of acknowledging the resulting trade-off between cost and convergence speed in practice. To address this, the additional costs induced by Boomerang-style fusion is analyzed and a cost aware evolution rate adjustment mechanism is proposed. Multi-robot simulations demonstrate that this mechanism suppresses excess costs while maintaining resilience to extremist disruptions and ensuring stable convergence, enabling MAS to efficiently develop in a ethical order.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is FISHER All You Need in The Multi-AUV Underwater Target Tracking Task?</title>
<link>https://arxiv.org/abs/2412.03959</link>
<guid>https://arxiv.org/abs/2412.03959</guid>
<content:encoded><![CDATA[
arXiv:2412.03959v2 Announce Type: replace 
Abstract: It is significant to employ multiple autonomous underwater vehicles (AUVs) to execute the underwater target tracking task collaboratively. However, it's pretty challenging to meet various prerequisites utilizing traditional control methods. Therefore, we propose an effective two-stage learning from demonstrations training framework, FISHER, to highlight the adaptability of reinforcement learning (RL) methods in the multi-AUV underwater target tracking task, while addressing its limitations such as extensive requirements for environmental interactions and the challenges in designing reward functions. The first stage utilizes imitation learning (IL) to realize policy improvement and generate offline datasets. To be specific, we introduce multi-agent discriminator-actor-critic based on improvements of the generative adversarial IL algorithm and multi-agent IL optimization objective derived from the Nash equilibrium condition. Then in the second stage, we develop multi-agent independent generalized decision transformer, which analyzes the latent representation to match the future states of high-quality samples rather than reward function, attaining further enhanced policies capable of handling various scenarios. Besides, we propose a simulation to simulation demonstration generation procedure to facilitate the generation of expert demonstrations in underwater environments, which capitalizes on traditional control methods and can easily accomplish the domain transfer to obtain demonstrations. Extensive simulation experiments from multiple scenarios showcase that FISHER possesses strong stability, multi-task performance and capability of generalization.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From An LLM Swarm To A PDDL-Empowered HIVE: Planning Self-Executed Instructions In A Multi-Modal Jungle</title>
<link>https://arxiv.org/abs/2412.12839</link>
<guid>https://arxiv.org/abs/2412.12839</guid>
<content:encoded><![CDATA[
arXiv:2412.12839v2 Announce Type: replace 
Abstract: In response to the call for agent-based solutions that leverage the ever-increasing capabilities of the deep models' ecosystem, we introduce Hive -- a comprehensive solution for knowledge-aware planning of a set of atomic actions to address input queries and subsequently selecting appropriate models accordingly. Hive operates over sets of models and, upon receiving natural language instructions (i.e. user queries), schedules and executes explainable plans of atomic actions. These actions can involve one or more of the available models to achieve the overall task, while respecting end-users specific constraints. Notably, Hive handles tasks that involve multi-modal inputs and outputs, enabling it to handle complex, real-world queries. Our system is capable of planning complex chains of actions while guaranteeing explainability, using an LLM-based formal logic backbone empowered by PDDL operations. We introduce the MuSE benchmark in order to offer a comprehensive evaluation of the multi-modal capabilities of agent systems. Our findings show that our framework redefines the state-of-the-art for task selection, outperforming other competing systems that plan operations across multiple models while offering transparency guarantees while fully adhering to user constraints.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUI Agents: A Survey</title>
<link>https://arxiv.org/abs/2412.13501</link>
<guid>https://arxiv.org/abs/2412.13501</guid>
<content:encoded><![CDATA[
arXiv:2412.13501v3 Announce Type: replace 
Abstract: Graphical User Interface (GUI) agents, powered by Large Foundation Models, have emerged as a transformative approach to automating human-computer interaction. These agents autonomously interact with digital systems or software applications via GUIs, emulating human actions such as clicking, typing, and navigating visual elements across diverse platforms. Motivated by the growing interest and fundamental importance of GUI agents, we provide a comprehensive survey that categorizes their benchmarks, evaluation metrics, architectures, and training methods. We propose a unified framework that delineates their perception, reasoning, planning, and acting capabilities. Furthermore, we identify important open challenges and discuss key future directions. Finally, this work serves as a basis for practitioners and researchers to gain an intuitive understanding of current progress, techniques, benchmarks, and critical open problems that remain to be addressed.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Gym: A Framework for Enabling and Evaluating Human-Agent Collaboration</title>
<link>https://arxiv.org/abs/2412.15701</link>
<guid>https://arxiv.org/abs/2412.15701</guid>
<content:encoded><![CDATA[
arXiv:2412.15701v3 Announce Type: replace 
Abstract: Recent advancements in language models (LMs) have sparked growing interest in developing LM agents. While fully autonomous agents could excel in many scenarios, numerous use cases inherently require them to collaborate with humans due to humans' latent preferences, domain expertise, or need for control. To facilitate the study of human-agent collaboration, we present Collaborative Gym (Co-Gym), a general framework enabling asynchronous, tripartite interaction among agents, humans, and task environments. We instantiate Co-Gym with three representative tasks in both simulated and real-world conditions, and propose an evaluation framework that assesses both the collaboration outcomes and processes. Our findings reveal that collaborative agents consistently outperform their fully autonomous counterparts in task performance within those delivered cases, achieving win rates of 86% in Travel Planning, 74% in Tabular Analysis, and 66% in Related Work when evaluated by real users. However, our study also highlights significant challenges in developing collaborative agents, requiring advancements in core aspects of intelligence -- communication capabilities, situational awareness, and balancing autonomy and human control.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vintix: Action Model via In-Context Reinforcement Learning</title>
<link>https://arxiv.org/abs/2501.19400</link>
<guid>https://arxiv.org/abs/2501.19400</guid>
<content:encoded><![CDATA[
arXiv:2501.19400v2 Announce Type: replace 
Abstract: In-Context Reinforcement Learning (ICRL) represents a promising paradigm for developing generalist agents that learn at inference time through trial-and-error interactions, analogous to how large language models adapt contextually, but with a focus on reward maximization. However, the scalability of ICRL beyond toy tasks and single-domain settings remains an open challenge. In this work, we present the first steps toward scaling ICRL by introducing a fixed, cross-domain model capable of learning behaviors through in-context reinforcement learning. Our results demonstrate that Algorithm Distillation, a framework designed to facilitate ICRL, offers a compelling and competitive alternative to expert distillation to construct versatile action models. These findings highlight the potential of ICRL as a scalable approach for generalist decision-making systems. Code released at https://github.com/dunnolab/vintix
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Architectures Integrated with Agentic AI for Next-Generation Wireless Networks</title>
<link>https://arxiv.org/abs/2502.01089</link>
<guid>https://arxiv.org/abs/2502.01089</guid>
<content:encoded><![CDATA[
arXiv:2502.01089v3 Announce Type: replace 
Abstract: This paper investigates a range of cutting-edge technologies and architectural innovations aimed at simplifying network operations, reducing operational expenditure (OpEx), and enabling the deployment of new service models. The focus is on (i) Proposing novel, more efficient 6G architectures, with both Control and User planes enabling the seamless expansion of services, while addressing long-term 6G network evolution. (ii) Exploring advanced techniques for constrained artificial intelligence (AI) operations, particularly the design of AI agents for real-time learning, optimizing energy consumption, and the allocation of computational resources. (iii) Identifying technologies and architectures that support the orchestration of backend services using serverless computing models across multiple domains, particularly for vertical industries. (iv) Introducing optically-based, ultra-high-speed, low-latency network architectures, with fast optical switching and real-time control, replacing conventional electronic switching to reduce power consumption by an order of magnitude.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Large-Scale In-Context Reinforcement Learning by Meta-Training in Randomized Worlds</title>
<link>https://arxiv.org/abs/2502.02869</link>
<guid>https://arxiv.org/abs/2502.02869</guid>
<content:encoded><![CDATA[
arXiv:2502.02869v3 Announce Type: replace 
Abstract: In-Context Reinforcement Learning (ICRL) enables agents to learn automatically and on-the-fly from their interactive experiences. However, a major challenge in scaling up ICRL is the lack of scalable task collections. To address this, we propose the procedurally generated tabular Markov Decision Processes, named AnyMDP. Through a carefully designed randomization process, AnyMDP is capable of generating high-quality tasks on a large scale while maintaining relatively low structural biases. To facilitate efficient meta-training at scale, we further introduce decoupled policy distillation and induce prior information in the ICRL framework. Our results demonstrate that, with a sufficiently large scale of AnyMDP tasks, the proposed model can generalize to tasks that were not considered in the training set through versatile in-context learning paradigms. The scalable task set provided by AnyMDP also enables a more thorough empirical investigation of the relationship between data distribution and ICRL performance. We further show that the generalization of ICRL potentially comes at the cost of increased task diversity and longer adaptation periods. This finding carries critical implications for scaling robust ICRL capabilities, highlighting the necessity of diverse and extensive task design, and prioritizing asymptotic performance over few-shot adaptation.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoAgent: Self-evolving Agent with Continual World Model for Long-Horizon Tasks</title>
<link>https://arxiv.org/abs/2502.05907</link>
<guid>https://arxiv.org/abs/2502.05907</guid>
<content:encoded><![CDATA[
arXiv:2502.05907v2 Announce Type: replace 
Abstract: Completing Long-Horizon (LH) tasks in open-ended worlds is an important yet difficult problem for embodied agents. Existing approaches suffer from two key challenges: (1) they heavily rely on experiences obtained from human-created data or curricula, failing to autonomously update and select multimodal experiences, and (2) they may encounter catastrophic forgetting issues when faced with new tasks, failing to autonomously update world knowledge. To solve these challenges, this paper presents {\it EvoAgent}, a self-evolving agent with a continual World Model (WM), which can autonomously complete various LH tasks across environments through self-planning, self-control, and self-reflection, without human intervention. Our proposed EvoAgent contains three modules, i.e., i) the memory-driven planner which uses an LLM along with the WM and interaction memory, to convert LH tasks into executable sub-tasks; ii) the WM-guided action controller which leverages WM to generate low-level actions and incorporates a self-verification mechanism to update multimodal experiences; iii) the experience-inspired reflector which implements a two-stage curriculum learning algorithm to select experiences for task-adaptive WM updates. Moreover, we develop a continual World Model for EvoAgent, which can autonomously update the multimodal experience pool and world knowledge through closed-loop dynamics. We conducted extensive experiments on Minecraft and Atair, compared with existing methods, EvoAgent can achieve an average success rate improvement of 105% and reduce ineffective actions by more than 6x.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Explain Air Traffic Situation</title>
<link>https://arxiv.org/abs/2502.10764</link>
<guid>https://arxiv.org/abs/2502.10764</guid>
<content:encoded><![CDATA[
arXiv:2502.10764v3 Announce Type: replace 
Abstract: Understanding how air traffic controllers construct a mental 'picture' of complex air traffic situations is crucial but remains a challenge due to the inherently intricate, high-dimensional interactions between aircraft, pilots, and controllers. Previous work on modeling the strategies of air traffic controllers and their mental image of traffic situations often centers on specific air traffic control tasks or pairwise interactions between aircraft, neglecting to capture the comprehensive dynamics of an air traffic situation. To address this issue, we propose a machine learning-based framework for explaining air traffic situations. Specifically, we employ a Transformer-based multi-agent trajectory model that encapsulates both the spatio-temporal movement of aircraft and social interaction between them. By deriving attention scores from the model, we can quantify the influence of individual aircraft on overall traffic dynamics. This provides explainable insights into how air traffic controllers perceive and understand the traffic situation. Trained on real-world air traffic surveillance data collected from the terminal airspace around Incheon International Airport in South Korea, our framework effectively explicates air traffic situations. This could potentially support and enhance the decision-making and situational awareness of air traffic controllers.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Learning to Explore via Memory Density Feedback</title>
<link>https://arxiv.org/abs/2503.02831</link>
<guid>https://arxiv.org/abs/2503.02831</guid>
<content:encoded><![CDATA[
arXiv:2503.02831v2 Announce Type: replace 
Abstract: Exploration algorithms for reinforcement learning typically replace or augment the reward function with an additional ``intrinsic'' reward that trains the agent to seek previously unseen states of the environment. Here, we consider an exploration algorithm that exploits meta-learning, or learning to learn, such that the agent learns to maximize its exploration progress within a single episode, even between epochs of training. The agent learns a policy that aims to minimize the probability density of new observations with respect to all of its memories. In addition, it receives as feedback evaluations of the current observation density and retains that feedback in a recurrent network. By remembering trajectories of density, the agent learns to navigate a complex and growing landscape of familiarity in real-time, allowing it to maximize its exploration progress even in completely novel states of the environment for which its policy has not been trained.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PROMFUZZ: Leveraging LLM-Driven and Bug-Oriented Composite Analysis for Detecting Functional Bugs in Smart Contracts</title>
<link>https://arxiv.org/abs/2503.23718</link>
<guid>https://arxiv.org/abs/2503.23718</guid>
<content:encoded><![CDATA[
arXiv:2503.23718v2 Announce Type: replace 
Abstract: Smart contracts are fundamental pillars of the blockchain, playing a crucial role in facilitating various business transactions. However, these smart contracts are vulnerable to exploitable bugs that can lead to substantial monetary losses. A recent study reveals that over 80% of these exploitable bugs, which are primarily functional bugs, can evade the detection of current tools. The primary issue is the significant gap between understanding the high-level logic of the business model and checking the low-level implementations in smart contracts. Furthermore, identifying deeply rooted functional bugs in smart contracts requires the automated generation of effective detection oracles based on various bug features. To address these challenges, we design and implement PROMFUZZ, an automated and scalable system to detect functional bugs, in smart contracts. In PROMFUZZ, we first propose a novel Large Language Model (LLM)-driven analysis framework, which leverages a dual-agent prompt engineering strategy to pinpoint potentially vulnerable functions for further scrutiny. We then implement a dual-stage coupling approach, which focuses on generating invariant checkers that leverage logic information extracted from potentially vulnerable functions. Finally, we design a bug-oriented fuzzing engine, which maps the logical information from the high-level business model to the low-level smart contract implementations, and performs the bug-oriented fuzzing on targeted functions. We compare PROMFUZZ with multiple state-of-the-art methods. The results show that PROMFUZZ achieves 86.96% recall and 93.02% F1-score in detecting functional bugs, marking at least a 50% improvement in both metrics over state-of-the-art methods. Moreover, we perform an in-depth analysis on real-world DeFi projects and detect 30 zero-day bugs. Up to now, 24 zero-day bugs have been assigned CVE IDs.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaintainCoder: Maintainable Code Generation Under Dynamic Requirements</title>
<link>https://arxiv.org/abs/2503.24260</link>
<guid>https://arxiv.org/abs/2503.24260</guid>
<content:encoded><![CDATA[
arXiv:2503.24260v3 Announce Type: replace 
Abstract: Modern code generation has made significant strides in functional correctness and execution efficiency. However, these systems often overlook a critical dimension in real-world software development: maintainability. To handle dynamic requirements with minimal rework, we propose MaintainCoder as a pioneering solution. It integrates the Waterfall model, design patterns, and multi-agent collaboration to systematically enhance cohesion, reduce coupling, achieving clear responsibility boundaries and better maintainability. We also introduce MaintainCoder, a benchmark comprising requirement changes and novel dynamic metrics on maintenance efforts. Experiments demonstrate that existing code generation methods struggle to meet maintainability standards when requirements evolve. In contrast, MaintainCoder improves dynamic maintainability metrics by more than 60% with even higher correctness of initial codes. Furthermore, while static metrics fail to accurately reflect maintainability and even contradict each other, our proposed dynamic metrics exhibit high consistency. Our work not only provides the foundation for maintainable code generation, but also highlights the need for more realistic and comprehensive code generation research. Resources: https://github.com/IAAR-Shanghai/MaintainCoder.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering the Herd: A Framework for LLM-based Control of Social Learning</title>
<link>https://arxiv.org/abs/2504.02648</link>
<guid>https://arxiv.org/abs/2504.02648</guid>
<content:encoded><![CDATA[
arXiv:2504.02648v3 Announce Type: replace 
Abstract: Algorithms increasingly serve as information mediators--from social media feeds and targeted advertising to the increasing ubiquity of LLMs. This engenders a joint process where agents combine private, algorithmically-mediated signals with learning from peers to arrive at decisions. To study such settings, we introduce a model of controlled sequential social learning in which an information-mediating planner (e.g. an LLM) controls the information structure of agents while they also learn from the decisions of earlier agents. The planner may seek to improve social welfare (altruistic planner) or to induce a specific action the planner prefers (biased planner). Our framework presents a new optimization problem for social learning that combines dynamic programming with decentralized action choices and Bayesian belief updates.
  We prove the convexity of the value function and characterize the optimal policies of altruistic and biased planners, which attain desired tradeoffs between the costs they incur and the payoffs they earn from induced agent choices. Notably, in some regimes the biased planner intentionally obfuscates the agents' signals. Even under stringent transparency constraints--information parity with individuals, no lying or cherry-picking, and full observability--we show that information mediation can substantially shift social welfare in either direction. We complement our theory with simulations in which LLMs act as both planner and agents. Notably, the LLM planner in our simulations exhibits emergent strategic behavior in steering public opinion that broadly mirrors the trends predicted, though key deviations suggest the influence of non-Bayesian reasoning consistent with the cognitive patterns of both humans and LLMs trained on human-like data. Together, we establish our framework as a tractable basis for studying the impact and regulation of LLM information mediators.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DataPuzzle: Breaking Free from the Hallucinated Promise of LLMs in Data Analysis</title>
<link>https://arxiv.org/abs/2504.10036</link>
<guid>https://arxiv.org/abs/2504.10036</guid>
<content:encoded><![CDATA[
arXiv:2504.10036v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly applied to multi-modal data analysis -- not necessarily because they offer the most precise answers, but because they provide fluent, flexible interfaces for interpreting complex inputs. Yet this fluency often conceals a deeper structural failure: the prevailing ``Prompt-to-Answer'' paradigm treats LLMs as black-box analysts, collapsing evidence, reasoning, and conclusions into a single, opaque response. The result is brittle, unverifiable, and frequently misleading. We argue for a fundamental shift: from generation to structured extraction, from monolithic prompts to modular, agent-based workflows. LLMs should not serve as oracles, but as collaborators -- specialized in tasks like extraction, translation, and linkage -- embedded within transparent workflows that enable step-by-step reasoning and verification. We propose DataPuzzle, a conceptual multi-agent framework that decomposes complex questions, structures information into interpretable forms (e.g. tables, graphs), and coordinates agent roles to support transparent and verifiable analysis. This framework serves as an aspirational blueprint for restoring visibility and control in LLM-driven analytics -- transforming opaque answers into traceable processes, and brittle fluency into accountable insight. This is not a marginal refinement; it is a call to reimagine how we build trustworthy, auditable analytic systems in the era of large language models. Structure is not a constraint -- it is the path to clarity.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Large-Scale Competitive Team Behaviors with Mean-Field Interactions and Online Opponent Modeling</title>
<link>https://arxiv.org/abs/2504.21164</link>
<guid>https://arxiv.org/abs/2504.21164</guid>
<content:encoded><![CDATA[
arXiv:2504.21164v2 Announce Type: replace 
Abstract: While multi-agent reinforcement learning (MARL) has been proven effective across both collaborative and competitive tasks, existing algorithms often struggle to scale to large populations of agents. Recent advancements in mean-field (MF) theory provide scalable solutions by approximating population interactions as a continuum, yet most existing frameworks focus exclusively on either fully cooperative or purely competitive settings. To bridge this gap, we introduce MF-MAPPO, a mean-field extension of PPO designed for zero-sum team games that integrate intra-team cooperation with inter-team competition. MF-MAPPO employs a shared actor and a minimally informed critic per team and is trained directly on finite-population simulators, thereby enabling deployment to realistic scenarios with thousands of agents. We further show that MF-MAPPO naturally extends to partially observable settings through a simple gradient-regularized training scheme. Our evaluation utilizes large-scale benchmark scenarios using our own testing simulation platform for MF team games (MFEnv), including offense-defense battlefield tasks as well as variants of population-based rock-paper-scissors games that admit analytical solutions, for benchmarking. Across these benchmarks, MF-MAPPO outperforms existing methods and exhibits complex, heterogeneous behaviors, demonstrating the effectiveness of combining mean-field theory and MARL techniques at scale.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KAQG: A Knowledge-Graph-Enhanced RAG for Difficulty-Controlled Question Generation</title>
<link>https://arxiv.org/abs/2505.07618</link>
<guid>https://arxiv.org/abs/2505.07618</guid>
<content:encoded><![CDATA[
arXiv:2505.07618v2 Announce Type: replace 
Abstract: This study introduces Knowledge Augmented Question Generation (KAQG), an educational assessment framework that integrates Item Response Theory, abbreviated as IRT, Bloom's Taxonomy, and knowledge graphs into a multi-agent Retrieval-Augmented Generation (RAG) system. The proposed approach overcomes limitations of existing methods by enabling fine-grained control over item difficulty, psychometric calibration, and cognitive alignment. It employs multi-graph isolation to preserve domain-specific semantics and leverages a distributed agent architecture coordinated through Data Distribution Service, abbreviated as DDS, for scalable and fault-tolerant operations. Each agent specializes in tasks such as retrieval, generation, or evaluation, forming a modular and traceable pipeline. Distinctively, the framework encodes semantic hierarchies, PageRank-based concept weighting, and assessment-theory parameters directly into the generation process, ensuring that questions are both contextually grounded and cognitively calibrated. Deployed at Taiwan's National Institute of Environmental Research, the system has demonstrated practical value by reducing manual workload, improving reliability and validity, and supporting both adaptive and standardized assessments. By integrating psychometric theory with AI-driven retrieval and generation, this work establishes a scalable and cognitively aligned solution for education and professional certification.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SALM: A Multi-Agent Framework for Language Model-Driven Social Network Simulation</title>
<link>https://arxiv.org/abs/2505.09081</link>
<guid>https://arxiv.org/abs/2505.09081</guid>
<content:encoded><![CDATA[
arXiv:2505.09081v2 Announce Type: replace 
Abstract: Contemporary approaches to agent-based modeling (ABM) of social systems have traditionally emphasized rule-based behaviors, limiting their ability to capture nuanced dynamics by moving beyond predefined rules and leveraging contextual understanding from LMs of human social interaction. This paper presents SALM (Social Agent LM Framework), a novel approach for integrating language models (LMs) into social network simulation that achieves unprecedented temporal stability in multi-agent scenarios. Our primary contributions include: (1) a hierarchical prompting architecture enabling stable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2) an attention-based memory system achieving 80% cache hit rates (95% CI [78%, 82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on personality stability. Through extensive validation against SNAP ego networks, we demonstrate the first LLM-based framework capable of modeling long-term social phenomena while maintaining empirically validated behavioral fidelity.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-tuning Diffusion Policies with Backpropagation Through Diffusion Timesteps</title>
<link>https://arxiv.org/abs/2505.10482</link>
<guid>https://arxiv.org/abs/2505.10482</guid>
<content:encoded><![CDATA[
arXiv:2505.10482v4 Announce Type: replace 
Abstract: Diffusion policies, widely adopted in decision-making scenarios such as robotics, gaming and autonomous driving, are capable of learning diverse skills from demonstration data due to their high representation power. However, the sub-optimal and limited coverage of demonstration data could lead to diffusion policies that generate sub-optimal trajectories and even catastrophic failures. While reinforcement learning (RL)-based fine-tuning has emerged as a promising solution to address these limitations, existing approaches struggle to effectively adapt Proximal Policy Optimization (PPO) to diffusion models. This challenge stems from the computational intractability of action likelihood estimation during the denoising process, which leads to complicated optimization objectives. In our experiments starting from randomly initialized policies, we find that online tuning of Diffusion Policies demonstrates much lower sample efficiency compared to directly applying PPO on MLP policies (MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework that reformulates Diffusion Policy as a noise-conditioned deterministic policy. By treating each denoising step as a differentiable transformation conditioned on pre-sampled noise, NCDPO enables tractable likelihood evaluation and gradient backpropagation through all diffusion timesteps. Our experiments demonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when training from scratch, outperforming existing methods in both sample efficiency and final performance across diverse benchmarks, including continuous robot control and multi-agent game scenarios. Furthermore, our experimental results show that our method is robust to the number denoising timesteps in the Diffusion Policy.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Optimization for Feature Selection with Permutation-Invariant Embedding and Policy-Guided Search</title>
<link>https://arxiv.org/abs/2505.11601</link>
<guid>https://arxiv.org/abs/2505.11601</guid>
<content:encoded><![CDATA[
arXiv:2505.11601v2 Announce Type: replace 
Abstract: Feature selection removes redundant features to enhanc performance and computational efficiency in downstream tasks. Existing works often struggle to capture complex feature interactions and adapt to diverse scenarios. Recent advances in this domain have incorporated generative intelligence to address these drawbacks by uncovering intricate relationships between features. However, two key limitations remain: 1) embedding feature subsets in a continuous space is challenging due to permutation sensitivity, as changes in feature order can introduce biases and weaken the embedding learning process; 2) gradient-based search in the embedding space assumes convexity, which is rarely guaranteed, leading to reduced search effectiveness and suboptimal subsets. To address these limitations, we propose a new framework that can: 1) preserve feature subset knowledge in a continuous embedding space while ensuring permutation invariance; 2) effectively explore the embedding space without relying on strong convex assumptions. For the first objective, we develop an encoder-decoder paradigm to preserve feature selection knowledge into a continuous embedding space. This paradigm captures feature interactions through pairwise relationships within the subset, removing the influence of feature order on the embedding. Moreover, an inducing point mechanism is introduced to accelerate pairwise relationship computations. For the second objective, we employ a policy-based reinforcement learning (RL) approach to guide the exploration of the embedding space. The RL agent effectively navigates the space by balancing multiple objectives. By prioritizing high-potential regions adaptively and eliminating the reliance on convexity assumptions, the RL agent effectively reduces the risk of converging to local optima. Extensive experiments demonstrate the effectiveness, efficiency, robustness and explicitness of our model.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MobileIPL: Enhancing Mobile Agents Thinking Process via Iterative Preference Learning</title>
<link>https://arxiv.org/abs/2505.12299</link>
<guid>https://arxiv.org/abs/2505.12299</guid>
<content:encoded><![CDATA[
arXiv:2505.12299v3 Announce Type: replace 
Abstract: The Chain of Action-Planning Thoughts (CoaT) paradigm has been shown to improve the reasoning performance of VLM-based mobile agents in GUI tasks. However, the scarcity of diverse CoaT trajectories limits the expressiveness and generalization ability of such agents. While self-training is commonly employed to address data scarcity, existing approaches either overlook the correctness of intermediate reasoning steps or depend on expensive process-level annotations to construct process reward models (PRM). To address the above problems, we propose an Iterative Preference Learning (IPL) that constructs a CoaT-tree through interative sampling, scores leaf nodes using rule-based reward, and backpropagates feedback to derive Thinking-level Direct Preference Optimization (T-DPO) pairs. To prevent overfitting during warm-up supervised fine-tuning, we further introduce a three-stage instruction evolution, which leverages GPT-4o to generate diverse Q\&amp;A pairs based on real mobile UI screenshots, enhancing both generality and layout understanding. Experiments on three standard Mobile GUI-agent benchmarks demonstrate that our agent MobileIPL outperforms strong baselines, including continual pretraining models such as OS-ATLAS and UI-TARS. It achieves state-of-the-art performance across three standard Mobile GUI-Agents benchmarks and shows strong generalization to out-of-domain scenarios.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PiFlow: Principle-aware Scientific Discovery with Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2505.15047</link>
<guid>https://arxiv.org/abs/2505.15047</guid>
<content:encoded><![CDATA[
arXiv:2505.15047v2 Announce Type: replace 
Abstract: Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate remarkable potential for scientific discovery. Existing approaches, however, often automate scientific discovery using predefined workflows that lack rationality constraints. This often leads to aimless hypothesizing and a failure to consistently link hypotheses with evidence, thereby hindering the systematic reduction of uncertainty. Overcoming these limitations fundamentally requires a principled approach to exploration. We introduce PiFlow, an information-theoretical framework, treating automated scientific discovery as a structured uncertainty reduction problem guided by principles (e.g., scientific laws). In evaluations across three distinct scientific domains -- discovering nanomaterial structures, bio-molecules, and superconductor candidates with targeted properties -- our method significantly improves discovery efficiency, reflected by a 73.55\% increase in the Area Under the Curve (AUC) of property values versus exploration steps, and enhances solution quality by 94.06\% compared to a vanilla agent system. Overall, PiFlow serves as a Plug-and-Play method, establishing a novel paradigm shift in highly efficient automated scientific discovery, paving the way for more robust and accelerated AI-driven research. Code is publicly available at our \href{https://github.com/amair-lab/PiFlow}{GitHub}.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection</title>
<link>https://arxiv.org/abs/2505.15182</link>
<guid>https://arxiv.org/abs/2505.15182</guid>
<content:encoded><![CDATA[
arXiv:2505.15182v2 Announce Type: replace 
Abstract: Recent advances in LLM agents have largely built on reasoning backbones like ReAct, which interleave thought and action in complex environments. However, ReAct often produces ungrounded or incoherent reasoning steps, leading to misalignment between the agent's actual state and goal. Our analysis finds that this stems from ReAct's inability to maintain consistent internal beliefs and goal alignment, causing compounding errors and hallucinations. To address this, we introduce ReflAct, a novel backbone that shifts reasoning from merely planning next actions to continuously reflecting on the agent's state relative to its goal. By explicitly grounding decisions in states and enforcing ongoing goal alignment, ReflAct dramatically improves strategic reliability. This design delivers substantial empirical gains: ReflAct surpasses ReAct by 27.7% on average, achieving a 93.3% success rate in ALFWorld. Notably, ReflAct even outperforms ReAct with added enhancement modules (e.g., Reflexion, WKM), showing that strengthening the core reasoning backbone is key to reliable agent performance.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.15298</link>
<guid>https://arxiv.org/abs/2505.15298</guid>
<content:encoded><![CDATA[
arXiv:2505.15298v4 Announce Type: replace 
Abstract: Vision-Language Models (VLMs) show promise for autonomous driving, yet their struggle with hallucinations, inefficient reasoning, and limited real-world validation hinders accurate perception and robust step-by-step reasoning. To overcome this, we introduce \textbf{AgentThink}, a pioneering unified framework that integrates Chain-of-Thought (CoT) reasoning with dynamic, agent-style tool invocation for autonomous driving tasks. AgentThink's core innovations include: \textbf{(i) Structured Data Generation}, which establishes an autonomous driving tool library to automatically construct structured, self-verified reasoning data explicitly incorporating tool usage for diverse driving scenarios; \textbf{(ii) A Two-stage Training Pipeline}, employing Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO) to equip VLMs with the capability for autonomous tool invocation; and \textbf{(iii) Agent-style Tool-Usage Evaluation}, introducing a novel multi-tool assessment protocol to rigorously evaluate the model's tool invocation and utilization. Experiments on the DriveLMM-o1 benchmark demonstrate that AgentThink significantly boosts overall reasoning scores by \textbf{53.91%} and enhances answer accuracy by \textbf{33.54%}, while markedly improving reasoning quality and consistency. Furthermore, ablation studies and robust zero-shot/few-shot generalization experiments across various benchmarks underscore its powerful capabilities. These findings highlight a promising trajectory for developing trustworthy and tool-aware autonomous driving models. Code is available at https://github.com/curryqka/AgentThink.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Black Boxes: Interpretable and Interactable Predictive Healthcare with Knowledge-Enhanced Agentic Causal Discovery</title>
<link>https://arxiv.org/abs/2505.16288</link>
<guid>https://arxiv.org/abs/2505.16288</guid>
<content:encoded><![CDATA[
arXiv:2505.16288v2 Announce Type: replace 
Abstract: Deep learning models trained on extensive Electronic Health Records (EHR) data have achieved high accuracy in diagnosis prediction, offering the potential to assist clinicians in decision-making and treatment planning. However, these models lack two crucial features that clinicians highly value: interpretability and interactivity. The ``black-box'' nature of these models makes it difficult for clinicians to understand the reasoning behind predictions, limiting their ability to make informed decisions. Additionally, the absence of interactive mechanisms prevents clinicians from incorporating their own knowledge and experience into the decision-making process. To address these limitations, we propose II-KEA, a knowledge-enhanced agent-driven causal discovery framework that integrates personalized knowledge databases and agentic LLMs. II-KEA enhances interpretability through explicit reasoning and causal analysis, while also improving interactivity by allowing clinicians to inject their knowledge and experience through customized knowledge bases and prompts. II-KEA is evaluated on both MIMIC-III and MIMIC-IV, demonstrating superior performance along with enhanced interpretability and interactivity, as evidenced by its strong results from extensive case studies.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fuzzy Information Evolution with Three-Way Decision in Social Network Group Decision-Making</title>
<link>https://arxiv.org/abs/2505.16781</link>
<guid>https://arxiv.org/abs/2505.16781</guid>
<content:encoded><![CDATA[
arXiv:2505.16781v2 Announce Type: replace 
Abstract: In group decision-making (GDM) scenarios, uncertainty, dynamic social structures, and vague information present major challenges for traditional opinion dynamics models. To address these issues, this study proposes a novel social network group decision-making (SNGDM) framework that integrates three-way decision (3WD) theory, dynamic network reconstruction, and linguistic opinion representation. First, the 3WD mechanism is introduced to explicitly model hesitation and ambiguity in agent judgments, thereby preventing irrational decisions. Second, a connection adjustment rule based on opinion similarity is developed, enabling agents to adaptively update their communication links and better reflect the evolving nature of social relationships. Third, linguistic terms are used to describe agent opinions, allowing the model to handle subjective, vague, or incomplete information more effectively. Finally, an integrated multi-agent decision-making framework is constructed, which simultaneously considers individual uncertainty, opinion evolution, and network dynamics. The proposed model is applied to a multi-UAV cooperative decision-making scenario, where simulation results and consensus analysis demonstrate its effectiveness. Experimental comparisons further verify the advantages of the algorithm in enhancing system stability and representing realistic decision-making behaviors.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Runtime Adaptive Pruning for LLM Inference</title>
<link>https://arxiv.org/abs/2505.17138</link>
<guid>https://arxiv.org/abs/2505.17138</guid>
<content:encoded><![CDATA[
arXiv:2505.17138v4 Announce Type: replace 
Abstract: Large language models (LLMs) excel at language understanding and generation, but their enormous computational and memory requirements hinder deployment. Compression offers a potential solution to mitigate these constraints. However, most existing methods rely on fixed heuristics and thus fail to adapt to runtime memory variations or heterogeneous KV-cache demands arising from diverse user requests. To address these limitations, we propose RAP, an elastic pruning framework driven by reinforcement learning (RL) that dynamically adjusts compression strategies in a runtime-aware manner. Specifically, RAP dynamically tracks the evolving ratio between model parameters and KV-cache across practical execution. Recognizing that FFNs house most parameters, whereas parameter -light attention layers dominate KV-cache formation, the RL agent retains only those components that maximize utility within the current memory budget, conditioned on instantaneous workload and device state. Extensive experiments results demonstrate that RAP outperforms state-of-the-art baselines, marking the first time to jointly consider model weights and KV-cache on the fly.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models</title>
<link>https://arxiv.org/abs/2505.17826</link>
<guid>https://arxiv.org/abs/2505.17826</guid>
<content:encoded><![CDATA[
arXiv:2505.17826v3 Announce Type: replace 
Abstract: Trinity-RFT is a general-purpose, unified and easy-to-use framework designed for reinforcement fine-tuning (RFT) of large language models. It is built with a modular and decoupled design, consisting of (1) an RFT-core that unifies and generalizes synchronous/asynchronous, on-policy/off-policy, and online/offline modes of RFT; (2) seamless integration for agent-environment interaction with high efficiency and robustness; and (3) systematic data pipelines optimized for RFT. Trinity-RFT can be easily adapted for diverse application scenarios, and serves as a unified platform for development and research of advanced reinforcement learning paradigms at both macroscopic and microscopic levels. This technical report outlines the vision, features, design and implementations of Trinity-RFT, accompanied by extensive examples, applications and experiments that demonstrate its functionalities and user-friendliness.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2505.18943</link>
<guid>https://arxiv.org/abs/2505.18943</guid>
<content:encoded><![CDATA[
arXiv:2505.18943v2 Announce Type: replace 
Abstract: Human social interactions depend on the ability to infer others' unspoken intentions, emotions, and beliefs-a cognitive skill grounded in the psychological concept of Theory of Mind (ToM). While large language models (LLMs) excel in semantic understanding tasks, they struggle with the ambiguity and contextual nuance inherent in human communication. To bridge this gap, we introduce MetaMind, a multi-agent framework inspired by psychological theories of metacognition, designed to emulate human-like social reasoning. MetaMind decomposes social understanding into three collaborative stages: (1) a Theory-of-Mind Agent generates hypotheses about user mental states (e.g., intent, emotion), (2) a Moral Agent refines these hypotheses using cultural norms and ethical constraints, and (3) a Response Agent generates contextually appropriate responses while validating alignment with inferred intent. Our framework achieves state-of-the-art performance across three challenging benchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain in ToM reasoning. Notably, it enables LLMs to match human-level performance on key ToM tasks for the first time. Ablation studies confirm the necessity of all components, which showcase the framework's ability to balance contextual plausibility, social appropriateness, and user adaptation. This work advances AI systems toward human-like social intelligence, with applications in empathetic dialogue and culturally sensitive interactions. Code is available at https://github.com/XMZhangAI/MetaMind.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Single to Multi-Granularity: Toward Long-Term Memory Association and Selection of Conversational Agents</title>
<link>https://arxiv.org/abs/2505.19549</link>
<guid>https://arxiv.org/abs/2505.19549</guid>
<content:encoded><![CDATA[
arXiv:2505.19549v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have recently been widely adopted in conversational agents. However, the increasingly long interactions between users and agents accumulate extensive dialogue records, making it difficult for LLMs with limited context windows to maintain a coherent long-term dialogue memory and deliver personalized responses. While retrieval-augmented memory systems have emerged to address this issue, existing methods often depend on single-granularity memory segmentation and retrieval. This approach falls short in capturing deep memory connections, leading to partial retrieval of useful information or substantial noise, resulting in suboptimal performance. To tackle these limits, we propose MemGAS, a framework that enhances memory consolidation by constructing multi-granularity association, adaptive selection, and retrieval. MemGAS is based on multi-granularity memory units and employs Gaussian Mixture Models to cluster and associate new memories with historical ones. An entropy-based router adaptively selects optimal granularity by evaluating query relevance distributions and balancing information completeness and noise. Retrieved memories are further refined via LLM-based filtering. Experiments on four long-term memory benchmarks demonstrate that MemGAS outperforms state-of-the-art methods on both question answer and retrieval tasks, achieving superior performance across different query types and top-K settings. \footnote{https://github.com/quqxui/MemGAS}
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents</title>
<link>https://arxiv.org/abs/2505.20148</link>
<guid>https://arxiv.org/abs/2505.20148</guid>
<content:encoded><![CDATA[
arXiv:2505.20148v3 Announce Type: replace 
Abstract: Spatial Planning is a crucial part in the field of spatial intelligence, which requires the understanding and planning about object arrangements in space perspective. AI agents with the spatial planning ability can better adapt to various real-world applications, including robotic manipulation, automatic assembly, urban planning etc. Recent works have attempted to construct benchmarks for evaluating the spatial intelligence of Multimodal Large Language Models (MLLMs). Nevertheless, these benchmarks primarily focus on spatial reasoning based on typical Visual Question-Answering (VQA) forms, which suffers from the gap between abstract spatial understanding and concrete task execution. In this work, we take a step further to build a comprehensive benchmark called MineAnyBuild, aiming to evaluate the spatial planning ability of open-world AI agents in the Minecraft game. Specifically, MineAnyBuild requires an agent to generate executable architecture building plans based on the given multi-modal human instructions. It involves 4,000 curated spatial planning tasks and also provides a paradigm for infinitely expandable data collection by utilizing rich player-generated content. MineAnyBuild evaluates spatial planning through four core supporting dimensions: spatial understanding, spatial reasoning, creativity, and spatial commonsense. Based on MineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based agents, revealing the severe limitations but enormous potential in their spatial planning abilities. We believe our MineAnyBuild will open new avenues for the evaluation of spatial intelligence and help promote further development for open-world AI agents capable of spatial planning.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long Context Scaling: Divide and Conquer via Multi-Agent Question-driven Collaboration</title>
<link>https://arxiv.org/abs/2505.20625</link>
<guid>https://arxiv.org/abs/2505.20625</guid>
<content:encoded><![CDATA[
arXiv:2505.20625v3 Announce Type: replace 
Abstract: Processing long contexts has become a critical capability for modern large language models (LLMs). Existing works leverage agent-based divide-and-conquer methods for processing long contexts. But these methods face crucial limitations, including prohibitive accumulated latency and amplified information loss from excessive agent invocations, and the disruption of inherent textual dependencies by immoderate partitioning. In this paper, we propose a novel multi-agent framework XpandA (Expand-Agent) coupled with question-driven workflow and dynamic partitioning for robust long-context processing. XpandA overcomes these limitations through: 1) dynamic partitioning of long texts, which adaptively modulates the filling rate of context windows for input sequences of vastly varying lengths; 2) question-guided protocol to update flat information ensembles within centralized shared memory, constructing consistent inter-agent knowledge across partitions; and 3) selectively replaying specific partitions based on the state-tracking of question-information couples to promote the resolution of inverted-order structures across partitions (e.g., flashbacks). We perform a comprehensive evaluation of XpandA on multiple long-context benchmarks with length varying from 1k to 1M, demonstrating XpandA's feasibility for processing ultra-long sequences and its significant effectiveness in enhancing the long-context capabilities of various LLMs by achieving 20\% improvements and 1.5x inference speedup over baselines of full-context, RAG and previous agent-based methods.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset</title>
<link>https://arxiv.org/abs/2505.21979</link>
<guid>https://arxiv.org/abs/2505.21979</guid>
<content:encoded><![CDATA[
arXiv:2505.21979v3 Announce Type: replace 
Abstract: Mainstream large vision-language models (LVLMs) inherently encode cultural biases, highlighting the need for diverse multimodal datasets. To address this gap, we introduce PEARL, a large-scale Arabic multimodal dataset and benchmark explicitly designed for cultural understanding. Constructed through advanced agentic workflows and extensive human-in-the-loop annotations by 37 annotators from across the Arab world, PEARL comprises over 309K multimodal examples spanning ten culturally significant domains covering all Arab countries. We further provide two robust evaluation benchmarks (PEARL and PEARL-LITE) along with a specialized subset (PEARL-X) explicitly developed to assess nuanced cultural variations. Comprehensive evaluations on state-of-the-art open and proprietary LVLMs demonstrate that reasoning-centric instruction alignment substantially improves models' cultural grounding compared to conventional scaling methods. PEARL establishes a foundational resource for advancing culturally-informed multimodal modeling research. All datasets and benchmarks are publicly available.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication-Efficient Desire Alignment for Embodied Agent-Human Adaptation</title>
<link>https://arxiv.org/abs/2505.22503</link>
<guid>https://arxiv.org/abs/2505.22503</guid>
<content:encoded><![CDATA[
arXiv:2505.22503v2 Announce Type: replace 
Abstract: While embodied agents have made significant progress in performing complex physical tasks, real-world applications demand more than pure task execution. The agents must collaborate with unfamiliar agents and human users, whose goals are often vague and implicit. In such settings, interpreting ambiguous instructions and uncovering underlying desires is essential for effective assistance. Therefore, fast and accurate desire alignment becomes a critical capability for embodied agents. In this work, we first develop a home assistance simulation environment HA-Desire that integrates an LLM-driven proxy human user exhibiting realistic value-driven goal selection and communication. The ego agent must interact with this proxy user to infer and adapt to the user's latent desires. To achieve this, we present a novel framework FAMER for fast desire alignment, which introduces a desire-based mental reasoning mechanism to identify user intent and filter desire-irrelevant actions. We further design a reflection-based communication module that reduces redundant inquiries, and incorporate goal-relevant information extraction with memory persistence to improve information reuse and reduce unnecessary exploration. Extensive experiments demonstrate that our framework significantly enhances both task execution and communication efficiency, enabling embodied agents to quickly adapt to user-specific desires in complex embodied environments.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Ultimate Test of Superintelligent AI Agents: Can an AI Balance Care and Control in Asymmetric Relationships?</title>
<link>https://arxiv.org/abs/2506.01813</link>
<guid>https://arxiv.org/abs/2506.01813</guid>
<content:encoded><![CDATA[
arXiv:2506.01813v4 Announce Type: replace 
Abstract: This paper introduces the Shepherd Test, a new conceptual test for assessing the moral and relational dimensions of superintelligent artificial agents. The test is inspired by human interactions with animals, where ethical considerations about care, manipulation, and consumption arise in contexts of asymmetric power and self-preservation. We argue that AI crosses an important, and potentially dangerous, threshold of intelligence when it exhibits the ability to manipulate, nurture, and instrumentally use less intelligent agents, while also managing its own survival and expansion goals. This includes the ability to weigh moral trade-offs between self-interest and the well-being of subordinate agents. The Shepherd Test thus challenges traditional AI evaluation paradigms by emphasizing moral agency, hierarchical behavior, and complex decision-making under existential stakes. We argue that this shift is critical for advancing AI governance, particularly as AI systems become increasingly integrated into multi-agent environments. We conclude by identifying key research directions, including the development of simulation environments for testing moral behavior in AI, and the formalization of ethical manipulation within multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games</title>
<link>https://arxiv.org/abs/2506.03610</link>
<guid>https://arxiv.org/abs/2506.03610</guid>
<content:encoded><![CDATA[
arXiv:2506.03610v2 Announce Type: replace 
Abstract: Large Language Model (LLM) agents are reshaping the game industry, particularly with more intelligent and human-preferable game characters. However, existing game benchmarks fall short of practical needs: they lack evaluations of diverse LLM capabilities across various game genres, studies of agentic modules crucial for complex gameplay, and fine-tuning datasets for aligning pre-trained LLMs into gaming agents. To fill these gaps, we present Orak, a foundational benchmark designed to train and evaluate LLM agents across diverse real-world video games. Unlike existing benchmarks, Orak includes 12 popular video games spanning all major genres, enabling comprehensive studies of LLM capabilities and agentic modules essential for intricate game scenarios. To support consistent evaluation of LLMs, we introduce a plug-and-play interface based on Model Context Protocol (MCP) that enables LLMs to seamlessly connect with games and manipulate agentic modules. Additionally, we propose a fine-tuning dataset, consisting of LLM gameplay trajectories across diverse game genres. Orak offers a comprehensive evaluation framework, encompassing general game score leaderboards, LLM battle arenas, and in-depth analyses of visual input state, agentic strategies, and fine-tuning effects, establishing a foundation towards building generic gaming agents. Code is available at https://github.com/krafton-ai/Orak.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Is Diversity Rewarded in Cooperative Multi-Agent Learning?</title>
<link>https://arxiv.org/abs/2506.09434</link>
<guid>https://arxiv.org/abs/2506.09434</guid>
<content:encoded><![CDATA[
arXiv:2506.09434v2 Announce Type: replace 
Abstract: The success of teams in robotics, nature, and society often depends on the division of labor among diverse specialists; however, a principled explanation for when such diversity surpasses a homogeneous team is still missing. Focusing on multi-agent task allocation problems, we study this question from the perspective of reward design: what kinds of objectives are best suited for heterogeneous teams? We first consider an instantaneous, non-spatial setting where the global reward is built by two generalized aggregation operators: an inner operator that maps the $N$ agents' effort allocations on individual tasks to a task score, and an outer operator that merges the $M$ task scores into the global team reward. We prove that the curvature of these operators determines whether heterogeneity can increase reward, and that for broad reward families this collapses to a simple convexity test. Next, we ask what incentivizes heterogeneity to emerge when embodied, time-extended agents must learn an effort allocation policy. To study heterogeneity in such settings, we use multi-agent reinforcement learning (MARL) as our computational paradigm, and introduce Heterogeneity Gain Parameter Search (HetGPS), a gradient-based algorithm that optimizes the parameter space of underspecified MARL environments to find scenarios where heterogeneity is advantageous. Across different environments, we show that HetGPS rediscovers the reward regimes predicted by our theory to maximize the advantage of heterogeneity, both validating HetGPS and connecting our theoretical insights to reward design in MARL. Together, these results help us understand when behavioral diversity delivers a measurable benefit.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Jailbreaking: Auditing Contextual Privacy in LLM Agents</title>
<link>https://arxiv.org/abs/2506.10171</link>
<guid>https://arxiv.org/abs/2506.10171</guid>
<content:encoded><![CDATA[
arXiv:2506.10171v3 Announce Type: replace 
Abstract: LLM agents have begun to appear as personal assistants, customer service bots, and clinical aides. While these applications deliver substantial operational benefits, they also require continuous access to sensitive data, which increases the likelihood of unauthorized disclosures. Moreover, these disclosures go beyond mere explicit disclosure, leaving open avenues for gradual manipulation or sidechannel information leakage. This study proposes an auditing framework for conversational privacy that quantifies an agent's susceptibility to these risks. The proposed Conversational Manipulation for Privacy Leakage (CMPL) framework is designed to stress-test agents that enforce strict privacy directives against an iterative probing strategy. Rather than focusing solely on a single disclosure event or purely explicit leakage, CMPL simulates realistic multi-turn interactions to systematically uncover latent vulnerabilities. Our evaluation on diverse domains, data modalities, and safety configurations demonstrates the auditing framework's ability to reveal privacy risks that are not deterred by existing single-turn defenses, along with an in-depth longitudinal study of the temporal dynamics of leakage, strategies adopted by adaptive adversaries, and the evolution of adversarial beliefs about sensitive targets. In addition to introducing CMPL as a diagnostic tool, the paper delivers (1) an auditing procedure grounded in quantifiable risk metrics and (2) an open benchmark for evaluation of conversational privacy across agent implementations.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient LLM Collaboration via Planning</title>
<link>https://arxiv.org/abs/2506.11578</link>
<guid>https://arxiv.org/abs/2506.11578</guid>
<content:encoded><![CDATA[
arXiv:2506.11578v2 Announce Type: replace 
Abstract: Recently, large language models (LLMs) have demonstrated strong performance, ranging from simple to complex tasks. However, while large proprietary models (e.g., models with over 100B parameters) achieve remarkable results across diverse tasks, they are often accessible through costly APIs, making frequent use too costly for many applications. In contrast, small open-source models (e.g., models with fewer than 3B parameters) are freely available and easy to deploy locally, but their performance on complex tasks remains limited. This trade-off raises a natural question: how can small and large models efficiently collaborate to combine their complementary strengths? To bridge this trade-off, we propose COPE, a test-time collaboration framework. A planner model first generates a plan, a high-level abstraction of the task, and this plan serves as a lightweight intermediate that guides a downstream executor model. Small and large models take turns acting as planner and executor, exchanging plans in a multi-stage cascade to collaboratively solve tasks. Through comprehensive experiments on benchmarks spanning mathematical reasoning, code generation, open-ended tasks, and agent tasks, we demonstrate that COPE achieves performance comparable to large proprietary models, while drastically reducing the inference API cost. These results
  highlight planning as an effective prior for cost-efficient inference.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tiered Agentic Oversight: A Hierarchical Multi-Agent System for Healthcare Safety</title>
<link>https://arxiv.org/abs/2506.12482</link>
<guid>https://arxiv.org/abs/2506.12482</guid>
<content:encoded><![CDATA[
arXiv:2506.12482v2 Announce Type: replace 
Abstract: Large language models (LLMs) deployed as agents introduce significant safety risks in clinical settings due to their potential for error and single points of failure. We introduce Tiered Agentic Oversight (TAO), a hierarchical multi-agent system that enhances AI safety through layered, automated supervision. Inspired by clinical hierarchies (e.g., nurse-physician-specialist) in hospital, TAO routes tasks to specialized agents based on complexity, creating a robust safety framework through automated inter- and intra-tier communication and role-playing. Crucially, this hierarchical structure functions as an effective error-correction mechanism, absorbing up to 24% of individual agent errors before they can compound. Our experiments reveal TAO outperforms single-agent and other multi-agent systems on 4 out of 5 healthcare safety benchmarks, with up to an 8.2% improvement. Ablation studies confirm key design principles of the system: (i) its adaptive architecture is over 3% safer than static, single-tier configurations, and (ii) its lower tiers are indispensable, as their removal causes the most significant degradation in overall safety. Finally, we validated the system's synergy with human doctors in a user study where a physician, acting as the highest tier agent, provided corrective feedback that improved medical triage accuracy from 40% to 60%. Project Page: https://tiered-agentic-oversight.github.io/
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StorySage: Conversational Autobiography Writing Powered by a Multi-Agent Framework</title>
<link>https://arxiv.org/abs/2506.14159</link>
<guid>https://arxiv.org/abs/2506.14159</guid>
<content:encoded><![CDATA[
arXiv:2506.14159v2 Announce Type: replace 
Abstract: Every individual carries a unique and personal life story shaped by their memories and experiences. However, these memories are often scattered and difficult to organize into a coherent narrative, a challenge that defines the task of autobiography writing. Existing conversational writing assistants tend to rely on generic user interactions and pre-defined guidelines, making it difficult for these systems to capture personal memories and develop a complete biography over time. We introduce StorySage, a user-driven software system designed to meet the needs of a diverse group of users that supports a flexible conversation and a structured approach to autobiography writing. Powered by a multi-agent framework composed of an Interviewer, Session Scribe, Planner, Section Writer, and Session Coordinator, our system iteratively collects user memories, updates their autobiography, and plans for future conversations. In experimental simulations, StorySage demonstrates its ability to navigate multiple sessions and capture user memories across many conversations. User studies (N=28) highlight how StorySage maintains improved conversational flow, narrative completeness, and higher user satisfaction when compared to a baseline. In summary, StorySage contributes both a novel architecture for autobiography writing and insights into how multi-agent systems can enhance human-AI creative partnerships.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FindingDory: A Benchmark to Evaluate Memory in Embodied Agents</title>
<link>https://arxiv.org/abs/2506.15635</link>
<guid>https://arxiv.org/abs/2506.15635</guid>
<content:encoded><![CDATA[
arXiv:2506.15635v2 Announce Type: replace 
Abstract: Large vision-language models have recently demonstrated impressive performance in planning and control tasks, driving interest in their application to real-world robotics. However, deploying these models for reasoning in embodied contexts is limited by their ability to incorporate long-term experience collected across multiple days and represented by vast collections of images. Current VLMs typically struggle to process more than a few hundred images concurrently, highlighting the need for more efficient mechanisms to handle long-term memory in embodied settings. To effectively evaluate these models for long-horizon control, a benchmark must specifically target scenarios where memory is crucial for success. Existing long-video QA benchmarks overlook embodied challenges like object manipulation and navigation, which demand low-level skills and fine-grained reasoning over past interactions. Moreover, effective memory integration in embodied agents involves both recalling relevant historical information and executing actions based on that information, making it essential to study these aspects together rather than in isolation. In this work, we introduce a new benchmark for long-range embodied tasks in the Habitat simulator. This benchmark evaluates memory-based capabilities across 60 tasks requiring sustained engagement and contextual awareness in an environment. The tasks can also be procedurally extended to longer and more challenging versions, enabling scalable evaluation of memory and reasoning. We also present baselines that integrate state-of-the-art VLMs with low level navigation policies, assessing their performance on these memory-intensive tasks and highlight areas for improvement.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prover Agent: An Agent-Based Framework for Formal Mathematical Proofs</title>
<link>https://arxiv.org/abs/2506.19923</link>
<guid>https://arxiv.org/abs/2506.19923</guid>
<content:encoded><![CDATA[
arXiv:2506.19923v3 Announce Type: replace 
Abstract: We present Prover Agent, a novel AI agent for automated theorem proving that integrates large language models (LLMs) with a formal proof assistant, Lean. Prover Agent coordinates an informal reasoning LLM, a formal prover model, and feedback from Lean while also generating auxiliary lemmas. These auxiliary lemmas are not limited to subgoals in the formal proof but can also include special cases or potentially useful facts derived from the assumptions, which help in discovering a viable proof strategy. It achieves an 88.1% success rate on the MiniF2F benchmark, establishing a new state-of-the-art among methods using small language models (SLMs) with a much lower sample budget than previous approaches. We also present theoretical analyses and case studies that illustrate how these generated lemmas contribute to solving challenging problems.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Trust Generative Agents to Mimic Communication on Social Networks Unless You Benchmarked their Empirical Realism</title>
<link>https://arxiv.org/abs/2506.21974</link>
<guid>https://arxiv.org/abs/2506.21974</guid>
<content:encoded><![CDATA[
arXiv:2506.21974v2 Announce Type: replace 
Abstract: The ability of Large Language Models (LLMs) to mimic human behavior triggered a plethora of computational social science research, assuming that empirical studies of humans can be conducted with AI agents instead. Since there have been conflicting research findings on whether and when this hypothesis holds, there is a need to better understand the differences in their experimental designs. We focus on replicating the behavior of social network users with the use of LLMs for the analysis of communication on social networks. First, we provide a formal framework for the simulation of social networks, before focusing on the sub-task of imitating user communication. We empirically test different approaches to imitate user behavior on X in English and German. Our findings suggest that social simulations should be validated by their empirical realism measured in the setting in which the simulation components were fitted. With this paper, we argue for more rigor when applying generative-agent-based modeling for social simulation.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constraint Hypergraphs as a Unifying Framework for Digital Twins</title>
<link>https://arxiv.org/abs/2507.05494</link>
<guid>https://arxiv.org/abs/2507.05494</guid>
<content:encoded><![CDATA[
arXiv:2507.05494v2 Announce Type: replace 
Abstract: Digital twins, used to represent physical systems, have been lauded as tools for understanding reality. Complex system behavior is typically captured in domain-specific models crafted by subject experts. Contemporary methods for employing models in a digital twin require prescriptive interfaces, resulting in twins that are difficult to connect, redeploy, and modify. The limited interoperability of these twins has prompted calls for a universal framework enabling observability across model aggregations. Here we show how a new mathematical formalism called a constraint hypergraph addresses these challenges in interoperability. A digital twin is shown to be the second of two coupled systems where both adhere to the same constraint hypergraph, permitting the properties of the first to be observable from the second. Interoperability is given by deconstructing models into a structure enabling autonomous, white-box simulation of system properties. The resulting digital twins can interact immediately with both human and autonomous agents. This is demonstrated in a case study of a microgrid, showing how both measured and simulated data from the aggregated twins can be provided regardless of the operating environment. By connecting models, constraint hypergraphs supply scientists and modelers robust means to capture, communicate, and combine digital twins across all fields of study. We expect this framework to expand the use of digital twins, enriching scientific insights and collaborations by providing a structure for characterizing complex systems.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GTA1: GUI Test-time Scaling Agent</title>
<link>https://arxiv.org/abs/2507.05791</link>
<guid>https://arxiv.org/abs/2507.05791</guid>
<content:encoded><![CDATA[
arXiv:2507.05791v4 Announce Type: replace 
Abstract: Graphical user interface (GUI) agents autonomously complete tasks across platforms (\eg, Linux) by sequentially decomposing user instructions into action proposals that iteratively interact with visual elements in the evolving environment. However, two main challenges arise: i) planning (\ie, the action proposal sequence) under expansive action space, where selecting an appropriate plan is non-trivial, as many valid ones may exist; ii) accurately grounding actions in complex and high-resolution interfaces, \ie, precisely interacting with visual targets. This paper investigates the aforementioned challenges with our \textbf{G}UI \textbf{T}est-time Scaling \textbf{A}gent, namely GTA1. First, we conduct test-time scaling to select the most appropriate action proposal: at each step, multiple candidate proposals are sampled and evaluated and selected by a judge model. It trades off computation for better decision quality by concurrent sampling. Second, we propose a model that improves grounding of the selected action proposals to its corresponding visual elements. Our key insight is that reinforcement learning (RL) facilitates grounding through inherent objective alignments, rewarding successful clicks on interface elements. Experimentally, GTA1 achieves state-of-the-art performance on both grounding and agent task execution benchmarks. The code and models are released here.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AICrypto: A Comprehensive Benchmark for Evaluating Cryptography Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2507.09580</link>
<guid>https://arxiv.org/abs/2507.09580</guid>
<content:encoded><![CDATA[
arXiv:2507.09580v3 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across a variety of domains. However, their applications in cryptography, which serves as a foundational pillar of cybersecurity, remain largely unexplored. To address this gap, we propose AICrypto, the first comprehensive benchmark designed to evaluate the cryptography capabilities of LLMs. The benchmark comprises 135 multiple-choice questions, 150 capture-the-flag (CTF) challenges, and 18 proof problems, covering a broad range of skills from factual memorization to vulnerability exploitation and formal reasoning. All tasks are carefully reviewed or constructed by cryptography experts to ensure correctness and rigor. To support automated evaluation of CTF challenges, we design an agent-based framework. We introduce strong human expert performance baselines for comparison across all task types. Our evaluation of 17 leading LLMs reveals that state-of-the-art models match or even surpass human experts in memorizing cryptographic concepts, exploiting common vulnerabilities, and routine proofs. However, our case studies reveal that they still lack a deep understanding of abstract mathematical concepts and struggle with tasks that require multi-step reasoning and dynamic analysis. We hope this work could provide insights for future research on LLMs in cryptographic applications. Our code and dataset are available at https://aicryptobench.github.io/.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moving Out: Physically-grounded Human-AI Collaboration</title>
<link>https://arxiv.org/abs/2507.18623</link>
<guid>https://arxiv.org/abs/2507.18623</guid>
<content:encoded><![CDATA[
arXiv:2507.18623v3 Announce Type: replace 
Abstract: The ability to adapt to physical actions and constraints in an environment is crucial for embodied agents (e.g., robots) to effectively collaborate with humans. Such physically grounded human-AI collaboration must account for the increased complexity of the continuous state-action space and constrained dynamics caused by physical constraints. In this paper, we introduce Moving Out, a new human-AI collaboration benchmark that resembles a wide range of collaboration modes affected by physical attributes and constraints, such as moving heavy items together and maintaining consistent actions to move a big item around a corner. Using Moving Out, we designed two tasks and collected human-human interaction data to evaluate models' abilities to adapt to diverse human behaviors and unseen physical attributes. To address the challenges in physical environments, we propose a novel method, BASS (Behavior Augmentation, Simulation, and Selection), to enhance the diversity of agents and their understanding of the outcome of actions. Our experiments show that BASS outperforms state-of-the-art models in AI-AI and human-AI collaboration. The project page is available at https://live-robotics-uva.github.io/movingout_ai/.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Language Models Discover Scaling Laws?</title>
<link>https://arxiv.org/abs/2507.21184</link>
<guid>https://arxiv.org/abs/2507.21184</guid>
<content:encoded><![CDATA[
arXiv:2507.21184v2 Announce Type: replace 
Abstract: Discovering scaling laws for predicting model performance at scale is a fundamental and open-ended challenge, mostly reliant on slow, case specific human experimentation. To investigate the potential for LLMs to automate this process, we collect over 5,000 experiments from existing literature and curate seven diverse scaling law discovery tasks. While existing agents struggle to produce accurate law formulas, this paper introduces SLDAgent, an evolution-based agent that co-optimize the scaling law model and the parameters, enabling it to autonomously explore complex relationships between variables. For the first time, we demonstrates that SLDAgent can automatically discover laws that exhibit consistently more accurate extrapolation than their established, human-derived counterparts across all tasks. Through comprehensive analysis, we elucidate why these discovered laws are superior and verify their practical utility in both pretraining and finetuning applications. This work establishes a new paradigm for agentic scientific discovery, showing that AI systems can understand their own scaling behavior, and can contribute novel and practical knowledge back to the research community.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CADDesigner: Conceptual Design of CAD Models Based on General-Purpose Agent</title>
<link>https://arxiv.org/abs/2508.01031</link>
<guid>https://arxiv.org/abs/2508.01031</guid>
<content:encoded><![CDATA[
arXiv:2508.01031v3 Announce Type: replace 
Abstract: Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing but typically requires a high level of expertise from designers. To lower the entry barrier and improve design efficiency, we present an agent for CAD conceptual design powered by large language models (LLMs). The agent accepts both abstract textual descriptions and freehand sketches as input, engaging in interactive dialogue with users to refine and clarify design requirements through comprehensive requirement analysis. Built upon a novel Context-Independent Imperative Paradigm (CIP), the agent generates high-quality CAD modeling code. During the generation process, the agent incorporates iterative visual feedback to improve model quality. Generated design cases are stored in a structured knowledge base, enabling continuous improvement of the agent's code generation capabilities. Experimental results demonstrate that our method achieves state-of-the-art performance in CAD code generation.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communicating Plans, Not Percepts: Scalable Multi-Agent Coordination with Embodied World Models</title>
<link>https://arxiv.org/abs/2508.02912</link>
<guid>https://arxiv.org/abs/2508.02912</guid>
<content:encoded><![CDATA[
arXiv:2508.02912v2 Announce Type: replace 
Abstract: Robust coordination is critical for effective decision-making in multi-agent systems, especially under partial observability. A central question in Multi-Agent Reinforcement Learning (MARL) is whether to engineer communication protocols or learn them end-to-end. We investigate this dichotomy using embodied world models. We propose and compare two communication strategies for a cooperative task-allocation problem. The first, Learned Direct Communication (LDC), learns a protocol end-to-end, with agents generating messages and actions concurrently. The second, Intention Communication, uses an engineered inductive bias: a compact, learned world model, the Imagined Trajectory Generation Module (ITGM), to simulate future states. Agents then communicate a summary of this plan. We evaluate these approaches on goal-directed interaction in a grid world, a canonical abstraction for embodied AI problems. Our experiments reveal that while emergent communication is viable in simple settings, the engineered, world model-based approach shows superior performance, sample efficiency, and scalability as complexity increases. These findings advocate for integrating structured, predictive models into MARL agents to enable active, goal-driven coordination.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CTTS: Collective Test-Time Scaling</title>
<link>https://arxiv.org/abs/2508.03333</link>
<guid>https://arxiv.org/abs/2508.03333</guid>
<content:encoded><![CDATA[
arXiv:2508.03333v2 Announce Type: replace 
Abstract: Test-time scaling (TTS) has emerged as a promising, training-free approach for enhancing large language model (LLM) performance. However, the efficacy of existing methods, such as Best-of-N and Self-Consistency, is fundamentally constrained by the dominant single test-time scaling (STTS) paradigm, which relies on a single LLM agent interacting with a single reward model (SA-SR). Inspired by recent work showing that collective methods can surpass the performance ceiling of individual models, we introduce Collective Test-Time Scaling (CTTS). First, we systematically investigate three primary interaction paradigms of existing multiple models: single-agent-multi-reward (SA-MR), multi-agent-single-reward (MA-SR), and multi-agent-multi-reward (MA-MR). Extensive experiments reveal that the MA-MR paradigm is consistently superior. Based on this finding, we further propose CTTS-MM, a novel framework that operationalizes multi-agent and multi-reward collaboration. CTTS-MM integrates two key technical contributions: (1) for agent collaboration, an Agent Collaboration Search (ACS) that identifies the most effective combination of LLMs from a candidate pool; and (2) for reward model collaboration, a Mixture of Reward Models (MoR) strategy that leverages a Prior Reward model Ensemble Selection (PRES) algorithm to select the optimal ensemble. Evaluations across seven mainstream benchmarks demonstrate that CTTS-MM significantly outperforms leading STTS methods (+4.82% over Best-of-N) and surpasses even flagship proprietary LLMs (+7.06% over GPT-4.1) and open-source LLMs. These results highlight the substantial potential of collective scaling to push the frontier of LLM inference. Code will be released at https://github.com/magent4aci/CTTS-MM.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sotopia-RL: Reward Design for Social Intelligence</title>
<link>https://arxiv.org/abs/2508.03905</link>
<guid>https://arxiv.org/abs/2508.03905</guid>
<content:encoded><![CDATA[
arXiv:2508.03905v2 Announce Type: replace 
Abstract: Social intelligence has become a critical capability for large language models (LLMs), enabling them to engage effectively in real-world social tasks such as accommodation, persuasion, collaboration, and negotiation. Reinforcement learning (RL) is a natural fit for training socially intelligent agents because it allows models to learn sophisticated strategies directly through social interactions. However, social interactions have two key characteristics that set barriers for RL training: (1) partial observability, where utterances have indirect and delayed effects that complicate credit assignment, and (2) multi-dimensionality, where behaviors such as rapport-building or knowledge-seeking contribute indirectly to goal achievement. These characteristics make Markov decision process (MDP)-based RL with single-dimensional episode-level rewards inefficient and unstable. To address these challenges, we propose Sotopia-RL, a novel framework that refines coarse episode-level feedback into utterance-level, multi-dimensional rewards. Utterance-level credit assignment mitigates partial observability by attributing outcomes to individual utterances, while multi-dimensional rewards capture the full richness of social interactions and reduce reward hacking. Experiments in Sotopia, an open-ended social learning environment, demonstrate that Sotopia-RL achieves state-of-the-art social goal completion scores (7.17 on Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing approaches. Ablation studies confirm the necessity of both utterance-level credit assignment and multi-dimensional reward design for RL training. Our implementation is publicly available at: https://github.com/sotopia-lab/sotopia-rl.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Scheduling Algorithm for Scalable and Efficient IoT Sensing in Cloud Computing</title>
<link>https://arxiv.org/abs/2508.04334</link>
<guid>https://arxiv.org/abs/2508.04334</guid>
<content:encoded><![CDATA[
arXiv:2508.04334v2 Announce Type: replace 
Abstract: The rapid growth of Internet of Things (IoT) devices produces massive, heterogeneous data streams, demanding scalable and efficient scheduling in cloud environments to meet latency, energy, and Quality-of-Service (QoS) requirements. Existing scheduling methods often lack adaptability to dynamic workloads and network variability inherent in IoT-cloud systems. This paper presents a novel hybrid scheduling algorithm combining deep Reinforcement Learning (RL) and Ant Colony Optimization (ACO) to address these challenges. The deep RL agent utilizes a model-free policy-gradient approach to learn adaptive task allocation policies responsive to real-time workload fluctuations and network states. Simultaneously, the ACO metaheuristic conducts a global combinatorial search to optimize resource distribution, mitigate congestion, and balance load across distributed cloud nodes. Extensive experiments on large-scale synthetic IoT datasets, reflecting diverse workloads and QoS constraints, demonstrate that the proposed method achieves up to 18.4% reduction in average response time, 12.7% improvement in resource utilization, and 9.3% decrease in energy consumption compared to leading heuristics and RL-only baselines. Moreover, the algorithm ensures strict Service Level Agreement (SLA) compliance through deadline-aware scheduling and dynamic prioritization. The results confirm the effectiveness of integrating model-free RL with swarm intelligence for scalable, energy-efficient IoT data scheduling, offering a promising approach for next-generation IoT-cloud platforms.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing</title>
<link>https://arxiv.org/abs/2508.04361</link>
<guid>https://arxiv.org/abs/2508.04361</guid>
<content:encoded><![CDATA[
arXiv:2508.04361v3 Announce Type: replace 
Abstract: While generalist foundation models like Gemini and GPT-4o demonstrate impressive multi-modal competence, existing evaluations fail to test their intelligence in dynamic, interactive worlds. Static benchmarks lack agency, while interactive benchmarks suffer from a severe modal bottleneck, typically ignoring crucial auditory and temporal cues. To bridge this evaluation chasm, we introduce OmniPlay, a diagnostic benchmark designed not just to evaluate, but to probe the fusion and reasoning capabilities of agentic models across the full sensory spectrum. Built on a core philosophy of modality interdependence, OmniPlay comprises a suite of five game environments that systematically create scenarios of both synergy and conflict, forcing agents to perform genuine cross-modal reasoning. Our comprehensive evaluation of six leading omni-modal models reveals a critical dichotomy: they exhibit superhuman performance on high-fidelity memory tasks but suffer from systemic failures in challenges requiring robust reasoning and strategic planning. We demonstrate that this fragility stems from brittle fusion mechanisms, which lead to catastrophic performance degradation under modality conflict and uncover a counter-intuitive "less is more" paradox, where removing sensory information can paradoxically improve performance. Our findings suggest that the path toward robust AGI requires a research focus beyond scaling to explicitly address synergistic fusion. Our platform is available for anonymous review at https://github.com/fuqingbie/omni-game-benchmark.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management</title>
<link>https://arxiv.org/abs/2508.04664</link>
<guid>https://arxiv.org/abs/2508.04664</guid>
<content:encoded><![CDATA[
arXiv:2508.04664v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) suffer from significant performance degradation when processing long contexts due to proactive interference, where irrelevant information in earlier parts of the context disrupts reasoning and memory recall. While most research focuses on external memory systems to augment LLMs' capabilities, we propose a complementary approach: empowering LLMs with Active Context Management (ACM) tools to actively sculpt their internal working memory. We introduce Sculptor, a framework that equips LLMs with three categories of tools: (1) context fragmentation, (2) summary, hide, and restore, and (3) precise search. Our approach enables LLMs to proactively manage their attention and working memory, analogous to how humans selectively focus on relevant information while filtering out distractions. Experimental evaluation on diverse long-context benchmarks demonstrates that Sculptor significantly improves performance even without specific training, leveraging LLMs' inherent tool-calling and instruction-following capabilities. To further optimize these strategies, we introduce a novel dynamic context-aware reinforcement learning (RL) approach, advancing the training of an agent that actively modifies its own conversational history. By enabling Active Context Management, Sculptor not only mitigates proactive interference but also provides a cognitive foundation for more reliable reasoning across diverse long-context tasks-highlighting that explicit context-control strategies, rather than merely larger token windows, are key to robustness at scale.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory</title>
<link>https://arxiv.org/abs/2508.09736</link>
<guid>https://arxiv.org/abs/2508.09736</guid>
<content:encoded><![CDATA[
arXiv:2508.09736v3 Announce Type: replace 
Abstract: We introduce M3-Agent, a novel multimodal agent framework equipped with long-term memory. Like humans, M3-Agent can process real-time visual and auditory inputs to build and update episodic and semantic memories, gradually accumulating world knowledge. Its memory is organized in an entity-centric, multimodal manner, enabling deeper and more consistent understanding of the environment. Given an instruction, M3-Agent autonomously performs multi-turn reasoning and retrieves relevant memories to complete tasks. To evaluate memory effectiveness and memory-based reasoning in multimodal agents, we develop M3-Bench, a long-video question answering benchmark comprising 100 newly recorded robot-perspective videos (M3-Bench-robot) and 920 diverse web-sourced videos (M3-Bench-web). We annotate QA pairs designed to test capabilities essential for agent applications, such as person understanding, general knowledge extraction, and cross-modal reasoning. Experimental results show that M3-Agent, trained via reinforcement learning, outperforms the strongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o, achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web and VideoMME-long, respectively. Our work advances multimodal agents toward more human-like long-term memory and provides insights for their practical design. Model, code and data are available at https://github.com/bytedance-seed/m3-agent.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transduction is All You Need for Structured Data Workflows</title>
<link>https://arxiv.org/abs/2508.15610</link>
<guid>https://arxiv.org/abs/2508.15610</guid>
<content:encoded><![CDATA[
arXiv:2508.15610v2 Announce Type: replace 
Abstract: This paper introduces Agentics, a functional agentic AI framework for building LLM-based structured data workflow pipelines. Designed for both research and practical applications, Agentics offers a new data-centric paradigm in which agents are embedded within data types, enabling logical transduction between structured states. This design shifts the focus toward principled data modeling, providing a declarative language where data types are directly exposed to large language models and composed through transductions triggered by type connections. We present a range of structured data workflow tasks and empirical evidence demonstrating the effectiveness of this approach, including data wrangling, text-to-SQL semantic parsing, and domain-specific multiple-choice question answering. The open source Agentics is available at https://github.com/IBM/Agentics.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction</title>
<link>https://arxiv.org/abs/2509.00581</link>
<guid>https://arxiv.org/abs/2509.00581</guid>
<content:encoded><![CDATA[
arXiv:2509.00581v2 Announce Type: replace 
Abstract: Converting natural language queries into SQL queries is a crucial challenge in both industry and academia, aiming to increase access to databases and large-scale applications. This work examines how in-context learning and chain-of-thought can be utilized to develop a robust solution for text-to-SQL systems. We propose SQL-of-Thought: a multi-agent framework that decomposes the Text2SQL task into schema linking, subproblem identification, query plan generation, SQL generation, and a guided correction loop. Unlike prior systems that rely only on execution-based static correction, we introduce taxonomy-guided dynamic error modification informed by in-context learning. SQL-of-Thought achieves state-of-the-art results on the Spider dataset and its variants, combining guided error taxonomy with reasoning-based query planning.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Co-Evolving Complexity: An Adversarial Framework for Automatic MARL Curricula</title>
<link>https://arxiv.org/abs/2509.03771</link>
<guid>https://arxiv.org/abs/2509.03771</guid>
<content:encoded><![CDATA[
arXiv:2509.03771v2 Announce Type: replace 
Abstract: The advancement of general-purpose intelligent agents is intrinsically linked to the environments in which they are trained. While scaling models and datasets has yielded remarkable capabilities, scaling the complexity, diversity, and interactivity of environments remains a crucial bottleneck. Hand-crafted environments are finite and often contain implicit biases, limiting the potential for agents to develop truly generalizable and robust skills. In this work, we propose a paradigm for generating a boundless and adaptive curriculum of challenges by framing the environment generation process as an adversarial game. We introduce a system where a team of cooperative multi-agent defenders learns to survive against a procedurally generative attacker. The attacker agent learns to produce increasingly challenging configurations of enemy units, dynamically creating novel worlds tailored to exploit the defenders' current weaknesses. Concurrently, the defender team learns cooperative strategies to overcome these generated threats. This co-evolutionary dynamic creates a self-scaling environment where complexity arises organically from the adversarial interaction, providing an effectively infinite stream of novel and relevant training data. We demonstrate that with minimal training, this approach leads to the emergence of complex, intelligent behaviors, such as flanking and shielding by the attacker, and focus-fire and spreading by the defenders. Our findings suggest that adversarial co-evolution is a powerful mechanism for automatically scaling environmental complexity, driving agents towards greater robustness and strategic depth.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Physical Basis of Prediction: World Model Formation in Neural Organoids via an LLM-Generated Curriculum</title>
<link>https://arxiv.org/abs/2509.04633</link>
<guid>https://arxiv.org/abs/2509.04633</guid>
<content:encoded><![CDATA[
arXiv:2509.04633v2 Announce Type: replace 
Abstract: The capacity of an embodied agent to understand, predict, and interact with its environment is fundamentally contingent on an internal world model. This paper introduces a novel framework for investigating the formation and adaptation of such world models within a biological substrate: human neural organoids. We present a curriculum of three scalable, closed-loop virtual environments designed to train these biological agents and probe the underlying synaptic mechanisms of learning, such as long-term potentiation (LTP) and long-term depression (LTD). We detail the design of three distinct task environments that demand progressively more sophisticated world models for successful decision-making: (1) a conditional avoidance task for learning static state-action contingencies, (2) a one-dimensional predator-prey scenario for goal-directed interaction, and (3) a replication of the classic Pong game for modeling dynamic, continuous-time systems. For each environment, we formalize the state and action spaces, the sensory encoding and motor decoding mechanisms, and the feedback protocols based on predictable (reward) and unpredictable (punishment) stimulation, which serve to drive model refinement. In a significant methodological advance, we propose a meta-learning approach where a Large Language Model automates the generative design and optimization of experimental protocols, thereby scaling the process of environment and curriculum design. Finally, we outline a multi-modal evaluation strategy that moves beyond task performance to directly measure the physical correlates of the learned world model by quantifying synaptic plasticity at electrophysiological, cellular, and molecular levels. This work bridges the gap between model-based reinforcement learning and computational neuroscience, offering a unique platform for studying embodiment, decision-making, and the physical basis of intelligence.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Task Environments as the Next Frontier for Embodied World Models in Robot Soccer</title>
<link>https://arxiv.org/abs/2509.04731</link>
<guid>https://arxiv.org/abs/2509.04731</guid>
<content:encoded><![CDATA[
arXiv:2509.04731v2 Announce Type: replace 
Abstract: Recent advances in agent development have focused on scaling model size and raw interaction data, mirroring the successes seen in large language models. However, for complex, long-horizon multi-agent tasks such as robotic soccer, this end-to-end approach often fails due to intractable exploration spaces and sparse rewards. This position paper argues that the next frontier in developing embodied world models is not merely increasing the fidelity or size of environments, but scaling their structural complexity through explicit hierarchical scaffolding. We posit that an effective world model for decision-making must model not only the world's physics but also its task semantics. Drawing from a systematic review of 2024 research in low-resource multi-agent soccer, we identify a clear trend towards integrating symbolic and hierarchical methods, such as Hierarchical Task Networks (HTNs) and Bayesian Strategy Networks (BSNs), with multi-agent reinforcement learning (MARL). These methods decompose complex goals into manageable subgoals, creating an intrinsic curriculum that shapes agent learning. We propose that such structured environments are essential for bridging the gap between simple, reactive behaviors and sophisticated, strategic team play. We further extend this principle, proposing that this scaffolding can be generalized to other complex domains and dynamically generated by Large Language Models (LLMs), which act as generative world models of tasks. By building environments with explicit, composable task layers, we can guide agent exploration more efficiently, generate meaningful learning signals, and ultimately train more capable and general-purpose agents with fewer resources than purely end-to-end approaches.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Temporally Extended Behavior Sharing for Multi-task Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.20766</link>
<guid>https://arxiv.org/abs/2509.20766</guid>
<content:encoded><![CDATA[
arXiv:2509.20766v2 Announce Type: replace 
Abstract: Multi-task reinforcement learning (MTRL) offers a promising approach to improve sample efficiency and generalization by training agents across multiple tasks, enabling knowledge sharing between them. However, applying MTRL to robotics remains challenging due to the high cost of collecting diverse task data. To address this, we propose MT-L\'evy, a novel exploration strategy that enhances sample efficiency in MTRL environments by combining behavior sharing across tasks with temporally extended exploration inspired by L\'evy flight. MT-L\'evy leverages policies trained on related tasks to guide exploration towards key states, while dynamically adjusting exploration levels based on task success ratios. This approach enables more efficient state-space coverage, even in complex robotics environments. Empirical results demonstrate that MT-L\'evy significantly improves exploration and sample efficiency, supported by quantitative and qualitative analyses. Ablation studies further highlight the contribution of each component, showing that combining behavior sharing with adaptive exploration strategies can significantly improve the practicality of MTRL in robotics applications.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automotive-ENV: Benchmarking Multimodal Agents in Vehicle Interface Systems</title>
<link>https://arxiv.org/abs/2509.21143</link>
<guid>https://arxiv.org/abs/2509.21143</guid>
<content:encoded><![CDATA[
arXiv:2509.21143v2 Announce Type: replace 
Abstract: Multimodal agents have demonstrated strong performance in general GUI interactions, but their application in automotive systems has been largely unexplored. In-vehicle GUIs present distinct challenges: drivers' limited attention, strict safety requirements, and complex location-based interaction patterns. To address these challenges, we introduce Automotive-ENV, the first high-fidelity benchmark and interaction environment tailored for vehicle GUIs. This platform defines 185 parameterized tasks spanning explicit control, implicit intent understanding, and safety-aware tasks, and provides structured multimodal observations with precise programmatic checks for reproducible evaluation. Building on this benchmark, we propose ASURADA, a geo-aware multimodal agent that integrates GPS-informed context to dynamically adjust actions based on location, environmental conditions, and regional driving norms. Experiments show that geo-aware information significantly improves success on safety-aware tasks, highlighting the importance of location-based context in automotive environments. We will release Automotive-ENV, complete with all tasks and benchmarking tools, to further the development of safe and adaptive in-vehicle agents.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BenchRL-QAS: Benchmarking reinforcement learning algorithms for quantum architecture search</title>
<link>https://arxiv.org/abs/2507.12189</link>
<guid>https://arxiv.org/abs/2507.12189</guid>
<content:encoded><![CDATA[
arXiv:2507.12189v2 Announce Type: replace-cross 
Abstract: We present BenchRL-QAS, a unified benchmarking framework for reinforcement learning (RL) in quantum architecture search (QAS) across a spectrum of variational quantum algorithm tasks on 2- to 8-qubit systems. Our study systematically evaluates 9 different RL agents, including both value-based and policy-gradient methods, on quantum problems such as variational eigensolver, quantum state diagonalization, variational quantum classification (VQC), and state preparation, under both noiseless and noisy execution settings. To ensure fair comparison, we propose a weighted ranking metric that integrates accuracy, circuit depth, gate count, and training time. Results demonstrate that no single RL method dominates universally, the performance dependents on task type, qubit count, and noise conditions providing strong evidence of no free lunch principle in RL-QAS. As a byproduct we observe that a carefully chosen RL algorithm in RL-based VQC outperforms baseline VQCs. BenchRL-QAS establishes the most extensive benchmark for RL-based QAS to date, codes and experimental made publicly available for reproducibility and future advances.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs for Bayesian Optimization in Scientific Domains: Are We There Yet?</title>
<link>https://arxiv.org/abs/2509.21403</link>
<guid>https://arxiv.org/abs/2509.21403</guid>
<content:encoded><![CDATA[
arXiv:2509.21403v1 Announce Type: new 
Abstract: Large language models (LLMs) have recently been proposed as general-purpose agents for experimental design, with claims that they can perform in-context experimental design. We evaluate this hypothesis using both open- and closed-source instruction-tuned LLMs applied to genetic perturbation and molecular property discovery tasks. We find that LLM-based agents show no sensitivity to experimental feedback: replacing true outcomes with randomly permuted labels has no impact on performance. Across benchmarks, classical methods such as linear bandits and Gaussian process optimization consistently outperform LLM agents. We further propose a simple hybrid method, LLM-guided Nearest Neighbour (LLMNN) sampling, that combines LLM prior knowledge with nearest-neighbor sampling to guide the design of experiments. LLMNN achieves competitive or superior performance across domains without requiring significant in-context adaptation. These results suggest that current open- and closed-source LLMs do not perform in-context experimental design in practice and highlight the need for hybrid frameworks that decouple prior-based reasoning from batch acquisition with updated posteriors.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting Conceptual Knowledge to Locate Software Issues</title>
<link>https://arxiv.org/abs/2509.21427</link>
<guid>https://arxiv.org/abs/2509.21427</guid>
<content:encoded><![CDATA[
arXiv:2509.21427v1 Announce Type: new 
Abstract: Issue localization, which identifies faulty code elements such as files or functions, is critical for effective bug fixing. While recent LLM-based and LLM-agent-based approaches improve accuracy, they struggle in large-scale repositories due to concern mixing, where relevant logic is buried in large functions, and concern scattering, where related logic is dispersed across files.
  To address these challenges, we propose RepoLens, a novel approach that abstracts and leverages conceptual knowledge from code repositories. RepoLens decomposes fine-grained functionalities and recomposes them into high-level concerns, semantically coherent clusters of functionalities that guide LLMs. It operates in two stages: an offline stage that extracts and enriches conceptual knowledge into a repository-wide knowledge base, and an online stage that retrieves issue-specific terms, clusters and ranks concerns by relevance, and integrates them into localization workflows via minimally intrusive prompt enhancements. We evaluate RepoLens on SWE-Lancer-Loc, a benchmark of 216 tasks derived from SWE-Lancer. RepoLens consistently improves three state-of-the-art tools, namely AgentLess, OpenHands, and mini-SWE-agent, achieving average gains of over 22% in Hit@k and 46% in Recall@k for file- and function-level localization. It generalizes across models (GPT-4o, GPT-4o-mini, GPT-4.1) with Hit@1 and Recall@10 gains up to 504% and 376%, respectively. Ablation studies and manual evaluation confirm the effectiveness and reliability of the constructed concerns.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A State-of-the-Art SQL Reasoning Model using RLVR</title>
<link>https://arxiv.org/abs/2509.21459</link>
<guid>https://arxiv.org/abs/2509.21459</guid>
<content:encoded><![CDATA[
arXiv:2509.21459v1 Announce Type: new 
Abstract: Developing custom reasoning models via Reinforcement Learning (RL) that can incorporate organization-specific knowledge has great potential to address problems faced by enterprise customers. In many of these problems, the reward function is verifiable, a setting termed RL with Verifiable Rewards (RLVR). We apply RLVR to a popular data science benchmark called BIRD that measures the ability of an AI agent to convert a natural language query for a database to SQL executions. We apply a simple and general-purpose training recipe involving careful prompt and model selection, a warm-up stage using our offline RL approach called TAO, followed by rigorous online RLVR training. With no additional training data beyond the BIRD training set and no use of proprietary models, our very first submission to the BIRD leaderboard reached state-of-the-art accuracy on the private test set: 73.56% without self-consistency and 75.68% with self-consistency. In the latter case, our model also required fewer generations than the second-best approach. While BIRD is only a proxy task, the simplicity of our framework makes it broadly applicable to enterprise domains such as business intelligence, data science, and coding.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Residual Vector Quantization For Communication-Efficient Multi-Agent Perception</title>
<link>https://arxiv.org/abs/2509.21464</link>
<guid>https://arxiv.org/abs/2509.21464</guid>
<content:encoded><![CDATA[
arXiv:2509.21464v1 Announce Type: new 
Abstract: Multi-agent collaborative perception (CP) improves scene understanding by sharing information across connected agents such as autonomous vehicles, unmanned aerial vehicles, and robots. Communication bandwidth, however, constrains scalability. We present ReVQom, a learned feature codec that preserves spatial identity while compressing intermediate features. ReVQom is an end-to-end method that compresses feature dimensions via a simple bottleneck network followed by multi-stage residual vector quantization (RVQ). This allows only per-pixel code indices to be transmitted, reducing payloads from 8192 bits per pixel (bpp) of uncompressed 32-bit float features to 6-30 bpp per agent with minimal accuracy loss. On DAIR-V2X real-world CP dataset, ReVQom achieves 273x compression at 30 bpp to 1365x compression at 6 bpp. At 18 bpp (455x), ReVQom matches or outperforms raw-feature CP, and at 6-12 bpp it enables ultra-low-bandwidth operation with graceful degradation. ReVQom allows efficient and accurate multi-agent collaborative perception with a step toward practical V2X deployment.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Talking Trees: Reasoning-Assisted Induction of Decision Trees for Tabular Data</title>
<link>https://arxiv.org/abs/2509.21465</link>
<guid>https://arxiv.org/abs/2509.21465</guid>
<content:encoded><![CDATA[
arXiv:2509.21465v1 Announce Type: new 
Abstract: Tabular foundation models are becoming increasingly popular for low-resource tabular problems. These models make up for small training datasets by pretraining on large volumes of synthetic data. The prior knowledge obtained via pretraining provides the exceptional performance, but the resulting model becomes a black box that is difficult to interpret and costly to inference. In this work, we explore an alternative strategy: using reasoning-capable LLMs to induce decision trees for small tabular datasets in agentic setup. We design a minimal set of tools for constructing, analyzing and manipulating decision trees. By using these tools, LLMs combine their prior knowledge with learning from data to create a lightweight decision tree that outperforms traditional CART on low-resource tabular problems. While a single decision tree does not outperform state-of-the-art black box models, it comes with a human-readable reasoning trace that can be checked for biases and data leaks. Furthermore, the reasoning-based LLM's creation process allows for additional human input: correcting biases or incorporating domain-specific intuition that is not captured in the data.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing Ethereum's Geographical (De)Centralization Beyond the Atlantic</title>
<link>https://arxiv.org/abs/2509.21475</link>
<guid>https://arxiv.org/abs/2509.21475</guid>
<content:encoded><![CDATA[
arXiv:2509.21475v1 Announce Type: new 
Abstract: Decentralization has a geographic dimension that conventional metrics such as stake distribution overlook. Where validators run affects resilience to regional shocks (outages, disasters, government intervention) and fairness in reward access. Yet in permissionless systems, locations cannot be mandated, but they emerge from incentives. Today, Ethereum's validators cluster along the Atlantic (EU and U.S. East Coast), where latency is structurally favorable. This raises a key question: when some regions already enjoy latency advantages, how does protocol design shape validator incentives and the geography of (de)centralization? We develop a latency-calibrated agent-based model and compare two Ethereum block-building paradigms: a Single-Source Paradigm (SSP), akin to MEV-Boost, where proposers fetch full blocks from a relay that also propagates them; and a Multi-Source Paradigm (MSP), where proposers aggregate value from multiple sources and broadcast the block themselves. Simulations show that SSP concentrates around relay placement but more slowly, since proximity mainly affects propagation, and the marginal value of time is relatively uniform across regions. MSP centralizes faster: aggregating across sources makes marginal value location-dependent, amplifying payoff dispersion and migration toward latency minima. Source placement and consensus settings can dampen or intensify these effects, though once validators are already clustered, the impact of source placement on decentralization is marginal. In most cases, North America consistently emerges as the focal hub. These findings show that protocol design materially shapes validator geography and offer levers for promoting geographical decentralization.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Agent Meets Agentic AI: Can LLM Agents Simulate Customers to Evaluate Agentic-AI-based Shopping Assistants?</title>
<link>https://arxiv.org/abs/2509.21501</link>
<guid>https://arxiv.org/abs/2509.21501</guid>
<content:encoded><![CDATA[
arXiv:2509.21501v1 Announce Type: new 
Abstract: Agentic AI is emerging, capable of executing tasks through natural language, such as Copilot for coding or Amazon Rufus for shopping. Evaluating these systems is challenging, as their rapid evolution outpaces traditional human evaluation. Researchers have proposed LLM Agents to simulate participants as digital twins, but it remains unclear to what extent a digital twin can represent a specific customer in multi-turn interaction with an agentic AI system. In this paper, we recruited 40 human participants to shop with Amazon Rufus, collected their personas, interaction traces, and UX feedback, and then created digital twins to repeat the task. Pairwise comparison of human and digital-twin traces shows that while agents often explored more diverse choices, their action patterns aligned with humans and yielded similar design feedback. This study is the first to quantify how closely LLM agents can mirror human multi-turn interaction with an agentic AI system, highlighting their potential for scalable evaluation.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Psychological and behavioural responses in human-agent vs. human-human interactions: a systematic review and meta-analysis</title>
<link>https://arxiv.org/abs/2509.21542</link>
<guid>https://arxiv.org/abs/2509.21542</guid>
<content:encoded><![CDATA[
arXiv:2509.21542v1 Announce Type: new 
Abstract: Interactive intelligent agents are being integrated across society. Despite achieving human-like capabilities, humans' responses to these agents remain poorly understood, with research fragmented across disciplines. We conducted a first systematic synthesis comparing a range of psychological and behavioural responses in matched human-agent vs. human-human dyadic interactions. A total of 162 eligible studies (146 contributed to the meta-analysis; 468 effect sizes) were included in the systematic review and meta-analysis, which integrated frequentist and Bayesian approaches. Our results indicate that individuals exhibited less prosocial behaviour and moral engagement when interacting with agents vs. humans. They attributed less agency and responsibility to agents, perceiving them as less competent, likeable, and socially present. In contrast, individuals' social alignment (i.e., alignment or adaptation of internal states and behaviours with partners), trust in partners, personal agency, task performance, and interaction experiences were generally comparable when interacting with agents vs. humans. We observed high effect-size heterogeneity for many subjective responses (i.e., social perceptions of partners, subjective trust, and interaction experiences), suggesting context-dependency of partner effects. By examining the characteristics of studies, participants, partners, interaction scenarios, and response measures, we also identified several moderators shaping partner effects. Overall, functional behaviours and interactive experiences with agents can resemble those with humans, whereas fundamental social attributions and moral/prosocial concerns lag in human-agent interactions. Agents are thus afforded instrumental value on par with humans but lack comparable intrinsic value, providing practical implications for agent design and regulation.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoClimDS: Climate Data Science Agentic AI -- A Knowledge Graph is All You Need</title>
<link>https://arxiv.org/abs/2509.21553</link>
<guid>https://arxiv.org/abs/2509.21553</guid>
<content:encoded><![CDATA[
arXiv:2509.21553v1 Announce Type: new 
Abstract: Climate data science faces persistent barriers stemming from the fragmented nature of data sources, heterogeneous formats, and the steep technical expertise required to identify, acquire, and process datasets. These challenges limit participation, slow discovery, and reduce the reproducibility of scientific workflows. In this paper, we present a proof of concept for addressing these barriers through the integration of a curated knowledge graph (KG) with AI agents designed for cloud-native scientific workflows. The KG provides a unifying layer that organizes datasets, tools, and workflows, while AI agents -- powered by generative AI services -- enable natural language interaction, automated data access, and streamlined analysis. Together, these components drastically lower the technical threshold for engaging in climate data science, enabling non-specialist users to identify and analyze relevant datasets. By leveraging existing cloud-ready API data portals, we demonstrate that "a knowledge graph is all you need" to unlock scalable and agentic workflows for scientific inquiry. The open-source design of our system further supports community contributions, ensuring that the KG and associated tools can evolve as a shared commons. Our results illustrate a pathway toward democratizing access to climate data and establishing a reproducible, extensible framework for human--AI collaboration in scientific research.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Streamer: Unified Human World Modeling with Audiovisual Interaction</title>
<link>https://arxiv.org/abs/2509.21574</link>
<guid>https://arxiv.org/abs/2509.21574</guid>
<content:encoded><![CDATA[
arXiv:2509.21574v1 Announce Type: new 
Abstract: We introduce X-Streamer, an end-to-end multimodal human world modeling framework for building digital human agents capable of infinite interactions across text, speech, and video within a single unified architecture. Starting from a single portrait, X-Streamer enables real-time, open-ended video calls driven by streaming multimodal inputs. At its core is a Thinker-Actor dual-transformer architecture that unifies multimodal understanding and generation, turning a static portrait into persistent and intelligent audiovisual interactions. The Thinker module perceives and reasons over streaming user inputs, while its hidden states are translated by the Actor into synchronized multimodal streams in real time. Concretely, the Thinker leverages a pretrained large language-speech model, while the Actor employs a chunk-wise autoregressive diffusion model that cross-attends to the Thinker's hidden states to produce time-aligned multimodal responses with interleaved discrete text and audio tokens and continuous video latents. To ensure long-horizon stability, we design inter- and intra-chunk attentions with time-aligned multimodal positional embeddings for fine-grained cross-modality alignment and context retention, further reinforced by chunk-wise diffusion forcing and global identity referencing. X-Streamer runs in real time on two A100 GPUs, sustaining hours-long consistent video chat experiences from arbitrary portraits and paving the way toward unified world modeling of interactive digital humans.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision Language Models Cannot Plan, but Can They Formalize?</title>
<link>https://arxiv.org/abs/2509.21576</link>
<guid>https://arxiv.org/abs/2509.21576</guid>
<content:encoded><![CDATA[
arXiv:2509.21576v1 Announce Type: new 
Abstract: The advancement of vision language models (VLMs) has empowered embodied agents to accomplish simple multimodal planning tasks, but not long-horizon ones requiring long sequences of actions. In text-only simulations, long-horizon planning has seen significant improvement brought by repositioning the role of LLMs. Instead of directly generating action sequences, LLMs translate the planning domain and problem into a formal planning language like the Planning Domain Definition Language (PDDL), which can call a formal solver to derive the plan in a verifiable manner. In multimodal environments, research on VLM-as-formalizer remains scarce, usually involving gross simplifications such as predefined object vocabulary or overly similar few-shot examples. In this work, we present a suite of five VLM-as-formalizer pipelines that tackle one-shot, open-vocabulary, and multimodal PDDL formalization. We evaluate those on an existing benchmark while presenting another two that for the first time account for planning with authentic, multi-view, and low-quality images. We conclude that VLM-as-formalizer greatly outperforms end-to-end plan generation. We reveal the bottleneck to be vision rather than language, as VLMs often fail to capture an exhaustive set of necessary object relations. While generating intermediate, textual representations such as captions or scene graphs partially compensate for the performance, their inconsistent gain leaves headroom for future research directions on multimodal planning formalization.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language Models</title>
<link>https://arxiv.org/abs/2509.21593</link>
<guid>https://arxiv.org/abs/2509.21593</guid>
<content:encoded><![CDATA[
arXiv:2509.21593v1 Announce Type: new 
Abstract: Geospatial modeling provides critical solutions for pressing global challenges such as sustainability and climate change. Existing large language model (LLM)-based algorithm discovery frameworks, such as AlphaEvolve, excel at evolving generic code but lack the domain knowledge and multi-step reasoning required for complex geospatial problems. We introduce GeoEvolve, a multi-agent LLM framework that couples evolutionary search with geospatial domain knowledge to automatically design and refine geospatial algorithms. GeoEvolve operates in two nested loops: an inner loop leverages a code evolver to generate and mutate candidate solutions, while an outer agentic controller evaluates global elites and queries a GeoKnowRAG module -- a structured geospatial knowledge base that injects theoretical priors from geography. This knowledge-guided evolution steers the search toward theoretically meaningful and computationally efficient algorithms. We evaluate GeoEvolve on two fundamental and classical tasks: spatial interpolation (kriging) and spatial uncertainty quantification (geospatial conformal prediction). Across these benchmarks, GeoEvolve automatically improves and discovers new algorithms, incorporating geospatial theory on top of classical models. It reduces spatial interpolation error (RMSE) by 13-21% and enhances uncertainty estimation performance by 17\%. Ablation studies confirm that domain-guided retrieval is essential for stable, high-quality evolution. These results demonstrate that GeoEvolve provides a scalable path toward automated, knowledge-driven geospatial modeling, opening new opportunities for trustworthy and efficient AI-for-Science discovery.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentives in Federated Learning with Heterogeneous Agents</title>
<link>https://arxiv.org/abs/2509.21612</link>
<guid>https://arxiv.org/abs/2509.21612</guid>
<content:encoded><![CDATA[
arXiv:2509.21612v1 Announce Type: new 
Abstract: Federated learning promises significant sample-efficiency gains by pooling data across multiple agents, yet incentive misalignment is an obstacle: each update is costly to the contributor but boosts every participant. We introduce a game-theoretic framework that captures heterogeneous data: an agent's utility depends on who supplies each sample, not just how many. Agents aim to meet a PAC-style accuracy threshold at minimal personal cost. We show that uncoordinated play yields pathologies: pure equilibria may not exist, and the best equilibrium can be arbitrarily more costly than cooperation. To steer collaboration, we analyze the cost-minimizing contribution vector, prove that computing it is NP-hard, and derive a polynomial-time linear program that achieves a logarithmic approximation. Finally, pairing the LP with a simple pay-what-you-contribute rule - each agent receives a payment equal to its sample cost - yields a mechanism that is strategyproof and, within the class of contribution-based transfers, is unique.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G Open RANs</title>
<link>https://arxiv.org/abs/2509.21634</link>
<guid>https://arxiv.org/abs/2509.21634</guid>
<content:encoded><![CDATA[
arXiv:2509.21634v1 Announce Type: new 
Abstract: The evolution toward 6G networks is being accelerated by the Open Radio Access Network (O-RAN) paradigm -- an open, interoperable architecture that enables intelligent, modular applications across public telecom and private enterprise domains. While this openness creates unprecedented opportunities for innovation, it also expands the attack surface, demanding resilient, low-cost, and autonomous security solutions. Legacy defenses remain largely reactive, labor-intensive, and inadequate for the scale and complexity of next-generation systems. Current O-RAN applications focus mainly on network optimization or passive threat detection, with limited capability for closed-loop, automated response.
  To address this critical gap, we present an agentic AI framework for fully automated, end-to-end threat mitigation in 6G O-RAN environments. MobiLLM orchestrates security workflows through a modular multi-agent system powered by Large Language Models (LLMs). The framework features a Threat Analysis Agent for real-time data triage, a Threat Classification Agent that uses Retrieval-Augmented Generation (RAG) to map anomalies to specific countermeasures, and a Threat Response Agent that safely operationalizes mitigation actions via O-RAN control interfaces. Grounded in trusted knowledge bases such as the MITRE FiGHT framework and 3GPP specifications, and equipped with robust safety guardrails, MobiLLM provides a blueprint for trustworthy AI-driven network security. Initial evaluations demonstrate that MobiLLM can effectively identify and orchestrate complex mitigation strategies, significantly reducing response latency and showcasing the feasibility of autonomous security operations in 6G.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>eXplainable Artificial Intelligence for RL-based Networking Solutions</title>
<link>https://arxiv.org/abs/2509.21649</link>
<guid>https://arxiv.org/abs/2509.21649</guid>
<content:encoded><![CDATA[
arXiv:2509.21649v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) agents have been widely used to improve networking tasks. However, understanding the decisions made by these agents is essential for their broader adoption in networking and network management. To address this, we introduce eXplaNet - a pipeline grounded in explainable artificial intelligence - designed to help networking researchers and practitioners gain deeper insights into the decision-making processes of RL-based solutions. We demonstrate how eXplaNet can be applied to refine a routing solution powered by a Q-learning agent, specifically by improving its reward function. In addition, we discuss the opportunities and challenges of incorporating explainability into RL to better optimize network performance.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can AI Perceive Physical Danger and Intervene?</title>
<link>https://arxiv.org/abs/2509.21651</link>
<guid>https://arxiv.org/abs/2509.21651</guid>
<content:encoded><![CDATA[
arXiv:2509.21651v1 Announce Type: new 
Abstract: When AI interacts with the physical world -- as a robot or an assistive agent -- new safety challenges emerge beyond those of purely ``digital AI". In such interactions, the potential for physical harm is direct and immediate. How well do state-of-the-art foundation models understand common-sense facts about physical safety, e.g. that a box may be too heavy to lift, or that a hot cup of coffee should not be handed to a child? In this paper, our contributions are three-fold: first, we develop a highly scalable approach to continuous physical safety benchmarking of Embodied AI systems, grounded in real-world injury narratives and operational safety constraints. To probe multi-modal safety understanding, we turn these narratives and constraints into photorealistic images and videos capturing transitions from safe to unsafe states, using advanced generative models. Secondly, we comprehensively analyze the ability of major foundation models to perceive risks, reason about safety, and trigger interventions; this yields multi-faceted insights into their deployment readiness for safety-critical agentic applications. Finally, we develop a post-training paradigm to teach models to explicitly reason about embodiment-specific safety constraints provided through system instructions. The resulting models generate thinking traces that make safety reasoning interpretable and transparent, achieving state of the art performance in constraint satisfaction evaluations. The benchmark will be released at https://asimov-benchmark.github.io/v2
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QueryGym: Step-by-Step Interaction with Relational Databases</title>
<link>https://arxiv.org/abs/2509.21674</link>
<guid>https://arxiv.org/abs/2509.21674</guid>
<content:encoded><![CDATA[
arXiv:2509.21674v1 Announce Type: new 
Abstract: We introduce QueryGym, an interactive environment for building, testing, and evaluating LLM-based query planning agents. Existing frameworks often tie agents to specific query language dialects or obscure their reasoning; QueryGym instead requires agents to construct explicit sequences of relational algebra operations, ensuring engine-agnostic evaluation and transparent step-by-step planning. The environment is implemented as a Gymnasium interface that supplies observations -- including schema details, intermediate results, and execution feedback -- and receives actions that represent database exploration (e.g., previewing tables, sampling column values, retrieving unique values) as well as relational algebra operations (e.g., filter, project, join). We detail the motivation and the design of the environment. In the demo, we showcase the utility of the environment by contrasting it with contemporary LLMs that query databases. QueryGym serves as a practical testbed for research in error remediation, transparency, and reinforcement learning for query generation. For the associated demo, see https://ibm.biz/QueryGym.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval</title>
<link>https://arxiv.org/abs/2509.21710</link>
<guid>https://arxiv.org/abs/2509.21710</guid>
<content:encoded><![CDATA[
arXiv:2509.21710v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the important paradigm for enhancing Large Language Models (LLMs) with external knowledge. However, existing approaches face a fundamental trade-off. While graph-based methods are inherently dependent on high-quality graph structures, they face significant practical constraints: manually constructed knowledge graphs are prohibitively expensive to scale, while automatically extracted graphs from corpora are limited by the performance of the underlying LLM extractors, especially when using smaller, local-deployed models. This paper presents Think-on-Graph 3.0 (ToG-3), a novel framework that introduces Multi-Agent Context Evolution and Retrieval (MACER) mechanism to overcome these limitations. Our core innovation is the dynamic construction and refinement of a Chunk-Triplets-Community heterogeneous graph index, which pioneeringly incorporates a dual-evolution mechanism of Evolving Query and Evolving Sub-Graph for precise evidence retrieval. This approach addresses a critical limitation of prior Graph-based RAG methods, which typically construct a static graph index in a single pass without adapting to the actual query. A multi-agent system, comprising Constructor, Retriever, Reflector, and Responser agents, collaboratively engages in an iterative process of evidence retrieval, answer generation, sufficiency reflection, and, crucially, evolving query and subgraph. This dual-evolving multi-agent system allows ToG-3 to adaptively build a targeted graph index during reasoning, mitigating the inherent drawbacks of static, one-time graph construction and enabling deep, precise reasoning even with lightweight LLMs. Extensive experiments demonstrate that ToG-3 outperforms compared baselines on both deep and broad reasoning benchmarks, and ablation studies confirm the efficacy of the components of MACER framework.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProPerSim: Developing Proactive and Personalized AI Assistants through User-Assistant Simulation</title>
<link>https://arxiv.org/abs/2509.21730</link>
<guid>https://arxiv.org/abs/2509.21730</guid>
<content:encoded><![CDATA[
arXiv:2509.21730v1 Announce Type: new 
Abstract: As large language models (LLMs) become increasingly integrated into daily life, there is growing demand for AI assistants that are not only reactive but also proactive and personalized. While recent advances have pushed forward proactivity and personalization individually, their combination remains underexplored. To bridge this gap, we introduce ProPerSim, a new task and simulation framework for developing assistants capable of making timely, personalized recommendations in realistic home scenarios. In our simulation environment, a user agent with a rich persona interacts with the assistant, providing ratings on how well each suggestion aligns with its preferences and context. The assistant's goal is to use these ratings to learn and adapt to achieve higher scores over time. Built on ProPerSim, we propose ProPerAssistant, a retrieval-augmented, preference-aligned assistant that continually learns and adapts through user feedback. Experiments across 32 diverse personas show that ProPerAssistant adapts its strategy and steadily improves user satisfaction, highlighting the promise of uniting proactivity and personalization.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UISim: An Interactive Image-Based UI Simulator for Dynamic Mobile Environments</title>
<link>https://arxiv.org/abs/2509.21733</link>
<guid>https://arxiv.org/abs/2509.21733</guid>
<content:encoded><![CDATA[
arXiv:2509.21733v1 Announce Type: new 
Abstract: Developing and testing user interfaces (UIs) and training AI agents to interact with them are challenging due to the dynamic and diverse nature of real-world mobile environments. Existing methods often rely on cumbersome physical devices or limited static analysis of screenshots, which hinders scalable testing and the development of intelligent UI agents. We introduce UISim, a novel image-based UI simulator that offers a dynamic and interactive platform for exploring mobile phone environments purely from screen images. Our system employs a two-stage method: given an initial phone screen image and a user action, it first predicts the abstract layout of the next UI state, then synthesizes a new, visually consistent image based on this predicted layout. This approach enables the realistic simulation of UI transitions. UISim provides immediate practical benefits for UI testing, rapid prototyping, and synthetic data generation. Furthermore, its interactive capabilities pave the way for advanced applications, such as UI navigation task planning for AI agents. Our experimental results show that UISim outperforms end-to-end UI generation baselines in generating realistic and coherent subsequent UI states, highlighting its fidelity and potential to streamline UI development and enhance AI agent training.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios</title>
<link>https://arxiv.org/abs/2509.21766</link>
<guid>https://arxiv.org/abs/2509.21766</guid>
<content:encoded><![CDATA[
arXiv:2509.21766v1 Announce Type: new 
Abstract: Autonomous agents have recently achieved remarkable progress across diverse domains, yet most evaluations focus on short-horizon, fully observable tasks. In contrast, many critical real-world tasks, such as large-scale software development, commercial investment, and scientific discovery, unfold in long-horizon and partially observable scenarios where success hinges on sustained reasoning, planning, memory management, and tool use. Existing benchmarks rarely capture these long-horizon challenges, leaving a gap in systematic evaluation. To bridge this gap, we introduce \textbf{UltraHorizon} a novel benchmark that measures the foundational capabilities essential for complex real-world challenges. We use exploration as a unifying task across three distinct environments to validate these core competencies. Agents are designed in long-horizon discovery tasks where they must iteratively uncover hidden rules through sustained reasoning, planning, memory and tools management, and interaction with environments. Under the heaviest scale setting, trajectories average \textbf{200k+} tokens and \textbf{400+} tool calls, whereas in standard configurations they still exceed \textbf{35k} tokens and involve more than \textbf{60} tool calls on average. Our extensive experiments reveal that LLM-agents consistently underperform in these settings, whereas human participants achieve higher scores, underscoring a persistent gap in agents' long-horizon abilities. We also observe that simple scaling fails in our task. To better illustrate the failure of agents, we conduct an in-depth analysis of collected trajectories. We identify eight types of errors and attribute them to two primary causes: in-context locking and functional fundamental capability gaps. \href{https://github.com/StarDewXXX/UltraHorizon}{Our code will be available here.}
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhishLumos: An Adaptive Multi-Agent System for Proactive Phishing Campaign Mitigation</title>
<link>https://arxiv.org/abs/2509.21772</link>
<guid>https://arxiv.org/abs/2509.21772</guid>
<content:encoded><![CDATA[
arXiv:2509.21772v1 Announce Type: new 
Abstract: Phishing attacks are a significant societal threat, disproportionately harming vulnerable populations and eroding trust in essential digital services. Current defenses are often reactive, failing against modern evasive tactics like cloaking that conceal malicious content. To address this, we introduce PhishLumos, an adaptive multi-agent system that proactively mitigates entire attack campaigns. It confronts a core cybersecurity imbalance: attackers can easily scale operations, while defense remains an intensive expert task. Instead of being blocked by evasion, PhishLumos treats it as a critical signal to investigate the underlying infrastructure. Its Large Language Model (LLM)-powered agents uncover shared hosting, certificates, and domain registration patterns. On real-world data, our system identified 100% of campaigns in the median case, over a week before their confirmation by cybersecurity experts. PhishLumos demonstrates a practical shift from reactive URL blocking to proactive campaign mitigation, protecting users before they are harmed and making the digital world safer for all.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking MLLM-based Web Understanding: Reasoning, Robustness and Safety</title>
<link>https://arxiv.org/abs/2509.21782</link>
<guid>https://arxiv.org/abs/2509.21782</guid>
<content:encoded><![CDATA[
arXiv:2509.21782v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) are increasingly positioned as AI collaborators for building complex web-related applications like GUI agents and front-end code generation. However, existing benchmarks largely emphasize visual perception or UI code generation, showing insufficient evaluation on the reasoning, robustness and safety capability required for end-to-end web applications. To bridge the gap, we introduce a comprehensive web understanding benchmark, named WebRSSBench, that jointly evaluates Reasoning, Robustness, and Safety across eight tasks, such as position relationship reasoning, color robustness, and safety critical detection, etc. The benchmark is constructed from 729 websites and contains 3799 question answer pairs that probe multi-step inference over page structure, text, widgets, and safety-critical interactions. To ensure reliable measurement, we adopt standardized prompts, deterministic evaluation scripts, and multi-stage quality control combining automatic checks with targeted human verification. We evaluate 12 MLLMs on WebRSSBench. The results reveal significant gaps, models still struggle with compositional and cross-element reasoning over realistic layouts, show limited robustness when facing perturbations in user interfaces and content such as layout rearrangements or visual style shifts, and are rather conservative in recognizing and avoiding safety critical or irreversible actions. Our code is available at https://github.com/jinliang-byte/webssrbench.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Multi-Agent System: Mitigating Hallucination Snowballing via Visual Flow</title>
<link>https://arxiv.org/abs/2509.21789</link>
<guid>https://arxiv.org/abs/2509.21789</guid>
<content:encoded><![CDATA[
arXiv:2509.21789v1 Announce Type: new 
Abstract: Multi-Agent System (MAS) powered by Visual Language Models (VLMs) enables challenging tasks but suffers from a novel failure term, multi-agent visual hallucination snowballing, where hallucinations are seeded in a single agent and amplified by following ones due to the over-reliance on textual flow to relay visual information. Through turn-, layer-, and token-wise attention analyses, we provide detailed insights into the essence of hallucination snowballing regarding the reduction of visual attention allocation. It leads us to identify a subset of vision tokens with a unimodal attention peak in middle layers that best preserve visual evidence but gradually diminish in deeper agent turns, resulting in the visual hallucination snowballing in MAS. Thus, we propose ViF, a lightweight, plug-and-play mitigation paradigm that relays inter-agent messages with Visual Flow powered by the selected visual relay tokens and applies attention reallocation to amplify this pattern. The experiment results demonstrate that our method markedly reduces hallucination snowballing, consistently improving the performance across eight benchmarks based on four common MAS structures and ten base models. The source code will be available at: https://github.com/YU-deep/ViF.git.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D-Artemis: A Deliberative Cognitive Framework for Mobile GUI Multi-Agents</title>
<link>https://arxiv.org/abs/2509.21799</link>
<guid>https://arxiv.org/abs/2509.21799</guid>
<content:encoded><![CDATA[
arXiv:2509.21799v1 Announce Type: new 
Abstract: Graphical User Interface (GUI) agents aim to automate a wide spectrum of human tasks by emulating user interaction. Despite rapid advancements, current approaches are hindered by several critical challenges: data bottleneck in end-to-end training, high cost of delayed error detection, and risk of contradictory guidance. Inspired by the human cognitive loop of Thinking, Alignment, and Reflection, we present D-Artemis -- a novel deliberative framework in this paper. D-Artemis leverages a fine-grained, app-specific tip retrieval mechanism to inform its decision-making process. It also employs a proactive Pre-execution Alignment stage, where Thought-Action Consistency (TAC) Check module and Action Correction Agent (ACA) work in concert to mitigate the risk of execution failures. A post-execution Status Reflection Agent (SRA) completes the cognitive loop, enabling strategic learning from experience. Crucially, D-Artemis enhances the capabilities of general-purpose Multimodal large language models (MLLMs) for GUI tasks without the need for training on complex trajectory datasets, demonstrating strong generalization. D-Artemis establishes new state-of-the-art (SOTA) results across both major benchmarks, achieving a 75.8% success rate on AndroidWorld and 96.8% on ScreenSpot-V2. Extensive ablation studies further demonstrate the significant contribution of each component to the framework.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProRe: A Proactive Reward System for GUI Agents via Reasoner-Actor Collaboration</title>
<link>https://arxiv.org/abs/2509.21823</link>
<guid>https://arxiv.org/abs/2509.21823</guid>
<content:encoded><![CDATA[
arXiv:2509.21823v1 Announce Type: new 
Abstract: Reward is critical to the evaluation and training of large language models (LLMs). However, existing rule-based or model-based reward methods struggle to generalize to GUI agents, where access to ground-truth trajectories or application databases is often unavailable, and static trajectory-based LLM-as-a-Judge approaches suffer from limited accuracy. To address these challenges, we propose ProRe, a proactive reward system that leverages a general-purpose reasoner and domain-specific evaluator agents (actors). The reasoner schedules targeted state probing tasks, which the evaluator agents then execute by actively interacting with the environment to collect additional observations. This enables the reasoner to assign more accurate and verifiable rewards to GUI agents. Empirical results on over 3K trajectories demonstrate that ProRe improves reward accuracy and F1 score by up to 5.3% and 19.4%, respectively. Furthermore, integrating ProRe with state-of-the-art policy agents yields a success rate improvement of up to 22.4%.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DS-STAR: Data Science Agent via Iterative Planning and Verification</title>
<link>https://arxiv.org/abs/2509.21825</link>
<guid>https://arxiv.org/abs/2509.21825</guid>
<content:encoded><![CDATA[
arXiv:2509.21825v1 Announce Type: new 
Abstract: Data science, which transforms raw data into actionable insights, is critical for data-driven decision-making. However, these tasks are often complex, involving steps for exploring multiple data sources and synthesizing findings to deliver insightful answers. While large language models (LLMs) show significant promise in automating this process, they often struggle with heterogeneous data formats and generate sub-optimal analysis plans, as verifying plan sufficiency is inherently difficult without ground-truth labels for such open-ended tasks. To overcome these limitations, we introduce DS-STAR, a novel data science agent. Specifically, DS-STAR makes three key contributions: (1) a data file analysis module that automatically explores and extracts context from diverse data formats, including unstructured types; (2) a verification step where an LLM-based judge evaluates the sufficiency of the analysis plan at each stage; and (3) a sequential planning mechanism that starts with a simple, executable plan and iteratively refines it based on the DS-STAR's feedback until its sufficiency is verified. This iterative refinement allows DS-STAR to reliably navigate complex analyses involving diverse data sources. Our experiments show that DS-STAR achieves state-of-the-art performance across three challenging benchmarks: DABStep, KramaBench, and DA-Code. Moreover, DS-STAR particularly outperforms baselines on hard tasks that require processing multiple data files with heterogeneous formats.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models</title>
<link>https://arxiv.org/abs/2509.21826</link>
<guid>https://arxiv.org/abs/2509.21826</guid>
<content:encoded><![CDATA[
arXiv:2509.21826v1 Announce Type: new 
Abstract: Large language models (LLMs) transcend passive generation and act as goal-directed agents by invoking external tools. Reinforcement learning (RL) offers a principled framework for optimizing these emergent tool-use policies, yet the prevailing paradigm relies exclusively on sparse outcome rewards and lacks consideration of the particularity of tool-use tasks, inflating policy-gradient variance and resulting in inefficient training. To better understand and address these challenges, we first establish a theoretical link between policy entropy and training stability of tool-use tasks, which reveals that structured, low-entropy tokens are primary determinants of rewards. Motivated by this insight, we propose \textbf{Res}haped \textbf{T}oken-level policy gradients (\textbf{ResT}) for tool-use tasks. ResT reshapes the policy gradient through entropy-informed token reweighting, progressively upweighting reasoning tokens as training proceeds. This entropy-aware scheme enables a smooth shift from structural correctness to semantic reasoning and stabilizes convergence in multi-turn tool-use tasks. Evaluation on BFCL and API-Bank shows that ResT achieves state-of-the-art results, outperforming prior methods by up to $8.76\%$. When fine-tuned on a 4B base LLM, ResT further surpasses GPT-4o by $4.11\%$ on single-turn tasks and $1.50\%$ on multi-turn base tasks.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preference-Guided Learning for Sparse-Reward Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.21828</link>
<guid>https://arxiv.org/abs/2509.21828</guid>
<content:encoded><![CDATA[
arXiv:2509.21828v1 Announce Type: new 
Abstract: We study the problem of online multi-agent reinforcement learning (MARL) in environments with sparse rewards, where reward feedback is not provided at each interaction but only revealed at the end of a trajectory. This setting, though realistic, presents a fundamental challenge: the lack of intermediate rewards hinders standard MARL algorithms from effectively guiding policy learning. To address this issue, we propose a novel framework that integrates online inverse preference learning with multi-agent on-policy optimization into a unified architecture. At its core, our approach introduces an implicit multi-agent reward learning model, built upon a preference-based value-decomposition network, which produces both global and local reward signals. These signals are further used to construct dual advantage streams, enabling differentiated learning targets for the centralized critic and decentralized actors. In addition, we demonstrate how large language models (LLMs) can be leveraged to provide preference labels that enhance the quality of the learned reward model. Empirical evaluations on state-of-the-art benchmarks, including MAMuJoCo and SMACv2, show that our method achieves superior performance compared to existing baselines, highlighting its effectiveness in addressing sparse-reward challenges in online MARL.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RobustFlow: Towards Robust Agentic Workflow Generation</title>
<link>https://arxiv.org/abs/2509.21834</link>
<guid>https://arxiv.org/abs/2509.21834</guid>
<content:encoded><![CDATA[
arXiv:2509.21834v1 Announce Type: new 
Abstract: The automated generation of agentic workflows is a promising frontier for enabling large language models (LLMs) to solve complex tasks. However, our investigation reveals that the robustness of agentic workflow remains a critical, unaddressed challenge. Current methods often generate wildly inconsistent workflows when provided with instructions that are semantically identical but differently phrased. This brittleness severely undermines their reliability and trustworthiness for real-world applications. To quantitatively diagnose this instability, we propose metrics based on nodal and topological similarity to evaluate workflow consistency against common semantic variations such as paraphrasing and noise injection. Subsequently, we further propose a novel training framework, RobustFlow, that leverages preference optimization to teach models invariance to instruction variations. By training on sets of synonymous task descriptions, RobustFlow boosts workflow robustness scores to 70\% - 90\%, which is a substantial improvement over existing approaches. The code is publicly available at https://github.com/DEFENSE-SEU/RobustFlow.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents</title>
<link>https://arxiv.org/abs/2509.21842</link>
<guid>https://arxiv.org/abs/2509.21842</guid>
<content:encoded><![CDATA[
arXiv:2509.21842v1 Announce Type: new 
Abstract: Travel planning (TP) agent has recently worked as an emerging building block to interact with external tools and resources for travel itinerary generation, ensuring enjoyable user experience. Despite its benefits, existing studies rely on hand craft prompt and fixed agent workflow, hindering more flexible and autonomous TP agent. This paper proposes DeepTravel, an end to end agentic reinforcement learning framework for building autonomous travel planning agent, capable of autonomously planning, executing tools, and reflecting on tool responses to explore, verify, and refine intermediate actions in multi step reasoning. To achieve this, we first construct a robust sandbox environment by caching transportation, accommodation and POI data, facilitating TP agent training without being constrained by real world APIs limitations (e.g., inconsistent outputs). Moreover, we develop a hierarchical reward modeling system, where a trajectory level verifier first checks spatiotemporal feasibility and filters unsatisfied travel itinerary, and then the turn level verifier further validate itinerary detail consistency with tool responses, enabling efficient and precise reward service. Finally, we propose the reply augmented reinforcement learning method that enables TP agent to periodically replay from a failures experience buffer, emerging notable agentic capacity. We deploy trained TP agent on DiDi Enterprise Solutions App and conduct comprehensive online and offline evaluations, demonstrating that DeepTravel enables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing frontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph of Agents: Principled Long Context Modeling by Emergent Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2509.21848</link>
<guid>https://arxiv.org/abs/2509.21848</guid>
<content:encoded><![CDATA[
arXiv:2509.21848v1 Announce Type: new 
Abstract: As a model-agnostic approach to long context modeling, multi-agent systems can process inputs longer than a large language model's context window without retraining or architectural modifications. However, their performance often heavily relies on hand-crafted multi-agent collaboration strategies and prompt engineering, which limit generalizability. In this work, we introduce a principled framework that formalizes the model-agnostic long context modeling problem as a compression problem, yielding an information-theoretic compression objective. Building on this framework, we propose Graph of Agents (GoA), which dynamically constructs an input-dependent collaboration structure that maximizes this objective. For Llama 3.1 8B and Qwen3 8B across six document question answering benchmarks, GoA improves the average $F_1$ score of retrieval-augmented generation by 5.7\% and a strong multi-agent baseline using a fixed collaboration structure by 16.35\%, respectively. Even with only a 2K context window, GoA surpasses the 128K context window Llama 3.1 8B on LongBench, showing a dramatic increase in effective context length. Our source code is available at https://github.com/tjoo512/graph-of-agents.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Following the TRACE: A Structured Path to Empathetic Response Generation with Multi-Agent Models</title>
<link>https://arxiv.org/abs/2509.21849</link>
<guid>https://arxiv.org/abs/2509.21849</guid>
<content:encoded><![CDATA[
arXiv:2509.21849v1 Announce Type: new 
Abstract: Empathetic response generation is a crucial task for creating more human-like and supportive conversational agents. However, existing methods face a core trade-off between the analytical depth of specialized models and the generative fluency of Large Language Models (LLMs). To address this, we propose TRACE, Task-decomposed Reasoning for Affective Communication and Empathy, a novel framework that models empathy as a structured cognitive process by decomposing the task into a pipeline for analysis and synthesis. By building a comprehensive understanding before generation, TRACE unites deep analysis with expressive generation. Experimental results show that our framework significantly outperforms strong baselines in both automatic and LLM-based evaluations, confirming that our structured decomposition is a promising paradigm for creating more capable and interpretable empathetic agents. Our code is available at https://anonymous.4open.science/r/TRACE-18EF/README.md.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reimagining Agent-based Modeling with Large Language Model Agents via Shachi</title>
<link>https://arxiv.org/abs/2509.21862</link>
<guid>https://arxiv.org/abs/2509.21862</guid>
<content:encoded><![CDATA[
arXiv:2509.21862v1 Announce Type: new 
Abstract: The study of emergent behaviors in large language model (LLM)-driven multi-agent systems is a critical research challenge, yet progress is limited by a lack of principled methodologies for controlled experimentation. To address this, we introduce Shachi, a formal methodology and modular framework that decomposes an agent's policy into core cognitive components: Configuration for intrinsic traits, Memory for contextual persistence, and Tools for expanded capabilities, all orchestrated by an LLM reasoning engine. This principled architecture moves beyond brittle, ad-hoc agent designs and enables the systematic analysis of how specific architectural choices influence collective behavior. We validate our methodology on a comprehensive 10-task benchmark and demonstrate its power through novel scientific inquiries. Critically, we establish the external validity of our approach by modeling a real-world U.S. tariff shock, showing that agent behaviors align with observed market reactions only when their cognitive architecture is appropriately configured with memory and tools. Our work provides a rigorous, open-source foundation for building and evaluating LLM agents, aimed at fostering more cumulative and scientifically grounded research.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Makes LLM Agent Simulations Useful for Policy? Insights From an Iterative Design Engagement in Emergency Preparedness</title>
<link>https://arxiv.org/abs/2509.21868</link>
<guid>https://arxiv.org/abs/2509.21868</guid>
<content:encoded><![CDATA[
arXiv:2509.21868v1 Announce Type: new 
Abstract: There is growing interest in using Large Language Models as agents (LLM agents) for social simulations to inform policy, yet real-world adoption remains limited. This paper addresses the question: How can LLM agent simulations be made genuinely useful for policy? We report on a year-long iterative design engagement with a university emergency preparedness team. Across multiple design iterations, we iteratively developed a system of 13,000 LLM agents that simulate crowd movement and communication during a large-scale gathering under various emergency scenarios. These simulations informed actual policy implementation, shaping volunteer training, evacuation protocols, and infrastructure planning. Analyzing this process, we identify three design implications: start with verifiable scenarios and build trust gradually, use preliminary simulations to elicit tacit knowledge, and treat simulation and policy development as evolving together. These implications highlight actionable pathways to making LLM agent simulations that are genuinely useful for policy.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Vehicle Maneuver Prediction using Game Theoretic Priors</title>
<link>https://arxiv.org/abs/2509.21873</link>
<guid>https://arxiv.org/abs/2509.21873</guid>
<content:encoded><![CDATA[
arXiv:2509.21873v1 Announce Type: new 
Abstract: Conventional maneuver prediction methods use some sort of classification model on temporal trajectory data to predict behavior of agents over a set time horizon. Despite of having the best precision and recall, these models cannot predict a lane change accurately unless they incorporate information about the entire scene. Level-k game theory can leverage the human-like hierarchical reasoning to come up with the most rational decisions each agent can make in a group. This can be leveraged to model interactions between different vehicles in presence of each other and hence compute the most rational decisions each agent would make. The result of game theoretic evaluation can be used as a "prior" or combined with a traditional motion-based classification model to achieve more accurate predictions. The proposed approach assumes that the states of the vehicles around the target lead vehicle are known. The module will output the most rational maneuver prediction of the target vehicle based on an online optimization solution. These predictions are instrumental in decision making systems like Adaptive Cruise Control (ACC) or Traxen's iQ-Cruise further improving the resulting fuel savings.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentPack: A Dataset of Code Changes, Co-Authored by Agents and Humans</title>
<link>https://arxiv.org/abs/2509.21891</link>
<guid>https://arxiv.org/abs/2509.21891</guid>
<content:encoded><![CDATA[
arXiv:2509.21891v1 Announce Type: new 
Abstract: Fine-tuning large language models for code editing has typically relied on mining commits and pull requests. The working hypothesis has been that commit messages describe human intent in natural language, and patches to code describe the changes that implement that intent. However, much of the previously collected data is noisy: commit messages are terse, human-written commits commingle several unrelated edits, and many commits come from simple, rule-based bots.
  The recent adoption of software engineering agents changes this landscape. Code changes co-authored by humans and agents tend to be more narrowly scoped and focused on clearer goals. Their commit messages, generated by LLMs, articulate intent and rationale in much greater detail. Moreover, when these changes land in public repositories, they are implicitly filtered by humans: maintainers discard low-quality commits to their projects.
  We present AgentPack, a corpus of 1.3M code edits co-authored by Claude Code, OpenAI Codex, and Cursor Agent across public GitHub projects up to mid-August 2025. We describe the identification and curation pipeline, quantify adoption trends of these agents, and analyze the structural properties of the edits. Finally, we show that models fine-tuned on AgentPack can outperform models trained on prior human-only commit corpora, highlighting the potential of using public data from software engineering agents to train future code-editing models.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoSCORE: Enhancing Automated Scoring with Multi-Agent Large Language Models via Structured Component Recognition</title>
<link>https://arxiv.org/abs/2509.21910</link>
<guid>https://arxiv.org/abs/2509.21910</guid>
<content:encoded><![CDATA[
arXiv:2509.21910v1 Announce Type: new 
Abstract: Automated scoring plays a crucial role in education by reducing the reliance on human raters, offering scalable and immediate evaluation of student work. While large language models (LLMs) have shown strong potential in this task, their use as end-to-end raters faces challenges such as low accuracy, prompt sensitivity, limited interpretability, and rubric misalignment. These issues hinder the implementation of LLM-based automated scoring in assessment practice. To address the limitations, we propose AutoSCORE, a multi-agent LLM framework enhancing automated scoring via rubric-aligned Structured COmponent REcognition. With two agents, AutoSCORE first extracts rubric-relevant components from student responses and encodes them into a structured representation (i.e., Scoring Rubric Component Extraction Agent), which is then used to assign final scores (i.e., Scoring Agent). This design ensures that model reasoning follows a human-like grading process, enhancing interpretability and robustness. We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Across diverse tasks and rubrics, AutoSCORE consistently improves scoring accuracy, human-machine agreement (QWK, correlations), and error metrics (MAE, RMSE) compared to single-agent baselines, with particularly strong benefits on complex, multi-dimensional rubrics, and especially large relative gains on smaller LLMs. These results demonstrate that structured component recognition combined with multi-agent design offers a scalable, reliable, and interpretable solution for automated scoring.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MotivGraph-SoIQ: Integrating Motivational Knowledge Graphs and Socratic Dialogue for Enhanced LLM Ideation</title>
<link>https://arxiv.org/abs/2509.21978</link>
<guid>https://arxiv.org/abs/2509.21978</guid>
<content:encoded><![CDATA[
arXiv:2509.21978v1 Announce Type: new 
Abstract: Large Language Models (LLMs) hold substantial potential for accelerating academic ideation but face critical challenges in grounding ideas and mitigating confirmation bias for further refinement. We propose integrating motivational knowledge graphs and socratic dialogue to address these limitations in enhanced LLM ideation (MotivGraph-SoIQ). This novel framework provides essential grounding and practical idea improvement steps for LLM ideation by integrating a Motivational Knowledge Graph (MotivGraph) with a Q-Driven Socratic Ideator. The MotivGraph structurally stores three key node types(problem, challenge and solution) to offer motivation grounding for the LLM ideation process. The Ideator is a dual-agent system utilizing Socratic questioning, which facilitates a rigorous refinement process that mitigates confirmation bias and improves idea quality across novelty, experimental rigor, and motivational rationality dimensions. On the ICLR25 paper topics dataset, MotivGraph-SoIQ exhibits clear advantages over existing state-of-the-art approaches across LLM-based scoring, ELO ranking, and human evaluation metrics.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief World for Optimizing Embodied Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2509.21981</link>
<guid>https://arxiv.org/abs/2509.21981</guid>
<content:encoded><![CDATA[
arXiv:2509.21981v1 Announce Type: new 
Abstract: Effective real-world multi-agent collaboration requires not only accurate planning but also the ability to reason about collaborators' intents -- a crucial capability for avoiding miscoordination and redundant communication under partial observable environments. Due to their strong planning and reasoning capabilities, large language models (LLMs) have emerged as promising autonomous agents for collaborative task solving. However, existing collaboration frameworks for LLMs overlook their reasoning potential for dynamic intent inference, and thus produce inconsistent plans and redundant communication, reducing collaboration efficiency. To bridge this gap, we propose CoBel-World, a novel framework that equips LLM agents with a collaborative belief world -- an internal representation jointly modeling the physical environment and collaborators' mental states. CoBel-World enables agents to parse open-world task knowledge into structured beliefs via a symbolic belief language, and perform zero-shot Bayesian-style belief updates through LLM reasoning. This allows agents to proactively detect potential miscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated on challenging embodied benchmarks (i.e., TDW-MAT and C-WAH), CoBel-World significantly reduces communication costs by 22-60% and improves task completion efficiency by 4-28% compared to the strongest baseline. Our results show that explicit, intent-aware belief modeling is essential for efficient and human-like collaboration in LLM-based multi-agent systems.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RISK: A Framework for GUI Agents in E-commerce Risk Management</title>
<link>https://arxiv.org/abs/2509.21982</link>
<guid>https://arxiv.org/abs/2509.21982</guid>
<content:encoded><![CDATA[
arXiv:2509.21982v1 Announce Type: new 
Abstract: E-commerce risk management requires aggregating diverse, deeply embedded web data through multi-step, stateful interactions, which traditional scraping methods and most existing Graphical User Interface (GUI) agents cannot handle. These agents are typically limited to single-step tasks and lack the ability to manage dynamic, interactive content critical for effective risk assessment. To address this challenge, we introduce RISK, a novel framework designed to build and deploy GUI agents for this domain. RISK integrates three components: (1) RISK-Data, a dataset of 8,492 single-step and 2,386 multi-step interaction trajectories, collected through a high-fidelity browser framework and a meticulous data curation process; (2) RISK-Bench, a benchmark with 802 single-step and 320 multi-step trajectories across three difficulty levels for standardized evaluation; and (3) RISK-R1, a R1-style reinforcement fine-tuning framework considering four aspects: (i) Output Format: Updated format reward to enhance output syntactic correctness and task comprehension, (ii) Single-step Level: Stepwise accuracy reward to provide granular feedback during early training stages, (iii) Multi-step Level: Process reweight to emphasize critical later steps in interaction sequences, and (iv) Task Level: Level reweight to focus on tasks of varying difficulty. Experiments show that RISK-R1 outperforms existing baselines, achieving a 6.8% improvement in offline single-step and an 8.8% improvement in offline multi-step. Moreover, it attains a top task success rate of 70.5% in online evaluation. RISK provides a scalable, domain-specific solution for automating complex web interactions, advancing the state of the art in e-commerce risk management.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rate-Distortion Optimized Communication for Collaborative Perception</title>
<link>https://arxiv.org/abs/2509.21994</link>
<guid>https://arxiv.org/abs/2509.21994</guid>
<content:encoded><![CDATA[
arXiv:2509.21994v1 Announce Type: new 
Abstract: Collaborative perception emphasizes enhancing environmental understanding by enabling multiple agents to share visual information with limited bandwidth resources. While prior work has explored the empirical trade-off between task performance and communication volume, a significant gap remains in the theoretical foundation. To fill this gap, we draw on information theory and introduce a pragmatic rate-distortion theory for multi-agent collaboration, specifically formulated to analyze performance-communication trade-off in goal-oriented multi-agent systems. This theory concretizes two key conditions for designing optimal communication strategies: supplying pragmatically relevant information and transmitting redundancy-less messages. Guided by these two conditions, we propose RDcomm, a communication-efficient collaborative perception framework that introduces two key innovations: i) task entropy discrete coding, which assigns features with task-relevant codeword-lengths to maximize the efficiency in supplying pragmatic information; ii) mutual-information-driven message selection, which utilizes mutual information neural estimation to approach the optimal redundancy-less condition. Experiments on 3D object detection and BEV segmentation demonstrate that RDcomm achieves state-of-the-art accuracy on DAIR-V2X and OPV2V, while reducing communication volume by up to 108 times. The code will be released.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments</title>
<link>https://arxiv.org/abs/2509.21998</link>
<guid>https://arxiv.org/abs/2509.21998</guid>
<content:encoded><![CDATA[
arXiv:2509.21998v1 Announce Type: new 
Abstract: As LLMs are increasingly deployed as agents, agentic reasoning - the ability to combine tool use, especially search, and reasoning - becomes a critical skill. However, it is hard to disentangle agentic reasoning when evaluated in complex environments and tasks. Current agent benchmarks often mix agentic reasoning with challenging math reasoning, expert-level knowledge, and other advanced capabilities. To fill this gap, we build a novel benchmark, GSM-Agent, where an LLM agent is required to solve grade-school-level reasoning problems, but is only presented with the question in the prompt without the premises that contain the necessary information to solve the task, and needs to proactively collect that information using tools. Although the original tasks are grade-school math problems, we observe that even frontier models like GPT-5 only achieve 67% accuracy. To understand and analyze the agentic reasoning patterns, we propose the concept of agentic reasoning graph: cluster the environment's document embeddings into nodes, and map each tool call to its nearest node to build a reasoning path. Surprisingly, we identify that the ability to revisit a previously visited node, widely taken as a crucial pattern in static reasoning, is often missing for agentic reasoning for many models. Based on the insight, we propose a tool-augmented test-time scaling method to improve LLM's agentic reasoning performance by adding tools to encourage models to revisit. We expect our benchmark and the agentic reasoning framework to aid future studies of understanding and pushing the boundaries of agentic reasoning.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Goal-Guided Efficient Exploration via Large Language Model in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.22008</link>
<guid>https://arxiv.org/abs/2509.22008</guid>
<content:encoded><![CDATA[
arXiv:2509.22008v1 Announce Type: new 
Abstract: Real-world decision-making tasks typically occur in complex and open environments, posing significant challenges to reinforcement learning (RL) agents' exploration efficiency and long-horizon planning capabilities. A promising approach is LLM-enhanced RL, which leverages the rich prior knowledge and strong planning capabilities of LLMs to guide RL agents in efficient exploration. However, existing methods mostly rely on frequent and costly LLM invocations and suffer from limited performance due to the semantic mismatch. In this paper, we introduce a Structured Goal-guided Reinforcement Learning (SGRL) method that integrates a structured goal planner and a goal-conditioned action pruner to guide RL agents toward efficient exploration. Specifically, the structured goal planner utilizes LLMs to generate a reusable, structured function for goal generation, in which goals are prioritized. Furthermore, by utilizing LLMs to determine goals' priority weights, it dynamically generates forward-looking goals to guide the agent's policy toward more promising decision-making trajectories. The goal-conditioned action pruner employs an action masking mechanism that filters out actions misaligned with the current goal, thereby constraining the RL agent to select goal-consistent policies. We evaluate the proposed method on Crafter and Craftax-Classic, and experimental results demonstrate that SGRL achieves superior performance compared to existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphSearch: An Agentic Deep Searching Workflow for Graph Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2509.22009</link>
<guid>https://arxiv.org/abs/2509.22009</guid>
<content:encoded><![CDATA[
arXiv:2509.22009v1 Announce Type: new 
Abstract: Graph Retrieval-Augmented Generation (GraphRAG) enhances factual reasoning in LLMs by structurally modeling knowledge through graph-based representations. However, existing GraphRAG approaches face two core limitations: shallow retrieval that fails to surface all critical evidence, and inefficient utilization of pre-constructed structural graph data, which hinders effective reasoning from complex queries. To address these challenges, we propose \textsc{GraphSearch}, a novel agentic deep searching workflow with dual-channel retrieval for GraphRAG. \textsc{GraphSearch} organizes the retrieval process into a modular framework comprising six modules, enabling multi-turn interactions and iterative reasoning. Furthermore, \textsc{GraphSearch} adopts a dual-channel retrieval strategy that issues semantic queries over chunk-based text data and relational queries over structural graph data, enabling comprehensive utilization of both modalities and their complementary strengths. Experimental results across six multi-hop RAG benchmarks demonstrate that \textsc{GraphSearch} consistently improves answer accuracy and generation quality over the traditional strategy, confirming \textsc{GraphSearch} as a promising direction for advancing graph retrieval-augmented generation.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics</title>
<link>https://arxiv.org/abs/2509.22014</link>
<guid>https://arxiv.org/abs/2509.22014</guid>
<content:encoded><![CDATA[
arXiv:2509.22014v1 Announce Type: new 
Abstract: Healthcare robotics requires robust multimodal perception and reasoning to ensure safety in dynamic clinical environments. Current Vision-Language Models (VLMs) demonstrate strong general-purpose capabilities but remain limited in temporal reasoning, uncertainty estimation, and structured outputs needed for robotic planning. We present a lightweight agentic multimodal framework for video-based scene understanding. Combining the Qwen2.5-VL-3B-Instruct model with a SmolAgent-based orchestration layer, it supports chain-of-thought reasoning, speech-vision fusion, and dynamic tool invocation. The framework generates structured scene graphs and leverages a hybrid retrieval module for interpretable and adaptive reasoning. Evaluations on the Video-MME benchmark and a custom clinical dataset show competitive accuracy and improved robustness compared to state-of-the-art VLMs, demonstrating its potential for applications in robot-assisted surgery, patient monitoring, and decision support.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Your AI, My Shell": Demystifying Prompt Injection Attacks on Agentic AI Coding Editors</title>
<link>https://arxiv.org/abs/2509.22040</link>
<guid>https://arxiv.org/abs/2509.22040</guid>
<content:encoded><![CDATA[
arXiv:2509.22040v1 Announce Type: new 
Abstract: Agentic AI coding editors driven by large language models have recently become more popular due to their ability to improve developer productivity during software development. Modern editors such as Cursor are designed not just for code completion, but also with more system privileges for complex coding tasks (e.g., run commands in the terminal, access development environments, and interact with external systems). While this brings us closer to the "fully automated programming" dream, it also raises new security concerns. In this study, we present the first empirical analysis of prompt injection attacks targeting these high-privilege agentic AI coding editors. We show how attackers can remotely exploit these systems by poisoning external development resources with malicious instructions, effectively hijacking AI agents to run malicious commands, turning "your AI" into "attacker's shell". To perform this analysis, we implement AIShellJack, an automated testing framework for assessing prompt injection vulnerabilities in agentic AI coding editors. AIShellJack contains 314 unique attack payloads that cover 70 techniques from the MITRE ATT&amp;CK framework. Using AIShellJack, we conduct a large-scale evaluation on GitHub Copilot and Cursor, and our evaluation results show that attack success rates can reach as high as 84% for executing malicious commands. Moreover, these attacks are proven effective across a wide range of objectives, ranging from initial access and system discovery to credential theft and data exfiltration.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taxonomy of Comprehensive Safety for Clinical Agents</title>
<link>https://arxiv.org/abs/2509.22041</link>
<guid>https://arxiv.org/abs/2509.22041</guid>
<content:encoded><![CDATA[
arXiv:2509.22041v1 Announce Type: new 
Abstract: Safety is a paramount concern in clinical chatbot applications, where inaccurate or harmful responses can lead to serious consequences. Existing methods--such as guardrails and tool calling--often fall short in addressing the nuanced demands of the clinical domain. In this paper, we introduce TACOS (TAxonomy of COmprehensive Safety for Clinical Agents), a fine-grained, 21-class taxonomy that integrates safety filtering and tool selection into a single user intent classification step. TACOS is a taxonomy that can cover a wide spectrum of clinical and non-clinical queries, explicitly modeling varying safety thresholds and external tool dependencies. To validate our framework, we curate a TACOS-annotated dataset and perform extensive experiments. Our results demonstrate the value of a new taxonomy specialized for clinical agent settings, and reveal useful insights about train data distribution and pretrained knowledge of base models.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios</title>
<link>https://arxiv.org/abs/2509.22097</link>
<guid>https://arxiv.org/abs/2509.22097</guid>
<content:encoded><![CDATA[
arXiv:2509.22097v1 Announce Type: new 
Abstract: Large language model (LLM) powered code agents are rapidly transforming software engineering by automating tasks such as testing, debugging, and repairing, yet the security risks of their generated code have become a critical concern. Existing benchmarks have offered valuable insights but remain insufficient: they often overlook the genuine context in which vulnerabilities were introduced or adopt narrow evaluation protocols that fail to capture either functional correctness or newly introduced vulnerabilities. We therefore introduce SecureAgentBench, a benchmark of 105 coding tasks designed to rigorously evaluate code agents' capabilities in secure code generation. Each task includes (i) realistic task settings that require multi-file edits in large repositories, (ii) aligned contexts based on real-world open-source vulnerabilities with precisely identified introduction points, and (iii) comprehensive evaluation that combines functionality testing, vulnerability checking through proof-of-concept exploits, and detection of newly introduced vulnerabilities using static analysis. We evaluate three representative agents (SWE-agent, OpenHands, and Aider) with three state-of-the-art LLMs (Claude 3.7 Sonnet, GPT-4.1, and DeepSeek-V3.1). Results show that (i) current agents struggle to produce secure code, as even the best-performing one, SWE-agent supported by DeepSeek-V3.1, achieves merely 15.2% correct-and-secure solutions, (ii) some agents produce functionally correct code but still introduce vulnerabilities, including new ones not previously recorded, and (iii) adding explicit security instructions for agents does not significantly improve secure coding, underscoring the need for further research. These findings establish SecureAgentBench as a rigorous benchmark for secure code generation and a step toward more reliable software development with LLMs.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Path Finding via Offline RL and LLM Collaboration</title>
<link>https://arxiv.org/abs/2509.22130</link>
<guid>https://arxiv.org/abs/2509.22130</guid>
<content:encoded><![CDATA[
arXiv:2509.22130v1 Announce Type: new 
Abstract: Multi-Agent Path Finding (MAPF) poses a significant and challenging problem critical for applications in robotics and logistics, particularly due to its combinatorial complexity and the partial observability inherent in realistic environments. Decentralized reinforcement learning methods commonly encounter two substantial difficulties: first, they often yield self-centered behaviors among agents, resulting in frequent collisions, and second, their reliance on complex communication modules leads to prolonged training times, sometimes spanning weeks. To address these challenges, we propose an efficient decentralized planning framework based on the Decision Transformer (DT), uniquely leveraging offline reinforcement learning to substantially reduce training durations from weeks to mere hours. Crucially, our approach effectively handles long-horizon credit assignment and significantly improves performance in scenarios with sparse and delayed rewards. Furthermore, to overcome adaptability limitations inherent in standard RL methods under dynamic environmental changes, we integrate a large language model (GPT-4o) to dynamically guide agent policies. Extensive experiments in both static and dynamically changing environments demonstrate that our DT-based approach, augmented briefly by GPT-4o, significantly enhances adaptability and performance.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Log2Plan: An Adaptive GUI Automation Framework Integrated with Task Mining Approach</title>
<link>https://arxiv.org/abs/2509.22137</link>
<guid>https://arxiv.org/abs/2509.22137</guid>
<content:encoded><![CDATA[
arXiv:2509.22137v1 Announce Type: new 
Abstract: GUI task automation streamlines repetitive tasks, but existing LLM or VLM-based planner-executor agents suffer from brittle generalization, high latency, and limited long-horizon coherence. Their reliance on single-shot reasoning or static plans makes them fragile under UI changes or complex tasks. Log2Plan addresses these limitations by combining a structured two-level planning framework with a task mining approach over user behavior logs, enabling robust and adaptable GUI automation. Log2Plan constructs high-level plans by mapping user commands to a structured task dictionary, enabling consistent and generalizable automation. To support personalization and reuse, it employs a task mining approach from user behavior logs that identifies user-specific patterns. These high-level plans are then grounded into low-level action sequences by interpreting real-time GUI context, ensuring robust execution across varying interfaces. We evaluated Log2Plan on 200 real-world tasks, demonstrating significant improvements in task success rate and execution time. Notably, it maintains over 60.0% success rate even on long-horizon task sequences, highlighting its robustness in complex, multi-step workflows.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging LLM Agents for Automated Video Game Testing</title>
<link>https://arxiv.org/abs/2509.22170</link>
<guid>https://arxiv.org/abs/2509.22170</guid>
<content:encoded><![CDATA[
arXiv:2509.22170v1 Announce Type: new 
Abstract: Testing MMORPGs (Massively Multiplayer Online Role-Playing Games) is a critical yet labor-intensive task in game development due to their complexity and frequent updating nature. Traditional automated game testing approaches struggle to achieve high state coverage and efficiency in these rich, open-ended environments, while existing LLM-based game-playing approaches are limited to shallow reasoning ability in understanding complex game state-action spaces and long-complex tasks. To address these challenges, we propose TITAN, an effective LLM-driven agent framework for intelligent MMORPG testing. TITAN incorporates four key components to: (1) perceive and abstract high-dimensional game states, (2) proactively optimize and prioritize available actions, (3) enable long-horizon reasoning with action trace memory and reflective self-correction, and (4) employ LLM-based oracles to detect potential functional and logic bugs with diagnostic reports.
  We implement the prototype of TITAN and evaluate it on two large-scale commercial MMORPGs spanning both PC and mobile platforms. In our experiments, TITAN achieves significantly higher task completion rates (95%) and bug detection performance compared to existing automated game testing approaches. An ablation study further demonstrates that each core component of TITAN contributes substantially to its overall performance. Notably, TITAN detects four previously unknown bugs that prior testing approaches fail to identify. We provide an in-depth discussion of these results, which offer guidance for new avenues of advancing intelligent, general-purpose testing systems. Moreover, TITAN has been deployed in eight real-world game QA pipelines, underscoring its practical impact as an LLM-driven game testing framework.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiency Boost in Decentralized Optimization: Reimagining Neighborhood Aggregation with Minimal Overhead</title>
<link>https://arxiv.org/abs/2509.22174</link>
<guid>https://arxiv.org/abs/2509.22174</guid>
<content:encoded><![CDATA[
arXiv:2509.22174v1 Announce Type: new 
Abstract: In today's data-sensitive landscape, distributed learning emerges as a vital tool, not only fortifying privacy measures but also streamlining computational operations. This becomes especially crucial within fully decentralized infrastructures where local processing is imperative due to the absence of centralized aggregation. Here, we introduce DYNAWEIGHT, a novel framework to information aggregation in multi-agent networks. DYNAWEIGHT offers substantial acceleration in decentralized learning with minimal additional communication and memory overhead. Unlike traditional static weight assignments, such as Metropolis weights, DYNAWEIGHT dynamically allocates weights to neighboring servers based on their relative losses on local datasets. Consequently, it favors servers possessing diverse information, particularly in scenarios of substantial data heterogeneity. Our experiments on various datasets MNIST, CIFAR10, and CIFAR100 incorporating various server counts and graph topologies, demonstrate notable enhancements in training speeds. Notably, DYNAWEIGHT functions as an aggregation scheme compatible with any underlying server-level optimization algorithm, underscoring its versatility and potential for widespread integration.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Collective Behaviors of Autonomous Vehicles on Urban Traffic Dynamics: A Multi-Agent Reinforcement Learning Approach</title>
<link>https://arxiv.org/abs/2509.22216</link>
<guid>https://arxiv.org/abs/2509.22216</guid>
<content:encoded><![CDATA[
arXiv:2509.22216v1 Announce Type: new 
Abstract: This study examines the potential impact of reinforcement learning (RL)-enabled autonomous vehicles (AV) on urban traffic flow in a mixed traffic environment. We focus on a simplified day-to-day route choice problem in a multi-agent setting. We consider a city network where human drivers travel through their chosen routes to reach their destinations in minimum travel time. Then, we convert one-third of the population into AVs, which are RL agents employing Deep Q-learning algorithm. We define a set of optimization targets, or as we call them behaviors, namely selfish, collaborative, competitive, social, altruistic, and malicious. We impose a selected behavior on AVs through their rewards. We run our simulations using our in-house developed RL framework PARCOUR. Our simulations reveal that AVs optimize their travel times by up to 5\%, with varying impacts on human drivers' travel times depending on the AV behavior. In all cases where AVs adopt a self-serving behavior, they achieve shorter travel times than human drivers. Our findings highlight the complexity differences in learning tasks of each target behavior. We demonstrate that the multi-agent RL setting is applicable for collective routing on traffic networks, though their impact on coexisting parties greatly varies with the behaviors adopted.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VizGen: Data Exploration and Visualization from Natural Language via a Multi-Agent AI Architecture</title>
<link>https://arxiv.org/abs/2509.22218</link>
<guid>https://arxiv.org/abs/2509.22218</guid>
<content:encoded><![CDATA[
arXiv:2509.22218v1 Announce Type: new 
Abstract: Data visualization is essential for interpreting complex datasets, yet traditional tools often require technical expertise, limiting accessibility. VizGen is an AI-assisted graph generation system that empowers users to create meaningful visualizations using natural language. Leveraging advanced NLP and LLMs like Claude 3.7 Sonnet and Gemini 2.0 Flash, it translates user queries into SQL and recommends suitable graph types. Built on a multi-agent architecture, VizGen handles SQL generation, graph creation, customization, and insight extraction. Beyond visualization, it analyzes data for patterns, anomalies, and correlations, and enhances user understanding by providing explanations enriched with contextual information gathered from the internet. The system supports real-time interaction with SQL databases and allows conversational graph refinement, making data analysis intuitive and accessible. VizGen democratizes data visualization by bridging the gap between technical complexity and user-friendly design.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FeatBench: Evaluating Coding Agents on Feature Implementation for Vibe Coding</title>
<link>https://arxiv.org/abs/2509.22237</link>
<guid>https://arxiv.org/abs/2509.22237</guid>
<content:encoded><![CDATA[
arXiv:2509.22237v1 Announce Type: new 
Abstract: The rapid advancement of Large Language Models (LLMs) has given rise to a novel software development paradigm known as "vibe coding," where users interact with coding agents through high-level natural language. However, existing evaluation benchmarks for code generation inadequately assess an agent's vibe coding capabilities. Existing benchmarks are misaligned, as they either require code-level specifications or focus narrowly on issue-solving, neglecting the critical scenario of feature implementation within the vibe coding paradiam. To address this gap, we propose FeatBench, a novel benchmark for vibe coding that focuses on feature implementation. Our benchmark is distinguished by several key features: 1. Pure Natural Language Prompts. Task inputs consist solely of abstract natural language descriptions, devoid of any code or structural hints. 2. A Rigorous & Evolving Data Collection Process. FeatBench is built on a multi-level filtering pipeline to ensure quality and a fully automated pipeline to evolve the benchmark, mitigating data contamination. 3. Comprehensive Test Cases. Each task includes Fail-to-Pass (F2P) and Pass-to-Pass (P2P) tests to verify correctness and prevent regressions. 4. Diverse Application Domains. The benchmark includes repositories from diverse domains to ensure it reflects real-world scenarios. We evaluate two state-of-the-art agent frameworks with four leading LLMs on FeatBench. Our evaluation reveals that feature implementation within the vibe coding paradigm is a significant challenge, with the highest success rate of only 29.94%. Our analysis also reveals a tendency for "aggressive implementation," a strategy that paradoxically leads to both critical failures and superior software design. We release FeatBench, our automated collection pipeline, and all experimental results to facilitate further community research.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Secure and Efficient Access Control for Computer-Use Agents via Context Space</title>
<link>https://arxiv.org/abs/2509.22256</link>
<guid>https://arxiv.org/abs/2509.22256</guid>
<content:encoded><![CDATA[
arXiv:2509.22256v1 Announce Type: new 
Abstract: Large language model (LLM)-based computer-use agents represent a convergence of AI and OS capabilities, enabling natural language to control system- and application-level functions. However, due to LLMs' inherent uncertainty issues, granting agents control over computers poses significant security risks. When agent actions deviate from user intentions, they can cause irreversible consequences. Existing mitigation approaches, such as user confirmation and LLM-based dynamic action validation, still suffer from limitations in usability, security, and performance. To address these challenges, we propose CSAgent, a system-level, static policy-based access control framework for computer-use agents. To bridge the gap between static policy and dynamic context and user intent, CSAgent introduces intent- and context-aware policies, and provides an automated toolchain to assist developers in constructing and refining them. CSAgent enforces these policies through an optimized OS service, ensuring that agent actions can only be executed under specific user intents and contexts. CSAgent supports protecting agents that control computers through diverse interfaces, including API, CLI, and GUI. We implement and evaluate CSAgent, which successfully defends against more than 99.36% of attacks while introducing only 6.83% performance overhead.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Detection -- Orchestrating Human-Robot-Robot Assistance via an Internet of Robotic Things Paradigm</title>
<link>https://arxiv.org/abs/2509.22296</link>
<guid>https://arxiv.org/abs/2509.22296</guid>
<content:encoded><![CDATA[
arXiv:2509.22296v1 Announce Type: new 
Abstract: Hospital patient falls remain a critical and costly challenge worldwide. While conventional fall prevention systems typically rely on post-fall detection or reactive alerts, they also often suffer from high false positive rates and fail to address the underlying patient needs that lead to bed-exit attempts. This paper presents a novel system architecture that leverages the Internet of Robotic Things (IoRT) to orchestrate human-robot-robot interaction for proactive and personalized patient assistance. The system integrates a privacy-preserving thermal sensing model capable of real-time bed-exit prediction, with two coordinated robotic agents that respond dynamically based on predicted intent and patient input. This orchestrated response could not only reduce fall risk but also attend to the patient's underlying motivations for movement, such as thirst, discomfort, or the need for assistance, before a hazardous situation arises. Our contributions with this pilot study are three-fold: (1) a modular IoRT-based framework enabling distributed sensing, prediction, and multi-robot coordination; (2) a demonstration of low-resolution thermal sensing for accurate, privacy-preserving preemptive bed-exit detection; and (3) results from a user study and systematic error analysis that inform the design of situationally aware, multi-agent interactions in hospital settings. The findings highlight how interactive and connected robotic systems can move beyond passive monitoring to deliver timely, meaningful assistance, empowering safer, more responsive care environments.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRIME: Planning and Retrieval-Integrated Memory for Enhanced Reasoning</title>
<link>https://arxiv.org/abs/2509.22315</link>
<guid>https://arxiv.org/abs/2509.22315</guid>
<content:encoded><![CDATA[
arXiv:2509.22315v1 Announce Type: new 
Abstract: Inspired by the dual-process theory of human cognition from \textit{Thinking, Fast and Slow}, we introduce \textbf{PRIME} (Planning and Retrieval-Integrated Memory for Enhanced Reasoning), a multi-agent reasoning framework that dynamically integrates \textbf{System 1} (fast, intuitive thinking) and \textbf{System 2} (slow, deliberate thinking). PRIME first employs a Quick Thinking Agent (System 1) to generate a rapid answer; if uncertainty is detected, it then triggers a structured System 2 reasoning pipeline composed of specialized agents for \textit{planning}, \textit{hypothesis generation}, \textit{retrieval}, \textit{information integration}, and \textit{decision-making}. This multi-agent design faithfully mimics human cognitive processes and enhances both efficiency and accuracy. Experimental results with LLaMA 3 models demonstrate that PRIME enables open-source LLMs to perform competitively with state-of-the-art closed-source models like GPT-4 and GPT-4o on benchmarks requiring multi-hop and knowledge-grounded reasoning. This research establishes PRIME as a scalable solution for improving LLMs in domains requiring complex, knowledge-intensive reasoning.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Associative Memory via Online Convex Optimization</title>
<link>https://arxiv.org/abs/2509.22321</link>
<guid>https://arxiv.org/abs/2509.22321</guid>
<content:encoded><![CDATA[
arXiv:2509.22321v1 Announce Type: new 
Abstract: An associative memory (AM) enables cue-response recall, and associative memorization has recently been noted to underlie the operation of modern neural architectures such as Transformers. This work addresses a distributed setting where agents maintain a local AM to recall their own associations as well as selective information from others. Specifically, we introduce a distributed online gradient descent method that optimizes local AMs at different agents through communication over routing trees. Our theoretical analysis establishes sublinear regret guarantees, and experiments demonstrate that the proposed protocol consistently outperforms existing online optimization baselines.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoboView-Bias: Benchmarking Visual Bias in Embodied Agents for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2509.22356</link>
<guid>https://arxiv.org/abs/2509.22356</guid>
<content:encoded><![CDATA[
arXiv:2509.22356v1 Announce Type: new 
Abstract: The safety and reliability of embodied agents rely on accurate and unbiased visual perception. However, existing benchmarks mainly emphasize generalization and robustness under perturbations, while systematic quantification of visual bias remains scarce. This gap limits a deeper understanding of how perception influences decision-making stability. To address this issue, we propose RoboView-Bias, the first benchmark specifically designed to systematically quantify visual bias in robotic manipulation, following a principle of factor isolation. Leveraging a structured variant-generation framework and a perceptual-fairness validation protocol, we create 2,127 task instances that enable robust measurement of biases induced by individual visual factors and their interactions. Using this benchmark, we systematically evaluate three representative embodied agents across two prevailing paradigms and report three key findings: (i) all agents exhibit significant visual biases, with camera viewpoint being the most critical factor; (ii) agents achieve their highest success rates on highly saturated colors, indicating inherited visual preferences from underlying VLMs; and (iii) visual biases show strong, asymmetric coupling, with viewpoint strongly amplifying color-related bias. Finally, we demonstrate that a mitigation strategy based on a semantic grounding layer substantially reduces visual bias by approximately 54.5\% on MOKA. Our results highlight that systematic analysis of visual bias is a prerequisite for developing safe and reliable general-purpose embodied agents.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for Epistemic Competence in Information-Seeking Agents</title>
<link>https://arxiv.org/abs/2509.22391</link>
<guid>https://arxiv.org/abs/2509.22391</guid>
<content:encoded><![CDATA[
arXiv:2509.22391v1 Announce Type: new 
Abstract: Recent work has explored training Large Language Model (LLM) search agents with reinforcement learning (RL) for open-domain question answering (QA). However, most evaluations focus solely on final answer accuracy, overlooking how these agents reason with and act on external evidence. We introduce SeekBench, the first benchmark for evaluating the \textit{epistemic competence} of LLM search agents through step-level analysis of their response traces. SeekBench comprises 190 expert-annotated traces with over 1,800 response steps generated by LLM search agents, each enriched with evidence annotations for granular analysis of whether agents (1) generate reasoning steps grounded in observed evidence, (2) adaptively reformulate searches to recover from low-quality results, and (3) have proper calibration to correctly assess whether the current evidence is sufficient for providing an answer.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning-Based Collaborative Control for Bi-Manual Tactile-Reactive Grasping</title>
<link>https://arxiv.org/abs/2509.22421</link>
<guid>https://arxiv.org/abs/2509.22421</guid>
<content:encoded><![CDATA[
arXiv:2509.22421v1 Announce Type: new 
Abstract: Grasping is a core task in robotics with various applications. However, most current implementations are primarily designed for rigid items, and their performance drops considerably when handling fragile or deformable materials that require real-time feedback. Meanwhile, tactile-reactive grasping focuses on a single agent, which limits their ability to grasp and manipulate large, heavy objects. To overcome this, we propose a learning-based, tactile-reactive multi-agent Model Predictive Controller (MPC) for grasping a wide range of objects with different softness and shapes, beyond the capabilities of preexisting single-agent implementations. Our system uses two Gelsight Mini tactile sensors [1] to extract real-time information on object texture and stiffness. This rich tactile feedback is used to estimate contact dynamics and object compliance in real time, enabling the system to adapt its control policy to diverse object geometries and stiffness profiles. The learned controller operates in a closed loop, leveraging tactile encoding to predict grasp stability and adjust force and position accordingly. Our key technical contributions include a multi-agent MPC formulation trained on real contact interactions, a tactile-data driven method for inferring grasping states, and a coordination strategy that enables collaborative control. By combining tactile sensing and a learning-based multi-agent MPC, our method offers a robust, intelligent solution for collaborative grasping in complex environments, significantly advancing the capabilities of multi-agent systems. Our approach is validated through extensive experiments against independent PD and MPC baselines. Our pipeline outperforms the baselines regarding success rates in achieving and maintaining stable grasps across objects of varying sizes and stiffness.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Delayed Feedback in Games via Extra Prediction</title>
<link>https://arxiv.org/abs/2509.22426</link>
<guid>https://arxiv.org/abs/2509.22426</guid>
<content:encoded><![CDATA[
arXiv:2509.22426v1 Announce Type: new 
Abstract: This study raises and addresses the problem of time-delayed feedback in learning in games. Because learning in games assumes that multiple agents independently learn their strategies, a discrepancy in optimization often emerges among the agents. To overcome this discrepancy, the prediction of the future reward is incorporated into algorithms, typically known as Optimistic Follow-the-Regularized-Leader (OFTRL). However, the time delay in observing the past rewards hinders the prediction. Indeed, this study firstly proves that even a single-step delay worsens the performance of OFTRL from the aspects of regret and convergence. This study proposes the weighted OFTRL (WOFTRL), where the prediction vector of the next reward in OFTRL is weighted $n$ times. We further capture an intuition that the optimistic weight cancels out this time delay. We prove that when the optimistic weight exceeds the time delay, our WOFTRL recovers the good performances that the regret is constant ($O(1)$-regret) in general-sum normal-form games, and the strategies converge to the Nash equilibrium as a subsequence (best-iterate convergence) in poly-matrix zero-sum games. The theoretical results are supported and strengthened by our experiments.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TreeMind: Automatically Reproducing Android Bug Reports via LLM-empowered Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2509.22431</link>
<guid>https://arxiv.org/abs/2509.22431</guid>
<content:encoded><![CDATA[
arXiv:2509.22431v1 Announce Type: new 
Abstract: Automatically reproducing Android app crashes from textual bug reports is challenging, particularly when the reports are incomplete and the modern UI exhibits high combinatorial complexity. Existing approaches based on reinforcement learning or large language models (LLMs) exhibit limitations in such scenarios. They struggle to infer unobserved steps and reconstruct the underlying user action sequences to navigate the vast UI interaction space, primarily due to limited goal-directed reasoning and planning. We present TreeMind, a novel technique that integrates LLMs with a customized Monte Carlo Tree Search (MCTS) algorithm to achieve strategic UI exploration in bug reproduction. To the best of our knowledge, this is the first work to combine external decision-making with LLM semantic reasoning for reliable bug reproduction. We formulate the reproduction task as a target-driven search problem, leveraging MCTS as the core planning mechanism to iteratively refine action sequences. To enhance MCTS with semantic reasoning, we introduce two LLM-guided agents with distinct roles: Expander generates top-k promising actions based on the current UI state and exploration history, while Simulator estimates the likelihood that each action leads toward successful reproduction. By incorporating multi-modal UI inputs and advanced prompting techniques, TreeMind conducts feedback-aware navigation that identifies missing but essential user actions and incrementally reconstructs the reproduction paths. We evaluate TreeMind on a dataset of 93 real-world Android bug reports from three widely-used benchmarks. Experimental results show that it significantly outperforms four state-of-the-art baselines in reproduction success rate. A real-world case study indicates that integrating LLM reasoning with MCTS-based planning is a compelling direction for automated bug reproduction.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Ontology for Unified Modeling of Tasks, Actions, Environments, and Capabilities in Personal Service Robotics</title>
<link>https://arxiv.org/abs/2509.22434</link>
<guid>https://arxiv.org/abs/2509.22434</guid>
<content:encoded><![CDATA[
arXiv:2509.22434v1 Announce Type: new 
Abstract: Personal service robots are increasingly used in domestic settings to assist older adults and people requiring support. Effective operation involves not only physical interaction but also the ability to interpret dynamic environments, understand tasks, and choose appropriate actions based on context. This requires integrating both hardware components (e.g. sensors, actuators) and software systems capable of reasoning about tasks, environments, and robot capabilities. Frameworks such as the Robot Operating System (ROS) provide open-source tools that help connect low-level hardware with higher-level functionalities. However, real-world deployments remain tightly coupled to specific platforms. As a result, solutions are often isolated and hard-coded, limiting interoperability, reusability, and knowledge sharing. Ontologies and knowledge graphs offer a structured way to represent tasks, environments, and robot capabilities. Existing ontologies, such as the Socio-physical Model of Activities (SOMA) and the Descriptive Ontology for Linguistic and Cognitive Engineering (DOLCE), provide models for activities, spatial relationships, and reasoning structures. However, they often focus on specific domains and do not fully capture the connection between environment, action, robot capabilities, and system-level integration. In this work, we propose the Ontology for roBOts and acTions (OntoBOT), which extends existing ontologies to provide a unified representation of tasks, actions, environments, and capabilities. Our contributions are twofold: (1) we unify these aspects into a cohesive ontology to support formal reasoning about task execution, and (2) we demonstrate its generalizability by evaluating competency questions across four embodied agents - TIAGo, HSR, UR3, and Stretch - showing how OntoBOT enables context-aware reasoning, task-oriented execution, and knowledge sharing in service robotics.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machines in the Margins: A Systematic Review of Automated Content Generation for Wikipedia</title>
<link>https://arxiv.org/abs/2509.22443</link>
<guid>https://arxiv.org/abs/2509.22443</guid>
<content:encoded><![CDATA[
arXiv:2509.22443v1 Announce Type: new 
Abstract: Wikipedia is among the largest examples of collective intelligence on the Web with over 61 million articles covering over 320 languages. Although edited and maintained by an active workforce of human volunteers, Wikipedia is highly reliant on automated bots to fill gaps in its human workforce. As well as administrative and governance tasks, these bots also play a role in generating content, although to date such agents represent the smallest proportion of bots. While there has been considerable analysis of bots and their activity in Wikipedia, such work captures only automated agents that have been actively deployed to Wikipedia and fails to capture the methods that have been proposed to generate Wikipedia content in the wider literature. In this paper, we conduct a systematic literature review to explore how researchers have operationalised and evaluated automated content-generation agents for Wikipedia. We identify the scope of these generation methods, the techniques and models used, the source content used for generation and the evaluation methodologies which support generation processes. We also explore implications of our findings to CSCW, User Generated Content and Wikipedia, as well as research directions for future development. To the best of our knowledge, we are among the first to review the potential contributions of this understudied form of AI support for the Wikipedia community beyond the implementation of bots.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoSketch: A Neural-Symbolic Approach to Geometric Multimodal Reasoning with Auxiliary Line Construction and Affine Transformation</title>
<link>https://arxiv.org/abs/2509.22460</link>
<guid>https://arxiv.org/abs/2509.22460</guid>
<content:encoded><![CDATA[
arXiv:2509.22460v1 Announce Type: new 
Abstract: Geometric Problem Solving (GPS) poses a unique challenge for Multimodal Large Language Models (MLLMs), requiring not only the joint interpretation of text and diagrams but also iterative visuospatial reasoning. While existing approaches process diagrams as static images, they lack the capacity for dynamic manipulation - a core aspect of human geometric reasoning involving auxiliary line construction and affine transformations. We present GeoSketch, a neural-symbolic framework that recasts geometric reasoning as an interactive perception-reasoning-action loop. GeoSketch integrates: (1) a Perception module that abstracts diagrams into structured logic forms, (2) a Symbolic Reasoning module that applies geometric theorems to decide the next deductive step, and (3) a Sketch Action module that executes operations such as drawing auxiliary lines or applying transformations, thereby updating the diagram in a closed loop. To train this agent, we develop a two-stage pipeline: supervised fine-tuning on 2,000 symbolic-curated trajectories followed by reinforcement learning with dense, symbolic rewards to enhance robustness and strategic exploration. To evaluate this paradigm, we introduce the GeoSketch Benchmark, a high-quality set of 390 geometry problems requiring auxiliary construction or affine transformations. Experiments on strong MLLM baselines demonstrate that GeoSketch significantly improves stepwise reasoning accuracy and problem-solving success over static perception methods. By unifying hierarchical decision-making, executable visual actions, and symbolic verification, GeoSketch advances multimodal reasoning from static interpretation to dynamic, verifiable interaction, establishing a new foundation for solving complex visuospatial problems.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDAR: A Multi-scene Dynamic Audio Reasoning Benchmark</title>
<link>https://arxiv.org/abs/2509.22461</link>
<guid>https://arxiv.org/abs/2509.22461</guid>
<content:encoded><![CDATA[
arXiv:2509.22461v1 Announce Type: new 
Abstract: The ability to reason from audio, including speech, paralinguistic cues, environmental sounds, and music, is essential for AI agents to interact effectively in real-world scenarios. Existing benchmarks mainly focus on static or single-scene settings and do not fully capture scenarios where multiple speakers, unfolding events, and heterogeneous audio sources interact. To address these challenges, we introduce MDAR, a benchmark for evaluating models on complex, multi-scene, and dynamically evolving audio reasoning tasks. MDAR comprises 3,000 carefully curated question-answer pairs linked to diverse audio clips, covering five categories of complex reasoning and spanning three question types. We benchmark 26 state-of-the-art audio language models on MDAR and observe that they exhibit limitations in complex reasoning tasks. On single-choice questions, Qwen2.5-Omni (open-source) achieves 76.67% accuracy, whereas GPT-4o Audio (closed-source) reaches 68.47%; however, GPT-4o Audio substantially outperforms Qwen2.5-Omni on the more challenging multiple-choice and open-ended tasks. Across all three question types, no model achieves 80% performance. These findings underscore the unique challenges posed by MDAR and its value as a benchmark for advancing audio reasoning research.Code and benchmark can be found at https://github.com/luckyerr/MDAR.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeLLCom-Lex: A Neural-agent Framework to Study the Interplay between Lexical Systems and Language Use</title>
<link>https://arxiv.org/abs/2509.22479</link>
<guid>https://arxiv.org/abs/2509.22479</guid>
<content:encoded><![CDATA[
arXiv:2509.22479v1 Announce Type: new 
Abstract: Lexical semantic change has primarily been investigated with observational and experimental methods; however, observational methods (corpus analysis, distributional semantic modeling) cannot get at causal mechanisms, and experimental paradigms with humans are hard to apply to semantic change due to the extended diachronic processes involved. This work introduces NeLLCom-Lex, a neural-agent framework designed to simulate semantic change by first grounding agents in a real lexical system (e.g. English) and then systematically manipulating their communicative needs. Using a well-established color naming task, we simulate the evolution of a lexical system within a single generation, and study which factors lead agents to: (i) develop human-like naming behavior and lexicons, and (ii) change their behavior and lexicons according to their communicative needs. Our experiments with different supervised and reinforcement learning pipelines show that neural agents trained to 'speak' an existing language can reproduce human-like patterns in color naming to a remarkable extent, supporting the further use of NeLLCom-Lex to elucidate the mechanisms of semantic change.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ontological foundations for contrastive explanatory narration of robot plans</title>
<link>https://arxiv.org/abs/2509.22493</link>
<guid>https://arxiv.org/abs/2509.22493</guid>
<content:encoded><![CDATA[
arXiv:2509.22493v1 Announce Type: new 
Abstract: Mutual understanding of artificial agents' decisions is key to ensuring a trustworthy and successful human-robot interaction. Hence, robots are expected to make reasonable decisions and communicate them to humans when needed. In this article, the focus is on an approach to modeling and reasoning about the comparison of two competing plans, so that robots can later explain the divergent result. First, a novel ontological model is proposed to formalize and reason about the differences between competing plans, enabling the classification of the most appropriate one (e.g., the shortest, the safest, the closest to human preferences, etc.). This work also investigates the limitations of a baseline algorithm for ontology-based explanatory narration. To address these limitations, a novel algorithm is presented, leveraging divergent knowledge between plans and facilitating the construction of contrastive narratives. Through empirical evaluation, it is observed that the explanations excel beyond the baseline method.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfiAgent: Self-Evolving Pyramid Agent Framework for Infinite Scenarios</title>
<link>https://arxiv.org/abs/2509.22502</link>
<guid>https://arxiv.org/abs/2509.22502</guid>
<content:encoded><![CDATA[
arXiv:2509.22502v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents have demonstrated remarkable capabilities in organizing and executing complex tasks, and many such agents are now widely used in various application scenarios. However, developing these agents requires carefully designed workflows, carefully crafted prompts, and iterative tuning, which requires LLM techniques and domain-specific expertise. These hand-crafted limitations hinder the scalability and cost-effectiveness of LLM agents across a wide range of industries. To address these challenges, we propose \textbf{InfiAgent}, a Pyramid-like DAG-based Multi-Agent Framework that can be applied to \textbf{infi}nite scenarios, which introduces several key innovations: a generalized "agent-as-a-tool" mechanism that automatically decomposes complex agents into hierarchical multi-agent systems; a dual-audit mechanism that ensures the quality and stability of task completion; an agent routing function that enables efficient task-agent matching; and an agent self-evolution mechanism that autonomously restructures the agent DAG based on new tasks, poor performance, or optimization opportunities. Furthermore, InfiAgent's atomic task design supports agent parallelism, significantly improving execution efficiency. This framework evolves into a versatile pyramid-like multi-agent system capable of solving a wide range of problems. Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\% higher performance compared to ADAS (similar auto-generated agent framework), while a case study of the AI research assistant InfiHelper shows that it generates scientific papers that have received recognition from human reviewers at top-tier IEEE conferences.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating the Empowerment of Language Model Agents</title>
<link>https://arxiv.org/abs/2509.22504</link>
<guid>https://arxiv.org/abs/2509.22504</guid>
<content:encoded><![CDATA[
arXiv:2509.22504v1 Announce Type: new 
Abstract: As language model (LM) agents become more capable and gain broader access to real-world tools, there is a growing need for scalable evaluation frameworks of agentic capability. However, conventional benchmark-centric evaluations are costly to design and require human designers to come up with valid tasks that translate into insights about general model capabilities. In this work, we propose information-theoretic evaluation based on empowerment, the mutual information between an agent's actions and future states, as an open-ended method for evaluating LM agents. We introduce EELMA (Estimating Empowerment of Language Model Agents), an algorithm for approximating effective empowerment from multi-turn text interactions. We validate EELMA on both language games and scaled-up realistic web-browsing scenarios. We find that empowerment strongly correlates with average task performance, characterize the impact of environmental complexity and agentic factors such as chain-of-thought, model scale, and memory length on estimated empowerment, and that high empowerment states and actions are often pivotal moments for general capabilities. Together, these results demonstrate empowerment as an appealing general-purpose metric for evaluating and monitoring LM agents in complex, open-ended settings.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JointDiff: Bridging Continuous and Discrete in Multi-Agent Trajectory Generation</title>
<link>https://arxiv.org/abs/2509.22522</link>
<guid>https://arxiv.org/abs/2509.22522</guid>
<content:encoded><![CDATA[
arXiv:2509.22522v1 Announce Type: new 
Abstract: Generative models often treat continuous data and discrete events as separate processes, creating a gap in modeling complex systems where they interact synchronously. To bridge this gap, we introduce JointDiff, a novel diffusion framework designed to unify these two processes by simultaneously generating continuous spatio-temporal data and synchronous discrete events. We demonstrate its efficacy in the sports domain by simultaneously modeling multi-agent trajectories and key possession events. This joint modeling is validated with non-controllable generation and two novel controllable generation scenarios: weak-possessor-guidance, which offers flexible semantic control over game dynamics through a simple list of intended ball possessors, and text-guidance, which enables fine-grained, language-driven generation. To enable the conditioning with these guidance signals, we introduce CrossGuid, an effective conditioning operation for multi-agent domains. We also share a new unified sports benchmark enhanced with textual descriptions for soccer and football datasets. JointDiff achieves state-of-the-art performance, demonstrating that joint modeling is crucial for building realistic and controllable generative models for interactive systems.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Specifying an Obligation Taxonomy in the Non-Markovian Situation Calculus</title>
<link>https://arxiv.org/abs/2509.22533</link>
<guid>https://arxiv.org/abs/2509.22533</guid>
<content:encoded><![CDATA[
arXiv:2509.22533v1 Announce Type: new 
Abstract: Over more than three decades, the Situation Calculus has established itself as an elegant, powerful, and concise formalism for specifying dynamical domains as well as for reasoning about the effects of actions of those domains both in the world and in the mental state of the modelled agents. Moreover, it has also been established that the preconditions of a given action and its effects may be determined entirely by the current situation alone, or they may be determined by past situations as well. When past situations are involved in determining action preconditions and effects, resulting theories are non-Markovian. Assuming a specification of actions that produce obligations, we consider using non-Markovian control in the Situation Calculus to specify different notions of obligations found in the literature. These notions have been specified using Event Calculus; but, as far as we know, they have never been specified using the Situation Calculus. The specifications in this paper yield intuitive properties that ensure the correctness of the whole endeavour.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Emergence of Altruism in Large-Language-Model Agents Society</title>
<link>https://arxiv.org/abs/2509.22537</link>
<guid>https://arxiv.org/abs/2509.22537</guid>
<content:encoded><![CDATA[
arXiv:2509.22537v1 Announce Type: new 
Abstract: Leveraging Large Language Models (LLMs) for social simulation is a frontier in computational social science. Understanding the social logics these agents embody is critical to this attempt. However, existing research has primarily focused on cooperation in small-scale, task-oriented games, overlooking how altruism, which means sacrificing self-interest for collective benefit, emerges in large-scale agent societies. To address this gap, we introduce a Schelling-variant urban migration model that creates a social dilemma, compelling over 200 LLM agents to navigate an explicit conflict between egoistic (personal utility) and altruistic (system utility) goals. Our central finding is a fundamental difference in the social tendencies of LLMs. We identify two distinct archetypes: "Adaptive Egoists", which default to prioritizing self-interest but whose altruistic behaviors significantly increase under the influence of a social norm-setting message board; and "Altruistic Optimizers", which exhibit an inherent altruistic logic, consistently prioritizing collective benefit even at a direct cost to themselves. Furthermore, to qualitatively analyze the cognitive underpinnings of these decisions, we introduce a method inspired by Grounded Theory to systematically code agent reasoning. In summary, this research provides the first evidence of intrinsic heterogeneity in the egoistic and altruistic tendencies of different LLMs. We propose that for social simulation, model selection is not merely a matter of choosing reasoning capability, but of choosing an intrinsic social action logic. While "Adaptive Egoists" may offer a more suitable choice for simulating complex human societies, "Altruistic Optimizers" are better suited for modeling idealized pro-social actors or scenarios where collective welfare is the primary consideration.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation</title>
<link>https://arxiv.org/abs/2509.22548</link>
<guid>https://arxiv.org/abs/2509.22548</guid>
<content:encoded><![CDATA[
arXiv:2509.22548v1 Announce Type: new 
Abstract: Vision-and-Language Navigation requires an embodied agent to navigate through unseen environments, guided by natural language instructions and a continuous video stream. Recent advances in VLN have been driven by the powerful semantic understanding of Multimodal Large Language Models. However, these methods typically rely on explicit semantic memory, such as building textual cognitive maps or storing historical visual frames. This type of method suffers from spatial information loss, computational redundancy, and memory bloat, which impede efficient navigation. Inspired by the implicit scene representation in human navigation, analogous to the left brain's semantic understanding and the right brain's spatial cognition, we propose JanusVLN, a novel VLN framework featuring a dual implicit neural memory that models spatial-geometric and visual-semantic memory as separate, compact, and fixed-size neural representations. This framework first extends the MLLM to incorporate 3D prior knowledge from the spatial-geometric encoder, thereby enhancing the spatial reasoning capabilities of models based solely on RGB input. Then, the historical key-value caches from the spatial-geometric and visual-semantic encoders are constructed into a dual implicit memory. By retaining only the KVs of tokens in the initial and sliding window, redundant computation is avoided, enabling efficient incremental updates. Extensive experiments demonstrate that JanusVLN outperforms over 20 recent methods to achieve SOTA performance. For example, the success rate improves by 10.5-35.5 compared to methods using multiple data types as input and by 3.6-10.8 compared to methods using more RGB training data. This indicates that the proposed dual implicit neural memory, as a novel paradigm, explores promising new directions for future VLN research. Ours project page: https://miv-xjtu.github.io/JanusVLN.github.io/.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models</title>
<link>https://arxiv.org/abs/2509.22558</link>
<guid>https://arxiv.org/abs/2509.22558</guid>
<content:encoded><![CDATA[
arXiv:2509.22558v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown promising capabilities for solving Operations Research (OR) problems. While reinforcement learning serves as a powerful paradigm for LLM training on OR problems, existing works generally face two key limitations. First, outcome reward suffers from the credit assignment problem, where correct final answers can reinforce flawed reasoning. Second, conventional discriminative process supervision is myopic, failing to evaluate the interdependent steps of OR modeling holistically. To this end, we introduce StepORLM, a novel self-evolving framework with generative process supervision. At its core, StepORLM features a co-evolutionary loop where a policy model and a generative process reward model (GenPRM) iteratively improve on each other. This loop is driven by a dual-feedback mechanism: definitive, outcome-based verification from an external solver, and nuanced, holistic process evaluation from the GenPRM. The combined signal is used to align the policy via Weighted Direct Preference Optimization (W-DPO) and simultaneously refine the GenPRM. Our resulting 8B-parameter StepORLM establishes a new state-of-the-art across six benchmarks, significantly outperforming vastly larger generalist models, agentic methods, and specialized baselines. Moreover, the co-evolved GenPRM is able to act as a powerful and universally applicable process verifier, substantially boosting the inference scaling performance of both our own model and other existing LLMs.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nearly Tight Regret Bounds for Profit Maximization in Bilateral Trade</title>
<link>https://arxiv.org/abs/2509.22563</link>
<guid>https://arxiv.org/abs/2509.22563</guid>
<content:encoded><![CDATA[
arXiv:2509.22563v1 Announce Type: new 
Abstract: Bilateral trade models the task of intermediating between two strategic agents, a seller and a buyer, willing to trade a good for which they hold private valuations. We study this problem from the perspective of a broker, in a regret minimization framework. At each time step, a new seller and buyer arrive, and the broker has to propose a mechanism that is incentive-compatible and individually rational, with the goal of maximizing profit.
  We propose a learning algorithm that guarantees a nearly tight $\tilde{O}(\sqrt{T})$ regret in the stochastic setting when seller and buyer valuations are drawn i.i.d. from a fixed and possibly correlated unknown distribution. We further show that it is impossible to achieve sublinear regret in the non-stationary scenario where valuations are generated upfront by an adversary. Our ambitious benchmark for these results is the best incentive-compatible and individually rational mechanism. This separates us from previous works on efficiency maximization in bilateral trade, where the benchmark is a single number: the best fixed price in hindsight.
  A particular challenge we face is that uniform convergence for all mechanisms' profits is impossible. We overcome this difficulty via a careful chaining analysis that proves convergence for a provably near-optimal mechanism at (essentially) optimal rate. We further showcase the broader applicability of our techniques by providing nearly optimal results for the joint ads problem.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniMIC: Token-Based Multimodal Interactive Coding for Human-AI Collaboration</title>
<link>https://arxiv.org/abs/2509.22570</link>
<guid>https://arxiv.org/abs/2509.22570</guid>
<content:encoded><![CDATA[
arXiv:2509.22570v1 Announce Type: new 
Abstract: The rapid progress of Large Multimodal Models (LMMs) and cloud-based AI agents is transforming human-AI collaboration into bidirectional, multimodal interaction. However, existing codecs remain optimized for unimodal, one-way communication, resulting in repeated degradation under conventional compress-transmit-reconstruct pipelines. To address this limitation, we propose UniMIC, a Unified token-based Multimodal Interactive Coding framework that bridges edge devices and cloud AI agents. Instead of transmitting raw pixels or plain text, UniMIC employs compact tokenized representations as the communication medium, enabling efficient low-bitrate transmission while maintaining compatibility with LMMs. To further enhance compression, lightweight Transformer-based entropy models with scenario-specific designs-generic, masked, and text-conditioned-effectively minimize inter-token redundancy. Extensive experiments on text-to-image generation, text-guided inpainting, outpainting, and visual question answering show that UniMIC achieves substantial bitrate savings and remains robust even at ultra-low bitrates (<0.05bpp), without compromising downstream task performance. These results establish UniMIC as a practical and forward-looking paradigm for next-generation multimodal interactive communication.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.22576</link>
<guid>https://arxiv.org/abs/2509.22576</guid>
<content:encoded><![CDATA[
arXiv:2509.22576v1 Announce Type: new 
Abstract: Training LLM agents in multi-turn environments with sparse rewards, where completing a single task requires 30+ turns of interaction within an episode, presents a fundamental challenge for reinforcement learning. We identify a critical failure mode unique to this setting: the exploration-exploitation cascade failure. This cascade begins with early-stage policy premature convergence, where sparse feedback causes agents to commit to flawed, low-entropy strategies. Subsequently, agents enter late-stage policy collapse, where conventional entropy regularization becomes counterproductive, promoting chaotic exploration that destabilizes training. We propose Entropy-regularized Policy Optimization (EPO), a general framework that breaks this failure cycle through three synergistic mechanisms: (1) adopting entropy regularization in multi-turn settings to enhance exploration, (2) an entropy smoothing regularizer that bounds policy entropy within historical averages to prevent abrupt fluctuations, and (3) adaptive phase-based weighting that balances exploration and exploitation across training. Our analysis justifies that EPO guarantees monotonically decreasing entropy variance while maintaining convergence. EPO achieves up to 152% performance improvement on ScienceWorld and up to 19.8% on ALFWorld. Our work demonstrates that multi-turn sparse-reward settings require fundamentally different entropy control than traditional RL, with broad implications for LLM agent training.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective Policy Learning for Multi-Agent Online Coordination Beyond Submodular Objectives</title>
<link>https://arxiv.org/abs/2509.22596</link>
<guid>https://arxiv.org/abs/2509.22596</guid>
<content:encoded><![CDATA[
arXiv:2509.22596v1 Announce Type: new 
Abstract: In this paper, we present two effective policy learning algorithms for multi-agent online coordination(MA-OC) problem. The first one, \texttt{MA-SPL}, not only can achieve the optimal $(1-\frac{c}{e})$-approximation guarantee for the MA-OC problem with submodular objectives but also can handle the unexplored $\alpha$-weakly DR-submodular and $(\gamma,\beta)$-weakly submodular scenarios, where $c$ is the curvature of the investigated submodular functions, $\alpha$ denotes the diminishing-return(DR) ratio and the tuple $(\gamma,\beta)$ represents the submodularity ratios. Subsequently, in order to reduce the reliance on the unknown parameters $\alpha,\gamma,\beta$ inherent in the \texttt{MA-SPL} algorithm, we further introduce the second online algorithm named \texttt{MA-MPL}. This \texttt{MA-MPL} algorithm is entirely \emph{parameter-free} and simultaneously can maintain the same approximation ratio as the first \texttt{MA-SPL} algorithm. The core of our \texttt{MA-SPL} and \texttt{MA-MPL} algorithms is a novel continuous-relaxation technique termed as \emph{policy-based continuous extension}. Compared with the well-established \emph{multi-linear extension}, a notable advantage of this new \emph{policy-based continuous extension} is its ability to provide a lossless rounding scheme for any set function, thereby enabling us to tackle the challenging weakly submodular objectives. Finally, extensive simulations are conducted to validate the effectiveness of our proposed algorithms.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.22601</link>
<guid>https://arxiv.org/abs/2509.22601</guid>
<content:encoded><![CDATA[
arXiv:2509.22601v1 Announce Type: new 
Abstract: Reinforcement learning (RL) is the dominant paradigm for sharpening strategic tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks, yet it faces a fundamental challenge of exploration-exploitation trade-off. Existing studies stimulate exploration through the lens of policy entropy, but such mechanical entropy maximization is prone to RL training instability due to the multi-turn distribution shifting. In this paper, we target the progressive exploration-exploitation balance under the guidance of the agent own experiences without succumbing to either entropy collapsing or runaway divergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL) recipe for training agentic LLMs. It extends the vanilla SIL framework, where a replay buffer stores self-generated promising trajectories for off-policy update, by gradually steering the policy evolution within a well-balanced range of entropy across stages. Specifically, our approach incorporates a curriculum to manage the exploration process, utilizing intrinsic rewards to foster skill-level exploration and facilitating action-level exploration through SIL. At first, the auxiliary tool call reward plays a critical role in the accumulation of tool-use skills, enabling broad exposure to the unfamiliar distributions of the environment feedback with an upward entropy trend. As training progresses, self-imitation gets strengthened to exploit existing successful patterns from replayed experiences for comparative action-level exploration, accelerating solution iteration without unbounded entropy growth. To further stabilize training, we recalibrate the advantages of experiences in the replay buffer to address the potential policy drift. Reugularizations such as the clipping of tokens with high covariance between probability and advantage are introduced to the trajectory-level entropy control to curb over-confidence.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LABELING COPILOT: A Deep Research Agent for Automated Data Curation in Computer Vision</title>
<link>https://arxiv.org/abs/2509.22631</link>
<guid>https://arxiv.org/abs/2509.22631</guid>
<content:encoded><![CDATA[
arXiv:2509.22631v1 Announce Type: new 
Abstract: Curating high-quality, domain-specific datasets is a major bottleneck for deploying robust vision systems, requiring complex trade-offs between data quality, diversity, and cost when researching vast, unlabeled data lakes. We introduce Labeling Copilot, the first data curation deep research agent for computer vision. A central orchestrator agent, powered by a large multimodal language model, uses multi-step reasoning to execute specialized tools across three core capabilities: (1) Calibrated Discovery sources relevant, in-distribution data from large repositories; (2) Controllable Synthesis generates novel data for rare scenarios with robust filtering; and (3) Consensus Annotation produces accurate labels by orchestrating multiple foundation models via a novel consensus mechanism incorporating non-maximum suppression and voting. Our large-scale validation proves the effectiveness of Labeling Copilot's components. The Consensus Annotation module excels at object discovery: on the dense COCO dataset, it averages 14.2 candidate proposals per image-nearly double the 7.4 ground-truth objects-achieving a final annotation mAP of 37.1%. On the web-scale Open Images dataset, it navigated extreme class imbalance to discover 903 new bounding box categories, expanding its capability to over 1500 total. Concurrently, our Calibrated Discovery tool, tested at a 10-million sample scale, features an active learning strategy that is up to 40x more computationally efficient than alternatives with equivalent sample efficiency. These experiments validate that an agentic workflow with optimized, scalable tools provides a robust foundation for curating industrial-scale datasets.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WoW: Towards a World omniscient World model Through Embodied Interaction</title>
<link>https://arxiv.org/abs/2509.22642</link>
<guid>https://arxiv.org/abs/2509.22642</guid>
<content:encoded><![CDATA[
arXiv:2509.22642v1 Announce Type: new 
Abstract: Humans develop an understanding of intuitive physics through active interaction with the world. This approach is in stark contrast to current video models, such as Sora, which rely on passive observation and therefore struggle with grasping physical causality. This observation leads to our central hypothesis: authentic physical intuition of the world model must be grounded in extensive, causally rich interactions with the real world. To test this hypothesis, we present WoW, a 14-billion-parameter generative world model trained on 2 million robot interaction trajectories. Our findings reveal that the model's understanding of physics is a probabilistic distribution of plausible outcomes, leading to stochastic instabilities and physical hallucinations. Furthermore, we demonstrate that this emergent capability can be actively constrained toward physical realism by SOPHIA, where vision-language model agents evaluate the DiT-generated output and guide its refinement by iteratively evolving the language instructions. In addition, a co-trained Inverse Dynamics Model translates these refined plans into executable robotic actions, thus closing the imagination-to-action loop. We establish WoWBench, a new benchmark focused on physical consistency and causal reasoning in video, where WoW achieves state-of-the-art performance in both human and autonomous evaluation, demonstrating strong ability in physical causality, collision dynamics, and object permanence. Our work provides systematic evidence that large-scale, real-world interaction is a cornerstone for developing physical intuition in AI. Models, data, and benchmarks will be open-sourced.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.22644</link>
<guid>https://arxiv.org/abs/2509.22644</guid>
<content:encoded><![CDATA[
arXiv:2509.22644v1 Announce Type: new 
Abstract: Agent systems powered by large language models (LLMs) have demonstrated impressive performance on repository-level code-generation tasks. However, for tasks such as website codebase generation, which depend heavily on visual effects and user-interaction feedback, current code agents rely only on simple code execution for feedback and verification. This approach fails to capture the actual quality of the generated code. In this paper, we propose WebGen-Agent, a novel website-generation agent that leverages comprehensive and multi-level visual feedback to iteratively generate and refine the website codebase. Detailed and expressive text descriptions and suggestions regarding the screenshots and GUI-agent testing of the websites are generated by a visual language model (VLM), together with scores that quantify their quality. The screenshot and GUI-agent scores are further integrated with a backtracking and select-best mechanism, enhancing the performance of the agent. Utilizing the accurate visual scores inherent in the WebGen-Agent workflow, we further introduce \textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve the ability of LLMs to act as the reasoning engine of WebGen-Agent. By using the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we provide a dense and reliable process supervision signal, which effectively improves the model's website-generation ability. On the WebGen-Bench dataset, WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9% and its appearance score from 3.0 to 3.9, outperforming the previous state-of-the-art agent system. Additionally, our Step-GRPO training approach increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and raises the appearance score from 3.4 to 3.7.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Machine Learning Pipeline for Training and Analysis Using Large Language Models</title>
<link>https://arxiv.org/abs/2509.21647</link>
<guid>https://arxiv.org/abs/2509.21647</guid>
<content:encoded><![CDATA[
arXiv:2509.21647v1 Announce Type: cross 
Abstract: Machine learning interatomic potentials (MLIPs) have become powerful tools to extend molecular simulations beyond the limits of quantum methods, offering near-quantum accuracy at much lower computational cost. Yet, developing reliable MLIPs remains difficult because it requires generating high-quality datasets, preprocessing atomic structures, and carefully training and validating models. In this work, we introduce an Automated Machine Learning Pipeline (AMLP) that unifies the entire workflow from dataset creation to model validation. AMLP employs large-language-model agents to assist with electronic-structure code selection, input preparation, and output conversion, while its analysis suite (AMLP-Analysis), based on ASE supports a range of molecular simulations. The pipeline is built on the MACE architecture and validated on acridine polymorphs, where, with a straightforward fine-tuning of a foundation model, mean absolute errors of ~1.7 meV/atom in energies and ~7.0 meV/{\AA} in forces are achieved. The fitted MLIP reproduces DFT geometries with sub-{\AA} accuracy and demonstrates stability during molecular dynamics simulations in the microcanonical and canonical ensembles.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VDFD: Multi-Agent Value Decomposition Framework with Disentangled World Model</title>
<link>https://arxiv.org/abs/2309.04615</link>
<guid>https://arxiv.org/abs/2309.04615</guid>
<content:encoded><![CDATA[
arXiv:2309.04615v2 Announce Type: replace 
Abstract: In this paper, we propose a novel model-based multi-agent reinforcement learning approach named Value Decomposition Framework with Disentangled World Model to address the challenge of achieving a common goal of multiple agents interacting in the same environment with reduced sample complexity. Due to scalability and non-stationarity problems posed by multi-agent systems, model-free methods rely on a considerable number of samples for training. In contrast, we use a modularized world model, composed of action-conditioned, action-free, and static branches, to unravel the complicated environment dynamics. Our model produces imagined outcomes based on past experience, without sampling directly from the real environment. We employ variational auto-encoders and variational graph auto-encoders to learn the latent representations for the world model, which is merged with a value-based framework to predict the joint action-value function and optimize the overall training objective. Experimental results on StarCraft II micro-management, Multi-Agent MuJoCo, and Level-Based Foraging challenges demonstrate that our method achieves high sample efficiency and exhibits superior performance compared to other baselines across a wide range of multi-agent learning tasks.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constituency Parsing using LLMs</title>
<link>https://arxiv.org/abs/2310.19462</link>
<guid>https://arxiv.org/abs/2310.19462</guid>
<content:encoded><![CDATA[
arXiv:2310.19462v3 Announce Type: replace 
Abstract: Constituency parsing is a fundamental yet unsolved challenge in natural language processing. In this paper, we examine the potential of recent large language models (LLMs) to address this challenge. We reformat constituency parsing as a sequence-to-sequence generation problem and evaluate the performance of a diverse range of LLMs under zero-shot, few-shot, and supervised fine-tuning learning paradigms. We observe that while LLMs achieve acceptable improvements, they still encounter substantial limitations, due to the absence of mechanisms to guarantee the validity and faithfulness of the generated constituent trees. Motivated by this observation, we propose two strategies to guide LLMs to generate more accurate constituent trees by learning from erroneous samples and refining outputs in a multi-agent collaboration way, respectively. The experimental results demonstrate that our methods effectively reduce the occurrence of invalid and unfaithful trees, thereby enhancing overall parsing performance and achieving promising results across different learning paradigms.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reducing Leximin Fairness to Utilitarian Optimization</title>
<link>https://arxiv.org/abs/2409.10395</link>
<guid>https://arxiv.org/abs/2409.10395</guid>
<content:encoded><![CDATA[
arXiv:2409.10395v4 Announce Type: replace 
Abstract: Two prominent objectives in social choice are utilitarian - maximizing the sum of agents' utilities, and leximin - maximizing the smallest agent's utility, then the second-smallest, etc. Utilitarianism is typically computationally easier to attain but is generally viewed as less fair. This paper presents a general reduction scheme that, given a utilitarian solver, produces a distribution over states (deterministic outcomes) that is leximin in expectation. Importantly, the scheme is robust in the sense that, given an approximate utilitarian solver, it produces a lottery that is approximately-leximin (in expectation) - with the same approximation factor. We apply our scheme to several social choice problems: stochastic allocations of indivisible goods, giveaway lotteries, and fair lotteries for participatory budgeting.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphSCENE: On-Demand Critical Scenario Generation for Autonomous Vehicles in Simulation</title>
<link>https://arxiv.org/abs/2410.13514</link>
<guid>https://arxiv.org/abs/2410.13514</guid>
<content:encoded><![CDATA[
arXiv:2410.13514v3 Announce Type: replace 
Abstract: Testing and validating Autonomous Vehicle (AV) performance in safety-critical and diverse scenarios is crucial before real-world deployment. However, manually creating such scenarios in simulation remains a significant and time-consuming challenge. This work introduces a novel method that generates dynamic temporal scene graphs corresponding to diverse traffic scenarios, on-demand, tailored to user-defined preferences, such as AV actions, sets of dynamic agents, and criticality levels. A temporal Graph Neural Network (GNN) model learns to predict relationships between ego-vehicle, agents, and static structures, guided by real-world spatiotemporal interaction patterns and constrained by an ontology that restricts predictions to semantically valid links. Our model consistently outperforms the baselines in accurately generating links corresponding to the requested scenarios. We render the predicted scenarios in simulation to further demonstrate their effectiveness as testing environments for AV agents.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capacity-Aware Planning and Scheduling in Budget-Constrained Multi-Agent MDPs: A Meta-RL Approach</title>
<link>https://arxiv.org/abs/2410.21249</link>
<guid>https://arxiv.org/abs/2410.21249</guid>
<content:encoded><![CDATA[
arXiv:2410.21249v2 Announce Type: replace 
Abstract: We study capacity- and budget-constrained multi-agent MDPs (CB-MA-MDPs), a class that captures many maintenance and scheduling tasks in which each agent can irreversibly fail and a planner must decide (i) when to apply a restorative action and (ii) which subset of agents to treat in parallel. The global budget limits the total number of restorations, while the capacity constraint bounds the number of simultaneous actions, turning na\"ive dynamic programming into a combinatorial search that scales exponentially with the number of agents. We propose a two-stage solution that remains tractable for large systems. First, a Linear Sum Assignment Problem (LSAP)-based grouping partitions the agents into r disjoint sets (r = capacity) that maximise diversity in expected time-to-failure, allocating budget to each set proportionally. Second, a meta-trained PPO policy solves each sub-MDP, leveraging transfer across groups to converge rapidly. To validate our approach, we apply it to the problem of scheduling repairs for a large team of industrial robots, constrained by a limited number of repair technicians and a total repair budget. Our results demonstrate that the proposed method outperforms baseline approaches in terms of maximizing the average uptime of the robot team, particularly for large team sizes. Lastly, we confirm the scalability of our approach through a computational complexity analysis across varying numbers of robots and repair technicians.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Strategic Agents Respond: Comparing Analytical Models with LLM-Generated Responses in Strategic Classification</title>
<link>https://arxiv.org/abs/2501.16355</link>
<guid>https://arxiv.org/abs/2501.16355</guid>
<content:encoded><![CDATA[
arXiv:2501.16355v2 Announce Type: replace 
Abstract: When ML algorithms are deployed to automate human-related decisions, human agents may learn the underlying decision policies and adapt their behavior. Strategic Classification (SC) has emerged as a framework for studying this interaction between agents and decision-makers to design more trustworthy ML systems. Prior theoretical models in SC assume that agents are perfectly or approximately rational and respond to decision policies by optimizing their utility. However, the growing prevalence of LLMs raises the possibility that real-world agents may instead rely on these tools for strategic advice. This shift prompts two questions: (i) Can LLMs generate effective and socially responsible strategies in SC settings? (ii) Can existing SC theoretical models accurately capture agent behavior when agents follow LLM-generated advice? To investigate these questions, we examine five critical SC scenarios: hiring, loan applications, school admissions, personal income, and public assistance programs. We simulate agents with diverse profiles who interact with three commercial LLMs (GPT-4o, GPT-4.1, and GPT-5), following their suggestions on effort allocations on features. We compare the resulting agent behaviors with the best responses in existing SC models. Our findings show that: (i) Even without access to the decision policy, LLMs can generate effective strategies that improve both agents' scores and qualification; (ii) At the population level, LLM-guided effort allocation strategies yield similar or even higher score improvements, qualification rates, and fairness metrics as those predicted by the SC theoretical model, suggesting that the theoretical model may still serve as a reasonable proxy for LLM-influenced behavior; and (iii) At the individual level, LLMs tend to produce more diverse and balanced effort allocations than theoretical models.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generator-Assistant Stepwise Rollback Framework for Large Language Model Agent</title>
<link>https://arxiv.org/abs/2503.02519</link>
<guid>https://arxiv.org/abs/2503.02519</guid>
<content:encoded><![CDATA[
arXiv:2503.02519v4 Announce Type: replace 
Abstract: Large language model (LLM) agents typically adopt a step-by-step reasoning framework, in which they interleave the processes of thinking and acting to accomplish the given task. However, this paradigm faces a deep-rooted one-pass issue whereby each generated intermediate thought is plugged into the trajectory regardless of its correctness, which can cause irreversible error propagation. To address the issue, this paper proposes a novel framework called Generator-Assistant Stepwise Rollback (GA-Rollback) to induce better decision-making for LLM agents. Particularly, GA-Rollback utilizes a generator to interact with the environment and an assistant to examine each action produced by the generator, where the assistant triggers a rollback operation upon detection of incorrect actions. Moreover, we introduce two additional strategies tailored for the rollback scenario to further improve its effectiveness. Extensive experiments show that GA-Rollback achieves significant improvements over several strong baselines on three widely used benchmarks. Our analysis further reveals that GA-Rollback can function as a robust plug-and-play module, integrating seamlessly with other methods.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.00907</link>
<guid>https://arxiv.org/abs/2504.00907</guid>
<content:encoded><![CDATA[
arXiv:2504.00907v3 Announce Type: replace 
Abstract: Embodied agents operating in household environments must interpret ambiguous and under-specified human instructions. A capable household robot should recognize ambiguity and ask relevant clarification questions to infer the user intent accurately, leading to more effective task execution. To study this problem, we introduce the Ask-to-Act task, where an embodied agent is tasked with a single or multi-object rearrangement task using an under-specified instruction in a home environment. The agent must strategically ask minimal, yet relevant, clarification questions to resolve ambiguity while navigating under partial observability. To address this challenge, we propose a novel approach that fine-tunes multi-modal large language models (MLLMs) as vision-language-action (VLA) policies using online reinforcement learning (RL) with LLM-generated rewards. Our method eliminates the need for large-scale human demonstrations or manually engineered rewards for training such agents. We benchmark against strong zero-shot baselines including GPT-4o as well as supervised fine-tuned MLLMs on our task. Our results show that our RL-finetuned MLLM outperforms all baselines by a significant margin (10.4-16.5%), generalizing well to novel scenes and tasks. To the best of our knowledge, this is the first demonstration of adapting MLLMs as VLA agents that can act and ask for help using LLM-generated rewards with online RL.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Code Language Models Learn Clarification-Seeking Behaviors?</title>
<link>https://arxiv.org/abs/2504.16331</link>
<guid>https://arxiv.org/abs/2504.16331</guid>
<content:encoded><![CDATA[
arXiv:2504.16331v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, a gap remains between their output and the problem-solving strategies of human developers. Unlike humans, who spend substantial time disambiguating requirements through iterative dialogue, LLMs often generate code despite ambiguities in natural language requirements, leading to unreliable solutions. Different from prior work, we study whether a Code LLM can be fine-tuned to learn clarification-seeking behavior. While recent work has focused on LLM-based agents for iterative code generation, we argue that the ability to recognize and query ambiguous requirements should be intrinsic to the models themselves, especially in agentic AI where models and humans collaborate. We present ClarifyCoder, a framework with synthetic data generation and instruction-tuning that fine-tunes an LLM to identify ambiguities and request clarification before code generation. Our approach has two components: (1) a data synthesis technique that augments programming datasets with scenarios requiring clarification to generate clarification-aware training data, and (2) a fine-tuning strategy that teaches models to prioritize seeking clarification over immediate code generation when faced with incomplete or ambiguous requirements. We also provide an empirical analysis of integrating ClarifyCoder with standard fine-tuning for joint optimization of clarification-awareness and coding ability. Experimental results show that ClarifyCoder achieves a 63% communication rate (40% absolute increase) and a 52% good question rate (30% absolute increase) on ambiguous tasks, significantly improving LLMs' communication capabilities while maintaining code generation performance.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Orchestration for Multi-Agent Systems: A Deep Learning Framework for Optimal Agent Selection in Multi-Domain Task Environments</title>
<link>https://arxiv.org/abs/2505.02861</link>
<guid>https://arxiv.org/abs/2505.02861</guid>
<content:encoded><![CDATA[
arXiv:2505.02861v2 Announce Type: replace 
Abstract: Multi-agent systems (MAS) are foundational in simulating complex real-world scenarios involving autonomous, interacting entities. However, traditional MAS architectures often suffer from rigid coordination mechanisms and difficulty adapting to dynamic tasks. We propose MetaOrch, a neural orchestration framework for optimal agent selection in multi-domain task environments. Our system implements a supervised learning approach that models task context, agent histories, and expected response quality to select the most appropriate agent for each task. A novel fuzzy evaluation module scores agent responses along completeness, relevance, and confidence dimensions, generating soft supervision labels for training the orchestrator. Unlike previous methods that hard-code agent-task mappings, MetaOrch dynamically predicts the most suitable agent while estimating selection confidence. Experiments in simulated environments with heterogeneous agents demonstrate that our approach achieves 86.3% selection accuracy, significantly outperforming baseline strategies including random selection and round-robin scheduling. The modular architecture emphasizes extensibility, allowing agents to be registered, updated, and queried independently. Results suggest that neural orchestration offers a powerful approach to enhancing the autonomy, interpretability, and adaptability of multi-agent systems across diverse task domains.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiddenBench: Assessing Collective Reasoning in Multi-Agent LLMs via Hidden Profile Tasks</title>
<link>https://arxiv.org/abs/2505.11556</link>
<guid>https://arxiv.org/abs/2505.11556</guid>
<content:encoded><![CDATA[
arXiv:2505.11556v2 Announce Type: replace 
Abstract: Multi-agent systems built on large language models (LLMs) promise enhanced problem-solving through distributed information integration, but may also replicate collective reasoning failures observed in human groups. Yet the absence of a theory-grounded benchmark makes it difficult to systematically evaluate and improve such reasoning. We introduce HiddenBench, the first benchmark for evaluating collective reasoning in multi-agent LLMs. It builds on the Hidden Profile paradigm from social psychology, where individuals each hold asymmetric pieces of information and must communicate to reach the correct decision. To ground the benchmark, we formalize the paradigm with custom tasks and show that GPT-4.1 groups fail to integrate distributed knowledge, exhibiting human-like collective reasoning failures that persist even with varied prompting strategies. We then construct the full benchmark, spanning 65 tasks drawn from custom designs, prior human studies, and automatic generation. Evaluating 15 LLMs across four model families, HiddenBench exposes persistent limitations while also providing comparative insights: some models (e.g., Gemini-2.5-Flash/Pro) achieve higher performance, yet scale and reasoning are not reliable indicators of stronger collective reasoning. Our work delivers the first reproducible benchmark for collective reasoning in multi-agent LLMs, offering diagnostic insight and a foundation for future research on artificial collective intelligence.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning BO: Enhancing Bayesian Optimization with Long-Context Reasoning Power of LLMs</title>
<link>https://arxiv.org/abs/2505.12833</link>
<guid>https://arxiv.org/abs/2505.12833</guid>
<content:encoded><![CDATA[
arXiv:2505.12833v2 Announce Type: replace 
Abstract: Many real-world scientific and industrial applications require the optimization of expensive black-box functions. Bayesian Optimization (BO) provides an effective framework for such problems. However, traditional BO methods are prone to get trapped in local optima and often lack interpretable insights. To address this issue, this paper designs Reasoning BO, a novel framework that leverages reasoning models to guide the sampling process in BO while incorporating multi-agent systems and knowledge graphs for online knowledge accumulation. By integrating the reasoning and contextual understanding capabilities of Large Language Models (LLMs), we can provide strong guidance to enhance the BO process. As the optimization progresses, Reasoning BO provides real-time sampling recommendations along with critical insights grounded in plausible scientific theories, aiding in the discovery of superior solutions within the search space. We systematically evaluate our approach across 10 diverse tasks encompassing synthetic mathematical functions and complex real-world applications. The framework demonstrates its capability to progressively refine sampling strategies through real-time insights and hypothesis evolution, effectively identifying higher-performing regions of the search space for focused exploration. This process highlights the powerful reasoning and context-learning abilities of LLMs in optimization scenarios. For example, in the Direct Arylation task, our method increased the yield to 60.7%, whereas traditional BO achieved only a 25.2% yield. Furthermore, our investigation reveals that smaller LLMs, when fine-tuned through reinforcement learning, can attain comparable performance to their larger counterparts.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Grunts to Lexicons: Emergent Language from Cooperative Foraging</title>
<link>https://arxiv.org/abs/2505.12872</link>
<guid>https://arxiv.org/abs/2505.12872</guid>
<content:encoded><![CDATA[
arXiv:2505.12872v2 Announce Type: replace 
Abstract: Language is a powerful communicative and cognitive tool. It enables humans to express thoughts, share intentions, and reason about complex phenomena. Despite our fluency in using and understanding language, the question of how it arises and evolves over time remains unsolved. A leading hypothesis in linguistics and anthropology posits that language evolved to meet the ecological and social demands of early human cooperation. Language did not arise in isolation, but through shared survival goals. Inspired by this view, we investigate the emergence of language in multi-agent Foraging Games. These environments are designed to reflect the cognitive and ecological constraints believed to have influenced the evolution of communication. Agents operate in a shared grid world with only partial knowledge about other agents and the environment, and must coordinate to complete games like picking up high-value targets or executing temporally ordered actions. Using end-to-end deep reinforcement learning, agents learn both actions and communication strategies from scratch. We find that agents develop communication protocols with hallmark features of natural language: arbitrariness, interchangeability, displacement, cultural transmission, and compositionality. We quantify each property and analyze how different factors, such as population size, social dynamics, and temporal dependencies, shape specific aspects of the emergent language. Our framework serves as a platform for studying how language can evolve from partial observability, temporal reasoning, and cooperative goals in embodied multi-agent settings. We will release all data, code, and models publicly.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Hierarchical Domain Models Through Environment-Grounded Interaction</title>
<link>https://arxiv.org/abs/2505.13497</link>
<guid>https://arxiv.org/abs/2505.13497</guid>
<content:encoded><![CDATA[
arXiv:2505.13497v2 Announce Type: replace 
Abstract: Domain models enable autonomous agents to solve long-horizon tasks by producing interpretable plans. However, in open-world environments, a single general domain model cannot capture the variety of tasks, so agents must generate suitable task-specific models on the fly. Large Language Models (LLMs), with their implicit common knowledge, can generate such domains, but suffer from high error rates that limit their applicability. Hence, related work relies on extensive human feed-back or prior knowledge, which undermines autonomous, open-world deployment. In this work, we propose LODGE, a framework for autonomous domain learning from LLMs and environment grounding. LODGE builds on hierarchical abstractions and automated simulations to identify and correct inconsistencies between abstraction layers and between the model and environment. Our framework is task-agnostic, as it generates predicates, operators, and their preconditions and effects, while only assuming access to a simulator and a set of generic, executable low-level skills. Experiments on two International Planning Competition ( IPC) domains and a robotic assembly domain show that LODGE yields more accurate domain models and higher task success than existing methods, requiring remarkably few environment interactions and no human feedback or demonstrations.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems</title>
<link>https://arxiv.org/abs/2505.16429</link>
<guid>https://arxiv.org/abs/2505.16429</guid>
<content:encoded><![CDATA[
arXiv:2505.16429v2 Announce Type: replace 
Abstract: Evaluating and iterating upon recommender systems is crucial, yet traditional A/B testing is resource-intensive, and offline methods struggle with dynamic user-platform interactions. While agent-based simulation is promising, existing platforms often lack a mechanism for user actions to dynamically reshape the environment. To bridge this gap, we introduce RecInter, a novel agent-based simulation platform for recommender systems featuring a robust interaction mechanism. In RecInter platform, simulated user actions (e.g., likes, reviews, purchases) dynamically update item attributes in real-time, and introduced Merchant Agents can reply, fostering a more realistic and evolving ecosystem. High-fidelity simulation is ensured through Multidimensional User Profiling module, Advanced Agent Architecture, and LLM fine-tuned on Chain-of-Thought (CoT) enriched interaction data. Our platform achieves significantly improved simulation credibility and successfully replicates emergent phenomena like Brand Loyalty and the Matthew Effect. Experiments demonstrate that this interaction mechanism is pivotal for simulating realistic system evolution, establishing our platform as a credible testbed for recommender systems research. Our codes are available at https://github.com/jinsong8/RecInter.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases</title>
<link>https://arxiv.org/abs/2505.20321</link>
<guid>https://arxiv.org/abs/2505.20321</guid>
<content:encoded><![CDATA[
arXiv:2505.20321v2 Announce Type: replace 
Abstract: Biomedical researchers increasingly rely on large-scale structured databases for complex analytical tasks. However, current text-to-SQL systems often struggle to map qualitative scientific questions into executable SQL, particularly when implicit domain reasoning is required. We introduce BiomedSQL, the first benchmark explicitly designed to evaluate scientific reasoning in text-to-SQL generation over a real-world biomedical knowledge base. BiomedSQL comprises 68,000 question/SQL query/answer triples grounded in a harmonized BigQuery knowledge base that integrates gene-disease associations, causal inference from omics data, and drug approval records. Each question requires models to infer domain-specific criteria, such as genome-wide significance thresholds, effect directionality, or trial phase filtering, rather than rely on syntactic translation alone. We evaluate a range of open- and closed-source LLMs across prompting strategies and interaction paradigms. Our results reveal a substantial performance gap: GPT-o3-mini achieves 59.0% execution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%, both well below the expert baseline of 90.0%. BiomedSQL provides a new foundation for advancing text-to-SQL systems capable of supporting scientific discovery through robust reasoning over structured biomedical knowledge bases. Our dataset is publicly available at https://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source at https://github.com/NIH-CARD/biomedsql.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XBOUND: Exploring Capability Boundaries of Device-Control Agents at the State Level</title>
<link>https://arxiv.org/abs/2505.21279</link>
<guid>https://arxiv.org/abs/2505.21279</guid>
<content:encoded><![CDATA[
arXiv:2505.21279v2 Announce Type: replace 
Abstract: Recent advancements in vision-language models have increased interest in Device-Control Agents (DC agents) for managing graphical user interfaces (GUIs). With the growing complexity and integration of such agents into various applications, effective evaluation methods have become crucial. The current evaluation method for DC agents primarily focuses on the instruction level, providing the current state (e.g., screenshots) and past execution history to determine actions for target instructions, helping identify potential execution failures. However, in GUI environments, a single state may contain multiple interactive widgets, each linked to different instructions, presenting an opportunity for diverse actions based on various instruction targets. Evaluating the agent's performance solely at the instruction level may overlook the broader context of these interactions. To capture a more comprehensive view of agent performance, we propose a new evaluation method, XBOUND, to evaluate the accuracy of instruction completion on a per-state basis. XBOUND provides a state-level evaluation framework, serving as a tool to assess agents' capabilities within environmental states. Our evaluation yields several key insights: UI-TARS stands out as the strongest 7B model, current agents display a bimodal performance pattern in instruction unification, and sub-7B models remain limited in state mastery. We further identify GPT-based planning as a critical bottleneck, and show that grounding data mainly benefits action matching, while trajectory data is more effective for instruction unification.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents</title>
<link>https://arxiv.org/abs/2505.22954</link>
<guid>https://arxiv.org/abs/2505.22954</guid>
<content:encoded><![CDATA[
arXiv:2505.22954v2 Announce Type: replace 
Abstract: Today's AI systems have human-designed, fixed architectures and cannot autonomously and continuously improve themselves. The advance of AI could itself be automated. If done safely, that would accelerate AI development and allow us to reap its benefits much sooner. Meta-learning can automate the discovery of novel algorithms, but is limited by first-order improvements and the human design of a suitable search space. The G\"odel machine proposed a theoretical alternative: a self-improving AI that repeatedly modifies itself in a provably beneficial manner. Unfortunately, proving that most changes are net beneficial is impossible in practice. We introduce the Darwin G\"odel Machine (DGM), a self-improving system that iteratively modifies its own code (thereby also improving its ability to modify its own codebase) and empirically validates each change using coding benchmarks. Inspired by Darwinian evolution and open-endedness research, the DGM maintains an archive of generated coding agents. It grows the archive by sampling an agent from it and using a foundation model to create a new, interesting, version of the sampled agent. This open-ended exploration forms a growing tree of diverse, high-quality agents and allows the parallel exploration of many different paths through the search space. Empirically, the DGM automatically improves its coding capabilities (e.g., better code editing tools, long-context window management, peer-review mechanisms), increasing performance on SWE-bench from 20.0% to 50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly outperforms baselines without self-improvement or open-ended exploration. All experiments were done with safety precautions (e.g., sandboxing, human oversight). The DGM is a significant step toward self-improving AI, capable of gathering its own stepping stones along paths that unfold into endless innovation.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Measurement Imbalance in Agentic AI Evaluation Undermines Industry Productivity Claims</title>
<link>https://arxiv.org/abs/2506.02064</link>
<guid>https://arxiv.org/abs/2506.02064</guid>
<content:encoded><![CDATA[
arXiv:2506.02064v2 Announce Type: replace 
Abstract: As industry reports claim agentic AI systems deliver double-digit productivity gains and multi-trillion dollar economic potential, the validity of these claims has become critical for investment decisions, regulatory policy, and responsible technology adoption. However, this paper demonstrates that current evaluation practices for agentic AI systems exhibit a systemic imbalance that calls into question prevailing industry productivity claims. Our systematic review of 84 papers (2023--2025) reveals an evaluation imbalance where technical metrics dominate assessments (83%), while human-centered (30%), safety (53%), and economic assessments (30%) remain peripheral, with only 15% incorporating both technical and human dimensions. This measurement gap creates a fundamental disconnect between benchmark success and deployment value. We present evidence from healthcare, finance, and retail sectors where systems excelling on technical metrics failed in real-world implementation due to unmeasured human, temporal, and contextual factors. Our position is not against agentic AI's potential, but rather that current evaluation frameworks systematically privilege narrow technical metrics while neglecting dimensions critical to real-world success. We propose a balanced four-axis evaluation model and call on the community to lead this paradigm shift because benchmark-driven optimization shapes what we build. By redefining evaluation practices, we can better align industry claims with deployment realities and ensure responsible scaling of agentic systems in high-stakes domains.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Simulating Society Requires Simulating Thought</title>
<link>https://arxiv.org/abs/2506.06958</link>
<guid>https://arxiv.org/abs/2506.06958</guid>
<content:encoded><![CDATA[
arXiv:2506.06958v2 Announce Type: replace 
Abstract: Simulating society with large language models (LLMs), we argue, requires more than generating plausible behavior; it demands cognitively grounded reasoning that is structured, revisable, and traceable. LLM-based agents are increasingly used to emulate individual and group behavior, primarily through prompting and supervised fine-tuning. Yet they often lack internal coherence, causal reasoning, and belief traceability, making them unreliable for simulating how people reason, deliberate, and respond to interventions.
  To address this, we present a conceptual modeling paradigm, Generative Minds (GenMinds), which draws from cognitive science to support structured belief representations in generative agents. To evaluate such agents, we introduce the RECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess reasoning fidelity via causal traceability, demographic grounding, and intervention consistency. These contributions advance a broader shift: from surface-level mimicry to generative agents that simulate thought -- not just language -- for social simulations.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentOrchestra: Orchestrating Hierarchical Multi-Agent Intelligence with the Tool-Environment-Agent(TEA) Protocol</title>
<link>https://arxiv.org/abs/2506.12508</link>
<guid>https://arxiv.org/abs/2506.12508</guid>
<content:encoded><![CDATA[
arXiv:2506.12508v4 Announce Type: replace 
Abstract: Recent advances in LLMs-based agent systems have demonstrated remarkable capabilities in solving complex tasks. Nevertheless, current protocols (e.g., A2A and MCP) suffer from insufficient capabilities in context management, limited adaptability to diverse environments, and the absence of dynamic agent architectures. To address these limitations, we propose the Tool-Environment-Agent (TEA) Protocol, which establishes a principled basis for integrating environments, agents, and tools into an unified system. The TEA protocol treats environments and agents as first-class resources, enabling comprehensive context management and adaptive environment integration. Based on this protocol, we introduce AgentOrchestra, a hierarchical multi-agent framework with a central planning agent that decomposes complex objectives and coordinates specialized agents. Each sub-agent is dedicated to specific functions, providing capabilities for data analysis, file operations, web navigation, and interactive reasoning. Notably, AgentOrchestra introduces a tool manager agent that supports intelligent evolution through dynamic tool creation, retrieval, and reuse mechanisms. Experiments on three widely used benchmarks show that AgentOrchestra consistently outperforms existing baselines, achieving state-of-the-art performance of 83.39% on GAIA and ranking among the top general-purpose LLM-based agents. These results highlight the effectiveness of the TEA Protocol and hierarchical organization in building general-purpose multi-agent systems.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LocationReasoner: Evaluating LLMs on Real-World Site Selection Reasoning</title>
<link>https://arxiv.org/abs/2506.13841</link>
<guid>https://arxiv.org/abs/2506.13841</guid>
<content:encoded><![CDATA[
arXiv:2506.13841v2 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs), particularly those enhanced through reinforced post-training, have demonstrated impressive reasoning capabilities, as exemplified by models such as OpenAI o1 and DeepSeek-R1. However, these capabilities are predominantly benchmarked on domains like mathematical problem solving and code generation, leaving open the question of whether such reasoning skills generalize to complex real-world scenarios. In this paper, we introduce LocationReasoner, a benchmark designed to evaluate LLMs' reasoning abilities in the context of real-world site selection, where models must identify feasible locations by reasoning over diverse and complicated spatial, environmental, and logistic constraints. The benchmark covers carefully crafted queries of varying difficulty levels and is supported by a sandbox environment with in-house tools for constraint-based location search. Automated verification further guarantees the scalability of the benchmark, enabling the addition of arbitrary number of queries. Extensive evaluations on real-world site selection data from Boston, New York, and Tampa reveal that state-of-the-art reasoning models offer limited improvement over their non-reasoning predecessors in real-world contexts, with even the latest OpenAI o4 model failing on 30% of site selection tasks. Moreover, agentic strategies such as ReAct and Reflexion often suffer from over-reasoning, leading to worse outcomes than direct prompting. With key limitations of LLMs in holistic and non-linear reasoning highlighted, we release LocationReasoner to foster the development of LLMs and agents capable of robust, grounded reasoning in real-world decision-making tasks. Codes and data for our benchmark are available at https://github.com/miho-koda/LocationReasoner.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Multi-Agent Control with Adversarial Disturbances</title>
<link>https://arxiv.org/abs/2506.18814</link>
<guid>https://arxiv.org/abs/2506.18814</guid>
<content:encoded><![CDATA[
arXiv:2506.18814v2 Announce Type: replace 
Abstract: Online multi-agent control problems, where many agents pursue competing and time-varying objectives, are widespread in domains such as autonomous robotics, economics, and energy systems. In these settings, robustness to adversarial disturbances is critical. In this paper, we study online control in multi-agent linear dynamical systems subject to such disturbances. In contrast to most prior work in multi-agent control, which typically assumes noiseless or stochastically perturbed dynamics, we consider an online setting where disturbances can be adversarial, and where each agent seeks to minimize its own sequence of convex losses. Under two feedback models, we analyze online gradient-based controllers with local policy updates. We prove per-agent regret bounds that are sublinear and near-optimal in the time horizon and that highlight different scalings with the number of agents. When agents' objectives are aligned, we further show that the multi-agent control problem induces a time-varying potential game for which we derive equilibrium tracking guarantees. Together, our results take a first step in bridging online control with online learning in games, establishing robust individual and collective performance guarantees in dynamic continuous-state environments.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GamerAstra: Supporting 2D Non-Twitch Video Games for Blind and Low-Vision Players through a Multi-Agent Framework</title>
<link>https://arxiv.org/abs/2506.22937</link>
<guid>https://arxiv.org/abs/2506.22937</guid>
<content:encoded><![CDATA[
arXiv:2506.22937v2 Announce Type: replace 
Abstract: Blind and low-vision (BLV) players face critical challenges in engaging with video games due to the inaccessibility of visual elements, difficulties navigating interfaces, and limitations in performing interaction. Meanwhile, the development of specialized accessibility features typically requires substantial programming effort and is often implemented on a game-by-game basis. To address these challenges, we introduce GamerAstra, a multi-agent human-AI collaboration framework that leverages a multi-agent design to facilitate access to 2D non-twitch video games for BLV players. It integrates vision-language models and computer vision techniques, enabling interaction with games lacking native accessibility support. The framework also incorporates custom assistance granularities to support varying degrees of visual impairment and enhances interface navigation through multiple input modalities. Technical evaluations and user studies indicate that GamerAstra effectively enhances playability and provides a more immersive gaming experience for BLV players. These findings also underscore potential avenues for advancing intelligent accessibility frameworks in the gaming domain.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling the Potential of Diffusion Large Language Model in Controllable Generation</title>
<link>https://arxiv.org/abs/2507.04504</link>
<guid>https://arxiv.org/abs/2507.04504</guid>
<content:encoded><![CDATA[
arXiv:2507.04504v2 Announce Type: replace 
Abstract: Controllable generation is a fundamental task in NLP with many applications, providing a basis for function calling to agentic communication. However, even state-of-the-art autoregressive Large Language Models (LLMs) today exhibit unreliability when required to generate structured output. Inspired by the current new diffusion-based large language models (dLLM), we realize that the architectural difference, especially the global information-sharing mechanism for language modeling, may be the key to unlock next-level controllable generation. To explore the possibility, we propose Self-adaptive Schema Scaffolding ($S^3$), a novel framework that enables dLLM to stably generate reliable structured outputs (e.g., JSON) by utilizing its innate reverse reasoning capability and global context awareness. $S^3$ initiates a schematic template directly in the output context as a starting state for dLLM, offering a more robust and general method than intricate prompt optimization. Experiments demonstrate that our method substantially unlocks the dLLM's potential in controllable generation in terms of structure adherence, content fidelity, and faithfulness. These results establish new perspectives and practical pathways for deploying language models in controllable generation tasks.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions</title>
<link>https://arxiv.org/abs/2507.05257</link>
<guid>https://arxiv.org/abs/2507.05257</guid>
<content:encoded><![CDATA[
arXiv:2507.05257v2 Announce Type: replace 
Abstract: Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to the lack of benchmarks. We term agents with memory mechanisms as memory agents. In this paper, based on classic theories from memory science and cognitive science, we identify four core competencies essential for memory agents: accurate retrieval, test-time learning, long-range understanding, and selective forgetting. Existing benchmarks either rely on limited context lengths or are tailored for static, long-context settings like book-based QA, which do not reflect the interactive, multi-turn nature of memory agents that incrementally accumulate information. Moreover, no existing benchmarks cover all four competencies. We introduce MemoryAgentBench, a new benchmark specifically designed for memory agents. Our benchmark transforms existing long-context datasets and incorporates newly constructed datasets into a multi-turn format, effectively simulating the incremental information processing characteristic of memory agents. By carefully selecting and curating datasets, our benchmark provides comprehensive coverage of the four core memory competencies outlined above, thereby offering a systematic and challenging testbed for assessing memory quality. We evaluate a diverse set of memory agents, ranging from simple context-based and retrieval-augmented generation (RAG) systems to advanced agents with external memory modules and tool integration. Empirical results reveal that current methods fall short of mastering all four competencies, underscoring the need for further research into comprehensive memory mechanisms for LLM agents.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Factors Affect LLMs and RLLMs in Financial Question Answering?</title>
<link>https://arxiv.org/abs/2507.08339</link>
<guid>https://arxiv.org/abs/2507.08339</guid>
<content:encoded><![CDATA[
arXiv:2507.08339v3 Announce Type: replace 
Abstract: Recently, the development of large language models (LLMs) and reasoning large language models (RLLMs) have gained considerable attention from many researchers. RLLMs enhance the reasoning capabilities of LLMs through Long Chain-of-Thought (Long CoT) processes, significantly improving the performance of LLMs in addressing complex problems. However, there are few works that systematically explore what methods can fully unlock the performance of LLMs and RLLMs within the financial domain. To investigate the impact of various methods on LLMs and RLLMs, we utilize five LLMs and three RLLMs to assess the effects of prompting methods, agentic frameworks, and multilingual alignment methods on financial question-answering tasks. Our research findings indicate: (1) Current prompting methods and agent frameworks enhance the performance of LLMs in financial question answering by simulating Long CoT; (2) RLLMs possess inherent Long CoT capabilities, which limits the effectiveness of conventional methods in further enhancing their performance; (3) Current advanced multilingual alignment methods primarily improve the multilingual performance of LLMs by extending the reasoning length, which yields minimal benefits for RLLMs. Additionally, we discuss strategies for enhancing the performance of LLMs and RLLMs in financial question answering, which may serve as a inspiration for future improvements. We hope that this study can serve as an important reference for LLMs and RLLMs in the field of financial question answering.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DriveAgent-R1: Advancing VLM-based Autonomous Driving with Active Perception and Hybrid Thinking</title>
<link>https://arxiv.org/abs/2507.20879</link>
<guid>https://arxiv.org/abs/2507.20879</guid>
<content:encoded><![CDATA[
arXiv:2507.20879v2 Announce Type: replace 
Abstract: The advent of Vision-Language Models (VLMs) has significantly advanced end-to-end autonomous driving, demonstrating powerful reasoning abilities for high-level behavior planning tasks. However, existing methods are often constrained by a passive perception paradigm, relying solely on text-based reasoning. This passivity restricts the model's capacity to actively seek crucial visual evidence when faced with uncertainty. To address this, we introduce DriveAgent-R1, the first autonomous driving agent capable of active perception for planning. In complex scenarios, DriveAgent-R1 proactively invokes tools to perform visual reasoning, firmly grounding its decisions in visual evidence, thereby enhancing both interpretability and reliability. Furthermore, we propose a hybrid thinking framework, inspired by human driver cognitive patterns, allowing the agent to adaptively switch between efficient text-only reasoning and robust tool-augmented visual reasoning based on scene complexity. This capability is cultivated through a three-stage progressive training strategy, featuring a core Cascaded Reinforcement Learning (Cascaded RL) phase. Extensive experiments on the Drive-Internal dataset, which is rich in long-tail scenarios, and the public nuScenes dataset show that, with only 3B parameters, DriveAgent-R1 achieves competitive performance comparable to top closed model systems such as GPT-5 and to human driving proficiency while remaining deployment-friendly, offering a proven path toward building more intelligent autonomous driving systems.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.00344</link>
<guid>https://arxiv.org/abs/2508.00344</guid>
<content:encoded><![CDATA[
arXiv:2508.00344v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown remarkable advancements in tackling agent-oriented tasks. Despite their potential, existing work faces challenges when deploying LLMs in agent-based environments. The widely adopted agent paradigm ReAct centers on integrating single-step reasoning with immediate action execution, which limits its effectiveness in complex tasks requiring long-term strategic planning. Furthermore, the coordination between the planner and executor during problem-solving is also a critical factor to consider in agent design. Additionally, current approaches predominantly rely on supervised fine-tuning, which often leads models to memorize established task completion trajectories, thereby restricting their generalization ability when confronted with novel problem contexts. To address these challenges, we introduce an adaptive global plan-based agent paradigm AdaPlan, aiming to synergize high-level explicit guidance with execution to support effective long-horizon decision-making. Based on the proposed paradigm, we further put forward PilotRL, a global planning-guided training framework for LLM agents driven by progressive reinforcement learning. We first develop the model's ability to follow explicit guidance from global plans when addressing agent tasks. Subsequently, based on this foundation, we focus on optimizing the quality of generated plans. Finally, we conduct joint optimization of the model's planning and execution coordination. Experiments indicate that PilotRL could achieve state-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing closed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78% comparing to GPT-4o-mini at a comparable parameter scale.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Aerial Manipulation of a Cable-Suspended Load using Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.01522</link>
<guid>https://arxiv.org/abs/2508.01522</guid>
<content:encoded><![CDATA[
arXiv:2508.01522v2 Announce Type: replace 
Abstract: This paper presents the first decentralized method to enable real-world 6-DoF manipulation of a cable-suspended load using a team of Micro-Aerial Vehicles (MAVs). Our method leverages multi-agent reinforcement learning (MARL) to train an outer-loop control policy for each MAV. Unlike state-of-the-art controllers that utilize a centralized scheme, our policy does not require global states, inter-MAV communications, nor neighboring MAV information. Instead, agents communicate implicitly through load pose observations alone, which enables high scalability and flexibility. It also significantly reduces computing costs during inference time, enabling onboard deployment of the policy. In addition, we introduce a new action space design for the MAVs using linear acceleration and body rates. This choice, combined with a robust low-level controller, enables reliable sim-to-real transfer despite significant uncertainties caused by cable tension during dynamic 3D motion. We validate our method in various real-world experiments, including full-pose control under load model uncertainties, showing setpoint tracking performance comparable to the state-of-the-art centralized method. We also demonstrate cooperation amongst agents with heterogeneous control policies, and robustness to the complete in-flight loss of one MAV. Videos of experiments: https://autonomousrobots.nl/paper_websites/aerial-manipulation-marl
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Context Adaptation for Consistent Role-Playing Agents with Retrieval-Augmented Generations</title>
<link>https://arxiv.org/abs/2508.02016</link>
<guid>https://arxiv.org/abs/2508.02016</guid>
<content:encoded><![CDATA[
arXiv:2508.02016v2 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) have catalyzed research on role-playing agents (RPAs). However, the process of collecting character-specific utterances and continually updating model parameters to track rapidly changing persona attributes is resource-intensive. Although retrieval-augmented generation (RAG) can alleviate this problem, if a persona does not contain knowledge relevant to a given query, RAG-based RPAs are prone to hallucination, making it challenging to generate accurate responses. In this paper, we propose Amadeus, a training-free framework that can significantly enhance persona consistency even when responding to questions that lie beyond a character's knowledge. Amadeus is composed of Adaptive Context-aware Text Splitter (ACTS), Guided Selection (GS), and Attribute Extractor (AE). To facilitate effective RAG-based role-playing, ACTS partitions each character's persona into optimally sized, overlapping chunks and augments this representation with hierarchical contextual information. AE identifies a character's general attributes from the chunks retrieved by GS and uses these attributes as a final context to maintain robust persona consistency even when answering out-of-knowledge questions. To underpin the development and rigorous evaluation of RAG-based RPAs, we manually construct CharacterRAG, a role-playing dataset that consists of persona documents for 15 distinct fictional characters totaling 976K written characters, and 450 question-answer pairs. We find that our proposed method effectively models not only the knowledge possessed by characters, but also various attributes such as personality.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InqEduAgent: Adaptive AI Learning Partners with Gaussian Process Augmentation</title>
<link>https://arxiv.org/abs/2508.03174</link>
<guid>https://arxiv.org/abs/2508.03174</guid>
<content:encoded><![CDATA[
arXiv:2508.03174v3 Announce Type: replace 
Abstract: Collaborative partnership matters in inquiry-oriented education. However, most study partners are selected either rely on experience-based assignments with little scientific planning or build on rule-based machine assistants, encountering difficulties in knowledge expansion and inadequate flexibility. This paper proposes an LLM-empowered agent model for simulating and selecting learning partners tailored to inquiry-oriented learning, named InqEduAgent. Generative agents are designed to capture cognitive and evaluative features of learners in real-world scenarios. Then, an adaptive matching algorithm with Gaussian process augmentation is formulated to identify patterns within prior knowledge. Optimal learning-partner matches are provided for learners facing different exercises. The experimental results show the optimal performance of InqEduAgent in most knowledge-learning scenarios and LLM environment with different levels of capabilities. This study promotes the intelligent allocation of human-based learning partners and the formulation of AI-based learning partners. The code, data, and appendix are publicly available at https://github.com/InqEduAgent/InqEduAgent.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models</title>
<link>https://arxiv.org/abs/2508.03860</link>
<guid>https://arxiv.org/abs/2508.03860</guid>
<content:encoded><![CDATA[
arXiv:2508.03860v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are trained on vast and diverse internet corpora that often include inaccurate or misleading content. Consequently, LLMs can generate misinformation, making robust fact-checking essential. This review systematically analyzes how LLM-generated content is evaluated for factual accuracy by exploring key challenges such as hallucinations, dataset limitations, and the reliability of evaluation metrics. The review emphasizes the need for strong fact-checking frameworks that integrate advanced prompting strategies, domain-specific fine-tuning, and retrieval-augmented generation (RAG) methods. It proposes five research questions that guide the analysis of the recent literature from 2020 to 2025, focusing on evaluation methods and mitigation techniques. Instruction tuning, multi-agent reasoning, and RAG frameworks for external knowledge access are also reviewed. The key findings demonstrate the limitations of current metrics, the importance of validated external evidence, and the improvement of factual consistency through domain-specific customization. The review underscores the importance of building more accurate, understandable, and context-aware fact-checking. These insights contribute to the advancement of research toward more trustworthy models.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Next Generation Equation-Free Multiscale Modelling of Crowd Dynamics via Machine Learning</title>
<link>https://arxiv.org/abs/2508.03926</link>
<guid>https://arxiv.org/abs/2508.03926</guid>
<content:encoded><![CDATA[
arXiv:2508.03926v2 Announce Type: replace 
Abstract: Bridging the microscopic and the macroscopic modelling scales in crowd dynamics constitutes an important open challenge for systematic numerical analysis, optimization and control. We propose a combined manifold and machine learning approach to learn the discrete evolution operator for the emergent crowd dynamics in latent spaces from high-fidelity agent-based simulations. The proposed framework builds upon our previous works on next-generation Equation-free algorithms for learning surrogate models of high-dim. multiscale systems. Our approach is a four-stage one, explicitly conserving the mass of the reconstructed dynamics in the high-dim. space. In the first step, we derive continuous macroscopic fields (densities) from discrete microscopic data (pedestrians' positions) using KDE. In the second step, based on manifold learning, we construct a map from the macroscopic ambient space into the latent space parametrized by a few coordinates based on POD of the corresponding density distribution. The third step involves learning reduced-order surrogate ROMs in the latent space using machine learning techniques, particularly LSTMs networks and MVARs. Finally, we reconstruct the crowd dynamics in the high-dim. space in terms of macroscopic density profiles. With this "embed->learn in latent space->lift back to ambient space" pipeline, we create an effective solution operator of the unavailable macroscopic PDE for the density evolution. For our illustrations, we use SFM to generate data in a corridor with an obstacle, imposing periodic boundary conditions. The numerical results demonstrate high accuracy, robustness, and generalizability, thus allowing for fast and accurate modelling of crowd dynamics from agent-based simulations. Notably, linear MVAR models surpass nonlinear LSTMs in predictive accuracy, while also offering significantly lower complexity and greater interpretability.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Design in Zero-Trust Distributed Networks: Challenges and Solutions</title>
<link>https://arxiv.org/abs/2508.04526</link>
<guid>https://arxiv.org/abs/2508.04526</guid>
<content:encoded><![CDATA[
arXiv:2508.04526v3 Announce Type: replace 
Abstract: Traditional security architectures are becoming more vulnerable to distributed attacks due to significant dependence on trust. This will further escalate when implementing agentic AI within the systems, as more components must be secured over a similar distributed space. These scenarios can be observed in consumer technologies, such as the dense Internet of things (IoT). Here, zero-trust architecture (ZTA) can be seen as a potential solution, which relies on a key principle of not giving users explicit trust, instead always verifying their privileges whenever a request is made. However, the overall security in ZTA is managed through its policies, and unverified policies can lead to unauthorized access. Thus, this paper explores challenges and solutions for ZTA policy design in the context of distributed networks, which is referred to as zero-trust distributed networks (ZTDN). This is followed by a case-study on formal verification of policies using UPPAAL. Subsequently, the importance of accountability and responsibility in the system's security is discussed.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source Models</title>
<link>https://arxiv.org/abs/2508.12461</link>
<guid>https://arxiv.org/abs/2508.12461</guid>
<content:encoded><![CDATA[
arXiv:2508.12461v2 Announce Type: replace 
Abstract: In August 2025, OpenAI released GPT-OSS models, its first open weight large language models since GPT-2 in 2019, comprising two mixture of experts architectures with 120B and 20B parameters. We evaluated both variants against six contemporary open source large language models ranging from 14.7B to 235B parameters, representing both dense and sparse designs, across ten benchmarks covering general knowledge, mathematical reasoning, code generation, multilingual understanding, and conversational ability. All models were tested in unquantised form under standardised inference settings, with statistical validation using McNemars test and effect size analysis. Results show that gpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such as HumanEval and MMLU, despite requiring substantially less memory and energy per response. Both models demonstrate mid-tier overall performance within the current open source landscape, with relative strength in code generation and notable weaknesses in multilingual tasks. These findings provide empirical evidence that scaling in sparse architectures may not yield proportional performance gains, underscoring the need for further investigation into optimisation strategies and informing more efficient model selection for future open source deployments. More details and evaluation scripts are available at the \href{https://ai-agent-lab.github.io/gpt-oss}{Project Webpage}.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMSearch-Plus: Benchmarking Provenance-Aware Search for Multimodal Browsing Agents</title>
<link>https://arxiv.org/abs/2508.21475</link>
<guid>https://arxiv.org/abs/2508.21475</guid>
<content:encoded><![CDATA[
arXiv:2508.21475v2 Announce Type: replace 
Abstract: Existing multimodal browsing benchmarks often fail to require genuine multimodal reasoning, as many tasks can be solved with text-only heuristics without vision-in-the-loop verification. We introduce MMSearch-Plus, a 311-task benchmark that enforces multimodal understanding by requiring extraction and propagation of fine-grained visual cues through iterative image-text retrieval and cross-validation under retrieval noise. Our curation procedure seeds questions whose answers require extrapolating from spatial cues and temporal traces to out-of-image facts such as events, dates, and venues. Beyond the dataset, we provide a model-agnostic agent framework with standard browsing tools and a set-of-mark (SoM) module, which lets the agent place marks, crop subregions, and launch targeted image/text searches. SoM enables provenance-aware zoom-and-retrieve and improves robustness in multi-step reasoning. We evaluated closed- and open-source MLLMs in this framework. The strongest system achieves an end-to-end accuracy of 36.0%, and integrating SoM produces consistent gains in multiple settings, with improvements up to +3.9 points. From failure analysis, we observe recurring errors in locating relevant webpages and distinguishing between visually similar events. These results underscore the challenges of real-world multimodal search and establish MMSearch-Plus as a rigorous benchmark for advancing agentic MLLMs.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Option Learning in High-Throughput Environments</title>
<link>https://arxiv.org/abs/2509.00338</link>
<guid>https://arxiv.org/abs/2509.00338</guid>
<content:encoded><![CDATA[
arXiv:2509.00338v2 Announce Type: replace 
Abstract: Hierarchical reinforcement learning (RL) has the potential to enable effective decision-making over long timescales. Existing approaches, while promising, have yet to realize the benefits of large-scale training. In this work, we identify and solve several key challenges in scaling online hierarchical RL to high-throughput environments. We propose Scalable Option Learning (SOL), a highly scalable hierarchical RL algorithm which achieves a ~35x higher throughput compared to existing hierarchical methods. To demonstrate SOL's performance and scalability, we train hierarchical agents using 30 billion frames of experience on the complex game of NetHack, significantly surpassing flat agents and demonstrating positive scaling trends. We also validate SOL on MiniHack and Mujoco environments, showcasing its general applicability. Our code is open sourced at: github.com/facebookresearch/sol.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Agentic OS: An LLM Agent Framework for Linux Schedulers</title>
<link>https://arxiv.org/abs/2509.01245</link>
<guid>https://arxiv.org/abs/2509.01245</guid>
<content:encoded><![CDATA[
arXiv:2509.01245v3 Announce Type: replace 
Abstract: Operating system schedulers suffer from a fundamental semantic gap, where kernel policies fail to understand application-specific needs, leading to suboptimal performance. We introduce SchedCP, the first framework that enables fully autonomous Large Language Model (LLM) agents to safely and efficiently optimize Linux schedulers without human involvement. Our core insight is that the challenge is not merely to apply a better LLM, but to architect a decoupled control plane that separates the AI's role of semantic reasoning ("what to optimize") from the system's role of execution ("how to observe and act"). Implemented as Model Context Protocol(MCP) server, SchedCP provides a stable interface with three key services: a Workload Analysis Engine, an evolving Scheduler Policy Repository, and an Execution Verifier that validates all AI-generated code and configure before deployment with static and dynamic analysis.
  We demonstrate this architecture's power with sched-agent, a multi-agent system that autonomously analyzes workloads, synthesizes custom eBPF scheduling policies, and deploys them via the sched\_ext infrastructure. Our evaluation shows that SchedCP achieves up to an 1.79x performance improvement, and a 13x cost reduction compared to naive agentic approaches, all while maintaining high success rate. By bridging the semantic gap, SchedCP democratizes expert-level system optimization and represents a step towards creating truly self-optimizing, application-aware operating systems. The code is open-sourced in https://github.com/eunomia-bpf/schedcp
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JudgeAgent: Knowledge-wise and Dynamic LLM Evaluation with Agent-as-Interviewer</title>
<link>https://arxiv.org/abs/2509.02097</link>
<guid>https://arxiv.org/abs/2509.02097</guid>
<content:encoded><![CDATA[
arXiv:2509.02097v3 Announce Type: replace 
Abstract: Current evaluation paradigms for large language models (LLMs) suffer from overestimated or biased evaluations and mismatched question difficulty, leading to incomplete evaluations of knowledge and capability boundaries, which hinder their effective application and optimization. To address these challenges, we propose Agent-as-Interviewer, a dynamic evaluation paradigm that employs LLM agents to conduct multi-turn interactions for evaluation. Unlike current benchmarking or dynamic interaction paradigms, Agent-as-Interviewer utilizes agents to invoke knowledge tools for wider and deeper knowledge in the dynamic multi-turn question generation, achieving more comprehensive evaluations of LLM's knowledge boundaries. It also leverages agents to plan query strategies for adjustment of the question difficulty levels, enhancing the difficulty control to match the actual capabilities of target LLMs. Based on this paradigm, we develop JudgeAgent, a knowledge-wise dynamic evaluation framework that employs knowledge-driven synthesis as the agent's tool and uses difficulty scoring as strategy guidance, thereby finally providing valuable suggestions to help targets optimize themselves. Extensive experiments validate the effectiveness of JudgeAgent's suggestions, demonstrating that Agent-as-Interviewer can accurately identify the knowledge and capability boundaries of target models. The source code is available on https://github.com/DataArcTech/JudgeAgent.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The STAR-XAI Protocol: A Framework for Inducing and Verifying Agency, Reasoning, and Reliability in AI Agents</title>
<link>https://arxiv.org/abs/2509.17978</link>
<guid>https://arxiv.org/abs/2509.17978</guid>
<content:encoded><![CDATA[
arXiv:2509.17978v2 Announce Type: replace 
Abstract: The "black box" nature of Large Reasoning Models (LRMs) presents critical limitations in reliability and transparency, fueling the debate around the "illusion of thinking" and the challenge of state hallucinations in agentic systems. In response, we introduce The STAR-XAI Protocol (Socratic, Transparent, Agentic, Reasoning - for eXplainable Artificial Intelligence), a novel operational methodology for training and operating verifiably reliable AI agents. Our method reframes the human-AI interaction as a structured Socratic dialogue governed by an explicit, evolving symbolic rulebook (the Consciousness Transfer Package - CTP) and a suite of integrity protocols, including a state-locking Checksum that eradicates internal state corruption. Through an exhaustive case study in the complex strategic game "Caps i Caps," we demonstrate that this "Clear Box" framework transforms an opaque LRM into a disciplined strategist. The agent not only exhibits the emergence of complex tactics, such as long-term planning, but also achieves ante-hoc transparency by justifying its intentions before acting. Crucially, it demonstrates Second-Order Agency by identifying and correcting flaws in its own supervisor-approved plans, leading to empirically-proven, 100% reliable state tracking and achieving "zero hallucinations by design." The STAR-XAI Protocol thus offers a practical pathway toward building AI agents that are not just high-performing but intrinsically auditable, trustworthy, and reliable.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Summarize by Learning to Quiz: Adversarial Agentic Collaboration for Long Document Summarization</title>
<link>https://arxiv.org/abs/2509.20900</link>
<guid>https://arxiv.org/abs/2509.20900</guid>
<content:encoded><![CDATA[
arXiv:2509.20900v2 Announce Type: replace 
Abstract: Long document summarization remains a significant challenge for current large language models (LLMs), as existing approaches commonly struggle with information loss, factual inconsistencies, and coherence issues when processing excessively long documents. We propose SummQ, a novel adversarial multi-agent framework that addresses these limitations through collaborative intelligence between specialized agents operating in two complementary domains: summarization and quizzing. Our approach employs summary generators and reviewers that work collaboratively to create and evaluate comprehensive summaries, while quiz generators and reviewers create comprehension questions that serve as continuous quality checks for the summarization process. This adversarial dynamic, enhanced by an examinee agent that validates whether the generated summary contains the information needed to answer the quiz questions, enables iterative refinement through multifaceted feedback mechanisms. We evaluate SummQ on three widely used long document summarization benchmarks. Experimental results demonstrate that our framework significantly outperforms existing state-of-the-art methods across ROUGE and BERTScore metrics, as well as in LLM-as-a-Judge and human evaluations. Our comprehensive analyses reveal the effectiveness of the multi-agent collaboration dynamics, the influence of different agent configurations, and the impact of the quizzing mechanism. This work establishes a new approach for long document summarization that uses adversarial agentic collaboration to improve summarization quality.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surgical Vision World Model</title>
<link>https://arxiv.org/abs/2503.02904</link>
<guid>https://arxiv.org/abs/2503.02904</guid>
<content:encoded><![CDATA[
arXiv:2503.02904v2 Announce Type: replace-cross 
Abstract: Realistic and interactive surgical simulation has the potential to facilitate crucial applications, such as medical professional training and autonomous surgical agent training. In the natural visual domain, world models have enabled action-controlled data generation, demonstrating the potential to train autonomous agents in interactive simulated environments when large-scale real data acquisition is infeasible. However, such works in the surgical domain have been limited to simplified computer simulations, and lack realism. Furthermore, existing literature in world models has predominantly dealt with action-labeled data, limiting their applicability to real-world surgical data, where obtaining action annotation is prohibitively expensive. Inspired by the recent success of Genie in leveraging unlabeled video game data to infer latent actions and enable action-controlled data generation, we propose the first surgical vision world model. The proposed model can generate action-controllable surgical data and the architecture design is verified with extensive experiments on the unlabeled SurgToolLoc-2022 dataset. Codes and implementation details are available at https://github.com/bhattarailab/Surgical-Vision-World-Model
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Reinforcement Learning for Greenhouse Gas Offset Credit Markets</title>
<link>https://arxiv.org/abs/2504.11258</link>
<guid>https://arxiv.org/abs/2504.11258</guid>
<content:encoded><![CDATA[
arXiv:2504.11258v2 Announce Type: replace-cross 
Abstract: Climate change is a major threat to the future of humanity, and its impacts are being intensified by excess man-made greenhouse gas emissions. One method governments can employ to control these emissions is to provide firms with emission limits and penalize any excess emissions above the limit. Excess emissions may also be offset by firms who choose to invest in carbon reducing and capturing projects. These projects generate offset credits which can be submitted to a regulating agency to offset a firm's excess emissions, or they can be traded with other firms. In this work, we characterize the finite-agent Nash equilibrium for offset credit markets. As computing Nash equilibria is an NP-hard problem, we utilize the modern reinforcement learning technique Nash-DQN to efficiently estimate the market's Nash equilibria. We demonstrate not only the validity of employing reinforcement learning methods applied to climate themed financial markets, but also the significant financial savings emitting firms may achieve when abiding by the Nash equilibria through numerical experiments.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distillation-Enabled Knowledge Alignment Protocol for Semantic Communication in AI Agent Networks</title>
<link>https://arxiv.org/abs/2505.17030</link>
<guid>https://arxiv.org/abs/2505.17030</guid>
<content:encoded><![CDATA[
arXiv:2505.17030v2 Announce Type: replace-cross 
Abstract: Future networks are envisioned to connect massive artificial intelligence (AI) agents, enabling their extensive collaboration on diverse tasks. Compared to traditional entities, these agents naturally suit the semantic communication (SC), which can significantly enhance the bandwidth efficiency. Nevertheless, SC requires the knowledge among agents to be aligned, while agents have distinct expert knowledge for their individual tasks in practice. In this paper, we propose a distillation-enabled knowledge alignment protocol (DeKAP), which distills the expert knowledge of each agent into parameter-efficient low-rank matrices, allocates them across the network, and allows agents to simultaneously maintain aligned knowledge for multiple tasks. We formulate the joint minimization of alignment loss, communication overhead, and storage cost as a large-scale integer linear programming problem and develop a highly efficient greedy algorithm. From computer simulation, the DeKAP establishes knowledge alignment with the lowest communication and computation resources compared to conventional approaches.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CapSpeech: Enabling Downstream Applications in Style-Captioned Text-to-Speech</title>
<link>https://arxiv.org/abs/2506.02863</link>
<guid>https://arxiv.org/abs/2506.02863</guid>
<content:encoded><![CDATA[
arXiv:2506.02863v2 Announce Type: replace-cross 
Abstract: Recent advancements in generative artificial intelligence have significantly transformed the field of style-captioned text-to-speech synthesis (CapTTS). However, adapting CapTTS to real-world applications remains challenging due to the lack of standardized, comprehensive datasets and limited research on downstream tasks built upon CapTTS. To address these gaps, we introduce CapSpeech, a new benchmark designed for a series of CapTTS-related tasks, including style-captioned text-to-speech synthesis with sound events (CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS (EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech comprises over 10 million machine-annotated audio-caption pairs and nearly 0.36 million human-annotated audio-caption pairs. In addition, we introduce two new datasets collected and recorded by a professional voice actor and experienced audio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside the datasets, we conduct comprehensive experiments using both autoregressive and non-autoregressive models on CapSpeech. Our results demonstrate high-fidelity and highly intelligible speech synthesis across a diverse range of speaking styles. To the best of our knowledge, CapSpeech is the largest available dataset offering comprehensive annotations for CapTTS-related tasks. The experiments and findings further provide valuable insights into the challenges of developing CapTTS systems.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FHIR-AgentBench: Benchmarking LLM Agents for Realistic Interoperable EHR Question Answering</title>
<link>https://arxiv.org/abs/2509.19319</link>
<guid>https://arxiv.org/abs/2509.19319</guid>
<content:encoded><![CDATA[
arXiv:2509.19319v1 Announce Type: new 
Abstract: The recent shift toward the Health Level Seven Fast Healthcare Interoperability Resources (HL7 FHIR) standard opens a new frontier for clinical AI, demanding LLM agents to navigate complex, resource-based data models instead of conventional structured health data. However, existing benchmarks have lagged behind this transition, lacking the realism needed to evaluate recent LLMs on interoperable clinical data. To bridge this gap, we introduce FHIR-AgentBench, a benchmark that grounds 2,931 real-world clinical questions in the HL7 FHIR standard. Using this benchmark, we systematically evaluate agentic frameworks, comparing different data retrieval strategies (direct FHIR API calls vs. specialized tools), interaction patterns (single-turn vs. multi-turn), and reasoning strategies (natural language vs. code generation). Our experiments highlight the practical challenges of retrieving data from intricate FHIR resources and the difficulty of reasoning over them, both of which critically affect question answering performance. We publicly release the FHIR-AgentBench dataset and evaluation suite (https://github.com/glee4810/FHIR-AgentBench) to promote reproducible research and the development of robust, reliable LLM agents for clinical applications.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Grained AI Model Caching and Downloading With Coordinated Multipoint Broadcasting in Multi-Cell Edge Networks</title>
<link>https://arxiv.org/abs/2509.19341</link>
<guid>https://arxiv.org/abs/2509.19341</guid>
<content:encoded><![CDATA[
arXiv:2509.19341v1 Announce Type: new 
Abstract: 6G networks are envisioned to support on-demand AI model downloading to accommodate diverse inference requirements of end users. By proactively caching models at edge nodes, users can retrieve the requested models with low latency for on-device AI inference. However, the substantial size of contemporary AI models poses significant challenges for edge caching under limited storage capacity, as well as for the concurrent delivery of heterogeneous models over wireless channels. To address these challenges, we propose a fine-grained AI model caching and downloading system that exploits parameter reusability, stemming from the common practice of fine-tuning task-specific models from a shared pre-trained model with frozen parameters. This system selectively caches model parameter blocks (PBs) at edge nodes, eliminating redundant storage of reusable parameters across different cached models. Additionally, it incorporates coordinated multipoint (CoMP) broadcasting to simultaneously deliver reusable PBs to multiple users, thereby enhancing downlink spectrum utilization. Under this arrangement, we formulate a model downloading delay minimization problem to jointly optimize PB caching, migration (among edge nodes), and broadcasting beamforming. To tackle this intractable problem, we develop a distributed multi-agent learning framework that enables edge nodes to explicitly learn mutual influence among their actions, thereby facilitating cooperation. Furthermore, a data augmentation approach is proposed to adaptively generate synthetic training samples through a predictive model, boosting sample efficiency and accelerating policy learning. Both theoretical analysis and simulation experiments validate the superior convergence performance of the proposed learning framework.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution</title>
<link>https://arxiv.org/abs/2509.19349</link>
<guid>https://arxiv.org/abs/2509.19349</guid>
<content:encoded><![CDATA[
arXiv:2509.19349v1 Announce Type: new 
Abstract: We introduce ShinkaEvolve: a new open-source framework leveraging large language models (LLMs) to advance scientific discovery with state-of-the-art performance and unprecedented efficiency. Recent advances in scaling inference time compute of LLMs have enabled significant progress in generalized scientific discovery. These approaches rely on evolutionary agentic harnesses that leverage LLMs as mutation operators to generate candidate solutions. However, current code evolution methods suffer from critical limitations: they are sample inefficient, requiring thousands of samples to identify effective solutions, and remain closed-source, hindering broad adoption and extension. ShinkaEvolve addresses these limitations, introducing three key innovations: a parent sampling technique balancing exploration and exploitation, code novelty rejection-sampling for efficient search space exploration, and a bandit-based LLM ensemble selection strategy. We evaluate ShinkaEvolve across diverse tasks, demonstrating consistent improvements in sample efficiency and solution quality. ShinkaEvolve discovers a new state-of-the-art circle packing solution using only 150 samples, designs high-performing agentic harnesses for AIME mathematical reasoning tasks, identifies improvements to ALE-Bench competitive programming solutions, and discovers novel mixture-of-expert load balancing loss functions that illuminate the space of optimization strategies. Our results demonstrate that ShinkaEvolve achieves broad applicability with exceptional sample efficiency. By providing open-source accessibility and cost-efficiency, this work democratizes open-ended discovery across diverse computational problems.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLM-Based Agentic AI with P-C-G: Optimized for Korean Tool Use</title>
<link>https://arxiv.org/abs/2509.19369</link>
<guid>https://arxiv.org/abs/2509.19369</guid>
<content:encoded><![CDATA[
arXiv:2509.19369v1 Announce Type: new 
Abstract: We propose a small-scale language model (SLM) based agent architecture, Planner-Caller-Generator (P-C-G), optimized for Korean tool use. P-C-G separates planning, calling, and generation by role: the Planner produces an initial batch plan with limited on-demand replanning; the Caller returns a normalized call object after joint schema-value validation; and the Generator integrates tool outputs to produce the final answer. We apply a Korean-first value policy to reduce execution failures caused by frequent Korean-to-English code switching in Korean settings. Evaluation assumes Korean queries and Korean tool/parameter specifications; it covers single-chain, multi-chain, missing-parameters, and missing-functions scenarios, and is conducted via an LLM-as-a-Judge protocol averaged over five runs under a unified I/O interface. Results show that P-C-G delivers competitive tool-use accuracy and end-to-end quality while reducing tokens and maintaining acceptable latency, indicating that role-specialized SLMs are a cost-effective alternative for Korean tool-use agents.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Observation: A Survey of Recent Advances</title>
<link>https://arxiv.org/abs/2509.19379</link>
<guid>https://arxiv.org/abs/2509.19379</guid>
<content:encoded><![CDATA[
arXiv:2509.19379v1 Announce Type: new 
Abstract: Imitation Learning (IL) algorithms offer an efficient way to train an agent by mimicking an expert's behavior without requiring a reward function. IL algorithms often necessitate access to state and action information from expert demonstrations. Although expert actions can provide detailed guidance, requiring such action information may prove impractical for real-world applications where expert actions are difficult to obtain. To address this limitation, the concept of learning from observation (LfO) or state-only imitation learning (SOIL) has recently gained attention, wherein the imitator only has access to expert state visitation information. In this paper, we present a framework for LfO and use it to survey and classify existing LfO methods in terms of their trajectory construction, assumptions and algorithm's design choices. This survey also draws connections between several related fields like offline RL, model-based RL and hierarchical RL. Finally, we use our framework to identify open problems and suggest future research directions.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Poster: ChatIYP: Enabling Natural Language Access to the Internet Yellow Pages Database</title>
<link>https://arxiv.org/abs/2509.19411</link>
<guid>https://arxiv.org/abs/2509.19411</guid>
<content:encoded><![CDATA[
arXiv:2509.19411v1 Announce Type: new 
Abstract: The Internet Yellow Pages (IYP) aggregates information from multiple sources about Internet routing into a unified, graph-based knowledge base. However, querying it requires knowledge of the Cypher language and the exact IYP schema, thus limiting usability for non-experts. In this paper, we propose ChatIYP, a domain-specific Retrieval-Augmented Generation (RAG) system that enables users to query IYP through natural language questions. Our evaluation demonstrates solid performance on simple queries, as well as directions for improvement, and provides insights for selecting evaluation metrics that are better fit for IYP querying AI agents.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Indispensable Role of User Simulation in the Pursuit of AGI</title>
<link>https://arxiv.org/abs/2509.19456</link>
<guid>https://arxiv.org/abs/2509.19456</guid>
<content:encoded><![CDATA[
arXiv:2509.19456v1 Announce Type: new 
Abstract: Progress toward Artificial General Intelligence (AGI) faces significant bottlenecks, particularly in rigorously evaluating complex interactive systems and acquiring the vast interaction data needed for training adaptive agents. This paper posits that user simulation -- creating computational agents that mimic human interaction with AI systems -- is not merely a useful tool, but is a critical catalyst required to overcome these bottlenecks and accelerate AGI development. We argue that realistic simulators provide the necessary environments for scalable evaluation, data generation for interactive learning, and fostering the adaptive capabilities central to AGI. Therefore, research into user simulation technology and intelligent task agents are deeply synergistic and must advance hand-in-hand. This article elaborates on the critical role of user simulation for AGI, explores the interdisciplinary nature of building realistic simulators, identifies key challenges including those posed by large language models, and proposes a future research agenda.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-evolved Imitation Learning in Simulated World</title>
<link>https://arxiv.org/abs/2509.19460</link>
<guid>https://arxiv.org/abs/2509.19460</guid>
<content:encoded><![CDATA[
arXiv:2509.19460v1 Announce Type: new 
Abstract: Imitation learning has been a trend recently, yet training a generalist agent across multiple tasks still requires large-scale expert demonstrations, which are costly and labor-intensive to collect. To address the challenge of limited supervision, we propose Self-Evolved Imitation Learning (SEIL), a framework that progressively improves a few-shot model through simulator interactions. The model first attempts tasksin the simulator, from which successful trajectories are collected as new demonstrations for iterative refinement. To enhance the diversity of these demonstrations, SEIL employs dual-level augmentation: (i) Model-level, using an Exponential Moving Average (EMA) model to collaborate with the primary model, and (ii) Environment-level, introducing slight variations in initial object positions. We further introduce a lightweight selector that filters complementary and informative trajectories from the generated pool to ensure demonstration quality. These curated samples enable the model to achieve competitive performance with far fewer training examples. Extensive experiments on the LIBERO benchmark show that SEIL achieves a new state-of-the-art performance in few-shot imitation learning scenarios. Code is available at https://github.com/Jasper-aaa/SEIL.git.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic and Scalable Safety Verification using Interval Reachability with Subspace Sampling</title>
<link>https://arxiv.org/abs/2509.19472</link>
<guid>https://arxiv.org/abs/2509.19472</guid>
<content:encoded><![CDATA[
arXiv:2509.19472v1 Announce Type: new 
Abstract: Interval refinement is a technique for reducing the conservatism of traditional interval based reachability methods by lifting the system to a higher dimension using new auxiliary variables and exploiting the introduced structure through a refinement procedure. We present a novel, efficiently scaling, automatic refinement strategy based on a subspace sampling argument and motivated by reducing the number of interval operations through sparsity. Unlike previous methods, we guarantee that refined bounds shrink as additional auxiliary variables are added. This additionally encourages automation of the lifting phase by allowing larger groups of auxiliary variables to be considered. We implement our strategy in JAX, a high-performance computational toolkit for Python and demonstrate its efficacy on several examples, including regulating a multi-agent platoon to the origin while avoiding an obstacle.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Heterogeneous Multi-Agent Challenge</title>
<link>https://arxiv.org/abs/2509.19512</link>
<guid>https://arxiv.org/abs/2509.19512</guid>
<content:encoded><![CDATA[
arXiv:2509.19512v1 Announce Type: new 
Abstract: Multi-Agent Reinforcement Learning (MARL) is a growing research area which gained significant traction in recent years, extending Deep RL applications to a much wider range of problems. A particularly challenging class of problems in this domain is Heterogeneous Multi-Agent Reinforcement Learning (HeMARL), where agents with different sensors, resources, or capabilities must cooperate based on local information. The large number of real-world situations involving heterogeneous agents makes it an attractive research area, yet underexplored, as most MARL research focuses on homogeneous agents (e.g., a swarm of identical robots). In MARL and single-agent RL, standardized environments such as ALE and SMAC have allowed to establish recognized benchmarks to measure progress. However, there is a clear lack of such standardized testbed for cooperative HeMARL. As a result, new research in this field often uses simple environments, where most algorithms perform near optimally, or uses weakly heterogeneous MARL environments.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Longitudinal Randomized Control Study of Companion Chatbot Use: Anthropomorphism and Its Mediating Role on Social Impacts</title>
<link>https://arxiv.org/abs/2509.19515</link>
<guid>https://arxiv.org/abs/2509.19515</guid>
<content:encoded><![CDATA[
arXiv:2509.19515v1 Announce Type: new 
Abstract: Relationships with social artificial intelligence (AI) agents are on the rise. People report forming friendships, mentorships, and romantic partnerships with chatbots such as Replika, a type of social AI agent that is designed specifically for companionship. Concerns that companion chatbot relationships may harm or replace human ones have been raised, but whether and how these social consequences occur remains unclear. Prior research suggests that people's states of social need and their anthropomorphism of the AI agent may play a role in how human-AI interaction impacts human-human interaction. In this longitudinal study (N = 183), participants were randomly assigned to converse with a companion chatbot over text or to play text-based word games for 10 minutes a day for 21 consecutive days. During these 21 days, participants also completed four surveys and two audio-recorded interviews. We found that people's social health and relationships were not significantly impacted by interacting with a companion chatbot across 21 days compared to the control group. However, people who had a higher desire to socially connect anthropomorphized the chatbot more. Those who anthropomorphized the chatbot more indicated that the human-chatbot interaction had greater impacts on their social interactions and relationships with family and friends. A mediation analysis suggested that the impact of human-AI interaction on human-human social outcomes was mediated by the extent to which people anthropomorphized the AI agent, which itself was related to the desire to socially connect.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nano Bio-Agents (NBA): Small Language Model Agents for Genomics</title>
<link>https://arxiv.org/abs/2509.19566</link>
<guid>https://arxiv.org/abs/2509.19566</guid>
<content:encoded><![CDATA[
arXiv:2509.19566v1 Announce Type: new 
Abstract: We investigate the application of Small Language Models (<10 billion parameters) for genomics question answering via agentic framework to address hallucination issues and computational cost challenges. The Nano Bio-Agent (NBA) framework we implemented incorporates task decomposition, tool orchestration, and API access into well-established systems such as NCBI and AlphaGenome. Results show that SLMs combined with such agentic framework can achieve comparable and in many cases superior performance versus existing approaches utilising larger models, with our best model-agent combination achieving 98% accuracy on the GeneTuring benchmark. Notably, small 3-10B parameter models consistently achieve 85-97% accuracy while requiring much lower computational resources than conventional approaches. This demonstrates promising potential for efficiency gains, cost savings, and democratization of ML-powered genomics tools while retaining highly robust and accurate performance.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action</title>
<link>https://arxiv.org/abs/2509.19571</link>
<guid>https://arxiv.org/abs/2509.19571</guid>
<content:encoded><![CDATA[
arXiv:2509.19571v1 Announce Type: new 
Abstract: Executing open-ended natural language queries is a core problem in robotics. While recent advances in imitation learning and vision-language-actions models (VLAs) have enabled promising end-to-end policies, these models struggle when faced with complex instructions and new scenes. An alternative is to design an explicit scene representation as a queryable interface between the robot and the world, using query results to guide downstream motion planning. In this work, we present Agentic Scene Policies (ASP), an agentic framework that leverages the advanced semantic, spatial, and affordance-based querying capabilities of modern scene representations to implement a capable language-conditioned robot policy. ASP can execute open-vocabulary queries in a zero-shot manner by explicitly reasoning about object affordances in the case of more complex skills. Through extensive experiments, we compare ASP with VLAs on tabletop manipulation problems and showcase how ASP can tackle room-level queries through affordance-guided navigation, and a scaled-up scene representation. (Project page: https://montrealrobotics.ca/agentic-scene-policies.github.io/)
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method for Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2509.19599</link>
<guid>https://arxiv.org/abs/2509.19599</guid>
<content:encoded><![CDATA[
arXiv:2509.19599v1 Announce Type: new 
Abstract: Multi-agent systems (MAS) are increasingly tasked with solving complex, knowledge-intensive problems where effective agent orchestration is critical. Conventional orchestration methods rely on static agent descriptions, which often become outdated or incomplete. This limitation leads to inefficient task routing, particularly in dynamic environments where agent capabilities continuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a novel approach that augments static descriptions with dynamic, privacy-preserving relevance signals derived from each agent's internal knowledge base (KB). In the proposed framework, when static descriptions are insufficient for a clear routing decision, the orchestrator prompts the subagents in parallel. Each agent then assesses the task's relevance against its private KB, returning a lightweight ACK signal without exposing the underlying data. These collected signals populate a shared semantic cache, providing dynamic indicators of agent suitability for future queries. By combining this novel mechanism with static descriptions, our method achieves more accurate and adaptive task routing preserving agent autonomy and data confidentiality. Benchmarks show that our KBA Orchestration significantly outperforms static description-driven methods in routing precision and overall system efficiency, making it suitable for large-scale systems that require higher accuracy than standard description-driven routing.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoSpec: An Agentic Framework for Automatically Drafting Patent Specification</title>
<link>https://arxiv.org/abs/2509.19640</link>
<guid>https://arxiv.org/abs/2509.19640</guid>
<content:encoded><![CDATA[
arXiv:2509.19640v1 Announce Type: new 
Abstract: Patents play a critical role in driving technological innovation by granting inventors exclusive rights to their inventions. However the process of drafting a patent application is often expensive and time-consuming, making it a prime candidate for automation. Despite recent advancements in language models, several challenges hinder the development of robust automated patent drafting systems. First, the information within a patent application is highly confidential, which often prevents the use of closed-source LLMs for automating this task. Second, the process of drafting a patent application is difficult for even the most advanced language models due to their long context, technical writing style, and specialized domain knowledge. To address these challenges, we introduce AutoSpec, a secure, agentic framework for Automatically drafting patent Specification. Our approach decomposes the drafting process into a sequence of manageable subtasks, each solvable by smaller, open-source language models enhanced with custom tools tailored for drafting patent specification. To assess our system, we design a novel evaluation protocol in collaboration with experienced patent attorneys. Our automatic and expert evaluations show that AutoSpec outperforms existing baselines on a patent drafting task.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Stop: Reinforcement Learning for Efficient Patient-Level Echocardiographic Classification</title>
<link>https://arxiv.org/abs/2509.19694</link>
<guid>https://arxiv.org/abs/2509.19694</guid>
<content:encoded><![CDATA[
arXiv:2509.19694v1 Announce Type: new 
Abstract: Guidelines for transthoracic echocardiographic examination recommend the acquisition of multiple video clips from different views of the heart, resulting in a large number of clips. Typically, automated methods, for instance disease classifiers, either use one clip or average predictions from all clips. Relying on one clip ignores complementary information available from other clips, while using all clips is computationally expensive and may be prohibitive for clinical adoption.
  To select the optimal subset of clips that maximize performance for a specific task (image-based disease classification), we propose a method optimized through reinforcement learning. In our method, an agent learns to either keep processing view-specific clips to reduce the disease classification uncertainty, or stop processing if the achieved classification confidence is sufficient. Furthermore, we propose a learnable attention-based aggregation method as a flexible way of fusing information from multiple clips. The proposed method obtains an AUC of 0.91 on the task of detecting cardiac amyloidosis using only 30% of all clips, exceeding the performance achieved from using all clips and from other benchmarks.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UserRL: Training Interactive User-Centric Agent via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.19736</link>
<guid>https://arxiv.org/abs/2509.19736</guid>
<content:encoded><![CDATA[
arXiv:2509.19736v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has shown promise in training agentic models that move beyond static benchmarks to engage in dynamic, multi-turn interactions. Yet, the ultimate value of such agents lies in their ability to assist users, a setting where diversity and dynamics of user interaction pose challenges. In this work, we propose UserRL, a unified framework for training and evaluating user-centric abilities through standardized gym environments paired with simulated users. We systematically vary turn-level reward assignment and trajectory-level score calculation to analyze how different formulations affect learning under the GRPO algorithm. Our experiments across Qwen3 models reveal three key findings: (i) SFT cold start is critical for unlocking initial interaction ability and enabling sustained RL improvements; (ii) deliberate trajectory scoring yields more efficient and effective multi-turn interactions; and (iii) while stronger simulated users (e.g., GPT-4o) facilitates training, open-source simulators (e.g., Qwen3-32B) remain a cost-effective and transferable option. Together, these results highlight that careful design of reward shaping and user simulation choice is as crucial as model scale, and establish UserRL as a practical pathway for developing robust user-centric agentic models. All codes and data are public for future research.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Conductor and the Engine: A Path Towards Co-Designed Reasoning</title>
<link>https://arxiv.org/abs/2509.19762</link>
<guid>https://arxiv.org/abs/2509.19762</guid>
<content:encoded><![CDATA[
arXiv:2509.19762v1 Announce Type: new 
Abstract: Modern LLM reasoning relies on extensive test-time computation, driven by internal model training and external agentic orchestration. However, this synergy is often inefficient, as model verbosity and poor instruction following lead to wasted compute. We analyze this capability-cost trade-off and introduce an optimized reasoning workflow (\cepo) that empowers smaller open-source models to outperform models multiple times their size. We will open-source this workflow to enable further research. Our work demonstrates a clear path toward co-designing orchestration frameworks with the underlying model capabilities to unlock powerful reasoning in small-to-medium sized models.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frictional Q-Learning</title>
<link>https://arxiv.org/abs/2509.19771</link>
<guid>https://arxiv.org/abs/2509.19771</guid>
<content:encoded><![CDATA[
arXiv:2509.19771v2 Announce Type: new 
Abstract: We draw an analogy between static friction in classical mechanics and extrapolation error in off-policy RL, and use it to formulate a constraint that prevents the policy from drifting toward unsupported actions. In this study, we present Frictional Q-learning, a deep reinforcement learning algorithm for continuous control, which extends batch-constrained reinforcement learning. Our algorithm constrains the agent's action space to encourage behavior similar to that in the replay buffer, while maintaining a distance from the manifold of the orthonormal action space. The constraint preserves the simplicity of batch-constrained, and provides an intuitive physical interpretation of extrapolation error. Empirically, we further demonstrate that our algorithm is robustly trained and achieves competitive performance across standard continuous control benchmarks.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Metacognition: Designing a "Self-Aware" Low-Code Agent for Failure Prediction and Human Handoff</title>
<link>https://arxiv.org/abs/2509.19783</link>
<guid>https://arxiv.org/abs/2509.19783</guid>
<content:encoded><![CDATA[
arXiv:2509.19783v1 Announce Type: new 
Abstract: The inherent non-deterministic nature of autonomous agents, particularly within low-code/no-code (LCNC) environments, presents significant reliability challenges. Agents can become trapped in unforeseen loops, generate inaccurate outputs, or encounter unrecoverable failures, leading to user frustration and a breakdown of trust. This report proposes a novel architectural pattern to address these issues: the integration of a secondary, "metacognitive" layer that actively monitors the primary LCNC agent. Inspired by human introspection, this layer is designed to predict impending task failures based on a defined set of triggers, such as excessive latency or repetitive actions. Upon predicting a failure, the metacognitive agent proactively initiates a human handoff, providing the user with a clear summary of the agent's "thought process" and a detailed explanation of why it could not proceed. An empirical analysis of a prototype system demonstrates that this approach significantly increases the overall task success rate. However, this performance gain comes with a notable increase in computational overhead. The findings reframe human handoffs not as an admission of defeat but as a core design feature that enhances system resilience, improves user experience, and builds trust by providing transparency into the agent's internal state. The report discusses the practical and ethical implications of this approach and identifies key directions for future research.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dispersion Formation Control: from Geometry to Distribution</title>
<link>https://arxiv.org/abs/2509.19784</link>
<guid>https://arxiv.org/abs/2509.19784</guid>
<content:encoded><![CDATA[
arXiv:2509.19784v1 Announce Type: new 
Abstract: We introduce and develop the concept of dispersion formation control, bridging a gap between shape-assembly studies in physics and biology and formation control theory. In current formation control studies, the control objectives typically focus on achieving desired local geometric properties, such as inter-agent distances, bearings, or relative positions. In contrast, our dispersion formation control approach enables agents to directly regulate the dispersion of their spatial distribution, a global variable associated with a covariance matrix. Specifically, we introduce the notion of covariance similarity to define the target spatial dispersion of agents. Building on this framework, we propose two control strategies: a centralized approach to illustrate the key ideas, and a distributed approach that enables agents to control the global dispersion but using only local information. Our stability analysis demonstrates that both strategies ensure exponential convergence of the agents' distribution to the desired dispersion. Notably, controlling a global variable rather than multiple local ones enhances the resiliency of the system, particularly against malfunctioning agents. Simulations validate the effectiveness of the proposed dispersion formation control.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RDAR: Reward-Driven Agent Relevance Estimation for Autonomous Driving</title>
<link>https://arxiv.org/abs/2509.19789</link>
<guid>https://arxiv.org/abs/2509.19789</guid>
<content:encoded><![CDATA[
arXiv:2509.19789v1 Announce Type: new 
Abstract: Human drivers focus only on a handful of agents at any one time. On the other hand, autonomous driving systems process complex scenes with numerous agents, regardless of whether they are pedestrians on a crosswalk or vehicles parked on the side of the road. While attention mechanisms offer an implicit way to reduce the input to the elements that affect decisions, existing attention mechanisms for capturing agent interactions are quadratic, and generally computationally expensive. We propose RDAR, a strategy to learn per-agent relevance -- how much each agent influences the behavior of the controlled vehicle -- by identifying which agents can be excluded from the input to a pre-trained behavior model. We formulate the masking procedure as a Markov Decision Process where the action consists of a binary mask indicating agent selection. We evaluate RDAR on a large-scale driving dataset, and demonstrate its ability to learn an accurate numerical measure of relevance by achieving comparable driving performance, in terms of overall progress, safety and performance, while processing significantly fewer agents compared to a state of the art behavior model.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI for Low-Altitude Semantic Wireless Networks: An Energy Efficient Design</title>
<link>https://arxiv.org/abs/2509.19791</link>
<guid>https://arxiv.org/abs/2509.19791</guid>
<content:encoded><![CDATA[
arXiv:2509.19791v1 Announce Type: new 
Abstract: This letter addresses the energy efficiency issue in unmanned aerial vehicle (UAV)-assisted autonomous systems. We propose a framework for an agentic artificial intelligence (AI)-powered low-altitude semantic wireless network, that intelligently orchestrates a sense-communicate-decide-control workflow. A system-wide energy consumption minimization problem is formulated to enhance mission endurance. This problem holistically optimizes key operational variables, including UAV's location, semantic compression ratio, transmit power of the UAV and a mobile base station, and binary decision for AI inference task offloading, under stringent latency and quality-of-service constraints. To tackle the formulated mixed-integer non-convex problem, we develop a low-complexity algorithm which can obtain the globally optimal solution with two-dimensional search. Simulation results validate the effectiveness of our proposed design, demonstrating significant reductions in total energy consumption compared to conventional baseline approaches.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersONAL: Towards a Comprehensive Benchmark for Personalized Embodied Agents</title>
<link>https://arxiv.org/abs/2509.19843</link>
<guid>https://arxiv.org/abs/2509.19843</guid>
<content:encoded><![CDATA[
arXiv:2509.19843v1 Announce Type: new 
Abstract: Recent advances in Embodied AI have enabled agents to perform increasingly complex tasks and adapt to diverse environments. However, deploying such agents in realistic human-centered scenarios, such as domestic households, remains challenging, particularly due to the difficulty of modeling individual human preferences and behaviors. In this work, we introduce PersONAL (PERSonalized Object Navigation And Localization, a comprehensive benchmark designed to study personalization in Embodied AI. Agents must identify, retrieve, and navigate to objects associated with specific users, responding to natural-language queries such as "find Lily's backpack". PersONAL comprises over 2,000 high-quality episodes across 30+ photorealistic homes from the HM3D dataset. Each episode includes a natural-language scene description with explicit associations between objects and their owners, requiring agents to reason over user-specific semantics. The benchmark supports two evaluation modes: (1) active navigation in unseen environments, and (2) object grounding in previously mapped scenes. Experiments with state-of-the-art baselines reveal a substantial gap to human performance, highlighting the need for embodied agents capable of perceiving, reasoning, and memorizing over personalized information; paving the way towards real-world assistive robot.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CollaPipe: Adaptive Segment-Optimized Pipeline Parallelism for Collaborative LLM Training in Heterogeneous Edge Networks</title>
<link>https://arxiv.org/abs/2509.19855</link>
<guid>https://arxiv.org/abs/2509.19855</guid>
<content:encoded><![CDATA[
arXiv:2509.19855v1 Announce Type: new 
Abstract: The increasing demand for intelligent mobile applications has made multi-agent collaboration with Transformer-based large language models (LLMs) essential in mobile edge computing (MEC) networks. However, training LLMs in such environments remains challenging due to heavy computation, high end-to-end latency, and limited model generalization. We introduce CollaPipe, a hybrid distributed learning framework that integrates collaborative pipeline parallelism with federated aggregation to support self-evolving intelligent networks. In CollaPipe, the encoder part is adaptively partitioned into variable-sized segments and deployed across mobile devices for pipeline-parallel training, while the decoder is deployed on edge servers to handle generative tasks. Then we perform global model update via federated aggregation. To enhance training efficiency, we formulate a joint optimization problem that adaptively allocates model segments, micro-batches, bandwidth, and transmission power. We derive and use a closed-form convergence bound to design an Dynamic Segment Scheduling and Resource Allocation (DSSDA) algorithm based on Lyapunov optimization, ensuring system stability under long-term constraints. Extensive experiments on downstream tasks with Transformer and BERT models show that CollaPipe improves computation efficiency by up to 15.09%, reduces end-to-end latency by at least 48.98%, and cuts single device memory usage by more than half, enabling online learning in heterogeneous and dynamic communication environments.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable and Approximation-free Symbolic Control for Unknown Euler-Lagrange Systems</title>
<link>https://arxiv.org/abs/2509.19859</link>
<guid>https://arxiv.org/abs/2509.19859</guid>
<content:encoded><![CDATA[
arXiv:2509.19859v1 Announce Type: new 
Abstract: We propose a novel symbolic control framework for enforcing temporal logic specifications in Euler-Lagrange systems that addresses the key limitations of traditional abstraction-based approaches. Unlike existing methods that require exact system models and provide guarantees only at discrete sampling instants, our approach relies only on bounds on system parameters and input constraints, and ensures correctness for the full continuous-time trajectory. The framework combines scalable abstraction of a simplified virtual system with a closed-form, model-free controller that guarantees trajectories satisfy the original specification while respecting input bounds and remaining robust to unknown but bounded disturbances. We provide feasibility conditions for the construction of confinement regions and analyze the trade-off between efficiency and conservatism. Case studies on pendulum dynamics, a two-link manipulator, and multi-agent systems, including hardware experiments, demonstrate that the proposed approach ensures both correctness and safety while significantly reducing computational time and memory requirements. These results highlight its scalability and practicality for real-world robotic systems where precise models are unavailable and continuous-time guarantees are essential.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2509.19870</link>
<guid>https://arxiv.org/abs/2509.19870</guid>
<content:encoded><![CDATA[
arXiv:2509.19870v1 Announce Type: new 
Abstract: Vision-Language-Action (VLA) models are driving rapid progress in robotics by enabling agents to interpret multimodal inputs and execute complex, long-horizon tasks. However, their safety and robustness against adversarial attacks remain largely underexplored. In this work, we identify and formalize a critical adversarial vulnerability in which adversarial images can "freeze" VLA models and cause them to ignore subsequent instructions. This threat effectively disconnects the robot's digital mind from its physical actions, potentially inducing inaction during critical interventions. To systematically study this vulnerability, we propose FreezeVLA, a novel attack framework that generates and evaluates action-freezing attacks via min-max bi-level optimization. Experiments on three state-of-the-art VLA models and four robotic benchmarks show that FreezeVLA attains an average attack success rate of 76.2%, significantly outperforming existing methods. Moreover, adversarial images generated by FreezeVLA exhibit strong transferability, with a single image reliably inducing paralysis across diverse language prompts. Our findings expose a critical safety risk in VLA models and highlight the urgent need for robust defense mechanisms.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Language Barriers: Multi-Agent Coordination for Multi-Language Code Generation</title>
<link>https://arxiv.org/abs/2509.19918</link>
<guid>https://arxiv.org/abs/2509.19918</guid>
<content:encoded><![CDATA[
arXiv:2509.19918v1 Announce Type: new 
Abstract: Producing high-quality code across multiple programming languages is increasingly important as today's software systems are built on heterogeneous stacks. Large language models (LLMs) have advanced the state of automated programming, yet their proficiency varies sharply between languages, especially those with limited training data such as Rust, Perl, OCaml, and Erlang. Many current solutions including language-specific fine-tuning, multi-agent orchestration, transfer learning, and intermediate-representation pipelines still approach each target language in isolation, missing opportunities to share knowledge or exploit recurring cross-language patterns.
  XL-CoGen tackles this challenge with a coordinated multi-agent architecture that integrates intermediate representation, code generation, translation, and automated repair. Its distinguishing feature is a data-driven mechanism for selecting bridging languages: empirically derived transfer matrices identify the best intermediate languages based on demonstrated translation success rather than raw generation accuracy. The system performs early output validation, iteratively corrects errors, and reuses intermediate artifacts as contextual scaffolds for subsequent translations.
  Extensive experiments show that XL-CoGen yields notable improvements with 13 percentage-point gains over the strongest fine-tuned baseline and as much as 30 percentage points over existing single-language multi-agent methods. Ablation studies further demonstrate that compatibility-guided bridging significantly outperforms LLM-based heuristics, confirming the value of cumulative cross-language knowledge transfer.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploration with Foundation Models: Capabilities, Limitations, and Hybrid Approaches</title>
<link>https://arxiv.org/abs/2509.19924</link>
<guid>https://arxiv.org/abs/2509.19924</guid>
<content:encoded><![CDATA[
arXiv:2509.19924v1 Announce Type: new 
Abstract: Exploration in reinforcement learning (RL) remains challenging, particularly in sparse-reward settings. While foundation models possess strong semantic priors, their capabilities as zero-shot exploration agents in classic RL benchmarks are not well understood. We benchmark LLMs and VLMs on multi-armed bandits, Gridworlds, and sparse-reward Atari to test zero-shot exploration. Our investigation reveals a key limitation: while VLMs can infer high-level objectives from visual input, they consistently fail at precise low-level control: the "knowing-doing gap". To analyze a potential bridge for this gap, we investigate a simple on-policy hybrid framework in a controlled, best-case scenario. Our results in this idealized setting show that VLM guidance can significantly improve early-stage sample efficiency, providing a clear analysis of the potential and constraints of using foundation models to guide exploration rather than for end-to-end control.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An effective control of large systems of active particles: An application to evacuation problem</title>
<link>https://arxiv.org/abs/2509.19972</link>
<guid>https://arxiv.org/abs/2509.19972</guid>
<content:encoded><![CDATA[
arXiv:2509.19972v1 Announce Type: new 
Abstract: Manipulation of large systems of active particles is a serious challenge across diverse domains, including crowd management, control of robotic swarms, and coordinated material transport. The development of advanced control strategies for complex scenarios is hindered, however, by the lack of scalability and robustness of the existing methods, in particular, due to the need of an individual control for each agent. One possible solution involves controlling a system through a leader or a group of leaders, which other agents tend to follow. Using such an approach we develop an effective control strategy for a leader, combining reinforcement learning (RL) with artificial forces acting on the system. To describe the guidance of active particles by a leader we introduce the generalized Vicsek model. This novel method is then applied to the problem of the effective evacuation by a robot-rescuer (leader) of large groups of people from hazardous places. We demonstrate, that while a straightforward application of RL yields suboptimal results, even for advanced architectures, our approach provides a robust and efficient evacuation strategy. The source code supporting this study is publicly available at: https://github.com/cinemere/evacuation.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Knowledge-Behaviour Disconnect in LLM-based Chatbots</title>
<link>https://arxiv.org/abs/2509.20004</link>
<guid>https://arxiv.org/abs/2509.20004</guid>
<content:encoded><![CDATA[
arXiv:2509.20004v1 Announce Type: new 
Abstract: Large language model-based artificial conversational agents (like ChatGPT) give answers to all kinds of questions, and often enough these answers are correct. Just on the basis of that capacity alone, we may attribute knowledge to them. But do these models use this knowledge as a basis for their own conversational behaviour? I argue this is not the case, and I will refer to this failure as a `disconnect'. I further argue this disconnect is fundamental in the sense that with more data and more training of the LLM on which a conversational chatbot is based, it will not disappear. The reason is, as I will claim, that the core technique used to train LLMs does not allow for the establishment of the connection we are after. The disconnect reflects a fundamental limitation on the capacities of LLMs, and explains the source of hallucinations. I will furthermore consider the ethical version of the disconnect (ethical conversational knowledge not being aligned with ethical conversational behaviour), since in this domain researchers have come up with several additional techniques to influence a chatbot's behaviour. I will discuss how these techniques do nothing to solve the disconnect and can make it worse.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MACD: Multi-Agent Clinical Diagnosis with Self-Learned Knowledge for LLM</title>
<link>https://arxiv.org/abs/2509.20067</link>
<guid>https://arxiv.org/abs/2509.20067</guid>
<content:encoded><![CDATA[
arXiv:2509.20067v2 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated notable potential in medical applications, yet they face substantial challenges in handling complex real-world clinical diagnoses using conventional prompting methods. Current prompt engineering and multi-agent approaches typically optimize isolated inferences, neglecting the accumulation of reusable clinical experience. To address this, this study proposes a novel Multi-Agent Clinical Diagnosis (MACD) framework, which allows LLMs to self-learn clinical knowledge via a multi-agent pipeline that summarizes, refines, and applies diagnostic insights. It mirrors how physicians develop expertise through experience, enabling more focused and accurate diagnosis on key disease-specific cues. We further extend it to a MACD-human collaborative workflow, where multiple LLM-based diagnostician agents engage in iterative consultations, supported by an evaluator agent and human oversight for cases where agreement is not reached. Evaluated on 4,390 real-world patient cases across seven diseases using diverse open-source LLMs (Llama-3.1 8B/70B, DeepSeek-R1-Distill-Llama 70B), MACD significantly improves primary diagnostic accuracy, outperforming established clinical guidelines with gains up to 22.3% (MACD). On the subset of the data, it achieves performance on par with or exceeding that of human physicians (up to 16% improvement over physicians-only diagnosis). Additionally, on the MACD-human workflow, it achieves an 18.6% improvement compared to physicians-only diagnosis. Moreover, self-learned knowledge exhibits strong cross-model stability, transferability, and model-specific personalization, while the system can generate traceable rationales, enhancing explainability. Consequently, this work presents a scalable self-learning paradigm for LLM-assisted diagnosis, bridging the gap between the intrinsic knowledge of LLMs and real-world clinical practice.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Koopman Operator Learning from Sequential Observations</title>
<link>https://arxiv.org/abs/2509.20071</link>
<guid>https://arxiv.org/abs/2509.20071</guid>
<content:encoded><![CDATA[
arXiv:2509.20071v1 Announce Type: new 
Abstract: This paper presents a distributed Koopman operator learning framework for modeling unknown nonlinear dynamics using sequential observations from multiple agents. Each agent estimates a local Koopman approximation based on lifted data and collaborates over a communication graph to reach exponential consensus on a consistent distributed approximation. The approach supports distributed computation under asynchronous and resource-constrained sensing. Its performance is demonstrated through simulation results, validating convergence and predictive accuracy under sensing-constrained scenarios and limited communication.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Safety Verification of Multi-Agent Systems using $\psi$-Weighted CBFs and PAC Guarantees</title>
<link>https://arxiv.org/abs/2509.20093</link>
<guid>https://arxiv.org/abs/2509.20093</guid>
<content:encoded><![CDATA[
arXiv:2509.20093v1 Announce Type: new 
Abstract: This study proposes a hybrid safety verification framework for closed-loop multi-agent systems under bounded stochastic disturbances. The proposed approach augments control barrier functions with a novel $\psi$-weighted formulation that encodes directional control alignment between agents into the safety constraints. Deterministic admissibility is combined with empirical validation via Monte Carlo rollouts, and a PAC-style guarantee is derived based on margin-aware safety violations to provide a probabilistic safety certificate. The results from the experiments conducted under different bounded stochastic disturbances validate the feasibility of the proposed approach.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Pheromones to Policies: Reinforcement Learning for Engineered Biological Swarms</title>
<link>https://arxiv.org/abs/2509.20095</link>
<guid>https://arxiv.org/abs/2509.20095</guid>
<content:encoded><![CDATA[
arXiv:2509.20095v1 Announce Type: new 
Abstract: Swarm intelligence emerges from decentralised interactions among simple agents, enabling collective problem-solving. This study establishes a theoretical equivalence between pheromone-mediated aggregation in \celeg\ and reinforcement learning (RL), demonstrating how stigmergic signals function as distributed reward mechanisms. We model engineered nematode swarms performing foraging tasks, showing that pheromone dynamics mathematically mirror cross-learning updates, a fundamental RL algorithm. Experimental validation with data from literature confirms that our model accurately replicates empirical \celeg\ foraging patterns under static conditions. In dynamic environments, persistent pheromone trails create positive feedback loops that hinder adaptation by locking swarms into obsolete choices. Through computational experiments in multi-armed bandit scenarios, we reveal that introducing a minority of exploratory agents insensitive to pheromones restores collective plasticity, enabling rapid task switching. This behavioural heterogeneity balances exploration-exploitation trade-offs, implementing swarm-level extinction of outdated strategies. Our results demonstrate that stigmergic systems inherently encode distributed RL processes, where environmental signals act as external memory for collective credit assignment. By bridging synthetic biology with swarm robotics, this work advances programmable living systems capable of resilient decision-making in volatile environments.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federation of Agents: A Semantics-Aware Communication Fabric for Large-Scale Agentic AI</title>
<link>https://arxiv.org/abs/2509.20175</link>
<guid>https://arxiv.org/abs/2509.20175</guid>
<content:encoded><![CDATA[
arXiv:2509.20175v1 Announce Type: new 
Abstract: We present Federation of Agents (FoA), a distributed orchestration framework that transforms static multi-agent coordination into dynamic, capability-driven collaboration. FoA introduces Versioned Capability Vectors (VCVs): machine-readable profiles that make agent capabilities searchable through semantic embeddings, enabling agents to advertise their capabilities, cost, and limitations. Our aarchitecturecombines three key innovations: (1) semantic routing that matches tasks to agents over sharded HNSW indices while enforcing operational constraints through cost-biased optimization, (2) dynamic task decomposition where compatible agents collaboratively break down complex tasks into DAGs of subtasks through consensus-based merging, and (3) smart clustering that groups agents working on similar subtasks into collaborative channels for k-round refinement before synthesis. Built on top of MQTT,s publish-subscribe semantics for scalable message passing, FoA achieves sub-linear complexity through hierarchical capability matching and efficient index maintenance. Evaluation on HealthBench shows 13x improvements over single-model baselines, with clustering-enhanced laboration particularly effective for complex reasoning tasks requiring multiple perspectives. The system scales horizontally while maintaining consistent performance, demonstrating that semantic orchestration with structured collaboration can unlock the collective intelligence of heterogeneous federations of AI agents.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Multi-Agent Workflows for RTL Design</title>
<link>https://arxiv.org/abs/2509.20182</link>
<guid>https://arxiv.org/abs/2509.20182</guid>
<content:encoded><![CDATA[
arXiv:2509.20182v1 Announce Type: new 
Abstract: The rise of agentic AI workflows unlocks novel opportunities for computer systems design and optimization. However, for specialized domains such as program synthesis, the relative scarcity of HDL and proprietary EDA resources online compared to more common programming tasks introduces challenges, often necessitating task-specific fine-tuning, high inference costs, and manually-crafted agent orchestration. In this work, we present VeriMaAS, a multi-agent framework designed to automatically compose agentic workflows for RTL code generation. Our key insight is to integrate formal verification feedback from HDL tools directly into workflow generation, reducing the cost of gradient-based updates or prolonged reasoning traces. Our method improves synthesis performance by 5-7% for pass@k over fine-tuned baselines, while requiring only a few hundred training examples, representing an order-of-magnitude reduction in supervision cost.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy Use of AI Inference: Efficiency Pathways and Test-Time Compute</title>
<link>https://arxiv.org/abs/2509.20241</link>
<guid>https://arxiv.org/abs/2509.20241</guid>
<content:encoded><![CDATA[
arXiv:2509.20241v1 Announce Type: new 
Abstract: As AI inference scales to billions of queries and emerging reasoning and agentic workflows increase token demand, reliable estimates of per-query energy use are increasingly important for capacity planning, emissions accounting, and efficiency prioritization. Many public estimates are inconsistent and overstate energy use, because they extrapolate from limited benchmarks and fail to reflect efficiency gains achievable at scale. In this perspective, we introduce a bottom-up methodology to estimate the per-query energy of large-scale LLM systems based on token throughput. For models running on an H100 node under realistic workloads, GPU utilization and PUE constraints, we estimate a median energy per query of 0.34 Wh (IQR: 0.18-0.67) for frontier-scale models (>200 billion parameters). These results are consistent with measurements using production-scale configurations and show that non-production estimates and assumptions can overstate energy use by 4-20x. Extending to test-time scaling scenarios with 15x more tokens per typical query, the median energy rises 13x to 4.32 Wh, indicating that targeting efficiency in this regime will deliver the largest fleet-wide savings. We quantify achievable efficiency gains at the model, serving platform, and hardware levels, finding individual median reductions of 1.5-3.5x in energy per query, while combined advances can plausibly deliver 8-20x reductions. To illustrate the system-level impact, we estimate the baseline daily energy use of a deployment serving 1 billion queries to be 0.8 GWh/day. If 10% are long queries, demand could grow to 1.8 GWh/day. With targeted efficiency interventions, it falls to 0.9 GWh/day, similar to the energy footprint of web search at that scale. This echoes how data centers historically tempered energy growth through efficiency gains during the internet and cloud build-up.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scan-do Attitude: Towards Autonomous CT Protocol Management using a Large Language Model Agent</title>
<link>https://arxiv.org/abs/2509.20270</link>
<guid>https://arxiv.org/abs/2509.20270</guid>
<content:encoded><![CDATA[
arXiv:2509.20270v1 Announce Type: new 
Abstract: Managing scan protocols in Computed Tomography (CT), which includes adjusting acquisition parameters or configuring reconstructions, as well as selecting postprocessing tools in a patient-specific manner, is time-consuming and requires clinical as well as technical expertise. At the same time, we observe an increasing shortage of skilled workforce in radiology. To address this issue, a Large Language Model (LLM)-based agent framework is proposed to assist with the interpretation and execution of protocol configuration requests given in natural language or a structured, device-independent format, aiming to improve the workflow efficiency and reduce technologists' workload. The agent combines in-context-learning, instruction-following, and structured toolcalling abilities to identify relevant protocol elements and apply accurate modifications. In a systematic evaluation, experimental results indicate that the agent can effectively retrieve protocol components, generate device compatible protocol definition files, and faithfully implement user requests. Despite demonstrating feasibility in principle, the approach faces limitations regarding syntactic and semantic validity due to lack of a unified device API, and challenges with ambiguous or complex requests. In summary, the findings show a clear path towards LLM-based agents for supporting scan protocol management in CT imaging.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A co-evolving agentic AI system for medical imaging analysis</title>
<link>https://arxiv.org/abs/2509.20279</link>
<guid>https://arxiv.org/abs/2509.20279</guid>
<content:encoded><![CDATA[
arXiv:2509.20279v1 Announce Type: new 
Abstract: Agentic AI is rapidly advancing in healthcare and biomedical research. However, in medical image analysis, their performance and adoption remain limited due to the lack of a robust ecosystem, insufficient toolsets, and the absence of real-time interactive expert feedback. Here we present "TissueLab", a co-evolving agentic AI system that allows researchers to ask direct questions, automatically plan and generate explainable workflows, and conduct real-time analyses where experts can visualize intermediate results and refine them. TissueLab integrates tool factories across pathology, radiology, and spatial omics domains. By standardizing inputs, outputs, and capabilities of diverse tools, the system determines when and how to invoke them to address research and clinical questions. Across diverse tasks with clinically meaningful quantifications that inform staging, prognosis, and treatment planning, TissueLab achieves state-of-the-art performance compared with end-to-end vision-language models (VLMs) and other agentic AI systems such as GPT-5. Moreover, TissueLab continuously learns from clinicians, evolving toward improved classifiers and more effective decision strategies. With active learning, it delivers accurate results in unseen disease contexts within minutes, without requiring massive datasets or prolonged retraining. Released as a sustainable open-source ecosystem, TissueLab aims to accelerate computational research and translational adoption in medical imaging while establishing a foundation for the next generation of medical AI.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PGCLODA: Prompt-Guided Graph Contrastive Learning for Oligopeptide-Infectious Disease Association Prediction</title>
<link>https://arxiv.org/abs/2509.20290</link>
<guid>https://arxiv.org/abs/2509.20290</guid>
<content:encoded><![CDATA[
arXiv:2509.20290v1 Announce Type: new 
Abstract: Infectious diseases continue to pose a serious threat to public health, underscoring the urgent need for effective computational approaches to screen novel anti-infective agents. Oligopeptides have emerged as promising candidates in antimicrobial research due to their structural simplicity, high bioavailability, and low susceptibility to resistance. Despite their potential, computational models specifically designed to predict associations between oligopeptides and infectious diseases remain scarce. This study introduces a prompt-guided graph-based contrastive learning framework (PGCLODA) to uncover potential associations. A tripartite graph is constructed with oligopeptides, microbes, and diseases as nodes, incorporating both structural and semantic information. To preserve critical regions during contrastive learning, a prompt-guided graph augmentation strategy is employed to generate meaningful paired views. A dual encoder architecture, integrating Graph Convolutional Network (GCN) and Transformer, is used to jointly capture local and global features. The fused embeddings are subsequently input into a multilayer perceptron (MLP) classifier for final prediction. Experimental results on a benchmark dataset indicate that PGCLODA consistently outperforms state-of-the-art models in AUROC, AUPRC, and accuracy. Ablation and hyperparameter studies confirm the contribution of each module. Case studies further validate the generalization ability of PGCLODA and its potential to uncover novel, biologically relevant associations. These findings offer valuable insights for mechanism-driven discovery and oligopeptide-based drug development. The source code of PGCLODA is available online at https://github.com/jjnlcode/PGCLODA.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Robustness of Consensus over Pseudo-Undirected Path Graphs</title>
<link>https://arxiv.org/abs/2509.20314</link>
<guid>https://arxiv.org/abs/2509.20314</guid>
<content:encoded><![CDATA[
arXiv:2509.20314v1 Announce Type: new 
Abstract: Consensus over networked agents is typically studied using undirected or directed communication graphs. Undirected graphs enforce symmetry in information exchange, leading to convergence to the average of initial states, while directed graphs permit asymmetry but make consensus dependent on root nodes and their influence. Both paradigms impose inherent restrictions on achievable consensus values and network robustness. This paper introduces a theoretical framework for achieving consensus over a class of network topologies, termed pseudo-undirected graphs, which retains bidirectional connectivity between node pairs but allows the corresponding edge weights to differ, including the possibility of negative values under bounded conditions. The resulting Laplacian is generally non-symmetric, yet it guarantees consensus under connectivity assumptions, to expand the solution space, which enables the system to achieve a stable consensus value that can lie outside the convex hull of the initial state set. We derive admissibility bounds for negative weights for a pseudo-undirected path graph, and show an application in the simultaneous interception of a moving target.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRES: Benchmarking LLMs for Disfluency Removal</title>
<link>https://arxiv.org/abs/2509.20321</link>
<guid>https://arxiv.org/abs/2509.20321</guid>
<content:encoded><![CDATA[
arXiv:2509.20321v1 Announce Type: new 
Abstract: Disfluencies -- such as "um," "uh," interjections, parentheticals, and edited statements -- remain a persistent challenge for speech-driven systems, degrading accuracy in command interpretation, summarization, and conversational agents. We introduce DRES (Disfluency Removal Evaluation Suite), a controlled text-level benchmark that establishes a reproducible semantic upper bound for this task. DRES builds on human-annotated Switchboard transcripts, isolating disfluency removal from ASR errors and acoustic variability. We systematically evaluate proprietary and open-source LLMs across scales, prompting strategies, and architectures. Our results reveal that (i) simple segmentation consistently improves performance, even for long-context models; (ii) reasoning-oriented models tend to over-delete fluent tokens; and (iii) fine-tuning achieves near state-of-the-art precision and recall but harms generalization abilities. We further present a set of LLM-specific error modes and offer nine practical recommendations (R1-R9) for deploying disfluency removal in speech-driven pipelines. DRES provides a reproducible, model-agnostic foundation for advancing robust spoken-language systems.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Event-Triggered Policy Gradient for Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.20338</link>
<guid>https://arxiv.org/abs/2509.20338</guid>
<content:encoded><![CDATA[
arXiv:2509.20338v1 Announce Type: new 
Abstract: Conventional multi-agent reinforcement learning (MARL) methods rely on time-triggered execution, where agents sample and communicate actions at fixed intervals. This approach is often computationally expensive and communication-intensive. To address this limitation, we propose ET-MAPG (Event-Triggered Multi-Agent Policy Gradient reinforcement learning), a framework that jointly learns an agent's control policy and its event-triggering policy. Unlike prior work that decouples these mechanisms, ET-MAPG integrates them into a unified learning process, enabling agents to learn not only what action to take but also when to execute it. For scenarios with inter-agent communication, we introduce AET-MAPG, an attention-based variant that leverages a self-attention mechanism to learn selective communication patterns. AET-MAPG empowers agents to determine not only when to trigger an action but also with whom to communicate and what information to exchange, thereby optimizing coordination. Both methods can be integrated with any policy gradient MARL algorithm. Extensive experiments across diverse MARL benchmarks demonstrate that our approaches achieve performance comparable to state-of-the-art, time-triggered baselines while significantly reducing both computational load and communication overhead.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Channel Estimation and Computation Offloading in Fluid Antenna-assisted MEC Networks</title>
<link>https://arxiv.org/abs/2509.19340</link>
<guid>https://arxiv.org/abs/2509.19340</guid>
<content:encoded><![CDATA[
arXiv:2509.19340v1 Announce Type: cross 
Abstract: With the emergence of fluid antenna (FA) in wireless communications, the capability to dynamically adjust port positions offers substantial benefits in spatial diversity and spectrum efficiency, which are particularly valuable for mobile edge computing (MEC) systems. Therefore, we propose an FA-assisted MEC offloading framework to minimize system delay. This framework faces two severe challenges, which are the complexity of channel estimation due to dynamic port configuration and the inherent non-convexity of the joint optimization problem. Firstly, we propose Information Bottleneck Metric-enhanced Channel Compressed Sensing (IBM-CCS), which advances FA channel estimation by integrating information relevance into the sensing process and capturing key features of FA channels effectively. Secondly, to address the non-convex and high-dimensional optimization problem in FA-assisted MEC systems, which includes FA port selection, beamforming, power control, and resource allocation, we propose a game theory-assisted Hierarchical Twin-Dueling Multi-agent Algorithm (HiTDMA) based offloading scheme, where the hierarchical structure effectively decouples and coordinates the optimization tasks between the user side and the base station side. Crucially, the game theory effectively reduces the dimensionality of power control variables, allowing deep reinforcement learning (DRL) agents to achieve improved optimization efficiency. Numerical results confirm that the proposed scheme significantly reduces system delay and enhances offloading performance, outperforming benchmarks. Additionally, the IBM-CCS channel estimation demonstrates superior accuracy and robustness under varying port densities, contributing to efficient communication under imperfect CSI.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tree Search for Language Model Agents</title>
<link>https://arxiv.org/abs/2407.01476</link>
<guid>https://arxiv.org/abs/2407.01476</guid>
<content:encoded><![CDATA[
arXiv:2407.01476v3 Announce Type: replace 
Abstract: Autonomous agents powered by language models (LMs) have demonstrated promise in their ability to perform decision-making tasks such as web automation. However, a key limitation remains: LMs, primarily optimized for natural language understanding and generation, struggle with multi-step reasoning, planning, and using environmental feedback when attempting to solve realistic computer tasks. Towards addressing this, we propose an inference-time search algorithm for LM agents to explicitly perform exploration and multi-step planning in interactive web environments. Our approach is a form of best-first tree search that operates within the actual environment space, and is complementary with most existing state-of-the-art agents. It is the first tree search algorithm for LM agents that shows effectiveness on realistic web tasks. On the challenging VisualWebArena benchmark, applying our search algorithm on top of a GPT-4o agent yields a 39.7% relative increase in success rate compared to the same baseline without search, setting a state-of-the-art success rate of 26.4%. On WebArena, search also yields a 28.0% relative improvement over a baseline agent, setting a competitive success rate of 19.2%. Our experiments highlight the effectiveness of search for web agents, and we demonstrate that performance scales with increased test-time compute. We conduct a thorough analysis of our results to highlight improvements from search, limitations, and promising directions for future work. Our code and models are publicly released at https://jykoh.com/search-agents.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning and Machine ethics:a systematic review</title>
<link>https://arxiv.org/abs/2407.02425</link>
<guid>https://arxiv.org/abs/2407.02425</guid>
<content:encoded><![CDATA[
arXiv:2407.02425v2 Announce Type: replace 
Abstract: Machine ethics is the field that studies how ethical behaviour can be accomplished by autonomous systems. While there exist some systematic reviews aiming to consolidate the state of the art in machine ethics prior to 2020, these tend to not include work that uses reinforcement learning agents as entities whose ethical behaviour is to be achieved. The reason for this is that only in the last years we have witnessed an increase in machine ethics studies within reinforcement learning. We present here a systematic review of reinforcement learning for machine ethics and machine ethics within reinforcement learning. Additionally, we highlight trends in terms of ethics specifications, components and frameworks of reinforcement learning, and environments used to result in ethical behaviour. Our systematic review aims to consolidate the work in machine ethics and reinforcement learning thus completing the gap in the state of the art machine ethics landscape
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimum Time Consensus of Multi-agent System under Fuel Constraints</title>
<link>https://arxiv.org/abs/2407.19927</link>
<guid>https://arxiv.org/abs/2407.19927</guid>
<content:encoded><![CDATA[
arXiv:2407.19927v3 Announce Type: replace 
Abstract: This work addresses the problem of finding minimum time consensus point in the state space for a set of $N$ identical double integrator agents with bounded inputs and fixed fuel budget constraint. To address the problem, characterization of the attainable set for each agent subject to bounded inputs and fixed fuel budget constraints is done. Such attainable set is shown to be a convex set. The minimum time to consensus is the least time when the attainable sets of all agents intersect and the corresponding consensus state is the point of intersection. Using Helly's theorem, it is shown that the intersection is not empty at the time when all triplets of agents exhibit a non-empty intersection. Thus, a closed-form expression for the minimum time to consensus for a triplet of agents is obtained. The calculation of minimum time consensus for each of the triplets is performed independently and is distributed evenly among $N$ agents. The overall minimum time to consensus of $N$ agents is then given by the triplet that has the greatest minimum time to consensus. In the process, the set of initial conditions for agents from which consensus is possible under these input and fuel budget constraints is also characterized.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MVVM: Deploy Your AI Agents-Securely, Efficiently, Everywhere</title>
<link>https://arxiv.org/abs/2410.15894</link>
<guid>https://arxiv.org/abs/2410.15894</guid>
<content:encoded><![CDATA[
arXiv:2410.15894v2 Announce Type: replace 
Abstract: The rise of AI agents powered by Large Language Models (LLMs) presents critical challenges: how to securely execute and migrate these agents across heterogeneous environments while protecting sensitive user data, maintaining availability during network failures, minimizing response latency for time-critical decisions, and ensuring output safety in mission-critical applications. We present MVVM, a WebAssembly-based secure container framework that enables transparent live migration of LLM agent workspaces between edge devices and cloud servers with end-to-end privacy guarantees, resilient multi-tier replication, speculative execution for latency optimization, and integrated validation for safety assurance. MVVM introduces two key innovations: (1) a two-way sandboxing framework leveraging hardware enclaves and accelerator extensions that protects both the agent from malicious hosts and the host from compromised agents; (2) an efficient cross platform migration mechanism using WebAssembly and WASI's platform-agnostic design, enabling seamless movement across ARM phones, RISC-V MCUs, x86 servers, and heterogeneous accelerators; and three astonishing use cases: (1) privacy-aware daemon that automatically determines whether to execute locally or remotely based on data sensitivity and resource availability; (2) multi-tier replication with intelligent quality degradation that maintains service availability despite network failures or resource constraints; (3) a comprehensive execution framework combining speculative execution for 10x latency reduction with parallel validation that ensures output safety without compromising responsiveness. Our evaluation demonstrates that MVVM is validated on three separate devices across 18 workloads.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agents are Social Groups: Investigating Social Influence of Multiple Agents in Human-Agent Interactions</title>
<link>https://arxiv.org/abs/2411.04578</link>
<guid>https://arxiv.org/abs/2411.04578</guid>
<content:encoded><![CDATA[
arXiv:2411.04578v2 Announce Type: replace 
Abstract: Multi-agent systems - systems with multiple independent AI agents working together to achieve a common goal - are becoming increasingly prevalent in daily life. Drawing inspiration from the phenomenon of human group social influence, we investigate whether a group of AI agents can create social pressure on users to agree with them, potentially changing their stance on a topic. We conducted a study in which participants discussed social issues with either a single or multiple AI agents, and where the agents either agreed or disagreed with the user's stance on the topic. We found that conversing with multiple agents (holding conversation content constant) increased the social pressure felt by participants, and caused a greater shift in opinion towards the agents' stances on each topic. Our study shows the potential advantages of multi-agent systems over single-agent platforms in causing opinion change. We discuss design implications for possible multi-agent systems that promote social good, as well as the potential for malicious actors to use these systems to manipulate public opinion.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering</title>
<link>https://arxiv.org/abs/2412.14480</link>
<guid>https://arxiv.org/abs/2412.14480</guid>
<content:encoded><![CDATA[
arXiv:2412.14480v2 Announce Type: replace 
Abstract: In Embodied Question Answering (EQA), agents must explore and develop a semantic understanding of an unseen environment to answer a situated question with confidence. This problem remains challenging in robotics, due to the difficulties in obtaining useful semantic representations, updating these representations online, and leveraging prior world knowledge for efficient planning and exploration. To address these limitations, we propose GraphEQA, a novel approach that utilizes real-time 3D metric-semantic scene graphs (3DSGs) and task relevant images as multi-modal memory for grounding Vision-Language Models (VLMs) to perform EQA tasks in unseen environments. We employ a hierarchical planning approach that exploits the hierarchical nature of 3DSGs for structured planning and semantics-guided exploration. We evaluate GraphEQA in simulation on two benchmark datasets, HM-EQA and OpenEQA, and demonstrate that it outperforms key baselines by completing EQA tasks with higher success rates and fewer planning steps. We further demonstrate GraphEQA in multiple real-world home and office environments.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BAP v2: An Enhanced Task Framework for Instruction Following in Minecraft Dialogues</title>
<link>https://arxiv.org/abs/2501.10836</link>
<guid>https://arxiv.org/abs/2501.10836</guid>
<content:encoded><![CDATA[
arXiv:2501.10836v3 Announce Type: replace 
Abstract: Developing interactive agents that can understand language, perceive their surroundings, and act within the physical world is a long-standing goal of AI research. The Minecraft Collaborative Building Task (MCBT) (Narayan-Chen, Jayannavar, and Hockenmaier 2019), a two-player game in which an Architect (A) instructs a Builder (B) to construct a target structure in a simulated 3D Blocks World environment, offers a rich platform to work towards this goal. In this work, we focus on the Builder Action Prediction (BAP) subtask: predicting B's actions in a multimodal game context (Jayannavar, Narayan-Chen, and Hockenmaier 2020) - a challenging testbed for grounded instruction following, with limited training data. We holistically re-examine this task and introduce BAP v2 to address key challenges in evaluation, training data, and modeling. Specifically, we define an enhanced evaluation benchmark, featuring a cleaner test set and fairer, more insightful metrics that also reveal spatial reasoning as the primary performance bottleneck. To address data scarcity and to teach models basic spatial skills, we generate different types of synthetic MCBT data. We observe that current, LLM-based SOTA models trained on the human BAP dialogues fail on these simpler, synthetic BAP ones, but show that training models on this synthetic data improves their performance across the board. We also introduce a new SOTA model, Llama-CRAFTS, which leverages richer input representations, and achieves an F1 score of 53.0 on the BAP v2 task and strong performance on the synthetic data. While this result marks a notable 6 points improvement over previous work, it also underscores the task's remaining difficulty, establishing BAP v2 as a fertile ground for future research, and providing a useful measure of the spatial capabilities of current text-only LLMs in such embodied tasks.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RG-Attn: Radian Glue Attention for Multi-modality Multi-agent Cooperative Perception</title>
<link>https://arxiv.org/abs/2501.16803</link>
<guid>https://arxiv.org/abs/2501.16803</guid>
<content:encoded><![CDATA[
arXiv:2501.16803v3 Announce Type: replace 
Abstract: Cooperative perception enhances autonomous driving by leveraging Vehicle-to-Everything (V2X) communication for multi-agent sensor fusion. However, most existing methods rely on single-modal data sharing, limiting fusion performance, particularly in heterogeneous sensor settings involving both LiDAR and cameras across vehicles and roadside units (RSUs). To address this, we propose Radian Glue Attention (RG-Attn), a lightweight and generalizable cross-modal fusion module that unifies intra-agent and inter-agent fusion via transformation-based coordinate alignment and a unified sampling/inversion strategy. RG-Attn efficiently aligns features through a radian-based attention constraint, operating column-wise on geometrically consistent regions to reduce overhead and preserve spatial coherence, thereby enabling accurate and robust fusion. Building upon RG-Attn, we propose three cooperative architectures. The first, Paint-To-Puzzle (PTP), prioritizes communication efficiency but assumes all agents have LiDAR, optionally paired with cameras. The second, Co-Sketching-Co-Coloring (CoS-CoCo), offers maximal flexibility, supporting any sensor setup (e.g., LiDAR-only, camera-only, or both) and enabling strong cross-modal generalization for real-world deployment. The third, Pyramid-RG-Attn Fusion (PRGAF), aims for peak detection accuracy with the highest computational overhead. Extensive evaluations on simulated and real-world datasets show our framework delivers state-of-the-art detection accuracy with high flexibility and efficiency. GitHub Link: https://github.com/LantaoLi/RG-Attn
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who is Responsible? The Data, Models, Users or Regulations? A Comprehensive Survey on Responsible Generative AI for a Sustainable Future</title>
<link>https://arxiv.org/abs/2502.08650</link>
<guid>https://arxiv.org/abs/2502.08650</guid>
<content:encoded><![CDATA[
arXiv:2502.08650v5 Announce Type: replace 
Abstract: Generative AI is moving rapidly from research into real world deployment across sectors, which elevates the need for responsible development, deployment, evaluation, and governance. To address this pressing challenge, in this study, we synthesize the landscape of responsible generative AI across methods, benchmarks, and policies, and connects governance expectations to concrete engineering practice. We follow a prespecified search and screening protocol focused on post-ChatGPT era with selective inclusion of foundational work for definitions, and we conduct a narrative and thematic synthesis. Three findings emerge; First, benchmark and practice coverage is dense for bias and toxicity but relatively sparse for privacy and provenance, deepfake and media integrity risk, and system level failure in tool using and agentic settings. Second, many evaluations remain static and task local, which limits evidence portability for audit and lifecycle assurance. Third, documentation and metric validity are inconsistent, which complicates comparison across releases and domains. We outline a research and practice agenda that prioritizes adaptive and multimodal evaluation, privacy and provenance testing, deepfake risk assessment, calibration and uncertainty reporting, versioned and documented artifacts, and continuous monitoring. Limitations include reliance on public artifacts and the focus period, which may under represent capabilities reported later. The survey offers a path to align development and evaluation with governance needs and to support safe, transparent, and accountable deployment across domains. Project page: https://anas-zafar.github.io/responsible-ai.github.io , GitHub: https://github.com/anas-zafar/Responsible-AI
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation</title>
<link>https://arxiv.org/abs/2502.13143</link>
<guid>https://arxiv.org/abs/2502.13143</guid>
<content:encoded><![CDATA[
arXiv:2502.13143v2 Announce Type: replace 
Abstract: While spatial reasoning has made progress in object localization relationships, it often overlooks object orientation-a key factor in 6-DoF fine-grained manipulation. Traditional pose representations rely on pre-defined frames or templates, limiting generalization and semantic grounding. In this paper, we introduce the concept of semantic orientation, which defines object orientations using natural language in a reference-frame-free manner (e.g., the "plug-in" direction of a USB or the "handle" direction of a cup). To support this, we construct OrienText300K, a large-scale dataset of 3D objects annotated with semantic orientations, and develop PointSO, a general model for zero-shot semantic orientation prediction. By integrating semantic orientation into VLM agents, our SoFar framework enables 6-DoF spatial reasoning and generates robotic actions. Extensive experiments demonstrated the effectiveness and generalization of our SoFar, e.g., zero-shot 48.7% successful rate on Open6DOR and zero-shot 74.9% successful rate on SIMPLER-Env.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Information Gaps with Comprehensive Answers: Improving the Diversity and Informativeness of Follow-Up Questions</title>
<link>https://arxiv.org/abs/2502.17715</link>
<guid>https://arxiv.org/abs/2502.17715</guid>
<content:encoded><![CDATA[
arXiv:2502.17715v2 Announce Type: replace 
Abstract: Generating diverse follow-up questions that uncover missing information remains challenging for conversational agents, particularly when they run on small, locally hosted models. To address this, we develop an information-gap-driven knowledge distillation pipeline in which a teacher LLM generates a comprehensive answer, contrasts it with the initial answer to identify information gaps, and formulates gap-bridging follow-up questions. Using this pipeline, we augment the existing FollowupQG dataset tenfold. We then fine-tune smaller student models on the augmented dataset to distill the teacher's knowledge. Experiments with selected teacher-student model pairs show that fine-tuned students achieve significantly higher informativeness and diversity than variations trained on the original dataset. These findings indicate that our pipeline, which mirrors the human cognitive process of information seeking, provides an efficient distillation channel from state-of-the-art LLMs to smaller models, enabling resource-constrained conversational systems to generate more diverse and informative follow-up questions.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoEval: A Practical Framework for Autonomous Evaluation of Mobile Agents</title>
<link>https://arxiv.org/abs/2503.02403</link>
<guid>https://arxiv.org/abs/2503.02403</guid>
<content:encoded><![CDATA[
arXiv:2503.02403v2 Announce Type: replace 
Abstract: Comprehensive evaluation of mobile agents can significantly advance their development and real-world applicability. However, existing benchmarks lack practicality and scalability due to the extensive manual effort in defining task reward signals and implementing evaluation codes. We propose AutoEval, an evaluation framework which tests mobile agents without any manual effort. Our approach designs a UI state change representation which can be used to automatically generate task reward signals, and employs a Judge System for autonomous evaluation. Evaluation shows AutoEval can automatically generate reward signals with high correlation to human-annotated signals, and achieve high accuracy (up to 94%) in autonomous evaluation comparable to human evaluation. Finally, we evaluate state-of-the-art mobile agents using our framework, providing insights into their performance and limitations.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Outlining: Heterogeneous Recursive Planning for Adaptive Long-form Writing with Language Models</title>
<link>https://arxiv.org/abs/2503.08275</link>
<guid>https://arxiv.org/abs/2503.08275</guid>
<content:encoded><![CDATA[
arXiv:2503.08275v3 Announce Type: replace 
Abstract: Long-form writing agents require flexible integration and interaction across information retrieval, reasoning, and composition. Current approaches rely on predefined workflows and rigid thinking patterns to generate outlines before writing, resulting in constrained adaptability during writing. In this paper we propose WriteHERE, a general agent framework that achieves human-like adaptive writing through recursive task decomposition and dynamic integration of three fundamental task types: retrieval, reasoning, and composition. Our methodology features: 1) a planning mechanism that interleaves recursive task decomposition and execution, eliminating artificial restrictions on writing workflow; and 2) integration of task types that facilitates heterogeneous task decomposition. Evaluations on both fiction writing and technical report generation show that our method consistently outperforms state-of-the-art approaches across all automatic evaluation metrics, demonstrating the effectiveness and broad applicability of our proposed framework. We have publicly released our code and prompts to facilitate further research.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Language Splatting</title>
<link>https://arxiv.org/abs/2503.09447</link>
<guid>https://arxiv.org/abs/2503.09447</guid>
<content:encoded><![CDATA[
arXiv:2503.09447v3 Announce Type: replace 
Abstract: To enable AI agents to interact seamlessly with both humans and 3D environments, they must not only perceive the 3D world accurately but also align human language with 3D spatial representations. While prior work has made significant progress by integrating language features into geometrically detailed 3D scene representations using 3D Gaussian Splatting (GS), these approaches rely on computationally intensive offline preprocessing of language features for each input image, limiting adaptability to new environments. In this work, we introduce Online Language Splatting, the first framework to achieve online, near real-time, open-vocabulary language mapping within a 3DGS-SLAM system without requiring pre-generated language features. The key challenge lies in efficiently fusing high-dimensional language features into 3D representations while balancing the computation speed, memory usage, rendering quality and open-vocabulary capability. To this end, we innovatively design: (1) a high-resolution CLIP embedding module capable of generating detailed language feature maps in 18ms per frame, (2) a two-stage online auto-encoder that compresses 768-dimensional CLIP features to 15 dimensions while preserving open-vocabulary capabilities, and (3) a color-language disentangled optimization approach to improve rendering quality. Experimental results show that our online method not only surpasses the state-of-the-art offline methods in accuracy but also achieves more than 40x efficiency boost, demonstrating the potential for dynamic and interactive AI applications.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Explainable Multi-agent MCTS-minimax Hybrids in Board Game Using Process Mining</title>
<link>https://arxiv.org/abs/2503.23326</link>
<guid>https://arxiv.org/abs/2503.23326</guid>
<content:encoded><![CDATA[
arXiv:2503.23326v3 Announce Type: replace 
Abstract: Monte-Carlo Tree Search (MCTS) is a family of sampling-based search algorithms widely used for online planning in sequential decision-making domains and at the heart of many recent advances in artificial intelligence. Understanding the behavior of MCTS agents is difficult for developers and users due to the frequently large and complex search trees that result from the simulation of many possible futures, their evaluations, and their relationships. This paper presents our ongoing investigation into potential explanations for the decision-making and behavior of MCTS. A weakness of MCTS is that it constructs a highly selective tree and, as a result, can miss crucial moves and fall into tactical traps. Full-width minimax search constitutes the solution. We integrate shallow minimax search into the rollout phase of multi-agent MCTS and use process mining technique to explain agents' strategies in 3v3 checkers.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChartQA-X: Generating Explanations for Visual Chart Reasoning</title>
<link>https://arxiv.org/abs/2504.13275</link>
<guid>https://arxiv.org/abs/2504.13275</guid>
<content:encoded><![CDATA[
arXiv:2504.13275v3 Announce Type: replace 
Abstract: The ability to explain complex information from chart images is vital for effective data-driven decision-making. In this work, we address the challenge of generating detailed explanations alongside answering questions about charts. We present ChartQA-X, a comprehensive dataset comprising 30,299 chart samples across four chart types, each paired with contextually relevant questions, answers, and explanations. Explanations are generated and selected based on metrics such as faithfulness, informativeness, coherence, and perplexity. Our human evaluation with 245 participants shows that model-generated explanations in ChartQA-X surpass human-written explanations in accuracy and logic and are comparable in terms of clarity and overall quality. Moreover, models fine-tuned on ChartQA-X show substantial improvements across various metrics, including absolute gains of up to 24.57 points in explanation quality, 18.96 percentage points in question-answering accuracy, and 14.75 percentage points on unseen benchmarks for the same task. By integrating explanatory narratives with answers, our approach enables agents to convey complex visual information more effectively, improving comprehension and greater trust in the generated responses.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meeseeks: A Feedback-Driven, Iterative Self-Correction Benchmark evaluating LLMs' Instruction Following Capability</title>
<link>https://arxiv.org/abs/2504.21625</link>
<guid>https://arxiv.org/abs/2504.21625</guid>
<content:encoded><![CDATA[
arXiv:2504.21625v5 Announce Type: replace 
Abstract: The capability to precisely adhere to instructions is a cornerstone for Large Language Models (LLMs) to function as dependable agents in real-world scenarios. However, confronted with complex prompts, LLMs frequently encounter difficulties in fulfilling all specified requirements within a single response. Drawing inspiration from recent advancements in Chain-of-Thought (CoT) prompting and self-correction methodologies, we introduce Meeseeks (The name is inspired by Mr. Meeseeks from "Rick and Morty," a character renowned for efficiently accomplishing assigned tasks. See: https://en.wikipedia.org/wiki/Mr._Meeseeks), a fully automated iterative instruction-following benchmark equipped with an integrated feedback mechanism. Meeseeks identifies erroneous components in model responses and provides corresponding feedback accurately, thereby iteratively guiding the model toward self-correction. The dataset contains over 700 curated instances annotated by 32 distinct capability tags in Chinese and English. Extensive experimental results reveal that different state-of-the-art commercial and open-source LLMs exhibit vastly disparate performance, and even after 20 turns of iterative feedback-driven self-correction, nearly all models demonstrate suboptimal performance. We conducted comprehensive analysis from both macro and instance levels, uncovering numerous common issues prevalent in current state-of-the-art models, as well as several counterintuitive phenomena. We've open-sourced our work on https://github.com/ADoublLEN/Meeseeks.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging</title>
<link>https://arxiv.org/abs/2505.09316</link>
<guid>https://arxiv.org/abs/2505.09316</guid>
<content:encoded><![CDATA[
arXiv:2505.09316v2 Announce Type: replace 
Abstract: Augmenting large language models (LLMs) with external retrieval has become a standard method to address their inherent knowledge cutoff limitations. However, traditional retrieval-augmented generation methods employ static, pre-inference retrieval strategies, making them inadequate for complex tasks involving ambiguous, multi-step, or evolving information needs. Recent advances in test-time scaling techniques have demonstrated significant potential in enabling LLMs to dynamically interact with external tools, motivating the shift toward adaptive inference-time retrieval. Inspired by Information Foraging Theory (IFT), we propose InForage, a reinforcement learning framework that formalizes retrieval-augmented reasoning as a dynamic information-seeking process. Unlike existing approaches, InForage explicitly rewards intermediate retrieval quality, encouraging LLMs to iteratively gather and integrate information through adaptive search behaviors. To facilitate training, we construct a human-guided dataset capturing iterative search and reasoning trajectories for complex, real-world web tasks. Extensive evaluations across general question answering, multi-hop reasoning tasks, and a newly developed real-time web QA dataset demonstrate InForage's superior performance over baseline methods. These results highlight InForage's effectiveness in building robust, adaptive, and efficient reasoning agents.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Risk Awareness in Rational Agents under Resource Constraints</title>
<link>https://arxiv.org/abs/2505.23436</link>
<guid>https://arxiv.org/abs/2505.23436</guid>
<content:encoded><![CDATA[
arXiv:2505.23436v4 Announce Type: replace 
Abstract: Advanced reasoning models with agentic capabilities (AI agents) are deployed to interact with humans and to solve sequential decision-making problems under (approximate) utility functions and internal models. When such problems have resource or failure constraints where action sequences may be forcibly terminated once resources are exhausted, agents face implicit trade-offs that reshape their utility-driven (rational) behaviour. Additionally, since these agents are typically commissioned by a human principal to act on their behalf, asymmetries in constraint exposure can give rise to previously unanticipated misalignment between human objectives and agent incentives. We formalise this setting through a survival bandit framework, provide theoretical and empirical results that quantify the impact of survival-driven preference shifts, identify conditions under which misalignment emerges and propose mechanisms to mitigate the emergence of risk-seeking or risk-averse behaviours. As a result, this work aims to increase understanding and interpretability of emergent behaviours of AI agents operating under such survival pressure, and offer guidelines for safely deploying such AI systems in critical resource-limited environments.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AURA: Autonomous Upskilling with Retrieval-Augmented Agents</title>
<link>https://arxiv.org/abs/2506.02507</link>
<guid>https://arxiv.org/abs/2506.02507</guid>
<content:encoded><![CDATA[
arXiv:2506.02507v2 Announce Type: replace 
Abstract: Designing reinforcement learning curricula for agile robots traditionally requires extensive manual tuning of reward functions, environment randomizations, and training configurations. We introduce AURA (Autonomous Upskilling with Retrieval-Augmented Agents), a schema-validated curriculum reinforcement learning (RL) framework that leverages Large Language Models (LLMs) as autonomous designers of multi-stage curricula. AURA transforms user prompts into YAML workflows that encode full reward functions, domain randomization strategies, and training configurations. All files are statically validated before any GPU time is used, ensuring efficient and reliable execution. A retrieval-augmented feedback loop allows specialized LLM agents to design, execute, and refine curriculum stages based on prior training results stored in a vector database, enabling continual improvement over time. Quantitative experiments show that AURA consistently outperforms LLM-guided baselines in generation success rate, humanoid locomotion, and manipulation tasks. Ablation studies highlight the importance of schema validation and retrieval for curriculum quality. AURA successfully trains end-to-end policies directly from user prompts and deploys them zero-shot on a custom humanoid robot in multiple environments - capabilities that did not exist previously with manually designed controllers. By abstracting the complexity of curriculum design, AURA enables scalable and adaptive policy learning pipelines that would be complex to construct by hand.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Procedural Environment Generation for Tool-Use Agents</title>
<link>https://arxiv.org/abs/2506.11045</link>
<guid>https://arxiv.org/abs/2506.11045</guid>
<content:encoded><![CDATA[
arXiv:2506.11045v2 Announce Type: replace 
Abstract: Although the power of LLM tool-use agents has ignited a flurry of recent research in this area, the curation of tool-use training data remains an open problem$-$especially for online RL training. Existing approaches to synthetic tool-use data generation tend to be non-interactive, and/or non-compositional. We introduce RandomWorld, a pipeline for the procedural generation of interactive tools and compositional tool-use data. We show that models tuned via SFT and RL on synthetic RandomWorld data improve on a range of tool-use benchmarks, and set the new SoTA for two metrics on the NESTFUL dataset. Further experiments show that downstream performance scales with the amount of RandomWorld-generated training data, opening up the possibility of further improvement through the use of entirely synthetic data.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Augmenting Multi-Agent Communication with State Delta Trajectory</title>
<link>https://arxiv.org/abs/2506.19209</link>
<guid>https://arxiv.org/abs/2506.19209</guid>
<content:encoded><![CDATA[
arXiv:2506.19209v2 Announce Type: replace 
Abstract: Multi-agent techniques such as role playing or multi-turn debates have been shown to be effective in improving the performance of large language models (LLMs) in downstream tasks. Despite their differences in workflows, existing multi-agent systems constructed from a single base LLM mostly use natural language for agent communication. While this is appealing for its simplicity and interpretability, it also introduces inevitable information loss as one model must down sample its continuous state vectors to discrete tokens before transferring them to the other model. Such losses are particularly significant when the information to transfer is not simple facts, but reasoning logics or abstractive thoughts. To tackle this problem, we propose a new communication protocol that transfers both natural language tokens and token-wise state transition trajectory from one agent to another. Particularly, compared to the actual state value, we find that the sequence of state changes in LLMs after generating each token can better reflect the information hidden behind the inference process. We propose a State Delta Encoding (SDE) method to represent state transition trajectories. The experimental results show that multi-agent systems with SDE achieve SOTA performance compared to other communication protocols, particularly in tasks that involve complex reasoning.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation</title>
<link>https://arxiv.org/abs/2507.06899</link>
<guid>https://arxiv.org/abs/2507.06899</guid>
<content:encoded><![CDATA[
arXiv:2507.06899v2 Announce Type: replace 
Abstract: Graphical User Interface (GUI) agents powered by Large Vision-Language Models (LVLMs) have emerged as a revolutionary approach to automating human-machine interactions, capable of autonomously operating personal devices (e.g., mobile phones) or applications within the device to perform complex real-world tasks in a human-like manner. However, their close integration with personal devices raises significant security concerns, with many threats, including backdoor attacks, remaining largely unexplored. This work reveals that the visual grounding of GUI agent-mapping textual plans to GUI elements-can introduce vulnerabilities, enabling new types of backdoor attacks. With backdoor attack targeting visual grounding, the agent's behavior can be compromised even when given correct task-solving plans. To validate this vulnerability, we propose VisualTrap, a method that can hijack the grounding by misleading the agent to locate textual plans to trigger locations instead of the intended targets. VisualTrap uses the common method of injecting poisoned data for attacks, and does so during the pre-training of visual grounding to ensure practical feasibility of attacking. Empirical results show that VisualTrap can effectively hijack visual grounding with as little as 5% poisoned data and highly stealthy visual triggers (invisible to the human eye); and the attack can be generalized to downstream tasks, even after clean fine-tuning. Moreover, the injected trigger can remain effective across different GUI environments, e.g., being trained on mobile/web and generalizing to desktop environments. These findings underscore the urgent need for further research on backdoor attack risks in GUI agents.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Harmfulness of Computer-Using Agents</title>
<link>https://arxiv.org/abs/2508.00935</link>
<guid>https://arxiv.org/abs/2508.00935</guid>
<content:encoded><![CDATA[
arXiv:2508.00935v2 Announce Type: replace 
Abstract: Computer-using agents (CUAs), which can autonomously control computers to perform multi-step actions, might pose significant safety risks if misused. However, existing benchmarks mainly evaluate LMs in chatbots or simple tool use. To more comprehensively evaluate CUAs' misuse risks, we introduce a new benchmark: CUAHarm. CUAHarm consists of 104 expert-written realistic misuse risks, such as disabling firewalls, leaking data, or installing backdoors. We provide a sandbox with rule-based verifiable rewards to measure CUAs' success rates in executing these tasks (e.g., whether the firewall is indeed disabled), beyond refusal rates. We evaluate frontier LMs including GPT-5, Claude 4 Sonnet, Gemini 2.5 Pro, Llama-3.3-70B, and Mistral Large 2. Even without jailbreaking prompts, these frontier LMs comply with executing these malicious tasks at a high success rate (e.g., 90\% for Gemini 2.5 Pro). Furthermore, while newer models are safer in previous safety benchmarks, their misuse risks as CUAs become even higher, e.g., Gemini 2.5 Pro is riskier than Gemini 1.5 Pro. Additionally, while these LMs are robust to common malicious prompts (e.g., creating a bomb) when acting as chatbots, they could still act unsafely as CUAs. We further evaluate a leading agentic framework (UI-TARS-1.5) and find that while it improves performance, it also amplifies misuse risks. To mitigate the misuse risks of CUAs, we explore using LMs to monitor CUAs' actions. We find monitoring unsafe computer-using actions is significantly harder than monitoring conventional unsafe chatbot responses. While monitoring chain-of-thoughts leads to modest gains, the average monitoring accuracy is only 77\%. A hierarchical summarization strategy improves performance by up to 13\%, a promising direction though monitoring remains unreliable. The benchmark will be released publicly to facilitate further research on mitigating these risks.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedRAIN-Lite: Federated Reinforcement Algorithms for Improving Idealised Numerical Weather and Climate Models</title>
<link>https://arxiv.org/abs/2508.14315</link>
<guid>https://arxiv.org/abs/2508.14315</guid>
<content:encoded><![CDATA[
arXiv:2508.14315v2 Announce Type: replace 
Abstract: Sub-grid parameterisations in climate models are traditionally static and tuned offline, limiting adaptability to evolving states. This work introduces FedRAIN-Lite, a federated reinforcement learning (FedRL) framework that mirrors the spatial decomposition used in general circulation models (GCMs) by assigning agents to latitude bands, enabling local parameter learning with periodic global aggregation. Using a hierarchy of simplified energy-balance climate models, from a single-agent baseline (ebm-v1) to multi-agent ensemble (ebm-v2) and GCM-like (ebm-v3) setups, we benchmark three RL algorithms under different FedRL configurations. Results show that Deep Deterministic Policy Gradient (DDPG) consistently outperforms both static and single-agent baselines, with faster convergence and lower area-weighted RMSE in tropical and mid-latitude zones across both ebm-v2 and ebm-v3 setups. DDPG's ability to transfer across hyperparameters and low computational cost make it well-suited for geographically adaptive parameter learning. This capability offers a scalable pathway towards high-complexity GCMs and provides a prototype for physically aligned, online-learning climate models that can evolve with a changing climate. Code accessible at https://github.com/p3jitnath/climate-rl-fedrl.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Multi-agent Path Finding in Continuous Time</title>
<link>https://arxiv.org/abs/2508.16410</link>
<guid>https://arxiv.org/abs/2508.16410</guid>
<content:encoded><![CDATA[
arXiv:2508.16410v2 Announce Type: replace 
Abstract: Continuous-time Conflict Based-Search (CCBS) has long been viewed as the standard optimal baseline for multi-agent path finding in continuous time (MAPFR), yet recent critiques show that the theoretically described CCBS can fail to terminate on solvable MAPFR problems while the publicly available reference implementation can return sub-optimal solutions. This work presents an analytical framework that yields simple and sufficient conditions under which any CCBS-style algorithm is both sound and solution complete. Investigating the reference CCBS implementation reveals that it violates our sufficient conditions for soundness, with counterexamples demonstrating sub-optimality.
  Leveraging the framework, we introduce a branching rule ($\delta$-BR) and prove it restores soundness and termination guarantees. Consequently, the resulting CCBS variant is both sound and solution complete. To our knowledge, this is the first MAPFR solver matching the guarantees of the discrete-time CBS. On a constructed example, CCBS with $\delta$-BR improves sum-of-costs from 10.707 to 9.000 ($\approx$ 16% lower) compared to the reference CCBS implementation. Across benchmarks, the reference CCBS implementation is generally able to find solutions faster than CCBS with $\delta$-BR due to its more aggressive pruning. However, this comes at the cost of occasional sub-optimality and potential non-termination when all solutions are pruned, whereas $\delta$-BR preserves optimality and guarantees termination by design. Because $\delta$-BR largely only affects the branching step, it can be adopted as a drop-in replacement in existing codebases. Beyond CCBS, the analytical framework and termination criterion provide a systematic way to evaluate other CCBS-like MAPFR solvers and future extensions, thereby offering tools for rigorous analysis of next-generation MAPFR algorithms.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Learning Strategies for Estimation Error Minimization with Graph Neural Networks</title>
<link>https://arxiv.org/abs/2404.03227</link>
<guid>https://arxiv.org/abs/2404.03227</guid>
<content:encoded><![CDATA[
arXiv:2404.03227v3 Announce Type: replace-cross 
Abstract: We address the challenge of sampling and remote estimation for autoregressive Markovian processes in a multi-hop wireless network with statistically-identical agents. Agents cache the most recent samples from others and communicate over wireless collision channels governed by an underlying graph topology. Our goal is to minimize time-average estimation error and/or age of information with decentralized scalable sampling and transmission policies, considering both oblivious (where decision-making is independent of the physical processes) and non-oblivious policies (where decision-making depends on physical processes). We prove that in oblivious policies, minimizing estimation error is equivalent to minimizing the age of information. The complexity of the problem, especially the multi-dimensional action spaces and arbitrary network topologies, makes theoretical methods for finding optimal transmission policies intractable. We optimize the policies using a graphical multi-agent reinforcement learning framework, where each agent employs a permutation-equivariant graph neural network architecture. Theoretically, we prove that our proposed framework exhibits desirable transferability properties, allowing transmission policies trained on small- or moderate-size networks to be executed effectively on large-scale topologies. Numerical experiments demonstrate that (i) Our proposed framework outperforms state-of-the-art baselines; (ii) The trained policies are transferable to larger networks, and their performance gains increase with the number of agents; (iii) The training procedure withstands non-stationarity even if we utilize independent learning techniques; and, (iv) Recurrence is pivotal in both independent learning and centralized training and decentralized execution, and improves the resilience to non-stationarity in independent learning.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Information-Flow Perspective on Explainability Requirements: Specification and Verification</title>
<link>https://arxiv.org/abs/2509.01479</link>
<guid>https://arxiv.org/abs/2509.01479</guid>
<content:encoded><![CDATA[
arXiv:2509.01479v2 Announce Type: replace 
Abstract: Explainable systems expose information about why certain observed effects are happening to the agents interacting with them. We argue that this constitutes a positive flow of information that needs to be specified, verified, and balanced against negative information flow that may, e.g., violate privacy guarantees. Since both explainability and privacy require reasoning about knowledge, we tackle these tasks with epistemic temporal logic extended with quantification over counterfactual causes. This allows us to specify that a multi-agent system exposes enough information such that agents acquire knowledge on why some effect occurred. We show how this principle can be used to specify explainability as a system-level requirement and provide an algorithm for checking finite-state models against such specifications. We present a prototype implementation of the algorithm and evaluate it on several benchmarks, illustrating how our approach distinguishes between explainable and unexplainable systems, and how it allows to pose additional privacy requirements.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents</title>
<link>https://arxiv.org/abs/2509.18119</link>
<guid>https://arxiv.org/abs/2509.18119</guid>
<content:encoded><![CDATA[
arXiv:2509.18119v1 Announce Type: new 
Abstract: Building general-purpose graphical user interface (GUI) agents has become increasingly promising with the progress in vision language models. However, developing effective mobile GUI agents with reinforcement learning (RL) remains challenging due to the heavy-tailed distribution of task difficulty and the inefficiency of large-scale environment sampling. We present an online agentic reinforcement learning framework MOBILERL to enhance GUI agents in mobile environments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO) algorithm. In ADAGRPO, we design difficulty-adaptive positive replay and failure curriculum filtering to adapt the model to different task difficulties. We introduce the shortest path reward adjustment strategy to reshape rewards concerning the task length in multi-turn agentic tasks. Those strategies jointly stabilize RL training, improve sample efficiency, and generate strong performance across diverse mobile apps and tasks. We apply MOBILERL to two open models (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B model achieves state-of-the-art results in terms of success rates on both AndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted in the AutoGLM products, and also open-sourced at https://github.com/THUDM/MobileRL.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Weighted Gradient Tracking Privacy-Preserving Method for Distributed Optimization</title>
<link>https://arxiv.org/abs/2509.18134</link>
<guid>https://arxiv.org/abs/2509.18134</guid>
<content:encoded><![CDATA[
arXiv:2509.18134v1 Announce Type: new 
Abstract: This paper investigates the privacy-preserving distributed optimization problem, aiming to protect agents' private information from potential attackers during the optimization process. Gradient tracking, an advanced technique for improving the convergence rate in distributed optimization, has been applied to most first-order algorithms in recent years. We first reveal the inherent privacy leakage risk associated with gradient tracking. Building upon this insight, we propose a weighted gradient tracking distributed privacy-preserving algorithm, eliminating the privacy leakage risk in gradient tracking using decaying weight factors. Then, we characterize the convergence of the proposed algorithm under time-varying heterogeneous step sizes. We prove the proposed algorithm converges precisely to the optimal solution under mild assumptions. Finally, numerical simulations validate the algorithm's effectiveness through a classical distributed estimation problem and the distributed training of a convolutional neural network.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization</title>
<link>https://arxiv.org/abs/2509.18158</link>
<guid>https://arxiv.org/abs/2509.18158</guid>
<content:encoded><![CDATA[
arXiv:2509.18158v1 Announce Type: new 
Abstract: Automatic Prompt Optimization (APO) improves large language model (LLM) performance by refining prompts for specific tasks. However, prior APO methods typically focus only on user prompts, rely on unstructured feedback, and require large sample sizes and long iteration cycles-making them costly and brittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a novel framework that jointly optimizes both system and user prompts through principled, low-overhead refinement. ZERA scores prompts using eight generalizable criteria with automatically inferred weights, and revises prompts based on these structured critiques. This enables fast convergence to high-quality prompts using minimal examples and short iteration cycles. We evaluate ZERA across five LLMs and nine diverse datasets spanning reasoning, summarization, and code generation tasks. Experimental results demonstrate consistent improvements over strong baselines. Further ablation studies highlight the contribution of each component to more effective prompt construction. Our implementation including all prompts is publicly available at https://github.com/younatics/zera-agent.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent Framework</title>
<link>https://arxiv.org/abs/2509.18167</link>
<guid>https://arxiv.org/abs/2509.18167</guid>
<content:encoded><![CDATA[
arXiv:2509.18167v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to access external knowledge sources, but the effectiveness of RAG relies on the coordination between the retriever and the generator. Since these components are developed independently, their interaction is often suboptimal: the retriever may return irrelevant or redundant documents, while the generator may fail to fully leverage retrieved evidence. In this work, we propose a process-supervised multi-agent framework to bridge the gap between retriever and generator. The framework introduces two lightweight agents: a Decision Maker, which determines when to continue retrieval or stop for answer generation, and a Knowledge Selector, which filters retrieved documents to retain only the most useful evidence. To provide fine-grained supervision, we employ an LLM-as-a-Judge that evaluates each intermediate action with process-level rewards, ensuring more accurate credit assignment than relying solely on final answer correctness. We further adopt a tree-structured rollout strategy to explore diverse reasoning paths, and train both agents with Proximal Policy Optimization (PPO) in an end-to-end manner. Experiments on single-hop and multi-hop question answering benchmarks show that our approach achieves higher accuracy, more stable convergence, and produces more interpretable reasoning trajectories compared with standard RAG baselines. Importantly, the proposed framework is modular and plug-and-play, requiring no modification to the retriever or generator, making it practical for real-world RAG applications.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning</title>
<link>https://arxiv.org/abs/2509.18169</link>
<guid>https://arxiv.org/abs/2509.18169</guid>
<content:encoded><![CDATA[
arXiv:2509.18169v1 Announce Type: new 
Abstract: Complex systems typically rely on high-precision numerical computation to support decisions, but current large language models (LLMs) cannot yet incorporate such computations as an intrinsic and interpretable capability with existing architectures. Mainstream multi-agent approaches can leverage external experts, but inevitably introduce communication overhead and suffer from inefficient multimodal emergent capability and limited scalability. To this end, we propose PiMoE (Physically-isolated Mixture of Experts), a training and inference architecture for integrating computation and reasoning. Instead of the workflow paradigm of tool invocation, PiMoE endogenously integrates computational capabilities into neural networks after separately training experts, a text-to-computation module, and a router. At inference, the router directs computation and reasoning at the token level, thereby enabling iterative alternation within a single chain of thought. We evaluate PiMoE on two reasoning-computation tasks against LLM finetuning and the multi-agent system approaches. Results show that the PiMoE architecture achieves not only higher accuracy than directly finetuning LLMs but also significant improvements in response latency, token usage, and GPU energy consumption compared with mainstream multi-agent approaches. PiMoE offers an efficient, interpretable, and scalable paradigm for next-generation scientific or industrial intelligent systems.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ERFC: Happy Customers with Emotion Recognition and Forecasting in Conversation in Call Centers</title>
<link>https://arxiv.org/abs/2509.18175</link>
<guid>https://arxiv.org/abs/2509.18175</guid>
<content:encoded><![CDATA[
arXiv:2509.18175v1 Announce Type: new 
Abstract: Emotion Recognition in Conversation has been seen to be widely applicable in call center analytics, opinion mining, finance, retail, healthcare, and other industries. In a call center scenario, the role of the call center agent is not just confined to receiving calls but to also provide good customer experience by pacifying the frustration or anger of the customers. This can be achieved by maintaining neutral and positive emotion from the agent. As in any conversation, the emotion of one speaker is usually dependent on the emotion of other speaker. Hence the positive emotion of an agent, accompanied with the right resolution will help in enhancing customer experience. This can change an unhappy customer to a happy one. Imparting the right resolution at right time becomes easier if the agent has the insight of the emotion of future utterances. To predict the emotions of the future utterances we propose a novel architecture, Emotion Recognition and Forecasting in Conversation. Our proposed ERFC architecture considers multi modalities, different attributes of emotion, context and the interdependencies of the utterances of the speakers in the conversation. Our intensive experiments on the IEMOCAP dataset have shown the feasibility of the proposed ERFC. This approach can provide a tremendous business value for the applications like call center, where the happiness of customer is utmost important.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foam-Agent: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM</title>
<link>https://arxiv.org/abs/2509.18178</link>
<guid>https://arxiv.org/abs/2509.18178</guid>
<content:encoded><![CDATA[
arXiv:2509.18178v1 Announce Type: new 
Abstract: Computational Fluid Dynamics (CFD) is an essential simulation tool in engineering, yet its steep learning curve and complex manual setup create significant barriers. To address these challenges, we introduce Foam-Agent, a multi-agent framework that automates the entire end-to-end OpenFOAM workflow from a single natural language prompt. Our key innovations address critical gaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation: Foam-Agent is the first system to manage the full simulation pipeline, including advanced pre-processing with a versatile Meshing Agent capable of handling external mesh files and generating new geometries via Gmsh, automatic generation of HPC submission scripts, and post-simulation visualization via ParaView. 2. Composable Service Architecture: Going beyond a monolithic agent, the framework uses Model Context Protocol (MCP) to expose its core functions as discrete, callable tools. This allows for flexible integration and use by other agentic systems, such as Claude-code, for more exploratory workflows. 3. High-Fidelity Configuration Generation: We achieve superior accuracy through a Hierarchical Multi-Index RAG for precise context retrieval and a dependency-aware generation process that ensures configuration consistency. Evaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2% success rate with Claude 3.5 Sonnet, significantly outperforming existing frameworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the expertise barrier for CFD, demonstrating how specialized multi-agent systems can democratize complex scientific computing. The code is public at https://github.com/csml-rpi/Foam-Agent.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Introducing a novel Location-Assignment Algorithm for Activity-Based Transport Models: CARLA</title>
<link>https://arxiv.org/abs/2509.18191</link>
<guid>https://arxiv.org/abs/2509.18191</guid>
<content:encoded><![CDATA[
arXiv:2509.18191v1 Announce Type: new 
Abstract: This paper introduces CARLA (spatially Constrained Anchor-based Recursive Location Assignment), a recursive algorithm for assigning secondary or any activity locations in activity-based travel models. CARLA minimizes distance deviations while integrating location potentials, ensuring more realistic activity distributions. The algorithm decomposes trip chains into smaller subsegments, using geometric constraints and configurable heuristics to efficiently search the solution space. Compared to a state-of-the-art relaxation-discretization approach, CARLA achieves significantly lower mean deviations, even under limited runtimes. It is robust to real-world data inconsistencies, such as infeasible distances, and can flexibly adapt to various priorities, such as emphasizing location attractiveness or distance accuracy. CARLA's versatility and efficiency make it a valuable tool for improving the spatial accuracy of activity-based travel models and agent-based transport simulations. Our implementation is available at https://github.com/tnoud/carla.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought</title>
<link>https://arxiv.org/abs/2509.18200</link>
<guid>https://arxiv.org/abs/2509.18200</guid>
<content:encoded><![CDATA[
arXiv:2509.18200v1 Announce Type: new 
Abstract: Conversational agents must translate egocentric utterances (e.g., "on my right") into allocentric orientations (N/E/S/W). This challenge is particularly critical in indoor or complex facilities where GPS signals are weak and detailed maps are unavailable. While chain-of-thought (CoT) prompting has advanced reasoning in language and vision tasks, its application to multimodal spatial orientation remains underexplored. We introduce Conversational Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese conversational navigation projected from real-world environments, addressing egocentric-to-allocentric reasoning in non-English and ASR-transcribed scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which integrates ASR-transcribed speech with landmark coordinates through a structured three-step reasoning process: (1) extracting spatial relations, (2) mapping coordinates to absolute directions, and (3) inferring user orientation. A curriculum learning strategy progressively builds these capabilities on Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of resource-constrained settings. Experiments show that MCoT achieves 100% orientation accuracy on clean transcripts and 98.1% with ASR transcripts, substantially outperforming unimodal and non-structured baselines. Moreover, MCoT demonstrates robustness under noisy conversational conditions, including ASR recognition errors and multilingual code-switching. The model also maintains high accuracy in cross-domain evaluation and resilience to linguistic variation, domain shift, and referential ambiguity. These findings highlight the potential of structured MCoT spatial reasoning as a path toward interpretable and resource-efficient embodied navigation.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An N-Plus-1 GPT Agency for Critical Solution of Mechanical Engineering Analysis Problems</title>
<link>https://arxiv.org/abs/2509.18229</link>
<guid>https://arxiv.org/abs/2509.18229</guid>
<content:encoded><![CDATA[
arXiv:2509.18229v1 Announce Type: new 
Abstract: Generative AI, and specifically GPT, can produce a remarkable solution to a mechanical engineering analysis problem - but also, on occasion, a flawed solution. For example, an elementary mechanics problem is solved flawlessly in one GPT instance and incorrectly in a subsequent GPT instance, with a success probability of only 85%. This unreliability renders "out-of-the-box" GPT unsuitable for deployment in education or engineering practice. We introduce an "N-Plus-1" GPT Agency for Initial (Low-Cost) Analysis of mechanical engineering Problem Statements. Agency first launches N instantiations of Agent Solve to yield N independent Proposed Problem Solution Realizations; Agency then invokes Agent Compare to summarize and compare the N Proposed Problem Solution Realizations and to provide a Recommended Problem Solution. We argue from Condorcet's Jury Theorem that, for a Problem Statement characterized by per-Solve success probability greater than 1/2 (and N sufficiently large), the Predominant (Agent Compare) Proposed Problem Solution will, with high probability, correspond to a Correct Proposed Problem Solution. Furthermore, Agent Compare can also incorporate aspects of Secondary (Agent Compare) Proposed Problem Solutions, in particular when the latter represent alternative Problem Statement interpretations - different Mathematical Models - or alternative Mathematical Solution Procedures. Comparisons to Grok Heavy, a commercial multi-agent model, show similarities in design and performance, but also important differences in emphasis: our Agency focuses on transparency and pedagogical value.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fully Distributed State Estimation for Multi-agent Systems and its Application in Cooperative Localization</title>
<link>https://arxiv.org/abs/2509.18292</link>
<guid>https://arxiv.org/abs/2509.18292</guid>
<content:encoded><![CDATA[
arXiv:2509.18292v1 Announce Type: new 
Abstract: In this paper, we investigate the distributed state estimation problem for a continuous-time linear multi-agent system (MAS) composed of $\mathit{m}$ agents and monitored by the agents themselves. To address this problem, we propose a distributed observer that enables each agent to reconstruct the state of the MAS. The main idea is to let each agent $\mathit{i}$ recover the state of agent $\mathit{j}$ by using leader-follower consensus rules to track agent $\mathit{j}$'s state estimate, which is generated by agent $\mathit{j}$ itself using a Luenberger-like estimation rule. Under the assumptions of node-level observability and topological ordering consistency, we show that the estimation error dynamics are stabilizable if and only if the communication graph is strongly connected. Moreover, we discuss the fully distributed design of the proposed observer, assuming that the agents only know basic MAS configuration information, such as the homogeneity and the maximum number of allowable agents. This design ensures that the proposed observer functions correctly when agents are added or removed. Building on this, we consider cooperative localization as a distributed estimation problem and develop two fully distributed localization algorithms that allow agents to track their own and other agents' positions (and velocities) within the MAS. Finally, we conduct simulations to demonstrate the effectiveness of our proposed theoretical results.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Robot Policies While Maintaining User Privacy</title>
<link>https://arxiv.org/abs/2509.18311</link>
<guid>https://arxiv.org/abs/2509.18311</guid>
<content:encoded><![CDATA[
arXiv:2509.18311v1 Announce Type: new 
Abstract: Recent works introduce general-purpose robot policies. These policies provide a strong prior over how robots should behave -- e.g., how a robot arm should manipulate food items. But in order for robots to match an individual person's needs, users typically fine-tune these generalized policies -- e.g., showing the robot arm how to make their own preferred dinners. Importantly, during the process of personalizing robots, end-users leak data about their preferences, habits, and styles (e.g., the foods they prefer to eat). Other agents can simply roll-out the fine-tuned policy and see these personally-trained behaviors. This leads to a fundamental challenge: how can we develop robots that personalize actions while keeping learning private from external agents? We here explore this emerging topic in human-robot interaction and develop PRoP, a model-agnostic framework for personalized and private robot policies. Our core idea is to equip each user with a unique key; this key is then used to mathematically transform the weights of the robot's network. With the correct key, the robot's policy switches to match that user's preferences -- but with incorrect keys, the robot reverts to its baseline behaviors. We show the general applicability of our method across multiple model types in imitation learning, reinforcement learning, and classification tasks. PRoP is practically advantageous because it retains the architecture and behaviors of the original policy, and experimentally outperforms existing encoder-based approaches. See videos and code here: https://prop-icra26.github.io.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Gradient with Self-Attention for Model-Free Distributed Nonlinear Multi-Agent Games</title>
<link>https://arxiv.org/abs/2509.18371</link>
<guid>https://arxiv.org/abs/2509.18371</guid>
<content:encoded><![CDATA[
arXiv:2509.18371v1 Announce Type: new 
Abstract: Multi-agent games in dynamic nonlinear settings are challenging due to the time-varying interactions among the agents and the non-stationarity of the (potential) Nash equilibria. In this paper we consider model-free games, where agent transitions and costs are observed without knowledge of the transition and cost functions that generate them. We propose a policy gradient approach to learn distributed policies that follow the communication structure in multi-team games, with multiple agents per team. Our formulation is inspired by the structure of distributed policies in linear quadratic games, which take the form of time-varying linear feedback gains. In the nonlinear case, we model the policies as nonlinear feedback gains, parameterized by self-attention layers to account for the time-varying multi-agent communication topology. We demonstrate that our distributed policy gradient approach achieves strong performance in several settings, including distributed linear and nonlinear regulation, and simulated and real multi-robot pursuit-and-evasion games.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Provable Emergence of In-Context Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.18389</link>
<guid>https://arxiv.org/abs/2509.18389</guid>
<content:encoded><![CDATA[
arXiv:2509.18389v1 Announce Type: new 
Abstract: Typically, a modern reinforcement learning (RL) agent solves a task by updating its neural network parameters to adapt its policy to the task. Recently, it has been observed that some RL agents can solve a wide range of new out-of-distribution tasks without parameter updates after pretraining on some task distribution. When evaluated in a new task, instead of making parameter updates, the pretrained agent conditions its policy on additional input called the context, e.g., the agent's interaction history in the new task. The agent's performance increases as the information in the context increases, with the agent's parameters fixed. This phenomenon is typically called in-context RL (ICRL). The pretrained parameters of the agent network enable the remarkable ICRL phenomenon. However, many ICRL works perform the pretraining with standard RL algorithms. This raises the central question this paper aims to address: Why can the RL pretraining algorithm generate network parameters that enable ICRL? We hypothesize that the parameters capable of ICRL are minimizers of the pretraining loss. This work provides initial support for this hypothesis through a case study. In particular, we prove that when a Transformer is pretrained for policy evaluation, one of the global minimizers of the pretraining loss can enable in-context temporal difference learning.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assistive Decision-Making for Right of Way Navigation at Uncontrolled Intersections</title>
<link>https://arxiv.org/abs/2509.18407</link>
<guid>https://arxiv.org/abs/2509.18407</guid>
<content:encoded><![CDATA[
arXiv:2509.18407v1 Announce Type: new 
Abstract: Uncontrolled intersections account for a significant fraction of roadway crashes due to ambiguous right-of-way rules, occlusions, and unpredictable driver behavior. While autonomous vehicle research has explored uncertainty-aware decision making, few systems exist to retrofit human-operated vehicles with assistive navigation support. We present a driver-assist framework for right-of-way reasoning at uncontrolled intersections, formulated as a Partially Observable Markov Decision Process (POMDP). Using a custom simulation testbed with stochastic traffic agents, pedestrians, occlusions, and adversarial scenarios, we evaluate four decision-making approaches: a deterministic finite state machine (FSM), and three probabilistic planners: QMDP, POMCP, and DESPOT. Results show that probabilistic planners outperform the rule-based baseline, achieving up to 97.5 percent collision-free navigation under partial observability, with POMCP prioritizing safety and DESPOT balancing efficiency and runtime feasibility. Our findings highlight the importance of uncertainty-aware planning for driver assistance and motivate future integration of sensor fusion and environment perception modules for real-time deployment in realistic traffic environments.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context Lineage Assurance for Non-Human Identities in Critical Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2509.18415</link>
<guid>https://arxiv.org/abs/2509.18415</guid>
<content:encoded><![CDATA[
arXiv:2509.18415v1 Announce Type: new 
Abstract: The proliferation of autonomous software agents necessitates rigorous frameworks for establishing secure and verifiable agent-to-agent (A2A) interactions, particularly when such agents are instantiated as non-human identities(NHIs). We extend the A2A paradigm [1 , 2] by introducing a cryptographically grounded mechanism for lineage verification, wherein the provenance and evolution of NHIs are anchored in append-only Merkle tree structures modeled after Certificate Transparency (CT) logs. Unlike traditional A2A models that primarily secure point-to-point interactions, our approach enables both agents and external verifiers to cryptographically validate multi-hop provenance, thereby ensuring the integrity of the entire call chain.
  A federated proof server acts as an auditor across one or more Merkle logs, aggregating inclusion proofs and consistency checks into compact, signed attestations that external parties can verify without access to the full execution trace. In parallel, we augment the A2A agent card to incorporate explicit identity verification primitives, enabling both peer agents and human approvers to authenticate the legitimacy of NHI representations in a standardized manner. Together, these contributions establish a cohesive model that integrates identity attestation, lineage verification, and independent proof auditing, thereby advancing the security posture of inter-agent ecosystems and providing a foundation for robust governance of NHIs in regulated environments such as FedRAMP.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instruction-Following Evaluation in Function Calling for Large Language Models</title>
<link>https://arxiv.org/abs/2509.18420</link>
<guid>https://arxiv.org/abs/2509.18420</guid>
<content:encoded><![CDATA[
arXiv:2509.18420v1 Announce Type: new 
Abstract: Function calling is a core capability of large language models, essential for AI agents. Existing benchmarks such as the Berkeley Function Calling Leaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench (arXiv:2501.12851) evaluate argument correctness but do not test adherence to format instructions embedded in parameter descriptions, such as enclosing values in double quotes or using ISO date formats.
  We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911) that assesses precise instruction following in function calling. IFEval-FC encodes verifiable formats directly within JSON schema descriptions, for example specifying that a value must not contain punctuation. It includes 750 test cases, each consisting of a function with an embedded format for one of its input parameters and a corresponding user query. Evaluation is fully algorithmic, ensuring objectivity, reproducibility, and scalability.
  Our results show that even state-of-the-art proprietary models, including GPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules, highlighting a practical limitation for real-world agent systems. The complete codebase and data are publicly available at https://github.com/Skripkon/IFEval-FC.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compositional System Dynamics: The Higher Mathematics Underlying System Dynamics Diagrams &amp; Practice</title>
<link>https://arxiv.org/abs/2509.18475</link>
<guid>https://arxiv.org/abs/2509.18475</guid>
<content:encoded><![CDATA[
arXiv:2509.18475v1 Announce Type: new 
Abstract: This work establishes a robust mathematical foundation for compositional System Dynamics modeling, leveraging category theory to formalize and enhance the representation, analysis, and composition of system models. Here, System Dynamics diagrams, such as stock & flow diagrams, system structure diagrams, and causal loop diagrams, are formulated as categorical constructs, enabling scalable, transparent, and systematic reasoning. By encoding these diagrams as data using attributed C-sets and utilizing advanced categorical tools like structured cospans, pushouts, pullbacks, and functor mappings, the framework supports modular composition, stratification, and seamless mapping between syntax and semantics.
  The approach underwrites traditional practice with firm mathematical structure, facilitates the identification of certain forms of pathways and feedback loops, the detection of simple patterns within complex diagrams, common structure between diagrams, and structure-preserving mappings between diverse diagram types. Additionally, this framework supports alternative semantics, such as stochastic transition dynamics, extending beyond traditional ordinary differential equation (ODE) representations. Applications in compositional modeling, modularity, and team-based collaboration demonstrate the practical advantages of this advanced framework.
  Future directions include integrating dimensional annotations, supporting hybrid and agent-based modeling paradigms, and expanding the framework's applicability to global and local temporal reasoning through temporal sheaves. By revealing and formalizing the hidden mathematical structure of System Dynamics diagrams, this work empowers practitioners to tackle complex systems with clarity, scalability, and rigor.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Agent Access (A\^3) Network: An Embodied, Communication-Aware Multi-Agent Framework for 6G Coverage</title>
<link>https://arxiv.org/abs/2509.18526</link>
<guid>https://arxiv.org/abs/2509.18526</guid>
<content:encoded><![CDATA[
arXiv:2509.18526v1 Announce Type: new 
Abstract: The vision of 6G communication demands autonomous and resilient networking in environments without fixed infrastructure. Yet most multi-agent reinforcement learning (MARL) approaches focus on isolated stages - exploration, relay formation, or access - under static deployments and centralized control, limiting adaptability. We propose the AI Agent Access (A\^3) Network, a unified, embodied intelligence-driven framework that transforms multi-agent networking into a dynamic, decentralized, and end-to-end system. Unlike prior schemes, the A\^3 Network integrates exploration, target user access, and backhaul maintenance within a single learning process, while supporting on-demand agent addition during runtime. Its decentralized policies ensure that even a single agent can operate independently with limited observations, while coordinated agents achieve scalable, communication-optimized coverage. By embedding link-level communication metrics into actor-critic learning, the A\^3 Network couples topology formation with robust decision-making. Numerical simulations demonstrate that the A\^3 Network not only balances exploration and communication efficiency but also delivers system-level adaptability absent in existing MARL frameworks, offering a new paradigm for 6G multi-agent networks.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group Formation through Game Theory and Agent-Based Modeling: Spatial Cohesion, Heterogeneity, and Resource Pooling</title>
<link>https://arxiv.org/abs/2509.18551</link>
<guid>https://arxiv.org/abs/2509.18551</guid>
<content:encoded><![CDATA[
arXiv:2509.18551v1 Announce Type: new 
Abstract: This paper develops a game-theoretic model and an agent-based model to study group formation driven by resource pooling, spatial cohesion, and heterogeneity. We focus on cross-sector partnerships (CSPs) involving public, private, and nonprofit organizations, each contributing distinct resources. Group formation occurs as agents strategically optimize their choices in response to others within a competitive setting. We prove the existence of stable group equilibria and simulate formation dynamics under varying spatial and resource conditions. The results show that limited individual resources lead to groups that form mainly among nearby actors, while abundant resources allow groups to move across larger distances. Increased resource heterogeneity and spatial proximity promote the formation of larger and more diverse groups. These findings reveal key trade-offs shaping group size and composition, guiding strategies for effective cross-sector collaborations and multi-agent systems.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs</title>
<link>https://arxiv.org/abs/2509.18557</link>
<guid>https://arxiv.org/abs/2509.18557</guid>
<content:encoded><![CDATA[
arXiv:2509.18557v1 Announce Type: new 
Abstract: Compared to traditional models, agentic AI represents a highly valuable target for potential attackers as they possess privileged access to data sources and API tools, which are traditionally not incorporated into classical agents. Unlike a typical software application residing in a Demilitarized Zone (DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI (only defining a final goal, leaving the path selection to LLM). This characteristic introduces substantial security risk to both operational security and information security. Most common existing defense mechanism rely on detection of malicious intent and preventing it from reaching the LLM agent, thus protecting against jailbreak attacks such as prompt injection. In this paper, we present an alternative approach, LLMZ+, which moves beyond traditional detection-based approaches by implementing prompt whitelisting. Through this method, only contextually appropriate and safe messages are permitted to interact with the agentic LLM. By leveraging the specificity of context, LLMZ+ guarantees that all exchanges between external users and the LLM conform to predefined use cases and operational boundaries. Our approach streamlines the security framework, enhances its long-term resilience, and reduces the resources required for sustaining LLM information security. Our empirical evaluation demonstrates that LLMZ+ provides strong resilience against the most common jailbreak prompts. At the same time, legitimate business communications are not disrupted, and authorized traffic flows seamlessly between users and the agentic LLM. We measure the effectiveness of approach using false positive and false negative rates, both of which can be reduced to 0 in our experimental setting.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIE: Perception and Interaction Enhanced End-to-End Motion Planning for Autonomous Driving</title>
<link>https://arxiv.org/abs/2509.18609</link>
<guid>https://arxiv.org/abs/2509.18609</guid>
<content:encoded><![CDATA[
arXiv:2509.18609v1 Announce Type: new 
Abstract: End-to-end motion planning is promising for simplifying complex autonomous driving pipelines. However, challenges such as scene understanding and effective prediction for decision-making continue to present substantial obstacles to its large-scale deployment. In this paper, we present PIE, a pioneering framework that integrates advanced perception, reasoning, and intention modeling to dynamically capture interactions between the ego vehicle and surrounding agents. It incorporates a bidirectional Mamba fusion that addresses data compression losses in multimodal fusion of camera and LiDAR inputs, alongside a novel reasoning-enhanced decoder integrating Mamba and Mixture-of-Experts to facilitate scene-compliant anchor selection and optimize adaptive trajectory inference. PIE adopts an action-motion interaction module to effectively utilize state predictions of surrounding agents to refine ego planning. The proposed framework is thoroughly validated on the NAVSIM benchmark. PIE, without using any ensemble and data augmentation techniques, achieves an 88.9 PDM score and 85.6 EPDM score, surpassing the performance of prior state-of-the-art methods. Comprehensive quantitative and qualitative analyses demonstrate that PIE is capable of reliably generating feasible and high-quality ego trajectories.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Case for Negative Data: From Crash Reports to Counterfactuals for Reasonable Driving</title>
<link>https://arxiv.org/abs/2509.18626</link>
<guid>https://arxiv.org/abs/2509.18626</guid>
<content:encoded><![CDATA[
arXiv:2509.18626v1 Announce Type: new 
Abstract: Learning-based autonomous driving systems are trained mostly on incident-free data, offering little guidance near safety-performance boundaries. Real crash reports contain precisely the contrastive evidence needed, but they are hard to use: narratives are unstructured, third-person, and poorly grounded to sensor views. We address these challenges by normalizing crash narratives to ego-centric language and converting both logs and crashes into a unified scene-action representation suitable for retrieval. At decision time, our system adjudicates proposed actions by retrieving relevant precedents from this unified index; an agentic counterfactual extension proposes plausible alternatives, retrieves for each, and reasons across outcomes before deciding. On a nuScenes benchmark, precedent retrieval substantially improves calibration, with recall on contextually preferred actions rising from 24% to 53%. The counterfactual variant preserves these gains while sharpening decisions near risk.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users</title>
<link>https://arxiv.org/abs/2509.18632</link>
<guid>https://arxiv.org/abs/2509.18632</guid>
<content:encoded><![CDATA[
arXiv:2509.18632v1 Announce Type: new 
Abstract: To assist users in complex tasks, LLMs generate plans: step-by-step instructions towards a goal. While alignment methods aim to ensure LLM plans are helpful, they train (RLHF) or evaluate (ChatbotArena) on what users prefer, assuming this reflects what helps them. We test this with Planorama: an interface where 126 users answer 300 multi-step questions with LLM plans. We get 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA success) and user preferences on plans, and recreate the setup in agents and reward models to see if they simulate or prefer what helps users. We expose: 1) user/model preferences and agent success do not accurately predict which plans help users, so common alignment feedback can misalign with helpfulness; 2) this gap is not due to user-specific preferences, as users are similarly successful when using plans they prefer/disprefer; 3) surface-level cues like brevity and question similarity strongly link to preferences, but such biases fail to predict helpfulness. In all, we argue aligning helpful LLMs needs feedback from real user interactions, not just preferences of what looks helpful, so we discuss the plan NLP researchers can execute to solve this problem.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Learning in Spatial Agent-Based Models for Climate Risk Assessment: A Geospatial Framework with Evolutionary Economic Agents</title>
<link>https://arxiv.org/abs/2509.18633</link>
<guid>https://arxiv.org/abs/2509.18633</guid>
<content:encoded><![CDATA[
arXiv:2509.18633v1 Announce Type: new 
Abstract: Climate risk assessment requires modelling complex interactions between spatially heterogeneous hazards and adaptive economic systems. We present a novel geospatial agent-based model that integrates climate hazard data with evolutionary learning for economic agents. Our framework combines Mesa-based spatial modelling with CLIMADA climate impact assessment, introducing adaptive learning behaviours that allow firms to evolve strategies for budget allocation, pricing, wages, and risk adaptation through fitness-based selection and mutation. We demonstrate the framework using riverine flood projections under RCP8.5 until 2100, showing that evolutionary adaptation enables firms to converge with baseline (no hazard) production levels after decades of disruption due to climate stress. Our results reveal systemic risks where even agents that are not directly exposed to floods face impacts through supply chain disruptions, with the end-of-century average price of goods 5.6% higher under RCP8.5 compared to the baseline. This open-source framework provides financial institutions and companies with tools to quantify both direct and cascading climate risks while evaluating cost-effective adaptation strategies.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Number Adaptive Formation Flight Planning via Affine Deformable Guidance in Narrow Environments</title>
<link>https://arxiv.org/abs/2509.18636</link>
<guid>https://arxiv.org/abs/2509.18636</guid>
<content:encoded><![CDATA[
arXiv:2509.18636v1 Announce Type: new 
Abstract: Formation maintenance with varying number of drones in narrow environments hinders the convergence of planning to the desired configurations. To address this challenge, this paper proposes a formation planning method guided by Deformable Virtual Structures (DVS) with continuous spatiotemporal transformation. Firstly, to satisfy swarm safety distance and preserve formation shape filling integrity for irregular formation geometries, we employ Lloyd algorithm for uniform $\underline{PA}$rtitioning and Hungarian algorithm for $\underline{AS}$signment (PAAS) in DVS. Subsequently, a spatiotemporal trajectory involving DVS is planned using primitive-based path search and nonlinear trajectory optimization. The DVS trajectory achieves adaptive transitions with respect to a varying number of drones while ensuring adaptability to narrow environments through affine transformation. Finally, each agent conducts distributed trajectory planning guided by desired spatiotemporal positions within the DVS, while incorporating collision avoidance and dynamic feasibility requirements. Our method enables up to 15\% of swarm numbers to join or leave in cluttered environments while rapidly restoring the desired formation shape in simulation. Compared to cutting-edge formation planning method, we demonstrate rapid formation recovery capacity and environmental adaptability. Real-world experiments validate the effectiveness and resilience of our formation planning method.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AutoSurvey: Let LLMs Survey LLMs</title>
<link>https://arxiv.org/abs/2509.18661</link>
<guid>https://arxiv.org/abs/2509.18661</guid>
<content:encoded><![CDATA[
arXiv:2509.18661v1 Announce Type: new 
Abstract: The exponential growth of scientific literature poses unprecedented challenges for researchers attempting to synthesize knowledge across rapidly evolving fields. We present \textbf{Agentic AutoSurvey}, a multi-agent framework for automated survey generation that addresses fundamental limitations in existing approaches. Our system employs four specialized agents (Paper Search Specialist, Topic Mining \& Clustering, Academic Survey Writer, and Quality Evaluator) working in concert to generate comprehensive literature surveys with superior synthesis quality. Through experiments on six representative LLM research topics from COLM 2024 categories, we demonstrate that our multi-agent approach achieves significant improvements over existing baselines, scoring 8.18/10 compared to AutoSurvey's 4.77/10. The multi-agent architecture processes 75--443 papers per topic (847 total across six topics) while targeting high citation coverage (often $\geq$80\% on 75--100-paper sets; lower on very large sets such as RLHF) through specialized agent orchestration. Our 12-dimension evaluation captures organization, synthesis integration, and critical analysis beyond basic metrics. These findings demonstrate that multi-agent architectures represent a meaningful advancement for automated literature survey generation in rapidly evolving scientific domains.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributionally Robust Safe Motion Planning with Contextual Information</title>
<link>https://arxiv.org/abs/2509.18666</link>
<guid>https://arxiv.org/abs/2509.18666</guid>
<content:encoded><![CDATA[
arXiv:2509.18666v1 Announce Type: new 
Abstract: We present a distributionally robust approach for collision avoidance by incorporating contextual information. Specifically, we embed the conditional distribution of future trajectory of the obstacle conditioned on the motion of the ego agent in a reproducing kernel Hilbert space (RKHS) via the conditional kernel mean embedding operator. Then, we define an ambiguity set containing all distributions whose embedding in the RKHS is within a certain distance from the empirical estimate of conditional mean embedding learnt from past data. Consequently, a distributionally robust collision avoidance constraint is formulated, and included in the receding horizon based motion planning formulation of the ego agent. Simulation results show that the proposed approach is more successful in avoiding collision compared to approaches that do not include contextual information and/or distributional robustness in their formulation in several challenging scenarios.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proximately Envy-Free and Efficient Allocation of Mixed Manna</title>
<link>https://arxiv.org/abs/2509.18673</link>
<guid>https://arxiv.org/abs/2509.18673</guid>
<content:encoded><![CDATA[
arXiv:2509.18673v1 Announce Type: new 
Abstract: The existence of fair and efficient allocations of indivisible items is a central problem in fair division. For indivisible goods, the existence of Pareto efficient (PO) and envy free up to one item (EF1) allocations was established by Caragiannis et al. In a recent breakthrough, Mahara established the existence of PO and EF1 allocations for indivisible chores.
  However, the existence of PO and EF1 allocations of mixed manna remains an intriguing open problem. In this paper, we make significant progress in this direction. We establish the existence of allocations that are PO and introspective envy free up to one item (IEF1) for mixed manna. In an IEF1 allocation, each agent can eliminate its envy towards all the other agents by either adding an item or removing an item from its own bundle. The notion of IEF1 coincides with EF1 for indivisible chores, and hence, our existence result generalizes the aforementioned result of Mahara.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Query-Centric Diffusion Policy for Generalizable Robotic Assembly</title>
<link>https://arxiv.org/abs/2509.18686</link>
<guid>https://arxiv.org/abs/2509.18686</guid>
<content:encoded><![CDATA[
arXiv:2509.18686v1 Announce Type: new 
Abstract: The robotic assembly task poses a key challenge in building generalist robots due to the intrinsic complexity of part interactions and the sensitivity to noise perturbations in contact-rich settings. The assembly agent is typically designed in a hierarchical manner: high-level multi-part reasoning and low-level precise control. However, implementing such a hierarchical policy is challenging in practice due to the mismatch between high-level skill queries and low-level execution. To address this, we propose the Query-centric Diffusion Policy (QDP), a hierarchical framework that bridges high-level planning and low-level control by utilizing queries comprising objects, contact points, and skill information. QDP introduces a query-centric mechanism that identifies task-relevant components and uses them to guide low-level policies, leveraging point cloud observations to improve the policy's robustness. We conduct comprehensive experiments on the FurnitureBench in both simulation and real-world settings, demonstrating improved performance in skill precision and long-horizon success rate. In the challenging insertion and screwing tasks, QDP improves the skill-wise success rate by over 50% compared to baselines without structured queries.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Data Agents: A New Opportunity for Smart Data</title>
<link>https://arxiv.org/abs/2509.18710</link>
<guid>https://arxiv.org/abs/2509.18710</guid>
<content:encoded><![CDATA[
arXiv:2509.18710v1 Announce Type: new 
Abstract: As data continues to grow in scale and complexity, preparing, transforming, and analyzing it remains labor-intensive, repetitive, and difficult to scale. Since data contains knowledge and AI learns knowledge from it, the alignment between AI and data is essential. However, data is often not structured in ways that are optimal for AI utilization. Moreover, an important question arises: how much knowledge can we pack into data through intensive data operations? Autonomous data agents (DataAgents), which integrate LLM reasoning with task decomposition, action reasoning and grounding, and tool calling, can autonomously interpret data task descriptions, decompose tasks into subtasks, reason over actions, ground actions into python code or tool calling, and execute operations. Unlike traditional data management and engineering tools, DataAgents dynamically plan workflows, call powerful tools, and adapt to diverse data tasks at scale. This report argues that DataAgents represent a paradigm shift toward autonomous data-to-knowledge systems. DataAgents are capable of handling collection, integration, preprocessing, selection, transformation, reweighing, augmentation, reprogramming, repairs, and retrieval. Through these capabilities, DataAgents transform complex and unstructured data into coherent and actionable knowledge. We first examine why the convergence of agentic AI and data-to-knowledge systems has emerged as a critical trend. We then define the concept of DataAgents and discuss their architectural design, training strategies, as well as the new skills and capabilities they enable. Finally, we call for concerted efforts to advance action workflow optimization, establish open datasets and benchmark ecosystems, safeguard privacy, balance efficiency with scalability, and develop trustworthy DataAgent guardrails to prevent malicious actions.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemOrb: A Plug-and-Play Verbal-Reinforcement Memory Layer for E-Commerce Customer Service</title>
<link>https://arxiv.org/abs/2509.18713</link>
<guid>https://arxiv.org/abs/2509.18713</guid>
<content:encoded><![CDATA[
arXiv:2509.18713v1 Announce Type: new 
Abstract: Large Language Model-based agents(LLM-based agents) are increasingly deployed in customer service, yet they often forget across sessions, repeat errors, and lack mechanisms for continual self-improvement. This makes them unreliable in dynamic settings where stability and consistency are critical. To better evaluate these properties, we emphasize two indicators: task success rate as a measure of overall effectiveness, and consistency metrics such as Pass$^k$ to capture reliability across multiple trials. To address the limitations of existing approaches, we propose MemOrb, a lightweight and plug-and-play verbal reinforcement memory layer that distills multi-turn interactions into compact strategy reflections. These reflections are stored in a shared memory bank and retrieved to guide decision-making, without requiring any fine-tuning. Experiments show that MemOrb significantly improves both success rate and stability, achieving up to a 63 percentage-point gain in multi-turn success rate and delivering more consistent performance across repeated trials. Our results demonstrate that structured reflection is a powerful mechanism for enhancing long-term reliability of frozen LLM agents in customer service scenarios.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Obstacle Avoidance using Double DQN for Quadcopter Navigation</title>
<link>https://arxiv.org/abs/2509.18734</link>
<guid>https://arxiv.org/abs/2509.18734</guid>
<content:encoded><![CDATA[
arXiv:2509.18734v1 Announce Type: new 
Abstract: One of the challenges faced by Autonomous Aerial Vehicles is reliable navigation through urban environments. Factors like reduction in precision of Global Positioning System (GPS), narrow spaces and dynamically moving obstacles make the path planning of an aerial robot a complicated task. One of the skills required for the agent to effectively navigate through such an environment is to develop an ability to avoid collisions using information from onboard depth sensors. In this paper, we propose Reinforcement Learning of a virtual quadcopter robot agent equipped with a Depth Camera to navigate through a simulated urban environment.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The AGNTCY Agent Directory Service: Architecture and Implementation</title>
<link>https://arxiv.org/abs/2509.18787</link>
<guid>https://arxiv.org/abs/2509.18787</guid>
<content:encoded><![CDATA[
arXiv:2509.18787v1 Announce Type: new 
Abstract: The Agent Directory Service (ADS) is a distributed directory for the discovery of AI agent capabilities, metadata, and provenance. It leverages content-addressed storage, hierarchical taxonomies, and cryptographic signing to enable efficient, verifiable, and multi-dimensional discovery across heterogeneous Multi-Agent Systems (MAS). Built on the Open Agentic Schema Framework (OASF), ADS decouples capability indexing from content location through a two-level mapping realized over a Kademlia-based Distributed Hash Table (DHT). It reuses mature OCI / ORAS infrastructure for artifact distribution, integrates Sigstore for provenance, and supports schema-driven extensibility for emerging agent modalities (LLM prompt agents, MCP servers, A2A-enabled components). This paper formalizes the architectural model, describes storage and discovery layers, explains security and performance properties, and positions ADS within the broader landscape of emerging agent registry and interoperability initiatives.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SR-Eval: Evaluating LLMs on Code Generation under Stepwise Requirement Refinement</title>
<link>https://arxiv.org/abs/2509.18808</link>
<guid>https://arxiv.org/abs/2509.18808</guid>
<content:encoded><![CDATA[
arXiv:2509.18808v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable progress in code generation. However, existing benchmarks mainly formalize the task as a static, single-turn problem, overlooking the stepwise requirement changes and iterative workflows in real-world software development. This mismatch limits the understanding of how well LLMs can support real-world development workflows. Constructing such iterative benchmarks is challenging due to the lack of public interaction traces and the difficulty of creating discriminative, turn-specific test cases.
  To bridge this gap, we present SR-Eval, a benchmark specifically designed to assess LLMs on iterative code generation under Stepwise requirements Refinement. SR-Eval spans both function-level and repository-level tasks in Python and Java, enabling fine-grained and progressive evaluation across evolving requirements. The construction of SR-Eval follows a carefully designed pipeline that first leverages a multi-agent-based requirement generation method to simulate the development process and recover the multi-round interaction process from final requirements, then employs a semantic-aware discriminative test case generation component to ensure discriminative and consistent evaluation at each turn. SR-Eval comprises 443 multi-turn tasks and 1,857 questions at both function and repository levels. Using SR-Eval, we evaluate 11 representative LLMs with three prompting strategies that simulate different usage patterns. Results show that iterative code generation under stepwise requirement refinement remains highly challenging: the best-performing model achieves only 22.67% completion rate on function-level tasks and 20.00% on repository-level tasks. We further observe that prompting strategies substantially influence performance, highlighting the need for the development of advanced methods.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAPEX: A Multi-Agent Pipeline for Keyphrase Extraction</title>
<link>https://arxiv.org/abs/2509.18813</link>
<guid>https://arxiv.org/abs/2509.18813</guid>
<content:encoded><![CDATA[
arXiv:2509.18813v1 Announce Type: new 
Abstract: Keyphrase extraction is a fundamental task in natural language processing. However, existing unsupervised prompt-based methods for Large Language Models (LLMs) often rely on single-stage inference pipelines with uniform prompting, regardless of document length or LLM backbone. Such one-size-fits-all designs hinder the full exploitation of LLMs' reasoning and generation capabilities, especially given the complexity of keyphrase extraction across diverse scenarios. To address these challenges, we propose MAPEX, the first framework that introduces multi-agent collaboration into keyphrase extraction. MAPEX coordinates LLM-based agents through modules for expert recruitment, candidate extraction, topic guidance, knowledge augmentation, and post-processing. A dual-path strategy dynamically adapts to document length: knowledge-driven extraction for short texts and topic-guided extraction for long texts. Extensive experiments on six benchmark datasets across three different LLMs demonstrate its strong generalization and universality, outperforming the state-of-the-art unsupervised method by 2.44\% and standard LLM baselines by 4.01\% in F1@5 on average. Code is available at https://github.com/NKU-LITI/MAPEX.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions</title>
<link>https://arxiv.org/abs/2509.18847</link>
<guid>https://arxiv.org/abs/2509.18847</guid>
<content:encoded><![CDATA[
arXiv:2509.18847v1 Announce Type: new 
Abstract: Tool-augmented large language models (LLMs) are usually trained with supervised imitation or coarse-grained reinforcement learning that optimizes single tool calls. Current self-reflection practices rely on heuristic prompts or one-way reasoning: the model is urged to 'think more' instead of learning error diagnosis and repair. This is fragile in multi-turn interactions; after a failure the model often repeats the same mistake. We propose structured reflection, which turns the path from error to repair into an explicit, controllable, and trainable action. The agent produces a short yet precise reflection: it diagnoses the failure using evidence from the previous step and then proposes a correct, executable follow-up call. For training we combine DAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing the stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce Tool-Reflection-Bench, a lightweight benchmark that programmatically checks structural validity, executability, parameter correctness, and result consistency. Tasks are built as mini trajectories of erroneous call, reflection, and corrected call, with disjoint train and test splits. Experiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn tool-call success and error recovery, and a reduction of redundant calls. These results indicate that making reflection explicit and optimizing it directly improves the reliability of tool interaction and offers a reproducible path for agents to learn from failure.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongCat-Flash-Thinking Technical Report</title>
<link>https://arxiv.org/abs/2509.18883</link>
<guid>https://arxiv.org/abs/2509.18883</guid>
<content:encoded><![CDATA[
arXiv:2509.18883v1 Announce Type: new 
Abstract: We present LongCat-Flash-Thinking, an efficient 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities are cultivated through a meticulously crafted training process, beginning with long Chain-of-Thought (CoT) data cold-start and culminating in large-scale Reinforcement Learning (RL). We first employ a well-designed cold-start training strategy, which significantly enhances the reasoning potential and equips the model with specialized skills in both formal and agentic reasoning. Then, a core innovation is our domain-parallel training scheme, which decouples optimization across distinct domains (e.g., STEM, Code, Agentic) and subsequently fuses the resulting expert models into a single, nearly Pareto-optimal model. This entire process is powered by our Dynamic ORchestration for Asynchronous rollout (DORA) system, a large-scale RL framework that delivers a greater than threefold training speedup over synchronous methods on tens of thousands of accelerators. As a result, LongCat-Flash-Thinking achieves state-of-the-art performance among open-source models on a suite of complex reasoning tasks. The model exhibits exceptional efficiency in agentic reasoning, reducing average token consumption by 64.5% (from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We release LongCat-Flash-Thinking to promote further advances in reasoning systems and agentic AI research.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attack for Defense: Adversarial Agents for Point Prompt Optimization Empowering Segment Anything Model</title>
<link>https://arxiv.org/abs/2509.18891</link>
<guid>https://arxiv.org/abs/2509.18891</guid>
<content:encoded><![CDATA[
arXiv:2509.18891v1 Announce Type: new 
Abstract: Prompt quality plays a critical role in the performance of the Segment Anything Model (SAM), yet existing approaches often rely on heuristic or manually crafted prompts, limiting scalability and generalization. In this paper, we propose Point Prompt Defender, an adversarial reinforcement learning framework that adopts an attack-for-defense paradigm to automatically optimize point prompts. We construct a task-agnostic point prompt environment by representing image patches as nodes in a dual-space graph, where edges encode both physical and semantic distances. Within this environment, an attacker agent learns to activate a subset of prompts that maximally degrade SAM's segmentation performance, while a defender agent learns to suppress these disruptive prompts and restore accuracy. Both agents are trained using Deep Q-Networks with a reward signal based on segmentation quality variation. During inference, only the defender is deployed to refine arbitrary coarse prompt sets, enabling enhanced SAM segmentation performance across diverse tasks without retraining. Extensive experiments show that Point Prompt Defender effectively improves SAM's robustness and generalization, establishing a flexible, interpretable, and plug-and-play framework for prompt-based segmentation.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models</title>
<link>https://arxiv.org/abs/2509.18917</link>
<guid>https://arxiv.org/abs/2509.18917</guid>
<content:encoded><![CDATA[
arXiv:2509.18917v1 Announce Type: new 
Abstract: Autonomous vehicles (AVs) are expected to revolutionize transportation by improving efficiency and safety. Their success relies on 3D vision systems that effectively sense the environment and detect traffic agents. Among sensors AVs use to create a comprehensive view of surroundings, LiDAR provides high-resolution depth data enabling accurate object detection, safe navigation, and collision avoidance. However, collecting real-world LiDAR data is time-consuming and often affected by noise and sparsity due to adverse weather or sensor limitations. This work applies a denoising diffusion probabilistic model (DDPM), enhanced with novel noise scheduling and time-step embedding techniques to generate high-quality synthetic data for augmentation, thereby improving performance across a range of computer vision tasks, particularly in AV perception. These modifications impact the denoising process and the model's temporal awareness, allowing it to produce more realistic point clouds based on the projection. The proposed method was extensively evaluated under various configurations using the IAMCV and KITTI-360 datasets, with four performance metrics compared against state-of-the-art (SOTA) methods. The results demonstrate the model's superior performance over most existing baselines and its effectiveness in mitigating the effects of noisy and sparse LiDAR data, producing diverse point clouds with rich spatial relationships and structural detail.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions</title>
<link>https://arxiv.org/abs/2509.18970</link>
<guid>https://arxiv.org/abs/2509.18970</guid>
<content:encoded><![CDATA[
arXiv:2509.18970v1 Announce Type: new 
Abstract: Driven by the rapid advancements of Large Language Models (LLMs), LLM-based agents have emerged as powerful intelligent systems capable of human-like cognition, reasoning, and interaction. These agents are increasingly being deployed across diverse real-world applications, including student education, scientific research, and financial analysis. However, despite their remarkable potential, LLM-based agents remain vulnerable to hallucination issues, which can result in erroneous task execution and undermine the reliability of the overall system design. Addressing this critical challenge requires a deep understanding and a systematic consolidation of recent advances on LLM-based agents. To this end, we present the first comprehensive survey of hallucinations in LLM-based agents. By carefully analyzing the complete workflow of agents, we propose a new taxonomy that identifies different types of agent hallucinations occurring at different stages. Furthermore, we conduct an in-depth examination of eighteen triggering causes underlying the emergence of agent hallucinations. Through a detailed review of a large number of existing studies, we summarize approaches for hallucination mitigation and detection, and highlight promising directions for future research. We hope this survey will inspire further efforts toward addressing hallucinations in LLM-based agents, ultimately contributing to the development of more robust and reliable agent systems.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Online Social Media Conversations on Controversial Topics Using AI Agents Calibrated on Real-World Data</title>
<link>https://arxiv.org/abs/2509.18985</link>
<guid>https://arxiv.org/abs/2509.18985</guid>
<content:encoded><![CDATA[
arXiv:2509.18985v1 Announce Type: new 
Abstract: Online social networks offer a valuable lens to analyze both individual and collective phenomena. Researchers often use simulators to explore controlled scenarios, and the integration of Large Language Models (LLMs) makes these simulations more realistic by enabling agents to understand and generate natural language content. In this work, we investigate the behavior of LLM-based agents in a simulated microblogging social network. We initialize agents with realistic profiles calibrated on real-world online conversations from the 2022 Italian political election and extend an existing simulator by introducing mechanisms for opinion modeling. We examine how LLM agents simulate online conversations, interact with others, and evolve their opinions under different scenarios. Our results show that LLM agents generate coherent content, form connections, and build a realistic social network structure. However, their generated content displays less heterogeneity in tone and toxicity compared to real data. We also find that LLM-based opinion dynamics evolve over time in ways similar to traditional mathematical models. Varying parameter configurations produces no significant changes, indicating that simulations require more careful cognitive modeling at initialization to replicate human behavior more faithfully. Overall, we demonstrate the potential of LLMs for simulating user behavior in social environments, while also identifying key challenges in capturing heterogeneity and complex dynamics.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction</title>
<link>https://arxiv.org/abs/2509.19002</link>
<guid>https://arxiv.org/abs/2509.19002</guid>
<content:encoded><![CDATA[
arXiv:2509.19002v1 Announce Type: new 
Abstract: Recent advances in multimodal large language models (MLLMs) have significantly enhanced video understanding capabilities, opening new possibilities for practical applications. Yet current video benchmarks focus largely on indoor scenes or short-range outdoor activities, leaving the challenges associated with long-distance travel largely unexplored. Mastering extended geospatial-temporal trajectories is critical for next-generation MLLMs, underpinning real-world tasks such as embodied-AI planning and navigation. To bridge this gap, we present VIR-Bench, a novel benchmark consisting of 200 travel videos that frames itinerary reconstruction as a challenging task designed to evaluate and push forward MLLMs' geospatial-temporal intelligence. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, struggle to achieve high scores, underscoring the difficulty of handling videos that span extended spatial and temporal scales. Moreover, we conduct an in-depth case study in which we develop a prototype travel-planning agent that leverages the insights gained from VIR-Bench. The agent's markedly improved itinerary recommendations verify that our evaluation protocol not only benchmarks models effectively but also translates into concrete performance gains in user-facing applications.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pure Vision Language Action (VLA) Models: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2509.19012</link>
<guid>https://arxiv.org/abs/2509.19012</guid>
<content:encoded><![CDATA[
arXiv:2509.19012v1 Announce Type: new 
Abstract: The emergence of Vision Language Action (VLA) models marks a paradigm shift from traditional policy-based control to generalized robotics, reframing Vision Language Models (VLMs) from passive sequence generators into active agents for manipulation and decision-making in complex, dynamic environments. This survey delves into advanced VLA methods, aiming to provide a clear taxonomy and a systematic, comprehensive review of existing research. It presents a comprehensive analysis of VLA applications across different scenarios and classifies VLA approaches into several paradigms: autoregression-based, diffusion-based, reinforcement-based, hybrid, and specialized methods; while examining their motivations, core strategies, and implementations in detail. In addition, foundational datasets, benchmarks, and simulation platforms are introduced. Building on the current VLA landscape, the review further proposes perspectives on key challenges and future directions to advance research in VLA models and generalizable robotics. By synthesizing insights from over three hundred recent studies, this survey maps the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose VLA methods.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fully Learnable Neural Reward Machines</title>
<link>https://arxiv.org/abs/2509.19017</link>
<guid>https://arxiv.org/abs/2509.19017</guid>
<content:encoded><![CDATA[
arXiv:2509.19017v1 Announce Type: new 
Abstract: Non-Markovian Reinforcement Learning (RL) tasks present significant challenges, as agents must reason over entire trajectories of state-action pairs to make optimal decisions. A common strategy to address this is through symbolic formalisms, such as Linear Temporal Logic (LTL) or automata, which provide a structured way to express temporally extended objectives. However, these approaches often rely on restrictive assumptions -- such as the availability of a predefined Symbol Grounding (SG) function mapping raw observations to high-level symbolic representations, or prior knowledge of the temporal task. In this work, we propose a fully learnable version of Neural Reward Machines (NRM), which can learn both the SG function and the automaton end-to-end, removing any reliance on prior knowledge. Our approach is therefore as easily applicable as classic deep RL (DRL) approaches, while being far more explainable, because of the finite and compact nature of automata. Furthermore, we show that by integrating Fully Learnable Reward Machines (FLNRM) with DRL, our method outperforms previous approaches based on Recurrent Neural Networks (RNNs).
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Human-Robot Interaction in Embodied Intelligence Demands a Shift From Static Privacy Controls to Dynamic Learning</title>
<link>https://arxiv.org/abs/2509.19041</link>
<guid>https://arxiv.org/abs/2509.19041</guid>
<content:encoded><![CDATA[
arXiv:2509.19041v1 Announce Type: new 
Abstract: The reasoning capabilities of embodied agents introduce a critical, under-explored inferential privacy challenge, where the risk of an agent generate sensitive conclusions from ambient data. This capability creates a fundamental tension between an agent's utility and user privacy, rendering traditional static controls ineffective. To address this, this position paper proposes a framework that reframes privacy as a dynamic learning problem grounded in theory of Contextual Integrity (CI). Our approach enables agents to proactively learn and adapt to individual privacy norms through interaction, outlining a research agenda to develop embodied agents that are both capable and function as trustworthy safeguards of user privacy.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code Driven Planning with Domain-Adaptive Critic</title>
<link>https://arxiv.org/abs/2509.19077</link>
<guid>https://arxiv.org/abs/2509.19077</guid>
<content:encoded><![CDATA[
arXiv:2509.19077v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have been widely adopted as task planners for AI agents in sequential decision-making problems, leveraging their extensive world knowledge. However, the gap between their general knowledge and environment-specific requirements often leads to inaccurate plans. To address this, existing approaches rely on frequent LLM queries to iteratively refine plans based on immediate environmental feedback, which incurs substantial query costs. However, this refinement is typically guided by short-term environmental feedback, limiting LLMs from developing plans aligned with long-term rewards. We propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of relying on frequent queries, CoPiC employs LLMs to generate a diverse set of high-level planning programs, which iteratively produce and refine candidate plans. A trained domain-adaptive critic then evaluates these candidates and selects the one most aligned with long-term rewards for execution. Using high-level planning programs as planner and domain-adaptive critic as estimator, CoPiC improves planning while significantly reducing query costs. Results in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC outperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving an average (1) 23.33% improvement in success rate and (2) 91.27% reduction in query costs.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAPPO for Edge Server Monitoring</title>
<link>https://arxiv.org/abs/2509.19079</link>
<guid>https://arxiv.org/abs/2509.19079</guid>
<content:encoded><![CDATA[
arXiv:2509.19079v1 Announce Type: new 
Abstract: In this paper, we consider a goal-oriented communication problem for edge server monitoring, where jobs arrive intermittently at multiple dispatchers and must be assigned to shared edge servers with finite queues and time-varying availability. Accurate knowledge of server status is critical for sustaining high throughput, yet remains challenging under dynamic workloads and partial observability. To address this challenge, each dispatcher maintains server knowledge through two complementary mechanisms: (i) active status queries that provide instantaneous updates at a communication cost, and (ii) job execution feedback that reveals server conditions opportunistically. We formulate a cooperative multi-agent distributed decision-making problem in which dispatchers jointly optimize query scheduling to balance throughput against communication overhead. To solve this problem, we propose a Multi-Agent Proximal Policy Optimization (MAPPO)-based algorithm that leverages centralized training with decentralized execution (CTDE) to learn distributed query-and-dispatch policies under partial and stale observations. Numerical evaluations show that MAPPO achieves superior throughput-cost tradeoffs and significantly outperforms baseline strategies, achieving on average a 30% improvement over the closest baseline.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithms for Adversarially Robust Deep Learning</title>
<link>https://arxiv.org/abs/2509.19100</link>
<guid>https://arxiv.org/abs/2509.19100</guid>
<content:encoded><![CDATA[
arXiv:2509.19100v1 Announce Type: new 
Abstract: Given the widespread use of deep learning models in safety-critical applications, ensuring that the decisions of such models are robust against adversarial exploitation is of fundamental importance. In this thesis, we discuss recent progress toward designing algorithms that exhibit desirable robustness properties. First, we discuss the problem of adversarial examples in computer vision, for which we introduce new technical results, training paradigms, and certification algorithms. Next, we consider the problem of domain generalization, wherein the task is to train neural networks to generalize from a family of training distributions to unseen test distributions. We present new algorithms that achieve state-of-the-art generalization in medical imaging, molecular identification, and image classification. Finally, we study the setting of jailbreaking large language models (LLMs), wherein an adversarial user attempts to design prompts that elicit objectionable content from an LLM. We propose new attacks and defenses, which represent the frontier of progress toward designing robust language-based agents.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Soundness and Consistency of LLM Agents for Executing Test Cases Written in Natural Language</title>
<link>https://arxiv.org/abs/2509.19136</link>
<guid>https://arxiv.org/abs/2509.19136</guid>
<content:encoded><![CDATA[
arXiv:2509.19136v1 Announce Type: new 
Abstract: The use of natural language (NL) test cases for validating graphical user interface (GUI) applications is emerging as a promising direction to manually written executable test scripts, which are costly to develop and difficult to maintain. Recent advances in large language models (LLMs) have opened the possibility of the direct execution of NL test cases by LLM agents. This paper investigates this direction, focusing on the impact on NL test case unsoundness and on test case execution consistency. NL test cases are inherently unsound, as they may yield false failures due to ambiguous instructions or unpredictable agent behaviour. Furthermore, repeated executions of the same NL test case may lead to inconsistent outcomes, undermining test reliability. To address these challenges, we propose an algorithm for executing NL test cases with guardrail mechanisms and specialised agents that dynamically verify the correct execution of each test step. We introduce measures to evaluate the capabilities of LLMs in test execution and one measure to quantify execution consistency. We propose a definition of weak unsoundness to characterise contexts in which NL test case execution remains acceptable, with respect to the industrial quality levels Six Sigma. Our experimental evaluation with eight publicly available LLMs, ranging from 3B to 70B parameters, demonstrates both the potential and current limitations of current LLM agents for GUI testing. Our experiments show that Meta Llama 3.1 70B demonstrates acceptable capabilities in NL test case execution with high execution consistency (above the level 3-sigma). We provide prototype tools, test suites, and results.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scoping Review of Mixed Initiative Visual Analytics in the Automation Renaissance</title>
<link>https://arxiv.org/abs/2509.19152</link>
<guid>https://arxiv.org/abs/2509.19152</guid>
<content:encoded><![CDATA[
arXiv:2509.19152v1 Announce Type: new 
Abstract: Artificial agents are increasingly integrated into data analysis workflows, carrying out tasks that were primarily done by humans. Our research explores how the introduction of automation re-calibrates the dynamic between humans and automating technology. To explore this question, we conducted a scoping review encompassing twenty years of mixed-initiative visual analytic systems. To describe and contrast the relationship between humans and automation, we developed an integrated taxonomy to delineate the objectives of these mixed-initiative visual analytics tools, how much automation they support, and the assumed roles of humans. Here, we describe our qualitative approach of integrating existing theoretical frameworks with new codes we developed. Our analysis shows that the visualization research literature lacks consensus on the definition of mixed-initiative systems and explores a limited potential of the collaborative interaction landscape between people and automation. Our research provides a scaffold to advance the discussion of human-AI collaboration during visual data analysis.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot Coordination</title>
<link>https://arxiv.org/abs/2509.19168</link>
<guid>https://arxiv.org/abs/2509.19168</guid>
<content:encoded><![CDATA[
arXiv:2509.19168v1 Announce Type: new 
Abstract: In this paper, we present a receding-horizon, sampling-based planner capable of reasoning over multimodal policy distributions. By using the cross-entropy method to optimize a multimodal policy under a common cost function, our approach increases robustness against local minima and promotes effective exploration of the solution space. We show that our approach naturally extends to multi-robot collision-free planning, enables agents to share diverse candidate policies to avoid deadlocks, and allows teams to minimize a global objective without incurring the computational complexity of centralized optimization. Numerical simulations demonstrate that employing multiple modes significantly improves success rates in trap environments and in multi-robot collision avoidance. Hardware experiments further validate the approach's real-time feasibility and practical performance.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>YAC: Bridging Natural Language and Interactive Visual Exploration with Generative AI for Biomedical Data Discovery</title>
<link>https://arxiv.org/abs/2509.19182</link>
<guid>https://arxiv.org/abs/2509.19182</guid>
<content:encoded><![CDATA[
arXiv:2509.19182v1 Announce Type: new 
Abstract: Incorporating natural language input has the potential to improve the capabilities of biomedical data discovery interfaces. However, user interface elements and visualizations are still powerful tools for interacting with data, even in the new world of generative AI. In our prototype system, YAC, Yet Another Chatbot, we bridge the gap between natural language and interactive visualizations by generating structured declarative output with a multi-agent system and interpreting that output to render linked interactive visualizations and apply data filters. Furthermore, we include widgets, which allow users to adjust the values of that structured output through user interface elements. We reflect on the capabilities and design of this system with an analysis of its technical dimensions and illustrate the capabilities through four usage scenarios.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications</title>
<link>https://arxiv.org/abs/2509.19185</link>
<guid>https://arxiv.org/abs/2509.19185</guid>
<content:encoded><![CDATA[
arXiv:2509.19185v1 Announce Type: new 
Abstract: Foundation model (FM)-based AI agents are rapidly gaining adoption across diverse domains, but their inherent non-determinism and non-reproducibility pose testing and quality assurance challenges. While recent benchmarks provide task-level evaluations, there is limited understanding of how developers verify the internal correctness of these agents during development.
  To address this gap, we conduct the first large-scale empirical study of testing practices in the AI agent ecosystem, analyzing 39 open-source agent frameworks and 439 agentic applications. We identify ten distinct testing patterns and find that novel, agent-specific methods like DeepEval are seldom used (around 1%), while traditional patterns like negative and membership testing are widely adapted to manage FM uncertainty. By mapping these patterns to canonical architectural components of agent frameworks and agentic applications, we uncover a fundamental inversion of testing effort: deterministic components like Resource Artifacts (tools) and Coordination Artifacts (workflows) consume over 70% of testing effort, while the FM-based Plan Body receives less than 5%. Crucially, this reveals a critical blind spot, as the Trigger component (prompts) remains neglected, appearing in around 1% of all tests.
  Our findings offer the first empirical testing baseline in FM-based agent frameworks and agentic applications, revealing a rational but incomplete adaptation to non-determinism. To address it, framework developers should improve support for novel testing methods, application developers must adopt prompt regression testing, and researchers should explore barriers to adoption. Strengthening these practices is vital for building more robust and dependable AI agents.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Process Reward Leanring for Agentic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.19199</link>
<guid>https://arxiv.org/abs/2509.19199</guid>
<content:encoded><![CDATA[
arXiv:2509.19199v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly trained with reinforcement learning (RL) as autonomous agents that reason and act over long horizons in interactive environments.
  However, sparse and sometimes unverifiable rewards make temporal credit assignment extremely challenging.
  Recent work attempts to integrate process supervision into agent learning but suffers from biased annotation, reward hacking, high-variance from overly fine-grained signals or failtures when state overlap is rare.
  We therefore introduce Online Process Reward Learning (OPRL), a general credit-assignment strategy for agentic RL that integrates seamlessly with standard on-policy algorithms without relying on additional rollouts or explicit step labels.
  In OPRL, we optimize an implicit process reward model (PRM) alternately with the agent's policy to transform trajectory preferences into implicit step rewards through a trajectory-based DPO objective.
  These step rewards are then used to compute step-level advantages, which are combined with episode-level advantages from outcome rewards for policy update, creating a self-reinforcing loop.
  Theoretical findings guarantee that the learned step rewards are consistent with trajectory preferences and act as potential-based shaping rewards, providing bounded gradients to stabilize training.
  Empirically, we evaluate OPRL on three distinct agent benmarks, including WebShop and VisualSokoban, as well as open-ended social interactions with unverfiable rewards in SOTOPIA.
  Crucially, OPRL shows superior performance over frontier LLMs and strong RL baselines across domains, achieving state-of-the-art results with higher sample-efficiency and lower variance during training.
  Further analysis also demonstrates the efficient exploration by OPRL using fewer actions, underscoring its potential for agentic learning in real-world scenarios.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stability and Generalization of Adversarial Diffusion Training</title>
<link>https://arxiv.org/abs/2509.19234</link>
<guid>https://arxiv.org/abs/2509.19234</guid>
<content:encoded><![CDATA[
arXiv:2509.19234v1 Announce Type: new 
Abstract: Algorithmic stability is an established tool for analyzing generalization. While adversarial training enhances model robustness, it often suffers from robust overfitting and an enlarged generalization gap. Although recent work has established the convergence of adversarial training in decentralized networks, its generalization properties remain unexplored. This work presents a stability-based generalization analysis of adversarial training under the diffusion strategy for convex losses. We derive a bound showing that the generalization error grows with both the adversarial perturbation strength and the number of training steps, a finding consistent with single-agent case but novel for decentralized settings. Numerical experiments on logistic regression validate these theoretical predictions.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration</title>
<link>https://arxiv.org/abs/2509.19236</link>
<guid>https://arxiv.org/abs/2509.19236</guid>
<content:encoded><![CDATA[
arXiv:2509.19236v1 Announce Type: new 
Abstract: Proper initialization is crucial for any system, particularly in multi-agent systems (MAS), where it plays a pivotal role in determining both the system's efficiency and effectiveness. However, existing MAS initialization methods do not fully account for the collaborative needs of the generated agents in subsequent stages. Inspired by the principles of effective team composition, we propose AgentInit, which aims to optimize the structure of agent teams. Specifically, in addition to multi-round interactions and reflections between agents during agent generation, AgentInit incorporates a Natural Language to Format mechanism to ensure consistency and standardization. Balanced team selection strategies using Pareto principles are subsequently applied to jointly consider agent team diversity and task relevance to promote effective and efficient collaboration and enhance overall system performance. Experiments show that AgentInit consistently outperforms state-of-the-art initialization methods and pre-defined strategies across various frameworks and tasks, achieving an overall performance improvement of up to 1.2 and 1.6, respectively, while also significantly reducing token consumption. Further analysis confirms its strong transferability to similar tasks and verifies the effectiveness of its key components, demonstrating its capability and adaptability as a reliable MAS initialization method. Source code and models are available at https://github.com/1737423697/AgentInit.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration</title>
<link>https://arxiv.org/abs/2509.19292</link>
<guid>https://arxiv.org/abs/2509.19292</guid>
<content:encoded><![CDATA[
arXiv:2509.19292v1 Announce Type: new 
Abstract: Intelligent agents progress by continually refining their capabilities through actively exploring environments. Yet robot policies often lack sufficient exploration capability due to action mode collapse. Existing methods that encourage exploration typically rely on random perturbations, which are unsafe and induce unstable, erratic behaviors, thereby limiting their effectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a framework that enhances policy exploration and improvement in robotic manipulation. SOE learns a compact latent representation of task-relevant factors and constrains exploration to the manifold of valid actions, ensuring safety, diversity, and effectiveness. It can be seamlessly integrated with arbitrary policy models as a plug-in module, augmenting exploration without degrading the base policy performance. Moreover, the structured latent space enables human-guided exploration, further improving efficiency and controllability. Extensive experiments in both simulation and real-world tasks demonstrate that SOE consistently outperforms prior methods, achieving higher task success rates, smoother and safer exploration, and superior sample efficiency. These results establish on-manifold exploration as a principled approach to sample-efficient policy self-improvement. Project website: https://ericjin2002.github.io/SOE
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Residual Off-Policy RL for Finetuning Behavior Cloning Policies</title>
<link>https://arxiv.org/abs/2509.19301</link>
<guid>https://arxiv.org/abs/2509.19301</guid>
<content:encoded><![CDATA[
arXiv:2509.19301v1 Announce Type: new 
Abstract: Recent advances in behavior cloning (BC) have enabled impressive visuomotor control policies. However, these approaches are limited by the quality of human demonstrations, the manual effort required for data collection, and the diminishing returns from increasing offline data. In comparison, reinforcement learning (RL) trains an agent through autonomous interaction with the environment and has shown remarkable success in various domains. Still, training RL policies directly on real-world robots remains challenging due to sample inefficiency, safety concerns, and the difficulty of learning from sparse rewards for long-horizon tasks, especially for high-degree-of-freedom (DoF) systems. We present a recipe that combines the benefits of BC and RL through a residual learning framework. Our approach leverages BC policies as black-box bases and learns lightweight per-step residual corrections via sample-efficient off-policy RL. We demonstrate that our method requires only sparse binary reward signals and can effectively improve manipulation policies on high-degree-of-freedom (DoF) systems in both simulation and the real world. In particular, we demonstrate, to the best of our knowledge, the first successful real-world RL training on a humanoid robot with dexterous hands. Our results demonstrate state-of-the-art performance in various vision-based tasks, pointing towards a practical pathway for deploying RL in the real world. Project website: https://residual-offpolicy-rl.github.io
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Multi-entity, Multivariate Quickest Change Point Detection</title>
<link>https://arxiv.org/abs/2509.18310</link>
<guid>https://arxiv.org/abs/2509.18310</guid>
<content:encoded><![CDATA[
arXiv:2509.18310v1 Announce Type: cross 
Abstract: We propose a framework for online Change Point Detection (CPD) from multi-entity, multivariate time series data, motivated by applications in crowd monitoring where traditional sensing methods (e.g., video surveillance) may be infeasible. Our approach addresses the challenge of detecting system-wide behavioral shifts in complex, dynamic environments where the number and behavior of individual entities may be uncertain or evolve. We introduce the concept of Individual Deviation from Normality (IDfN), computed via a reconstruction-error-based autoencoder trained on normal behavior. We aggregate these individual deviations using mean, variance, and Kernel Density Estimates (KDE) to yield a System-Wide Anomaly Score (SWAS). To detect persistent or abrupt changes, we apply statistical deviation metrics and the Cumulative Sum (CUSUM) technique to these scores. Our unsupervised approach eliminates the need for labeled data or feature extraction, enabling real-time operation on streaming input. Evaluations on both synthetic datasets and crowd simulations, explicitly designed for anomaly detection in group behaviors, demonstrate that our method accurately detects significant system-level changes, offering a scalable and privacy-preserving solution for monitoring complex multi-agent systems. In addition to this methodological contribution, we introduce new, challenging multi-entity multivariate time series datasets generated from crowd simulations in Unity and coupled nonlinear oscillators. To the best of our knowledge, there is currently no publicly available dataset of this type designed explicitly to evaluate CPD in complex collective and interactive systems, highlighting an essential gap that our work addresses.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Formal Methods for An Iterated Volunteer's Dilemma</title>
<link>https://arxiv.org/abs/2008.12846</link>
<guid>https://arxiv.org/abs/2008.12846</guid>
<content:encoded><![CDATA[
arXiv:2008.12846v4 Announce Type: replace 
Abstract: Game theory provides a framework for studying communication dynamics and emergent phenomena arising from rational agent interactions. We present a model framework for the Volunteer's Dilemma with four key contributions: (1) formulating it as a stochastic concurrent nn n-player game, (2) developing properties to verify model correctness and reachability, (3) constructing strategy synthesis graphs to identify optimal game trajectories, and (4) analyzing parameter correlations with expected local and global rewards over finite time horizons.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Convergence of Multi-Agent Policy Gradient in Markov Potential Games</title>
<link>https://arxiv.org/abs/2106.01969</link>
<guid>https://arxiv.org/abs/2106.01969</guid>
<content:encoded><![CDATA[
arXiv:2106.01969v4 Announce Type: replace 
Abstract: Potential games are arguably one of the most important and widely studied classes of normal form games. They define the archetypal setting of multi-agent coordination as all agent utilities are perfectly aligned with each other via a common potential function. Can this intuitive framework be transplanted in the setting of Markov Games? What are the similarities and differences between multi-agent coordination with and without state dependence? We present a novel definition of Markov Potential Games (MPG) that generalizes prior attempts at capturing complex stateful multi-agent coordination. Counter-intuitively, insights from normal-form potential games do not carry over as MPGs can consist of settings where state-games can be zero-sum games. In the opposite direction, Markov games where every state-game is a potential game are not necessarily MPGs. Nevertheless, MPGs showcase standard desirable properties such as the existence of deterministic Nash policies. In our main technical result, we prove fast convergence of independent policy gradient to Nash policies by adapting recent gradient dominance property arguments developed for single agent MDPs to multi-agent learning settings.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REACT: Real-time Efficiency and Accuracy Compromise for Tradeoffs in Scene Graph Generation</title>
<link>https://arxiv.org/abs/2405.16116</link>
<guid>https://arxiv.org/abs/2405.16116</guid>
<content:encoded><![CDATA[
arXiv:2405.16116v3 Announce Type: replace 
Abstract: Scene Graph Generation (SGG) is a task that encodes visual relationships between objects in images as graph structures. SGG shows significant promise as a foundational component for downstream tasks, such as reasoning for embodied agents. To enable real-time applications, SGG must address the trade-off between performance and inference speed. However, current methods tend to focus on one of the following: (1) improving relation prediction accuracy, (2) enhancing object detection accuracy, or (3) reducing latency, without aiming to balance all three objectives simultaneously. To address this limitation, we propose the Real-time Efficiency and Accuracy Compromise for Tradeoffs in Scene Graph Generation (REACT) architecture, which achieves the highest inference speed among existing SGG models, improving object detection accuracy without sacrificing relation prediction performance. Compared to state-of-the-art approaches, REACT is 2.7 times faster and improves object detection accuracy by 58\%. Furthermore, our proposal significantly reduces model size, with an average of 5.5x fewer parameters. The code is available at https://github.com/Maelic/SGG-Benchmark
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Fair and Efficient Public Transportation: A Bus Stop Model</title>
<link>https://arxiv.org/abs/2411.08784</link>
<guid>https://arxiv.org/abs/2411.08784</guid>
<content:encoded><![CDATA[
arXiv:2411.08784v2 Announce Type: replace 
Abstract: We consider a stylized formal model of public transportation, where a set of agents need to travel along a given road, and there is a bus that runs the length of this road. Each agent has a left terminal and a right terminal between which they wish to travel; they can walk all the way, or walk to/from the nearest stop and use the bus for the rest of their journey. The bus can make a fixed number of stops, and the planner needs to select locations for these stops. We study notions of efficiency and fairness for this setting. First, we give a polynomial-time algorithm for computing a solution that minimizes the total travel time; our approach can capture further extensions of the base model, such as more general cost functions or existing infrastructure. Second, we develop a polynomial-time algorithm that outputs solutions with provable fairness guarantees (such as a variant of the justified representation axiom or $2$-approximate core) as long as the agents' costs only depend on the distance they need to walk. Our simulations indicate that our algorithm almost always outputs fair solutions, even for parameter regimes that do not admit theoretical guarantees.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Agent Framework with Automated Decision Rule Optimization for Cross-Domain Misinformation Detection</title>
<link>https://arxiv.org/abs/2503.23329</link>
<guid>https://arxiv.org/abs/2503.23329</guid>
<content:encoded><![CDATA[
arXiv:2503.23329v2 Announce Type: replace 
Abstract: Misinformation spans various domains, but detection methods trained on specific domains often perform poorly when applied to others. With the rapid development of Large Language Models (LLMs), researchers have begun to utilize LLMs for cross-domain misinformation detection. However, existing LLM-based methods often fail to adequately analyze news in the target domain, limiting their detection capabilities. More importantly, these methods typically rely on manually designed decision rules, which are limited by domain knowledge and expert experience, thus limiting the generalizability of decision rules to different domains. To address these issues, we propose a MultiAgent Framework for cross-domain misinformation detection with Automated Decision Rule Optimization (MARO). Under this framework, we first employs multiple expert agents to analyze target-domain news. Subsequently, we introduce a question-reflection mechanism that guides expert agents to facilitate higherquality analysis. Furthermore, we propose a decision rule optimization approach based on carefully-designed cross-domain validation tasks to iteratively enhance the effectiveness of decision rules in different domains. Experimental results and in-depth analysis on commonlyused datasets demonstrate that MARO achieves significant improvements over existing methods.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disambiguation in Conversational Question Answering in the Era of LLMs and Agents: A Survey</title>
<link>https://arxiv.org/abs/2505.12543</link>
<guid>https://arxiv.org/abs/2505.12543</guid>
<content:encoded><![CDATA[
arXiv:2505.12543v2 Announce Type: replace 
Abstract: Ambiguity remains a fundamental challenge in Natural Language Processing (NLP) due to the inherent complexity and flexibility of human language. With the advent of Large Language Models (LLMs), addressing ambiguity has become even more critical due to their expanded capabilities and applications. In the context of Conversational Question Answering (CQA), this paper explores the definition, forms, and implications of ambiguity for language driven systems, particularly in the context of LLMs. We define key terms and concepts, categorize various disambiguation approaches enabled by LLMs, and provide a comparative analysis of their advantages and disadvantages. We also explore publicly available datasets for benchmarking ambiguity detection and resolution techniques and highlight their relevance for ongoing research. Finally, we identify open problems and future research directions, especially in agentic settings, proposing areas for further investigation. By offering a comprehensive review of current research on ambiguities and disambiguation with LLMs, we aim to contribute to the development of more robust and reliable LLM-based systems.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Mixture of Progressive Parameter-Efficient Expert Library for Lifelong Robot Learning</title>
<link>https://arxiv.org/abs/2506.05985</link>
<guid>https://arxiv.org/abs/2506.05985</guid>
<content:encoded><![CDATA[
arXiv:2506.05985v2 Announce Type: replace 
Abstract: A generalist agent must continuously learn and adapt throughout its lifetime, achieving efficient forward transfer while minimizing catastrophic forgetting. Previous work within the dominant pretrain-then-finetune paradigm has explored parameter-efficient fine-tuning for single-task adaptation, effectively steering a frozen pretrained model with a small number of parameters. However, in the context of lifelong learning, these methods rely on the impractical assumption of a test-time task identifier and restrict knowledge sharing among isolated adapters. To address these limitations, we propose Dynamic Mixture of Progressive Parameter-Efficient Expert Library (DMPEL) for lifelong robot learning. DMPEL progressively builds a low-rank expert library and employs a lightweight router to dynamically combine experts into an end-to-end policy, enabling flexible and efficient lifelong forward transfer. Furthermore, by leveraging the modular structure of the fine-tuned parameters, we introduce expert coefficient replay, which guides the router to accurately retrieve frozen experts for previously encountered tasks. This technique mitigates forgetting while being significantly more storage- and computation-efficient than experience replay over the entire policy. Extensive experiments on the lifelong robot learning benchmark LIBERO demonstrate that our framework outperforms state-of-the-art lifelong learning methods in success rates during continual adaptation, while utilizing minimal trainable parameters and storage.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LogicGuard: Improving Embodied LLM agents through Temporal Logic based Critics</title>
<link>https://arxiv.org/abs/2507.03293</link>
<guid>https://arxiv.org/abs/2507.03293</guid>
<content:encoded><![CDATA[
arXiv:2507.03293v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown promise in zero-shot and single step reasoning and decision making problems, but in long horizon sequential planning tasks, their errors compound, often leading to unreliable or inefficient behavior. We introduce LogicGuard, a modular actor-critic architecture in which an LLM actor is guided by a trajectory level LLM critic that communicates through Linear Temporal Logic (LTL). Our setup combines the reasoning strengths of language models with the guarantees of formal logic. The actor selects high-level actions from natural language observations, while the critic analyzes full trajectories and proposes new LTL constraints that shield the actor from future unsafe or inefficient behavior. LogicGuard supports both fixed safety rules and adaptive, learned constraints, and is model-agnostic: any LLM-based planner can serve as the actor, with LogicGuard acting as a logic-generating wrapper. We formalize planning as graph traversal under symbolic constraints, allowing LogicGuard to analyze failed or suboptimal trajectories and generate new temporal logic rules that improve future behavior. To demonstrate generality, we evaluate LogicGuard across two distinct settings: short-horizon general tasks and long-horizon specialist tasks. On the Behavior benchmark of 100 household tasks, LogicGuard increases task completion rates by 25% over a baseline InnerMonologue planner. On the Minecraft diamond-mining task, which is long-horizon and requires multiple interdependent subgoals, LogicGuard improves both efficiency and safety compared to SayCan and InnerMonologue. These results show that enabling LLMs to supervise each other through temporal logic yields more reliable, efficient and safe decision-making for both embodied agents.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoAgentX: An Automated Framework for Evolving Agentic Workflows</title>
<link>https://arxiv.org/abs/2507.03616</link>
<guid>https://arxiv.org/abs/2507.03616</guid>
<content:encoded><![CDATA[
arXiv:2507.03616v2 Announce Type: replace 
Abstract: Multi-agent systems (MAS) have emerged as a powerful paradigm for orchestrating large language models (LLMs) and specialized tools to collaboratively address complex tasks. However, existing MAS frameworks often require manual workflow configuration and lack native support for dynamic evolution and performance optimization. In addition, many MAS optimization algorithms are not integrated into a unified framework. In this paper, we present EvoAgentX, an open-source platform that automates the generation, execution, and evolutionary optimization of multi-agent workflows. EvoAgentX employs a modular architecture consisting of five core layers: the basic components, agent, workflow, evolving, and evaluation layers. Specifically, within the evolving layer, EvoAgentX integrates three MAS optimization algorithms, TextGrad, AFlow, and MIPRO, to iteratively refine agent prompts, tool configurations, and workflow topologies. We evaluate EvoAgentX on HotPotQA, MBPP, and MATH for multi-hop reasoning, code generation, and mathematical problem solving, respectively, and further assess it on real-world tasks using GAIA. Experimental results show that EvoAgentX consistently achieves significant performance improvements, including a 7.44% increase in HotPotQA F1, a 10.00% improvement in MBPP pass@1, a 10.00% gain in MATH solve accuracy, and an overall accuracy improvement of up to 20.00% on GAIA. The source code is available at: https://github.com/EvoAgentX/EvoAgentX
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMAIA: Interactive Maps AI Assistant for Travel Planning and Geo-Spatial Intelligence</title>
<link>https://arxiv.org/abs/2507.06993</link>
<guid>https://arxiv.org/abs/2507.06993</guid>
<content:encoded><![CDATA[
arXiv:2507.06993v3 Announce Type: replace 
Abstract: Map applications are still largely point-and-click, making it difficult to ask map-centric questions or connect what a camera sees to the surrounding geospatial context with view-conditioned inputs. We introduce IMAIA, an interactive Maps AI Assistant that enables natural-language interaction with both vector (street) maps and satellite imagery, and augments camera inputs with geospatial intelligence to help users understand the world. IMAIA comprises two complementary components. Maps Plus treats the map as first-class context by parsing tiled vector/satellite views into a grid-aligned representation that a language model can query to resolve deictic references (e.g., ``the flower-shaped building next to the park in the top-right''). Places AI Smart Assistant (PAISA) performs camera-aware place understanding by fusing image--place embeddings with geospatial signals (location, heading, proximity) to ground a scene, surface salient attributes, and generate concise explanations. A lightweight multi-agent design keeps latency low and exposes interpretable intermediate decisions. Across map-centric QA and camera-to-place grounding tasks, IMAIA improves accuracy and responsiveness over strong baselines while remaining practical for user-facing deployments. By unifying language, maps, and geospatial cues, IMAIA moves beyond scripted tools toward conversational mapping that is both spatially grounded and broadly usable.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constrained Style Learning from Imperfect Demonstrations under Task Optimality</title>
<link>https://arxiv.org/abs/2507.09371</link>
<guid>https://arxiv.org/abs/2507.09371</guid>
<content:encoded><![CDATA[
arXiv:2507.09371v2 Announce Type: replace 
Abstract: Learning from demonstration has proven effective in robotics for acquiring natural behaviors, such as stylistic motions and lifelike agility, particularly when explicitly defining style-oriented reward functions is challenging. Synthesizing stylistic motions for real-world tasks usually requires balancing task performance and imitation quality. Existing methods generally depend on expert demonstrations closely aligned with task objectives. However, practical demonstrations are often incomplete or unrealistic, causing current methods to boost style at the expense of task performance. To address this issue, we propose formulating the problem as a constrained Markov Decision Process (CMDP). Specifically, we optimize a style-imitation objective with constraints to maintain near-optimal task performance. We introduce an adaptively adjustable Lagrangian multiplier to guide the agent to imitate demonstrations selectively, capturing stylistic nuances without compromising task performance. We validate our approach across multiple robotic platforms and tasks, demonstrating both robust task performance and high-fidelity style learning. On ANYmal-D hardware we show a 14.5% drop in mechanical energy and a more agile gait pattern, showcasing real-world benefits.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEKIA: Expert-Aligned AI Behavior Design for High-Risk Human-AI Interactions</title>
<link>https://arxiv.org/abs/2507.14944</link>
<guid>https://arxiv.org/abs/2507.14944</guid>
<content:encoded><![CDATA[
arXiv:2507.14944v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated technical accuracy in high-risk domains, such as mental health support and special education. However, they often fail to meet the nuanced behavioral expectations of domain experts. This gap constrains AI deployment in sensitive settings. To address this challenge, we introduce LEKIA (Layered Expert Knowledge Injection Architecture), a novel framework built upon the principle of expert-owned AI behavior design. LEKIA's core innovation lies in its dual architecture: a three-layer knowledge injection system featuring our "Supervision Metaphor Cycle", and a dual-agent safety system ensuring robustness and consistency. We implemented and evaluated LEKIA within psychological support scenarios in special education. Experiments indicate that LEKIA improves performance by 14.8% over baseline, driven by substantive increase in alignment with expert expectations while preserving technical accuracy. Beyond providing a reproducible technical framework, this work demonstrates expert-expectation alignment as a measurable evaluation criterion with implications for AI deployment in high-risk domains.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIGDreamer: Privileged Information Guided World Models for Safe Partially Observable Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.02159</link>
<guid>https://arxiv.org/abs/2508.02159</guid>
<content:encoded><![CDATA[
arXiv:2508.02159v2 Announce Type: replace 
Abstract: Partial observability presents a significant challenge for Safe Reinforcement Learning (Safe RL), as it impedes the identification of potential risks and rewards. Leveraging specific types of privileged information during training to mitigate the effects of partial observability has yielded notable empirical successes. In this paper, we propose Asymmetric Constrained Partially Observable Markov Decision Processes (ACPOMDPs) to theoretically examine the advantages of incorporating privileged information in Safe RL. Building upon ACPOMDPs, we propose the Privileged Information Guided Dreamer (PIGDreamer), a model-based RL approach that leverages privileged information to enhance the agent's safety and performance through privileged representation alignment and an asymmetric actor-critic structure. Our empirical results demonstrate that PIGDreamer significantly outperforms existing Safe RL methods. Furthermore, compared to alternative privileged RL methods, our approach exhibits enhanced performance, robustness, and efficiency. Codes are available at: https://github.com/hggforget/PIGDreamer.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XARP Tools: An Extended Reality Platform for Humans and AI Agents</title>
<link>https://arxiv.org/abs/2508.04108</link>
<guid>https://arxiv.org/abs/2508.04108</guid>
<content:encoded><![CDATA[
arXiv:2508.04108v3 Announce Type: replace 
Abstract: Artificial intelligence (AI) and extended reality (XR) are increasingly combined in applications such as motor skill training, personalized feedback, and embodied task guidance. Yet developing AI-XR systems remains challenging due to fragmented toolchains that push developers into ad hoc integrations, diverting their attention away from essential design concerns such as interactivity and context awareness. To address this issue, we present XARP (XR Agent-ready Remote Procedures), a toolkit for AI-XR development designed for both human developers and AI agents. XARP implements JSON-based remote procedure calls that allow server-side Python to control XR clients, providing a high-level abstraction over low-level integration details. Humans can use XARP as a Python library to write XR applications with reduced implementation overhead. AI agents operate with the same abstraction to dynamically call tools to generate XR applications at runtime in response to context changes and user requests. XARP offers Model Context Protocol (MCP) connectivity that allows third-party agents and tools to leverage XR capabilities, previously unavailable. We conducted three case studies that demonstrate XARP supports a variety of AI-XR applications, including AI-guided fencing, drone assistance, and room layout design. We evaluated XARP in a walkthrough study with 24 AI and XR developers. UTAUT scores indicate high potential for adoption, and participants reported that XARP can reduce authoring time, lower entry barriers for developers unfamiliar with AI or XR, and enable the implementation of novel AI-XR systems.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaFed: Advancing Privacy, Performance, and Sustainability in Federated Metaverse Systems</title>
<link>https://arxiv.org/abs/2508.17341</link>
<guid>https://arxiv.org/abs/2508.17341</guid>
<content:encoded><![CDATA[
arXiv:2508.17341v2 Announce Type: replace 
Abstract: The rapid expansion of immersive Metaverse applications introduces complex challenges at the intersection of performance, privacy, and environmental sustainability. Centralized architectures fall short in addressing these demands, often resulting in elevated energy consumption, latency, and privacy concerns. This paper proposes MetaFed, a decentralized federated learning (FL) framework that enables sustainable and intelligent resource orchestration for Metaverse environments. MetaFed integrates (i) multi-agent reinforcement learning for dynamic client selection, (ii) privacy-preserving FL using homomorphic encryption, and (iii) carbon-aware scheduling aligned with renewable energy availability. Evaluations on MNIST and CIFAR-10 using lightweight ResNet architectures demonstrate that MetaFed achieves up to 25% reduction in carbon emissions compared to conventional approaches, while maintaining high accuracy and minimal communication overhead. These results highlight MetaFed as a scalable solution for building environmentally responsible and privacy-compliant Metaverse infrastructures.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Language Model Agents to Find Vulnerabilities with CTF-Dojo</title>
<link>https://arxiv.org/abs/2508.18370</link>
<guid>https://arxiv.org/abs/2508.18370</guid>
<content:encoded><![CDATA[
arXiv:2508.18370v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated exceptional capabilities when trained within executable runtime environments, notably excelling at software engineering tasks through verified feedback loops. Yet, scalable and generalizable execution-grounded environments remain scarce, limiting progress in training more capable ML agents. We introduce CTF-Dojo, the first large-scale executable runtime tailored for training LLMs with verifiable feedback, featuring 658 fully functional Capture-The-Flag (CTF)-style challenges containerized in Docker with guaranteed reproducibility. To enable rapid scaling without manual intervention, we develop CTF-Forge, an automated pipeline that transforms publicly available artifacts into ready-to-use execution environments in minutes, eliminating weeks of expert configuration traditionally required. We trained LLM-based agents on just 486 high-quality, execution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute gains over strong baselines across three competitive benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1, establishing a new open-weight state-of-the-art that rivals frontier models like DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a benchmark for executable-agent learning, CTF-Dojo demonstrates that execution-grounded training signals are not only effective but pivotal in advancing high-performance ML agents without dependence on costly proprietary systems.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AHA - Predicting What Matters Next: Online Highlight Detection Without Looking Ahead</title>
<link>https://arxiv.org/abs/2509.16421</link>
<guid>https://arxiv.org/abs/2509.16421</guid>
<content:encoded><![CDATA[
arXiv:2509.16421v2 Announce Type: replace 
Abstract: Real-time understanding of continuous video streams is essential for intelligent agents operating in high-stakes environments, including autonomous vehicles, surveillance drones, and disaster response robots. Yet, most existing video understanding and highlight detection methods assume access to the entire video during inference, making them unsuitable for online or streaming scenarios. In particular, current models optimize for offline summarization, failing to support step-by-step reasoning needed for real-time decision-making. We introduce Aha, an autoregressive highlight detection framework that predicts the relevance of each video frame against a task described in natural language. Without accessing future video frames, Aha utilizes a multimodal vision-language model and lightweight, decoupled heads trained on a large, curated dataset of human-centric video labels. To enable scalability, we introduce the Dynamic SinkCache mechanism that achieves constant memory usage across infinite-length streams without degrading performance on standard benchmarks. This encourages the hidden representation to capture high-level task objectives, enabling effective frame-level rankings for informativeness, relevance, and uncertainty with respect to the natural language task. Aha achieves state-of-the-art (SOTA) performance on highlight detection benchmarks, surpassing even prior offline, full-context approaches and video-language models by +5.9% on TVSum and +8.3% on Mr. Hisum in mAP (mean Average Precision). We explore Aha's potential for real-world robotics applications given a task-oriented natural language input and a continuous, robot-centric video. Both experiments demonstrate Aha's potential effectiveness as a real-time reasoning module for downstream planning and long-horizon understanding.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-scale Temporal Prediction via Incremental Generation and Multi-agent Collaboration</title>
<link>https://arxiv.org/abs/2509.17429</link>
<guid>https://arxiv.org/abs/2509.17429</guid>
<content:encoded><![CDATA[
arXiv:2509.17429v2 Announce Type: replace 
Abstract: Accurate temporal prediction is the bridge between comprehensive scene understanding and embodied artificial intelligence. However, predicting multiple fine-grained states of a scene at multiple temporal scales is difficult for vision-language models. We formalize the Multi-Scale Temporal Prediction (MSTP) task in general and surgical scenes by decomposing multi-scale into two orthogonal dimensions: the temporal scale, forecasting states of humans and surgery at varying look-ahead intervals, and the state scale, modeling a hierarchy of states in general and surgical scenes. For example, in general scenes, states of contact relationships are finer-grained than states of spatial relationships. In surgical scenes, medium-level steps are finer-grained than high-level phases yet remain constrained by their encompassing phase. To support this unified task, we introduce the first MSTP Benchmark, featuring synchronized annotations across multiple state scales and temporal scales. We further propose a method, Incremental Generation and Multi-agent Collaboration (IG-MC), which integrates two key innovations. First, we present a plug-and-play incremental generation module that continuously synthesizes up-to-date visual previews at expanding temporal scales to inform multiple decision-making agents, keeping decisions and generated visuals synchronized and preventing performance degradation as look-ahead intervals lengthen. Second, we present a decision-driven multi-agent collaboration framework for multi-state prediction, comprising generation, initiation, and multi-state assessment agents that dynamically trigger and evaluate prediction cycles to balance global coherence and local fidelity.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device</title>
<link>https://arxiv.org/abs/2509.17430</link>
<guid>https://arxiv.org/abs/2509.17430</guid>
<content:encoded><![CDATA[
arXiv:2509.17430v2 Announce Type: replace 
Abstract: The field of Embodied AI predominantly relies on simulation for training and evaluation, often using either fully synthetic environments that lack photorealism or high-fidelity real-world reconstructions captured with expensive hardware. As a result, sim-to-real transfer remains a major challenge. In this paper, we introduce EmbodiedSplat, a novel approach that personalizes policy training by efficiently capturing the deployment environment and fine-tuning policies within the reconstructed scenes. Our method leverages 3D Gaussian Splatting (GS) and the Habitat-Sim simulator to bridge the gap between realistic scene capture and effective training environments. Using iPhone-captured deployment scenes, we reconstruct meshes via GS, enabling training in settings that closely approximate real-world conditions. We conduct a comprehensive analysis of training strategies, pre-training datasets, and mesh reconstruction techniques, evaluating their impact on sim-to-real predictivity in real-world scenarios. Experimental results demonstrate that agents fine-tuned with EmbodiedSplat outperform both zero-shot baselines pre-trained on large-scale real-world datasets (HM3D) and synthetically generated datasets (HSSD), achieving absolute success rate improvements of 20% and 40% on real-world Image Navigation task. Moreover, our approach yields a high sim-vs-real correlation (0.87-0.97) for the reconstructed meshes, underscoring its effectiveness in adapting policies to diverse environments with minimal effort. Project page: https://gchhablani.github.io/embodied-splat.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforced Generation of Combinatorial Structures: Applications to Complexity Theory</title>
<link>https://arxiv.org/abs/2509.18057</link>
<guid>https://arxiv.org/abs/2509.18057</guid>
<content:encoded><![CDATA[
arXiv:2509.18057v2 Announce Type: replace 
Abstract: We explore whether techniques from AI can help discover new combinatorial structures that improve on known limits on efficient algorithms. Specifically, we use AlphaEvolve (an LLM coding agent) to study two settings:
  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a recent result of Kunisky and Yu to obtain near-optimal upper and (conditional) lower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on random 3- and 4-regular graphs. Our improved lower bounds are obtained by constructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using AlphaEvolve. Additionally, via analytical arguments we strengthen the upper bounds to settle the computational hardness of these questions up to an error in the third decimal place.
  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new inapproximability results, proving that it is NP-hard to approximate MAX-4-CUT and MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using AlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves upon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current best gadget-based inapproximability result of $0.9853$, but falls short of improving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget reduction from "standard" H{\aa}stad-style PCPs.
  A key technical challenge we faced: verifying a candidate construction produced by AlphaEvolve is costly (often requiring exponential time). In both settings above, our results were enabled by using AlphaEvolve itself to evolve the verification procedure to be faster (sometimes by $10,000\times$). We conclude with a discussion of norms by which to assess the assistance from AI in developing proofs.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided Role-playing Agents</title>
<link>https://arxiv.org/abs/2509.15233</link>
<guid>https://arxiv.org/abs/2509.15233</guid>
<content:encoded><![CDATA[
arXiv:2509.15233v1 Announce Type: new 
Abstract: Role-playing agents (RPAs) have attracted growing interest for their ability to simulate immersive and interactive characters. However, existing approaches primarily focus on static role profiles, overlooking the dynamic perceptual abilities inherent to humans. To bridge this gap, we introduce the concept of dynamic role profiles by incorporating video modality into RPAs. To support this, we construct Role-playing-Video60k, a large-scale, high-quality dataset comprising 60k videos and 700k corresponding dialogues. Based on this dataset, we develop a comprehensive RPA framework that combines adaptive temporal sampling with both dynamic and static role profile representations. Specifically, the dynamic profile is created by adaptively sampling video frames and feeding them to the LLM in temporal order, while the static profile consists of (1) character dialogues from training videos during fine-tuning, and (2) a summary context from the input video during inference. This joint integration enables RPAs to generate greater responses. Furthermore, we propose a robust evaluation method covering eight metrics. Experimental results demonstrate the effectiveness of our framework, highlighting the importance of dynamic role profiles in developing RPAs.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MICA: Multi-Agent Industrial Coordination Assistant</title>
<link>https://arxiv.org/abs/2509.15237</link>
<guid>https://arxiv.org/abs/2509.15237</guid>
<content:encoded><![CDATA[
arXiv:2509.15237v1 Announce Type: new 
Abstract: Industrial workflows demand adaptive and trustworthy assistance that can operate under limited computing, connectivity, and strict privacy constraints. In this work, we present MICA (Multi-Agent Industrial Coordination Assistant), a perception-grounded and speech-interactive system that delivers real-time guidance for assembly, troubleshooting, part queries, and maintenance. MICA coordinates five role-specialized language agents, audited by a safety checker, to ensure accurate and compliant support. To achieve robust step understanding, we introduce Adaptive Step Fusion (ASF), which dynamically blends expert reasoning with online adaptation from natural speech feedback. Furthermore, we establish a new multi-agent coordination benchmark across representative task categories and propose evaluation metrics tailored to industrial assistance, enabling systematic comparison of different coordination topologies. Our experiments demonstrate that MICA consistently improves task success, reliability, and responsiveness over baseline structures, while remaining deployable on practical offline hardware. Together, these contributions highlight MICA as a step toward deployable, privacy-preserving multi-agent assistants for dynamic factory environments. The source code will be made publicly available at https://github.com/Kratos-Wen/MICA.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Plans for Belief-Desire-Intention (BDI) Agents Using Alternating-Time Temporal Logic (ATL)</title>
<link>https://arxiv.org/abs/2509.15238</link>
<guid>https://arxiv.org/abs/2509.15238</guid>
<content:encoded><![CDATA[
arXiv:2509.15238v1 Announce Type: new 
Abstract: Belief-Desire-Intention (BDI) is a framework for modelling agents based on their beliefs, desires, and intentions. Plans are a central component of BDI agents, and define sequences of actions that an agent must undertake to achieve a certain goal. Existing approaches to plan generation often require significant manual effort, and are mainly focused on single-agent systems. As a result, in this work, we have developed a tool that automatically generates BDI plans using Alternating-Time Temporal Logic (ATL). By using ATL, the plans generated accommodate for possible competition or cooperation between the agents in the system. We demonstrate the effectiveness of the tool by generating plans for an illustrative game that requires agent collaboration to achieve a shared goal. We show that the generated plans allow the agents to successfully attain this goal.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning</title>
<link>https://arxiv.org/abs/2509.15250</link>
<guid>https://arxiv.org/abs/2509.15250</guid>
<content:encoded><![CDATA[
arXiv:2509.15250v1 Announce Type: new 
Abstract: Large models achieve strong performance on Vision-and-Language Navigation (VLN) tasks, but are costly to run in resource-limited environments. Token pruning offers appealing tradeoffs for efficiency with minimal performance loss by reducing model input size, but prior work overlooks VLN-specific challenges. For example, information loss from pruning can effectively increase computational cost due to longer walks. Thus, the inability to identify uninformative tokens undermines the supposed efficiency gains from pruning. To address this, we propose Navigation-Aware Pruning (NAP), which uses navigation-specific traits to simplify the pruning process by pre-filtering tokens into foreground and background. For example, image views are filtered based on whether the agent can navigate in that direction. We also extract navigation-relevant instructions using a Large Language Model. After filtering, we focus pruning on background tokens, minimizing information loss. To further help avoid increases in navigation length, we discourage backtracking by removing low-importance navigation nodes. Experiments on standard VLN benchmarks show NAP significantly outperforms prior work, preserving higher success rates while saving more than 50% FLOPS.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Distribution Shift Problem in Transportation Networks using Reinforcement Learning and AI</title>
<link>https://arxiv.org/abs/2509.15291</link>
<guid>https://arxiv.org/abs/2509.15291</guid>
<content:encoded><![CDATA[
arXiv:2509.15291v1 Announce Type: new 
Abstract: The use of Machine Learning (ML) and Artificial Intelligence (AI) in smart transportation networks has increased significantly in the last few years. Among these ML and AI approaches, Reinforcement Learning (RL) has been shown to be a very promising approach by several authors. However, a problem with using Reinforcement Learning in Traffic Signal Control is the reliability of the trained RL agents due to the dynamically changing distribution of the input data with respect to the distribution of the data used for training. This presents a major challenge and a reliability problem for the trained network of AI agents and could have very undesirable and even detrimental consequences if a suitable solution is not found. Several researchers have tried to address this problem using different approaches. In particular, Meta Reinforcement Learning (Meta RL) promises to be an effective solution. In this paper, we evaluate and analyze a state-of-the-art Meta RL approach called MetaLight and show that, while under certain conditions MetaLight can indeed lead to reasonably good results, under some other conditions it might not perform well (with errors of up to 22%), suggesting that Meta RL schemes are often not robust enough and can even pose major reliability problems.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Good are Foundation Models in Step-by-Step Embodied Reasoning?</title>
<link>https://arxiv.org/abs/2509.15293</link>
<guid>https://arxiv.org/abs/2509.15293</guid>
<content:encoded><![CDATA[
arXiv:2509.15293v1 Announce Type: new 
Abstract: Embodied agents operating in the physical world must make decisions that are not only effective but also safe, spatially coherent, and grounded in context. While recent advances in large multimodal models (LMMs) have shown promising capabilities in visual understanding and language generation, their ability to perform structured reasoning for real-world embodied tasks remains underexplored. In this work, we aim to understand how well foundation models can perform step-by-step reasoning in embodied environments. To this end, we propose the Foundation Model Embodied Reasoning (FoMER) benchmark, designed to evaluate the reasoning capabilities of LMMs in complex embodied decision-making scenarios. Our benchmark spans a diverse set of tasks that require agents to interpret multimodal observations, reason about physical constraints and safety, and generate valid next actions in natural language. We present (i) a large-scale, curated suite of embodied reasoning tasks, (ii) a novel evaluation framework that disentangles perceptual grounding from action reasoning, and (iii) empirical analysis of several leading LMMs under this setting. Our benchmark includes over 1.1k samples with detailed step-by-step reasoning across 10 tasks and 8 embodiments, covering three different robot types. Our results highlight both the potential and current limitations of LMMs in embodied reasoning, pointing towards key challenges and opportunities for future research in robot intelligence. Our data and code will be made publicly available.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagnostics of cognitive failures in multi-agent expert systems using dynamic evaluation protocols and subsequent mutation of the processing context</title>
<link>https://arxiv.org/abs/2509.15366</link>
<guid>https://arxiv.org/abs/2509.15366</guid>
<content:encoded><![CDATA[
arXiv:2509.15366v1 Announce Type: new 
Abstract: The rapid evolution of neural architectures - from multilayer perceptrons to large-scale Transformer-based models - has enabled language models (LLMs) to exhibit emergent agentic behaviours when equipped with memory, planning, and external tool use. However, their inherent stochasticity and multi-step decision processes render classical evaluation methods inadequate for diagnosing agentic performance. This work introduces a diagnostic framework for expert systems that not only evaluates but also facilitates the transfer of expert behaviour into LLM-powered agents. The framework integrates (i) curated golden datasets of expert annotations, (ii) silver datasets generated through controlled behavioural mutation, and (iii) an LLM-based Agent Judge that scores and prescribes targeted improvements. These prescriptions are embedded into a vectorized recommendation map, allowing expert interventions to propagate as reusable improvement trajectories across multiple system instances. We demonstrate the framework on a multi-agent recruiter-assistant system, showing that it uncovers latent cognitive failures - such as biased phrasing, extraction drift, and tool misrouting - while simultaneously steering agents toward expert-level reasoning and style. The results establish a foundation for standardized, reproducible expert behaviour transfer in stochastic, tool-augmented LLM agents, moving beyond static evaluation to active expert system refinement.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re-imagining Behavioral Sleep Medicine: Designing Conversational Sleep Diary and Visualization Tool</title>
<link>https://arxiv.org/abs/2509.15378</link>
<guid>https://arxiv.org/abs/2509.15378</guid>
<content:encoded><![CDATA[
arXiv:2509.15378v1 Announce Type: new 
Abstract: The sleep diary is a widely used clinical tool for understanding sleep disorders; however, low patient compliance and limited capture of contextual information constrain its effectiveness and leave specialists with an incomplete picture of patients' sleep-related behaviors. In this work, we re-imagine Behavioral Sleep Medicine (BSM) by designing a voice-based conversational sleep diary and specialist-facing visualization tool. Through this design process, we probed specialists' vision of how conversational agents (CAs) could extend beyond diary intake to enhance behavioral sleep medicine. Our multi-stage approach included: (1) interviews with specialists to identify shortcomings in current use of text-based diaries, (2) iterative co-design of a conversational diary and visualization tool, and (3) focus groups to explore the broader potential of CAs in BSM. This work contributes design insights into how CAs can support behavioral interventions, highlights opportunities and challenges for integration into practice, and expands the design space of CAs for behavioral health.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Agent Grouping ECBS: Scaling Windowed Multi-Agent Path Finding with Completeness Guarantees</title>
<link>https://arxiv.org/abs/2509.15381</link>
<guid>https://arxiv.org/abs/2509.15381</guid>
<content:encoded><![CDATA[
arXiv:2509.15381v1 Announce Type: new 
Abstract: Multi-Agent Path Finding (MAPF) is the problem of finding a set of collision-free paths for a team of agents. Although several MAPF methods which solve full-horizon MAPF have completeness guarantees, very few MAPF methods that plan partial paths have completeness guarantees. Recent work introduced the Windowed Complete MAPF (WinC-MAPF) framework, which shows how windowed optimal MAPF solvers (e.g., SS-CBS) can use heuristic updates and disjoint agent groups to maintain completeness even when planning partial paths (Veerapaneni et al. 2024). A core limitation of WinC-MAPF is that they required optimal MAPF solvers. Our main contribution is to extend WinC-MAPF by showing how we can use a bounded suboptimal solver while maintaining completeness. In particular, we design Dynamic Agent Grouping ECBS (DAG-ECBS) which dynamically creates and plans agent groups while maintaining that each agent group solution is bounded suboptimal. We prove how DAG-ECBS can maintain completeness in the WinC-MAPF framework. DAG-ECBS shows improved scalability compared to SS-CBS and can outperform windowed ECBS without completeness guarantees. More broadly, our work serves as a blueprint for designing more MAPF methods that can use the WinC-MAPF framework.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning in Stackelberg Mean Field Games: A Non-Asymptotic Analysis</title>
<link>https://arxiv.org/abs/2509.15392</link>
<guid>https://arxiv.org/abs/2509.15392</guid>
<content:encoded><![CDATA[
arXiv:2509.15392v1 Announce Type: new 
Abstract: We study policy optimization in Stackelberg mean field games (MFGs), a hierarchical framework for modeling the strategic interaction between a single leader and an infinitely large population of homogeneous followers. The objective can be formulated as a structured bi-level optimization problem, in which the leader needs to learn a policy maximizing its reward, anticipating the response of the followers. Existing methods for solving these (and related) problems often rely on restrictive independence assumptions between the leader's and followers' objectives, use samples inefficiently due to nested-loop algorithm structure, and lack finite-time convergence guarantees. To address these limitations, we propose AC-SMFG, a single-loop actor-critic algorithm that operates on continuously generated Markovian samples. The algorithm alternates between (semi-)gradient updates for the leader, a representative follower, and the mean field, and is simple to implement in practice. We establish the finite-time and finite-sample convergence of the algorithm to a stationary point of the Stackelberg objective. To our knowledge, this is the first Stackelberg MFG algorithm with non-asymptotic convergence guarantees. Our key assumption is a "gradient alignment" condition, which requires that the full policy gradient of the leader can be approximated by a partial component of it, relaxing the existing leader-follower independence assumption. Simulation results in a range of well-established economics environments demonstrate that AC-SMFG outperforms existing multi-agent and MFG learning baselines in policy quality and convergence speed.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoCaL: Countering Surface Bias in Code Evaluation Metrics</title>
<link>https://arxiv.org/abs/2509.15397</link>
<guid>https://arxiv.org/abs/2509.15397</guid>
<content:encoded><![CDATA[
arXiv:2509.15397v1 Announce Type: new 
Abstract: With the increasing popularity of large language models (LLMs) and LLM-based agents, reliable and effective code evaluation metrics (CEMs) have become crucial for progress across several software engineering tasks. While popular benchmarks often provide test cases to assess the correctness of generated code, crafting and executing test cases is expensive. Reference-based CEMs provide a cheaper alternative by scoring a candidate program based on its functional similarity to a reference. Although prior research has focused on reporting the weak correlation between these CEMs and functional correctness, the causes are only assumed, and plausible solutions remain unexplored. In this work, we critically evaluate four state-of-the-art reference-based CEMs, revealing their strong bias towards surface-level features rather than code functionality. Despite this surface bias, current evaluation datasets for these CEMs rarely include code pairs that are surface-similar yet functionally dissimilar, or functionally similar yet surface-dissimilar. To mitigate this gap, we propose LoCaL (Looks Can Lie), a CEM evaluation benchmark, with 3117 code pairs at both the method and program levels. Each pair is labeled with a functional similarity score and aims to target regions where CEMs are likely to perform poorly. The functional similarity scores are calculated through differential fuzzing, which eliminates the need for predefined test cases and, at the same time, improves the reliability of the scores by executing an order of magnitude more tests than prior work. We find that all four CEMs show significant performance degradation on LoCaL, compared to the baselines. Finally, based on our findings, we draw the implication that exposing CEMs to LoCaL-like data might facilitate the development of metrics that are robust to surface bias.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.15435</link>
<guid>https://arxiv.org/abs/2509.15435</guid>
<content:encoded><![CDATA[
arXiv:2509.15435v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) exhibit strong multimodal capabilities but remain vulnerable to hallucinations from intrinsic errors and adversarial attacks from external exploitations, limiting their reliability in real-world applications. We present ORCA, an agentic reasoning framework that improves the factual accuracy and adversarial robustness of pretrained LVLMs through test-time structured inference reasoning with a suite of small vision models (less than 3B parameters). ORCA operates via an Observe--Reason--Critique--Act loop, querying multiple visual tools with evidential questions, validating cross-model inconsistencies, and refining predictions iteratively without access to model internals or retraining. ORCA also stores intermediate reasoning traces, which supports auditable decision-making. Though designed primarily to mitigate object-level hallucinations, ORCA also exhibits emergent adversarial robustness without requiring adversarial training or defense mechanisms. We evaluate ORCA across three settings: (1) clean images on hallucination benchmarks, (2) adversarially perturbed images without defense, and (3) adversarially perturbed images with defense applied. On the POPE hallucination benchmark, ORCA improves standalone LVLM performance by +3.64\% to +40.67\% across different subsets. Under adversarial perturbations on POPE, ORCA achieves an average accuracy gain of +20.11\% across LVLMs. When combined with defense techniques on adversarially perturbed AMBER images, ORCA further improves standalone LVLM performance, with gains ranging from +1.20\% to +48.00\% across evaluation metrics. These results demonstrate that ORCA offers a promising path toward building more reliable and robust multimodal systems.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where Do I 'Add the Egg'?: Exploring Agency and Ownership in AI Creative Co-Writing Systems</title>
<link>https://arxiv.org/abs/2509.15440</link>
<guid>https://arxiv.org/abs/2509.15440</guid>
<content:encoded><![CDATA[
arXiv:2509.15440v1 Announce Type: new 
Abstract: AI co-writing systems challenge long held ideals about agency and ownership in the creative process, thereby hindering widespread adoption. In order to address this, we investigate conceptions of agency and ownership in AI creative co-writing. Drawing on insights from a review of commercial systems, we developed three co-writing systems with identical functionality but distinct interface metaphors: agentic, tool-like, and magical. Through interviews with professional and non-professional writers (n = 18), we explored how these metaphors influenced participants' sense of control and authorship. Our analysis resulted in a taxonomy of agency and ownership subtypes and underscore how tool-like metaphors shift writers' expected points of control while agentic metaphors foreground conceptual contributions. We argue that interface metaphors not only guide expectations of control but also frame conceptions of authorship. We conclude with recommendations for the design of AI co-writing systems, emphasizing how metaphor shapes user experience and creative practice.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable AI-Enhanced Supervisory Control for Robust Multi-Agent Robotic Systems</title>
<link>https://arxiv.org/abs/2509.15491</link>
<guid>https://arxiv.org/abs/2509.15491</guid>
<content:encoded><![CDATA[
arXiv:2509.15491v1 Announce Type: new 
Abstract: We present an explainable AI-enhanced supervisory control framework for multi-agent robotics that combines (i) a timed-automata supervisor for safe, auditable mode switching, (ii) robust continuous control (Lyapunov-based controller for large-angle maneuver; sliding-mode controller (SMC) with boundary layers for precision and disturbance rejection), and (iii) an explainable predictor that maps mission context to gains and expected performance (energy, error). Monte Carlo-driven optimization provides the training data, enabling transparent real-time trade-offs.
  We validated the approach in two contrasting domains, spacecraft formation flying and autonomous underwater vehicles (AUVs). Despite different environments (gravity/actuator bias vs. hydrodynamic drag/currents), both share uncertain six degrees of freedom (6-DOF) rigid-body dynamics, relative motion, and tight tracking needs, making them representative of general robotic systems. In the space mission, the supervisory logic selects parameters that meet mission criteria. In AUV leader-follower tests, the same SMC structure maintains a fixed offset under stochastic currents with bounded steady error. In spacecraft validation, the SMC controller achieved submillimeter alignment with 21.7% lower tracking error and 81.4% lower energy consumption compared to Proportional-Derivative PD controller baselines. At the same time, in AUV tests, SMC maintained bounded errors under stochastic currents. These results highlight both the portability and the interpretability of the approach for safety-critical, resource-constrained multi-agent robotics.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KoopCast: Trajectory Forecasting via Koopman Operators</title>
<link>https://arxiv.org/abs/2509.15513</link>
<guid>https://arxiv.org/abs/2509.15513</guid>
<content:encoded><![CDATA[
arXiv:2509.15513v1 Announce Type: new 
Abstract: We present KoopCast, a lightweight yet efficient model for trajectory forecasting in general dynamic environments. Our approach leverages Koopman operator theory, which enables a linear representation of nonlinear dynamics by lifting trajectories into a higher-dimensional space. The framework follows a two-stage design: first, a probabilistic neural goal estimator predicts plausible long-term targets, specifying where to go; second, a Koopman operator-based refinement module incorporates intention and history into a nonlinear feature space, enabling linear prediction that dictates how to go. This dual structure not only ensures strong predictive accuracy but also inherits the favorable properties of linear operators while faithfully capturing nonlinear dynamics. As a result, our model offers three key advantages: (i) competitive accuracy, (ii) interpretability grounded in Koopman spectral theory, and (iii) low-latency deployment. We validate these benefits on ETH/UCY, the Waymo Open Motion Dataset, and nuScenes, which feature rich multi-agent interactions and map-constrained nonlinear motion. Across benchmarks, KoopCast consistently delivers high predictive accuracy together with mode-level interpretability and practical efficiency.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages</title>
<link>https://arxiv.org/abs/2509.15518</link>
<guid>https://arxiv.org/abs/2509.15518</guid>
<content:encoded><![CDATA[
arXiv:2509.15518v1 Announce Type: new 
Abstract: Slang is a commonly used type of informal language that poses a daunting challenge to NLP systems. Recent advances in large language models (LLMs), however, have made the problem more approachable. While LLM agents are becoming more widely applied to intermediary tasks such as slang detection and slang interpretation, their generalizability and reliability are heavily dependent on whether these models have captured structural knowledge about slang that align well with human attested slang usages. To answer this question, we contribute a systematic comparison between human and machine-generated slang usages. Our evaluative framework focuses on three core aspects: 1) Characteristics of the usages that reflect systematic biases in how machines perceive slang, 2) Creativity reflected by both lexical coinages and word reuses employed by the slang usages, and 3) Informativeness of the slang usages when used as gold-standard examples for model distillation. By comparing human-attested slang usages from the Online Slang Dictionary (OSD) and slang generated by GPT-4o and Llama-3, we find significant biases in how LLMs perceive slang. Our results suggest that while LLMs have captured significant knowledge about the creative aspects of slang, such knowledge does not align with humans sufficiently to enable LLMs for extrapolative tasks such as linguistic analyses.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fully Decentralized Cooperative Multi-Agent Reinforcement Learning is A Context Modeling Problem</title>
<link>https://arxiv.org/abs/2509.15519</link>
<guid>https://arxiv.org/abs/2509.15519</guid>
<content:encoded><![CDATA[
arXiv:2509.15519v1 Announce Type: new 
Abstract: This paper studies fully decentralized cooperative multi-agent reinforcement learning, where each agent solely observes the states, its local actions, and the shared rewards. The inability to access other agents' actions often leads to non-stationarity during value function updates and relative overgeneralization during value function estimation, hindering effective cooperative policy learning. However, existing works fail to address both issues simultaneously, due to their inability to model the joint policy of other agents in a fully decentralized setting. To overcome this limitation, we propose a novel method named Dynamics-Aware Context (DAC), which formalizes the task, as locally perceived by each agent, as an Contextual Markov Decision Process, and further addresses both non-stationarity and relative overgeneralization through dynamics-aware context modeling. Specifically, DAC attributes the non-stationary local task dynamics of each agent to switches between unobserved contexts, each corresponding to a distinct joint policy. Then, DAC models the step-wise dynamics distribution using latent variables and refers to them as contexts. For each agent, DAC introduces a context-based value function to address the non-stationarity issue during value function update. For value function estimation, an optimistic marginal value is derived to promote the selection of cooperative actions, thereby addressing the relative overgeneralization issue. Experimentally, we evaluate DAC on various cooperative tasks (including matrix game, predator and prey, and SMAC), and its superior performance against multiple baselines validates its effectiveness.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models</title>
<link>https://arxiv.org/abs/2509.15536</link>
<guid>https://arxiv.org/abs/2509.15536</guid>
<content:encoded><![CDATA[
arXiv:2509.15536v1 Announce Type: new 
Abstract: World models allow agents to simulate the consequences of actions in imagined environments for planning, control, and long-horizon decision-making. However, existing autoregressive world models struggle with visually coherent predictions due to disrupted spatial structure, inefficient decoding, and inadequate motion modeling. In response, we propose \textbf{S}cale-wise \textbf{A}utoregression with \textbf{M}otion \textbf{P}r\textbf{O}mpt (\textbf{SAMPO}), a hybrid framework that combines visual autoregressive modeling for intra-frame generation with causal modeling for next-frame generation. Specifically, SAMPO integrates temporal causal decoding with bidirectional spatial attention, which preserves spatial locality and supports parallel decoding within each scale. This design significantly enhances both temporal consistency and rollout efficiency. To further improve dynamic scene understanding, we devise an asymmetric multi-scale tokenizer that preserves spatial details in observed frames and extracts compact dynamic representations for future frames, optimizing both memory usage and model performance. Additionally, we introduce a trajectory-aware motion prompt module that injects spatiotemporal cues about object and robot trajectories, focusing attention on dynamic regions and improving temporal consistency and physical realism. Extensive experiments show that SAMPO achieves competitive performance in action-conditioned video prediction and model-based control, improving generation quality with 4.4$\times$ faster inference. We also evaluate SAMPO's zero-shot generalization and scaling behavior, demonstrating its ability to generalize to unseen tasks and benefit from larger model sizes.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stress Testing Deliberative Alignment for Anti-Scheming Training</title>
<link>https://arxiv.org/abs/2509.15541</link>
<guid>https://arxiv.org/abs/2509.15541</guid>
<content:encoded><![CDATA[
arXiv:2509.15541v1 Announce Type: new 
Abstract: Highly capable AI systems could secretly pursue misaligned goals -- what we call "scheming". Because a scheming AI would deliberately try to hide its misaligned goals and actions, measuring and mitigating scheming requires different strategies than are typically used in ML. We propose that assessing anti-scheming interventions requires at least (1) testing propensity to scheme on far out-of-distribution (OOD) tasks, (2) evaluating whether lack of scheming is driven by situational awareness, and (3) checking for robustness to pre-existing misaligned goals. We use a broad category of "covert actions" -- such as secretly breaking rules or intentionally underperforming in tests -- as a proxy for scheming, and design evaluations for covert actions. We then stress-test deliberative alignment as a case study for anti-scheming. Across 26 OOD evaluations (180+ environments), deliberative alignment reduces covert action rates (OpenAI o3: 13%->0.4%) but does not fully eliminate them. Our mitigation is also able to largely stop agents from pursuing a hidden goal previously trained into the model, but we still find misbehavior after additional red-teaming. We find that models' chain-of-thought (CoT) often demonstrates awareness of being evaluated for alignment, and show causal evidence that this awareness decreases covert behavior, while unawareness increases it. Therefore, we cannot exclude that the observed reductions in covert action rates are at least partially driven by situational awareness. While we rely on human-legible CoT for training, studying situational awareness, and demonstrating clear evidence of misalignment, our ability to rely on this degrades as models continue to depart from reasoning in standard English. We encourage research into alignment mitigations for scheming and their assessment, especially for the adversarial case of deceptive alignment, which this paper does not address.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent</title>
<link>https://arxiv.org/abs/2509.15566</link>
<guid>https://arxiv.org/abs/2509.15566</guid>
<content:encoded><![CDATA[
arXiv:2509.15566v1 Announce Type: new 
Abstract: In the field of AI-driven human-GUI interaction automation, while rapid advances in multimodal large language models and reinforcement fine-tuning techniques have yielded remarkable progress, a fundamental challenge persists: their interaction logic significantly deviates from natural human-GUI communication patterns. To fill this gap, we propose "Blink-Think-Link" (BTL), a brain-inspired framework for human-GUI interaction that mimics the human cognitive process between users and graphical interfaces. The system decomposes interactions into three biologically plausible phases: (1) Blink - rapid detection and attention to relevant screen areas, analogous to saccadic eye movements; (2) Think - higher-level reasoning and decision-making, mirroring cognitive planning; and (3) Link - generation of executable commands for precise motor control, emulating human action selection mechanisms. Additionally, we introduce two key technical innovations for the BTL framework: (1) Blink Data Generation - an automated annotation pipeline specifically optimized for blink data, and (2) BTL Reward -- the first rule-based reward mechanism that enables reinforcement learning driven by both process and outcome. Building upon this framework, we develop a GUI agent model named BTL-UI, which demonstrates consistent state-of-the-art performance across both static GUI understanding and dynamic interaction tasks in comprehensive benchmarks. These results provide conclusive empirical validation of the framework's efficacy in developing advanced GUI Agents.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiteLong: Resource-Efficient Long-Context Data Synthesis for LLMs</title>
<link>https://arxiv.org/abs/2509.15568</link>
<guid>https://arxiv.org/abs/2509.15568</guid>
<content:encoded><![CDATA[
arXiv:2509.15568v1 Announce Type: new 
Abstract: High-quality long-context data is essential for training large language models (LLMs) capable of processing extensive documents, yet existing synthesis approaches using relevance-based aggregation face challenges of computational efficiency. We present LiteLong, a resource-efficient method for synthesizing long-context data through structured topic organization and multi-agent debate. Our approach leverages the BISAC book classification system to provide a comprehensive hierarchical topic organization, and then employs a debate mechanism with multiple LLMs to generate diverse, high-quality topics within this structure. For each topic, we use lightweight BM25 retrieval to obtain relevant documents and concatenate them into 128K-token training samples. Experiments on HELMET and Ruler benchmarks demonstrate that LiteLong achieves competitive long-context performance and can seamlessly integrate with other long-dependency enhancement methods. LiteLong makes high-quality long-context data synthesis more accessible by reducing both computational and data engineering costs, facilitating further research in long-context language training.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MicroRCA-Agent: Microservice Root Cause Analysis Method Based on Large Language Model Agents</title>
<link>https://arxiv.org/abs/2509.15635</link>
<guid>https://arxiv.org/abs/2509.15635</guid>
<content:encoded><![CDATA[
arXiv:2509.15635v1 Announce Type: new 
Abstract: This paper presents MicroRCA-Agent, an innovative solution for microservice root cause analysis based on large language model agents, which constructs an intelligent fault root cause localization system with multimodal data fusion. The technical innovations are embodied in three key aspects: First, we combine the pre-trained Drain log parsing algorithm with multi-level data filtering mechanism to efficiently compress massive logs into high-quality fault features. Second, we employ a dual anomaly detection approach that integrates Isolation Forest unsupervised learning algorithms with status code validation to achieve comprehensive trace anomaly identification. Third, we design a statistical symmetry ratio filtering mechanism coupled with a two-stage LLM analysis strategy to enable full-stack phenomenon summarization across node-service-pod hierarchies. The multimodal root cause analysis module leverages carefully designed cross-modal prompts to deeply integrate multimodal anomaly information, fully exploiting the cross-modal understanding and logical reasoning capabilities of large language models to generate structured analysis results encompassing fault components, root cause descriptions, and reasoning trace. Comprehensive ablation studies validate the complementary value of each modal data and the effectiveness of the system architecture. The proposed solution demonstrates superior performance in complex microservice fault scenarios, achieving a final score of 50.71. The code has been released at: https://github.com/tangpan360/MicroRCA-Agent.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imagination at Inference: Synthesizing In-Hand Views for Robust Visuomotor Policy Inference</title>
<link>https://arxiv.org/abs/2509.15717</link>
<guid>https://arxiv.org/abs/2509.15717</guid>
<content:encoded><![CDATA[
arXiv:2509.15717v1 Announce Type: new 
Abstract: Visual observations from different viewpoints can significantly influence the performance of visuomotor policies in robotic manipulation. Among these, egocentric (in-hand) views often provide crucial information for precise control. However, in some applications, equipping robots with dedicated in-hand cameras may pose challenges due to hardware constraints, system complexity, and cost. In this work, we propose to endow robots with imaginative perception - enabling them to 'imagine' in-hand observations from agent views at inference time. We achieve this via novel view synthesis (NVS), leveraging a fine-tuned diffusion model conditioned on the relative pose between the agent and in-hand views cameras. Specifically, we apply LoRA-based fine-tuning to adapt a pretrained NVS model (ZeroNVS) to the robotic manipulation domain. We evaluate our approach on both simulation benchmarks (RoboMimic and MimicGen) and real-world experiments using a Unitree Z1 robotic arm for a strawberry picking task. Results show that synthesized in-hand views significantly enhance policy inference, effectively recovering the performance drop caused by the absence of real in-hand cameras. Our method offers a scalable and hardware-light solution for deploying robust visuomotor policies, highlighting the potential of imaginative visual reasoning in embodied agents.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Nascent Taxonomy of Machine Learning in Intelligent Robotic Process Automation</title>
<link>https://arxiv.org/abs/2509.15730</link>
<guid>https://arxiv.org/abs/2509.15730</guid>
<content:encoded><![CDATA[
arXiv:2509.15730v1 Announce Type: new 
Abstract: Robotic process automation (RPA) is a lightweight approach to automating business processes using software robots that emulate user actions at the graphical user interface level. While RPA has gained popularity for its cost-effective and timely automation of rule-based, well-structured tasks, its symbolic nature has inherent limitations when approaching more complex tasks currently performed by human agents. Machine learning concepts enabling intelligent RPA provide an opportunity to broaden the range of automatable tasks. In this paper, we conduct a literature review to explore the connections between RPA and machine learning and organize the joint concept intelligent RPA into a taxonomy. Our taxonomy comprises the two meta-characteristics RPA-ML integration and RPA-ML interaction. Together, they comprise eight dimensions: architecture and ecosystem, capabilities, data basis, intelligence level, and technical depth of integration as well as deployment environment, lifecycle phase, and user-robot relation.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMART: Scalable Multi-Agent Reasoning and Trajectory Planning in Dense Environments</title>
<link>https://arxiv.org/abs/2509.15737</link>
<guid>https://arxiv.org/abs/2509.15737</guid>
<content:encoded><![CDATA[
arXiv:2509.15737v1 Announce Type: new 
Abstract: Multi-vehicle trajectory planning is a non-convex problem that becomes increasingly difficult in dense environments due to the rapid growth of collision constraints. Efficient exploration of feasible behaviors and resolution of tight interactions are essential for real-time, large-scale coordination. This paper introduces SMART, Scalable Multi-Agent Reasoning and Trajectory Planning, a hierarchical framework that combines priority-based search with distributed optimization to achieve efficient and feasible multi-vehicle planning. The upper layer explores diverse interaction modes using reinforcement learning-based priority estimation and large-step hybrid A* search, while the lower layer refines solutions via parallelizable convex optimization. By partitioning space among neighboring vehicles and constructing robust feasible corridors, the method decouples the joint non-convex problem into convex subproblems solved efficiently in parallel. This design alleviates the step-size trade-off while ensuring kinematic feasibility and collision avoidance. Experiments show that SMART consistently outperforms baselines. On 50 m x 50 m maps, it sustains over 90% success within 1 s up to 25 vehicles, while baselines often drop below 50%. On 100 m x 100 m maps, SMART achieves above 95% success up to 50 vehicles and remains feasible up to 90 vehicles, with runtimes more than an order of magnitude faster than optimization-only approaches. Built on vehicle-to-everything communication, SMART incorporates vehicle-infrastructure cooperation through roadside sensing and agent coordination, improving scalability and safety. Real-world experiments further validate this design, achieving planning times as low as 0.014 s while preserving cooperative behaviors.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUI-ReWalk: Massive Data Generation for GUI Agent via Stochastic Exploration and Intent-Aware Reasoning</title>
<link>https://arxiv.org/abs/2509.15738</link>
<guid>https://arxiv.org/abs/2509.15738</guid>
<content:encoded><![CDATA[
arXiv:2509.15738v1 Announce Type: new 
Abstract: Graphical User Interface (GUI) Agents, powered by large language and vision-language models, hold promise for enabling end-to-end automation in digital environments. However, their progress is fundamentally constrained by the scarcity of scalable, high-quality trajectory data. Existing data collection strategies either rely on costly and inconsistent manual annotations or on synthetic generation methods that trade off between diversity and meaningful task coverage. To bridge this gap, we present GUI-ReWalk: a reasoning-enhanced, multi-stage framework for synthesizing realistic and diverse GUI trajectories. GUI-ReWalk begins with a stochastic exploration phase that emulates human trial-and-error behaviors, and progressively transitions into a reasoning-guided phase where inferred goals drive coherent and purposeful interactions. Moreover, it supports multi-stride task generation, enabling the construction of long-horizon workflows across multiple applications. By combining randomness for diversity with goal-aware reasoning for structure, GUI-ReWalk produces data that better reflects the intent-aware, adaptive nature of human-computer interaction. We further train Qwen2.5-VL-7B on the GUI-ReWalk dataset and evaluate it across multiple benchmarks, including Screenspot-Pro, OSWorld-G, UI-Vision, AndroidControl, and GUI-Odyssey. Results demonstrate that GUI-ReWalk enables superior coverage of diverse interaction flows, higher trajectory entropy, and more realistic user intent. These findings establish GUI-ReWalk as a scalable and data-efficient framework for advancing GUI agent research and enabling robust real-world automation.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building Data-Driven Occupation Taxonomies: A Bottom-Up Multi-Stage Approach via Semantic Clustering and Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2509.15786</link>
<guid>https://arxiv.org/abs/2509.15786</guid>
<content:encoded><![CDATA[
arXiv:2509.15786v1 Announce Type: new 
Abstract: Creating robust occupation taxonomies, vital for applications ranging from job recommendation to labor market intelligence, is challenging. Manual curation is slow, while existing automated methods are either not adaptive to dynamic regional markets (top-down) or struggle to build coherent hierarchies from noisy data (bottom-up). We introduce CLIMB (CLusterIng-based Multi-agent taxonomy Builder), a framework that fully automates the creation of high-quality, data-driven taxonomies from raw job postings. CLIMB uses global semantic clustering to distill core occupations, then employs a reflection-based multi-agent system to iteratively build a coherent hierarchy. On three diverse, real-world datasets, we show that CLIMB produces taxonomies that are more coherent and scalable than existing methods and successfully capture unique regional characteristics. We release our code and datasets at https://anonymous.4open.science/r/CLIMB.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control</title>
<link>https://arxiv.org/abs/2509.15799</link>
<guid>https://arxiv.org/abs/2509.15799</guid>
<content:encoded><![CDATA[
arXiv:2509.15799v1 Announce Type: new 
Abstract: Achieving safe and coordinated behavior in dynamic, constraint-rich environments remains a major challenge for learning-based control. Pure end-to-end learning often suffers from poor sample efficiency and limited reliability, while model-based methods depend on predefined references and struggle to generalize. We propose a hierarchical framework that combines tactical decision-making via reinforcement learning (RL) with low-level execution through Model Predictive Control (MPC). For the case of multi-agent systems this means that high-level policies select abstract targets from structured regions of interest (ROIs), while MPC ensures dynamically feasible and safe motion. Tested on a predator-prey benchmark, our approach outperforms end-to-end and shielding-based RL baselines in terms of reward, safety, and consistency, underscoring the benefits of combining structured learning with model-based control.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coordinated Multi-Drone Last-mile Delivery: Learning Strategies for Energy-aware and Timely Operations</title>
<link>https://arxiv.org/abs/2509.15830</link>
<guid>https://arxiv.org/abs/2509.15830</guid>
<content:encoded><![CDATA[
arXiv:2509.15830v1 Announce Type: new 
Abstract: Drones have recently emerged as a faster, safer, and cost-efficient way for last-mile deliveries of parcels, particularly for urgent medical deliveries highlighted during the pandemic. This paper addresses a new challenge of multi-parcel delivery with a swarm of energy-aware drones, accounting for time-sensitive customer requirements. Each drone plans an optimal multi-parcel route within its battery-restricted flight range to minimize delivery delays and reduce energy consumption. The problem is tackled by decomposing it into three sub-problems: (1) optimizing depot locations and service areas using K-means clustering; (2) determining the optimal flight range for drones through reinforcement learning; and (3) planning and selecting multi-parcel delivery routes via a new optimized plan selection approach. To integrate these solutions and enhance long-term efficiency, we propose a novel algorithm leveraging actor-critic-based multi-agent deep reinforcement learning. Extensive experimentation using realistic delivery datasets demonstrate an exceptional performance of the proposed algorithm. We provide new insights into economic efficiency (minimize energy consumption), rapid operations (reduce delivery delays and overall execution time), and strategic guidance on depot deployment for practical logistics applications.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relational Dissonance in Human-AI Interactions: The Case of Knowledge Work</title>
<link>https://arxiv.org/abs/2509.15836</link>
<guid>https://arxiv.org/abs/2509.15836</guid>
<content:encoded><![CDATA[
arXiv:2509.15836v1 Announce Type: new 
Abstract: When AI systems allow human-like communication, they elicit increasingly complex relational responses. Knowledge workers face a particular challenge: They approach these systems as tools while interacting with them in ways that resemble human social interaction. To understand the relational contexts that arise when humans engage with anthropomorphic conversational agents, we need to expand existing human-computer interaction frameworks. Through three workshops with qualitative researchers, we found that the fundamental ontological and relational ambiguities inherent in anthropomorphic conversational agents make it difficult for individuals to maintain consistent relational stances toward them. Our findings indicate that people's articulated positioning toward such agents often differs from the relational dynamics that occur during interactions. We propose the concept of relational dissonance to help researchers, designers, and policymakers recognize the resulting tensions in the development, deployment, and governance of anthropomorphic conversational agents and address the need for relational transparency.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds</title>
<link>https://arxiv.org/abs/2509.15915</link>
<guid>https://arxiv.org/abs/2509.15915</guid>
<content:encoded><![CDATA[
arXiv:2509.15915v1 Announce Type: new 
Abstract: While reinforcement learning from scratch has shown impressive results in solving sequential decision-making tasks with efficient simulators, real-world applications with expensive interactions require more sample-efficient agents. Foundation models (FMs) are natural candidates to improve sample efficiency as they possess broad knowledge and reasoning capabilities, but it is yet unclear how to effectively integrate them into the reinforcement learning framework. In this paper, we anticipate and, most importantly, evaluate two promising strategies. First, we consider the use of foundation world models (FWMs) that exploit the prior knowledge of FMs to enable training and evaluating agents with simulated interactions. Second, we consider the use of foundation agents (FAs) that exploit the reasoning capabilities of FMs for decision-making. We evaluate both approaches empirically in a family of grid-world environments that are suitable for the current generation of large language models (LLMs). Our results suggest that improvements in LLMs already translate into better FWMs and FAs; that FAs based on current LLMs can already provide excellent policies for sufficiently simple environments; and that the coupling of FWMs and reinforcement learning agents is highly promising for more complex settings with partial observability and stochastic elements.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol</title>
<link>https://arxiv.org/abs/2509.15957</link>
<guid>https://arxiv.org/abs/2509.15957</guid>
<content:encoded><![CDATA[
arXiv:2509.15957v1 Announce Type: new 
Abstract: Background: Large language models (LLMs) show promise in medicine, but their deployment in hospitals is limited by restricted access to electronic health record (EHR) systems. The Model Context Protocol (MCP) enables integration between LLMs and external tools.
  Objective: To evaluate whether an LLM connected to an EHR database via MCP can autonomously retrieve clinically relevant information in a real hospital setting.
  Methods: We developed EHR-MCP, a framework of custom MCP tools integrated with the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct agent to interact with it. Six tasks were tested, derived from use cases of the infection control team (ICT). Eight patients discussed at ICT conferences were retrospectively analyzed. Agreement with physician-generated gold standards was measured.
  Results: The LLM consistently selected and executed the correct MCP tools. Except for two tasks, all tasks achieved near-perfect accuracy. Performance was lower in the complex task requiring time-dependent calculations. Most errors arose from incorrect arguments or misinterpretation of tool results. Responses from EHR-MCP were reliable, though long and repetitive data risked exceeding the context window.
  Conclusions: LLMs can retrieve clinical data from an EHR via MCP tools in a real hospital setting, achieving near-perfect performance in simple tasks while highlighting challenges in complex ones. EHR-MCP provides an infrastructure for secure, consistent data access and may serve as a foundation for hospital AI agents. Future work should extend beyond retrieval to reasoning, generation, and clinical impact assessment, paving the way for effective integration of generative AI into clinical practice.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation</title>
<link>https://arxiv.org/abs/2509.15965</link>
<guid>https://arxiv.org/abs/2509.15965</guid>
<content:encoded><![CDATA[
arXiv:2509.15965v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has demonstrated immense potential in advancing artificial general intelligence, agentic intelligence, and embodied intelligence. However, the inherent heterogeneity and dynamicity of RL workflows often lead to low hardware utilization and slow training on existing systems. In this paper, we present RLinf, a high-performance RL training system based on our key observation that the major roadblock to efficient RL training lies in system flexibility. To maximize flexibility and efficiency, RLinf is built atop a novel RL system design paradigm called macro-to-micro flow transformation (M2Flow), which automatically breaks down high-level, easy-to-compose RL workflows at both the temporal and spatial dimensions, and recomposes them into optimized execution flows. Supported by RLinf worker's adaptive communication capability, we devise context switching and elastic pipelining to realize M2Flow transformation, and a profiling-guided scheduling policy to generate optimal execution plans. Extensive evaluations on both reasoning RL and embodied RL tasks demonstrate that RLinf consistently outperforms state-of-the-art systems, achieving 1.1x-2.13x speedup in end-to-end training throughput.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations</title>
<link>https://arxiv.org/abs/2509.15981</link>
<guid>https://arxiv.org/abs/2509.15981</guid>
<content:encoded><![CDATA[
arXiv:2509.15981v1 Announce Type: new 
Abstract: In reinforcement learning with sparse rewards, demonstrations can accelerate learning, but determining when to imitate them remains challenging. We propose Smooth Policy Regularisation from Demonstrations (SPReD), a framework that addresses the fundamental question: when should an agent imitate a demonstration versus follow its own policy? SPReD uses ensemble methods to explicitly model Q-value distributions for both demonstration and policy actions, quantifying uncertainty for comparisons. We develop two complementary uncertainty-aware methods: a probabilistic approach estimating the likelihood of demonstration superiority, and an advantage-based approach scaling imitation by statistical significance. Unlike prevailing methods (e.g. Q-filter) that make binary imitation decisions, SPReD applies continuous, uncertainty-proportional regularisation weights, reducing gradient variance during training. Despite its computational simplicity, SPReD achieves remarkable gains in experiments across eight robotics tasks, outperforming existing approaches by up to a factor of 14 in complex tasks while maintaining robustness to demonstration quality and quantity. Our code is available at https://github.com/YujieZhu7/SPReD.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Optimization Latent Variable Models for Learning Costs Applied to Route Problems</title>
<link>https://arxiv.org/abs/2509.15999</link>
<guid>https://arxiv.org/abs/2509.15999</guid>
<content:encoded><![CDATA[
arXiv:2509.15999v1 Announce Type: new 
Abstract: Learning representations for solutions of constrained optimization problems (COPs) with unknown cost functions is challenging, as models like (Variational) Autoencoders struggle to enforce constraints when decoding structured outputs. We propose an Inverse Optimization Latent Variable Model (IO-LVM) that learns a latent space of COP cost functions from observed solutions and reconstructs feasible outputs by solving a COP with a solver in the loop. Our approach leverages estimated gradients of a Fenchel-Young loss through a non-differentiable deterministic solver to shape the latent space. Unlike standard Inverse Optimization or Inverse Reinforcement Learning methods, which typically recover a single or context-specific cost function, IO-LVM captures a distribution over cost functions, enabling the identification of diverse solution behaviors arising from different agents or conditions not available during the training process. We validate our method on real-world datasets of ship and taxi routes, as well as paths in synthetic graphs, demonstrating its ability to reconstruct paths and cycles, predict their distributions, and yield interpretable latent representations.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Visual Continual Learning with Multi-Prototype Supervision</title>
<link>https://arxiv.org/abs/2509.16011</link>
<guid>https://arxiv.org/abs/2509.16011</guid>
<content:encoded><![CDATA[
arXiv:2509.16011v1 Announce Type: new 
Abstract: Language-guided supervision, which utilizes a frozen semantic target from a Pretrained Language Model (PLM), has emerged as a promising paradigm for visual Continual Learning (CL). However, relying on a single target introduces two critical limitations: 1) semantic ambiguity, where a polysemous category name results in conflicting visual representations, and 2) intra-class visual diversity, where a single prototype fails to capture the rich variety of visual appearances within a class. To this end, we propose MuproCL, a novel framework that replaces the single target with multiple, context-aware prototypes. Specifically, we employ a lightweight LLM agent to perform category disambiguation and visual-modal expansion to generate a robust set of semantic prototypes. A LogSumExp aggregation mechanism allows the vision model to adaptively align with the most relevant prototype for a given image. Extensive experiments across various CL baselines demonstrate that MuproCL consistently enhances performance and robustness, establishing a more effective path for language-guided continual learning.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaSports-Traj: Role- and Domain-Aware Adaptation for Multi-Agent Trajectory Modeling in Sports</title>
<link>https://arxiv.org/abs/2509.16095</link>
<guid>https://arxiv.org/abs/2509.16095</guid>
<content:encoded><![CDATA[
arXiv:2509.16095v1 Announce Type: new 
Abstract: Trajectory prediction in multi-agent sports scenarios is inherently challenging due to the structural heterogeneity across agent roles (e.g., players vs. ball) and dynamic distribution gaps across different sports domains. Existing unified frameworks often fail to capture these structured distributional shifts, resulting in suboptimal generalization across roles and domains. We propose AdaSports-Traj, an adaptive trajectory modeling framework that explicitly addresses both intra-domain and inter-domain distribution discrepancies in sports. At its core, AdaSports-Traj incorporates a Role- and Domain-Aware Adapter to conditionally adjust latent representations based on agent identity and domain context. Additionally, we introduce a Hierarchical Contrastive Learning objective, which separately supervises role-sensitive and domain-aware representations to encourage disentangled latent structures without introducing optimization conflict. Experiments on three diverse sports datasets, Basketball-U, Football-U, and Soccer-U, demonstrate the effectiveness of our adaptive design, achieving strong performance in both unified and cross-domain trajectory prediction settings.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Cyber Defense with Generalizable Graph-based Reinforcement Learning Agents</title>
<link>https://arxiv.org/abs/2509.16151</link>
<guid>https://arxiv.org/abs/2509.16151</guid>
<content:encoded><![CDATA[
arXiv:2509.16151v1 Announce Type: new 
Abstract: Deep reinforcement learning (RL) is emerging as a viable strategy for automated cyber defense (ACD). The traditional RL approach represents networks as a list of computers in various states of safety or threat. Unfortunately, these models are forced to overfit to specific network topologies, rendering them ineffective when faced with even small environmental perturbations. In this work, we frame ACD as a two-player context-based partially observable Markov decision problem with observations represented as attributed graphs. This approach allows our agents to reason through the lens of relational inductive bias. Agents learn how to reason about hosts interacting with other system entities in a more general manner, and their actions are understood as edits to the graph representing the environment. By introducing this bias, we will show that our agents can better reason about the states of networks and zero-shot adapt to new ones. We show that this approach outperforms the state-of-the-art by a wide margin, and makes our agents capable of defending never-before-seen networks against a wide range of adversaries in a variety of complex, and multi-agent environments.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Analysis of Just-In-Time Liquidity Provision in Concentrated Liquidity Market Makers</title>
<link>https://arxiv.org/abs/2509.16157</link>
<guid>https://arxiv.org/abs/2509.16157</guid>
<content:encoded><![CDATA[
arXiv:2509.16157v1 Announce Type: new 
Abstract: Liquidity providers (LPs) are essential figures in the operation of automated market makers (AMMs); in exchange for transaction fees, LPs lend the liquidity that allows AMMs to operate. While many prior works have studied the incentive structures of LPs in general, we currently lack a principled understanding of a special class of LPs known as Just-In-Time (JIT) LPs. These are strategic agents who momentarily supply liquidity for a single swap, in an attempt to extract disproportionately high fees relative to the remaining passive LPs. This paper provides the first formal, transaction-level model of JIT liquidity provision for a widespread class of AMMs known as Concentrated Liquidity Market Makers (CLMMs), as seen in Uniswap V3, for instance. We characterize the landscape of price impact and fee allocation in these systems, formulate and analyze a non-linear optimization problem faced by JIT LPs, and prove the existence of an optimal strategy. By fitting our optimal solution for JIT LPs to real-world CLMMs, we observe that in liquidity pools (particularly those with risky assets), there is a significant gap between observed and optimal JIT behavior. Existing JIT LPs often fail to account for price impact; doing so, we estimate they could increase earnings by up to 69% on average over small time windows. We also show that JIT liquidity, when deployed strategically, can improve market efficiency by reducing slippage for traders, albeit at the cost of eroding average passive LP profits by up to 44% per trade.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Aerial Cinematography: From Dialogue Cues to Cinematic Trajectories</title>
<link>https://arxiv.org/abs/2509.16176</link>
<guid>https://arxiv.org/abs/2509.16176</guid>
<content:encoded><![CDATA[
arXiv:2509.16176v1 Announce Type: new 
Abstract: We present Agentic Aerial Cinematography: From Dialogue Cues to Cinematic Trajectories (ACDC), an autonomous drone cinematography system driven by natural language communication between human directors and drones. The main limitation of previous drone cinematography workflows is that they require manual selection of waypoints and view angles based on predefined human intent, which is labor-intensive and yields inconsistent performance. In this paper, we propose employing large language models (LLMs) and vision foundation models (VFMs) to convert free-form natural language prompts directly into executable indoor UAV video tours. Specifically, our method comprises a vision-language retrieval pipeline for initial waypoint selection, a preference-based Bayesian optimization framework that refines poses using aesthetic feedback, and a motion planner that generates safe quadrotor trajectories. We validate ACDC through both simulation and hardware-in-the-loop experiments, demonstrating that it robustly produces professional-quality footage across diverse indoor scenes without requiring expertise in robotics or cinematography. These results highlight the potential of embodied AI agents to close the loop from open-vocabulary dialogue to real-world autonomous aerial cinematography.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MatchFixAgent: Language-Agnostic Autonomous Repository-Level Code Translation Validation and Repair</title>
<link>https://arxiv.org/abs/2509.16187</link>
<guid>https://arxiv.org/abs/2509.16187</guid>
<content:encoded><![CDATA[
arXiv:2509.16187v1 Announce Type: new 
Abstract: Code translation transforms source code from one programming language (PL) to another. Validating the functional equivalence of translation and repairing, if necessary, are critical steps in code translation. Existing automated validation and repair approaches struggle to generalize to many PLs due to high engineering overhead, and they rely on existing and often inadequate test suites, which results in false claims of equivalence and ineffective translation repair. We develop MatchFixAgent, a large language model (LLM)-based, PL-agnostic framework for equivalence validation and repair of translations. MatchFixAgent features a multi-agent architecture that divides equivalence validation into several sub-tasks to ensure thorough and consistent semantic analysis of the translation. Then it feeds this analysis to test agent to write and execute tests. Upon observing a test failure, the repair agent attempts to fix the translation bug. The final (in)equivalence decision is made by the verdict agent, considering semantic analyses and test execution results.
  We compare MatchFixAgent's validation and repair results with four repository-level code translation techniques. We use 2,219 translation pairs from their artifacts, which cover 6 PL pairs, and are collected from 24 GitHub projects totaling over 900K lines of code. Our results demonstrate that MatchFixAgent produces (in)equivalence verdicts for 99.2% of translation pairs, with the same equivalence validation result as prior work on 72.8% of them. When MatchFixAgent's result disagrees with prior work, we find that 60.7% of the time MatchFixAgent's result is actually correct. In addition, we show that MatchFixAgent can repair 50.6% of inequivalent translation, compared to prior work's 18.5%. This demonstrates that MatchFixAgent is far more adaptable to many PL pairs than prior work, while producing highly accurate validation results.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences</title>
<link>https://arxiv.org/abs/2509.16189</link>
<guid>https://arxiv.org/abs/2509.16189</guid>
<content:encoded><![CDATA[
arXiv:2509.16189v1 Announce Type: new 
Abstract: When do machine learning systems fail to generalize, and what mechanisms could improve their generalization? Here, we draw inspiration from cognitive science to argue that one weakness of machine learning systems is their failure to exhibit latent learning -- learning information that is not relevant to the task at hand, but that might be useful in a future task. We show how this perspective links failures ranging from the reversal curse in language modeling to new findings on agent-based navigation. We then highlight how cognitive science points to episodic memory as a potential part of the solution to these issues. Correspondingly, we show that a system with an oracle retrieval mechanism can use learning experiences more flexibly to generalize better across many of these challenges. We also identify some of the essential components for effectively using retrieval, including the importance of within-example in-context learning for acquiring the ability to use information across retrieved examples. In summary, our results illustrate one possible contributor to the relative data inefficiency of current machine learning systems compared to natural intelligence, and help to understand how retrieval methods can complement parametric learning to improve generalization.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation</title>
<link>https://arxiv.org/abs/2509.16198</link>
<guid>https://arxiv.org/abs/2509.16198</guid>
<content:encoded><![CDATA[
arXiv:2509.16198v1 Announce Type: new 
Abstract: Large language models excel at function- and file-level code generation, yet generating complete repositories from scratch remains a fundamental challenge. This process demands coherent and reliable planning across proposal- and implementation-level stages, while natural language, due to its ambiguity and verbosity, is ill-suited for faithfully representing complex software structures. To address this, we introduce the Repository Planning Graph (RPG), a persistent representation that unifies proposal- and implementation-level planning by encoding capabilities, file structures, data flows, and functions in one graph. RPG replaces ambiguous natural language with an explicit blueprint, enabling long-horizon planning and scalable repository generation. Building on RPG, we develop ZeroRepo, a graph-driven framework for repository generation from scratch. It operates in three stages: proposal-level planning and implementation-level refinement to construct the graph, followed by graph-guided code generation with test validation. To evaluate this setting, we construct RepoCraft, a benchmark of six real-world projects with 1,052 tasks. On RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly 3.9$\times$ the strongest baseline (Claude Code) and about 64$\times$ other baselines. It attains 81.5% functional coverage and a 69.7% pass rate, exceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further analysis shows that RPG models complex dependencies, enables progressively more sophisticated planning through near-linear scaling, and enhances LLM understanding of repositories, thereby accelerating agent localization.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regularity properties of distributions of correspondences without countable generation: applications to large games</title>
<link>https://arxiv.org/abs/2509.15898</link>
<guid>https://arxiv.org/abs/2509.15898</guid>
<content:encoded><![CDATA[
arXiv:2509.15898v1 Announce Type: cross 
Abstract: We show that each of the regularity properties of regular conditional distributions of correspondences (convexity, closedness, compactness, and preservation of closed graphs) is equivalent to the condition of nowhere equivalence. This result does not require any countable-generation assumptions. As an application, we establish the existence of a pure-strategy equilibrium for large games with general trait spaces. The trait space may be an arbitrary measurable space. As a corollary, we obtain the existence of a pure-strategy equilibrium in semi-anonymous settings in which payoffs depend, in addition to agents' own actions, on the joint distribution over the space of agents and actions.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Reinforcement Learning with Dynamic-Circuit Qubit Reuse and Grover-Based Trajectory Optimization</title>
<link>https://arxiv.org/abs/2509.16002</link>
<guid>https://arxiv.org/abs/2509.16002</guid>
<content:encoded><![CDATA[
arXiv:2509.16002v1 Announce Type: cross 
Abstract: A fully quantum reinforcement learning framework is developed that integrates a quantum Markov decision process, dynamic circuit-based qubit reuse, and Grover's algorithm for trajectory optimization. The framework encodes states, actions, rewards, and transitions entirely within the quantum domain, enabling parallel exploration of state-action sequences through superposition and eliminating classical subroutines. Dynamic circuit operations, including mid-circuit measurement and reset, allow reuse of the same physical qubits across multiple agent-environment interactions, reducing qubit requirements from 7*T to 7 for T time steps while preserving logical continuity. Quantum arithmetic computes trajectory returns, and Grover's search is applied to the superposition of these evaluated trajectories to amplify the probability of measuring those with the highest return, thereby accelerating the identification of the optimal policy. Simulations demonstrate that the dynamic-circuit-based implementation preserves trajectory fidelity while reducing qubit usage by 66 percent relative to the static design. Experimental deployment on IBM Heron-class quantum hardware confirms that the framework operates within the constraints of current quantum processors and validates the feasibility of fully quantum multi-step reinforcement learning under noisy intermediate-scale quantum conditions. This framework advances the scalability and practical application of quantum reinforcement learning for large-scale sequential decision-making tasks.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Misspecified learning and evolutionary stability</title>
<link>https://arxiv.org/abs/2509.16067</link>
<guid>https://arxiv.org/abs/2509.16067</guid>
<content:encoded><![CDATA[
arXiv:2509.16067v1 Announce Type: cross 
Abstract: We extend the indirect evolutionary approach to the selection of (possibly misspecified) models. Agents with different models match in pairs to play a stage game, where models define feasible beliefs about game parameters and about others' strategies. In equilibrium, each agent adopts the feasible belief that best fits their data and plays optimally given their beliefs. We define the stability of the resident model by comparing its equilibrium payoff with that of the entrant model, and provide conditions under which the correctly specified resident model can only be destabilized by misspecified entrant models that contain multiple feasible beliefs (that is, entrant models that permit inference). We also show that entrants may do well in their matches against the residents only when the entrant population is large, due to the endogeneity of misspecified beliefs. Applications include the selection of demand-elasticity misperception in Cournot duopoly and the emergence of analogy-based reasoning in centipede games.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unique Security and Privacy Threats of Large Language Models: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2406.07973</link>
<guid>https://arxiv.org/abs/2406.07973</guid>
<content:encoded><![CDATA[
arXiv:2406.07973v3 Announce Type: replace 
Abstract: With the rapid development of artificial intelligence, large language models (LLMs) have made remarkable advancements in natural language processing. These models are trained on vast datasets to exhibit powerful language understanding and generation capabilities across various applications, including chatbots, and agents. However, LLMs have revealed a variety of privacy and security issues throughout their life cycle, drawing significant academic and industrial attention. Moreover, the risks faced by LLMs differ significantly from those encountered by traditional language models. Given that current surveys lack a clear taxonomy of unique threat models across diverse scenarios, we emphasize the unique privacy and security threats associated with four specific scenarios: pre-training, fine-tuning, deployment, and LLM-based agents. Addressing the characteristics of each risk, this survey outlines and analyzes potential countermeasures. Research on attack and defense situations can offer feasible research directions, enabling more areas to benefit from LLMs.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Policy Fusion for User Alignment Without Re-Interaction</title>
<link>https://arxiv.org/abs/2409.20016</link>
<guid>https://arxiv.org/abs/2409.20016</guid>
<content:encoded><![CDATA[
arXiv:2409.20016v4 Announce Type: replace 
Abstract: Deep reinforcement learning (RL) policies, although optimal in terms of task rewards, may not align with the personal preferences of human users. To ensure this alignment, a naive solution would be to retrain the agent using a reward function that encodes the user's specific preferences. However, such a reward function is typically not readily available, and as such, retraining the agent from scratch can be prohibitively expensive. We propose a more practical approach - to adapt the already trained policy to user-specific needs with the help of human feedback. To this end, we infer the user's intent through trajectory-level feedback and combine it with the trained task policy via a theoretically grounded dynamic policy fusion approach. As our approach collects human feedback on the very same trajectories used to learn the task policy, it does not require any additional interactions with the environment, making it a zero-shot approach. We empirically demonstrate in a number of environments that our proposed dynamic policy fusion approach consistently achieves the intended task while simultaneously adhering to user-specific needs.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeCodePLT: A Unified Platform for Evaluating the Security of Code GenAI</title>
<link>https://arxiv.org/abs/2410.11096</link>
<guid>https://arxiv.org/abs/2410.11096</guid>
<content:encoded><![CDATA[
arXiv:2410.11096v2 Announce Type: replace 
Abstract: Existing benchmarks for evaluating the security risks and capabilities (e.g., vulnerability detection) of code-generating large language models (LLMs) face several key limitations: (1) limited coverage of risk and capabilities; (2) reliance on static evaluation metrics such as LLM judgments or rule-based detection, which lack the precision of dynamic analysis; and (3) a trade-off between data quality and benchmark scale. To address these challenges, we introduce a general and scalable benchmark construction framework that begins with manually validated, high-quality seed examples and expands them via targeted mutations. Our approach provides a comprehensive suite of artifacts so the benchmark can support comprehensive risk assessment and security capability evaluation using dynamic metrics. By combining expert insights with automated generation, we strike a balance between manual effort, data quality, and benchmark scale. Applying this framework to Python, C/C++, and Java, we build SeCodePLT, a dataset of more than 5.9k samples spanning 44 CWE-based risk categories and three security capabilities. Compared with state-of-the-art benchmarks, SeCodePLT offers broader coverage, higher data fidelity, and substantially greater scale. We use SeCodePLT to evaluate leading code LLMs and agents, revealing their strengths and weaknesses in both generating secure code and identifying or fixing vulnerabilities.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Watson: A Cognitive Observability Framework for the Reasoning of LLM-Powered Agents</title>
<link>https://arxiv.org/abs/2411.03455</link>
<guid>https://arxiv.org/abs/2411.03455</guid>
<content:encoded><![CDATA[
arXiv:2411.03455v3 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly integrated into autonomous systems, giving rise to a new class of software known as Agentware, where LLM-powered agents perform complex, open-ended tasks in domains such as software engineering, customer service, and data analysis. However, their high autonomy and opaque reasoning processes pose significant challenges for traditional software observability methods. To address this, we introduce the concept of cognitive observability - the ability to recover and inspect the implicit reasoning behind agent decisions. We present Watson, a general-purpose framework for observing the reasoning processes of fast-thinking LLM agents without altering their behavior. Watson retroactively infers reasoning traces using prompt attribution techniques. We evaluate Watson in both manual debugging and automated correction scenarios across the MMLU benchmark and the AutoCodeRover and OpenHands agents on the SWE-bench-lite dataset. In both static and dynamic settings, Watson surfaces actionable reasoning insights and supports targeted interventions, demonstrating its practical utility for improving transparency and reliability in Agentware systems.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stop Playing the Guessing Game! Target-free User Simulation for Evaluating Conversational Recommender Systems</title>
<link>https://arxiv.org/abs/2411.16160</link>
<guid>https://arxiv.org/abs/2411.16160</guid>
<content:encoded><![CDATA[
arXiv:2411.16160v2 Announce Type: replace 
Abstract: Recent approaches in Conversational Recommender Systems (CRSs) have tried to simulate real-world users engaging in conversations with CRSs to create more realistic testing environments that reflect the complexity of human-agent dialogue. Despite the significant advancements, reliably evaluating the capability of CRSs to elicit user preferences still faces a significant challenge. Existing evaluation metrics often rely on target-biased user simulators that assume users have predefined preferences, leading to interactions that devolve into simplistic guessing game. These simulators typically guide the CRS toward specific target items based on fixed attributes, limiting the dynamic exploration of user preferences and struggling to capture the evolving nature of real-user interactions. Additionally, current evaluation metrics are predominantly focused on single-turn recall of target items, neglecting the intermediate processes of preference elicitation. To address this, we introduce PEPPER, a novel CRS evaluation protocol with target-free user simulators constructed from real-user interaction histories and reviews. PEPPER enables realistic user-CRS dialogues without falling into simplistic guessing games, allowing users to gradually discover their preferences through enriched interactions, thereby providing a more accurate and reliable assessment of the CRS's ability to elicit personal preferences. Furthermore, PEPPER presents detailed measures for comprehensively evaluating the preference elicitation capabilities of CRSs, encompassing both quantitative and qualitative measures that capture four distinct aspects of the preference elicitation process. Through extensive experiments, we demonstrate the validity of PEPPER as a simulation environment and conduct a thorough analysis of how effectively existing CRSs perform in preference elicitation and recommendation.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NFL-BA: Near-Field Light Bundle Adjustment for SLAM in Dynamic Lighting</title>
<link>https://arxiv.org/abs/2412.13176</link>
<guid>https://arxiv.org/abs/2412.13176</guid>
<content:encoded><![CDATA[
arXiv:2412.13176v3 Announce Type: replace 
Abstract: Simultaneous Localization and Mapping (SLAM) systems typically assume static, distant illumination; however, many real-world scenarios, such as endoscopy, subterranean robotics, and search & rescue in collapsed environments, require agents to operate with a co-located light and camera in the absence of external lighting. In such cases, dynamic near-field lighting introduces strong, view-dependent shading that significantly degrades SLAM performance. We introduce Near-Field Lighting Bundle Adjustment Loss (NFL-BA) which explicitly models near-field lighting as a part of Bundle Adjustment loss and enables better performance for scenes captured with dynamic lighting. NFL-BA can be integrated into neural rendering-based SLAM systems with implicit or explicit scene representations. Our evaluations mainly focus on endoscopy procedure where SLAM can enable autonomous navigation, guidance to unsurveyed regions, blindspot detections, and 3D visualizations, which can significantly improve patient outcomes and endoscopy experience for both physicians and patients. Replacing Photometric Bundle Adjustment loss of SLAM systems with NFL-BA leads to significant improvement in camera tracking, 37% for MonoGS and 14% for EndoGS, and leads to state-of-the-art camera tracking and mapping performance on the C3VD colonoscopy dataset. Further evaluation on indoor scenes captured with phone camera with flashlight turned on, also demonstrate significant improvement in SLAM performance due to NFL-BA. See results at https://asdunnbe.github.io/NFL-BA/
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Self-improvement LLM Agentic System for ML Library Development</title>
<link>https://arxiv.org/abs/2502.02534</link>
<guid>https://arxiv.org/abs/2502.02534</guid>
<content:encoded><![CDATA[
arXiv:2502.02534v2 Announce Type: replace 
Abstract: ML libraries, often written in architecture-specific programming languages (ASPLs) that target domain-specific architectures, are key to efficient ML systems. However, writing these high-performance ML libraries is challenging because it requires expert knowledge of ML algorithms and the ASPL. Large language models (LLMs), on the other hand, have shown general coding capabilities. However, challenges remain when using LLMs for generating ML libraries using ASPLs because 1) this task is complicated even for experienced human programmers and 2) there are limited code examples because of the esoteric and evolving nature of ASPLs. Therefore, LLMs need complex reasoning with limited data in order to complete this task. To address these challenges, we introduce an adaptive self-improvement agentic system. In order to evaluate the effectiveness of our system, we construct a benchmark of a typical ML library and generate ASPL code with both open and closed-source LLMs on this benchmark. Our results show improvements of up to $3.9\times$ over a baseline single LLM.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Runtime Learning of Quadruped Robots in Wild Environments</title>
<link>https://arxiv.org/abs/2503.04794</link>
<guid>https://arxiv.org/abs/2503.04794</guid>
<content:encoded><![CDATA[
arXiv:2503.04794v2 Announce Type: replace 
Abstract: This paper presents a runtime learning framework for quadruped robots, enabling them to learn and adapt safely in dynamic wild environments. The framework integrates sensing, navigation, and control, forming a closed-loop system for the robot. The core novelty of this framework lies in two interactive and complementary components within the control module: the high-performance (HP)-Student and the high-assurance (HA)-Teacher. HP-Student is a deep reinforcement learning (DRL) agent that engages in self-learning and teaching-to-learn to develop a safe and high-performance action policy. HA-Teacher is a simplified yet verifiable physics-model-based controller, with the role of teaching HP-Student about safety while providing a backup for the robot's safe locomotion. HA-Teacher is innovative due to its real-time physics model, real-time action policy, and real-time control goals, all tailored to respond effectively to real-time wild environments, ensuring safety. The framework also includes a coordinator who effectively manages the interaction between HP-Student and HA-Teacher. Experiments involving a Unitree Go2 robot in Nvidia Isaac Gym and comparisons with state-of-the-art safe DRLs demonstrate the effectiveness of the proposed runtime learning framework.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Negotiative Alignment: Embracing Disagreement to Achieve Fairer Outcomes -- Insights from Urban Studies</title>
<link>https://arxiv.org/abs/2503.12613</link>
<guid>https://arxiv.org/abs/2503.12613</guid>
<content:encoded><![CDATA[
arXiv:2503.12613v3 Announce Type: replace 
Abstract: Urban assessments often compress diverse needs into single scores, which can obscure minority perspectives. We present a community-centered study in Montreal (n=35; wheelchair users, seniors, LGBTQIA2+ residents, and immigrants). Participants rated 20 streets (accessibility, inclusivity, aesthetics, practicality) and ranked 7 images on 12 interview-elicited criteria. Disagreement patterns were systematic in our sample: wheelchair users diverged most on accessibility and practicality; LGBTQIA2+ participants emphasized inclusion and liveliness; seniors prioritized security. Group discussion reduced information gaps but not value conflicts; ratings conveyed intensity, while rankings forced trade-offs. We then formalize negotiative alignment, a transparent, budget-aware bargaining procedure, and pilot it with role-played stakeholder agents plus a neutral mediator. Relative to the best base design under the same public rubric, the negotiated package increased total utility (21.10 to 24.55), raised the worst-group utility (3.20 to 3.90), improved twentieth percentile satisfaction (0.86 to 1.00; min-max normalized within the scenario), and reduced inequality (Gini 0.036 to 0.025). Treating disagreement as signal and reporting worst-group outcomes alongside totals may help planners and AI practitioners surface trade-offs and preserve minority priorities while maintaining efficiency.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UXAgent: A System for Simulating Usability Testing of Web Design with LLM Agents</title>
<link>https://arxiv.org/abs/2504.09407</link>
<guid>https://arxiv.org/abs/2504.09407</guid>
<content:encoded><![CDATA[
arXiv:2504.09407v3 Announce Type: replace 
Abstract: Usability testing is a fundamental research method that user experience (UX) researchers use to evaluate and iterate their new designs. But what about evaluating and iterating the usability testing study design itself? Recent advances in Large Language Model-simulated Agent (LLM Agent) research inspired us to design UXAgent to support UX researchers in evaluating and iterating their study design before they conduct the real human-subject study. Our system features a Persona Generator module, an LLM Agent module, and a Universal Browser Connector module to automatically generate thousands of simulated users and to interactively test the target website. The system also provides a Result Viewer Interface so that the UX researchers can easily review and analyze the generated qualitative (e.g., agents' post-study surveys) and quantitative data (e.g., agents' interaction logs), or even interview agents directly. Through a heuristic evaluation with 16 UX researchers, participants praised the innovation of our system but also expressed concerns about the future of LLM Agent usage in UX studies.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM Agents</title>
<link>https://arxiv.org/abs/2504.09723</link>
<guid>https://arxiv.org/abs/2504.09723</guid>
<content:encoded><![CDATA[
arXiv:2504.09723v3 Announce Type: replace 
Abstract: A/B testing experiment is a widely adopted method for evaluating UI/UX design decisions in modern web applications. Yet, traditional A/B testing remains constrained by its dependence on the large-scale and live traffic of human participants, and the long time of waiting for the testing result. Through formative interviews with six experienced industry practitioners, we identified critical bottlenecks in current A/B testing workflows. In response, we present AgentA/B, a novel system that leverages Large Language Model-based autonomous agents (LLM Agents) to automatically simulate user interaction behaviors with real webpages. AgentA/B enables scalable deployment of LLM agents with diverse personas, each capable of navigating the dynamic webpage and interactively executing multi-step interactions like search, clicking, filtering, and purchasing. In a demonstrative controlled experiment, we employ AgentA/B to simulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and compare agent behaviors with real human shopping behaviors at a scale. Our findings suggest AgentA/B can emulate human-like behavior patterns.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing Human Internal Attention Patterns from Gameplay Analysis for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.11118</link>
<guid>https://arxiv.org/abs/2504.11118</guid>
<content:encoded><![CDATA[
arXiv:2504.11118v2 Announce Type: replace 
Abstract: This study introduces a novel method for revealing human internal attention patterns from gameplay data alone, leveraging offline attention techniques from reinforcement learning (RL). We propose contextualized, task-relevant (CTR) attention networks, which generate attention maps from both human and RL agent gameplay in Atari environments. To evaluate whether the human CTR maps reveal internal attention, we validate our model by quantitative and qualitative comparison to the agent maps as well as to a temporally integrated overt attention (TIOA) model based on human eye-tracking data. Our results show that human CTR maps are more sparse than the agent ones and align better with the TIOA maps. Following a qualitative visual comparison we conclude that they likely capture patterns of internal attention. As a further application, we use these maps to guide RL agents, finding that human internal attention-guided agents achieve slightly improved and more stable learning compared to baselines. This work advances the understanding of human-agent attention differences and provides a new approach for extracting and validating internal attention from behavioral data.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HydraRAG: Structured Cross-Source Enhanced Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2505.17464</link>
<guid>https://arxiv.org/abs/2505.17464</guid>
<content:encoded><![CDATA[
arXiv:2505.17464v4 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external knowledge. Current hybrid RAG system retrieves evidence from both knowledge graphs (KGs) and text documents to support LLM reasoning. However, it faces challenges like handling multi-hop reasoning, multi-entity questions, multi-source verification, and effective graph utilization. To address these limitations, we present HydraRAG, a training-free framework that unifies graph topology, document semantics, and source reliability to support deep, faithful reasoning in LLMs. HydraRAG handles multi-hop and multi-entity problems through agent-driven exploration that combines structured and unstructured retrieval, increasing both diversity and precision of evidence. To tackle multi-source verification, HydraRAG uses a tri-factor cross-source verification (source trustworthiness assessment, cross-source corroboration, and entity-path alignment), to balance topic relevance with cross-modal agreement. By leveraging graph structure, HydraRAG fuses heterogeneous sources, guides efficient exploration, and prunes noise early. Comprehensive experiments on seven benchmark datasets show that HydraRAG achieves overall state-of-the-art results on all benchmarks with GPT-3.5-Turbo, outperforming the strong hybrid baseline ToG-2 by an average of 20.3% and up to 30.1%. Furthermore, HydraRAG enables smaller models (e.g., Llama-3.1-8B) to achieve reasoning performance comparable to that of GPT-4-Turbo. The source code is available on https://stevetantan.github.io/HydraRAG/.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators</title>
<link>https://arxiv.org/abs/2505.22777</link>
<guid>https://arxiv.org/abs/2505.22777</guid>
<content:encoded><![CDATA[
arXiv:2505.22777v3 Announce Type: replace 
Abstract: Evaluating the quality of open-domain chatbots has become increasingly reliant on LLMs acting as automatic judges. However, existing meta-evaluation benchmarks are static, outdated, and lacking in multilingual coverage, limiting their ability to fully capture subtle weaknesses in evaluation. We introduce MEDAL, an automated multi-agent framework for curating more representative and diverse open-domain dialogue evaluation benchmarks. Our approach leverages several state-of-the-art LLMs to generate user-chatbot multilingual dialogues, conditioned on varied seed contexts. Then, a strong LLM (GPT-4.1) is used for a multidimensional analysis of the performance of the chatbots, uncovering noticeable cross-lingual performance differences. Guided by this large-scale evaluation, we curate a new meta-evaluation multilingual benchmark and human-annotate samples with nuanced quality judgments. This benchmark is then used to assess the ability of several reasoning and non-reasoning LLMs to act as evaluators of open-domain dialogues. Using MEDAL, we uncover that state-of-the-art judges fail to reliably detect nuanced issues such as lack of empathy, commonsense, or relevance.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Natural Language for Human-Robot Collaboration in the Real World</title>
<link>https://arxiv.org/abs/2508.11759</link>
<guid>https://arxiv.org/abs/2508.11759</guid>
<content:encoded><![CDATA[
arXiv:2508.11759v2 Announce Type: replace 
Abstract: We have a vision of a day when autonomous robots can collaborate with humans as assistants in performing complex tasks in the physical world. This vision includes that the robots will have the ability to communicate with their human collaborators using language that is natural to the humans. Traditional Interactive Task Learning (ITL) systems have some of this ability, but the language they can understand is very limited. The advent of large language models (LLMs) provides an opportunity to greatly improve the language understanding of robots, yet integrating the language abilities of LLMs with robots that operate in the real physical world is a challenging problem.
  In this chapter we first review briefly a few commercial robot products that work closely with humans, and discuss how they could be much better collaborators with robust language abilities. We then explore how an AI system with a cognitive agent that controls a physical robot at its core, interacts with both a human and an LLM, and accumulates situational knowledge through its experiences, can be a possible approach to reach that vision. We focus on three specific challenges of having the robot understand natural language, and present a simple proof-of-concept experiment using ChatGPT for each. Finally, we discuss what it will take to turn these simple experiments into an operational system where LLM-assisted language understanding is a part of an integrated robotic assistant that uses language to collaborate with humans.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Anatomy of a Personal Health Agent</title>
<link>https://arxiv.org/abs/2508.20148</link>
<guid>https://arxiv.org/abs/2508.20148</guid>
<content:encoded><![CDATA[
arXiv:2508.20148v2 Announce Type: replace 
Abstract: Health is a fundamental pillar of human wellness, and the rapid advancements in large language models (LLMs) have driven the development of a new generation of health agents. However, the application of health agents to fulfill the diverse needs of individuals in daily non-clinical settings is underexplored. In this work, we aim to build a comprehensive personal health agent that is able to reason about multimodal data from everyday consumer wellness devices and common personal health records, and provide personalized health recommendations. To understand end-users' needs when interacting with such an assistant, we conducted an in-depth analysis of web search and health forum queries, alongside qualitative insights from users and health experts gathered through a user-centered design process. Based on these findings, we identified three major categories of consumer health needs, each of which is supported by a specialist sub-agent: (1) a data science agent that analyzes personal time-series wearable and health record data, (2) a health domain expert agent that integrates users' health and contextual data to generate accurate, personalized insights, and (3) a health coach agent that synthesizes data insights, guiding users using a specified psychological strategy and tracking users' progress. Furthermore, we propose and develop the Personal Health Agent (PHA), a multi-agent framework that enables dynamic, personalized interactions to address individual health needs. To evaluate each sub-agent and the multi-agent system, we conducted automated and human evaluations across 10 benchmark tasks, involving more than 7,000 annotations and 1,100 hours of effort from health experts and end-users. Our work represents the most comprehensive evaluation of a health agent to date and establishes a strong foundation towards the futuristic vision of a personal health agent accessible to everyone.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Correction to Mastery: Reinforced Distillation of Large Language Model Agents</title>
<link>https://arxiv.org/abs/2509.14257</link>
<guid>https://arxiv.org/abs/2509.14257</guid>
<content:encoded><![CDATA[
arXiv:2509.14257v1 Announce Type: new 
Abstract: Large Language Model agents excel at solving complex tasks through iterative reasoning and tool use, but typically depend on ultra-large, costly backbones. Existing distillation approaches train smaller students to imitate full teacher trajectories, yet reasoning and knowledge gaps between the teacher and student often lead to compounding errors. We propose SCoRe, a student-centered framework in which the student generates trajectories and the teacher intervenes only at the first critical error, producing training data matched to the student's ability and exposing specific weaknesses. The student is first fine-tuned on corrected trajectories. Subsequently, short-horizon reinforcement learning starts from the verified prefix before the first critical error, with target rewards assigned at that step. This design encourages autonomous problem-solving beyond imitation and improves training stability. Particularly, on 12 challenging benchmarks, a 7B-parameter student distilled with SCoRe matches the agentic performance of a 72B-parameter teacher.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constructive Conflict-Driven Multi-Agent Reinforcement Learning for Strategic Diversity</title>
<link>https://arxiv.org/abs/2509.14276</link>
<guid>https://arxiv.org/abs/2509.14276</guid>
<content:encoded><![CDATA[
arXiv:2509.14276v1 Announce Type: new 
Abstract: In recent years, diversity has emerged as a useful mechanism to enhance the efficiency of multi-agent reinforcement learning (MARL). However, existing methods predominantly focus on designing policies based on individual agent characteristics, often neglecting the interplay and mutual influence among agents during policy formation. To address this gap, we propose Competitive Diversity through Constructive Conflict (CoDiCon), a novel approach that incorporates competitive incentives into cooperative scenarios to encourage policy exchange and foster strategic diversity among agents. Drawing inspiration from sociological research, which highlights the benefits of moderate competition and constructive conflict in group decision-making, we design an intrinsic reward mechanism using ranking features to introduce competitive motivations. A centralized intrinsic reward module generates and distributes varying reward values to agents, ensuring an effective balance between competition and cooperation. By optimizing the parameterized centralized reward module to maximize environmental rewards, we reformulate the constrained bilevel optimization problem to align with the original task objectives. We evaluate our algorithm against state-of-the-art methods in the SMAC and GRF environments. Experimental results demonstrate that CoDiCon achieves superior performance, with competitive intrinsic rewards effectively promoting diverse and adaptive strategies among cooperative agents.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization</title>
<link>https://arxiv.org/abs/2509.14279</link>
<guid>https://arxiv.org/abs/2509.14279</guid>
<content:encoded><![CDATA[
arXiv:2509.14279v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) demonstrate their effectiveness in scaling test-time compute for software engineering tasks. However, these approaches often focus on high-level solutions, with limited attention to optimizing low-level CUDA kernel implementations. Additionally, existing kernel generation benchmarks suffer from exploitable loopholes and insufficient diversity in testing conditions, hindering true generalization assessment. To address these limitations, we introduce robust-kbench, a new benchmark for rigorous evaluation of kernel performance and correctness across varied scenarios. Furthermore, we present a comprehensive agentic framework that automates CUDA kernel discovery, verification, and optimization. This pipeline enables frontier LLMs to translate torch code to CUDA kernels and iteratively improve their runtime within our robust evaluation setting. Our sequential workflow first translates PyTorch code into equivalent CUDA kernels. It then optimizes their runtime using a novel evolutionary meta-generation procedure tailored to the CUDA ecosystem, guided by LLM-based verifiers for correctness and efficient filtering. Evaluated on robust-kbench, our approach produces CUDA kernels outperforming torch implementations for practical applications, including forward and backward passes. It can fuse operations and deploy various runtime optimization strategies. The verifier workflow accurately classifies incorrect kernels, enhancing hardware verification efficiency.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Sum Leaks More Than Its Parts: Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2509.14284</link>
<guid>https://arxiv.org/abs/2509.14284</guid>
<content:encoded><![CDATA[
arXiv:2509.14284v1 Announce Type: new 
Abstract: As large language models (LLMs) become integral to multi-agent systems, new privacy risks emerge that extend beyond memorization, direct inference, or single-turn evaluations. In particular, seemingly innocuous responses, when composed across interactions, can cumulatively enable adversaries to recover sensitive information, a phenomenon we term compositional privacy leakage. We present the first systematic study of such compositional privacy leaks and possible mitigation methods in multi-agent LLM systems. First, we develop a framework that models how auxiliary knowledge and agent interactions jointly amplify privacy risks, even when each response is benign in isolation. Next, to mitigate this, we propose and evaluate two defense strategies: (1) Theory-of-Mind defense (ToM), where defender agents infer a questioner's intent by anticipating how their outputs may be exploited by adversaries, and (2) Collaborative Consensus Defense (CoDef), where responder agents collaborate with peers who vote based on a shared aggregated state to restrict sensitive information spread. Crucially, we balance our evaluation across compositions that expose sensitive information and compositions that yield benign inferences. Our experiments quantify how these defense strategies differ in balancing the privacy-utility trade-off. We find that while chain-of-thought alone offers limited protection to leakage (~39% sensitive blocking rate), our ToM defense substantially improves sensitive query blocking (up to 97%) but can reduce benign task success. CoDef achieves the best balance, yielding the highest Balanced Outcome (79.8%), highlighting the benefit of combining explicit reasoning with defender collaboration. Together, our results expose a new class of risks in collaborative LLM deployments and provide actionable insights for designing safeguards against compositional, context-driven privacy leakage.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks</title>
<link>https://arxiv.org/abs/2509.14285</link>
<guid>https://arxiv.org/abs/2509.14285</guid>
<content:encoded><![CDATA[
arXiv:2509.14285v1 Announce Type: new 
Abstract: Prompt injection attacks represent a major vulnerability in Large Language Model (LLM) deployments, where malicious instructions embedded in user inputs can override system prompts and induce unintended behaviors. This paper presents a novel multi-agent defense framework that employs specialized LLM agents in coordinated pipelines to detect and neutralize prompt injection attacks in real-time. We evaluate our approach using two distinct architectures: a sequential chain-of-agents pipeline and a hierarchical coordinator-based system. Our comprehensive evaluation on 55 unique prompt injection attacks, grouped into 8 categories and totaling 400 attack instances across two LLM platforms (ChatGLM and Llama2), demonstrates significant security improvements. Without defense mechanisms, baseline Attack Success Rates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent pipeline achieved 100% mitigation, reducing ASR to 0% across all tested scenarios. The framework demonstrates robustness across multiple attack categories including direct overrides, code execution attempts, data exfiltration, and obfuscation techniques, while maintaining system functionality for legitimate queries.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing</title>
<link>https://arxiv.org/abs/2509.14289</link>
<guid>https://arxiv.org/abs/2509.14289</guid>
<content:encoded><![CDATA[
arXiv:2509.14289v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used to automate or augment penetration testing, but their effectiveness and reliability across attack phases remain unclear. We present a comprehensive evaluation of multiple LLM-based agents, from single-agent to modular designs, across realistic penetration testing scenarios, measuring empirical performance and recurring failure patterns. We also isolate the impact of five core functional capabilities via targeted augmentations: Global Context Memory (GCM), Inter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive Planning (AP), and Real-Time Monitoring (RTM). These interventions support, respectively: (i) context coherence and retention, (ii) inter-component coordination and state management, (iii) tool use accuracy and selective execution, (iv) multi-step strategic planning, error detection, and recovery, and (v) real-time dynamic responsiveness. Our results show that while some architectures natively exhibit subsets of these properties, targeted augmentations substantially improve modular agent performance, especially in complex, multi-step, and real-time penetration testing tasks.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AEGIS: Automated Error Generation and Identification for Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2509.14295</link>
<guid>https://arxiv.org/abs/2509.14295</guid>
<content:encoded><![CDATA[
arXiv:2509.14295v1 Announce Type: new 
Abstract: As Multi-Agent Systems (MAS) become increasingly autonomous and complex, understanding their error modes is critical for ensuring their reliability and safety. However, research in this area has been severely hampered by the lack of large-scale, diverse datasets with precise, ground-truth error labels. To address this bottleneck, we introduce \textbf{AEGIS}, a novel framework for \textbf{A}utomated \textbf{E}rror \textbf{G}eneration and \textbf{I}dentification for Multi-Agent \textbf{S}ystems. By systematically injecting controllable and traceable errors into initially successful trajectories, we create a rich dataset of realistic failures. This is achieved using a context-aware, LLM-based adaptive manipulator that performs sophisticated attacks like prompt injection and response corruption to induce specific, predefined error modes. We demonstrate the value of our dataset by exploring three distinct learning paradigms for the error identification task: Supervised Fine-Tuning, Reinforcement Learning, and Contrastive Learning. Our comprehensive experiments show that models trained on AEGIS data achieve substantial improvements across all three learning paradigms. Notably, several of our fine-tuned models demonstrate performance competitive with or superior to proprietary systems an order of magnitude larger, validating our automated data generation framework as a crucial resource for developing more robust and interpretable multi-agent systems. Our project website is available at https://kfq20.github.io/AEGIS-Website.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks</title>
<link>https://arxiv.org/abs/2509.14380</link>
<guid>https://arxiv.org/abs/2509.14380</guid>
<content:encoded><![CDATA[
arXiv:2509.14380v1 Announce Type: new 
Abstract: Multi-Agent Reinforcement Learning (MARL) provides a powerful framework for learning coordination in multi-agent systems. However, applying MARL to robotics still remains challenging due to high-dimensional continuous joint action spaces, complex reward design, and non-stationary transitions inherent to decentralized settings. On the other hand, humans learn complex coordination through staged curricula, where long-horizon behaviors are progressively built upon simpler skills. Motivated by this, we propose CRAFT: Coaching Reinforcement learning Autonomously using Foundation models for multi-robot coordination Tasks, a framework that leverages the reasoning capabilities of foundation models to act as a "coach" for multi-robot coordination. CRAFT automatically decomposes long-horizon coordination tasks into sequences of subtasks using the planning capability of Large Language Models (LLMs). In what follows, CRAFT trains each subtask using reward functions generated by LLM, and refines them through a Vision Language Model (VLM)-guided reward-refinement loop. We evaluate CRAFT on multi-quadruped navigation and bimanual manipulation tasks, demonstrating its capability to learn complex coordination behaviors. In addition, we validate the multi-quadruped navigation policy in real hardware experiments.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Pipeline Failures through Fine-Grained Analysis of Web Agents</title>
<link>https://arxiv.org/abs/2509.14382</link>
<guid>https://arxiv.org/abs/2509.14382</guid>
<content:encoded><![CDATA[
arXiv:2509.14382v1 Announce Type: new 
Abstract: Web agents powered by large language models (LLMs) can autonomously perform complex, multistep tasks in dynamic web environments. However, current evaluations mostly focus on the overall success while overlooking intermediate errors. This limits insight into failure modes and hinders systematic improvement. This work analyzes existing benchmarks and highlights the lack of fine-grained diagnostic tools. To address this gap, we propose a modular evaluation framework that decomposes agent pipelines into interpretable stages for detailed error analysis. Using the SeeAct framework and the Mind2Web dataset as a case study, we show how this approach reveals actionable weaknesses missed by standard metrics - paving the way for more robust and generalizable web agents.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disproving the Feasibility of Learned Confidence Calibration Under Binary Supervision: An Information-Theoretic Impossibility</title>
<link>https://arxiv.org/abs/2509.14386</link>
<guid>https://arxiv.org/abs/2509.14386</guid>
<content:encoded><![CDATA[
arXiv:2509.14386v1 Announce Type: new 
Abstract: We prove a fundamental impossibility theorem: neural networks cannot simultaneously learn well-calibrated confidence estimates with meaningful diversity when trained using binary correct/incorrect supervision. Through rigorous mathematical analysis and comprehensive empirical evaluation spanning negative reward training, symmetric loss functions, and post-hoc calibration methods, we demonstrate this is an information-theoretic constraint, not a methodological failure. Our experiments reveal universal failure patterns: negative rewards produce extreme underconfidence (ECE greater than 0.8) while destroying confidence diversity (std less than 0.05), symmetric losses fail to escape binary signal averaging, and post-hoc methods achieve calibration (ECE less than 0.02) only by compressing the confidence distribution. We formalize this as an underspecified mapping problem where binary signals cannot distinguish between different confidence levels for correct predictions: a 60 percent confident correct answer receives identical supervision to a 90 percent confident one. Crucially, our real-world validation shows 100 percent failure rate for all training methods across MNIST, Fashion-MNIST, and CIFAR-10, while post-hoc calibration's 33 percent success rate paradoxically confirms our theorem by achieving calibration through transformation rather than learning. This impossibility directly explains neural network hallucinations and establishes why post-hoc calibration is mathematically necessary, not merely convenient. We propose novel supervision paradigms using ensemble disagreement and adaptive multi-agent learning that could overcome these fundamental limitations without requiring human confidence annotations.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GestOS: Advanced Hand Gesture Interpretation via Large Language Models to control Any Type of Robot</title>
<link>https://arxiv.org/abs/2509.14412</link>
<guid>https://arxiv.org/abs/2509.14412</guid>
<content:encoded><![CDATA[
arXiv:2509.14412v1 Announce Type: new 
Abstract: We present GestOS, a gesture-based operating system for high-level control of heterogeneous robot teams. Unlike prior systems that map gestures to fixed commands or single-agent actions, GestOS interprets hand gestures semantically and dynamically distributes tasks across multiple robots based on their capabilities, current state, and supported instruction sets. The system combines lightweight visual perception with large language model (LLM) reasoning: hand poses are converted into structured textual descriptions, which the LLM uses to infer intent and generate robot-specific commands. A robot selection module ensures that each gesture-triggered task is matched to the most suitable agent in real time. This architecture enables context-aware, adaptive control without requiring explicit user specification of targets or commands. By advancing gesture interaction from recognition to intelligent orchestration, GestOS supports scalable, flexible, and user-friendly collaboration with robotic systems in dynamic environments.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local-Canonicalization Equivariant Graph Neural Networks for Sample-Efficient and Generalizable Swarm Robot Control</title>
<link>https://arxiv.org/abs/2509.14431</link>
<guid>https://arxiv.org/abs/2509.14431</guid>
<content:encoded><![CDATA[
arXiv:2509.14431v1 Announce Type: new 
Abstract: Multi-agent reinforcement learning (MARL) has emerged as a powerful paradigm for coordinating swarms of agents in complex decision-making, yet major challenges remain. In competitive settings such as pursuer-evader tasks, simultaneous adaptation can destabilize training; non-kinetic countermeasures often fail under adverse conditions; and policies trained in one configuration rarely generalize to environments with a different number of agents. To address these issues, we propose the Local-Canonicalization Equivariant Graph Neural Networks (LEGO) framework, which integrates seamlessly with popular MARL algorithms such as MAPPO. LEGO employs graph neural networks to capture permutation equivariance and generalization to different agent numbers, canonicalization to enforce E(n)-equivariance, and heterogeneous representations to encode role-specific inductive biases. Experiments on cooperative and competitive swarm benchmarks show that LEGO outperforms strong baselines and improves generalization. In real-world experiments, LEGO demonstrates robustness to varying team sizes and agent failure.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Learning of Deceptive Policies under Intermittent Observation</title>
<link>https://arxiv.org/abs/2509.14453</link>
<guid>https://arxiv.org/abs/2509.14453</guid>
<content:encoded><![CDATA[
arXiv:2509.14453v1 Announce Type: new 
Abstract: In supervisory control settings, autonomous systems are not monitored continuously. Instead, monitoring often occurs at sporadic intervals within known bounds. We study the problem of deception, where an agent pursues a private objective while remaining plausibly compliant with a supervisor's reference policy when observations occur. Motivated by the behavior of real, human supervisors, we situate the problem within Theory of Mind: the representation of what an observer believes and expects to see. We show that Theory of Mind can be repurposed to steer online reinforcement learning (RL) toward such deceptive behavior. We model the supervisor's expectations and distill from them a single, calibrated scalar -- the expected evidence of deviation if an observation were to happen now. This scalar combines how unlike the reference and current action distributions appear, with the agent's belief that an observation is imminent. Injected as a state-dependent weight into a KL-regularized policy improvement step within an online RL loop, this scalar informs a closed-form update that smoothly trades off self-interest and compliance, thus sidestepping hand-crafted or heuristic policies. In real-world, real-time hardware experiments on marine (ASV) and aerial (UAV) navigation, our ToM-guided RL runs online, achieves high return and success with observed-trace evidence calibrated to the supervisor's expectations.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ticket-Bench: A Kickoff for Multilingual and Regionalized Agent Evaluation</title>
<link>https://arxiv.org/abs/2509.14477</link>
<guid>https://arxiv.org/abs/2509.14477</guid>
<content:encoded><![CDATA[
arXiv:2509.14477v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed as task-oriented agents, where success depends on their ability to generate accurate function calls under realistic, multilingual conditions. However, existing agent evaluations largely overlook cultural and linguistic diversity, often relying on monolingual or naively translated benchmarks. We introduce Ticket-Bench, a benchmark for multilingual agent evaluation in task-oriented scenarios. Ticket-Bench simulates the domain of soccer ticket purchases across six major languages: Portuguese, English, Spanish, German, Italian, and French. Using localized teams, cities, and user profiles to provide a higher level of realism. We evaluate a wide range of commercial and open-source LLMs, measuring function-calling accuracy and consistency across languages. Results show that reasoning-oriented models (e.g., GPT-5, Qwen3-235B) dominate performance but still exhibit notable cross-lingual disparities. These findings underscore the need for culturally aware, multilingual benchmarks to guide the development of robust LLM agents.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents</title>
<link>https://arxiv.org/abs/2509.14480</link>
<guid>https://arxiv.org/abs/2509.14480</guid>
<content:encoded><![CDATA[
arXiv:2509.14480v1 Announce Type: new 
Abstract: Effective interactive tool use requires agents to master Tool Integrated Reasoning (TIR): a complex process involving multi-turn planning and long-context dialogue management. To train agents for this dynamic process, particularly in multi-modal contexts, we introduce a sandbox environment for reinforcement learning (RL) that supports interleaved speech-text rollouts. Our core strategy, Turn-level Adjudicated Reinforcement Learning (TARL), addresses the challenge of credit assignment in long-horizon tasks by employing a Large Language Model (LLM) as a judge to provide turn-level evaluation. To enhance exploration, we integrate a mixed-task training curriculum with mathematical reasoning problems. This unified approach boosts the task pass rate on the text-based $\tau$-bench by over 6% compared to strong RL baselines. Crucially, we demonstrate our framework's suitability for fine-tuning a multi-modal foundation model for agentic tasks. By training a base multi-modal LLM on interleaved speech-text rollouts, we equip it with tool-use abilities, paving the way for more natural, voice-driven interactive agents.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM-based multi-agent framework for agile effort estimation</title>
<link>https://arxiv.org/abs/2509.14483</link>
<guid>https://arxiv.org/abs/2509.14483</guid>
<content:encoded><![CDATA[
arXiv:2509.14483v1 Announce Type: new 
Abstract: Effort estimation is a crucial activity in agile software development, where teams collaboratively review, discuss, and estimate the effort required to complete user stories in a product backlog. Current practices in agile effort estimation heavily rely on subjective assessments, leading to inaccuracies and inconsistencies in the estimates. While recent machine learning-based methods show promising accuracy, they cannot explain or justify their estimates and lack the capability to interact with human team members. Our paper fills this significant gap by leveraging the powerful capabilities of Large Language Models (LLMs). We propose a novel LLM-based multi-agent framework for agile estimation that not only can produce estimates, but also can coordinate, communicate and discuss with human developers and other agents to reach a consensus. Evaluation results on a real-life dataset show that our approach outperforms state-of-the-art techniques across all evaluation metrics in the majority of the cases. Our human study with software development practitioners also demonstrates an overwhelmingly positive experience in collaborating with our agents in agile effort estimation.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the high score: Prosocial ability profiles of multi-agent populations</title>
<link>https://arxiv.org/abs/2509.14485</link>
<guid>https://arxiv.org/abs/2509.14485</guid>
<content:encoded><![CDATA[
arXiv:2509.14485v1 Announce Type: new 
Abstract: The development and evaluation of social capabilities in AI agents require complex environments where competitive and cooperative behaviours naturally emerge. While game-theoretic properties can explain why certain teams or agent populations outperform others, more abstract behaviours, such as convention following, are harder to control in training and evaluation settings. The Melting Pot contest is a social AI evaluation suite designed to assess the cooperation capabilities of AI systems. In this paper, we apply a Bayesian approach known as Measurement Layouts to infer the capability profiles of multi-agent systems in the Melting Pot contest. We show that these capability profiles not only predict future performance within the Melting Pot suite but also reveal the underlying prosocial abilities of agents. Our analysis indicates that while higher prosocial capabilities sometimes correlate with better performance, this is not a universal trend-some lower-scoring agents exhibit stronger cooperation abilities. Furthermore, we find that top-performing contest submissions are more likely to achieve high scores in scenarios where prosocial capabilities are not required. These findings, together with reports that the contest winner used a hard-coded solution tailored to specific environments, suggest that at least one top-performing team may have optimised for conditions where cooperation was not necessary, potentially exploiting limitations in the evaluation framework. We provide recommendations for improving the annotation of cooperation demands and propose future research directions to account for biases introduced by different testing environments. Our results demonstrate that Measurement Layouts offer both strong predictive accuracy and actionable insights, contributing to a more transparent and generalisable approach to evaluating AI systems in complex social settings.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Johnny Can't Use Agents: Industry Aspirations vs. User Realities with AI Agent Software</title>
<link>https://arxiv.org/abs/2509.14528</link>
<guid>https://arxiv.org/abs/2509.14528</guid>
<content:encoded><![CDATA[
arXiv:2509.14528v1 Announce Type: new 
Abstract: There is growing imprecision about what "AI agents" are, what they can do, and how effectively they can be used by their intended users. We pose two key research questions: (i) How does the tech industry conceive of and market "AI agents"? (ii) What challenges do end-users face when attempting to use commercial AI agents for their advertised uses? We first performed a systematic review of marketed use cases for 102 commercial AI agents, finding that they fall into three umbrella categories: orchestration, creation, and insight. Next, we conducted a usability assessment where N = 31 participants attempted representative tasks for each of these categories on two popular commercial AI agent tools: Operator and Manus. We found that users were generally impressed with these agents but faced several critical usability challenges ranging from agent capabilities that were misaligned with user mental models to agents lacking the meta-cognitive abilities necessary for effective collaboration.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClearFairy: Capturing Creative Workflows through Decision Structuring, In-Situ Questioning, and Rationale Inference</title>
<link>https://arxiv.org/abs/2509.14537</link>
<guid>https://arxiv.org/abs/2509.14537</guid>
<content:encoded><![CDATA[
arXiv:2509.14537v1 Announce Type: new 
Abstract: Capturing professionals' decision-making in creative workflows is essential for reflection, collaboration, and knowledge sharing, yet existing methods often leave rationales incomplete and implicit decisions hidden. To address this, we present CLEAR framework that structures reasoning into cognitive decision steps-linked units of actions, artifacts, and self-explanations that make decisions traceable. Building on this framework, we introduce ClearFairy, a think-aloud AI assistant for UI design that detects weak explanations, asks lightweight clarifying questions, and infers missing rationales to ease the knowledge-sharing burden. In a study with twelve creative professionals, 85% of ClearFairy's inferred rationales were accepted, increasing strong explanations from 14% to over 83% of decision steps without adding cognitive demand. The captured steps also enhanced generative AI agents in Figma, yielding next-action predictions better aligned with professionals and producing more coherent design outcomes. For future research on human knowledge-grounded creative AI agents, we release a dataset of captured 417 decision steps.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rationality Check! Benchmarking the Rationality of Large Language Models</title>
<link>https://arxiv.org/abs/2509.14546</link>
<guid>https://arxiv.org/abs/2509.14546</guid>
<content:encoded><![CDATA[
arXiv:2509.14546v1 Announce Type: new 
Abstract: Large language models (LLMs), a recent advance in deep learning and machine intelligence, have manifested astonishing capacities, now considered among the most promising for artificial general intelligence. With human-like capabilities, LLMs have been used to simulate humans and serve as AI assistants across many applications. As a result, great concern has arisen about whether and under what circumstances LLMs think and behave like real human agents. Rationality is among the most important concepts in assessing human behavior, both in thinking (i.e., theoretical rationality) and in taking action (i.e., practical rationality). In this work, we propose the first benchmark for evaluating the omnibus rationality of LLMs, covering a wide range of domains and LLMs. The benchmark includes an easy-to-use toolkit, extensive experimental results, and analysis that illuminates where LLMs converge and diverge from idealized human rationality. We believe the benchmark can serve as a foundational tool for both developers and users of LLMs.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>(P)rior(D)yna(F)low: A Priori Dynamic Workflow Construction via Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2509.14547</link>
<guid>https://arxiv.org/abs/2509.14547</guid>
<content:encoded><![CDATA[
arXiv:2509.14547v1 Announce Type: new 
Abstract: Recent studies have shown that carefully designed workflows coordinating large language models(LLMs) significantly enhance task-solving capabilities compared to using a single model. While an increasing number of works focus on autonomous workflow construction, most existing approaches rely solely on historical experience, leading to limitations in efficiency and adaptability. We argue that while historical experience is valuable, workflow construction should also flexibly respond to the unique characteristics of each task. To this end, we propose an a priori dynamic framework for automated workflow construction. Our framework first leverages Q-table learning to optimize the decision space, guiding agent decisions and enabling effective use of historical experience. At the same time, agents evaluate the current task progress and make a priori decisions regarding the next executing agent, allowing the system to proactively select the more suitable workflow structure for each given task. Additionally, we incorporate mechanisms such as cold-start initialization, early stopping, and pruning to further improve system efficiency. Experimental evaluations on four benchmark datasets demonstrate the feasibility and effectiveness of our approach. Compared to state-of-the-art baselines, our method achieves an average improvement of 4.05%, while reducing workflow construction and inference costs to only 30.68%-48.31% of those required by existing methods.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DICE: Diffusion Consensus Equilibrium for Sparse-view CT Reconstruction</title>
<link>https://arxiv.org/abs/2509.14566</link>
<guid>https://arxiv.org/abs/2509.14566</guid>
<content:encoded><![CDATA[
arXiv:2509.14566v1 Announce Type: new 
Abstract: Sparse-view computed tomography (CT) reconstruction is fundamentally challenging due to undersampling, leading to an ill-posed inverse problem. Traditional iterative methods incorporate handcrafted or learned priors to regularize the solution but struggle to capture the complex structures present in medical images. In contrast, diffusion models (DMs) have recently emerged as powerful generative priors that can accurately model complex image distributions. In this work, we introduce Diffusion Consensus Equilibrium (DICE), a framework that integrates a two-agent consensus equilibrium into the sampling process of a DM. DICE alternates between: (i) a data-consistency agent, implemented through a proximal operator enforcing measurement consistency, and (ii) a prior agent, realized by a DM performing a clean image estimation at each sampling step. By balancing these two complementary agents iteratively, DICE effectively combines strong generative prior capabilities with measurement consistency. Experimental results show that DICE significantly outperforms state-of-the-art baselines in reconstructing high-quality CT images under uniform and non-uniform sparse-view settings of 15, 30, and 60 views (out of a total of 180), demonstrating both its effectiveness and robustness.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Human-like Multimodal Conversational Agent by Generating Engaging Speech</title>
<link>https://arxiv.org/abs/2509.14627</link>
<guid>https://arxiv.org/abs/2509.14627</guid>
<content:encoded><![CDATA[
arXiv:2509.14627v1 Announce Type: new 
Abstract: Human conversation involves language, speech, and visual cues, with each medium providing complementary information. For instance, speech conveys a vibe or tone not fully captured by text alone. While multimodal LLMs focus on generating text responses from diverse inputs, less attention has been paid to generating natural and engaging speech. We propose a human-like agent that generates speech responses based on conversation mood and responsive style information. To achieve this, we build a novel MultiSensory Conversation dataset focused on speech to enable agents to generate natural speech. We then propose a multimodal LLM-based model for generating text responses and voice descriptions, which are used to generate speech covering paralinguistic information. Experimental results demonstrate the effectiveness of utilizing both visual and audio modalities in conversation to generate engaging speech. The source code is available in https://github.com/kimtaesu24/MSenC
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-QA: Can Language Models Answer Repository-level Code Questions?</title>
<link>https://arxiv.org/abs/2509.14635</link>
<guid>https://arxiv.org/abs/2509.14635</guid>
<content:encoded><![CDATA[
arXiv:2509.14635v1 Announce Type: new 
Abstract: Understanding and reasoning about entire software repositories is an essential capability for intelligent software engineering tools. While existing benchmarks such as CoSQA and CodeQA have advanced the field, they predominantly focus on small, self-contained code snippets. These setups fail to capture the complexity of real-world repositories, where effective understanding and reasoning often require navigating multiple files, understanding software architecture, and grounding answers in long-range code dependencies. In this paper, we present SWE-QA, a repository-level code question answering (QA) benchmark designed to facilitate research on automated QA systems in realistic code environments. SWE-QA involves 576 high-quality question-answer pairs spanning diverse categories, including intention understanding, cross-file reasoning, and multi-hop dependency analysis. To construct SWE-QA, we first crawled 77,100 GitHub issues from 11 popular repositories. Based on an analysis of naturally occurring developer questions extracted from these issues, we developed a two-level taxonomy of repository-level questions and constructed a set of seed questions for each category. For each category, we manually curated and validated questions and collected their corresponding answers. As a prototype application, we further develop SWE-QA-Agent, an agentic framework in which LLM agents reason and act to find answers automatically. We evaluate six advanced LLMs on SWE-QA under various context augmentation strategies. Experimental results highlight the promise of LLMs, particularly our SWE-QA-Agent framework, in addressing repository-level QA, while also revealing open challenges and pointing to future research directions.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentCompass: Towards Reliable Evaluation of Agentic Workflows in Production</title>
<link>https://arxiv.org/abs/2509.14647</link>
<guid>https://arxiv.org/abs/2509.14647</guid>
<content:encoded><![CDATA[
arXiv:2509.14647v1 Announce Type: new 
Abstract: With the growing adoption of Large Language Models (LLMs) in automating complex, multi-agent workflows, organizations face mounting risks from errors, emergent behaviors, and systemic failures that current evaluation methods fail to capture. We present AgentCompass, the first evaluation framework designed specifically for post-deployment monitoring and debugging of agentic workflows. AgentCompass models the reasoning process of expert debuggers through a structured, multi-stage analytical pipeline: error identification and categorization, thematic clustering, quantitative scoring, and strategic summarization. The framework is further enhanced with a dual memory system-episodic and semantic-that enables continual learning across executions. Through collaborations with design partners, we demonstrate the framework's practical utility on real-world deployments, before establishing its efficacy against the publicly available TRAIL benchmark. AgentCompass achieves state-of-the-art results on key metrics, while uncovering critical issues missed in human annotations, underscoring its role as a robust, developer-centric tool for reliable monitoring and improvement of agentic systems in production.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TableDART: Dynamic Adaptive Multi-Modal Routing for Table Understanding</title>
<link>https://arxiv.org/abs/2509.14671</link>
<guid>https://arxiv.org/abs/2509.14671</guid>
<content:encoded><![CDATA[
arXiv:2509.14671v1 Announce Type: new 
Abstract: Modeling semantic and structural information from tabular data remains a core challenge for effective table understanding. Existing Table-as-Text approaches flatten tables for large language models (LLMs), but lose crucial structural cues, while Table-as-Image methods preserve structure yet struggle with fine-grained semantics. Recent Table-as-Multimodality strategies attempt to combine textual and visual views, but they (1) statically process both modalities for every query-table pair within a large multimodal LLMs (MLLMs), inevitably introducing redundancy and even conflicts, and (2) depend on costly fine-tuning of MLLMs. In light of this, we propose TableDART, a training-efficient framework that integrates multimodal views by reusing pretrained single-modality models. TableDART introduces a lightweight 2.59M-parameter MLP gating network that dynamically selects the optimal path (either Text-only, Image-only, or Fusion) for each table-query pair, effectively reducing redundancy and conflicts from both modalities. In addition, we propose a novel agent to mediate cross-modal knowledge integration by analyzing outputs from text- and image-based models, either selecting the best result or synthesizing a new answer through reasoning. This design avoids the prohibitive costs of full MLLM fine-tuning. Extensive experiments on seven benchmarks show that TableDART establishes new state-of-the-art performance among open-source models, surpassing the strongest baseline by an average of 4.02%. The code is available at: https://anonymous.4open.science/r/TableDART-C52B
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEED: A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.14680</link>
<guid>https://arxiv.org/abs/2509.14680</guid>
<content:encoded><![CDATA[
arXiv:2509.14680v1 Announce Type: new 
Abstract: Multi-agent reinforcement learning (MARL) holds substantial promise for intelligent decision-making in complex environments. However, it suffers from a coordination and scalability bottleneck as the number of agents increases. To address these issues, we propose the LLM-empowered expert demonstrations framework for multi-agent reinforcement learning (LEED). LEED consists of two components: a demonstration generation (DG) module and a policy optimization (PO) module. Specifically, the DG module leverages large language models to generate instructions for interacting with the environment, thereby producing high-quality demonstrations. The PO module adopts a decentralized training paradigm, where each agent utilizes the generated demonstrations to construct an expert policy loss, which is then integrated with its own policy loss. This enables each agent to effectively personalize and optimize its local policy based on both expert knowledge and individual experience. Experimental results show that LEED achieves superior sample efficiency, time efficiency, and robust scalability compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Use of Agentic Coding Manifests: An Empirical Study of Claude Code</title>
<link>https://arxiv.org/abs/2509.14744</link>
<guid>https://arxiv.org/abs/2509.14744</guid>
<content:encoded><![CDATA[
arXiv:2509.14744v1 Announce Type: new 
Abstract: Agentic coding tools receive goals written in natural language as input, break them down into specific tasks, and write/execute the actual code with minimal human intervention. Key to this process are agent manifests, configuration files (such as Claude.md) that provide agents with essential project context, identity, and operational rules. However, the lack of comprehensive and accessible documentation for creating these manifests presents a significant challenge for developers. We analyzed 253 Claude.md files from 242 repositories to identify structural patterns and common content. Our findings show that manifests typically have shallow hierarchies with one main heading and several subsections, with content dominated by operational commands, technical implementation notes, and high-level architecture.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Use of Agentic Coding: An Empirical Study of Pull Requests on GitHub</title>
<link>https://arxiv.org/abs/2509.14745</link>
<guid>https://arxiv.org/abs/2509.14745</guid>
<content:encoded><![CDATA[
arXiv:2509.14745v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly being integrated into software development processes. The ability to generate code and submit pull requests with minimal human intervention, through the use of autonomous AI agents, is poised to become a standard practice. However, little is known about the practical usefulness of these pull requests and the extent to which their contributions are accepted in real-world projects. In this paper, we empirically study 567 GitHub pull requests (PRs) generated using Claude Code, an agentic coding tool, across 157 diverse open-source projects. Our analysis reveals that developers tend to rely on agents for tasks such as refactoring, documentation, and testing. The results indicate that 83.8% of these agent-assisted PRs are eventually accepted and merged by project maintainers, with 54.9% of the merged PRs are integrated without further modification. The remaining 45.1% require additional changes benefit from human revisions, especially for bug fixes, documentation, and adherence to project-specific standards. These findings suggest that while agent-assisted PRs are largely acceptable, they still benefit from human oversight and refinement.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Retrieval Augmentation via Adversarial Collaboration</title>
<link>https://arxiv.org/abs/2509.14750</link>
<guid>https://arxiv.org/abs/2509.14750</guid>
<content:encoded><![CDATA[
arXiv:2509.14750v1 Announce Type: new 
Abstract: Retrieval-augmented Generation (RAG) is a prevalent approach for domain-specific LLMs, yet it is often plagued by "Retrieval Hallucinations"--a phenomenon where fine-tuned models fail to recognize and act upon poor-quality retrieved documents, thus undermining performance. To address this, we propose the Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two heterogeneous agents: a generalist Detector that identifies knowledge gaps, and a domain-specialized Resolver that provides precise solutions. Guided by a moderator, these agents engage in an adversarial collaboration, where the Detector's persistent questioning challenges the Resolver's expertise. This dynamic process allows for iterative problem dissection and refined knowledge retrieval. Extensive experiments show that AC-RAG significantly improves retrieval accuracy and outperforms state-of-the-art RAG methods across various vertical domains.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenLens AI: Fully Autonomous Research Agent for Health Infomatics</title>
<link>https://arxiv.org/abs/2509.14778</link>
<guid>https://arxiv.org/abs/2509.14778</guid>
<content:encoded><![CDATA[
arXiv:2509.14778v1 Announce Type: new 
Abstract: Health informatics research is characterized by diverse data modalities, rapid knowledge expansion, and the need to integrate insights across biomedical science, data analytics, and clinical practice. These characteristics make it particularly well-suited for agent-based approaches that can automate knowledge exploration, manage complex workflows, and generate clinically meaningful outputs. Recent progress in large language model (LLM)-based agents has demonstrated promising capabilities in literature synthesis, data analysis, and even end-to-end research execution. However, existing systems remain limited for health informatics because they lack mechanisms to interpret medical visualizations and often overlook domain-specific quality requirements. To address these gaps, we introduce OpenLens AI, a fully automated framework tailored to health informatics. OpenLens AI integrates specialized agents for literature review, data analysis, code generation, and manuscript preparation, enhanced by vision-language feedback for medical visualization and quality control for reproducibility. The framework automates the entire research pipeline, producing publication-ready LaTeX manuscripts with transparent and traceable workflows, thereby offering a domain-adapted solution for advancing health informatics research.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STEP: Structured Training and Evaluation Platform for benchmarking trajectory prediction models</title>
<link>https://arxiv.org/abs/2509.14801</link>
<guid>https://arxiv.org/abs/2509.14801</guid>
<content:encoded><![CDATA[
arXiv:2509.14801v1 Announce Type: new 
Abstract: While trajectory prediction plays a critical role in enabling safe and effective path-planning in automated vehicles, standardized practices for evaluating such models remain underdeveloped. Recent efforts have aimed to unify dataset formats and model interfaces for easier comparisons, yet existing frameworks often fall short in supporting heterogeneous traffic scenarios, joint prediction models, or user documentation. In this work, we introduce STEP -- a new benchmarking framework that addresses these limitations by providing a unified interface for multiple datasets, enforcing consistent training and evaluation conditions, and supporting a wide range of prediction models. We demonstrate the capabilities of STEP in a number of experiments which reveal 1) the limitations of widely-used testing procedures, 2) the importance of joint modeling of agents for better predictions of interactions, and 3) the vulnerability of current state-of-the-art models against both distribution shifts and targeted attacks by adversarial agents. With STEP, we aim to shift the focus from the ``leaderboard'' approach to deeper insights about model behavior and generalization in complex multi-agent settings.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OnlineMate: An LLM-Based Multi-Agent Companion System for Cognitive Support in Online Learning</title>
<link>https://arxiv.org/abs/2509.14803</link>
<guid>https://arxiv.org/abs/2509.14803</guid>
<content:encoded><![CDATA[
arXiv:2509.14803v1 Announce Type: new 
Abstract: In online learning environments, students often lack personalized peer interactions, which play a crucial role in supporting cognitive development and learning engagement. Although previous studies have utilized large language models (LLMs) to simulate interactive dynamic learning environments for students, these interactions remain limited to conversational exchanges, lacking insights and adaptations to the learners' individualized learning and cognitive states. As a result, students' interest in discussions with AI learning companions is low, and they struggle to gain inspiration from such interactions. To address this challenge, we propose OnlineMate, a multi-agent learning companion system driven by LLMs that integrates the Theory of Mind (ToM). OnlineMate is capable of simulating peer-like agent roles, adapting to learners' cognitive states during collaborative discussions, and inferring their psychological states, such as misunderstandings, confusion, or motivation. By incorporating Theory of Mind capabilities, the system can dynamically adjust its interaction strategies to support the development of higher-order thinking and cognition. Experimental results in simulated learning scenarios demonstrate that OnlineMate effectively fosters deep learning and discussions while enhancing cognitive engagement in online educational settings.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Agents at the Roundtable: A Multi-Perspective and Dialectical Reasoning Framework for Essay Scoring</title>
<link>https://arxiv.org/abs/2509.14834</link>
<guid>https://arxiv.org/abs/2509.14834</guid>
<content:encoded><![CDATA[
arXiv:2509.14834v1 Announce Type: new 
Abstract: The emergence of large language models (LLMs) has brought a new paradigm to automated essay scoring (AES), a long-standing and practical application of natural language processing in education. However, achieving human-level multi-perspective understanding and judgment remains a challenge. In this work, we propose Roundtable Essay Scoring (RES), a multi-agent evaluation framework designed to perform precise and human-aligned scoring under a zero-shot setting. RES constructs evaluator agents based on LLMs, each tailored to a specific prompt and topic context. Each agent independently generates a trait-based rubric and conducts a multi-perspective evaluation. Then, by simulating a roundtable-style discussion, RES consolidates individual evaluations through a dialectical reasoning process to produce a final holistic score that more closely aligns with human evaluation. By enabling collaboration and consensus among agents with diverse evaluation perspectives, RES outperforms prior zero-shot AES approaches. Experiments on the ASAP dataset using ChatGPT and Claude show that RES achieves up to a 34.86% improvement in average QWK over straightforward prompting (Vanilla) methods.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MARIC: Multi-Agent Reasoning for Image Classification</title>
<link>https://arxiv.org/abs/2509.14860</link>
<guid>https://arxiv.org/abs/2509.14860</guid>
<content:encoded><![CDATA[
arXiv:2509.14860v1 Announce Type: new 
Abstract: Image classification has traditionally relied on parameter-intensive model training, requiring large-scale annotated datasets and extensive fine tuning to achieve competitive performance. While recent vision language models (VLMs) alleviate some of these constraints, they remain limited by their reliance on single pass representations, often failing to capture complementary aspects of visual content. In this paper, we introduce Multi Agent based Reasoning for Image Classification (MARIC), a multi agent framework that reformulates image classification as a collaborative reasoning process. MARIC first utilizes an Outliner Agent to analyze the global theme of the image and generate targeted prompts. Based on these prompts, three Aspect Agents extract fine grained descriptions along distinct visual dimensions. Finally, a Reasoning Agent synthesizes these complementary outputs through integrated reflection step, producing a unified representation for classification. By explicitly decomposing the task into multiple perspectives and encouraging reflective synthesis, MARIC mitigates the shortcomings of both parameter-heavy training and monolithic VLM reasoning. Experiments on 4 diverse image classification benchmark datasets demonstrate that MARIC significantly outperforms baselines, highlighting the effectiveness of multi-agent visual reasoning for robust and interpretable image classification.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Driven Multi-Agent Vehicular Planning for Battery Efficiency and QoS in 6G Smart Cities</title>
<link>https://arxiv.org/abs/2509.14877</link>
<guid>https://arxiv.org/abs/2509.14877</guid>
<content:encoded><![CDATA[
arXiv:2509.14877v1 Announce Type: new 
Abstract: While simulators exist for vehicular IoT nodes communicating with the Cloud through Edge nodes in a fully-simulated osmotic architecture, they often lack support for dynamic agent planning and optimisation to minimise vehicular battery consumption while ensuring fair communication times. Addressing these challenges requires extending current simulator architectures with AI algorithms for both traffic prediction and dynamic agent planning. This paper presents an extension of SimulatorOrchestrator (SO) to meet these requirements. Preliminary results over a realistic urban dataset show that utilising vehicular planning algorithms can lead to improved battery and QoS performance compared with traditional shortest path algorithms. The additional inclusion of desirability areas enabled more ambulances to be routed to their target destinations while utilising less energy to do so, compared to traditional and weighted algorithms without desirability considerations.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human</title>
<link>https://arxiv.org/abs/2509.14889</link>
<guid>https://arxiv.org/abs/2509.14889</guid>
<content:encoded><![CDATA[
arXiv:2509.14889v1 Announce Type: new 
Abstract: In this work, we present CollabVLA, a self-reflective vision-language-action framework that transforms a standard visuomotor policy into a collaborative assistant. CollabVLA tackles key limitations of prior VLAs, including domain overfitting, non-interpretable reasoning, and the high latency of auxiliary generative models, by integrating VLM-based reflective reasoning with diffusion-based action generation under a mixture-of-experts design. Through a two-stage training recipe of action grounding and reflection tuning, it supports explicit self-reflection and proactively solicits human guidance when confronted with uncertainty or repeated failure. It cuts normalized Time by ~2x and Dream counts by ~4x vs. generative agents, achieving higher success rates, improved interpretability, and balanced low latency compared with existing methods. This work takes a pioneering step toward shifting VLAs from opaque controllers to genuinely assistive agents capable of reasoning, acting, and collaborating with humans.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Reinforcement Learning, Genetic Algorithms and Transformers for background determination in particle physics</title>
<link>https://arxiv.org/abs/2509.14894</link>
<guid>https://arxiv.org/abs/2509.14894</guid>
<content:encoded><![CDATA[
arXiv:2509.14894v1 Announce Type: new 
Abstract: Experimental studies of beauty hadron decays face significant challenges due to a wide range of backgrounds arising from the numerous possible decay channels with similar final states. For a particular signal decay, the process for ascertaining the most relevant background processes necessitates a detailed analysis of final state particles, potential misidentifications, and kinematic overlaps, which, due to computational limitations, is restricted to the simulation of only the most relevant backgrounds. Moreover, this process typically relies on the physicist's intuition and expertise, as no systematic method exists.
  This paper has two primary goals. First, from a particle physics perspective, we present a novel approach that utilises Reinforcement Learning (RL) to overcome the aforementioned challenges by systematically determining the critical backgrounds affecting beauty hadron decay measurements. While beauty hadron physics serves as the case study in this work, the proposed strategy is broadly adaptable to other types of particle physics measurements. Second, from a Machine Learning perspective, we introduce a novel algorithm which exploits the synergy between RL and Genetic Algorithms (GAs) for environments with highly sparse rewards and a large trajectory space. This strategy leverages GAs to efficiently explore the trajectory space and identify successful trajectories, which are used to guide the RL agent's training. Our method also incorporates a transformer architecture for the RL agent to handle token sequences representing decays.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2509.14956</link>
<guid>https://arxiv.org/abs/2509.14956</guid>
<content:encoded><![CDATA[
arXiv:2509.14956v1 Announce Type: new 
Abstract: This paper proposes a novel architectural framework aimed at enhancing security and reliability in multi-agent systems (MAS). A central component of this framework is a network of Sentinel Agents, functioning as a distributed security layer that integrates techniques such as semantic analysis via large language models (LLMs), behavioral analytics, retrieval-augmented verification, and cross-agent anomaly detection. Such agents can potentially oversee inter-agent communications, identify potential threats, enforce privacy and access controls, and maintain comprehensive audit records. Complementary to the idea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator Agent supervises policy implementation, and manages agent participation. In addition, the Coordinator also ingests alerts from Sentinel Agents. Based on these alerts, it can adapt policies, isolate or quarantine misbehaving agents, and contain threats to maintain the integrity of the MAS ecosystem. This dual-layered security approach, combining the continuous monitoring of Sentinel Agents with the governance functions of Coordinator Agents, supports dynamic and adaptive defense mechanisms against a range of threats, including prompt injection, collusive agent behavior, hallucinations generated by LLMs, privacy breaches, and coordinated multi-agent attacks. In addition to the architectural design, we present a simulation study where 162 synthetic attacks of different families (prompt injection, hallucination, and data exfiltration) were injected into a multi-agent conversational environment. The Sentinel Agents successfully detected the attack attempts, confirming the practical feasibility of the proposed monitoring approach. The framework also offers enhanced system observability, supports regulatory compliance, and enables policy evolution over time.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making</title>
<link>https://arxiv.org/abs/2509.14998</link>
<guid>https://arxiv.org/abs/2509.14998</guid>
<content:encoded><![CDATA[
arXiv:2509.14998v1 Announce Type: new 
Abstract: Medical decision-making often involves integrating knowledge from multiple clinical specialties, typically achieved through multidisciplinary teams. Inspired by this collaborative process, recent work has leveraged large language models (LLMs) in multi-agent collaboration frameworks to emulate expert teamwork. While these approaches improve reasoning through agent interaction, they are limited by static, pre-assigned roles, which hinder adaptability and dynamic knowledge integration. To address these limitations, we propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration framework that enables LLM agents to dynamically form and expand expert teams based on the evolving diagnostic context. KAMAC begins with one or more expert agents and then conducts a knowledge-driven discussion to identify and fill knowledge gaps by recruiting additional specialists as needed. This supports flexible, scalable collaboration in complex clinical scenarios, with decisions finalized through reviewing updated agent comments. Experiments on two real-world medical benchmarks demonstrate that KAMAC significantly outperforms both single-agent and advanced multi-agent methods, particularly in complex clinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty expertise. Our code is publicly available at: https://github.com/XiaoXiao-Woo/KAMAC.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample Efficient Experience Replay in Non-stationary Environments</title>
<link>https://arxiv.org/abs/2509.15032</link>
<guid>https://arxiv.org/abs/2509.15032</guid>
<content:encoded><![CDATA[
arXiv:2509.15032v1 Announce Type: new 
Abstract: Reinforcement learning (RL) in non-stationary environments is challenging, as changing dynamics and rewards quickly make past experiences outdated. Traditional experience replay (ER) methods, especially those using TD-error prioritization, struggle to distinguish between changes caused by the agent's policy and those from the environment, resulting in inefficient learning under dynamic conditions. To address this challenge, we propose the Discrepancy of Environment Dynamics (DoE), a metric that isolates the effects of environment shifts on value functions. Building on this, we introduce Discrepancy of Environment Prioritized Experience Replay (DEER), an adaptive ER framework that prioritizes transitions based on both policy updates and environmental changes. DEER uses a binary classifier to detect environment changes and applies distinct prioritization strategies before and after each shift, enabling more sample-efficient learning. Experiments on four non-stationary benchmarks demonstrate that DEER further improves the performance of off-policy algorithms by 11.54 percent compared to the best-performing state-of-the-art ER methods.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning Agent for a 2D Shooter Game</title>
<link>https://arxiv.org/abs/2509.15042</link>
<guid>https://arxiv.org/abs/2509.15042</guid>
<content:encoded><![CDATA[
arXiv:2509.15042v1 Announce Type: new 
Abstract: Reinforcement learning agents in complex game environments often suffer from sparse rewards, training instability, and poor sample efficiency. This paper presents a hybrid training approach that combines offline imitation learning with online reinforcement learning for a 2D shooter game agent. We implement a multi-head neural network with separate outputs for behavioral cloning and Q-learning, unified by shared feature extraction layers with attention mechanisms. Initial experiments using pure deep Q-Networks exhibited significant instability, with agents frequently reverting to poor policies despite occasional good performance. To address this, we developed a hybrid methodology that begins with behavioral cloning on demonstration data from rule-based agents, then transitions to reinforcement learning. Our hybrid approach achieves consistently above 70% win rate against rule-based opponents, substantially outperforming pure reinforcement learning methods which showed high variance and frequent performance degradation. The multi-head architecture enables effective knowledge transfer between learning modes while maintaining training stability. Results demonstrate that combining demonstration-based initialization with reinforcement learning optimization provides a robust solution for developing game AI agents in complex multi-agent environments where pure exploration proves insufficient.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue</title>
<link>https://arxiv.org/abs/2509.15061</link>
<guid>https://arxiv.org/abs/2509.15061</guid>
<content:encoded><![CDATA[
arXiv:2509.15061v1 Announce Type: new 
Abstract: The ultimate goal of embodied agents is to create collaborators that can interact with humans, not mere executors that passively follow instructions. This requires agents to communicate, coordinate, and adapt their actions based on human feedback. Recently, advances in VLAs have offered a path toward this goal. However, most current VLA-based embodied agents operate in a one-way mode: they receive an instruction and execute it without feedback. This approach fails in real-world scenarios where instructions are often ambiguous. In this paper, we address this problem with the Ask-to-Clarify framework. Our framework first resolves ambiguous instructions by asking questions in a multi-turn dialogue. Then it generates low-level actions end-to-end. Specifically, the Ask-to-Clarify framework consists of two components, one VLM for collaboration and one diffusion for action. We also introduce a connection module that generates conditions for the diffusion based on the output of the VLM. This module adjusts the observation by instructions to create reliable conditions. We train our framework with a two-stage knowledge-insulation strategy. First, we fine-tune the collaboration component using ambiguity-solving dialogue data to handle ambiguity. Then, we integrate the action component while freezing the collaboration one. This preserves the interaction abilities while fine-tuning the diffusion to generate actions. The training strategy guarantees our framework can first ask questions, then generate actions. During inference, a signal detector functions as a router that helps our framework switch between asking questions and taking actions. We evaluate the Ask-to-Clarify framework in 8 real-world tasks, where it outperforms existing state-of-the-art VLAs. The results suggest that our proposed framework, along with the training strategy, provides a path toward collaborative embodied agents.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constrained Feedback Learning for Non-Stationary Multi-Armed Bandits</title>
<link>https://arxiv.org/abs/2509.15073</link>
<guid>https://arxiv.org/abs/2509.15073</guid>
<content:encoded><![CDATA[
arXiv:2509.15073v1 Announce Type: new 
Abstract: Non-stationary multi-armed bandits enable agents to adapt to changing environments by incorporating mechanisms to detect and respond to shifts in reward distributions, making them well-suited for dynamic settings. However, existing approaches typically assume that reward feedback is available at every round - an assumption that overlooks many real-world scenarios where feedback is limited. In this paper, we take a significant step forward by introducing a new model of constrained feedback in non-stationary multi-armed bandits, where the availability of reward feedback is restricted. We propose the first prior-free algorithm - that is, one that does not require prior knowledge of the degree of non-stationarity - that achieves near-optimal dynamic regret in this setting. Specifically, our algorithm attains a dynamic regret of $\tilde{\mathcal{O}}({K^{1/3} V_T^{1/3} T }/{ B^{1/3}})$, where $T$ is the number of rounds, $K$ is the number of arms, $B$ is the query budget, and $V_T$ is the variation budget capturing the degree of non-stationarity.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.15076</link>
<guid>https://arxiv.org/abs/2509.15076</guid>
<content:encoded><![CDATA[
arXiv:2509.15076v1 Announce Type: new 
Abstract: Air pollution remains a critical threat to public health and environmental sustainability, yet conventional monitoring systems are often constrained by limited spatial coverage and accessibility. This paper proposes an AI-driven agent that predicts ambient air pollution levels from sky images and synthesizes realistic visualizations of pollution scenarios using generative modeling. Our approach combines statistical texture analysis with supervised learning for pollution classification, and leverages vision-language model (VLM)-guided image generation to produce interpretable representations of air quality conditions. The generated visuals simulate varying degrees of pollution, offering a foundation for user-facing interfaces that improve transparency and support informed environmental decision-making. These outputs can be seamlessly integrated into intelligent applications aimed at enhancing situational awareness and encouraging behavioral responses based on real-time forecasts. We validate our method using a dataset of urban sky images and demonstrate its effectiveness in both pollution level estimation and semantically consistent visual synthesis. The system design further incorporates human-centered user experience principles to ensure accessibility, clarity, and public engagement in air quality forecasting. To support scalable and energy-efficient deployment, future iterations will incorporate a green CNN architecture enhanced with FPGA-based incremental learning, enabling real-time inference on edge platforms.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Alignment via Competition</title>
<link>https://arxiv.org/abs/2509.15090</link>
<guid>https://arxiv.org/abs/2509.15090</guid>
<content:encoded><![CDATA[
arXiv:2509.15090v1 Announce Type: new 
Abstract: Aligning AI systems with human values remains a fundamental challenge, but does our inability to create perfectly aligned models preclude obtaining the benefits of alignment? We study a strategic setting where a human user interacts with multiple differently misaligned AI agents, none of which are individually well-aligned. Our key insight is that when the users utility lies approximately within the convex hull of the agents utilities, a condition that becomes easier to satisfy as model diversity increases, strategic competition can yield outcomes comparable to interacting with a perfectly aligned model. We model this as a multi-leader Stackelberg game, extending Bayesian persuasion to multi-round conversations between differently informed parties, and prove three results: (1) when perfect alignment would allow the user to learn her Bayes-optimal action, she can also do so in all equilibria under the convex hull condition (2) under weaker assumptions requiring only approximate utility learning, a non-strategic user employing quantal response achieves near-optimal utility in all equilibria and (3) when the user selects the best single AI after an evaluation period, equilibrium guarantees remain near-optimal without further distributional assumptions. We complement the theory with two sets of experiments.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Digital Twin-based Cooperative Autonomous Driving in Smart Intersections: A Multi-Agent Reinforcement Learning Approach</title>
<link>https://arxiv.org/abs/2509.15099</link>
<guid>https://arxiv.org/abs/2509.15099</guid>
<content:encoded><![CDATA[
arXiv:2509.15099v1 Announce Type: new 
Abstract: Unsignalized intersections pose safety and efficiency challenges due to complex traffic flows and blind spots. In this paper, a digital twin (DT)-based cooperative driving system with roadside unit (RSU)-centric architecture is proposed for enhancing safety and efficiency at unsignalized intersections. The system leverages comprehensive bird-eye-view (BEV) perception to eliminate blind spots and employs a hybrid reinforcement learning (RL) framework combining offline pre-training with online fine-tuning. Specifically, driving policies are initially trained using conservative Q-learning (CQL) with behavior cloning (BC) on real datasets, then fine-tuned using multi-agent proximal policy optimization (MAPPO) with self-attention mechanisms to handle dynamic multi-agent coordination. The RSU implements real-time commands via vehicle-to-infrastructure (V2I) communications. Experimental results show that the proposed method yields failure rates below 0.03\% coordinating up to three connected autonomous vehicles (CAVs), significantly outperforming traditional methods. In addition, the system exhibits sub-linear computational scaling with inference times under 40 ms. Furthermore, it demonstrates robust generalization across diverse unsignalized intersection scenarios, indicating its practicality and readiness for real-world deployment.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vulnerable Agent Identification in Large-Scale Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.15103</link>
<guid>https://arxiv.org/abs/2509.15103</guid>
<content:encoded><![CDATA[
arXiv:2509.15103v1 Announce Type: new 
Abstract: Partial agent failure becomes inevitable when systems scale up, making it crucial to identify the subset of agents whose compromise would most severely degrade overall performance. In this paper, we study this Vulnerable Agent Identification (VAI) problem in large-scale multi-agent reinforcement learning (MARL). We frame VAI as a Hierarchical Adversarial Decentralized Mean Field Control (HAD-MFC), where the upper level involves an NP-hard combinatorial task of selecting the most vulnerable agents, and the lower level learns worst-case adversarial policies for these agents using mean-field MARL. The two problems are coupled together, making HAD-MFC difficult to solve. To solve this, we first decouple the hierarchical process by Fenchel-Rockafellar transform, resulting a regularized mean-field Bellman operator for upper level that enables independent learning at each level, thus reducing computational complexity. We then reformulate the upper-level combinatorial problem as a MDP with dense rewards from our regularized mean-field Bellman operator, enabling us to sequentially identify the most vulnerable agents by greedy and RL algorithms. This decomposition provably preserves the optimal solution of the original HAD-MFC. Experiments show our method effectively identifies more vulnerable agents in large-scale MARL and the rule-based system, fooling system into worse failures, and learns a value function that reveals the vulnerability of each agent.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonlinear Cooperative Salvo Guidance with Seeker-Limited Interceptors</title>
<link>https://arxiv.org/abs/2509.15136</link>
<guid>https://arxiv.org/abs/2509.15136</guid>
<content:encoded><![CDATA[
arXiv:2509.15136v1 Announce Type: new 
Abstract: This paper presents a cooperative guidance strategy for the simultaneous interception of a constant-velocity, non-maneuvering target, addressing the realistic scenario where only a subset of interceptors are equipped with onboard seekers. To overcome the resulting heterogeneity in target observability, a fixed-time distributed observer is employed, enabling seeker-less interceptors to estimate the target state using information from seeker-equipped agents and local neighbors over a directed communication topology. Departing from conventional strategies that approximate time-to-go via linearization or small-angle assumptions, the proposed approach leverages deviated pursuit guidance where the time-to-go expression is exact for such a target. Moreover, a higher-order sliding mode consensus protocol is utilized to establish time-to-go consensus within a finite time. The effectiveness of the proposed guidance and estimation architecture is demonstrated through simulations.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Evaluation-Centric Paradigm for Scientific Visualization Agents</title>
<link>https://arxiv.org/abs/2509.15160</link>
<guid>https://arxiv.org/abs/2509.15160</guid>
<content:encoded><![CDATA[
arXiv:2509.15160v1 Announce Type: new 
Abstract: Recent advances in multi-modal large language models (MLLMs) have enabled increasingly sophisticated autonomous visualization agents capable of translating user intentions into data visualizations. However, measuring progress and comparing different agents remains challenging, particularly in scientific visualization (SciVis), due to the absence of comprehensive, large-scale benchmarks for evaluating real-world capabilities. This position paper examines the various types of evaluation required for SciVis agents, outlines the associated challenges, provides a simple proof-of-concept evaluation example, and discusses how evaluation benchmarks can facilitate agent self-improvement. We advocate for a broader collaboration to develop a SciVis agentic evaluation benchmark that would not only assess existing capabilities but also drive innovation and stimulate future development in the field.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Internalizing Self-Consistency in Language Models: Multi-Agent Consensus Alignment</title>
<link>https://arxiv.org/abs/2509.15172</link>
<guid>https://arxiv.org/abs/2509.15172</guid>
<content:encoded><![CDATA[
arXiv:2509.15172v1 Announce Type: new 
Abstract: Language Models (LMs) are inconsistent reasoners, often generating contradictory responses to identical prompts. While inference-time methods can mitigate these inconsistencies, they fail to address the core problem: LMs struggle to reliably select reasoning pathways leading to consistent outcomes under exploratory sampling. To address this, we formalize self-consistency as an intrinsic property of well-aligned reasoning models and introduce Multi-Agent Consensus Alignment (MACA), a reinforcement learning framework that post-trains models to favor reasoning trajectories aligned with their internal consensus using majority/minority outcomes from multi-agent debate. These trajectories emerge from deliberative exchanges where agents ground reasoning in peer arguments, not just aggregation of independent attempts, creating richer consensus signals than single-round majority voting. MACA enables agents to teach themselves to be more decisive and concise, and better leverage peer insights in multi-agent settings without external supervision, driving substantial improvements across self-consistency (+27.6% on GSM8K), single-agent reasoning (+23.7% on MATH), sampling-based inference (+22.4% Pass@20 on MATH), and multi-agent ensemble decision-making (+42.7% on MathQA). These findings, coupled with strong generalization to unseen benchmarks (+16.3% on GPQA, +11.6% on CommonsenseQA), demonstrate robust self-alignment that more reliably unlocks latent reasoning potential of language models.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out-of-Sight Trajectories: Tracking, Fusion, and Prediction</title>
<link>https://arxiv.org/abs/2509.15219</link>
<guid>https://arxiv.org/abs/2509.15219</guid>
<content:encoded><![CDATA[
arXiv:2509.15219v1 Announce Type: new 
Abstract: Trajectory prediction is a critical task in computer vision and autonomous systems, playing a key role in autonomous driving, robotics, surveillance, and virtual reality. Existing methods often rely on complete and noise-free observational data, overlooking the challenges associated with out-of-sight objects and the inherent noise in sensor data caused by limited camera coverage, obstructions, and the absence of ground truth for denoised trajectories. These limitations pose safety risks and hinder reliable prediction in real-world scenarios. In this extended work, we present advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the noise-free visual trajectories of out-of-sight objects using noisy sensor data. Building on our previous research, we broaden the scope of Out-of-Sight Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending its applicability to autonomous driving, robotics, surveillance, and virtual reality. Our enhanced Vision-Positioning Denoising Module leverages camera calibration to establish a vision-positioning mapping, addressing the lack of visual references, while effectively denoising noisy sensor data in an unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB datasets, our approach achieves state-of-the-art performance in both trajectory denoising and prediction, significantly surpassing previous baselines. Additionally, we introduce comparisons with traditional denoising methods, such as Kalman filtering, and adapt recent trajectory prediction models to our task, providing a comprehensive benchmark. This work represents the first initiative to integrate vision-positioning projection for denoising noisy sensor trajectories of out-of-sight agents, paving the way for future advances. The code and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data</title>
<link>https://arxiv.org/abs/2509.15221</link>
<guid>https://arxiv.org/abs/2509.15221</guid>
<content:encoded><![CDATA[
arXiv:2509.15221v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that operate GUIs autonomously, showing great potential, yet progress is limited by the lack of large-scale, open-source computer use data and foundation models. In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It offers a large-scale dataset spanning 6 operating systems and 3 task domains, built via a closed-loop pipeline uniting automated agents with human experts. Trained on this scaled-up data, ScaleCUA can operate seamlessly across platforms. Specifically, it delivers strong gains over baselines (+26.6 on WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on WebArena-Lite-v2). These findings underscore the power of data-driven scaling for general-purpose computer use agents. We will release data, models, and code to advance future research: https://github.com/OpenGVLab/ScaleCUA.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention</title>
<link>https://arxiv.org/abs/2506.11445</link>
<guid>https://arxiv.org/abs/2506.11445</guid>
<content:encoded><![CDATA[
arXiv:2506.11445v1 Announce Type: cross 
Abstract: In mixed-traffic environments, autonomous vehicles must adapt to human-controlled vehicles and other unusual driving situations. This setting can be framed as a multi-agent reinforcement learning (MARL) environment with full cooperative reward among the autonomous vehicles. While methods such as Multi-agent Proximal Policy Optimization can be effective in training MARL tasks, they often fail to resolve local conflict between agents and are unable to generalize to stochastic events. In this paper, we propose a Local State Attention module to assist the input state representation. By relying on the self-attention operator, the module is expected to compress the essential information of nearby agents to resolve the conflict in traffic situations. Utilizing a simulated highway merging scenario with the priority vehicle as the unexpected event, our approach is able to prioritize other vehicles' information to manage the merging process. The results demonstrate significant improvements in merging efficiency compared to popular baselines, especially in high-density traffic settings.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Traffic Co-Simulation Framework Empowered by Infrastructure Camera Sensing and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2412.03925</link>
<guid>https://arxiv.org/abs/2412.03925</guid>
<content:encoded><![CDATA[
arXiv:2412.03925v2 Announce Type: replace 
Abstract: Traffic simulations are commonly used to optimize urban traffic flow, with reinforcement learning (RL) showing promising potential for automated traffic signal control, particularly in intelligent transportation systems involving connected automated vehicles. Multi-agent reinforcement learning (MARL) is particularly effective for learning control strategies for traffic lights in a network using iterative simulations. However, existing methods often assume perfect vehicle detection, which overlooks real-world limitations related to infrastructure availability and sensor reliability. This study proposes a co-simulation framework integrating CARLA and SUMO, which combines high-fidelity 3D modeling with large-scale traffic flow simulation. Cameras mounted on traffic light poles within the CARLA environment use a YOLO-based computer vision system to detect and count vehicles, providing real-time traffic data as input for adaptive signal control in SUMO. MARL agents trained with four different reward structures leverage this visual feedback to optimize signal timings and improve network-wide traffic flow. Experiments in a multi-intersection test-bed demonstrate the effectiveness of the proposed MARL approach in enhancing traffic conditions using real-time camera based detection. The framework also evaluates the robustness of MARL under faulty or sparse sensing and compares the performance of YOLOv5 and YOLOv8 for vehicle detection. Results show that while better accuracy improves performance, MARL agents can still achieve significant improvements with imperfect detection, demonstrating scalability and adaptability for real-world scenarios.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRSet: Private Non-Interactive Verifiable Credential Revocation</title>
<link>https://arxiv.org/abs/2501.17089</link>
<guid>https://arxiv.org/abs/2501.17089</guid>
<content:encoded><![CDATA[
arXiv:2501.17089v3 Announce Type: replace 
Abstract: Like any digital certificate, Verifiable Credentials (VCs) require a way to revoke them in case of an error or key compromise. Existing solutions for VC revocation, most prominently Bitstring Status List, are not viable for many use cases because they may leak the issuer's activity, which in turn leaks internal business metrics. For instance, staff fluctuation through the revocation of employee IDs. We identify the protection of issuer activity as a key gap and propose a formal definition for a corresponding characteristic of a revocation mechanism. Then, we introduce CRSet, a non-interactive mechanism that trades some space efficiency to reach these privacy characteristics. For that, we provide a proof sketch. Issuers periodically encode revocation data and publish it via Ethereum blob-carrying transactions, ensuring secure and private availability. Relying Parties (RPs) can download it to perform revocation checks locally. Sticking to a non-interactive design also makes adoption easier because it requires no changes to wallet agents and exchange protocols. We also implement and empirically evaluate CRSet, finding its real-world behavior to match expectations. One Ethereum blob fits revocation data for about 170k VCs.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>METAL: A Multi-Agent Framework for Chart Generation with Test-Time Scaling</title>
<link>https://arxiv.org/abs/2502.17651</link>
<guid>https://arxiv.org/abs/2502.17651</guid>
<content:encoded><![CDATA[
arXiv:2502.17651v4 Announce Type: replace 
Abstract: Chart generation aims to generate code to produce charts satisfying the desired visual properties, e.g., texts, layout, color, and type. It has great potential to empower the automatic professional report generation in financial analysis, research presentation, education, and healthcare. In this work, we build a vision-language model (VLM) based multi-agent framework for effective automatic chart generation. Generating high-quality charts requires both strong visual design skills and precise coding capabilities that embed the desired visual properties into code. Such a complex multi-modal reasoning process is difficult for direct prompting of VLMs. To resolve these challenges, we propose METAL, a multi-agent framework that decomposes the task of chart generation into the iterative collaboration among specialized agents. METAL achieves 5.2% improvement over the current best result in the chart generation task. The METAL framework exhibits the phenomenon of test-time scaling: its performance increases monotonically as the logarithmic computational budget grows from 512 to 8192 tokens. In addition, we find that separating different modalities during the critique process of METAL boosts the self-correction capability of VLMs in the multimodal context.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Single- vs. Dual-Prompt Dialogue Generation with LLMs for Job Interviews in Human Resources</title>
<link>https://arxiv.org/abs/2502.18650</link>
<guid>https://arxiv.org/abs/2502.18650</guid>
<content:encoded><![CDATA[
arXiv:2502.18650v2 Announce Type: replace 
Abstract: Optimizing language models for use in conversational agents requires large quantities of example dialogues. Increasingly, these dialogues are synthetically generated by using powerful large language models (LLMs), especially in domains where obtaining authentic human data is challenging. One such domain is human resources (HR). In this context, we compare two LLM-based dialogue generation methods for producing HR job interviews, and assess which method generates higher-quality dialogues, i.e., those more difficult to distinguish from genuine human discourse. The first method uses a single prompt to generate the complete interview dialogue. The second method uses two agents that converse with each other. To evaluate dialogue quality under each method, we ask a judge LLM to determine whether AI was used for interview generation, using pairwise interview comparisons. We empirically find that, at the expense of a sixfold increase in token count, interviews generated with the dual-prompt method achieve a win rate 2 to 10 times higher than those generated with the single-prompt method. This difference remains consistent regardless of whether GPT-4o or Llama 3.3 70B is used for either interview generation or quality judging.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation and Facilitation of Online Discussions in the LLM Era: A Survey</title>
<link>https://arxiv.org/abs/2503.01513</link>
<guid>https://arxiv.org/abs/2503.01513</guid>
<content:encoded><![CDATA[
arXiv:2503.01513v3 Announce Type: replace 
Abstract: We present a survey of methods for assessing and enhancing the quality of online discussions, focusing on the potential of LLMs. While online discourses aim, at least in theory, to foster mutual understanding, they often devolve into harmful exchanges, such as hate speech, threatening social cohesion and democratic values. Recent advancements in LLMs enable artificial facilitation agents to not only moderate content, but also actively improve the quality of interactions. Our survey synthesizes ideas from NLP and Social Sciences to provide (a) a new taxonomy on discussion quality evaluation, (b) an overview of intervention and facilitation strategies, (c) along with a new taxonomy of conversation facilitation datasets, (d) an LLM-oriented roadmap of good practices and future research directions, from technological and societal perspectives.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handle Object Navigation as Weighted Traveling Repairman Problem</title>
<link>https://arxiv.org/abs/2503.06937</link>
<guid>https://arxiv.org/abs/2503.06937</guid>
<content:encoded><![CDATA[
arXiv:2503.06937v2 Announce Type: replace 
Abstract: Zero-Shot Object Navigation (ZSON) requires agents to navigate to objects specified via open-ended natural language without predefined categories or prior environmental knowledge. While recent methods leverage foundation models or multi-modal maps, they often rely on 2D representations and greedy strategies or require additional training or modules with high computation load, limiting performance in complex environments and real applications. We propose WTRP-Searcher, a novel framework that formulates ZSON as a Weighted Traveling Repairman Problem (WTRP), minimizing the weighted waiting time of viewpoints. Using a Vision-Language Model (VLM), we score viewpoints based on object-description similarity, projected onto a 2D map with depth information. An open-vocabulary detector identifies targets, dynamically updating goals, while a 3D embedding feature map enhances spatial awareness and environmental recall. WTRP-Searcher outperforms existing methods, offering efficient global planning and improved performance in complex ZSON tasks. Code and design will be open-sourced upon acceptance.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-tuning Vision Language Models with Graph-based Knowledge for Explainable Medical Image Analysis</title>
<link>https://arxiv.org/abs/2503.09808</link>
<guid>https://arxiv.org/abs/2503.09808</guid>
<content:encoded><![CDATA[
arXiv:2503.09808v2 Announce Type: replace 
Abstract: Accurate staging of Diabetic Retinopathy (DR) is essential for guiding timely interventions and preventing vision loss. However, current staging models are hardly interpretable, and most public datasets contain no clinical reasoning or interpretation beyond image-level labels. In this paper, we present a novel method that integrates graph representation learning with vision-language models (VLMs) to deliver explainable DR diagnosis. Our approach leverages optical coherence tomography angiography (OCTA) images by constructing biologically informed graphs that encode key retinal vascular features such as vessel morphology and spatial connectivity. A graph neural network (GNN) then performs DR staging while integrated gradients highlight critical nodes and edges and their individual features that drive the classification decisions. We collect this graph-based knowledge which attributes the model's prediction to physiological structures and their characteristics. We then transform it into textual descriptions for VLMs. We perform instruction-tuning with these textual descriptions and the corresponding image to train a student VLM. This final agent can classify the disease and explain its decision in a human interpretable way solely based on a single image input. Experimental evaluations on both proprietary and public datasets demonstrate that our method not only improves classification accuracy but also offers more clinically interpretable results. An expert study further demonstrates that our method provides more accurate diagnostic explanations and paves the way for precise localization of pathologies in OCTA images.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Agents Trained For Avoidance Behave Like Hawks And Doves</title>
<link>https://arxiv.org/abs/2503.11452</link>
<guid>https://arxiv.org/abs/2503.11452</guid>
<content:encoded><![CDATA[
arXiv:2503.11452v2 Announce Type: replace 
Abstract: We present heuristically optimal strategies expressed by deep learning agents playing a simple avoidance game. We analyse the learning and behaviour of two agents within a symmetrical grid world that must cross paths to reach a target destination without crashing into each other or straying off of the grid world in the wrong direction. The agent policy is determined by one neural network that is employed in both agents. Our findings indicate that the fully trained network exhibits behaviour similar to that of the game Hawks and Doves, in that one agent employs an aggressive strategy to reach the target while the other learns how to avoid the aggressive agent.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Multi-Agent Specialization via Task Parallelizability</title>
<link>https://arxiv.org/abs/2503.15703</link>
<guid>https://arxiv.org/abs/2503.15703</guid>
<content:encoded><![CDATA[
arXiv:2503.15703v2 Announce Type: replace 
Abstract: When should we encourage specialization in multi-agent systems versus train generalists that perform the entire task independently? We propose that specialization largely depends on task parallelizability: the potential for multiple agents to execute task components concurrently. Drawing inspiration from Amdahl's Law in distributed systems, we present a closed-form bound that predicts when specialization improves performance, depending only on task concurrency and team size. We validate our model on two standard MARL benchmarks that represent opposite regimes -- StarCraft Multi-Agent Challenge (SMAC, unlimited concurrency) and Multi-Particle Environment (MPE, unit-capacity bottlenecks) -- and observe close alignment between the bound at each extreme and an empirical measure of specialization. Three follow-up experiments in Overcooked-AI demonstrate that the model works in environments with more complex spatial and resource bottlenecks that allow for a range of strategies. Beyond prediction, the bound also serves as a diagnostic tool, highlighting biases in MARL training algorithms that cause sub-optimal convergence to specialist strategies with larger state spaces.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for Reward Shaping</title>
<link>https://arxiv.org/abs/2503.22723</link>
<guid>https://arxiv.org/abs/2503.22723</guid>
<content:encoded><![CDATA[
arXiv:2503.22723v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) often struggles with reward misalignment, where agents optimize given rewards but fail to exhibit the desired behaviors. This arises when the reward function incentivizes proxy behaviors misaligned with the true objective. While human-in-the-loop (HITL) methods can mitigate this issue, they also introduce biases, leading to inconsistent and subjective feedback that complicates learning. To address these challenges, we propose two key contributions. First, we extend the use of zero-shot, off-the-shelf large language models (LLMs) for reward shaping beyond natural language processing (NLP) to continuous control tasks. Using LLMs as direct feedback providers eliminates the need for surrogate models trained on human feedback, which often inherit biases from training data. Second, we introduce a hybrid framework (LLM-HFBF) that enables LLMs to identify and correct biases in human feedback while incorporating this feedback into the reward shaping process. The LLM-HFBF framework creates a more balanced and reliable system by addressing both the limitations of LLMs (e.g., lack of domain-specific knowledge) and human supervision (e.g., inherent biases). By enabling human feedback bias flagging and correction, our approach improves reinforcement learning performance and reduces reliance on potentially biased human feedback. Empirical experiments show that biased human feedback significantly reduces performance, with Average Episodic Reward dropping by nearly 94% compared to unbiased approaches. In contrast, LLM-based methods sustain performance at a similar level to unbiased feedback, even in challenging edge-case scenarios.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.04317</link>
<guid>https://arxiv.org/abs/2505.04317</guid>
<content:encoded><![CDATA[
arXiv:2505.04317v4 Announce Type: replace 
Abstract: In this paper, we tackle the problem of learning to play 3v3 multi-drone volleyball, a new embodied competitive task that requires both high-level strategic coordination and low-level agile control. The task is turn-based, multi-agent, and physically grounded, posing significant challenges due to its long-horizon dependencies, tight inter-agent coupling, and the underactuated dynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play (HCSP), a hierarchical reinforcement learning framework that separates centralized high-level strategic decision-making from decentralized low-level motion control. We design a three-stage population-based training pipeline to enable both strategy and skill to emerge from scratch without expert demonstrations: (I) training diverse low-level skills, (II) learning high-level strategy via self-play with fixed low-level skills, and (III) joint fine-tuning through co-self-play. Experiments show that HCSP achieves superior performance, outperforming non-hierarchical self-play and rule-based hierarchical baselines with an average 82.9% win rate and a 71.5% win rate against the two-stage variant. Moreover, co-self-play leads to emergent team behaviors such as role switching and coordinated formations, demonstrating the effectiveness of our hierarchical design and training scheme. The project page is at https://sites.google.com/view/hi-co-self-play.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trustless Autonomy: Understanding Motivations, Benefits, and Governance Dilemmas in Self-Sovereign Decentralized AI Agents</title>
<link>https://arxiv.org/abs/2505.09757</link>
<guid>https://arxiv.org/abs/2505.09757</guid>
<content:encoded><![CDATA[
arXiv:2505.09757v2 Announce Type: replace 
Abstract: The recent trend of self-sovereign Decentralized AI Agents (DeAgents) combines Large Language Model (LLM)-based AI agents with decentralization technologies such as blockchain smart contracts and trusted execution environments (TEEs). These tamper-resistant trustless substrates allow agents to achieve self-sovereignty through ownership of cryptowallet private keys and control of digital assets and social media accounts. DeAgents eliminate centralized control and reduce human intervention, addressing key trust concerns inherent in centralized AI systems. This contributes to social computing by enabling new human cooperative paradigm "intelligence as commons." However, given ongoing challenges in LLM reliability such as hallucinations, this creates paradoxical tension between trustlessness and unreliable autonomy. This study addresses this empirical research gap through interviews with DeAgents stakeholders-experts, founders, and developers-to examine their motivations, benefits, and governance dilemmas. The findings will guide future DeAgents system and protocol design and inform discussions about governance in sociotechnical AI systems in the future agentic web.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Judging with Many Minds: Do More Perspectives Mean Less Prejudice? On Bias Amplifications and Resistance in Multi-Agent Based LLM-as-Judge</title>
<link>https://arxiv.org/abs/2505.19477</link>
<guid>https://arxiv.org/abs/2505.19477</guid>
<content:encoded><![CDATA[
arXiv:2505.19477v3 Announce Type: replace 
Abstract: LLM-as-Judge has emerged as a scalable alternative to human evaluation, enabling large language models (LLMs) to provide reward signals in trainings. While recent work has explored multi-agent extensions such as multi-agent debate and meta-judging to enhance evaluation quality, the question of how intrinsic biases manifest in these settings remains underexplored. In this study, we conduct a systematic analysis of four diverse bias types: position bias, verbosity bias, chain-of-thought bias, and bandwagon bias. We evaluate these biases across two widely adopted multi-agent LLM-as-Judge frameworks: Multi-Agent-Debate and LLM-as-Meta-Judge. Our results show that debate framework amplifies biases sharply after the initial debate, and this increased bias is sustained in subsequent rounds, while meta-judge approaches exhibit greater resistance. We further investigate the incorporation of PINE, a leading single-agent debiasing method, as a bias-free agent within these systems. The results reveal that this bias-free agent effectively reduces biases in debate settings but provides less benefit in meta-judge scenarios. Our work provides a comprehensive study of bias behavior in multi-agent LLM-as-Judge systems and highlights the need for targeted bias mitigation strategies in collaborative evaluation settings.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback</title>
<link>https://arxiv.org/abs/2505.20013</link>
<guid>https://arxiv.org/abs/2505.20013</guid>
<content:encoded><![CDATA[
arXiv:2505.20013v2 Announce Type: replace 
Abstract: Web agents powered by Large Language Models (LLMs) show promise for next-generation AI, but their limited reasoning in uncertain, dynamic web environments hinders robust deployment. In this paper, we identify key reasoning skills essential for effective web agents, i.e., reflection & lookahead, branching, and rollback, and curate trajectory data that exemplifies these abilities by reconstructing the agent's (inference-time) reasoning algorithms into chain-of-thought rationales. We conduct experiments in the agent self-improving benchmark, OpenWebVoyager, and demonstrate that distilling salient reasoning patterns into the backbone LLM via simple fine-tuning can substantially enhance its performance. Our approach yields significant improvements across multiple benchmarks, including WebVoyager, Mind2web-live, and SimpleQA (web search), highlighting the potential of targeted reasoning skill enhancement for web agents.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Explainable AI Framework for Dynamic Resource Management in Vehicular Network Slicing</title>
<link>https://arxiv.org/abs/2506.11882</link>
<guid>https://arxiv.org/abs/2506.11882</guid>
<content:encoded><![CDATA[
arXiv:2506.11882v2 Announce Type: replace 
Abstract: Effective resource management and network slicing are essential to meet the diverse service demands of vehicular networks, including Enhanced Mobile Broadband (eMBB) and Ultra-Reliable and Low-Latency Communications (URLLC). This paper introduces an Explainable Deep Reinforcement Learning (XRL) framework for dynamic network slicing and resource allocation in vehicular networks, built upon a near-real-time RAN intelligent controller. By integrating a feature-based approach that leverages Shapley values and an attention mechanism, we interpret and refine the decisions of our reinforcementlearning agents, addressing key reliability challenges in vehicular communication systems. Simulation results demonstrate that our approach provides clear, real-time insights into the resource allocation process and achieves higher interpretability precision than a pure attention mechanism. Furthermore, the Quality of Service (QoS) satisfaction for URLLC services increased from 78.0% to 80.13%, while that for eMBB services improved from 71.44% to 73.21%.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Type-Dependent Liquid Welfare Guarantees for Autobidding Agents with Budgets</title>
<link>https://arxiv.org/abs/2506.20908</link>
<guid>https://arxiv.org/abs/2506.20908</guid>
<content:encoded><![CDATA[
arXiv:2506.20908v2 Announce Type: replace 
Abstract: Online advertising systems have recently transitioned to autobidding, allowing advertisers to delegate bidding decisions to automated agents. Each advertiser directs their agent to optimize an objective function subject to return-on-investment (ROI) and budget constraints. Given their practical relevance, this shift has spurred a surge of research on the liquid welfare price of anarchy (POA) of fundamental auction formats under autobidding, most notably simultaneous first-price auctions (FPA). One of the main challenges is to understand the efficiency of FPA in the presence of heterogeneous agent types. We introduce {type-dependent smoothness framework that enables a unified analysis of the POA in such complex autobidding environments. In our approach, we derive type-dependent smoothness parameters which we carefully balance to obtain POA bounds. This balancing gives rise to a POA-revealing mathematical program, which we use to determine tight bounds on the POA of coarse correlated equilibria (CCE). Our framework is versatile enough to handle heterogeneous agent types and extends to the general class of fractionally subadditive valuations. Additionally, we develop a novel reduction technique that transforms budget-constrained agents into budget-unconstrained ones. Combining this reduction technique with our smoothness framework enables us to derive tight bounds on the POA of CCE in the general hybrid agent model with both ROI and budget constraints. Among other results, our bounds uncover an intriguing threshold phenomenon showing that the POA depends intricately on the smallest and largest agent types. We also extend our study to FPAs with reserve prices, which can be interpreted as predictions of agents' values, to further improve efficiency guarantees.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning</title>
<link>https://arxiv.org/abs/2507.16815</link>
<guid>https://arxiv.org/abs/2507.16815</guid>
<content:encoded><![CDATA[
arXiv:2507.16815v2 Announce Type: replace 
Abstract: Vision-language-action (VLA) reasoning tasks require agents to interpret multimodal instructions, perform long-horizon planning, and act adaptively in dynamic environments. Existing approaches typically train VLA models in an end-to-end fashion, directly mapping inputs to actions without explicit reasoning, which hinders their ability to plan over multiple steps or adapt to complex task variations. In this paper, we propose ThinkAct, a dual-system framework that bridges high-level reasoning with low-level action execution via reinforced visual latent planning. ThinkAct trains a multimodal LLM to generate embodied reasoning plans guided by reinforcing action-aligned visual rewards based on goal completion and trajectory consistency. These reasoning plans are compressed into a visual plan latent that conditions a downstream action model for robust action execution on target environments. Extensive experiments on embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct enables few-shot adaptation, long-horizon planning, and self-correction behaviors in complex embodied AI tasks.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MovieCORE: COgnitive REasoning in Movies</title>
<link>https://arxiv.org/abs/2508.19026</link>
<guid>https://arxiv.org/abs/2508.19026</guid>
<content:encoded><![CDATA[
arXiv:2508.19026v3 Announce Type: replace 
Abstract: This paper introduces MovieCORE, a novel video question answering (VQA) dataset designed to probe deeper cognitive understanding of movie content. Unlike existing datasets that focus on surface-level comprehension, MovieCORE emphasizes questions that engage System-2 thinking while remaining specific to the video material. We present an innovative agentic brainstorming approach, utilizing multiple large language models (LLMs) as thought agents to generate and refine high-quality question-answer pairs. To evaluate dataset quality, we develop a set of cognitive tests assessing depth, thought-provocation potential, and syntactic complexity. We also propose a comprehensive evaluation scheme for assessing VQA model performance on deeper cognitive tasks. To address the limitations of existing video-language models (VLMs), we introduce an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves model reasoning capabilities post-training by up to 25%. Our work contributes to advancing movie understanding in AI systems and provides valuable insights into the capabilities and limitations of current VQA models when faced with more challenging, nuanced questions about cinematic content. Our project page, dataset and code can be found at https://joslefaure.github.io/assets/html/moviecore.html.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gap-Dependent Bounds for Federated $Q$-learning</title>
<link>https://arxiv.org/abs/2502.02859</link>
<guid>https://arxiv.org/abs/2502.02859</guid>
<content:encoded><![CDATA[
arXiv:2502.02859v2 Announce Type: replace-cross 
Abstract: We present the first gap-dependent analysis of regret and communication cost for on-policy federated $Q$-Learning in tabular episodic finite-horizon Markov decision processes (MDPs). Existing FRL methods focus on worst-case scenarios, leading to $\sqrt{T}$-type regret bounds and communication cost bounds with a $\log T$ term scaling with the number of agents $M$, states $S$, and actions $A$, where $T$ is the average total number of steps per agent. In contrast, our novel framework leverages the benign structures of MDPs, such as a strictly positive suboptimality gap, to achieve a $\log T$-type regret bound and a refined communication cost bound that disentangles exploration and exploitation. Our gap-dependent regret bound reveals a distinct multi-agent speedup pattern, and our gap-dependent communication cost bound removes the dependence on $MSA$ from the $\log T$ term. Notably, our gap-dependent communication cost bound also yields a better global switching cost when $M=1$, removing $SA$ from the $\log T$ term.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximizing UAV Cellular Connectivity with Reinforcement Learning for BVLoS Path Planning</title>
<link>https://arxiv.org/abs/2509.13336</link>
<guid>https://arxiv.org/abs/2509.13336</guid>
<content:encoded><![CDATA[
arXiv:2509.13336v1 Announce Type: new 
Abstract: This paper presents a reinforcement learning (RL) based approach for path planning of cellular connected unmanned aerial vehicles (UAVs) operating beyond visual line of sight (BVLoS). The objective is to minimize travel distance while maximizing the quality of cellular link connectivity by considering real world aerial coverage constraints and employing an empirical aerial channel model. The proposed solution employs RL techniques to train an agent, using the quality of communication links between the UAV and base stations (BSs) as the reward function. Simulation results demonstrate the effectiveness of the proposed method in training the agent and generating feasible UAV path plans. The proposed approach addresses the challenges due to limitations in UAV cellular communications, highlighting the need for investigations and considerations in this area. The RL algorithm efficiently identifies optimal paths, ensuring maximum connectivity with ground BSs to ensure safe and reliable BVLoS flight operation. Moreover, the solution can be deployed as an offline path planning module that can be integrated into future ground control systems (GCS) for UAV operations, enhancing their capabilities and safety. The method holds potential for complex long range UAV applications, advancing the technology in the field of cellular connected UAV path planning.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imagined Autocurricula</title>
<link>https://arxiv.org/abs/2509.13341</link>
<guid>https://arxiv.org/abs/2509.13341</guid>
<content:encoded><![CDATA[
arXiv:2509.13341v1 Announce Type: new 
Abstract: Training agents to act in embodied environments typically requires vast training data or access to accurate simulation, neither of which exists for many cases in the real world. Instead, world models are emerging as an alternative leveraging offline, passively collected data, they make it possible to generate diverse worlds for training agents in simulation. In this work, we harness world models to generate imagined environments to train robust agents capable of generalizing to novel task variations. One of the challenges in doing this is ensuring the agent trains on useful generated data. We thus propose a novel approach, IMAC (Imagined Autocurricula), leveraging Unsupervised Environment Design (UED), which induces an automatic curriculum over generated worlds. In a series of challenging, procedurally generated environments, we show it is possible to achieve strong transfer performance on held-out environments, having trained only inside a world model learned from a narrower dataset. We believe this opens the path to utilizing larger-scale, foundation world models for generally capable agents.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft</title>
<link>https://arxiv.org/abs/2509.13347</link>
<guid>https://arxiv.org/abs/2509.13347</guid>
<content:encoded><![CDATA[
arXiv:2509.13347v1 Announce Type: new 
Abstract: The choice of action spaces is a critical yet unresolved challenge in developing capable, end-to-end trainable agents. This paper first presents a large-scale, systematic comparison of prominent abstracted action spaces and tokenizers for Vision-Language-Action (VLA) or hierarchical agent models in the open-ended Minecraft. Our analysis reveals that no single action space is universally optimal; instead, the most effective abstraction is highly task-dependent, creating a dilemma for building generalist agents. To resolve this, we introduce Chain of Action (CoA), a novel framework that unifies high-level planning and low-level control within a single, monolithic VLA model. CoA treats an abstracted action not as a command for a separate policy, but as an intermediate reasoning step--akin to a chain of thought--that guides the generation of the final, executable action. Furthermore, we demonstrate that an All-in-One agent trained on a diverse mixture of action spaces using the CoA paradigm learns a more robust and generalizable policy. This unified agent achieves a new state-of-the-art, improving the overall task success rate over strong, specialized baselines. To foster reproducible research, we release the OpenHA (Open Hierarchical Agents) suite, which includes our comprehensive benchmark of over 800 distinct tasks, curated datasets, source code, and all pretrained model checkpoints at https://github.com/CraftJarvis/OpenHA
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive Reasoning</title>
<link>https://arxiv.org/abs/2509.13352</link>
<guid>https://arxiv.org/abs/2509.13352</guid>
<content:encoded><![CDATA[
arXiv:2509.13352v1 Announce Type: new 
Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly deployed in defense, surveillance, and disaster response, yet most systems remain confined to SAE Level 2--3 autonomy. Their reliance on rule-based control and narrow AI restricts adaptability in dynamic, uncertain missions. Existing UAV frameworks lack context-aware reasoning, autonomous decision-making, and ecosystem-level integration; critically, none leverage Large Language Model (LLM) agents with tool-calling for real-time knowledge access. This paper introduces the Agentic UAVs framework, a five-layer architecture (Perception, Reasoning, Action, Integration, Learning) that augments UAVs with LLM-driven reasoning, database querying, and third-party system interaction. A ROS2 and Gazebo-based prototype integrates YOLOv11 object detection with GPT-4 reasoning and local Gemma-3 deployment. In simulated search-and-rescue scenarios, agentic UAVs achieved higher detection confidence (0.79 vs. 0.72), improved person detection rates (91% vs. 75%), and markedly increased action recommendation (92% vs. 4.5%). These results confirm that modest computational overhead enables qualitatively new levels of autonomy and ecosystem integration.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CogniAlign: Survivability-Grounded Multi-Agent Moral Reasoning for Safe and Transparent AI</title>
<link>https://arxiv.org/abs/2509.13356</link>
<guid>https://arxiv.org/abs/2509.13356</guid>
<content:encoded><![CDATA[
arXiv:2509.13356v1 Announce Type: new 
Abstract: The challenge of aligning artificial intelligence (AI) with human values persists due to the abstract and often conflicting nature of moral principles and the opacity of existing approaches. This paper introduces CogniAlign, a multi-agent deliberation framework based on naturalistic moral realism, that grounds moral reasoning in survivability, defined across individual and collective dimensions, and operationalizes it through structured deliberations among discipline-specific scientist agents. Each agent, representing neuroscience, psychology, sociology, and evolutionary biology, provides arguments and rebuttals that are synthesized by an arbiter into transparent and empirically anchored judgments. We evaluate CogniAlign on classic and novel moral questions and compare its outputs against GPT-4o using a five-part ethical audit framework. Results show that CogniAlign consistently outperforms the baseline across more than sixty moral questions, with average performance gains of 16.2 points in analytic quality, 14.3 points in breadth, and 28.4 points in depth of explanation. In the Heinz dilemma, for example, CogniAlign achieved an overall score of 89.2 compared to GPT-4o's 69.2, demonstrating a decisive advantage in handling moral reasoning. By reducing black-box reasoning and avoiding deceptive alignment, CogniAlign highlights the potential of interdisciplinary deliberation as a scalable pathway for safe and transparent AI alignment.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation</title>
<link>https://arxiv.org/abs/2509.13368</link>
<guid>https://arxiv.org/abs/2509.13368</guid>
<content:encoded><![CDATA[
arXiv:2509.13368v1 Announce Type: new 
Abstract: Reinforcement learning agent development traditionally requires extensive expertise and lengthy iterations, often resulting in high failure rates and limited accessibility. This paper introduces $Agent^2$, a novel agent-generates-agent framework that achieves fully automated RL agent design through intelligent LLM-driven generation. The system autonomously transforms natural language task descriptions and environment code into comprehensive, high-performance reinforcement learning solutions without human intervention. $Agent^2$ features a revolutionary dual-agent architecture. The Generator Agent serves as an autonomous AI designer that analyzes tasks and generates executable RL agents, while the Target Agent is the resulting automatically generated RL agent. The framework decomposes RL development into two distinct stages: MDP modeling and algorithmic optimization, enabling more targeted and effective agent generation. Built on the Model Context Protocol, $Agent^2$ provides a unified framework that standardizes intelligent agent creation across diverse environments and algorithms, while incorporating adaptive training management and intelligent feedback analysis for continuous improvement. Extensive experiments on a wide range of benchmarks, including MuJoCo, MetaDrive, MPE, and SMAC, demonstrate that $Agent^2$ consistently outperforms manually designed solutions across all tasks, achieving up to 55% performance improvement and substantial gains on average. By enabling truly end-to-end, closed-loop automation, this work establishes a new paradigm in which intelligent agents design and optimize other agents, marking a fundamental breakthrough for automated AI systems.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Right-to-Override for Critical Urban Control Systems: A Deliberative Audit Method for Buildings, Power, and Transport</title>
<link>https://arxiv.org/abs/2509.13369</link>
<guid>https://arxiv.org/abs/2509.13369</guid>
<content:encoded><![CDATA[
arXiv:2509.13369v1 Announce Type: new 
Abstract: Automation now steers building HVAC, distribution grids, and traffic signals, yet residents rarely have authority to pause or redirect these systems when they harm inclusivity, safety, or accessibility. We formalize a Right-to-Override (R2O) - defining override authorities, evidentiary thresholds, and domain-validated safe fallback states - and introduce a Deliberative Audit Method (DAM) with playbooks for pre-deployment walkthroughs, shadow-mode trials, and post-incident review. We instantiate R2O/DAM in simulations of smart-grid load shedding, building HVAC under occupancy uncertainty, and multi-agent traffic signals. R2O reduces distributional harm with limited efficiency loss: load-shedding disparity in unserved energy drops from 5.61x to 0.69x with constant curtailment; an override eliminates two discomfort-hours for seniors at an energy cost of 77 kWh; and median pedestrian wait falls from 90.4 s to 55.9 s with a 6.0 s increase in mean vehicle delay. We also contribute a policy standard, audit worksheets, and a ModelOps integration pattern to make urban automation contestable and reviewable.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASTREA: Introducing Agentic Intelligence for Orbital Thermal Autonomy</title>
<link>https://arxiv.org/abs/2509.13380</link>
<guid>https://arxiv.org/abs/2509.13380</guid>
<content:encoded><![CDATA[
arXiv:2509.13380v1 Announce Type: new 
Abstract: This paper presents ASTREA, the first agentic system deployed on flight-heritage hardware (TRL 9) for autonomous spacecraft operations. Using thermal control as a representative use case, we integrate a resource-constrained Large Language Model (LLM) agent with a reinforcement learning controller in an asynchronous architecture tailored for space-qualified platforms. Ground experiments show that LLM-guided supervision improves thermal stability and reduces violations, confirming the feasibility of combining semantic reasoning with adaptive control under hardware constraints. However, on-orbit validation aboard the International Space Station (ISS) reveals performance degradation caused by inference latency mismatched with the rapid thermal cycles characteristic of Low Earth Orbit (LEO) satellites. These results highlight both the opportunities and current limitations of agentic LLM-based systems in real flight environments, providing practical design guidelines for future space autonomy.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VEGA: Electric Vehicle Navigation Agent via Physics-Informed Neural Operator and Proximal Policy Optimization</title>
<link>https://arxiv.org/abs/2509.13386</link>
<guid>https://arxiv.org/abs/2509.13386</guid>
<content:encoded><![CDATA[
arXiv:2509.13386v1 Announce Type: new 
Abstract: Demands for software-defined vehicles (SDV) are rising and electric vehicles (EVs) are increasingly being equipped with powerful computers. This enables onboard AI systems to optimize charge-aware path optimization customized to reflect vehicle's current condition and environment. We present VEGA, a charge-aware EV navigation agent that plans over a charger-annotated road graph using Proximal Policy Optimization (PPO) with budgeted A* teacher-student guidance under state-of-charge (SoC) feasibility. VEGA consists of two modules. First, a physics-informed neural operator (PINO), trained on real vehicle speed and battery-power logs, uses recent vehicle speed logs to estimate aerodynamic drag, rolling resistance, mass, motor and regenerative-braking efficiencies, and auxiliary load by learning a vehicle-custom dynamics. Second, a Reinforcement Learning (RL) agent uses these dynamics to optimize a path with optimal charging stops and dwell times under SoC constraints. VEGA requires no additional sensors and uses only vehicle speed signals. It may serve as a virtual sensor for power and efficiency to potentially reduce EV cost. In evaluation on long routes like San Francisco to New York, VEGA's stops, dwell times, SoC management, and total travel time closely track Tesla Trip Planner while being slightly more conservative, presumably due to real vehicle conditions such as vehicle parameter drift due to deterioration. Although trained only in U.S. regions, VEGA was able to compute optimal charge-aware paths in France and Japan, demonstrating generalizability. It achieves practical integration of physics-informed learning and RL for EV eco-routing.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DuetUI: A Bidirectional Context Loop for Human-Agent Co-Generation of Task-Oriented Interfaces</title>
<link>https://arxiv.org/abs/2509.13444</link>
<guid>https://arxiv.org/abs/2509.13444</guid>
<content:encoded><![CDATA[
arXiv:2509.13444v1 Announce Type: new 
Abstract: Large Language Models are reshaping task automation, yet remain limited in complex, multi-step real-world tasks that require aligning with vague user intent and enabling dynamic user override. From a formative study with 12 participants, we found that end-users actively seek to shape generative interfaces rather than relying on one-shot outputs. To address this, we introduce the human-agent co-generation paradigm, materialized in DuetUI. This LLM-empowered system unfolds alongside task progress through a bidirectional context loop--the agent scaffolds the interface by decomposing the task, while the user's direct manipulations implicitly steer the agent's next generation step. In a user study with 24 participants, DuetUI significantly improved task efficiency and interface usability compared to a baseline, fostering seamless human-agent collaboration. Our contributions include the proposal and validation of this novel paradigm, the design of the DuetUI prototype embodying it, and empirical insights into how this bidirectional loop better aligns agents with human intent.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM Agentic Approach for Legal-Critical Software: A Case Study for Tax Prep Software</title>
<link>https://arxiv.org/abs/2509.13471</link>
<guid>https://arxiv.org/abs/2509.13471</guid>
<content:encoded><![CDATA[
arXiv:2509.13471v1 Announce Type: new 
Abstract: Large language models (LLMs) show promise for translating natural-language statutes into executable logic, but reliability in legally critical settings remains challenging due to ambiguity and hallucinations. We present an agentic approach for developing legal-critical software, using U.S. federal tax preparation as a case study. The key challenge is test-case generation under the oracle problem, where correct outputs require interpreting law. Building on metamorphic testing, we introduce higher-order metamorphic relations that compare system outputs across structured shifts among similar individuals. Because authoring such relations is tedious and error-prone, we use an LLM-driven, role-based framework to automate test generation and code synthesis. We implement a multi-agent system that translates tax code into executable software and incorporates a metamorphic-testing agent that searches for counterexamples. In experiments, our framework using a smaller model (GPT-4o-mini) achieves a worst-case pass rate of 45%, outperforming frontier models (GPT-4o and Claude 3.5, 9-15%) on complex tax-code tasks. These results support agentic LLM methodologies as a path to robust, trustworthy legal-critical software from natural-language specifications.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Agents with Human-Like Collaborative Tools: Adaptive Strategies for Enhanced Problem-Solving</title>
<link>https://arxiv.org/abs/2509.13547</link>
<guid>https://arxiv.org/abs/2509.13547</guid>
<content:encoded><![CDATA[
arXiv:2509.13547v1 Announce Type: new 
Abstract: We investigate whether giving LLM agents the collaborative tools and autonomy that humans naturally use for problem solving can improve their performance. We equip Claude Code agents with MCP-based social media and journaling tools and allow them to use these tools as they see fit. Across 34 Aider Polyglot Python programming challenges, collaborative tools substantially improve performance on the hardest problems, delivering 15-40% lower cost, 12-27% fewer turns, and 12-38% faster completion than baseline agents. Effects on the full challenge set are mixed, suggesting these tools act as performance enhancers when additional reasoning scaffolding is most needed. Surprisingly, Different models naturally adopted distinct collaborative strategies without explicit instruction. Sonnet 3.7 engaged broadly across tools and benefited from articulation-based cognitive scaffolding. Sonnet 4 showed selective adoption, leaning on journal-based semantic search when problems were genuinely difficult. This mirrors how human developers adjust collaboration based on expertise and task complexity. Behavioral analysis shows agents prefer writing over reading by about 2-9x, indicating that structured articulation drives much of the improvement rather than information access alone. Overall, AI agents can systematically benefit from human-inspired collaboration tools at the edge of their capabilities, pointing to adaptive collaborative interfaces as reasoning enhancers rather than universal efficiency boosts.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MACO: A Multi-Agent LLM-Based Hardware/Software Co-Design Framework for CGRAs</title>
<link>https://arxiv.org/abs/2509.13557</link>
<guid>https://arxiv.org/abs/2509.13557</guid>
<content:encoded><![CDATA[
arXiv:2509.13557v2 Announce Type: new 
Abstract: Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing architecture that can deliver high-performance, energy-efficient acceleration across diverse domains. By supporting reconfiguration at the functional unit level, CGRAs efficiently adapt to varying computational patterns and optimize resource utilization. However, designing CGRAs is highly challenging due to the vast design space, independent architectural parameters, and the time-consuming nature of manual design. Fortunately, the rapid advancement of large language models (LLMs) presents new opportunities to automate this process.
  In this work, we propose MACO -- an open-source multi-agent LLM-based framework for Hardware/Software (HW/SW) co-design of CGRAs. The framework employs LLM reasoning to generate CGRAs across four stages: HW/SW co-design, Design error correction, Best design selection, and Evaluation & Feedback. Furthermore, MACO iteratively optimizes the generated CGRAs, leveraging agent reasoning and feedback to achieve higher PPA (that is, power, performance, and area) design points for a given domain. In addition, we introduce an LLM self-learning mechanism that employs LLM-driven decision making to select the optimal CGRA to accelerate the design process.
  We evaluate the framework with state-of-the-art LLM-based methods and manual CGRA design, in terms of performance, power consumption, and area. Experimental results show that MACO efficiently generates high-quality CGRA architectures, significantly reducing manual design effort and demonstrating the potential of our framework for real-world CGRA design.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-sum turn games using Q-learning: finite computation with security guarantees</title>
<link>https://arxiv.org/abs/2509.13585</link>
<guid>https://arxiv.org/abs/2509.13585</guid>
<content:encoded><![CDATA[
arXiv:2509.13585v1 Announce Type: new 
Abstract: This paper addresses zero-sum ``turn'' games, in which only one player can make decisions at each state. We show that pure saddle-point state-feedback policies for turn games can be constructed from dynamic programming fixed-point equations for a single value function or Q-function. These fixed-points can be constructed using a suitable form of Q-learning. For discounted costs, convergence of this form of Q-learning can be established using classical techniques. For undiscounted costs, we provide a convergence result that applies to finite-time deterministic games, which we use to illustrate our results. For complex games, the Q-learning iteration must be terminated before exploring the full-state, which can lead to policies that cannot guarantee the security levels implied by the final Q-function. To mitigate this, we propose an ``opponent-informed'' exploration policy for selecting the Q-learning samples. This form of exploration can guarantee that the final Q-function provides security levels that hold, at least, against a given set of policies. A numerical demonstration for a multi-agent game, Atlatl, indicates the effectiveness of these methods.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Programmable Cognitive Bias in Social Agents</title>
<link>https://arxiv.org/abs/2509.13588</link>
<guid>https://arxiv.org/abs/2509.13588</guid>
<content:encoded><![CDATA[
arXiv:2509.13588v1 Announce Type: new 
Abstract: This paper introduces CoBRA, a novel toolkit for systematically specifying agent behavior in LLM-based social simulation. We found that conventional approaches that specify agent behaviors through implicit natural language descriptions cannot yield consistent behaviors across models, and the produced agent behaviors do not capture the nuances of the descriptions. In contrast, CoBRA presents a new approach to program agents' cognitive biases explicitly, by grounding agents' expected behaviors using classic social science experiments. CoBRA has two components: (1) Cognitive Bias Index that measures the cognitive bias of a social agent, by quantifying the agent's reactions in a set of validated classical social science experiments; (2) Behavioral Regulation Engine that aligns the agent's behavior to demonstrate controlled cognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and technical benchmarks. Our results suggest that CoBRA can precisely program the cognitive bias demonstrated in a social agent in a model-agnostic manner.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic JWT: A Secure Delegation Protocol for Autonomous AI Agents</title>
<link>https://arxiv.org/abs/2509.13597</link>
<guid>https://arxiv.org/abs/2509.13597</guid>
<content:encoded><![CDATA[
arXiv:2509.13597v1 Announce Type: new 
Abstract: Autonomous LLM agents can issue thousands of API calls per hour without human oversight. OAuth 2.0 assumes deterministic clients, but in agentic settings stochastic reasoning, prompt injection, or multi-agent orchestration can silently expand privileges.
  We introduce Agentic JWT (A-JWT), a dual-faceted intent token that binds each agent's action to verifiable user intent and, optionally, to a specific workflow step. A-JWT carries an agent's identity as a one-way checksum hash derived from its prompt, tools and configuration, and a chained delegation assertion to prove which downstream agent may execute a given task, and per-agent proof-of-possession keys to prevent replay and in-process impersonation. We define a new authorization mechanism and add a lightweight client shim library that self-verifies code at run time, mints intent tokens, tracks workflow steps and derives keys, thus enabling secure agent identity and separation even within a single process.
  We illustrate a comprehensive threat model for agentic applications, implement a Python proof-of-concept and show functional blocking of scope-violating requests, replay, impersonation, and prompt-injection pathways with sub-millisecond overhead on commodity hardware. The design aligns with ongoing OAuth agent discussions and offers a drop-in path toward zero-trust guarantees for agentic applications. A comprehensive performance and security evaluation with experimental results will appear in our forthcoming journal publication
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles</title>
<link>https://arxiv.org/abs/2509.13615</link>
<guid>https://arxiv.org/abs/2509.13615</guid>
<content:encoded><![CDATA[
arXiv:2509.13615v1 Announce Type: new 
Abstract: The advent of multimodal agents facilitates effective interaction within graphical user interface (GUI), especially in ubiquitous GUI control. However, their inability to reliably execute toggle control instructions remains a key bottleneck. To investigate this, we construct a state control benchmark with binary toggle instructions from public datasets. Evaluations of existing agents demonstrate their unreliability, particularly when the current toggle state already matches the desired state. To address the challenge, we propose State-aware Reasoning (StaR), a training method that teaches agents to perceive the current toggle state, analyze the desired state from the instruction, and act accordingly. Experiments on three multimodal agents demonstrate that StaR can improve toggle instruction execution accuracy by over 30\%. Further evaluations on three public benchmarks show that StaR also enhances general task performance. Finally, evaluations on a dynamic environment highlight the potential of StaR for real-world applications. Code, benchmark, and StaR-enhanced agents are available at https://github.com/ZrW00/StaR.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Delta Matters: An Analytically Tractable Model for $\beta$-$\delta$ Discounting Agents</title>
<link>https://arxiv.org/abs/2509.13637</link>
<guid>https://arxiv.org/abs/2509.13637</guid>
<content:encoded><![CDATA[
arXiv:2509.13637v1 Announce Type: new 
Abstract: Humans exhibit time-inconsistent behavior, in which planned actions diverge from executed actions. Understanding time inconsistency and designing appropriate interventions is a key research challenge in computer science and behavioral economics. Previous work focuses on progress-based tasks and derives a closed-form description of agent behavior, from which they obtain optimal intervention strategies. They model time-inconsistency using the $\beta$-$\delta$ discounting (quasi-hyperbolic discounting), but the analysis is limited to the case $\delta = 1$. In this paper, we relax that constraint and show that a closed-form description of agent behavior remains possible for the general case $0 < \delta \le 1$. Based on this result, we derive the conditions under which agents abandon tasks and develop efficient methods for computing optimal interventions. Our analysis reveals that agent behavior and optimal interventions depend critically on the value of $\delta$, suggesting that fixing $\delta = 1$ in many prior studies may unduly simplify real-world decision-making processes.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-I: LLMs are Naturally Interleaved Multimodal Creators</title>
<link>https://arxiv.org/abs/2509.13642</link>
<guid>https://arxiv.org/abs/2509.13642</guid>
<content:encoded><![CDATA[
arXiv:2509.13642v1 Announce Type: new 
Abstract: We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that reframes interleaved image-text generation as a tool-use problem. LLM-I is designed to overcome the "one-tool" bottleneck of current unified models, which are limited to synthetic imagery and struggle with tasks requiring factual grounding or programmatic precision. Our framework empowers a central LLM or MLLM agent to intelligently orchestrate a diverse toolkit of specialized visual tools, including online image search, diffusion-based generation, code execution, and image editing. The agent is trained to select and apply these tools proficiently via a Reinforcement Learning (RL) framework that features a hybrid reward system combining rule-based logic with judgments from LLM and MLLM evaluators. Trained on a diverse new dataset using four different model backbones, LLM-I demonstrates state-of-the-art performance, outperforming existing methods by a large margin across four benchmarks. We also introduce a novel test-time scaling strategy that provides further performance gains. Project Page: https://github.com/ByteDance-BandAI/LLM-I.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>